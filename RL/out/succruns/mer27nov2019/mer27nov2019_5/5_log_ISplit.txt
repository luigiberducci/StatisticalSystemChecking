Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                48        
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/succruns/mer27nov2019_12_59_41_CET, out/succruns/mer27nov2019_12_59_41_CET/levels, out/succruns/mer27nov2019_12_59_41_CET/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting on succruns-v1.
Training for 100000 steps ...
    10/100000: episode: 1, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: --, mae: --, mean_q: --
    20/100000: episode: 2, duration: 0.005s, episode steps: 10, steps per second: 1954, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: --, mae: --, mean_q: --
    30/100000: episode: 3, duration: 0.005s, episode steps: 10, steps per second: 2001, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: --, mae: --, mean_q: --
    40/100000: episode: 4, duration: 0.005s, episode steps: 10, steps per second: 1953, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: --, mae: --, mean_q: --
    50/100000: episode: 5, duration: 0.005s, episode steps: 10, steps per second: 2019, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: --, mae: --, mean_q: --
    60/100000: episode: 6, duration: 0.005s, episode steps: 10, steps per second: 1987, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: --, mae: --, mean_q: --
    70/100000: episode: 7, duration: 0.005s, episode steps: 10, steps per second: 2009, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: --, mae: --, mean_q: --
    80/100000: episode: 8, duration: 0.005s, episode steps: 10, steps per second: 2016, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: --, mae: --, mean_q: --
    90/100000: episode: 9, duration: 0.005s, episode steps: 10, steps per second: 1980, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: --, mae: --, mean_q: --
   100/100000: episode: 10, duration: 0.005s, episode steps: 10, steps per second: 2029, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: --, mae: --, mean_q: --
   110/100000: episode: 11, duration: 0.005s, episode steps: 10, steps per second: 2021, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: --, mae: --, mean_q: --
   120/100000: episode: 12, duration: 0.005s, episode steps: 10, steps per second: 1979, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: --, mae: --, mean_q: --
   130/100000: episode: 13, duration: 0.005s, episode steps: 10, steps per second: 2083, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: --, mae: --, mean_q: --
   140/100000: episode: 14, duration: 0.005s, episode steps: 10, steps per second: 2048, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: --, mae: --, mean_q: --
   150/100000: episode: 15, duration: 0.005s, episode steps: 10, steps per second: 2069, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: --, mae: --, mean_q: --
   160/100000: episode: 16, duration: 0.005s, episode steps: 10, steps per second: 2078, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: --, mae: --, mean_q: --
   170/100000: episode: 17, duration: 0.005s, episode steps: 10, steps per second: 2031, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: --, mae: --, mean_q: --
   180/100000: episode: 18, duration: 0.005s, episode steps: 10, steps per second: 2084, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: --, mae: --, mean_q: --
   190/100000: episode: 19, duration: 0.005s, episode steps: 10, steps per second: 1993, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: --, mae: --, mean_q: --
   200/100000: episode: 20, duration: 0.005s, episode steps: 10, steps per second: 2035, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: --, mae: --, mean_q: --
   210/100000: episode: 21, duration: 0.005s, episode steps: 10, steps per second: 2036, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: --, mae: --, mean_q: --
   220/100000: episode: 22, duration: 0.005s, episode steps: 10, steps per second: 2074, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: --, mae: --, mean_q: --
   230/100000: episode: 23, duration: 0.005s, episode steps: 10, steps per second: 2114, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: --, mae: --, mean_q: --
   240/100000: episode: 24, duration: 0.005s, episode steps: 10, steps per second: 2140, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: --, mae: --, mean_q: --
   250/100000: episode: 25, duration: 0.005s, episode steps: 10, steps per second: 2077, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: --, mae: --, mean_q: --
   260/100000: episode: 26, duration: 0.005s, episode steps: 10, steps per second: 2053, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: --, mae: --, mean_q: --
   270/100000: episode: 27, duration: 0.005s, episode steps: 10, steps per second: 2124, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: --, mae: --, mean_q: --
   280/100000: episode: 28, duration: 0.005s, episode steps: 10, steps per second: 2134, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: --, mae: --, mean_q: --
   290/100000: episode: 29, duration: 0.005s, episode steps: 10, steps per second: 2103, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: --, mae: --, mean_q: --
   300/100000: episode: 30, duration: 0.005s, episode steps: 10, steps per second: 2136, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: --, mae: --, mean_q: --
   310/100000: episode: 31, duration: 0.005s, episode steps: 10, steps per second: 2119, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: --, mae: --, mean_q: --
   320/100000: episode: 32, duration: 0.005s, episode steps: 10, steps per second: 1995, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: --, mae: --, mean_q: --
   330/100000: episode: 33, duration: 0.005s, episode steps: 10, steps per second: 2180, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: --, mae: --, mean_q: --
   340/100000: episode: 34, duration: 0.005s, episode steps: 10, steps per second: 2045, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: --, mae: --, mean_q: --
   350/100000: episode: 35, duration: 0.005s, episode steps: 10, steps per second: 2062, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: --, mae: --, mean_q: --
   360/100000: episode: 36, duration: 0.005s, episode steps: 10, steps per second: 2084, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: --, mae: --, mean_q: --
   370/100000: episode: 37, duration: 0.005s, episode steps: 10, steps per second: 2073, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: --, mae: --, mean_q: --
   380/100000: episode: 38, duration: 0.005s, episode steps: 10, steps per second: 2076, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: --, mae: --, mean_q: --
   390/100000: episode: 39, duration: 0.005s, episode steps: 10, steps per second: 2079, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: --, mae: --, mean_q: --
   400/100000: episode: 40, duration: 0.005s, episode steps: 10, steps per second: 2091, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: --, mae: --, mean_q: --
   410/100000: episode: 41, duration: 0.005s, episode steps: 10, steps per second: 2138, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: --, mae: --, mean_q: --
   420/100000: episode: 42, duration: 0.005s, episode steps: 10, steps per second: 2129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: --, mae: --, mean_q: --
   430/100000: episode: 43, duration: 0.005s, episode steps: 10, steps per second: 2125, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: --, mae: --, mean_q: --
   440/100000: episode: 44, duration: 0.005s, episode steps: 10, steps per second: 2136, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: --, mae: --, mean_q: --
   450/100000: episode: 45, duration: 0.005s, episode steps: 10, steps per second: 2147, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: --, mae: --, mean_q: --
   460/100000: episode: 46, duration: 0.005s, episode steps: 10, steps per second: 2135, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: --, mae: --, mean_q: --
   470/100000: episode: 47, duration: 0.005s, episode steps: 10, steps per second: 2124, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: --, mae: --, mean_q: --
   480/100000: episode: 48, duration: 0.005s, episode steps: 10, steps per second: 2152, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: --, mae: --, mean_q: --
   490/100000: episode: 49, duration: 0.005s, episode steps: 10, steps per second: 2150, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: --, mae: --, mean_q: --
   500/100000: episode: 50, duration: 0.005s, episode steps: 10, steps per second: 2134, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: --, mae: --, mean_q: --
   510/100000: episode: 51, duration: 0.604s, episode steps: 10, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.023931, mae: 0.142952, mean_q: -0.274993
   520/100000: episode: 52, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.019471, mae: 0.146795, mean_q: -0.251628
   530/100000: episode: 53, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.018920, mae: 0.135165, mean_q: -0.257315
   540/100000: episode: 54, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.022970, mae: 0.126726, mean_q: -0.254335
   550/100000: episode: 55, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.016583, mae: 0.118734, mean_q: -0.252009
   560/100000: episode: 56, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.016888, mae: 0.123901, mean_q: -0.277709
   570/100000: episode: 57, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.014395, mae: 0.108555, mean_q: -0.254785
   580/100000: episode: 58, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.012957, mae: 0.108143, mean_q: -0.254022
   590/100000: episode: 59, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.014083, mae: 0.101917, mean_q: -0.263165
   600/100000: episode: 60, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.011004, mae: 0.089573, mean_q: -0.269182
   610/100000: episode: 61, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.009175, mae: 0.083630, mean_q: -0.245532
   620/100000: episode: 62, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.013340, mae: 0.099608, mean_q: -0.255351
   630/100000: episode: 63, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.009831, mae: 0.094962, mean_q: -0.242043
   640/100000: episode: 64, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.010639, mae: 0.090259, mean_q: -0.242471
   650/100000: episode: 65, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.013563, mae: 0.096091, mean_q: -0.259379
   660/100000: episode: 66, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.012167, mae: 0.090727, mean_q: -0.237476
   670/100000: episode: 67, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.008704, mae: 0.090643, mean_q: -0.229152
   680/100000: episode: 68, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.009223, mae: 0.083893, mean_q: -0.240856
   690/100000: episode: 69, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.007842, mae: 0.084629, mean_q: -0.224570
   700/100000: episode: 70, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.009743, mae: 0.088618, mean_q: -0.208564
   710/100000: episode: 71, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.007979, mae: 0.082120, mean_q: -0.222354
   720/100000: episode: 72, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.007845, mae: 0.075515, mean_q: -0.231272
   730/100000: episode: 73, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.006634, mae: 0.071868, mean_q: -0.226962
   740/100000: episode: 74, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.006883, mae: 0.072493, mean_q: -0.233901
   750/100000: episode: 75, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.005602, mae: 0.067456, mean_q: -0.218299
   760/100000: episode: 76, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.006464, mae: 0.074966, mean_q: -0.210669
   770/100000: episode: 77, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.008004, mae: 0.077406, mean_q: -0.214753
   780/100000: episode: 78, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.005454, mae: 0.066110, mean_q: -0.206970
   790/100000: episode: 79, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.003487, mae: 0.057316, mean_q: -0.204501
   800/100000: episode: 80, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.005240, mae: 0.059343, mean_q: -0.211466
   810/100000: episode: 81, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.002906, mae: 0.052452, mean_q: -0.197637
   820/100000: episode: 82, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.004330, mae: 0.058846, mean_q: -0.205920
   830/100000: episode: 83, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.003731, mae: 0.052232, mean_q: -0.200368
   840/100000: episode: 84, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.003362, mae: 0.049361, mean_q: -0.200145
   850/100000: episode: 85, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.004944, mae: 0.050305, mean_q: -0.204640
   860/100000: episode: 86, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.003230, mae: 0.049124, mean_q: -0.196602
   870/100000: episode: 87, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.004271, mae: 0.054472, mean_q: -0.186166
   880/100000: episode: 88, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.004456, mae: 0.057442, mean_q: -0.179561
   890/100000: episode: 89, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.003255, mae: 0.052232, mean_q: -0.172828
   900/100000: episode: 90, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.001646, mae: 0.037523, mean_q: -0.190313
   910/100000: episode: 91, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.002421, mae: 0.040248, mean_q: -0.195747
   920/100000: episode: 92, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.002455, mae: 0.042783, mean_q: -0.174389
   930/100000: episode: 93, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.002038, mae: 0.036154, mean_q: -0.179112
   940/100000: episode: 94, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001536, mae: 0.037239, mean_q: -0.172287
   950/100000: episode: 95, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.002237, mae: 0.036095, mean_q: -0.180079
   960/100000: episode: 96, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.001697, mae: 0.036335, mean_q: -0.171721
   970/100000: episode: 97, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.002340, mae: 0.037145, mean_q: -0.164417
   980/100000: episode: 98, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001285, mae: 0.031884, mean_q: -0.165610
   990/100000: episode: 99, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.002325, mae: 0.038795, mean_q: -0.163124
Step 1000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.1000.hdf5
  1000/100000: episode: 100, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.002000, mae: 0.037364, mean_q: -0.153198
  1010/100000: episode: 101, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000998, mae: 0.029521, mean_q: -0.157677
  1020/100000: episode: 102, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001571, mae: 0.027580, mean_q: -0.163934
  1030/100000: episode: 103, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.001344, mae: 0.031674, mean_q: -0.152274
  1040/100000: episode: 104, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001803, mae: 0.031970, mean_q: -0.150827
  1050/100000: episode: 105, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.001140, mae: 0.027115, mean_q: -0.147397
  1060/100000: episode: 106, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001459, mae: 0.028755, mean_q: -0.149864
  1070/100000: episode: 107, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001148, mae: 0.027867, mean_q: -0.141066
  1080/100000: episode: 108, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000655, mae: 0.022474, mean_q: -0.149707
  1090/100000: episode: 109, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000742, mae: 0.020934, mean_q: -0.143543
  1100/100000: episode: 110, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001200, mae: 0.027721, mean_q: -0.137552
  1110/100000: episode: 111, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000688, mae: 0.022952, mean_q: -0.135128
  1120/100000: episode: 112, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000926, mae: 0.023114, mean_q: -0.135756
  1130/100000: episode: 113, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000601, mae: 0.021833, mean_q: -0.132949
  1140/100000: episode: 114, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000831, mae: 0.022248, mean_q: -0.132831
  1150/100000: episode: 115, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.001211, mae: 0.024550, mean_q: -0.130650
  1160/100000: episode: 116, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000600, mae: 0.022091, mean_q: -0.127418
  1170/100000: episode: 117, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000732, mae: 0.020235, mean_q: -0.124506
  1180/100000: episode: 118, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001174, mae: 0.022658, mean_q: -0.120925
  1190/100000: episode: 119, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000803, mae: 0.021646, mean_q: -0.120813
  1200/100000: episode: 120, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000426, mae: 0.018269, mean_q: -0.120698
  1210/100000: episode: 121, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000475, mae: 0.018616, mean_q: -0.117422
  1220/100000: episode: 122, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000586, mae: 0.017441, mean_q: -0.120989
  1230/100000: episode: 123, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000898, mae: 0.020239, mean_q: -0.113144
  1240/100000: episode: 124, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000899, mae: 0.019575, mean_q: -0.115970
  1250/100000: episode: 125, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000342, mae: 0.016361, mean_q: -0.109642
  1260/100000: episode: 126, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000322, mae: 0.016684, mean_q: -0.105452
  1270/100000: episode: 127, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000306, mae: 0.016425, mean_q: -0.110630
  1280/100000: episode: 128, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000279, mae: 0.015814, mean_q: -0.103259
  1290/100000: episode: 129, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000597, mae: 0.017836, mean_q: -0.102958
  1300/100000: episode: 130, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000600, mae: 0.017588, mean_q: -0.104500
  1310/100000: episode: 131, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000548, mae: 0.015820, mean_q: -0.098429
  1320/100000: episode: 132, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000762, mae: 0.018037, mean_q: -0.095146
  1330/100000: episode: 133, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000394, mae: 0.017354, mean_q: -0.084719
  1340/100000: episode: 134, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000477, mae: 0.016153, mean_q: -0.092403
  1350/100000: episode: 135, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000190, mae: 0.014504, mean_q: -0.088426
  1360/100000: episode: 136, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000127, mae: 0.012919, mean_q: -0.086420
  1370/100000: episode: 137, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000714, mae: 0.015595, mean_q: -0.086839
  1380/100000: episode: 138, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000983, mae: 0.017489, mean_q: -0.084448
  1390/100000: episode: 139, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000448, mae: 0.014092, mean_q: -0.087831
  1400/100000: episode: 140, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000173, mae: 0.012817, mean_q: -0.081332
  1410/100000: episode: 141, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000388, mae: 0.013279, mean_q: -0.079375
  1420/100000: episode: 142, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000141, mae: 0.012061, mean_q: -0.077533
  1430/100000: episode: 143, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000209, mae: 0.012766, mean_q: -0.073341
[Info] FALSIFICATION!
  1440/100000: episode: 144, duration: 0.650s, episode steps: 10, steps per second: 15, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000431, mae: 0.013566, mean_q: -0.074699
  1450/100000: episode: 145, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001785, mae: 0.015445, mean_q: -0.068867
  1460/100000: episode: 146, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000121, mae: 0.011207, mean_q: -0.069890
  1470/100000: episode: 147, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000162, mae: 0.011769, mean_q: -0.066018
  1480/100000: episode: 148, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.001962, mae: 0.015605, mean_q: -0.066828
  1490/100000: episode: 149, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000289, mae: 0.011039, mean_q: -0.062815
  1500/100000: episode: 150, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000387, mae: 0.012068, mean_q: -0.058913
  1510/100000: episode: 151, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000107, mae: 0.010574, mean_q: -0.060001
  1520/100000: episode: 152, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000371, mae: 0.012413, mean_q: -0.059249
  1530/100000: episode: 153, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000145, mae: 0.010286, mean_q: -0.058720
  1540/100000: episode: 154, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.003434, mae: 0.017533, mean_q: -0.054507
  1550/100000: episode: 155, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000162, mae: 0.010987, mean_q: -0.048987
  1560/100000: episode: 156, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000085, mae: 0.009417, mean_q: -0.050212
  1570/100000: episode: 157, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000296, mae: 0.010968, mean_q: -0.048875
  1580/100000: episode: 158, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000496, mae: 0.011861, mean_q: -0.049724
  1590/100000: episode: 159, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000790, mae: 0.014425, mean_q: -0.045291
  1600/100000: episode: 160, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001659, mae: 0.013090, mean_q: -0.044423
  1610/100000: episode: 161, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000450, mae: 0.011185, mean_q: -0.042630
  1620/100000: episode: 162, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000303, mae: 0.011422, mean_q: -0.041646
  1630/100000: episode: 163, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000441, mae: 0.011252, mean_q: -0.041071
  1640/100000: episode: 164, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000073, mae: 0.009117, mean_q: -0.039511
  1650/100000: episode: 165, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001763, mae: 0.013025, mean_q: -0.037292
  1660/100000: episode: 166, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000320, mae: 0.011728, mean_q: -0.033090
  1670/100000: episode: 167, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000050, mae: 0.008838, mean_q: -0.033877
  1680/100000: episode: 168, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000505, mae: 0.012571, mean_q: -0.033879
  1690/100000: episode: 169, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000304, mae: 0.010837, mean_q: -0.029398
  1700/100000: episode: 170, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000471, mae: 0.011591, mean_q: -0.028711
  1710/100000: episode: 171, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000111, mae: 0.009844, mean_q: -0.028701
  1720/100000: episode: 172, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000260, mae: 0.010177, mean_q: -0.026580
  1730/100000: episode: 173, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000084, mae: 0.008767, mean_q: -0.025942
  1740/100000: episode: 174, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000445, mae: 0.010646, mean_q: -0.022734
  1750/100000: episode: 175, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000672, mae: 0.012549, mean_q: -0.025343
  1760/100000: episode: 176, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000315, mae: 0.011313, mean_q: -0.017542
  1770/100000: episode: 177, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000087, mae: 0.009142, mean_q: -0.020250
  1780/100000: episode: 178, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000269, mae: 0.009863, mean_q: -0.018244
  1790/100000: episode: 179, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000456, mae: 0.010983, mean_q: -0.012470
  1800/100000: episode: 180, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000272, mae: 0.009955, mean_q: -0.013339
  1810/100000: episode: 181, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000045, mae: 0.008133, mean_q: -0.015979
  1820/100000: episode: 182, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000040, mae: 0.007400, mean_q: -0.013482
  1830/100000: episode: 183, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000456, mae: 0.010180, mean_q: -0.013786
  1840/100000: episode: 184, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000060, mae: 0.007486, mean_q: -0.007670
  1850/100000: episode: 185, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000249, mae: 0.008814, mean_q: -0.010303
  1860/100000: episode: 186, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000075, mae: 0.007038, mean_q: -0.007723
  1870/100000: episode: 187, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000264, mae: 0.008284, mean_q: -0.009451
  1880/100000: episode: 188, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000042, mae: 0.007156, mean_q: -0.007393
  1890/100000: episode: 189, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000243, mae: 0.008304, mean_q: -0.006541
  1900/100000: episode: 190, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000247, mae: 0.008431, mean_q: -0.006454
  1910/100000: episode: 191, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000059, mae: 0.007249, mean_q: -0.003844
  1920/100000: episode: 192, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000227, mae: 0.007842, mean_q: -0.002285
  1930/100000: episode: 193, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000264, mae: 0.008442, mean_q: -0.001545
  1940/100000: episode: 194, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000284, mae: 0.008953, mean_q: 0.004043
  1950/100000: episode: 195, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000631, mae: 0.010428, mean_q: 0.001356
  1960/100000: episode: 196, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000627, mae: 0.011012, mean_q: 0.006757
  1970/100000: episode: 197, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.002599, mae: 0.017342, mean_q: 0.010168
  1980/100000: episode: 198, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000105, mae: 0.009767, mean_q: 0.011151
  1990/100000: episode: 199, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000420, mae: 0.010073, mean_q: 0.011929
Step 2000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.2000.hdf5
[Info] Complete ISplit Iteration
[Info] Levels: [0.030632997]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

  2000/100000: episode: 200, duration: 0.947s, episode steps: 10, steps per second: 11, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000990, mae: 0.013959, mean_q: 0.007643
  2010/100000: episode: 201, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000408, mae: 0.009829, mean_q: 0.012201
  2020/100000: episode: 202, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000783, mae: 0.012039, mean_q: 0.010420
  2030/100000: episode: 203, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.001867, mae: 0.012248, mean_q: 0.014296
  2040/100000: episode: 204, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000426, mae: 0.010358, mean_q: 0.014397
  2050/100000: episode: 205, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000067, mae: 0.007497, mean_q: 0.014309
  2060/100000: episode: 206, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001537, mae: 0.010333, mean_q: 0.018220
  2070/100000: episode: 207, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000596, mae: 0.010728, mean_q: 0.017849
  2080/100000: episode: 208, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000066, mae: 0.007228, mean_q: 0.018285
  2090/100000: episode: 209, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001543, mae: 0.010067, mean_q: 0.019994
  2100/100000: episode: 210, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000260, mae: 0.008433, mean_q: 0.020175
  2110/100000: episode: 211, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000227, mae: 0.007723, mean_q: 0.018913
  2120/100000: episode: 212, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000230, mae: 0.007135, mean_q: 0.019837
  2130/100000: episode: 213, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000090, mae: 0.007145, mean_q: 0.024461
  2140/100000: episode: 214, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000234, mae: 0.006920, mean_q: 0.019445
  2150/100000: episode: 215, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000253, mae: 0.007063, mean_q: 0.021477
  2160/100000: episode: 216, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001710, mae: 0.010409, mean_q: 0.023108
  2170/100000: episode: 217, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000091, mae: 0.007205, mean_q: 0.024843
  2180/100000: episode: 218, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000239, mae: 0.006954, mean_q: 0.026748
  2190/100000: episode: 219, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000390, mae: 0.007565, mean_q: 0.028133
  2200/100000: episode: 220, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000056, mae: 0.006076, mean_q: 0.025275
  2210/100000: episode: 221, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.002053, mae: 0.012661, mean_q: 0.026052
  2220/100000: episode: 222, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000426, mae: 0.009897, mean_q: 0.029040
  2230/100000: episode: 223, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001849, mae: 0.013074, mean_q: 0.031783
  2240/100000: episode: 224, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001882, mae: 0.013238, mean_q: 0.031949
  2250/100000: episode: 225, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000104, mae: 0.007899, mean_q: 0.031518
  2260/100000: episode: 226, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001861, mae: 0.011322, mean_q: 0.031013
  2270/100000: episode: 227, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000046, mae: 0.005804, mean_q: 0.032030
  2280/100000: episode: 228, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001529, mae: 0.009454, mean_q: 0.031868
  2290/100000: episode: 229, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001711, mae: 0.011017, mean_q: 0.032789
  2300/100000: episode: 230, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.001856, mae: 0.012451, mean_q: 0.033444
  2310/100000: episode: 231, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000249, mae: 0.008404, mean_q: 0.036749
  2320/100000: episode: 232, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001865, mae: 0.011709, mean_q: 0.035598
  2330/100000: episode: 233, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000082, mae: 0.006508, mean_q: 0.036176
  2340/100000: episode: 234, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000091, mae: 0.005410, mean_q: 0.035574
  2350/100000: episode: 235, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000251, mae: 0.005899, mean_q: 0.034003
  2360/100000: episode: 236, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000226, mae: 0.005417, mean_q: 0.033768
  2370/100000: episode: 237, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000077, mae: 0.004479, mean_q: 0.033614
  2380/100000: episode: 238, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000245, mae: 0.005481, mean_q: 0.034649
  2390/100000: episode: 239, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000046, mae: 0.003792, mean_q: 0.035062
  2400/100000: episode: 240, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000624, mae: 0.008095, mean_q: 0.034746
  2410/100000: episode: 241, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000050, mae: 0.003571, mean_q: 0.036347
  2420/100000: episode: 242, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000557, mae: 0.006853, mean_q: 0.036450
  2430/100000: episode: 243, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000250, mae: 0.005344, mean_q: 0.036084
  2440/100000: episode: 244, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000557, mae: 0.006362, mean_q: 0.035817
  2450/100000: episode: 245, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000436, mae: 0.006741, mean_q: 0.037109
  2460/100000: episode: 246, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000070, mae: 0.004367, mean_q: 0.036423
  2470/100000: episode: 247, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000061, mae: 0.005101, mean_q: 0.034638
  2480/100000: episode: 248, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000417, mae: 0.007086, mean_q: 0.034286
  2490/100000: episode: 249, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000752, mae: 0.008597, mean_q: 0.035354
  2500/100000: episode: 250, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000085, mae: 0.004107, mean_q: 0.036310
  2510/100000: episode: 251, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000068, mae: 0.003417, mean_q: 0.036254
  2520/100000: episode: 252, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000605, mae: 0.007397, mean_q: 0.036036
  2530/100000: episode: 253, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000415, mae: 0.006024, mean_q: 0.036603
  2540/100000: episode: 254, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000042, mae: 0.003319, mean_q: 0.036331
  2550/100000: episode: 255, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000231, mae: 0.004604, mean_q: 0.035203
  2560/100000: episode: 256, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000411, mae: 0.005916, mean_q: 0.035575
[Info] FALSIFICATION!
  2570/100000: episode: 257, duration: 0.301s, episode steps: 10, steps per second: 33, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.001835, mae: 0.007994, mean_q: 0.036933
  2580/100000: episode: 258, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000388, mae: 0.007237, mean_q: 0.038824
  2590/100000: episode: 259, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000243, mae: 0.005900, mean_q: 0.037747
  2600/100000: episode: 260, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000436, mae: 0.005302, mean_q: 0.036406
  2610/100000: episode: 261, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000195, mae: 0.003218, mean_q: 0.037402
  2620/100000: episode: 262, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000395, mae: 0.005650, mean_q: 0.037618
  2630/100000: episode: 263, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000226, mae: 0.003596, mean_q: 0.037173
  2640/100000: episode: 264, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000239, mae: 0.005190, mean_q: 0.036713
  2650/100000: episode: 265, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001521, mae: 0.007038, mean_q: 0.035867
  2660/100000: episode: 266, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000044, mae: 0.002626, mean_q: 0.036508
  2670/100000: episode: 267, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001682, mae: 0.007950, mean_q: 0.037475
  2680/100000: episode: 268, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.002041, mae: 0.010794, mean_q: 0.038752
  2690/100000: episode: 269, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000230, mae: 0.006802, mean_q: 0.040496
  2700/100000: episode: 270, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000039, mae: 0.003706, mean_q: 0.038461
  2710/100000: episode: 271, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000255, mae: 0.005248, mean_q: 0.037021
  2720/100000: episode: 272, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000058, mae: 0.003720, mean_q: 0.036727
  2730/100000: episode: 273, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000066, mae: 0.004022, mean_q: 0.036298
  2740/100000: episode: 274, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000092, mae: 0.005238, mean_q: 0.036145
  2750/100000: episode: 275, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000748, mae: 0.008389, mean_q: 0.035956
  2760/100000: episode: 276, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000208, mae: 0.003636, mean_q: 0.037013
  2770/100000: episode: 277, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001679, mae: 0.007317, mean_q: 0.036939
  2780/100000: episode: 278, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000035, mae: 0.002543, mean_q: 0.037376
  2790/100000: episode: 279, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000223, mae: 0.003321, mean_q: 0.037405
  2800/100000: episode: 280, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000223, mae: 0.004353, mean_q: 0.037624
  2810/100000: episode: 281, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.001702, mae: 0.008524, mean_q: 0.038426
  2820/100000: episode: 282, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000226, mae: 0.005719, mean_q: 0.038659
  2830/100000: episode: 283, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000207, mae: 0.004181, mean_q: 0.036636
  2840/100000: episode: 284, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000234, mae: 0.005682, mean_q: 0.035625
  2850/100000: episode: 285, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000230, mae: 0.005708, mean_q: 0.035299
  2860/100000: episode: 286, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.001721, mae: 0.008471, mean_q: 0.036036
  2870/100000: episode: 287, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000074, mae: 0.004971, mean_q: 0.037896
  2880/100000: episode: 288, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000073, mae: 0.003921, mean_q: 0.036431
  2890/100000: episode: 289, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000070, mae: 0.004989, mean_q: 0.034766
  2900/100000: episode: 290, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000235, mae: 0.005487, mean_q: 0.034669
  2910/100000: episode: 291, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001510, mae: 0.006644, mean_q: 0.036403
  2920/100000: episode: 292, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.001492, mae: 0.005935, mean_q: 0.037119
  2930/100000: episode: 293, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000225, mae: 0.004378, mean_q: 0.037313
  2940/100000: episode: 294, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000249, mae: 0.004889, mean_q: 0.037024
  2950/100000: episode: 295, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000554, mae: 0.006114, mean_q: 0.036184
  2960/100000: episode: 296, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000251, mae: 0.005293, mean_q: 0.036063
  2970/100000: episode: 297, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000069, mae: 0.003894, mean_q: 0.035734
  2980/100000: episode: 298, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000051, mae: 0.003006, mean_q: 0.036213
  2990/100000: episode: 299, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.001533, mae: 0.007072, mean_q: 0.036504
Step 3000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.3000.hdf5
[Info] Complete ISplit Iteration
[Info] Levels: [0.03847096]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

  3000/100000: episode: 300, duration: 0.829s, episode steps: 10, steps per second: 12, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000542, mae: 0.005924, mean_q: 0.037375
  3010/100000: episode: 301, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000401, mae: 0.006434, mean_q: 0.038064
  3020/100000: episode: 302, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.003368, mean_q: 0.037022
  3030/100000: episode: 303, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000233, mae: 0.004389, mean_q: 0.035896
  3040/100000: episode: 304, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000090, mae: 0.004022, mean_q: 0.035686
  3050/100000: episode: 305, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000361, mae: 0.003652, mean_q: 0.036331
  3060/100000: episode: 306, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000039, mae: 0.003364, mean_q: 0.038158
  3070/100000: episode: 307, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000216, mae: 0.004181, mean_q: 0.037397
  3080/100000: episode: 308, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.002917, mean_q: 0.036899
  3090/100000: episode: 309, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000273, mae: 0.005832, mean_q: 0.035903
  3100/100000: episode: 310, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001499, mae: 0.006543, mean_q: 0.036961
  3110/100000: episode: 311, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000229, mae: 0.004469, mean_q: 0.036373
  3120/100000: episode: 312, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000266, mae: 0.006039, mean_q: 0.035399
  3130/100000: episode: 313, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001529, mae: 0.007302, mean_q: 0.035635
  3140/100000: episode: 314, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000593, mae: 0.006812, mean_q: 0.037130
  3150/100000: episode: 315, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000108, mae: 0.005065, mean_q: 0.037577
  3160/100000: episode: 316, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000227, mae: 0.003673, mean_q: 0.036227
  3170/100000: episode: 317, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.002022, mae: 0.009944, mean_q: 0.037286
  3180/100000: episode: 318, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000418, mae: 0.006861, mean_q: 0.038547
  3190/100000: episode: 319, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000246, mae: 0.005598, mean_q: 0.038089
  3200/100000: episode: 320, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000248, mae: 0.005719, mean_q: 0.036416
  3210/100000: episode: 321, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001687, mae: 0.007814, mean_q: 0.036493
  3220/100000: episode: 322, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000054, mae: 0.003281, mean_q: 0.036509
  3230/100000: episode: 323, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000215, mae: 0.005007, mean_q: 0.035527
  3240/100000: episode: 324, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000199, mae: 0.004197, mean_q: 0.035245
  3250/100000: episode: 325, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001564, mae: 0.008491, mean_q: 0.035128
  3260/100000: episode: 326, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.001701, mae: 0.007912, mean_q: 0.036862
  3270/100000: episode: 327, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000743, mae: 0.009546, mean_q: 0.038906
  3280/100000: episode: 328, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000231, mae: 0.005478, mean_q: 0.038653
  3290/100000: episode: 329, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001708, mae: 0.008190, mean_q: 0.037201
  3300/100000: episode: 330, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000236, mae: 0.004795, mean_q: 0.037909
  3310/100000: episode: 331, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000397, mae: 0.005310, mean_q: 0.037336
  3320/100000: episode: 332, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000214, mae: 0.004267, mean_q: 0.036679
  3330/100000: episode: 333, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000043, mae: 0.003749, mean_q: 0.035957
  3340/100000: episode: 334, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000044, mae: 0.003918, mean_q: 0.034959
  3350/100000: episode: 335, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000435, mae: 0.007285, mean_q: 0.034901
  3360/100000: episode: 336, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000063, mae: 0.004116, mean_q: 0.035781
  3370/100000: episode: 337, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000045, mae: 0.003240, mean_q: 0.035313
  3380/100000: episode: 338, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.003573, mean_q: 0.034714
  3390/100000: episode: 339, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001498, mae: 0.007317, mean_q: 0.034060
  3400/100000: episode: 340, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000083, mae: 0.004134, mean_q: 0.035327
  3410/100000: episode: 341, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000394, mae: 0.004797, mean_q: 0.036115
  3420/100000: episode: 342, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.002950, mae: 0.009911, mean_q: 0.037351
  3430/100000: episode: 343, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.001517, mae: 0.008639, mean_q: 0.038118
  3440/100000: episode: 344, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000227, mae: 0.005192, mean_q: 0.037542
  3450/100000: episode: 345, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000390, mae: 0.004717, mean_q: 0.036225
  3460/100000: episode: 346, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000377, mae: 0.004427, mean_q: 0.036129
  3470/100000: episode: 347, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.001673, mae: 0.006930, mean_q: 0.036670
  3480/100000: episode: 348, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000099, mae: 0.005042, mean_q: 0.037172
  3490/100000: episode: 349, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.001698, mae: 0.007852, mean_q: 0.036270
  3500/100000: episode: 350, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000246, mae: 0.005491, mean_q: 0.037475
  3510/100000: episode: 351, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000232, mae: 0.004782, mean_q: 0.036703
  3520/100000: episode: 352, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000229, mae: 0.005401, mean_q: 0.034984
  3530/100000: episode: 353, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000228, mae: 0.005473, mean_q: 0.035049
  3540/100000: episode: 354, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000224, mae: 0.004686, mean_q: 0.035140
  3550/100000: episode: 355, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000227, mae: 0.004752, mean_q: 0.035228
  3560/100000: episode: 356, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000220, mae: 0.004270, mean_q: 0.035165
  3570/100000: episode: 357, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000043, mae: 0.003935, mean_q: 0.034754
  3580/100000: episode: 358, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000232, mae: 0.005674, mean_q: 0.034100
  3590/100000: episode: 359, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001512, mae: 0.007295, mean_q: 0.034207
  3600/100000: episode: 360, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000208, mae: 0.003524, mean_q: 0.035500
  3610/100000: episode: 361, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000411, mae: 0.005165, mean_q: 0.035486
  3620/100000: episode: 362, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000068, mae: 0.003420, mean_q: 0.035539
  3630/100000: episode: 363, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000094, mae: 0.004136, mean_q: 0.034672
  3640/100000: episode: 364, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000064, mae: 0.003955, mean_q: 0.034167
  3650/100000: episode: 365, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000202, mae: 0.004000, mean_q: 0.033921
  3660/100000: episode: 366, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000602, mae: 0.007378, mean_q: 0.034631
  3670/100000: episode: 367, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000079, mae: 0.004118, mean_q: 0.035027
  3680/100000: episode: 368, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000220, mae: 0.004430, mean_q: 0.033907
  3690/100000: episode: 369, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000453, mae: 0.005918, mean_q: 0.034312
  3700/100000: episode: 370, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000071, mae: 0.004003, mean_q: 0.035241
  3710/100000: episode: 371, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000386, mae: 0.005233, mean_q: 0.034341
  3720/100000: episode: 372, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000064, mae: 0.004566, mean_q: 0.033631
  3730/100000: episode: 373, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.003335, mae: 0.012574, mean_q: 0.033583
  3740/100000: episode: 374, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000206, mae: 0.005254, mean_q: 0.036771
  3750/100000: episode: 375, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000213, mae: 0.004293, mean_q: 0.036009
  3760/100000: episode: 376, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.002988, mean_q: 0.034572
  3770/100000: episode: 377, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000219, mae: 0.003772, mean_q: 0.034832
  3780/100000: episode: 378, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002941, mean_q: 0.034467
  3790/100000: episode: 379, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000255, mae: 0.005556, mean_q: 0.033359
  3800/100000: episode: 380, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000229, mae: 0.004583, mean_q: 0.033944
  3810/100000: episode: 381, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000227, mae: 0.004398, mean_q: 0.034895
  3820/100000: episode: 382, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000585, mae: 0.006691, mean_q: 0.035102
  3830/100000: episode: 383, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000045, mae: 0.003584, mean_q: 0.034937
  3840/100000: episode: 384, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000247, mae: 0.005521, mean_q: 0.033480
  3850/100000: episode: 385, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.004499, mean_q: 0.032764
  3860/100000: episode: 386, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000064, mae: 0.004922, mean_q: 0.031893
  3870/100000: episode: 387, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000403, mae: 0.005663, mean_q: 0.033096
  3880/100000: episode: 388, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001678, mae: 0.007846, mean_q: 0.035407
  3890/100000: episode: 389, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000239, mae: 0.005484, mean_q: 0.035540
  3900/100000: episode: 390, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000249, mae: 0.004553, mean_q: 0.034724
  3910/100000: episode: 391, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000444, mae: 0.007591, mean_q: 0.035719
  3920/100000: episode: 392, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000066, mae: 0.004171, mean_q: 0.035551
  3930/100000: episode: 393, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000059, mae: 0.003750, mean_q: 0.034912
  3940/100000: episode: 394, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000061, mae: 0.003223, mean_q: 0.033941
  3950/100000: episode: 395, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000433, mae: 0.006278, mean_q: 0.034027
  3960/100000: episode: 396, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000076, mae: 0.003328, mean_q: 0.034270
  3970/100000: episode: 397, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000256, mae: 0.004960, mean_q: 0.033829
  3980/100000: episode: 398, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000417, mae: 0.005493, mean_q: 0.034075
  3990/100000: episode: 399, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000192, mae: 0.002514, mean_q: 0.034201
Step 4000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.4000.hdf5
[Info] 1-TH LEVEL FOUND: 0.0344206802546978, Considering 100/100 traces
  4000/100000: episode: 400, duration: 0.781s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000025, mae: 0.002216, mean_q: 0.034613
[Info] 2-TH LEVEL FOUND: 0.03557372838258743, Considering 55/100 traces
  4008/100000: episode: 401, duration: 0.694s, episode steps: 8, steps per second: 12, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000025, mae: 0.002345, mean_q: 0.033979
  4017/100000: episode: 402, duration: 0.043s, episode steps: 9, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.667 [2.000, 10.000], loss: 0.000020, mae: 0.002749, mean_q: 0.033062
  4026/100000: episode: 403, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.778 [2.000, 10.000], loss: 0.000066, mae: 0.004182, mean_q: 0.032664
  4035/100000: episode: 404, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.722 [1.000, 10.000], loss: 0.000272, mae: 0.006189, mean_q: 0.032070
  4044/100000: episode: 405, duration: 0.043s, episode steps: 9, steps per second: 211, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.833 [2.000, 10.000], loss: 0.001642, mae: 0.005848, mean_q: 0.032773
  4053/100000: episode: 406, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.833 [1.000, 10.000], loss: 0.000097, mae: 0.004476, mean_q: 0.034985
  4062/100000: episode: 407, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.444 [1.000, 10.000], loss: 0.000458, mae: 0.007313, mean_q: 0.035596
  4071/100000: episode: 408, duration: 0.042s, episode steps: 9, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [1.000, 10.000], loss: 0.000067, mae: 0.003916, mean_q: 0.034902
  4080/100000: episode: 409, duration: 0.042s, episode steps: 9, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.444 [1.000, 10.000], loss: 0.000292, mae: 0.005090, mean_q: 0.033506
  4089/100000: episode: 410, duration: 0.042s, episode steps: 9, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.556 [2.000, 10.000], loss: 0.001648, mae: 0.005818, mean_q: 0.034308
  4098/100000: episode: 411, duration: 0.042s, episode steps: 9, steps per second: 217, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.278 [2.000, 10.000], loss: 0.000233, mae: 0.004459, mean_q: 0.035450
  4107/100000: episode: 412, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.389 [2.000, 10.000], loss: 0.000442, mae: 0.006453, mean_q: 0.035413
  4116/100000: episode: 413, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [2.000, 10.000], loss: 0.002055, mae: 0.009674, mean_q: 0.035522
  4125/100000: episode: 414, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.278 [2.000, 10.000], loss: 0.000272, mae: 0.005810, mean_q: 0.035577
  4134/100000: episode: 415, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.278 [1.000, 10.000], loss: 0.000031, mae: 0.002620, mean_q: 0.034260
  4143/100000: episode: 416, duration: 0.040s, episode steps: 9, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.222 [1.000, 10.000], loss: 0.000414, mae: 0.005235, mean_q: 0.033354
  4152/100000: episode: 417, duration: 0.042s, episode steps: 9, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.556 [1.000, 10.000], loss: 0.000233, mae: 0.004834, mean_q: 0.033439
  4161/100000: episode: 418, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.135, mean reward: 0.015 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [2.000, 10.000], loss: 0.000270, mae: 0.005286, mean_q: 0.033716
  4170/100000: episode: 419, duration: 0.042s, episode steps: 9, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.944 [2.000, 10.000], loss: 0.000039, mae: 0.003905, mean_q: 0.033112
  4179/100000: episode: 420, duration: 0.042s, episode steps: 9, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.278 [1.000, 10.000], loss: 0.000111, mae: 0.005585, mean_q: 0.032460
  4188/100000: episode: 421, duration: 0.041s, episode steps: 9, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.722 [1.000, 10.000], loss: 0.000259, mae: 0.005004, mean_q: 0.032660
  4197/100000: episode: 422, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.111 [2.000, 10.000], loss: 0.000033, mae: 0.003194, mean_q: 0.033027
  4206/100000: episode: 423, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.944 [1.000, 10.000], loss: 0.000281, mae: 0.005621, mean_q: 0.032459
  4215/100000: episode: 424, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.278 [1.000, 10.000], loss: 0.000068, mae: 0.003742, mean_q: 0.032764
  4224/100000: episode: 425, duration: 0.043s, episode steps: 9, steps per second: 212, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.611 [2.000, 10.000], loss: 0.000250, mae: 0.004883, mean_q: 0.032740
  4233/100000: episode: 426, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.444 [1.000, 10.000], loss: 0.000227, mae: 0.004233, mean_q: 0.032520
  4242/100000: episode: 427, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.556 [1.000, 10.000], loss: 0.000230, mae: 0.004341, mean_q: 0.032567
  4251/100000: episode: 428, duration: 0.041s, episode steps: 9, steps per second: 221, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [2.000, 10.000], loss: 0.000269, mae: 0.005411, mean_q: 0.032274
  4260/100000: episode: 429, duration: 0.041s, episode steps: 9, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [2.000, 10.000], loss: 0.000047, mae: 0.003493, mean_q: 0.031869
  4269/100000: episode: 430, duration: 0.041s, episode steps: 9, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000091, mae: 0.004990, mean_q: 0.031432
  4278/100000: episode: 431, duration: 0.041s, episode steps: 9, steps per second: 221, episode reward: 0.135, mean reward: 0.015 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [2.000, 10.000], loss: 0.000082, mae: 0.003971, mean_q: 0.031730
  4287/100000: episode: 432, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.368, mean reward: 0.041 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.889 [2.000, 10.000], loss: 0.000059, mae: 0.003920, mean_q: 0.032180
  4296/100000: episode: 433, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.135, mean reward: 0.015 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.611 [2.000, 10.000], loss: 0.000049, mae: 0.003696, mean_q: 0.031514
  4305/100000: episode: 434, duration: 0.042s, episode steps: 9, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.111 [2.000, 10.000], loss: 0.000023, mae: 0.003332, mean_q: 0.030717
  4314/100000: episode: 435, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.833 [2.000, 10.000], loss: 0.000242, mae: 0.004977, mean_q: 0.030522
  4323/100000: episode: 436, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.056 [1.000, 10.000], loss: 0.000452, mae: 0.005438, mean_q: 0.032278
  4332/100000: episode: 437, duration: 0.042s, episode steps: 9, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.611 [1.000, 10.000], loss: 0.000265, mae: 0.005975, mean_q: 0.033933
  4341/100000: episode: 438, duration: 0.042s, episode steps: 9, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.111 [1.000, 10.000], loss: 0.000051, mae: 0.003496, mean_q: 0.032701
  4350/100000: episode: 439, duration: 0.042s, episode steps: 9, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.111 [1.000, 10.000], loss: 0.000055, mae: 0.004047, mean_q: 0.031135
  4359/100000: episode: 440, duration: 0.042s, episode steps: 9, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.722 [2.000, 10.000], loss: 0.000686, mae: 0.008511, mean_q: 0.030647
  4368/100000: episode: 441, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [2.000, 10.000], loss: 0.001935, mae: 0.010993, mean_q: 0.034465
  4377/100000: episode: 442, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [1.000, 10.000], loss: 0.000277, mae: 0.007247, mean_q: 0.035390
  4386/100000: episode: 443, duration: 0.042s, episode steps: 9, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.667 [1.000, 10.000], loss: 0.000266, mae: 0.005789, mean_q: 0.033590
  4395/100000: episode: 444, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.889 [1.000, 10.000], loss: 0.000070, mae: 0.004165, mean_q: 0.032205
  4404/100000: episode: 445, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.278 [1.000, 10.000], loss: 0.000242, mae: 0.004774, mean_q: 0.031322
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.03557372838258743
2
  4413/100000: episode: 446, duration: 0.484s, episode steps: 9, steps per second: 19, episode reward: 0.050, mean reward: 0.006 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [1.000, 10.000], loss: 0.000032, mae: 0.003775, mean_q: 0.030845
  4423/100000: episode: 447, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000028, mae: 0.003908, mean_q: 0.030410
  4433/100000: episode: 448, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000224, mae: 0.004900, mean_q: 0.030569
  4443/100000: episode: 449, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000400, mae: 0.005117, mean_q: 0.031770
  4453/100000: episode: 450, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000556, mae: 0.006114, mean_q: 0.032795
  4463/100000: episode: 451, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000454, mae: 0.007216, mean_q: 0.033254
  4473/100000: episode: 452, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000206, mae: 0.003986, mean_q: 0.032870
  4483/100000: episode: 453, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000221, mae: 0.004121, mean_q: 0.031380
  4493/100000: episode: 454, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000433, mae: 0.005838, mean_q: 0.031645
  4503/100000: episode: 455, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.003319, mean_q: 0.032835
  4513/100000: episode: 456, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001666, mae: 0.006302, mean_q: 0.032145
  4523/100000: episode: 457, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000066, mae: 0.004120, mean_q: 0.033198
  4533/100000: episode: 458, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000268, mae: 0.004536, mean_q: 0.032445
  4543/100000: episode: 459, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000299, mae: 0.005699, mean_q: 0.033192
  4553/100000: episode: 460, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000059, mae: 0.004075, mean_q: 0.033121
  4563/100000: episode: 461, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000583, mae: 0.007292, mean_q: 0.030891
  4573/100000: episode: 462, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000041, mae: 0.002850, mean_q: 0.031930
  4583/100000: episode: 463, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000027, mae: 0.003300, mean_q: 0.030972
  4593/100000: episode: 464, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000217, mae: 0.004169, mean_q: 0.031185
  4603/100000: episode: 465, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000202, mae: 0.003296, mean_q: 0.031772
  4613/100000: episode: 466, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000048, mae: 0.003439, mean_q: 0.031247
  4623/100000: episode: 467, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.001501, mae: 0.005562, mean_q: 0.031064
  4633/100000: episode: 468, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000202, mae: 0.004214, mean_q: 0.033108
  4643/100000: episode: 469, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000045, mae: 0.003174, mean_q: 0.032290
  4653/100000: episode: 470, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001883, mae: 0.008707, mean_q: 0.031747
  4663/100000: episode: 471, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000031, mae: 0.003542, mean_q: 0.033139
  4673/100000: episode: 472, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000240, mae: 0.004705, mean_q: 0.031440
  4683/100000: episode: 473, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002933, mean_q: 0.031670
  4693/100000: episode: 474, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000221, mae: 0.003817, mean_q: 0.031664
  4703/100000: episode: 475, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000237, mae: 0.004102, mean_q: 0.032180
  4713/100000: episode: 476, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000076, mae: 0.003441, mean_q: 0.031723
  4723/100000: episode: 477, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000580, mae: 0.006370, mean_q: 0.031641
  4733/100000: episode: 478, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001683, mae: 0.008087, mean_q: 0.033661
  4743/100000: episode: 479, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000255, mae: 0.006348, mean_q: 0.034256
  4753/100000: episode: 480, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000395, mae: 0.004881, mean_q: 0.032341
  4763/100000: episode: 481, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000250, mae: 0.004946, mean_q: 0.032628
  4773/100000: episode: 482, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000203, mae: 0.003935, mean_q: 0.031552
  4783/100000: episode: 483, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.004048, mean_q: 0.030765
  4793/100000: episode: 484, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000025, mae: 0.003864, mean_q: 0.029924
  4803/100000: episode: 485, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000402, mae: 0.006174, mean_q: 0.030047
  4813/100000: episode: 486, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000398, mae: 0.005150, mean_q: 0.032272
  4823/100000: episode: 487, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000223, mae: 0.005087, mean_q: 0.033232
  4833/100000: episode: 488, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000029, mae: 0.003231, mean_q: 0.031042
  4843/100000: episode: 489, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000046, mae: 0.004740, mean_q: 0.029399
  4853/100000: episode: 490, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000047, mae: 0.004834, mean_q: 0.029006
  4863/100000: episode: 491, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000043, mae: 0.003304, mean_q: 0.030136
  4873/100000: episode: 492, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000083, mae: 0.003988, mean_q: 0.030549
  4883/100000: episode: 493, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000210, mae: 0.004508, mean_q: 0.030137
  4893/100000: episode: 494, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000648, mae: 0.007563, mean_q: 0.030323
  4903/100000: episode: 495, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000070, mae: 0.004302, mean_q: 0.032948
  4913/100000: episode: 496, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000242, mae: 0.005143, mean_q: 0.031916
  4923/100000: episode: 497, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000414, mae: 0.005049, mean_q: 0.031215
  4933/100000: episode: 498, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000095, mae: 0.003692, mean_q: 0.031030
  4943/100000: episode: 499, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001671, mae: 0.006486, mean_q: 0.030840
  4953/100000: episode: 500, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000210, mae: 0.004826, mean_q: 0.032583
  4963/100000: episode: 501, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000223, mae: 0.004094, mean_q: 0.031020
  4973/100000: episode: 502, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000058, mae: 0.003237, mean_q: 0.030638
  4983/100000: episode: 503, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.001676, mae: 0.007043, mean_q: 0.031051
  4993/100000: episode: 504, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000232, mae: 0.005485, mean_q: 0.032682
Step 5000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.5000.hdf5
  5003/100000: episode: 505, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000205, mae: 0.003782, mean_q: 0.031013
  5013/100000: episode: 506, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001708, mae: 0.007610, mean_q: 0.031248
  5023/100000: episode: 507, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001516, mae: 0.008381, mean_q: 0.034373
  5033/100000: episode: 508, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000048, mae: 0.003990, mean_q: 0.032452
  5043/100000: episode: 509, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000400, mae: 0.006136, mean_q: 0.029881
  5053/100000: episode: 510, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000423, mae: 0.006181, mean_q: 0.031906
  5063/100000: episode: 511, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000046, mae: 0.003342, mean_q: 0.032144
  5073/100000: episode: 512, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000047, mae: 0.003791, mean_q: 0.030472
  5083/100000: episode: 513, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000294, mae: 0.006162, mean_q: 0.030248
  5093/100000: episode: 514, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000398, mae: 0.004636, mean_q: 0.031250
  5103/100000: episode: 515, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000063, mae: 0.003497, mean_q: 0.032192
  5113/100000: episode: 516, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000098, mae: 0.004127, mean_q: 0.030866
  5123/100000: episode: 517, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000022, mae: 0.003135, mean_q: 0.029790
  5133/100000: episode: 518, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000380, mae: 0.005215, mean_q: 0.029764
  5143/100000: episode: 519, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000410, mae: 0.006196, mean_q: 0.032102
  5153/100000: episode: 520, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000416, mae: 0.006000, mean_q: 0.032125
  5163/100000: episode: 521, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000099, mae: 0.004186, mean_q: 0.031694
  5173/100000: episode: 522, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000030, mae: 0.003035, mean_q: 0.030705
  5183/100000: episode: 523, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000076, mae: 0.003918, mean_q: 0.030095
  5193/100000: episode: 524, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000049, mae: 0.004034, mean_q: 0.029691
  5203/100000: episode: 525, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000450, mae: 0.007519, mean_q: 0.029105
  5213/100000: episode: 526, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000071, mae: 0.003583, mean_q: 0.031847
  5223/100000: episode: 527, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000029, mae: 0.003389, mean_q: 0.031884
  5233/100000: episode: 528, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.001515, mae: 0.005978, mean_q: 0.031065
  5243/100000: episode: 529, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000406, mae: 0.004505, mean_q: 0.031223
  5253/100000: episode: 530, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.001529, mae: 0.006159, mean_q: 0.031048
  5263/100000: episode: 531, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000071, mae: 0.004906, mean_q: 0.032718
  5273/100000: episode: 532, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000103, mae: 0.004757, mean_q: 0.030795
  5283/100000: episode: 533, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000239, mae: 0.004540, mean_q: 0.030392
  5293/100000: episode: 534, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000040, mae: 0.002947, mean_q: 0.030274
  5303/100000: episode: 535, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000068, mae: 0.004326, mean_q: 0.029748
  5313/100000: episode: 536, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000289, mae: 0.005552, mean_q: 0.029772
  5323/100000: episode: 537, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000024, mae: 0.002345, mean_q: 0.031121
  5333/100000: episode: 538, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000227, mae: 0.004725, mean_q: 0.029835
  5343/100000: episode: 539, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000078, mae: 0.004091, mean_q: 0.029661
  5353/100000: episode: 540, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000042, mae: 0.003314, mean_q: 0.029438
  5363/100000: episode: 541, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000221, mae: 0.004337, mean_q: 0.029282
  5373/100000: episode: 542, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.002674, mean_q: 0.029859
  5383/100000: episode: 543, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000377, mae: 0.004039, mean_q: 0.029834
  5393/100000: episode: 544, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.001692, mae: 0.006792, mean_q: 0.030280
  5403/100000: episode: 545, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001529, mae: 0.008073, mean_q: 0.032472
[Info] 1-TH LEVEL FOUND: 0.031575385481119156, Considering 100/100 traces
  5413/100000: episode: 546, duration: 0.664s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000247, mae: 0.006408, mean_q: 0.032867
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.031575385481119156
1
  5422/100000: episode: 547, duration: 0.476s, episode steps: 9, steps per second: 19, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [2.000, 10.000], loss: 0.000099, mae: 0.004033, mean_q: 0.030192
  5432/100000: episode: 548, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000275, mae: 0.005724, mean_q: 0.029435
  5442/100000: episode: 549, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000057, mae: 0.002827, mean_q: 0.030528
  5452/100000: episode: 550, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000237, mae: 0.003787, mean_q: 0.030630
  5462/100000: episode: 551, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000202, mae: 0.003201, mean_q: 0.030279
  5472/100000: episode: 552, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000036, mae: 0.003611, mean_q: 0.029690
  5482/100000: episode: 553, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000388, mae: 0.005706, mean_q: 0.028760
  5492/100000: episode: 554, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000083, mae: 0.004008, mean_q: 0.029670
  5502/100000: episode: 555, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000057, mae: 0.002896, mean_q: 0.029611
  5512/100000: episode: 556, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000027, mae: 0.003197, mean_q: 0.029010
  5522/100000: episode: 557, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.003003, mae: 0.009787, mean_q: 0.029576
  5532/100000: episode: 558, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000047, mae: 0.005489, mean_q: 0.033824
  5542/100000: episode: 559, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000071, mae: 0.004490, mean_q: 0.030417
  5552/100000: episode: 560, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000240, mae: 0.005536, mean_q: 0.028486
  5562/100000: episode: 561, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000241, mae: 0.004689, mean_q: 0.029483
  5572/100000: episode: 562, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000250, mae: 0.004913, mean_q: 0.029730
  5582/100000: episode: 563, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.002453, mean_q: 0.029422
  5592/100000: episode: 564, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000238, mae: 0.004453, mean_q: 0.029094
  5602/100000: episode: 565, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000041, mae: 0.002624, mean_q: 0.029462
  5612/100000: episode: 566, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000060, mae: 0.003564, mean_q: 0.028830
  5622/100000: episode: 567, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000242, mae: 0.004507, mean_q: 0.030025
  5632/100000: episode: 568, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000400, mae: 0.004918, mean_q: 0.030070
  5642/100000: episode: 569, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000618, mae: 0.007476, mean_q: 0.030692
  5652/100000: episode: 570, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000066, mae: 0.004082, mean_q: 0.031089
  5662/100000: episode: 571, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000077, mae: 0.003419, mean_q: 0.029609
  5672/100000: episode: 572, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.001523, mae: 0.006473, mean_q: 0.030013
  5682/100000: episode: 573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000020, mae: 0.002641, mean_q: 0.030906
  5692/100000: episode: 574, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000037, mae: 0.002849, mean_q: 0.029122
  5702/100000: episode: 575, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.003642, mean_q: 0.028711
  5712/100000: episode: 576, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000401, mae: 0.005107, mean_q: 0.029937
  5722/100000: episode: 577, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000267, mae: 0.004809, mean_q: 0.030571
  5732/100000: episode: 578, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000062, mae: 0.003810, mean_q: 0.030826
  5742/100000: episode: 579, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000220, mae: 0.003352, mean_q: 0.030004
  5752/100000: episode: 580, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000039, mae: 0.003313, mean_q: 0.028741
  5762/100000: episode: 581, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000023, mae: 0.003349, mean_q: 0.028199
  5772/100000: episode: 582, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000248, mae: 0.005595, mean_q: 0.028220
  5782/100000: episode: 583, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000200, mae: 0.003114, mean_q: 0.029485
  5792/100000: episode: 584, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000244, mae: 0.004364, mean_q: 0.029326
  5802/100000: episode: 585, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000265, mae: 0.005215, mean_q: 0.030123
  5812/100000: episode: 586, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000242, mae: 0.004664, mean_q: 0.030106
  5822/100000: episode: 587, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000234, mae: 0.003602, mean_q: 0.029668
  5832/100000: episode: 588, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000222, mae: 0.004059, mean_q: 0.030215
  5842/100000: episode: 589, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000222, mae: 0.003628, mean_q: 0.029575
  5852/100000: episode: 590, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001522, mae: 0.006475, mean_q: 0.028973
  5862/100000: episode: 591, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000433, mae: 0.006301, mean_q: 0.030849
  5872/100000: episode: 592, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000273, mae: 0.006264, mean_q: 0.031607
  5882/100000: episode: 593, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000043, mae: 0.003050, mean_q: 0.030237
  5892/100000: episode: 594, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000388, mae: 0.005523, mean_q: 0.028578
  5902/100000: episode: 595, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000092, mae: 0.003466, mean_q: 0.029875
  5912/100000: episode: 596, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000044, mae: 0.002919, mean_q: 0.029512
  5922/100000: episode: 597, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000041, mae: 0.003435, mean_q: 0.028328
  5932/100000: episode: 598, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000381, mae: 0.004696, mean_q: 0.028753
  5942/100000: episode: 599, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000060, mae: 0.003761, mean_q: 0.030513
  5952/100000: episode: 600, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000068, mae: 0.002847, mean_q: 0.030087
  5962/100000: episode: 601, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002402, mean_q: 0.029724
  5972/100000: episode: 602, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000416, mae: 0.004802, mean_q: 0.029469
  5982/100000: episode: 603, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001553, mae: 0.007467, mean_q: 0.030736
  5992/100000: episode: 604, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000048, mae: 0.003870, mean_q: 0.031070
Step 6000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.6000.hdf5
  6002/100000: episode: 605, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000068, mae: 0.004334, mean_q: 0.028736
  6012/100000: episode: 606, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000220, mae: 0.004696, mean_q: 0.028157
  6022/100000: episode: 607, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000028, mae: 0.003341, mean_q: 0.028472
  6032/100000: episode: 608, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000075, mae: 0.004087, mean_q: 0.027827
  6042/100000: episode: 609, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000047, mae: 0.003924, mean_q: 0.028096
  6052/100000: episode: 610, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000094, mae: 0.004342, mean_q: 0.027681
  6062/100000: episode: 611, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000240, mae: 0.004273, mean_q: 0.029294
  6072/100000: episode: 612, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000254, mae: 0.004369, mean_q: 0.029413
  6082/100000: episode: 613, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000400, mae: 0.005782, mean_q: 0.030486
  6092/100000: episode: 614, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.002881, mean_q: 0.029400
  6102/100000: episode: 615, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000259, mae: 0.005382, mean_q: 0.027862
  6112/100000: episode: 616, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000239, mae: 0.004429, mean_q: 0.029398
  6122/100000: episode: 617, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000102, mae: 0.004542, mean_q: 0.029633
  6132/100000: episode: 618, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000042, mae: 0.003590, mean_q: 0.027902
  6142/100000: episode: 619, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000040, mae: 0.003796, mean_q: 0.027418
  6152/100000: episode: 620, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000073, mae: 0.003508, mean_q: 0.027901
  6162/100000: episode: 621, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.001908, mean_q: 0.028266
  6172/100000: episode: 622, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000209, mae: 0.004567, mean_q: 0.027236
  6182/100000: episode: 623, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000060, mae: 0.003649, mean_q: 0.027571
  6192/100000: episode: 624, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000020, mae: 0.002636, mean_q: 0.027437
  6202/100000: episode: 625, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.001681, mae: 0.006750, mean_q: 0.027840
  6212/100000: episode: 626, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000032, mae: 0.003859, mean_q: 0.029731
  6222/100000: episode: 627, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000421, mae: 0.005357, mean_q: 0.028509
  6232/100000: episode: 628, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000049, mae: 0.003432, mean_q: 0.028793
  6242/100000: episode: 629, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000243, mae: 0.005163, mean_q: 0.026819
  6252/100000: episode: 630, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001706, mae: 0.008460, mean_q: 0.030013
  6262/100000: episode: 631, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000060, mae: 0.004864, mean_q: 0.030522
  6272/100000: episode: 632, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000074, mae: 0.003556, mean_q: 0.027767
  6282/100000: episode: 633, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000217, mae: 0.004100, mean_q: 0.027740
  6292/100000: episode: 634, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000022, mae: 0.002488, mean_q: 0.027835
  6302/100000: episode: 635, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.002797, mean_q: 0.027741
  6312/100000: episode: 636, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000060, mae: 0.003547, mean_q: 0.027432
  6322/100000: episode: 637, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000240, mae: 0.004095, mean_q: 0.028100
  6332/100000: episode: 638, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000082, mae: 0.003834, mean_q: 0.028633
  6342/100000: episode: 639, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000222, mae: 0.003826, mean_q: 0.027706
  6352/100000: episode: 640, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000025, mae: 0.003123, mean_q: 0.027059
  6362/100000: episode: 641, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000240, mae: 0.004461, mean_q: 0.027030
  6372/100000: episode: 642, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.001702, mae: 0.007382, mean_q: 0.028575
  6382/100000: episode: 643, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000237, mae: 0.006313, mean_q: 0.031167
  6392/100000: episode: 644, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000095, mae: 0.004251, mean_q: 0.028916
  6402/100000: episode: 645, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000257, mae: 0.004780, mean_q: 0.027781
  6412/100000: episode: 646, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000075, mae: 0.003174, mean_q: 0.027894
[Info] 1-TH LEVEL FOUND: 0.030186234042048454, Considering 100/100 traces
  6422/100000: episode: 647, duration: 0.719s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000048, mae: 0.004154, mean_q: 0.029915
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.030186234042048454
1
  6431/100000: episode: 648, duration: 0.490s, episode steps: 9, steps per second: 18, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.389 [0.000, 10.000], loss: 0.000218, mae: 0.003827, mean_q: 0.029500
  6441/100000: episode: 649, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000042, mae: 0.002729, mean_q: 0.028272
  6451/100000: episode: 650, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000261, mae: 0.005200, mean_q: 0.027482
  6461/100000: episode: 651, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000020, mae: 0.002043, mean_q: 0.028245
  6471/100000: episode: 652, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000617, mae: 0.006511, mean_q: 0.028203
  6481/100000: episode: 653, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001725, mae: 0.010417, mean_q: 0.032380
  6491/100000: episode: 654, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000046, mae: 0.004609, mean_q: 0.030792
  6501/100000: episode: 655, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000062, mae: 0.004202, mean_q: 0.027577
  6511/100000: episode: 656, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000277, mae: 0.005610, mean_q: 0.027775
  6521/100000: episode: 657, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000021, mae: 0.002053, mean_q: 0.028572
  6531/100000: episode: 658, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000270, mae: 0.004328, mean_q: 0.028197
  6541/100000: episode: 659, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000166, mae: 0.005557, mean_q: 0.029550
  6551/100000: episode: 660, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000276, mae: 0.006424, mean_q: 0.030690
  6561/100000: episode: 661, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000259, mae: 0.005866, mean_q: 0.030640
  6571/100000: episode: 662, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.002520, mean_q: 0.028983
  6581/100000: episode: 663, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000223, mae: 0.004524, mean_q: 0.027898
  6591/100000: episode: 664, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000204, mae: 0.003261, mean_q: 0.029039
  6601/100000: episode: 665, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000226, mae: 0.004310, mean_q: 0.028381
  6611/100000: episode: 666, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000241, mae: 0.004087, mean_q: 0.028808
  6621/100000: episode: 667, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000044, mae: 0.002770, mean_q: 0.029110
  6631/100000: episode: 668, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000043, mae: 0.003109, mean_q: 0.028221
  6641/100000: episode: 669, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000038, mae: 0.002776, mean_q: 0.028163
  6651/100000: episode: 670, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000077, mae: 0.004075, mean_q: 0.027366
  6661/100000: episode: 671, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000044, mae: 0.003822, mean_q: 0.030105
  6671/100000: episode: 672, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.001518, mae: 0.007108, mean_q: 0.030133
  6681/100000: episode: 673, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000241, mae: 0.005660, mean_q: 0.030794
  6691/100000: episode: 674, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000034, mae: 0.002254, mean_q: 0.028968
  6701/100000: episode: 675, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000081, mae: 0.004653, mean_q: 0.027530
  6711/100000: episode: 676, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000201, mae: 0.003814, mean_q: 0.027806
  6721/100000: episode: 677, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000055, mae: 0.002852, mean_q: 0.029127
  6731/100000: episode: 678, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000096, mae: 0.004544, mean_q: 0.030100
  6741/100000: episode: 679, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000021, mae: 0.002504, mean_q: 0.028391
  6751/100000: episode: 680, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.001568, mae: 0.007003, mean_q: 0.028193
  6761/100000: episode: 681, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000220, mae: 0.005552, mean_q: 0.031390
  6771/100000: episode: 682, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000221, mae: 0.004831, mean_q: 0.030346
  6781/100000: episode: 683, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000093, mae: 0.004386, mean_q: 0.027796
  6791/100000: episode: 684, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.002539, mean_q: 0.028207
  6801/100000: episode: 685, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000249, mae: 0.003769, mean_q: 0.028802
  6811/100000: episode: 686, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000123, mae: 0.005167, mean_q: 0.029820
  6821/100000: episode: 687, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000041, mae: 0.002689, mean_q: 0.028770
  6831/100000: episode: 688, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001571, mae: 0.007310, mean_q: 0.028040
  6841/100000: episode: 689, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000116, mae: 0.006241, mean_q: 0.031925
  6851/100000: episode: 690, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000027, mae: 0.003471, mean_q: 0.029735
  6861/100000: episode: 691, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000061, mae: 0.004747, mean_q: 0.027093
  6871/100000: episode: 692, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000100, mae: 0.005281, mean_q: 0.027298
  6881/100000: episode: 693, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001927, mae: 0.009316, mean_q: 0.028539
  6891/100000: episode: 694, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001522, mae: 0.010163, mean_q: 0.033807
  6901/100000: episode: 695, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000045, mae: 0.004975, mean_q: 0.031717
  6911/100000: episode: 696, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000468, mae: 0.006452, mean_q: 0.028454
  6921/100000: episode: 697, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000045, mae: 0.003395, mean_q: 0.030562
  6931/100000: episode: 698, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000074, mae: 0.003627, mean_q: 0.028800
  6941/100000: episode: 699, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000020, mae: 0.002722, mean_q: 0.028706
  6951/100000: episode: 700, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000222, mae: 0.004272, mean_q: 0.028480
  6961/100000: episode: 701, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000252, mae: 0.003988, mean_q: 0.029002
  6971/100000: episode: 702, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000077, mae: 0.003274, mean_q: 0.029570
  6981/100000: episode: 703, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000060, mae: 0.002794, mean_q: 0.029268
  6991/100000: episode: 704, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000218, mae: 0.003508, mean_q: 0.028811
Step 7000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.7000.hdf5
  7001/100000: episode: 705, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000037, mae: 0.002385, mean_q: 0.028752
  7011/100000: episode: 706, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000280, mae: 0.005322, mean_q: 0.028527
  7021/100000: episode: 707, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000076, mae: 0.003431, mean_q: 0.029523
  7031/100000: episode: 708, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000029, mae: 0.002957, mean_q: 0.028739
  7041/100000: episode: 709, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000023, mae: 0.003675, mean_q: 0.027129
  7051/100000: episode: 710, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000056, mae: 0.004308, mean_q: 0.026651
  7061/100000: episode: 711, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000034, mae: 0.002439, mean_q: 0.027843
  7071/100000: episode: 712, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000015, mae: 0.001672, mean_q: 0.028305
  7081/100000: episode: 713, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000014, mae: 0.002163, mean_q: 0.027565
  7091/100000: episode: 714, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000209, mae: 0.004620, mean_q: 0.027093
  7101/100000: episode: 715, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.003888, mean_q: 0.027617
  7111/100000: episode: 716, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000094, mae: 0.003874, mean_q: 0.027503
  7121/100000: episode: 717, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000077, mae: 0.003375, mean_q: 0.027637
  7131/100000: episode: 718, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000019, mae: 0.001957, mean_q: 0.027798
  7141/100000: episode: 719, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000237, mae: 0.003976, mean_q: 0.027318
  7151/100000: episode: 720, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000240, mae: 0.003993, mean_q: 0.027593
  7161/100000: episode: 721, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000056, mae: 0.002874, mean_q: 0.028426
  7171/100000: episode: 722, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000057, mae: 0.002777, mean_q: 0.027929
  7181/100000: episode: 723, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000222, mae: 0.004107, mean_q: 0.027251
  7191/100000: episode: 724, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000050, mae: 0.003315, mean_q: 0.027643
  7201/100000: episode: 725, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000098, mae: 0.004462, mean_q: 0.026745
  7211/100000: episode: 726, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000226, mae: 0.004495, mean_q: 0.027041
  7221/100000: episode: 727, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000033, mae: 0.001832, mean_q: 0.027695
  7231/100000: episode: 728, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000113, mae: 0.004024, mean_q: 0.027106
  7241/100000: episode: 729, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000039, mae: 0.002456, mean_q: 0.027378
  7251/100000: episode: 730, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.003539, mean_q: 0.026444
  7261/100000: episode: 731, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000038, mae: 0.002913, mean_q: 0.026617
  7271/100000: episode: 732, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000225, mae: 0.004251, mean_q: 0.026536
  7281/100000: episode: 733, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000082, mae: 0.003814, mean_q: 0.026732
  7291/100000: episode: 734, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000047, mae: 0.003408, mean_q: 0.026627
  7301/100000: episode: 735, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000019, mae: 0.002784, mean_q: 0.025919
  7311/100000: episode: 736, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.003484, mean_q: 0.025537
  7321/100000: episode: 737, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000216, mae: 0.003447, mean_q: 0.025986
  7331/100000: episode: 738, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000071, mae: 0.002822, mean_q: 0.027101
  7341/100000: episode: 739, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.002887, mean_q: 0.026334
  7351/100000: episode: 740, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000222, mae: 0.003602, mean_q: 0.026835
  7361/100000: episode: 741, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000218, mae: 0.003580, mean_q: 0.027071
  7371/100000: episode: 742, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000224, mae: 0.003884, mean_q: 0.027011
  7381/100000: episode: 743, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000060, mae: 0.002919, mean_q: 0.026849
  7391/100000: episode: 744, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000025, mae: 0.003216, mean_q: 0.025747
  7401/100000: episode: 745, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000206, mae: 0.003930, mean_q: 0.025824
  7411/100000: episode: 746, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000257, mae: 0.004301, mean_q: 0.026961
  7421/100000: episode: 747, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000015, mae: 0.001790, mean_q: 0.026996
[Info] 1-TH LEVEL FOUND: 0.027097653597593307, Considering 100/100 traces
  7431/100000: episode: 748, duration: 0.714s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000030, mae: 0.002297, mean_q: 0.025674
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.027097653597593307
1
  7441/100000: episode: 749, duration: 0.487s, episode steps: 10, steps per second: 21, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000062, mae: 0.003920, mean_q: 0.025369
  7451/100000: episode: 750, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000096, mae: 0.003943, mean_q: 0.025610
  7461/100000: episode: 751, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000076, mae: 0.003162, mean_q: 0.026639
  7471/100000: episode: 752, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000205, mae: 0.003319, mean_q: 0.026212
  7481/100000: episode: 753, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002451, mean_q: 0.026536
  7491/100000: episode: 754, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000019, mae: 0.002558, mean_q: 0.025453
  7501/100000: episode: 755, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000042, mae: 0.003752, mean_q: 0.024739
  7511/100000: episode: 756, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000080, mae: 0.004128, mean_q: 0.025029
  7521/100000: episode: 757, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000037, mae: 0.002503, mean_q: 0.025478
  7531/100000: episode: 758, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000243, mae: 0.004295, mean_q: 0.025163
  7541/100000: episode: 759, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000054, mae: 0.002661, mean_q: 0.026278
  7551/100000: episode: 760, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000058, mae: 0.002870, mean_q: 0.025651
  7561/100000: episode: 761, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000059, mae: 0.003272, mean_q: 0.025273
  7571/100000: episode: 762, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000096, mae: 0.003822, mean_q: 0.025055
  7581/100000: episode: 763, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000063, mae: 0.003408, mean_q: 0.025403
  7591/100000: episode: 764, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000040, mae: 0.003006, mean_q: 0.024807
  7601/100000: episode: 765, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000056, mae: 0.003549, mean_q: 0.024043
  7611/100000: episode: 766, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000236, mae: 0.004395, mean_q: 0.026636
  7621/100000: episode: 767, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.003652, mean_q: 0.026781
  7631/100000: episode: 768, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000201, mae: 0.003015, mean_q: 0.025398
  7641/100000: episode: 769, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000037, mae: 0.002263, mean_q: 0.025740
  7651/100000: episode: 770, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000052, mae: 0.002383, mean_q: 0.025306
  7661/100000: episode: 771, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000050, mae: 0.002301, mean_q: 0.025219
  7671/100000: episode: 772, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000258, mae: 0.004252, mean_q: 0.025988
  7681/100000: episode: 773, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000020, mae: 0.002367, mean_q: 0.025400
  7691/100000: episode: 774, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000280, mae: 0.005382, mean_q: 0.024194
  7701/100000: episode: 775, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000039, mae: 0.002689, mean_q: 0.025984
  7711/100000: episode: 776, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.002907, mean_q: 0.025102
  7721/100000: episode: 777, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000040, mae: 0.003382, mean_q: 0.024269
  7731/100000: episode: 778, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000050, mae: 0.002414, mean_q: 0.024729
  7741/100000: episode: 779, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000218, mae: 0.003026, mean_q: 0.025183
  7751/100000: episode: 780, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000204, mae: 0.004238, mean_q: 0.026765
  7761/100000: episode: 781, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000016, mae: 0.002170, mean_q: 0.025153
  7771/100000: episode: 782, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000223, mae: 0.003993, mean_q: 0.024568
  7781/100000: episode: 783, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000069, mae: 0.002819, mean_q: 0.025867
  7791/100000: episode: 784, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000042, mae: 0.002868, mean_q: 0.025392
  7801/100000: episode: 785, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000071, mae: 0.002692, mean_q: 0.025130
  7811/100000: episode: 786, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000278, mae: 0.004332, mean_q: 0.025201
  7821/100000: episode: 787, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000081, mae: 0.003922, mean_q: 0.026039
  7831/100000: episode: 788, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000038, mae: 0.002411, mean_q: 0.025755
  7841/100000: episode: 789, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000014, mae: 0.001975, mean_q: 0.024735
  7851/100000: episode: 790, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.003283, mean_q: 0.024415
  7861/100000: episode: 791, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000096, mae: 0.003577, mean_q: 0.024836
  7871/100000: episode: 792, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000057, mae: 0.002658, mean_q: 0.025064
  7881/100000: episode: 793, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000091, mae: 0.003030, mean_q: 0.025048
  7891/100000: episode: 794, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.002447, mean_q: 0.025239
  7901/100000: episode: 795, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000019, mae: 0.003164, mean_q: 0.023589
  7911/100000: episode: 796, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000063, mae: 0.004573, mean_q: 0.023297
  7921/100000: episode: 797, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000292, mae: 0.004955, mean_q: 0.025358
  7931/100000: episode: 798, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000034, mae: 0.003391, mean_q: 0.026620
  7941/100000: episode: 799, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000031, mae: 0.002115, mean_q: 0.024575
  7951/100000: episode: 800, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.003341, mean_q: 0.023810
  7961/100000: episode: 801, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000077, mae: 0.003493, mean_q: 0.024130
  7971/100000: episode: 802, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000225, mae: 0.003803, mean_q: 0.025193
  7981/100000: episode: 803, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000093, mae: 0.003225, mean_q: 0.024532
  7991/100000: episode: 804, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000023, mae: 0.002762, mean_q: 0.024229
Step 8000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.8000.hdf5
  8001/100000: episode: 805, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000071, mae: 0.002783, mean_q: 0.024246
  8011/100000: episode: 806, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000019, mae: 0.002209, mean_q: 0.025010
  8021/100000: episode: 807, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000039, mae: 0.003098, mean_q: 0.023761
  8031/100000: episode: 808, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000206, mae: 0.003731, mean_q: 0.024002
  8041/100000: episode: 809, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000094, mae: 0.003363, mean_q: 0.023988
  8051/100000: episode: 810, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000216, mae: 0.003496, mean_q: 0.025357
  8061/100000: episode: 811, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000037, mae: 0.002432, mean_q: 0.024987
  8071/100000: episode: 812, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000030, mae: 0.002201, mean_q: 0.023832
  8081/100000: episode: 813, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000096, mae: 0.003618, mean_q: 0.024180
  8091/100000: episode: 814, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000018, mae: 0.002738, mean_q: 0.025679
  8101/100000: episode: 815, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000015, mae: 0.002329, mean_q: 0.023775
  8111/100000: episode: 816, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000099, mae: 0.004546, mean_q: 0.023315
  8121/100000: episode: 817, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000092, mae: 0.003308, mean_q: 0.023883
  8131/100000: episode: 818, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000059, mae: 0.002877, mean_q: 0.024612
  8141/100000: episode: 819, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000038, mae: 0.002852, mean_q: 0.023531
  8151/100000: episode: 820, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000038, mae: 0.003230, mean_q: 0.023105
  8161/100000: episode: 821, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000092, mae: 0.003393, mean_q: 0.023384
  8171/100000: episode: 822, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000013, mae: 0.001968, mean_q: 0.024876
  8181/100000: episode: 823, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000221, mae: 0.003506, mean_q: 0.023510
  8191/100000: episode: 824, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000016, mae: 0.001708, mean_q: 0.024284
  8201/100000: episode: 825, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000207, mae: 0.003755, mean_q: 0.023311
  8211/100000: episode: 826, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000226, mae: 0.003831, mean_q: 0.023775
  8221/100000: episode: 827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000011, mae: 0.001424, mean_q: 0.023564
  8231/100000: episode: 828, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000207, mae: 0.003810, mean_q: 0.023124
  8241/100000: episode: 829, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001482, mean_q: 0.024007
  8251/100000: episode: 830, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002401, mean_q: 0.023675
  8261/100000: episode: 831, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000014, mae: 0.002282, mean_q: 0.022837
  8271/100000: episode: 832, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000073, mae: 0.003116, mean_q: 0.023043
  8281/100000: episode: 833, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000034, mae: 0.002099, mean_q: 0.023797
  8291/100000: episode: 834, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000016, mae: 0.002545, mean_q: 0.022527
  8301/100000: episode: 835, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000204, mae: 0.003651, mean_q: 0.022560
  8311/100000: episode: 836, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.002372, mean_q: 0.023556
  8321/100000: episode: 837, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000035, mae: 0.002558, mean_q: 0.022651
  8331/100000: episode: 838, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000207, mae: 0.003529, mean_q: 0.023035
  8341/100000: episode: 839, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000057, mae: 0.002760, mean_q: 0.022892
  8351/100000: episode: 840, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000222, mae: 0.003239, mean_q: 0.023344
  8361/100000: episode: 841, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000053, mae: 0.003010, mean_q: 0.024276
  8371/100000: episode: 842, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000040, mae: 0.002787, mean_q: 0.023021
  8381/100000: episode: 843, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000220, mae: 0.003561, mean_q: 0.022554
  8391/100000: episode: 844, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000057, mae: 0.002923, mean_q: 0.023779
  8401/100000: episode: 845, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000037, mae: 0.002559, mean_q: 0.022713
  8411/100000: episode: 846, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000053, mae: 0.002459, mean_q: 0.023052
  8421/100000: episode: 847, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000091, mae: 0.002807, mean_q: 0.023034
  8431/100000: episode: 848, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000259, mae: 0.003783, mean_q: 0.023389
[Info] 1-TH LEVEL FOUND: 0.02283952385187149, Considering 100/100 traces
  8441/100000: episode: 849, duration: 0.718s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000037, mae: 0.002630, mean_q: 0.023717
[Info] 2-TH LEVEL FOUND: 0.02377193607389927, Considering 100/100 traces
  8450/100000: episode: 850, duration: 0.692s, episode steps: 9, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.444 [0.000, 10.000], loss: 0.000016, mae: 0.002281, mean_q: 0.022409
[Info] 3-TH LEVEL FOUND: 0.023830221965909004, Considering 100/100 traces
  8460/100000: episode: 851, duration: 0.677s, episode steps: 10, steps per second: 15, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000207, mae: 0.003868, mean_q: 0.022149
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023830221965909004
3
  8470/100000: episode: 852, duration: 0.567s, episode steps: 10, steps per second: 18, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000202, mae: 0.003059, mean_q: 0.023479
  8480/100000: episode: 853, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000043, mae: 0.003010, mean_q: 0.022748
  8490/100000: episode: 854, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000211, mae: 0.003822, mean_q: 0.022404
  8500/100000: episode: 855, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000056, mae: 0.002614, mean_q: 0.023237
  8510/100000: episode: 856, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000410, mae: 0.005222, mean_q: 0.024083
  8520/100000: episode: 857, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000024, mae: 0.003444, mean_q: 0.023751
  8530/100000: episode: 858, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.003428, mean_q: 0.021483
  8540/100000: episode: 859, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000015, mae: 0.001899, mean_q: 0.022786
  8550/100000: episode: 860, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000037, mae: 0.002540, mean_q: 0.022436
  8560/100000: episode: 861, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000060, mae: 0.002972, mean_q: 0.023218
  8570/100000: episode: 862, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002182, mean_q: 0.022693
  8580/100000: episode: 863, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000066, mae: 0.004134, mean_q: 0.021964
  8590/100000: episode: 864, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000222, mae: 0.003610, mean_q: 0.022041
  8600/100000: episode: 865, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000059, mae: 0.003384, mean_q: 0.023690
  8610/100000: episode: 866, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000240, mae: 0.004100, mean_q: 0.023697
  8620/100000: episode: 867, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000013, mae: 0.001642, mean_q: 0.022484
  8630/100000: episode: 868, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000253, mae: 0.004509, mean_q: 0.022738
  8640/100000: episode: 869, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000238, mae: 0.003523, mean_q: 0.023261
  8650/100000: episode: 870, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000057, mae: 0.003211, mean_q: 0.023836
  8660/100000: episode: 871, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000043, mae: 0.003226, mean_q: 0.022213
  8670/100000: episode: 872, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000074, mae: 0.003010, mean_q: 0.022550
  8680/100000: episode: 873, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000056, mae: 0.002662, mean_q: 0.022935
  8690/100000: episode: 874, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000202, mae: 0.002849, mean_q: 0.022961
  8700/100000: episode: 875, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000038, mae: 0.002503, mean_q: 0.022812
  8710/100000: episode: 876, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000036, mae: 0.003001, mean_q: 0.021709
  8720/100000: episode: 877, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000024, mae: 0.003024, mean_q: 0.021924
  8730/100000: episode: 878, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000244, mae: 0.003615, mean_q: 0.022489
  8740/100000: episode: 879, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000058, mae: 0.003589, mean_q: 0.024095
  8750/100000: episode: 880, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002947, mean_q: 0.023406
  8760/100000: episode: 881, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000221, mae: 0.003656, mean_q: 0.021825
  8770/100000: episode: 882, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000035, mae: 0.002503, mean_q: 0.023350
  8780/100000: episode: 883, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002317, mean_q: 0.022317
  8790/100000: episode: 884, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000223, mae: 0.003526, mean_q: 0.022811
  8800/100000: episode: 885, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000227, mae: 0.003878, mean_q: 0.023062
  8810/100000: episode: 886, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000016, mae: 0.001958, mean_q: 0.022963
  8820/100000: episode: 887, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000095, mae: 0.003739, mean_q: 0.021797
  8830/100000: episode: 888, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000075, mae: 0.003404, mean_q: 0.023517
  8840/100000: episode: 889, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000023, mae: 0.002624, mean_q: 0.022721
  8850/100000: episode: 890, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000031, mae: 0.002546, mean_q: 0.021707
  8860/100000: episode: 891, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000226, mae: 0.003989, mean_q: 0.021757
  8870/100000: episode: 892, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000037, mae: 0.002387, mean_q: 0.022809
  8880/100000: episode: 893, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000036, mae: 0.003021, mean_q: 0.021288
  8890/100000: episode: 894, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000041, mae: 0.003012, mean_q: 0.021799
  8900/100000: episode: 895, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.002358, mean_q: 0.021776
  8910/100000: episode: 896, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000198, mae: 0.002758, mean_q: 0.021748
  8920/100000: episode: 897, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000035, mae: 0.002236, mean_q: 0.021949
  8930/100000: episode: 898, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000257, mae: 0.003789, mean_q: 0.022154
  8940/100000: episode: 899, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000204, mae: 0.003898, mean_q: 0.023217
  8950/100000: episode: 900, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000249, mae: 0.004333, mean_q: 0.022527
  8960/100000: episode: 901, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000282, mae: 0.004264, mean_q: 0.021996
  8970/100000: episode: 902, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000058, mae: 0.003680, mean_q: 0.023652
  8980/100000: episode: 903, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000071, mae: 0.002711, mean_q: 0.022187
  8990/100000: episode: 904, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000013, mae: 0.001876, mean_q: 0.022011
Step 9000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.9000.hdf5
  9000/100000: episode: 905, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000058, mae: 0.002729, mean_q: 0.022086
  9010/100000: episode: 906, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000243, mae: 0.003652, mean_q: 0.022488
  9020/100000: episode: 907, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000060, mae: 0.002731, mean_q: 0.022421
  9030/100000: episode: 908, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000034, mae: 0.002146, mean_q: 0.022133
  9040/100000: episode: 909, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000054, mae: 0.002875, mean_q: 0.021408
  9050/100000: episode: 910, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000247, mae: 0.004165, mean_q: 0.022623
  9060/100000: episode: 911, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000392, mae: 0.005063, mean_q: 0.023334
  9070/100000: episode: 912, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000074, mae: 0.003156, mean_q: 0.023151
  9080/100000: episode: 913, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000033, mae: 0.001908, mean_q: 0.022228
  9090/100000: episode: 914, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.002969, mean_q: 0.021592
  9100/100000: episode: 915, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000014, mae: 0.002413, mean_q: 0.021263
  9110/100000: episode: 916, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000240, mae: 0.003639, mean_q: 0.021715
  9120/100000: episode: 917, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000204, mae: 0.003835, mean_q: 0.023317
  9130/100000: episode: 918, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000458, mae: 0.007173, mean_q: 0.024055
  9140/100000: episode: 919, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000019, mae: 0.002765, mean_q: 0.022364
  9150/100000: episode: 920, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000429, mae: 0.005287, mean_q: 0.022094
  9160/100000: episode: 921, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000445, mae: 0.007656, mean_q: 0.025628
  9170/100000: episode: 922, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000770, mae: 0.009278, mean_q: 0.026403
  9180/100000: episode: 923, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000039, mae: 0.003629, mean_q: 0.024837
  9190/100000: episode: 924, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000036, mae: 0.003549, mean_q: 0.021600
  9200/100000: episode: 925, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000251, mae: 0.004794, mean_q: 0.023347
  9210/100000: episode: 926, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000032, mae: 0.002340, mean_q: 0.023801
  9220/100000: episode: 927, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000263, mae: 0.004417, mean_q: 0.022609
  9230/100000: episode: 928, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.002058, mean_q: 0.023497
  9240/100000: episode: 929, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000053, mae: 0.002401, mean_q: 0.023166
  9250/100000: episode: 930, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000287, mae: 0.004843, mean_q: 0.023129
  9260/100000: episode: 931, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000236, mae: 0.003329, mean_q: 0.023614
  9270/100000: episode: 932, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000047, mae: 0.003618, mean_q: 0.024023
  9280/100000: episode: 933, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000020, mae: 0.002831, mean_q: 0.022353
  9290/100000: episode: 934, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000034, mae: 0.002734, mean_q: 0.022067
  9300/100000: episode: 935, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000260, mae: 0.004029, mean_q: 0.023061
  9310/100000: episode: 936, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000095, mae: 0.004618, mean_q: 0.024894
  9320/100000: episode: 937, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000080, mae: 0.003584, mean_q: 0.023132
  9330/100000: episode: 938, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000223, mae: 0.004071, mean_q: 0.024178
  9340/100000: episode: 939, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000015, mae: 0.002126, mean_q: 0.023222
  9350/100000: episode: 940, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000242, mae: 0.003873, mean_q: 0.023036
  9360/100000: episode: 941, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000255, mae: 0.004295, mean_q: 0.024179
  9370/100000: episode: 942, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000207, mae: 0.004494, mean_q: 0.025066
  9380/100000: episode: 943, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000052, mae: 0.002929, mean_q: 0.022491
  9390/100000: episode: 944, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000031, mae: 0.002124, mean_q: 0.023065
  9400/100000: episode: 945, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000202, mae: 0.002968, mean_q: 0.023200
  9410/100000: episode: 946, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000045, mae: 0.002979, mean_q: 0.023519
  9420/100000: episode: 947, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000222, mae: 0.003929, mean_q: 0.022178
  9430/100000: episode: 948, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000390, mae: 0.004907, mean_q: 0.024398
  9440/100000: episode: 949, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000051, mae: 0.003473, mean_q: 0.024839
  9450/100000: episode: 950, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000409, mae: 0.004732, mean_q: 0.023082
  9460/100000: episode: 951, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000222, mae: 0.003864, mean_q: 0.024331
[Info] 1-TH LEVEL FOUND: 0.023419173434376717, Considering 100/100 traces
  9470/100000: episode: 952, duration: 0.707s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000053, mae: 0.002925, mean_q: 0.024409
[Info] 2-TH LEVEL FOUND: 0.025251775979995728, Considering 100/100 traces
  9479/100000: episode: 953, duration: 0.705s, episode steps: 9, steps per second: 13, episode reward: 0.135, mean reward: 0.015 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.722 [2.000, 10.000], loss: 0.000225, mae: 0.003583, mean_q: 0.022838
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025251775979995728
2
  9489/100000: episode: 954, duration: 0.483s, episode steps: 10, steps per second: 21, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000244, mae: 0.004184, mean_q: 0.024034
  9499/100000: episode: 955, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000038, mae: 0.002975, mean_q: 0.024530
  9509/100000: episode: 956, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000054, mae: 0.002671, mean_q: 0.023223
  9519/100000: episode: 957, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002538, mean_q: 0.022935
  9529/100000: episode: 958, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000218, mae: 0.003129, mean_q: 0.023322
  9539/100000: episode: 959, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000087, mae: 0.002852, mean_q: 0.024042
  9549/100000: episode: 960, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000019, mae: 0.002452, mean_q: 0.023646
  9559/100000: episode: 961, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000256, mae: 0.004090, mean_q: 0.023484
  9569/100000: episode: 962, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000238, mae: 0.005169, mean_q: 0.025792
  9579/100000: episode: 963, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000220, mae: 0.003495, mean_q: 0.023974
  9589/100000: episode: 964, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000036, mae: 0.002669, mean_q: 0.023104
  9599/100000: episode: 965, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000013, mae: 0.002096, mean_q: 0.022859
  9609/100000: episode: 966, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000279, mae: 0.004626, mean_q: 0.023056
  9619/100000: episode: 967, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000229, mae: 0.004948, mean_q: 0.025105
  9629/100000: episode: 968, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000036, mae: 0.002170, mean_q: 0.023749
  9639/100000: episode: 969, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.002563, mean_q: 0.023158
  9649/100000: episode: 970, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000033, mae: 0.001905, mean_q: 0.023455
  9659/100000: episode: 971, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000059, mae: 0.002890, mean_q: 0.023721
  9669/100000: episode: 972, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000257, mae: 0.004056, mean_q: 0.022895
  9679/100000: episode: 973, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000220, mae: 0.004994, mean_q: 0.025977
  9689/100000: episode: 974, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000074, mae: 0.003465, mean_q: 0.024060
  9699/100000: episode: 975, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000032, mae: 0.002835, mean_q: 0.022537
  9709/100000: episode: 976, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000074, mae: 0.003151, mean_q: 0.022990
  9719/100000: episode: 977, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000206, mae: 0.003402, mean_q: 0.023937
  9729/100000: episode: 978, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000033, mae: 0.002056, mean_q: 0.023333
  9739/100000: episode: 979, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000199, mae: 0.002895, mean_q: 0.023098
  9749/100000: episode: 980, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000244, mae: 0.003812, mean_q: 0.023403
  9759/100000: episode: 981, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000201, mae: 0.002983, mean_q: 0.023848
  9769/100000: episode: 982, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000242, mae: 0.003774, mean_q: 0.023899
  9779/100000: episode: 983, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000080, mae: 0.004045, mean_q: 0.024686
  9789/100000: episode: 984, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000221, mae: 0.003262, mean_q: 0.023551
  9799/100000: episode: 985, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000207, mae: 0.003228, mean_q: 0.023624
  9809/100000: episode: 986, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000056, mae: 0.002512, mean_q: 0.023873
  9819/100000: episode: 987, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000388, mae: 0.003971, mean_q: 0.023816
  9829/100000: episode: 988, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000038, mae: 0.003463, mean_q: 0.025416
  9839/100000: episode: 989, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000033, mae: 0.002244, mean_q: 0.023562
  9849/100000: episode: 990, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000051, mae: 0.002854, mean_q: 0.023011
  9859/100000: episode: 991, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000507, mae: 0.007229, mean_q: 0.024600
  9869/100000: episode: 992, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000036, mae: 0.003787, mean_q: 0.026156
  9879/100000: episode: 993, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000093, mae: 0.003498, mean_q: 0.023544
  9889/100000: episode: 994, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000227, mae: 0.004124, mean_q: 0.024618
  9899/100000: episode: 995, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000031, mae: 0.002208, mean_q: 0.024631
  9909/100000: episode: 996, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000022, mae: 0.002873, mean_q: 0.023288
  9919/100000: episode: 997, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000059, mae: 0.003617, mean_q: 0.022739
  9929/100000: episode: 998, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000015, mae: 0.001858, mean_q: 0.023566
  9939/100000: episode: 999, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000218, mae: 0.003396, mean_q: 0.023018
  9949/100000: episode: 1000, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000056, mae: 0.002764, mean_q: 0.024283
  9959/100000: episode: 1001, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000043, mae: 0.003305, mean_q: 0.022976
  9969/100000: episode: 1002, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000208, mae: 0.004216, mean_q: 0.022593
  9979/100000: episode: 1003, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000056, mae: 0.002697, mean_q: 0.023172
  9989/100000: episode: 1004, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000009, mae: 0.001318, mean_q: 0.023182
  9999/100000: episode: 1005, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000199, mae: 0.002746, mean_q: 0.023175
Step 10000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.10000.hdf5
 10009/100000: episode: 1006, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000015, mae: 0.001676, mean_q: 0.023646
 10019/100000: episode: 1007, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000057, mae: 0.003074, mean_q: 0.022796
 10029/100000: episode: 1008, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000031, mae: 0.002618, mean_q: 0.022067
 10039/100000: episode: 1009, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000203, mae: 0.003219, mean_q: 0.022612
 10049/100000: episode: 1010, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000058, mae: 0.004159, mean_q: 0.025116
 10059/100000: episode: 1011, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.002608, mean_q: 0.022572
 10069/100000: episode: 1012, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000018, mae: 0.003193, mean_q: 0.021763
 10079/100000: episode: 1013, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000245, mae: 0.004404, mean_q: 0.023545
 10089/100000: episode: 1014, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000057, mae: 0.003427, mean_q: 0.024137
 10099/100000: episode: 1015, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000095, mae: 0.003408, mean_q: 0.022780
 10109/100000: episode: 1016, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000589, mae: 0.005412, mean_q: 0.023741
 10119/100000: episode: 1017, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000208, mae: 0.005297, mean_q: 0.025829
 10129/100000: episode: 1018, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000032, mae: 0.002055, mean_q: 0.023357
 10139/100000: episode: 1019, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000034, mae: 0.003013, mean_q: 0.022070
 10149/100000: episode: 1020, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000051, mae: 0.002245, mean_q: 0.023083
 10159/100000: episode: 1021, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000055, mae: 0.002629, mean_q: 0.023631
 10169/100000: episode: 1022, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000239, mae: 0.003675, mean_q: 0.022697
 10179/100000: episode: 1023, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000018, mae: 0.002526, mean_q: 0.024225
 10189/100000: episode: 1024, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000052, mae: 0.002653, mean_q: 0.022614
 10199/100000: episode: 1025, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000203, mae: 0.003688, mean_q: 0.022153
 10209/100000: episode: 1026, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000029, mae: 0.001489, mean_q: 0.023190
 10219/100000: episode: 1027, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000040, mae: 0.002845, mean_q: 0.022632
 10229/100000: episode: 1028, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000221, mae: 0.003538, mean_q: 0.022413
 10239/100000: episode: 1029, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000208, mae: 0.004285, mean_q: 0.024397
 10249/100000: episode: 1030, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000114, mae: 0.003852, mean_q: 0.023160
 10259/100000: episode: 1031, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000300, mae: 0.005553, mean_q: 0.024386
 10269/100000: episode: 1032, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000076, mae: 0.004057, mean_q: 0.024528
 10279/100000: episode: 1033, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000032, mae: 0.002307, mean_q: 0.022895
 10289/100000: episode: 1034, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000072, mae: 0.003187, mean_q: 0.022400
 10299/100000: episode: 1035, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000258, mae: 0.003959, mean_q: 0.023808
 10309/100000: episode: 1036, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000078, mae: 0.004172, mean_q: 0.024785
 10319/100000: episode: 1037, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000039, mae: 0.002984, mean_q: 0.022633
 10329/100000: episode: 1038, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000393, mae: 0.005084, mean_q: 0.022589
 10339/100000: episode: 1039, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000077, mae: 0.004268, mean_q: 0.024932
 10349/100000: episode: 1040, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000225, mae: 0.003738, mean_q: 0.023788
 10359/100000: episode: 1041, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000037, mae: 0.002701, mean_q: 0.023011
 10369/100000: episode: 1042, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000283, mae: 0.004976, mean_q: 0.022639
 10379/100000: episode: 1043, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000202, mae: 0.003197, mean_q: 0.023884
 10389/100000: episode: 1044, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002469, mean_q: 0.023802
 10399/100000: episode: 1045, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000224, mae: 0.003681, mean_q: 0.023711
 10409/100000: episode: 1046, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000222, mae: 0.003529, mean_q: 0.023022
 10419/100000: episode: 1047, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000020, mae: 0.002489, mean_q: 0.024149
 10429/100000: episode: 1048, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000239, mae: 0.003675, mean_q: 0.022802
 10439/100000: episode: 1049, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000018, mae: 0.002300, mean_q: 0.024169
 10449/100000: episode: 1050, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000201, mae: 0.002905, mean_q: 0.023161
 10459/100000: episode: 1051, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000037, mae: 0.002767, mean_q: 0.022637
 10469/100000: episode: 1052, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000034, mae: 0.002332, mean_q: 0.022837
 10479/100000: episode: 1053, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000035, mae: 0.002309, mean_q: 0.022770
[Info] 1-TH LEVEL FOUND: 0.02336604706943035, Considering 100/100 traces
 10489/100000: episode: 1054, duration: 0.722s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000059, mae: 0.002929, mean_q: 0.022884
[Info] 2-TH LEVEL FOUND: 0.02435368299484253, Considering 100/100 traces
 10499/100000: episode: 1055, duration: 0.651s, episode steps: 10, steps per second: 15, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000249, mae: 0.004469, mean_q: 0.023732
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02435368299484253
2
 10508/100000: episode: 1056, duration: 0.530s, episode steps: 9, steps per second: 17, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.278 [1.000, 10.000], loss: 0.000061, mae: 0.002902, mean_q: 0.023540
 10518/100000: episode: 1057, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000014, mae: 0.002360, mean_q: 0.022290
 10528/100000: episode: 1058, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000435, mae: 0.005657, mean_q: 0.022712
 10538/100000: episode: 1059, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000038, mae: 0.003333, mean_q: 0.024433
 10548/100000: episode: 1060, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000056, mae: 0.003284, mean_q: 0.022348
 10558/100000: episode: 1061, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000117, mae: 0.004160, mean_q: 0.022359
 10568/100000: episode: 1062, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000023, mae: 0.002685, mean_q: 0.023423
 10578/100000: episode: 1063, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002554, mean_q: 0.022290
 10588/100000: episode: 1064, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002995, mean_q: 0.022222
 10598/100000: episode: 1065, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.002689, mean_q: 0.023343
 10608/100000: episode: 1066, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.002713, mean_q: 0.023673
 10618/100000: episode: 1067, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000020, mae: 0.002296, mean_q: 0.022748
 10628/100000: episode: 1068, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000034, mae: 0.002914, mean_q: 0.021691
 10638/100000: episode: 1069, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000032, mae: 0.002538, mean_q: 0.021758
 10648/100000: episode: 1070, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000015, mae: 0.002055, mean_q: 0.022141
 10658/100000: episode: 1071, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000207, mae: 0.003526, mean_q: 0.021993
 10668/100000: episode: 1072, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000407, mae: 0.005642, mean_q: 0.024491
 10678/100000: episode: 1073, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000062, mae: 0.004006, mean_q: 0.023842
 10688/100000: episode: 1074, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000018, mae: 0.003158, mean_q: 0.021257
 10698/100000: episode: 1075, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000036, mae: 0.003242, mean_q: 0.021259
 10708/100000: episode: 1076, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000035, mae: 0.002088, mean_q: 0.022533
 10718/100000: episode: 1077, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000032, mae: 0.001944, mean_q: 0.022081
 10728/100000: episode: 1078, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000221, mae: 0.003168, mean_q: 0.022684
 10738/100000: episode: 1079, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000018, mae: 0.002245, mean_q: 0.022518
 10748/100000: episode: 1080, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000039, mae: 0.003142, mean_q: 0.021314
 10758/100000: episode: 1081, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000224, mae: 0.003638, mean_q: 0.022729
 10768/100000: episode: 1082, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000226, mae: 0.003715, mean_q: 0.022229
 10778/100000: episode: 1083, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000578, mae: 0.005391, mean_q: 0.022337
 10788/100000: episode: 1084, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000213, mae: 0.006277, mean_q: 0.025845
 10798/100000: episode: 1085, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000395, mae: 0.004946, mean_q: 0.022789
 10808/100000: episode: 1086, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000062, mae: 0.003251, mean_q: 0.022705
 10818/100000: episode: 1087, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000034, mae: 0.002330, mean_q: 0.022073
 10828/100000: episode: 1088, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000055, mae: 0.002534, mean_q: 0.022506
 10838/100000: episode: 1089, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000196, mae: 0.002466, mean_q: 0.022195
 10848/100000: episode: 1090, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000053, mae: 0.003435, mean_q: 0.024219
 10858/100000: episode: 1091, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000019, mae: 0.002524, mean_q: 0.022612
 10868/100000: episode: 1092, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000199, mae: 0.003281, mean_q: 0.021923
 10878/100000: episode: 1093, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000057, mae: 0.002502, mean_q: 0.022590
 10888/100000: episode: 1094, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000246, mae: 0.004255, mean_q: 0.023281
 10898/100000: episode: 1095, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000052, mae: 0.002470, mean_q: 0.022911
 10908/100000: episode: 1096, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000019, mae: 0.002536, mean_q: 0.022033
 10918/100000: episode: 1097, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002941, mean_q: 0.021336
 10928/100000: episode: 1098, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002631, mean_q: 0.021735
 10938/100000: episode: 1099, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.002465, mean_q: 0.021923
 10948/100000: episode: 1100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000006, mae: 0.001246, mean_q: 0.021690
 10958/100000: episode: 1101, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000030, mae: 0.001969, mean_q: 0.021752
 10968/100000: episode: 1102, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000448, mae: 0.005277, mean_q: 0.022298
 10978/100000: episode: 1103, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000017, mae: 0.003085, mean_q: 0.023931
 10988/100000: episode: 1104, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.002729, mean_q: 0.021066
 10998/100000: episode: 1105, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000011, mae: 0.002677, mean_q: 0.020828
Step 11000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.11000.hdf5
 11008/100000: episode: 1106, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000017, mae: 0.003150, mean_q: 0.020378
 11018/100000: episode: 1107, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.002789, mean_q: 0.020763
 11028/100000: episode: 1108, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.003009, mean_q: 0.020298
 11038/100000: episode: 1109, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000061, mae: 0.003683, mean_q: 0.020702
 11048/100000: episode: 1110, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000009, mae: 0.002172, mean_q: 0.020098
 11058/100000: episode: 1111, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000009, mae: 0.001875, mean_q: 0.020347
 11068/100000: episode: 1112, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000218, mae: 0.002853, mean_q: 0.020722
 11078/100000: episode: 1113, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000237, mae: 0.003872, mean_q: 0.022435
 11088/100000: episode: 1114, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000055, mae: 0.003428, mean_q: 0.022495
 11098/100000: episode: 1115, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002936, mean_q: 0.020468
 11108/100000: episode: 1116, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000072, mae: 0.002969, mean_q: 0.021429
 11118/100000: episode: 1117, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000013, mae: 0.002196, mean_q: 0.021694
 11128/100000: episode: 1118, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000011, mae: 0.002481, mean_q: 0.020060
 11138/100000: episode: 1119, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.002387, mean_q: 0.020052
 11148/100000: episode: 1120, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000055, mae: 0.002403, mean_q: 0.020759
 11158/100000: episode: 1121, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000074, mae: 0.002467, mean_q: 0.021104
 11168/100000: episode: 1122, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000221, mae: 0.003027, mean_q: 0.021227
 11178/100000: episode: 1123, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000032, mae: 0.001741, mean_q: 0.021234
 11188/100000: episode: 1124, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000200, mae: 0.002755, mean_q: 0.020611
 11198/100000: episode: 1125, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000010, mae: 0.001398, mean_q: 0.021040
 11208/100000: episode: 1126, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000202, mae: 0.002753, mean_q: 0.020809
 11218/100000: episode: 1127, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000018, mae: 0.002183, mean_q: 0.021240
 11228/100000: episode: 1128, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000061, mae: 0.003538, mean_q: 0.019834
 11238/100000: episode: 1129, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000039, mae: 0.002305, mean_q: 0.020845
 11248/100000: episode: 1130, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000053, mae: 0.002054, mean_q: 0.020780
 11258/100000: episode: 1131, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000017, mae: 0.002088, mean_q: 0.020494
 11268/100000: episode: 1132, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000038, mae: 0.003286, mean_q: 0.019444
 11278/100000: episode: 1133, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000035, mae: 0.002557, mean_q: 0.020034
 11288/100000: episode: 1134, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000038, mae: 0.002841, mean_q: 0.019809
 11298/100000: episode: 1135, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000036, mae: 0.002510, mean_q: 0.019808
 11308/100000: episode: 1136, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000009, mae: 0.001272, mean_q: 0.020074
 11318/100000: episode: 1137, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000266, mae: 0.004276, mean_q: 0.020207
 11328/100000: episode: 1138, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000226, mae: 0.004100, mean_q: 0.021261
 11338/100000: episode: 1139, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000031, mae: 0.001887, mean_q: 0.020731
 11348/100000: episode: 1140, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002712, mean_q: 0.019376
 11358/100000: episode: 1141, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000223, mae: 0.003372, mean_q: 0.019852
 11368/100000: episode: 1142, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000010, mae: 0.001683, mean_q: 0.020797
 11378/100000: episode: 1143, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000015, mae: 0.002469, mean_q: 0.019300
 11388/100000: episode: 1144, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000206, mae: 0.003662, mean_q: 0.019276
 11398/100000: episode: 1145, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000039, mae: 0.002671, mean_q: 0.020764
 11408/100000: episode: 1146, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000033, mae: 0.002123, mean_q: 0.019791
 11418/100000: episode: 1147, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000033, mae: 0.002111, mean_q: 0.019557
 11428/100000: episode: 1148, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000013, mae: 0.001416, mean_q: 0.020028
 11438/100000: episode: 1149, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000036, mae: 0.002647, mean_q: 0.019248
 11448/100000: episode: 1150, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000016, mae: 0.002350, mean_q: 0.019032
 11458/100000: episode: 1151, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000032, mae: 0.001808, mean_q: 0.019578
 11468/100000: episode: 1152, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000395, mae: 0.004282, mean_q: 0.020521
 11478/100000: episode: 1153, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000060, mae: 0.004668, mean_q: 0.022338
 11488/100000: episode: 1154, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000205, mae: 0.003431, mean_q: 0.019183
 11498/100000: episode: 1155, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000009, mae: 0.001435, mean_q: 0.020036
[Info] 1-TH LEVEL FOUND: 0.021361373364925385, Considering 100/100 traces
 11508/100000: episode: 1156, duration: 0.719s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000201, mae: 0.002979, mean_q: 0.020711
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021361373364925385
1
 11517/100000: episode: 1157, duration: 0.493s, episode steps: 9, steps per second: 18, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.611 [1.000, 10.000], loss: 0.000057, mae: 0.002408, mean_q: 0.020410
 11527/100000: episode: 1158, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000012, mae: 0.002462, mean_q: 0.018896
 11537/100000: episode: 1159, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000016, mae: 0.002353, mean_q: 0.019064
 11547/100000: episode: 1160, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000040, mae: 0.002997, mean_q: 0.020987
 11557/100000: episode: 1161, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000220, mae: 0.002877, mean_q: 0.020235
 11567/100000: episode: 1162, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000054, mae: 0.002411, mean_q: 0.020316
 11577/100000: episode: 1163, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000032, mae: 0.002066, mean_q: 0.019504
 11587/100000: episode: 1164, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000032, mae: 0.001790, mean_q: 0.020054
 11597/100000: episode: 1165, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000017, mae: 0.002458, mean_q: 0.019080
 11607/100000: episode: 1166, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000202, mae: 0.002975, mean_q: 0.019712
 11617/100000: episode: 1167, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000081, mae: 0.003503, mean_q: 0.020331
 11627/100000: episode: 1168, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000037, mae: 0.002591, mean_q: 0.019548
 11637/100000: episode: 1169, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000266, mae: 0.004314, mean_q: 0.018916
 11647/100000: episode: 1170, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000013, mae: 0.002250, mean_q: 0.020720
 11657/100000: episode: 1171, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000204, mae: 0.003579, mean_q: 0.018609
 11667/100000: episode: 1172, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000203, mae: 0.003276, mean_q: 0.020481
 11677/100000: episode: 1173, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000038, mae: 0.003120, mean_q: 0.019431
 11687/100000: episode: 1174, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000225, mae: 0.003635, mean_q: 0.019432
 11697/100000: episode: 1175, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000013, mae: 0.002131, mean_q: 0.019843
 11707/100000: episode: 1176, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000417, mae: 0.005110, mean_q: 0.018746
 11717/100000: episode: 1177, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000202, mae: 0.003494, mean_q: 0.020670
 11727/100000: episode: 1178, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000043, mae: 0.003026, mean_q: 0.020365
 11737/100000: episode: 1179, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000014, mae: 0.002165, mean_q: 0.019046
 11747/100000: episode: 1180, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000227, mae: 0.003990, mean_q: 0.020604
 11757/100000: episode: 1181, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.002453, mean_q: 0.020463
 11767/100000: episode: 1182, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000036, mae: 0.002115, mean_q: 0.019977
 11777/100000: episode: 1183, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000095, mae: 0.003182, mean_q: 0.020300
 11787/100000: episode: 1184, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000011, mae: 0.001736, mean_q: 0.020252
 11797/100000: episode: 1185, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000011, mae: 0.002307, mean_q: 0.018739
 11807/100000: episode: 1186, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000648, mae: 0.007474, mean_q: 0.020712
 11817/100000: episode: 1187, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000044, mae: 0.005014, mean_q: 0.022945
 11827/100000: episode: 1188, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002735, mean_q: 0.019576
 11837/100000: episode: 1189, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000246, mae: 0.004238, mean_q: 0.019222
 11847/100000: episode: 1190, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000013, mae: 0.002668, mean_q: 0.021330
 11857/100000: episode: 1191, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000221, mae: 0.002827, mean_q: 0.020176
 11867/100000: episode: 1192, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.002840, mean_q: 0.021165
 11877/100000: episode: 1193, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000011, mae: 0.001896, mean_q: 0.019540
 11887/100000: episode: 1194, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000436, mae: 0.005689, mean_q: 0.021360
 11897/100000: episode: 1195, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000244, mae: 0.005445, mean_q: 0.022642
 11907/100000: episode: 1196, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.002781, mean_q: 0.020610
 11917/100000: episode: 1197, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000515, mae: 0.006746, mean_q: 0.020901
 11927/100000: episode: 1198, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000066, mae: 0.006042, mean_q: 0.024339
 11937/100000: episode: 1199, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000012, mae: 0.002747, mean_q: 0.019514
 11947/100000: episode: 1200, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000033, mae: 0.003564, mean_q: 0.019518
 11957/100000: episode: 1201, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000202, mae: 0.003123, mean_q: 0.020159
 11967/100000: episode: 1202, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000013, mae: 0.001961, mean_q: 0.021015
 11977/100000: episode: 1203, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000208, mae: 0.003806, mean_q: 0.019552
 11987/100000: episode: 1204, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000205, mae: 0.003674, mean_q: 0.021798
 11997/100000: episode: 1205, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000227, mae: 0.004224, mean_q: 0.021597
Step 12000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.12000.hdf5
 12007/100000: episode: 1206, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000202, mae: 0.003750, mean_q: 0.022293
 12017/100000: episode: 1207, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000056, mae: 0.002543, mean_q: 0.020891
 12027/100000: episode: 1208, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000054, mae: 0.002318, mean_q: 0.020724
 12037/100000: episode: 1209, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000079, mae: 0.002944, mean_q: 0.020822
 12047/100000: episode: 1210, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000074, mae: 0.003665, mean_q: 0.022477
 12057/100000: episode: 1211, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000031, mae: 0.001961, mean_q: 0.021198
 12067/100000: episode: 1212, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000056, mae: 0.003020, mean_q: 0.020423
 12077/100000: episode: 1213, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000200, mae: 0.002718, mean_q: 0.020564
 12087/100000: episode: 1214, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000037, mae: 0.002423, mean_q: 0.021240
 12097/100000: episode: 1215, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000054, mae: 0.002667, mean_q: 0.020498
 12107/100000: episode: 1216, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.002501, mean_q: 0.020818
 12117/100000: episode: 1217, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000095, mae: 0.003129, mean_q: 0.020899
 12127/100000: episode: 1218, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002121, mean_q: 0.021041
 12137/100000: episode: 1219, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000241, mae: 0.003472, mean_q: 0.021302
 12147/100000: episode: 1220, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000011, mae: 0.001691, mean_q: 0.021219
 12157/100000: episode: 1221, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000012, mae: 0.002503, mean_q: 0.019664
 12167/100000: episode: 1222, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000222, mae: 0.003402, mean_q: 0.020749
 12177/100000: episode: 1223, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000052, mae: 0.002563, mean_q: 0.021311
 12187/100000: episode: 1224, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000040, mae: 0.002461, mean_q: 0.020793
 12197/100000: episode: 1225, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000056, mae: 0.002765, mean_q: 0.021584
 12207/100000: episode: 1226, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000206, mae: 0.003444, mean_q: 0.021493
 12217/100000: episode: 1227, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000051, mae: 0.001954, mean_q: 0.021147
 12227/100000: episode: 1228, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002270, mean_q: 0.020477
 12237/100000: episode: 1229, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000073, mae: 0.002889, mean_q: 0.020280
 12247/100000: episode: 1230, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000059, mae: 0.002656, mean_q: 0.021101
 12257/100000: episode: 1231, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000224, mae: 0.002981, mean_q: 0.020939
 12267/100000: episode: 1232, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000043, mae: 0.003399, mean_q: 0.021946
 12277/100000: episode: 1233, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000207, mae: 0.003696, mean_q: 0.019968
 12287/100000: episode: 1234, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000056, mae: 0.002543, mean_q: 0.021185
 12297/100000: episode: 1235, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000053, mae: 0.002079, mean_q: 0.020775
 12307/100000: episode: 1236, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000074, mae: 0.002563, mean_q: 0.021101
 12317/100000: episode: 1237, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000016, mae: 0.002047, mean_q: 0.020527
 12327/100000: episode: 1238, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000015, mae: 0.002757, mean_q: 0.019507
 12337/100000: episode: 1239, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000055, mae: 0.003278, mean_q: 0.019272
 12347/100000: episode: 1240, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000101, mae: 0.004362, mean_q: 0.021738
 12357/100000: episode: 1241, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.002577, mean_q: 0.020038
 12367/100000: episode: 1242, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000056, mae: 0.002902, mean_q: 0.019976
 12377/100000: episode: 1243, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002363, mean_q: 0.021074
 12387/100000: episode: 1244, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000398, mae: 0.004774, mean_q: 0.021046
 12397/100000: episode: 1245, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000207, mae: 0.003347, mean_q: 0.020973
 12407/100000: episode: 1246, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000012, mae: 0.001508, mean_q: 0.020415
 12417/100000: episode: 1247, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000416, mae: 0.004821, mean_q: 0.020520
 12427/100000: episode: 1248, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000222, mae: 0.004292, mean_q: 0.022253
 12437/100000: episode: 1249, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000059, mae: 0.002977, mean_q: 0.020895
 12447/100000: episode: 1250, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000263, mae: 0.003792, mean_q: 0.020666
 12457/100000: episode: 1251, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000053, mae: 0.003088, mean_q: 0.022042
 12467/100000: episode: 1252, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000222, mae: 0.003187, mean_q: 0.021198
[Info] FALSIFICATION!
 12477/100000: episode: 1253, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000054, mae: 0.002072, mean_q: 0.021022
 12487/100000: episode: 1254, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000201, mae: 0.002795, mean_q: 0.021461
 12497/100000: episode: 1255, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000044, mae: 0.003082, mean_q: 0.020560
 12507/100000: episode: 1256, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000008, mae: 0.001421, mean_q: 0.020293
[Info] Complete ISplit Iteration
[Info] Levels: [0.020589141]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 12517/100000: episode: 1257, duration: 0.742s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.002570, mean_q: 0.021063
 12527/100000: episode: 1258, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000031, mae: 0.002429, mean_q: 0.019750
 12537/100000: episode: 1259, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000118, mae: 0.003972, mean_q: 0.020879
 12547/100000: episode: 1260, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000018, mae: 0.002603, mean_q: 0.021258
 12557/100000: episode: 1261, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.002101, mean_q: 0.020569
 12567/100000: episode: 1262, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000385, mae: 0.004171, mean_q: 0.021943
 12577/100000: episode: 1263, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.001882, mae: 0.008815, mean_q: 0.023127
 12587/100000: episode: 1264, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000430, mae: 0.008062, mean_q: 0.025407
 12597/100000: episode: 1265, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000063, mae: 0.004418, mean_q: 0.022964
 12607/100000: episode: 1266, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.003194, mean_q: 0.020355
 12617/100000: episode: 1267, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000247, mae: 0.004414, mean_q: 0.021794
 12627/100000: episode: 1268, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002798, mean_q: 0.022408
 12637/100000: episode: 1269, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000016, mae: 0.002670, mean_q: 0.020538
 12647/100000: episode: 1270, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000095, mae: 0.003323, mean_q: 0.021187
 12657/100000: episode: 1271, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000019, mae: 0.002301, mean_q: 0.021800
 12667/100000: episode: 1272, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000029, mae: 0.002177, mean_q: 0.020614
 12677/100000: episode: 1273, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000029, mae: 0.001734, mean_q: 0.021159
 12687/100000: episode: 1274, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000427, mae: 0.004873, mean_q: 0.022227
 12697/100000: episode: 1275, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000433, mae: 0.007252, mean_q: 0.024587
 12707/100000: episode: 1276, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002593, mean_q: 0.021789
 12717/100000: episode: 1277, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000102, mae: 0.004748, mean_q: 0.020413
 12727/100000: episode: 1278, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000053, mae: 0.002456, mean_q: 0.022105
 12737/100000: episode: 1279, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000283, mae: 0.005163, mean_q: 0.022660
 12747/100000: episode: 1280, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000057, mae: 0.003120, mean_q: 0.022769
 12757/100000: episode: 1281, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000200, mae: 0.003221, mean_q: 0.020806
 12767/100000: episode: 1282, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000037, mae: 0.002517, mean_q: 0.021553
 12777/100000: episode: 1283, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000241, mae: 0.003680, mean_q: 0.022374
 12787/100000: episode: 1284, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000053, mae: 0.002466, mean_q: 0.022230
 12797/100000: episode: 1285, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000056, mae: 0.002590, mean_q: 0.021527
 12807/100000: episode: 1286, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000080, mae: 0.003060, mean_q: 0.021933
 12817/100000: episode: 1287, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000227, mae: 0.003900, mean_q: 0.022551
 12827/100000: episode: 1288, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000042, mae: 0.003000, mean_q: 0.022201
 12837/100000: episode: 1289, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000042, mae: 0.003198, mean_q: 0.021008
 12847/100000: episode: 1290, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000018, mae: 0.003149, mean_q: 0.020400
 12857/100000: episode: 1291, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000076, mae: 0.003783, mean_q: 0.020206
 12867/100000: episode: 1292, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000263, mae: 0.004396, mean_q: 0.022297
 12877/100000: episode: 1293, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000243, mae: 0.004681, mean_q: 0.022870
 12887/100000: episode: 1294, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000019, mae: 0.002344, mean_q: 0.021278
 12897/100000: episode: 1295, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000014, mae: 0.002751, mean_q: 0.020369
 12907/100000: episode: 1296, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002546, mean_q: 0.020598
 12917/100000: episode: 1297, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000031, mae: 0.001949, mean_q: 0.020894
 12927/100000: episode: 1298, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000246, mae: 0.003734, mean_q: 0.021366
 12937/100000: episode: 1299, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001719, mae: 0.007235, mean_q: 0.022551
 12947/100000: episode: 1300, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000263, mae: 0.007114, mean_q: 0.025672
 12957/100000: episode: 1301, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000216, mae: 0.004123, mean_q: 0.023199
 12967/100000: episode: 1302, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000220, mae: 0.003824, mean_q: 0.020945
 12977/100000: episode: 1303, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000248, mae: 0.004250, mean_q: 0.022432
 12987/100000: episode: 1304, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000041, mae: 0.003362, mean_q: 0.023038
 12997/100000: episode: 1305, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000043, mae: 0.003043, mean_q: 0.021419
Step 13000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.13000.hdf5
 13007/100000: episode: 1306, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000262, mae: 0.004182, mean_q: 0.022475
 13017/100000: episode: 1307, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000020, mae: 0.002348, mean_q: 0.022383
 13027/100000: episode: 1308, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000093, mae: 0.002931, mean_q: 0.021863
 13037/100000: episode: 1309, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000058, mae: 0.002689, mean_q: 0.021920
 13047/100000: episode: 1310, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002344, mean_q: 0.021838
 13057/100000: episode: 1311, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000287, mae: 0.005278, mean_q: 0.023117
 13067/100000: episode: 1312, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000073, mae: 0.003287, mean_q: 0.023119
 13077/100000: episode: 1313, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000620, mae: 0.006657, mean_q: 0.022822
 13087/100000: episode: 1314, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.004513, mean_q: 0.025101
 13097/100000: episode: 1315, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000077, mae: 0.003663, mean_q: 0.021806
 13107/100000: episode: 1316, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000017, mae: 0.002815, mean_q: 0.021261
 13117/100000: episode: 1317, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000201, mae: 0.002964, mean_q: 0.022216
 13127/100000: episode: 1318, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000057, mae: 0.003196, mean_q: 0.023094
 13137/100000: episode: 1319, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000032, mae: 0.002153, mean_q: 0.021788
 13147/100000: episode: 1320, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000012, mae: 0.002057, mean_q: 0.021747
 13157/100000: episode: 1321, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000058, mae: 0.003117, mean_q: 0.021435
 13167/100000: episode: 1322, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000053, mae: 0.002359, mean_q: 0.021890
 13177/100000: episode: 1323, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000058, mae: 0.003018, mean_q: 0.021462
 13187/100000: episode: 1324, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000011, mae: 0.001893, mean_q: 0.021296
 13197/100000: episode: 1325, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000227, mae: 0.003804, mean_q: 0.021341
 13207/100000: episode: 1326, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.003109, mean_q: 0.022825
 13217/100000: episode: 1327, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001536, mae: 0.007108, mean_q: 0.023999
 13227/100000: episode: 1328, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000014, mae: 0.002342, mean_q: 0.021925
 13237/100000: episode: 1329, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000079, mae: 0.004343, mean_q: 0.020582
 13247/100000: episode: 1330, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000036, mae: 0.002388, mean_q: 0.022111
 13257/100000: episode: 1331, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000060, mae: 0.002735, mean_q: 0.021910
 13267/100000: episode: 1332, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000019, mae: 0.002340, mean_q: 0.021402
 13277/100000: episode: 1333, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.002849, mean_q: 0.020623
 13287/100000: episode: 1334, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000057, mae: 0.003208, mean_q: 0.020772
 13297/100000: episode: 1335, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000203, mae: 0.002968, mean_q: 0.021898
 13307/100000: episode: 1336, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000096, mae: 0.003488, mean_q: 0.020938
 13317/100000: episode: 1337, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000222, mae: 0.003022, mean_q: 0.021567
 13327/100000: episode: 1338, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.001563, mae: 0.006031, mean_q: 0.022626
 13337/100000: episode: 1339, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000240, mae: 0.006117, mean_q: 0.025179
 13347/100000: episode: 1340, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.002811, mean_q: 0.021501
 13357/100000: episode: 1341, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000014, mae: 0.002853, mean_q: 0.020728
 13367/100000: episode: 1342, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000032, mae: 0.002231, mean_q: 0.021412
 13377/100000: episode: 1343, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000061, mae: 0.003067, mean_q: 0.021325
 13387/100000: episode: 1344, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000074, mae: 0.002494, mean_q: 0.021734
 13397/100000: episode: 1345, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.001913, mae: 0.009539, mean_q: 0.023907
 13407/100000: episode: 1346, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000101, mae: 0.005856, mean_q: 0.024690
 13417/100000: episode: 1347, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000227, mae: 0.004621, mean_q: 0.020508
 13427/100000: episode: 1348, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000282, mae: 0.005118, mean_q: 0.023182
 13437/100000: episode: 1349, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.001526, mae: 0.007009, mean_q: 0.024684
 13447/100000: episode: 1350, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000041, mae: 0.003520, mean_q: 0.023542
 13457/100000: episode: 1351, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000012, mae: 0.002320, mean_q: 0.021293
 13467/100000: episode: 1352, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000054, mae: 0.003095, mean_q: 0.021738
 13477/100000: episode: 1353, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000056, mae: 0.002955, mean_q: 0.021757
 13487/100000: episode: 1354, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000012, mae: 0.001971, mean_q: 0.021460
 13497/100000: episode: 1355, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001515, mae: 0.005912, mean_q: 0.020743
 13507/100000: episode: 1356, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001537, mae: 0.008938, mean_q: 0.026514
[Info] 1-TH LEVEL FOUND: 0.023588428273797035, Considering 100/100 traces
 13517/100000: episode: 1357, duration: 0.679s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000042, mae: 0.005583, mean_q: 0.026434
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023588428273797035
1
 13526/100000: episode: 1358, duration: 0.485s, episode steps: 9, steps per second: 19, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.889 [2.000, 10.000], loss: 0.000041, mae: 0.003659, mean_q: 0.021501
 13536/100000: episode: 1359, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000031, mae: 0.003131, mean_q: 0.021080
 13546/100000: episode: 1360, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001525, mae: 0.006013, mean_q: 0.023862
 13556/100000: episode: 1361, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000020, mae: 0.003388, mean_q: 0.024668
 13566/100000: episode: 1362, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000114, mae: 0.004266, mean_q: 0.021554
 13576/100000: episode: 1363, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000072, mae: 0.003185, mean_q: 0.023744
 13586/100000: episode: 1364, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000041, mae: 0.003006, mean_q: 0.023336
 13596/100000: episode: 1365, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000020, mae: 0.003135, mean_q: 0.021545
 13606/100000: episode: 1366, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000120, mae: 0.004378, mean_q: 0.022242
 13616/100000: episode: 1367, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000076, mae: 0.002978, mean_q: 0.023076
 13626/100000: episode: 1368, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.001786, mean_q: 0.022840
 13636/100000: episode: 1369, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000020, mae: 0.002716, mean_q: 0.021862
 13646/100000: episode: 1370, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.002860, mean_q: 0.021256
 13656/100000: episode: 1371, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000096, mae: 0.003916, mean_q: 0.021472
 13666/100000: episode: 1372, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.001513, mae: 0.005972, mean_q: 0.023533
 13676/100000: episode: 1373, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000040, mae: 0.003722, mean_q: 0.024193
 13686/100000: episode: 1374, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000020, mae: 0.003390, mean_q: 0.020962
 13696/100000: episode: 1375, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000039, mae: 0.003689, mean_q: 0.020740
 13706/100000: episode: 1376, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000056, mae: 0.002920, mean_q: 0.021976
 13716/100000: episode: 1377, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000036, mae: 0.002499, mean_q: 0.021773
 13726/100000: episode: 1378, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.002873, mean_q: 0.021347
 13736/100000: episode: 1379, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000075, mae: 0.003525, mean_q: 0.020767
 13746/100000: episode: 1380, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000054, mae: 0.002503, mean_q: 0.022062
 13756/100000: episode: 1381, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000016, mae: 0.002117, mean_q: 0.022238
 13766/100000: episode: 1382, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.003095, mean_q: 0.020444
 13776/100000: episode: 1383, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000083, mae: 0.003690, mean_q: 0.021505
 13786/100000: episode: 1384, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000249, mae: 0.004684, mean_q: 0.022667
 13796/100000: episode: 1385, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000216, mae: 0.002524, mean_q: 0.021658
 13806/100000: episode: 1386, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000051, mae: 0.002590, mean_q: 0.022697
 13816/100000: episode: 1387, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001831, mean_q: 0.021460
 13826/100000: episode: 1388, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002435, mean_q: 0.021148
 13836/100000: episode: 1389, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000096, mae: 0.003292, mean_q: 0.021821
 13846/100000: episode: 1390, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002334, mean_q: 0.022197
 13856/100000: episode: 1391, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000226, mae: 0.003698, mean_q: 0.021398
 13866/100000: episode: 1392, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000034, mae: 0.002266, mean_q: 0.021321
 13876/100000: episode: 1393, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000023, mae: 0.002951, mean_q: 0.020807
 13886/100000: episode: 1394, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000013, mae: 0.002137, mean_q: 0.020747
 13896/100000: episode: 1395, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000075, mae: 0.002785, mean_q: 0.021429
 13906/100000: episode: 1396, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001550, mae: 0.005224, mean_q: 0.021283
 13916/100000: episode: 1397, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000078, mae: 0.004858, mean_q: 0.023974
 13926/100000: episode: 1398, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000073, mae: 0.003302, mean_q: 0.022673
 13936/100000: episode: 1399, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000080, mae: 0.003568, mean_q: 0.021204
 13946/100000: episode: 1400, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000199, mae: 0.002584, mean_q: 0.021733
 13956/100000: episode: 1401, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000035, mae: 0.002355, mean_q: 0.021449
 13966/100000: episode: 1402, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000255, mae: 0.003516, mean_q: 0.021228
 13976/100000: episode: 1403, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000226, mae: 0.005109, mean_q: 0.023836
 13986/100000: episode: 1404, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000027, mae: 0.001877, mean_q: 0.021858
 13996/100000: episode: 1405, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.002608, mean_q: 0.020691
Step 14000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.14000.hdf5
 14006/100000: episode: 1406, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000056, mae: 0.002861, mean_q: 0.021270
 14016/100000: episode: 1407, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.001559, mae: 0.007429, mean_q: 0.023540
 14026/100000: episode: 1408, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000226, mae: 0.004053, mean_q: 0.022320
 14036/100000: episode: 1409, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000040, mae: 0.002908, mean_q: 0.021382
 14046/100000: episode: 1410, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000017, mae: 0.002408, mean_q: 0.021075
 14056/100000: episode: 1411, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000019, mae: 0.003422, mean_q: 0.020064
 14066/100000: episode: 1412, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000010, mae: 0.002565, mean_q: 0.019974
 14076/100000: episode: 1413, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000054, mae: 0.002754, mean_q: 0.020629
 14086/100000: episode: 1414, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002298, mean_q: 0.021389
 14096/100000: episode: 1415, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000011, mae: 0.001568, mean_q: 0.020994
 14106/100000: episode: 1416, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000036, mae: 0.002722, mean_q: 0.020161
 14116/100000: episode: 1417, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000135, mae: 0.004022, mean_q: 0.021492
 14126/100000: episode: 1418, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000056, mae: 0.003193, mean_q: 0.022371
 14136/100000: episode: 1419, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000077, mae: 0.003402, mean_q: 0.020674
 14146/100000: episode: 1420, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000015, mae: 0.002420, mean_q: 0.020134
 14156/100000: episode: 1421, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000011, mae: 0.001942, mean_q: 0.020379
 14166/100000: episode: 1422, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000010, mae: 0.002123, mean_q: 0.019914
 14176/100000: episode: 1423, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000011, mae: 0.002101, mean_q: 0.019765
 14186/100000: episode: 1424, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000039, mae: 0.002732, mean_q: 0.020253
 14196/100000: episode: 1425, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000010, mae: 0.002063, mean_q: 0.019740
 14206/100000: episode: 1426, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000037, mae: 0.002883, mean_q: 0.019463
 14216/100000: episode: 1427, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.001996, mean_q: 0.020114
 14226/100000: episode: 1428, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000056, mae: 0.003018, mean_q: 0.019642
 14236/100000: episode: 1429, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000012, mae: 0.002266, mean_q: 0.019126
 14246/100000: episode: 1430, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000059, mae: 0.002923, mean_q: 0.019793
 14256/100000: episode: 1431, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000218, mae: 0.002664, mean_q: 0.020184
 14266/100000: episode: 1432, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000051, mae: 0.002419, mean_q: 0.020940
 14276/100000: episode: 1433, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000016, mae: 0.002552, mean_q: 0.019445
 14286/100000: episode: 1434, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000119, mae: 0.004095, mean_q: 0.019095
 14296/100000: episode: 1435, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000036, mae: 0.002760, mean_q: 0.020911
 14306/100000: episode: 1436, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000248, mae: 0.003913, mean_q: 0.019672
 14316/100000: episode: 1437, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000018, mae: 0.002096, mean_q: 0.020451
 14326/100000: episode: 1438, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000014, mae: 0.001969, mean_q: 0.019551
 14336/100000: episode: 1439, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002675, mean_q: 0.018902
 14346/100000: episode: 1440, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000010, mae: 0.001749, mean_q: 0.019211
 14356/100000: episode: 1441, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000011, mae: 0.001891, mean_q: 0.019289
 14366/100000: episode: 1442, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000010, mae: 0.001979, mean_q: 0.018685
 14376/100000: episode: 1443, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001641, mae: 0.008243, mean_q: 0.021281
 14386/100000: episode: 1444, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000085, mae: 0.006142, mean_q: 0.023545
 14396/100000: episode: 1445, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000224, mae: 0.003432, mean_q: 0.020088
 14406/100000: episode: 1446, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002035, mean_q: 0.020119
 14416/100000: episode: 1447, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000030, mae: 0.001506, mean_q: 0.020399
 14426/100000: episode: 1448, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000207, mae: 0.003319, mean_q: 0.019458
 14436/100000: episode: 1449, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002838, mean_q: 0.020977
 14446/100000: episode: 1450, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000034, mae: 0.002258, mean_q: 0.020527
 14456/100000: episode: 1451, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000032, mae: 0.002757, mean_q: 0.018743
 14466/100000: episode: 1452, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000074, mae: 0.002956, mean_q: 0.019382
 14476/100000: episode: 1453, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002013, mean_q: 0.020340
 14486/100000: episode: 1454, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000036, mae: 0.002393, mean_q: 0.019662
 14496/100000: episode: 1455, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000011, mae: 0.002349, mean_q: 0.018747
 14506/100000: episode: 1456, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000014, mae: 0.002374, mean_q: 0.018728
 14516/100000: episode: 1457, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000032, mae: 0.002369, mean_q: 0.018902
[Info] 1-TH LEVEL FOUND: 0.01949618197977543, Considering 100/100 traces
 14526/100000: episode: 1458, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000032, mae: 0.001887, mean_q: 0.019484
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.01949618197977543
1
 14536/100000: episode: 1459, duration: 0.588s, episode steps: 10, steps per second: 17, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000071, mae: 0.002511, mean_q: 0.019123
 14546/100000: episode: 1460, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001558, mae: 0.007058, mean_q: 0.021643
 14556/100000: episode: 1461, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000038, mae: 0.002933, mean_q: 0.020328
 14566/100000: episode: 1462, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000014, mae: 0.002613, mean_q: 0.018433
 14576/100000: episode: 1463, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002445, mean_q: 0.019066
 14586/100000: episode: 1464, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000032, mae: 0.002260, mean_q: 0.018950
 14596/100000: episode: 1465, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000017, mae: 0.002269, mean_q: 0.018863
 14606/100000: episode: 1466, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000037, mae: 0.002411, mean_q: 0.019034
 14616/100000: episode: 1467, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.002354, mean_q: 0.018712
 14626/100000: episode: 1468, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000079, mae: 0.003061, mean_q: 0.018775
 14636/100000: episode: 1469, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000395, mae: 0.004971, mean_q: 0.020761
 14646/100000: episode: 1470, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000074, mae: 0.003287, mean_q: 0.020485
 14656/100000: episode: 1471, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000061, mae: 0.002867, mean_q: 0.019158
 14666/100000: episode: 1472, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000010, mae: 0.001641, mean_q: 0.018897
 14676/100000: episode: 1473, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002420, mean_q: 0.018428
 14686/100000: episode: 1474, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.002217, mean_q: 0.019396
 14696/100000: episode: 1475, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000055, mae: 0.002515, mean_q: 0.018723
 14706/100000: episode: 1476, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000010, mae: 0.001294, mean_q: 0.018987
 14716/100000: episode: 1477, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000029, mae: 0.001636, mean_q: 0.018745
 14726/100000: episode: 1478, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000032, mae: 0.002156, mean_q: 0.018558
 14736/100000: episode: 1479, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000010, mae: 0.001534, mean_q: 0.018722
 14746/100000: episode: 1480, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000028, mae: 0.001627, mean_q: 0.018556
 14756/100000: episode: 1481, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.001913, mean_q: 0.018383
 14766/100000: episode: 1482, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002192, mean_q: 0.018479
 14776/100000: episode: 1483, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000224, mae: 0.003067, mean_q: 0.018419
 14786/100000: episode: 1484, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000013, mae: 0.001843, mean_q: 0.018932
 14796/100000: episode: 1485, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000014, mae: 0.002156, mean_q: 0.017816
 14806/100000: episode: 1486, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000011, mae: 0.001537, mean_q: 0.018348
 14816/100000: episode: 1487, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000098, mae: 0.003339, mean_q: 0.018125
 14826/100000: episode: 1488, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000229, mae: 0.003508, mean_q: 0.019115
 14836/100000: episode: 1489, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001582, mae: 0.008444, mean_q: 0.021823
 14846/100000: episode: 1490, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.003378, mean_q: 0.020282
 14856/100000: episode: 1491, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000230, mae: 0.004390, mean_q: 0.017582
 14866/100000: episode: 1492, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000051, mae: 0.002139, mean_q: 0.019592
 14876/100000: episode: 1493, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002187, mean_q: 0.019304
 14886/100000: episode: 1494, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000010, mae: 0.002221, mean_q: 0.017821
 14896/100000: episode: 1495, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000032, mae: 0.002348, mean_q: 0.017871
 14906/100000: episode: 1496, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000030, mae: 0.001691, mean_q: 0.018949
 14916/100000: episode: 1497, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.001728, mae: 0.007224, mean_q: 0.020233
 14926/100000: episode: 1498, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000417, mae: 0.006554, mean_q: 0.021730
 14936/100000: episode: 1499, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000204, mae: 0.003281, mean_q: 0.019698
 14946/100000: episode: 1500, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000030, mae: 0.001759, mean_q: 0.018839
 14956/100000: episode: 1501, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000033, mae: 0.001959, mean_q: 0.018891
 14966/100000: episode: 1502, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.002141, mean_q: 0.018537
 14976/100000: episode: 1503, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000014, mae: 0.002486, mean_q: 0.017800
 14986/100000: episode: 1504, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000225, mae: 0.003164, mean_q: 0.018843
 14996/100000: episode: 1505, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000584, mae: 0.006234, mean_q: 0.020439
Step 15000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.15000.hdf5
 15006/100000: episode: 1506, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.003242, mean_q: 0.020981
 15016/100000: episode: 1507, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000294, mae: 0.004772, mean_q: 0.018722
 15026/100000: episode: 1508, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000036, mae: 0.002900, mean_q: 0.020501
 15036/100000: episode: 1509, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000014, mae: 0.001983, mean_q: 0.018893
 15046/100000: episode: 1510, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.002218, mean_q: 0.018404
 15056/100000: episode: 1511, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000030, mae: 0.001902, mean_q: 0.018634
 15066/100000: episode: 1512, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001545, mae: 0.007143, mean_q: 0.021274
 15076/100000: episode: 1513, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000038, mae: 0.003306, mean_q: 0.020305
 15086/100000: episode: 1514, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000008, mae: 0.002770, mean_q: 0.017293
 15096/100000: episode: 1515, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000391, mae: 0.004391, mean_q: 0.019637
 15106/100000: episode: 1516, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000230, mae: 0.005740, mean_q: 0.022214
 15116/100000: episode: 1517, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000227, mae: 0.003650, mean_q: 0.019308
 15126/100000: episode: 1518, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.002167, mean_q: 0.019227
 15136/100000: episode: 1519, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000052, mae: 0.001873, mean_q: 0.019543
 15146/100000: episode: 1520, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000009, mae: 0.001453, mean_q: 0.019454
 15156/100000: episode: 1521, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000036, mae: 0.003043, mean_q: 0.018225
 15166/100000: episode: 1522, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000015, mae: 0.002244, mean_q: 0.018508
 15176/100000: episode: 1523, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.002250, mean_q: 0.018519
 15186/100000: episode: 1524, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000037, mae: 0.002028, mean_q: 0.018929
 15196/100000: episode: 1525, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000009, mae: 0.001226, mean_q: 0.019089
 15206/100000: episode: 1526, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000029, mae: 0.001673, mean_q: 0.018656
 15216/100000: episode: 1527, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000200, mae: 0.002521, mean_q: 0.018924
 15226/100000: episode: 1528, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000050, mae: 0.001907, mean_q: 0.019364
 15236/100000: episode: 1529, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000017, mae: 0.003575, mean_q: 0.021496
 15246/100000: episode: 1530, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000032, mae: 0.002584, mean_q: 0.019035
 15256/100000: episode: 1531, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000419, mae: 0.004928, mean_q: 0.019686
 15266/100000: episode: 1532, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000222, mae: 0.004081, mean_q: 0.021015
 15276/100000: episode: 1533, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000010, mae: 0.001583, mean_q: 0.019455
 15286/100000: episode: 1534, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000037, mae: 0.003156, mean_q: 0.018128
 15296/100000: episode: 1535, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000059, mae: 0.002736, mean_q: 0.019126
 15306/100000: episode: 1536, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000037, mae: 0.002097, mean_q: 0.019085
 15316/100000: episode: 1537, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.002155, mean_q: 0.019184
 15326/100000: episode: 1538, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000411, mae: 0.003603, mean_q: 0.019184
 15336/100000: episode: 1539, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000205, mae: 0.004314, mean_q: 0.021162
 15346/100000: episode: 1540, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000060, mae: 0.002994, mean_q: 0.019918
 15356/100000: episode: 1541, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000078, mae: 0.002971, mean_q: 0.019010
 15366/100000: episode: 1542, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000054, mae: 0.003203, mean_q: 0.020995
 15376/100000: episode: 1543, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000053, mae: 0.002466, mean_q: 0.020023
 15386/100000: episode: 1544, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000012, mae: 0.002151, mean_q: 0.018659
 15396/100000: episode: 1545, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000246, mae: 0.003899, mean_q: 0.018944
 15406/100000: episode: 1546, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000036, mae: 0.002444, mean_q: 0.020196
 15416/100000: episode: 1547, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000037, mae: 0.002464, mean_q: 0.019128
 15426/100000: episode: 1548, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000007, mae: 0.001834, mean_q: 0.018452
 15436/100000: episode: 1549, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000059, mae: 0.002714, mean_q: 0.018887
 15446/100000: episode: 1550, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000222, mae: 0.003195, mean_q: 0.020011
 15456/100000: episode: 1551, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001537, mae: 0.005729, mean_q: 0.020353
 15466/100000: episode: 1552, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.002652, mean_q: 0.020897
 15476/100000: episode: 1553, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.003184, mean_q: 0.018235
 15486/100000: episode: 1554, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000030, mae: 0.001529, mean_q: 0.019438
 15496/100000: episode: 1555, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000011, mae: 0.001204, mean_q: 0.019568
 15506/100000: episode: 1556, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000056, mae: 0.002624, mean_q: 0.018934
 15516/100000: episode: 1557, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000199, mae: 0.002378, mean_q: 0.019258
 15526/100000: episode: 1558, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001545, mae: 0.007811, mean_q: 0.022356
[Info] 1-TH LEVEL FOUND: 0.018148571252822876, Considering 100/100 traces
 15536/100000: episode: 1559, duration: 0.719s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000035, mae: 0.002897, mean_q: 0.020536
[Info] 2-TH LEVEL FOUND: 0.018333084881305695, Considering 100/100 traces
 15546/100000: episode: 1560, duration: 0.655s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000031, mae: 0.003093, mean_q: 0.017773
[Info] 3-TH LEVEL FOUND: 0.019653093069791794, Considering 100/100 traces
 15556/100000: episode: 1561, duration: 0.727s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000011, mae: 0.001473, mean_q: 0.019096
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.019653093069791794
3
 15566/100000: episode: 1562, duration: 0.500s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000015, mae: 0.001855, mean_q: 0.019305
 15576/100000: episode: 1563, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000014, mae: 0.002132, mean_q: 0.018651
 15586/100000: episode: 1564, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000412, mae: 0.003942, mean_q: 0.019173
 15596/100000: episode: 1565, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001725, mae: 0.010009, mean_q: 0.024352
 15606/100000: episode: 1566, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000016, mae: 0.003437, mean_q: 0.022219
 15616/100000: episode: 1567, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000058, mae: 0.004278, mean_q: 0.017671
 15626/100000: episode: 1568, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000205, mae: 0.003490, mean_q: 0.020288
 15636/100000: episode: 1569, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000036, mae: 0.003010, mean_q: 0.021325
 15646/100000: episode: 1570, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002080, mean_q: 0.020468
 15656/100000: episode: 1571, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000013, mae: 0.001702, mean_q: 0.019694
 15666/100000: episode: 1572, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000062, mae: 0.002978, mean_q: 0.019662
 15676/100000: episode: 1573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001739, mae: 0.007548, mean_q: 0.021872
 15686/100000: episode: 1574, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000063, mae: 0.005114, mean_q: 0.023286
 15696/100000: episode: 1575, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000010, mae: 0.001495, mean_q: 0.020164
 15706/100000: episode: 1576, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000387, mae: 0.003948, mean_q: 0.019354
 15716/100000: episode: 1577, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000210, mae: 0.005213, mean_q: 0.023198
 15726/100000: episode: 1578, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000250, mae: 0.004661, mean_q: 0.021709
 15736/100000: episode: 1579, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000202, mae: 0.002731, mean_q: 0.020980
 15746/100000: episode: 1580, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000009, mae: 0.001633, mean_q: 0.020273
 15756/100000: episode: 1581, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000205, mae: 0.003242, mean_q: 0.020084
 15766/100000: episode: 1582, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000053, mae: 0.002176, mean_q: 0.020881
 15776/100000: episode: 1583, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000012, mae: 0.001265, mean_q: 0.020724
 15786/100000: episode: 1584, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000013, mae: 0.001775, mean_q: 0.020224
 15796/100000: episode: 1585, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.001553, mae: 0.005806, mean_q: 0.020774
 15806/100000: episode: 1586, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000016, mae: 0.003338, mean_q: 0.022884
 15816/100000: episode: 1587, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000203, mae: 0.003495, mean_q: 0.019600
 15826/100000: episode: 1588, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000052, mae: 0.002273, mean_q: 0.020374
 15836/100000: episode: 1589, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000200, mae: 0.003209, mean_q: 0.021763
 15846/100000: episode: 1590, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000012, mae: 0.001840, mean_q: 0.020133
 15856/100000: episode: 1591, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002584, mean_q: 0.019996
 15866/100000: episode: 1592, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000040, mae: 0.002536, mean_q: 0.020852
 15876/100000: episode: 1593, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000212, mae: 0.003759, mean_q: 0.019959
 15886/100000: episode: 1594, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000021, mae: 0.002113, mean_q: 0.020681
 15896/100000: episode: 1595, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000225, mae: 0.003365, mean_q: 0.020821
 15906/100000: episode: 1596, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000057, mae: 0.002620, mean_q: 0.020697
 15916/100000: episode: 1597, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.002816, mean_q: 0.020643
 15926/100000: episode: 1598, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000201, mae: 0.002689, mean_q: 0.020995
 15936/100000: episode: 1599, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.001953, mean_q: 0.020976
 15946/100000: episode: 1600, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000201, mae: 0.002500, mean_q: 0.020590
 15956/100000: episode: 1601, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000239, mae: 0.002872, mean_q: 0.020763
 15966/100000: episode: 1602, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002934, mean_q: 0.021411
 15976/100000: episode: 1603, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000037, mae: 0.002417, mean_q: 0.020291
 15986/100000: episode: 1604, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002106, mean_q: 0.020263
 15996/100000: episode: 1605, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000013, mae: 0.001531, mean_q: 0.020401
Step 16000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.16000.hdf5
 16006/100000: episode: 1606, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000056, mae: 0.002692, mean_q: 0.020254
 16016/100000: episode: 1607, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000010, mae: 0.001351, mean_q: 0.020798
 16026/100000: episode: 1608, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.001515, mae: 0.004712, mean_q: 0.020281
 16036/100000: episode: 1609, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000018, mae: 0.003586, mean_q: 0.022799
 16046/100000: episode: 1610, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000081, mae: 0.003398, mean_q: 0.020536
 16056/100000: episode: 1611, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000033, mae: 0.002490, mean_q: 0.021594
 16066/100000: episode: 1612, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000019, mae: 0.002304, mean_q: 0.020825
 16076/100000: episode: 1613, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000415, mae: 0.004768, mean_q: 0.020490
 16086/100000: episode: 1614, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000013, mae: 0.002394, mean_q: 0.022247
 16096/100000: episode: 1615, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000009, mae: 0.001766, mean_q: 0.020068
 16106/100000: episode: 1616, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000034, mae: 0.002981, mean_q: 0.019375
 16116/100000: episode: 1617, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000017, mae: 0.002243, mean_q: 0.020306
 16126/100000: episode: 1618, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000118, mae: 0.003666, mean_q: 0.020275
 16136/100000: episode: 1619, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000009, mae: 0.001061, mean_q: 0.020501
 16146/100000: episode: 1620, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000011, mae: 0.002022, mean_q: 0.019764
[Info] FALSIFICATION!
 16156/100000: episode: 1621, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000245, mae: 0.003837, mean_q: 0.020228
 16166/100000: episode: 1622, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000031, mae: 0.001879, mean_q: 0.020952
 16176/100000: episode: 1623, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000018, mae: 0.002060, mean_q: 0.019901
 16186/100000: episode: 1624, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000012, mae: 0.001498, mean_q: 0.020209
 16196/100000: episode: 1625, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001557, mae: 0.005537, mean_q: 0.020304
 16206/100000: episode: 1626, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000229, mae: 0.006393, mean_q: 0.024054
 16216/100000: episode: 1627, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.001531, mae: 0.009129, mean_q: 0.025851
 16226/100000: episode: 1628, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000060, mae: 0.003854, mean_q: 0.022795
 16236/100000: episode: 1629, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000012, mae: 0.002607, mean_q: 0.019867
 16246/100000: episode: 1630, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000071, mae: 0.002974, mean_q: 0.020247
 16256/100000: episode: 1631, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000204, mae: 0.002999, mean_q: 0.021521
 16266/100000: episode: 1632, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.002551, mean_q: 0.021275
 16276/100000: episode: 1633, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001730, mae: 0.008292, mean_q: 0.023268
 16286/100000: episode: 1634, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000017, mae: 0.003075, mean_q: 0.023268
 16296/100000: episode: 1635, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000575, mae: 0.004926, mean_q: 0.020836
 16306/100000: episode: 1636, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000038, mae: 0.004700, mean_q: 0.024850
 16316/100000: episode: 1637, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000083, mae: 0.004183, mean_q: 0.022719
 16326/100000: episode: 1638, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000017, mae: 0.002371, mean_q: 0.021072
 16336/100000: episode: 1639, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000019, mae: 0.002532, mean_q: 0.021089
 16346/100000: episode: 1640, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000062, mae: 0.003131, mean_q: 0.021119
 16356/100000: episode: 1641, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000017, mae: 0.002009, mean_q: 0.021722
 16366/100000: episode: 1642, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000041, mae: 0.002901, mean_q: 0.020754
 16376/100000: episode: 1643, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000016, mae: 0.001980, mean_q: 0.021237
 16386/100000: episode: 1644, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002433, mean_q: 0.020950
 16396/100000: episode: 1645, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000033, mae: 0.002469, mean_q: 0.020474
 16406/100000: episode: 1646, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000036, mae: 0.002273, mean_q: 0.020997
 16416/100000: episode: 1647, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000055, mae: 0.002471, mean_q: 0.021025
 16426/100000: episode: 1648, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000037, mae: 0.002330, mean_q: 0.020886
 16436/100000: episode: 1649, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000033, mae: 0.001786, mean_q: 0.021276
 16446/100000: episode: 1650, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000015, mae: 0.002263, mean_q: 0.020596
 16456/100000: episode: 1651, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000055, mae: 0.003098, mean_q: 0.020010
 16466/100000: episode: 1652, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000396, mae: 0.004332, mean_q: 0.020722
 16476/100000: episode: 1653, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002766, mean_q: 0.022406
 16486/100000: episode: 1654, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000018, mae: 0.002215, mean_q: 0.020966
 16496/100000: episode: 1655, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000229, mae: 0.004257, mean_q: 0.020099
 16506/100000: episode: 1656, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000014, mae: 0.001771, mean_q: 0.021184
 16516/100000: episode: 1657, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000034, mae: 0.001751, mean_q: 0.020886
 16526/100000: episode: 1658, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000013, mae: 0.001405, mean_q: 0.020772
 16536/100000: episode: 1659, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.001699, mae: 0.006610, mean_q: 0.022182
 16546/100000: episode: 1660, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000021, mae: 0.003693, mean_q: 0.023386
 16556/100000: episode: 1661, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000031, mae: 0.002942, mean_q: 0.019505
[Info] Complete ISplit Iteration
[Info] Levels: [0.021998644]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 16566/100000: episode: 1662, duration: 0.744s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.001538, mae: 0.006318, mean_q: 0.019818
 16576/100000: episode: 1663, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000040, mae: 0.004293, mean_q: 0.023462
 16586/100000: episode: 1664, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001721, mae: 0.010308, mean_q: 0.025921
 16596/100000: episode: 1665, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000014, mae: 0.003350, mean_q: 0.024365
 16606/100000: episode: 1666, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000009, mae: 0.003211, mean_q: 0.019247
 16616/100000: episode: 1667, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000095, mae: 0.003824, mean_q: 0.021051
 16626/100000: episode: 1668, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000033, mae: 0.002647, mean_q: 0.022828
 16636/100000: episode: 1669, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000034, mae: 0.001741, mean_q: 0.021658
 16646/100000: episode: 1670, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000223, mae: 0.003116, mean_q: 0.021668
 16656/100000: episode: 1671, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000015, mae: 0.001948, mean_q: 0.022326
 16666/100000: episode: 1672, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000052, mae: 0.002411, mean_q: 0.021263
 16676/100000: episode: 1673, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000016, mae: 0.002260, mean_q: 0.020937
 16686/100000: episode: 1674, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001559, mae: 0.006704, mean_q: 0.022055
 16696/100000: episode: 1675, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000023, mae: 0.003779, mean_q: 0.023862
 16706/100000: episode: 1676, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000063, mae: 0.003755, mean_q: 0.020885
 16716/100000: episode: 1677, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000037, mae: 0.003330, mean_q: 0.020250
 16726/100000: episode: 1678, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000039, mae: 0.003662, mean_q: 0.023354
 16736/100000: episode: 1679, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000073, mae: 0.003087, mean_q: 0.022387
 16746/100000: episode: 1680, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000053, mae: 0.002521, mean_q: 0.021297
 16756/100000: episode: 1681, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000076, mae: 0.002741, mean_q: 0.021721
 16766/100000: episode: 1682, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000033, mae: 0.001983, mean_q: 0.022064
 16776/100000: episode: 1683, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002796, mean_q: 0.020934
 16786/100000: episode: 1684, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000011, mae: 0.001917, mean_q: 0.020813
 16796/100000: episode: 1685, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000033, mae: 0.002074, mean_q: 0.021100
 16806/100000: episode: 1686, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.002475, mean_q: 0.021504
 16816/100000: episode: 1687, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.001632, mean_q: 0.021449
 16826/100000: episode: 1688, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000015, mae: 0.001878, mean_q: 0.022050
 16836/100000: episode: 1689, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002254, mean_q: 0.021173
 16846/100000: episode: 1690, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001512, mae: 0.005182, mean_q: 0.021302
 16856/100000: episode: 1691, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000063, mae: 0.004617, mean_q: 0.023732
 16866/100000: episode: 1692, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000035, mae: 0.002576, mean_q: 0.021471
 16876/100000: episode: 1693, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000011, mae: 0.002416, mean_q: 0.020114
 16886/100000: episode: 1694, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000033, mae: 0.001947, mean_q: 0.021264
 16896/100000: episode: 1695, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000249, mae: 0.006429, mean_q: 0.024753
 16906/100000: episode: 1696, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000257, mae: 0.005480, mean_q: 0.024361
 16916/100000: episode: 1697, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000207, mae: 0.003528, mean_q: 0.021591
 16926/100000: episode: 1698, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000220, mae: 0.003162, mean_q: 0.021963
 16936/100000: episode: 1699, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000205, mae: 0.003477, mean_q: 0.022785
 16946/100000: episode: 1700, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000053, mae: 0.002266, mean_q: 0.021926
 16956/100000: episode: 1701, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000012, mae: 0.001803, mean_q: 0.021475
 16966/100000: episode: 1702, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000036, mae: 0.002368, mean_q: 0.021438
 16976/100000: episode: 1703, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000055, mae: 0.002489, mean_q: 0.021821
 16986/100000: episode: 1704, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000203, mae: 0.002824, mean_q: 0.021902
 16996/100000: episode: 1705, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.002088, mean_q: 0.022035
Step 17000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.17000.hdf5
 17006/100000: episode: 1706, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000009, mae: 0.001690, mean_q: 0.021085
 17016/100000: episode: 1707, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000034, mae: 0.002388, mean_q: 0.021036
 17026/100000: episode: 1708, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.001614, mae: 0.008251, mean_q: 0.023736
 17036/100000: episode: 1709, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000041, mae: 0.004172, mean_q: 0.024295
 17046/100000: episode: 1710, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000014, mae: 0.002270, mean_q: 0.021110
 17056/100000: episode: 1711, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000040, mae: 0.002928, mean_q: 0.021156
 17066/100000: episode: 1712, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000218, mae: 0.003050, mean_q: 0.022452
 17076/100000: episode: 1713, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000060, mae: 0.003133, mean_q: 0.022160
 17086/100000: episode: 1714, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000059, mae: 0.003169, mean_q: 0.021136
 17096/100000: episode: 1715, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000015, mae: 0.001634, mean_q: 0.021943
 17106/100000: episode: 1716, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000034, mae: 0.001974, mean_q: 0.021713
 17116/100000: episode: 1717, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000039, mae: 0.003669, mean_q: 0.023817
 17126/100000: episode: 1718, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000015, mae: 0.002296, mean_q: 0.022552
 17136/100000: episode: 1719, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000240, mae: 0.003706, mean_q: 0.021795
 17146/100000: episode: 1720, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.002739, mean_q: 0.023230
 17156/100000: episode: 1721, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000038, mae: 0.002048, mean_q: 0.022363
 17166/100000: episode: 1722, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.003180, mae: 0.010142, mean_q: 0.023956
 17176/100000: episode: 1723, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000058, mae: 0.007537, mean_q: 0.028200
 17186/100000: episode: 1724, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.002275, mean_q: 0.022950
 17196/100000: episode: 1725, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000051, mae: 0.003649, mean_q: 0.020720
 17206/100000: episode: 1726, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.001533, mae: 0.006676, mean_q: 0.024412
 17216/100000: episode: 1727, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000057, mae: 0.003572, mean_q: 0.024439
 17226/100000: episode: 1728, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000032, mae: 0.002416, mean_q: 0.022089
 17236/100000: episode: 1729, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002460, mean_q: 0.021970
 17246/100000: episode: 1730, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000038, mae: 0.002728, mean_q: 0.022289
 17256/100000: episode: 1731, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000035, mae: 0.002410, mean_q: 0.022129
 17266/100000: episode: 1732, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000053, mae: 0.002535, mean_q: 0.022192
 17276/100000: episode: 1733, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000014, mae: 0.001419, mean_q: 0.022588
 17286/100000: episode: 1734, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000018, mae: 0.002565, mean_q: 0.021755
 17296/100000: episode: 1735, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000036, mae: 0.003446, mean_q: 0.020702
 17306/100000: episode: 1736, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000205, mae: 0.003356, mean_q: 0.021955
 17316/100000: episode: 1737, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002252, mean_q: 0.022797
 17326/100000: episode: 1738, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000077, mae: 0.003339, mean_q: 0.021656
 17336/100000: episode: 1739, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000039, mae: 0.002678, mean_q: 0.021779
 17346/100000: episode: 1740, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000206, mae: 0.003756, mean_q: 0.021185
 17356/100000: episode: 1741, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000041, mae: 0.002676, mean_q: 0.021807
 17366/100000: episode: 1742, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000069, mae: 0.002406, mean_q: 0.022477
 17376/100000: episode: 1743, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000052, mae: 0.002207, mean_q: 0.022178
 17386/100000: episode: 1744, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000043, mae: 0.003238, mean_q: 0.021225
 17396/100000: episode: 1745, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000028, mae: 0.001676, mean_q: 0.021409
 17406/100000: episode: 1746, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000011, mae: 0.001415, mean_q: 0.021865
 17416/100000: episode: 1747, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000396, mae: 0.004844, mean_q: 0.020878
 17426/100000: episode: 1748, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000020, mae: 0.002791, mean_q: 0.022785
 17436/100000: episode: 1749, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.001999, mean_q: 0.021736
 17446/100000: episode: 1750, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000265, mae: 0.004465, mean_q: 0.022414
 17456/100000: episode: 1751, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.001880, mae: 0.008575, mean_q: 0.024113
 17466/100000: episode: 1752, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.001529, mae: 0.008823, mean_q: 0.026333
 17476/100000: episode: 1753, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000051, mae: 0.003242, mean_q: 0.024343
 17486/100000: episode: 1754, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000055, mae: 0.002985, mean_q: 0.021851
 17496/100000: episode: 1755, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000056, mae: 0.003082, mean_q: 0.021758
 17506/100000: episode: 1756, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000404, mae: 0.004523, mean_q: 0.023442
 17516/100000: episode: 1757, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000055, mae: 0.003858, mean_q: 0.024686
 17526/100000: episode: 1758, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000208, mae: 0.003757, mean_q: 0.022258
 17536/100000: episode: 1759, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000014, mae: 0.002416, mean_q: 0.021705
 17546/100000: episode: 1760, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000018, mae: 0.003047, mean_q: 0.021378
 17556/100000: episode: 1761, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000044, mae: 0.003410, mean_q: 0.021540
[Info] 1-TH LEVEL FOUND: 0.022460216656327248, Considering 100/100 traces
 17566/100000: episode: 1762, duration: 0.710s, episode steps: 10, steps per second: 14, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000201, mae: 0.002641, mean_q: 0.022236
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022460216656327248
1
 17576/100000: episode: 1763, duration: 0.492s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000021, mae: 0.002280, mean_q: 0.022235
 17586/100000: episode: 1764, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000018, mae: 0.002720, mean_q: 0.021197
 17596/100000: episode: 1765, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000033, mae: 0.002290, mean_q: 0.021552
 17606/100000: episode: 1766, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000391, mae: 0.004254, mean_q: 0.021784
 17616/100000: episode: 1767, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000041, mae: 0.002733, mean_q: 0.022546
 17626/100000: episode: 1768, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002700, mean_q: 0.021169
 17636/100000: episode: 1769, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000010, mae: 0.002149, mean_q: 0.020908
 17646/100000: episode: 1770, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001530, mae: 0.005806, mean_q: 0.022528
 17656/100000: episode: 1771, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000306, mae: 0.006740, mean_q: 0.024291
 17666/100000: episode: 1772, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000041, mae: 0.003324, mean_q: 0.023448
 17676/100000: episode: 1773, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.002683, mean_q: 0.020959
 17686/100000: episode: 1774, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000034, mae: 0.002898, mean_q: 0.020933
 17696/100000: episode: 1775, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000209, mae: 0.003413, mean_q: 0.022055
 17706/100000: episode: 1776, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000037, mae: 0.002440, mean_q: 0.021921
 17716/100000: episode: 1777, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.002855, mean_q: 0.020887
 17726/100000: episode: 1778, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000038, mae: 0.002615, mean_q: 0.021389
 17736/100000: episode: 1779, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.001717, mae: 0.007004, mean_q: 0.022995
 17746/100000: episode: 1780, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000043, mae: 0.004542, mean_q: 0.024783
 17756/100000: episode: 1781, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000015, mae: 0.002620, mean_q: 0.021046
 17766/100000: episode: 1782, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000397, mae: 0.005507, mean_q: 0.020259
 17776/100000: episode: 1783, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000221, mae: 0.004011, mean_q: 0.023337
 17786/100000: episode: 1784, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002734, mean_q: 0.023334
 17796/100000: episode: 1785, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000223, mae: 0.003674, mean_q: 0.021313
 17806/100000: episode: 1786, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000035, mae: 0.002199, mean_q: 0.022400
 17816/100000: episode: 1787, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001529, mae: 0.006066, mean_q: 0.023578
 17826/100000: episode: 1788, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000045, mae: 0.003647, mean_q: 0.023237
 17836/100000: episode: 1789, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000221, mae: 0.004064, mean_q: 0.020528
 17846/100000: episode: 1790, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000036, mae: 0.002346, mean_q: 0.022523
 17856/100000: episode: 1791, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001516, mae: 0.005320, mean_q: 0.021997
 17866/100000: episode: 1792, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000455, mae: 0.007518, mean_q: 0.024691
 17876/100000: episode: 1793, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000224, mae: 0.005999, mean_q: 0.025841
 17886/100000: episode: 1794, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000040, mae: 0.003547, mean_q: 0.021430
 17896/100000: episode: 1795, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000033, mae: 0.003399, mean_q: 0.020587
 17906/100000: episode: 1796, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000224, mae: 0.003567, mean_q: 0.021906
 17916/100000: episode: 1797, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000018, mae: 0.002404, mean_q: 0.023117
 17926/100000: episode: 1798, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000281, mae: 0.004430, mean_q: 0.021844
 17936/100000: episode: 1799, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000224, mae: 0.003641, mean_q: 0.022712
 17946/100000: episode: 1800, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001508, mae: 0.005676, mean_q: 0.023699
 17956/100000: episode: 1801, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000037, mae: 0.003035, mean_q: 0.023847
 17966/100000: episode: 1802, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000057, mae: 0.003154, mean_q: 0.021584
 17976/100000: episode: 1803, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000035, mae: 0.002160, mean_q: 0.022381
 17986/100000: episode: 1804, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000014, mae: 0.001579, mean_q: 0.022433
 17996/100000: episode: 1805, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000051, mae: 0.002473, mean_q: 0.021931
Step 18000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.18000.hdf5
 18006/100000: episode: 1806, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.002833, mean_q: 0.021967
 18016/100000: episode: 1807, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000034, mae: 0.002594, mean_q: 0.021440
 18026/100000: episode: 1808, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000013, mae: 0.002024, mean_q: 0.021523
 18036/100000: episode: 1809, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000013, mae: 0.002187, mean_q: 0.021350
 18046/100000: episode: 1810, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000016, mae: 0.001896, mean_q: 0.021927
 18056/100000: episode: 1811, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000240, mae: 0.003313, mean_q: 0.022147
 18066/100000: episode: 1812, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000021, mae: 0.002568, mean_q: 0.022747
 18076/100000: episode: 1813, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000224, mae: 0.003709, mean_q: 0.021193
 18086/100000: episode: 1814, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000020, mae: 0.002187, mean_q: 0.022339
 18096/100000: episode: 1815, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000409, mae: 0.004351, mean_q: 0.021661
 18106/100000: episode: 1816, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000202, mae: 0.003281, mean_q: 0.022910
 18116/100000: episode: 1817, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.001508, mae: 0.004925, mean_q: 0.022746
 18126/100000: episode: 1818, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000033, mae: 0.003089, mean_q: 0.023915
 18136/100000: episode: 1819, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000031, mae: 0.002205, mean_q: 0.022278
 18146/100000: episode: 1820, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000784, mae: 0.007164, mean_q: 0.022787
 18156/100000: episode: 1821, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000057, mae: 0.004415, mean_q: 0.024941
 18166/100000: episode: 1822, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000018, mae: 0.002649, mean_q: 0.021560
 18176/100000: episode: 1823, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000014, mae: 0.002581, mean_q: 0.021085
 18186/100000: episode: 1824, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000019, mae: 0.002264, mean_q: 0.021919
 18196/100000: episode: 1825, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000222, mae: 0.003185, mean_q: 0.021850
 18206/100000: episode: 1826, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001697, mae: 0.007249, mean_q: 0.023648
 18216/100000: episode: 1827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000021, mae: 0.004623, mean_q: 0.026102
 18226/100000: episode: 1828, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000204, mae: 0.003555, mean_q: 0.022403
 18236/100000: episode: 1829, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000053, mae: 0.003934, mean_q: 0.024874
 18246/100000: episode: 1830, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000428, mae: 0.007000, mean_q: 0.025591
 18256/100000: episode: 1831, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000205, mae: 0.003924, mean_q: 0.024354
 18266/100000: episode: 1832, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000013, mae: 0.002502, mean_q: 0.021978
 18276/100000: episode: 1833, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000017, mae: 0.003008, mean_q: 0.021502
 18286/100000: episode: 1834, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000016, mae: 0.002313, mean_q: 0.022139
 18296/100000: episode: 1835, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002858, mean_q: 0.023586
 18306/100000: episode: 1836, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000033, mae: 0.002648, mean_q: 0.024154
 18316/100000: episode: 1837, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000016, mae: 0.002073, mean_q: 0.022521
 18326/100000: episode: 1838, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000058, mae: 0.003414, mean_q: 0.021736
 18336/100000: episode: 1839, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.002691, mean_q: 0.023003
 18346/100000: episode: 1840, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000205, mae: 0.003237, mean_q: 0.022662
 18356/100000: episode: 1841, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002012, mean_q: 0.022997
 18366/100000: episode: 1842, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000022, mae: 0.002502, mean_q: 0.023433
 18376/100000: episode: 1843, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000225, mae: 0.003556, mean_q: 0.022534
 18386/100000: episode: 1844, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000017, mae: 0.002170, mean_q: 0.022385
 18396/100000: episode: 1845, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000030, mae: 0.002259, mean_q: 0.021934
 18406/100000: episode: 1846, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000068, mae: 0.002329, mean_q: 0.022252
 18416/100000: episode: 1847, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000019, mae: 0.002220, mean_q: 0.023088
 18426/100000: episode: 1848, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000244, mae: 0.004546, mean_q: 0.021335
 18436/100000: episode: 1849, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000012, mae: 0.001716, mean_q: 0.022061
 18446/100000: episode: 1850, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000037, mae: 0.002223, mean_q: 0.022416
 18456/100000: episode: 1851, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000016, mae: 0.002207, mean_q: 0.021714
 18466/100000: episode: 1852, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000010, mae: 0.001995, mean_q: 0.021545
 18476/100000: episode: 1853, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000012, mae: 0.002261, mean_q: 0.021236
 18486/100000: episode: 1854, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000030, mae: 0.002555, mean_q: 0.020863
 18496/100000: episode: 1855, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000015, mae: 0.001963, mean_q: 0.021616
 18506/100000: episode: 1856, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002415, mean_q: 0.021345
 18516/100000: episode: 1857, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000055, mae: 0.002564, mean_q: 0.021645
 18526/100000: episode: 1858, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000014, mae: 0.002026, mean_q: 0.021335
 18536/100000: episode: 1859, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000225, mae: 0.003552, mean_q: 0.021434
 18546/100000: episode: 1860, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000016, mae: 0.001972, mean_q: 0.021753
 18556/100000: episode: 1861, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.001556, mae: 0.006312, mean_q: 0.021265
 18566/100000: episode: 1862, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.003751, mean_q: 0.024290
[Info] 1-TH LEVEL FOUND: 0.01993832364678383, Considering 100/100 traces
 18576/100000: episode: 1863, duration: 0.674s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.002636, mean_q: 0.021725
[Info] 2-TH LEVEL FOUND: 0.02054104581475258, Considering 100/100 traces
 18586/100000: episode: 1864, duration: 0.706s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000009, mae: 0.002616, mean_q: 0.019939
[Info] 3-TH LEVEL FOUND: 0.025750285014510155, Considering 100/100 traces
 18596/100000: episode: 1865, duration: 0.697s, episode steps: 10, steps per second: 14, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.001700, mae: 0.006874, mean_q: 0.022379
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025750285014510155
3
 18606/100000: episode: 1866, duration: 0.569s, episode steps: 10, steps per second: 18, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000023, mae: 0.004521, mean_q: 0.025108
 18616/100000: episode: 1867, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000017, mae: 0.002827, mean_q: 0.021228
 18626/100000: episode: 1868, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000056, mae: 0.004054, mean_q: 0.019617
 18636/100000: episode: 1869, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000034, mae: 0.002214, mean_q: 0.021755
 18646/100000: episode: 1870, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000035, mae: 0.002246, mean_q: 0.021401
 18656/100000: episode: 1871, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000250, mae: 0.004212, mean_q: 0.021378
 18666/100000: episode: 1872, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000204, mae: 0.003183, mean_q: 0.022259
 18676/100000: episode: 1873, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000082, mae: 0.003494, mean_q: 0.021661
 18686/100000: episode: 1874, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000021, mae: 0.002522, mean_q: 0.021226
 18696/100000: episode: 1875, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000015, mae: 0.002216, mean_q: 0.020840
 18706/100000: episode: 1876, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000036, mae: 0.002164, mean_q: 0.021359
 18716/100000: episode: 1877, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000077, mae: 0.002846, mean_q: 0.021350
 18726/100000: episode: 1878, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000015, mae: 0.001891, mean_q: 0.021381
 18736/100000: episode: 1879, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000031, mae: 0.002455, mean_q: 0.020493
 18746/100000: episode: 1880, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.001723, mae: 0.007016, mean_q: 0.021560
 18756/100000: episode: 1881, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000020, mae: 0.004169, mean_q: 0.024389
 18766/100000: episode: 1882, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000036, mae: 0.002759, mean_q: 0.020835
 18776/100000: episode: 1883, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000017, mae: 0.002883, mean_q: 0.020078
 18786/100000: episode: 1884, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000243, mae: 0.003764, mean_q: 0.021762
 18796/100000: episode: 1885, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000259, mae: 0.004662, mean_q: 0.022943
 18806/100000: episode: 1886, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000060, mae: 0.003182, mean_q: 0.022183
 18816/100000: episode: 1887, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000059, mae: 0.003361, mean_q: 0.020598
 18826/100000: episode: 1888, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000057, mae: 0.002601, mean_q: 0.021382
 18836/100000: episode: 1889, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000013, mae: 0.001823, mean_q: 0.021304
 18846/100000: episode: 1890, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000012, mae: 0.002681, mean_q: 0.019991
 18856/100000: episode: 1891, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002718, mean_q: 0.020265
 18866/100000: episode: 1892, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000011, mae: 0.001476, mean_q: 0.020865
 18876/100000: episode: 1893, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000018, mae: 0.002442, mean_q: 0.020496
 18886/100000: episode: 1894, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000017, mae: 0.002612, mean_q: 0.020088
 18896/100000: episode: 1895, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000251, mae: 0.004550, mean_q: 0.020257
 18906/100000: episode: 1896, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000219, mae: 0.003715, mean_q: 0.022375
 18916/100000: episode: 1897, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000209, mae: 0.004522, mean_q: 0.022883
 18926/100000: episode: 1898, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000260, mae: 0.003979, mean_q: 0.021880
 18936/100000: episode: 1899, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000041, mae: 0.002849, mean_q: 0.021933
 18946/100000: episode: 1900, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000030, mae: 0.002124, mean_q: 0.020611
 18956/100000: episode: 1901, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000252, mae: 0.004440, mean_q: 0.020934
 18966/100000: episode: 1902, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000008, mae: 0.001520, mean_q: 0.022040
 18976/100000: episode: 1903, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000073, mae: 0.002512, mean_q: 0.021032
 18986/100000: episode: 1904, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000012, mae: 0.001411, mean_q: 0.021124
 18996/100000: episode: 1905, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000035, mae: 0.002164, mean_q: 0.020752
Step 19000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.19000.hdf5
 19006/100000: episode: 1906, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002344, mean_q: 0.021583
 19016/100000: episode: 1907, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.002585, mean_q: 0.021152
 19026/100000: episode: 1908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000076, mae: 0.002999, mean_q: 0.020761
 19036/100000: episode: 1909, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002161, mean_q: 0.021489
 19046/100000: episode: 1910, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001517, mae: 0.005647, mean_q: 0.021867
 19056/100000: episode: 1911, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000037, mae: 0.002889, mean_q: 0.022258
 19066/100000: episode: 1912, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000036, mae: 0.002698, mean_q: 0.020551
 19076/100000: episode: 1913, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000221, mae: 0.003161, mean_q: 0.021095
 19086/100000: episode: 1914, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000204, mae: 0.003630, mean_q: 0.022367
 19096/100000: episode: 1915, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000014, mae: 0.001935, mean_q: 0.021221
 19106/100000: episode: 1916, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000014, mae: 0.002525, mean_q: 0.020062
 19116/100000: episode: 1917, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000225, mae: 0.003533, mean_q: 0.021187
 19126/100000: episode: 1918, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000077, mae: 0.003207, mean_q: 0.021837
 19136/100000: episode: 1919, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000225, mae: 0.004091, mean_q: 0.022391
 19146/100000: episode: 1920, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000016, mae: 0.002271, mean_q: 0.022148
 19156/100000: episode: 1921, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000038, mae: 0.003316, mean_q: 0.020020
 19166/100000: episode: 1922, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000056, mae: 0.002788, mean_q: 0.020691
 19176/100000: episode: 1923, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000202, mae: 0.002546, mean_q: 0.021369
 19186/100000: episode: 1924, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.001530, mae: 0.005280, mean_q: 0.021753
 19196/100000: episode: 1925, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.003272, mean_q: 0.023217
 19206/100000: episode: 1926, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.002642, mean_q: 0.021329
 19216/100000: episode: 1927, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000219, mae: 0.003112, mean_q: 0.020769
 19226/100000: episode: 1928, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000037, mae: 0.002266, mean_q: 0.021544
 19236/100000: episode: 1929, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000262, mae: 0.003842, mean_q: 0.021210
 19246/100000: episode: 1930, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000204, mae: 0.003285, mean_q: 0.021998
 19256/100000: episode: 1931, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000281, mae: 0.003940, mean_q: 0.021604
 19266/100000: episode: 1932, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000015, mae: 0.002096, mean_q: 0.022158
 19276/100000: episode: 1933, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000199, mae: 0.003161, mean_q: 0.020346
 19286/100000: episode: 1934, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000198, mae: 0.002512, mean_q: 0.021284
 19296/100000: episode: 1935, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000289, mae: 0.005631, mean_q: 0.022920
 19306/100000: episode: 1936, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000017, mae: 0.002412, mean_q: 0.022240
 19316/100000: episode: 1937, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000053, mae: 0.002731, mean_q: 0.020925
 19326/100000: episode: 1938, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000058, mae: 0.002697, mean_q: 0.021276
 19336/100000: episode: 1939, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000071, mae: 0.002606, mean_q: 0.021909
 19346/100000: episode: 1940, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.001891, mean_q: 0.021532
 19356/100000: episode: 1941, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000019, mae: 0.002143, mean_q: 0.021036
 19366/100000: episode: 1942, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000055, mae: 0.002248, mean_q: 0.021496
 19376/100000: episode: 1943, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000228, mae: 0.003620, mean_q: 0.021618
 19386/100000: episode: 1944, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000015, mae: 0.001764, mean_q: 0.021427
 19396/100000: episode: 1945, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000030, mae: 0.002087, mean_q: 0.020666
 19406/100000: episode: 1946, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000052, mae: 0.002102, mean_q: 0.021363
 19416/100000: episode: 1947, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000032, mae: 0.001746, mean_q: 0.021300
 19426/100000: episode: 1948, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000243, mae: 0.003631, mean_q: 0.021074
 19436/100000: episode: 1949, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000427, mae: 0.005667, mean_q: 0.023161
 19446/100000: episode: 1950, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000080, mae: 0.004340, mean_q: 0.023280
 19456/100000: episode: 1951, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000057, mae: 0.002600, mean_q: 0.021887
 19466/100000: episode: 1952, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000057, mae: 0.003078, mean_q: 0.020854
 19476/100000: episode: 1953, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000031, mae: 0.001823, mean_q: 0.021202
 19486/100000: episode: 1954, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000038, mae: 0.002304, mean_q: 0.021352
 19496/100000: episode: 1955, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000035, mae: 0.002034, mean_q: 0.021439
 19506/100000: episode: 1956, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000224, mae: 0.003411, mean_q: 0.021050
 19516/100000: episode: 1957, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000076, mae: 0.003409, mean_q: 0.022520
 19526/100000: episode: 1958, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000240, mae: 0.003549, mean_q: 0.022166
 19536/100000: episode: 1959, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000031, mae: 0.001776, mean_q: 0.021729
 19546/100000: episode: 1960, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000240, mae: 0.003436, mean_q: 0.021125
 19556/100000: episode: 1961, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.004284, mean_q: 0.024504
 19566/100000: episode: 1962, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000041, mae: 0.003207, mean_q: 0.022369
 19576/100000: episode: 1963, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.003005, mean_q: 0.020152
 19586/100000: episode: 1964, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000223, mae: 0.003615, mean_q: 0.022025
 19596/100000: episode: 1965, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000052, mae: 0.002557, mean_q: 0.022490
[Info] 1-TH LEVEL FOUND: 0.022683288902044296, Considering 100/100 traces
 19606/100000: episode: 1966, duration: 0.717s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000386, mae: 0.003734, mean_q: 0.021226
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022683288902044296
1
 19616/100000: episode: 1967, duration: 0.491s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000020, mae: 0.002726, mean_q: 0.022846
 19626/100000: episode: 1968, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000030, mae: 0.002279, mean_q: 0.020961
 19636/100000: episode: 1969, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000011, mae: 0.002205, mean_q: 0.020577
 19646/100000: episode: 1970, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000116, mae: 0.003684, mean_q: 0.021609
 19656/100000: episode: 1971, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002959, mean_q: 0.022990
 19666/100000: episode: 1972, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000202, mae: 0.003074, mean_q: 0.021263
 19676/100000: episode: 1973, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000030, mae: 0.001882, mean_q: 0.021097
 19686/100000: episode: 1974, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000031, mae: 0.001858, mean_q: 0.021234
 19696/100000: episode: 1975, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002257, mean_q: 0.021762
 19706/100000: episode: 1976, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000073, mae: 0.002582, mean_q: 0.021368
 19716/100000: episode: 1977, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000039, mae: 0.002349, mean_q: 0.021341
 19726/100000: episode: 1978, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000204, mae: 0.003441, mean_q: 0.020610
 19736/100000: episode: 1979, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000009, mae: 0.001352, mean_q: 0.021032
 19746/100000: episode: 1980, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000222, mae: 0.003100, mean_q: 0.021517
 19756/100000: episode: 1981, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000036, mae: 0.002509, mean_q: 0.022054
 19766/100000: episode: 1982, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000047, mae: 0.003416, mean_q: 0.020703
 19776/100000: episode: 1983, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000076, mae: 0.003005, mean_q: 0.020936
 19786/100000: episode: 1984, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000073, mae: 0.002894, mean_q: 0.022048
 19796/100000: episode: 1985, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000074, mae: 0.002563, mean_q: 0.021612
 19806/100000: episode: 1986, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000267, mae: 0.004033, mean_q: 0.021541
 19816/100000: episode: 1987, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000028, mae: 0.001842, mean_q: 0.022308
 19826/100000: episode: 1988, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000071, mae: 0.002350, mean_q: 0.021445
 19836/100000: episode: 1989, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000052, mae: 0.001921, mean_q: 0.021309
 19846/100000: episode: 1990, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002420, mean_q: 0.022257
 19856/100000: episode: 1991, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.002363, mean_q: 0.021091
 19866/100000: episode: 1992, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000028, mae: 0.002162, mean_q: 0.020418
 19876/100000: episode: 1993, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000223, mae: 0.003316, mean_q: 0.021515
 19886/100000: episode: 1994, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000057, mae: 0.003198, mean_q: 0.022517
 19896/100000: episode: 1995, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000011, mae: 0.001580, mean_q: 0.021265
 19906/100000: episode: 1996, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000016, mae: 0.002585, mean_q: 0.020209
 19916/100000: episode: 1997, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.002175, mean_q: 0.020647
 19926/100000: episode: 1998, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000008, mae: 0.001876, mean_q: 0.020174
 19936/100000: episode: 1999, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000011, mae: 0.001923, mean_q: 0.020283
 19946/100000: episode: 2000, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001901, mae: 0.007396, mean_q: 0.021704
 19956/100000: episode: 2001, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000215, mae: 0.007442, mean_q: 0.026386
 19966/100000: episode: 2002, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000204, mae: 0.003636, mean_q: 0.022939
 19976/100000: episode: 2003, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000018, mae: 0.002781, mean_q: 0.020531
 19986/100000: episode: 2004, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000059, mae: 0.003294, mean_q: 0.020788
 19996/100000: episode: 2005, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000015, mae: 0.001788, mean_q: 0.021171
Step 20000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.20000.hdf5
 20006/100000: episode: 2006, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000043, mae: 0.002683, mean_q: 0.021227
 20016/100000: episode: 2007, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000039, mae: 0.002707, mean_q: 0.021996
 20026/100000: episode: 2008, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001508, mae: 0.004433, mean_q: 0.021420
 20036/100000: episode: 2009, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000230, mae: 0.005710, mean_q: 0.024136
 20046/100000: episode: 2010, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.002353, mean_q: 0.022502
 20056/100000: episode: 2011, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000060, mae: 0.004033, mean_q: 0.020091
 20066/100000: episode: 2012, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000018, mae: 0.002068, mean_q: 0.021305
 20076/100000: episode: 2013, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000032, mae: 0.001892, mean_q: 0.021447
 20086/100000: episode: 2014, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000032, mae: 0.002060, mean_q: 0.020992
 20096/100000: episode: 2015, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000242, mae: 0.003565, mean_q: 0.021154
 20106/100000: episode: 2016, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000054, mae: 0.002537, mean_q: 0.021939
 20116/100000: episode: 2017, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000018, mae: 0.002323, mean_q: 0.021109
 20126/100000: episode: 2018, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000033, mae: 0.002625, mean_q: 0.020373
 20136/100000: episode: 2019, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.002362, mean_q: 0.020869
 20146/100000: episode: 2020, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002735, mean_q: 0.020717
 20156/100000: episode: 2021, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000011, mae: 0.001387, mean_q: 0.021059
 20166/100000: episode: 2022, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000038, mae: 0.002218, mean_q: 0.021005
 20176/100000: episode: 2023, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000208, mae: 0.003027, mean_q: 0.021007
 20186/100000: episode: 2024, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000199, mae: 0.003251, mean_q: 0.022379
 20196/100000: episode: 2025, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000038, mae: 0.003094, mean_q: 0.022426
 20206/100000: episode: 2026, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000257, mae: 0.003396, mean_q: 0.021401
 20216/100000: episode: 2027, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001514, mae: 0.006862, mean_q: 0.023599
 20226/100000: episode: 2028, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000061, mae: 0.003682, mean_q: 0.022320
 20236/100000: episode: 2029, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000246, mae: 0.004485, mean_q: 0.020183
 20246/100000: episode: 2030, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000055, mae: 0.003106, mean_q: 0.022564
 20256/100000: episode: 2031, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002297, mean_q: 0.021583
 20266/100000: episode: 2032, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000093, mae: 0.003126, mean_q: 0.021083
 20276/100000: episode: 2033, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002249, mean_q: 0.022053
 20286/100000: episode: 2034, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000013, mae: 0.001990, mean_q: 0.020896
 20296/100000: episode: 2035, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000053, mae: 0.002622, mean_q: 0.020755
 20306/100000: episode: 2036, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000302, mae: 0.004658, mean_q: 0.021826
 20316/100000: episode: 2037, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000281, mae: 0.006543, mean_q: 0.024632
 20326/100000: episode: 2038, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000041, mae: 0.003589, mean_q: 0.022909
 20336/100000: episode: 2039, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000054, mae: 0.003707, mean_q: 0.019817
 20346/100000: episode: 2040, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000052, mae: 0.002347, mean_q: 0.021742
 20356/100000: episode: 2041, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002224, mean_q: 0.022040
 20366/100000: episode: 2042, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000016, mae: 0.002845, mean_q: 0.020177
 20376/100000: episode: 2043, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000428, mae: 0.005294, mean_q: 0.022347
 20386/100000: episode: 2044, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000040, mae: 0.003788, mean_q: 0.023676
 20396/100000: episode: 2045, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000247, mae: 0.003807, mean_q: 0.021659
 20406/100000: episode: 2046, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000260, mae: 0.004660, mean_q: 0.023224
 20416/100000: episode: 2047, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000036, mae: 0.002718, mean_q: 0.022461
 20426/100000: episode: 2048, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000037, mae: 0.003524, mean_q: 0.020120
 20436/100000: episode: 2049, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002432, mean_q: 0.020914
 20446/100000: episode: 2050, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000052, mae: 0.002064, mean_q: 0.021695
 20456/100000: episode: 2051, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.001969, mean_q: 0.021290
 20466/100000: episode: 2052, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000012, mae: 0.002380, mean_q: 0.020344
 20476/100000: episode: 2053, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000021, mae: 0.002999, mean_q: 0.020461
 20486/100000: episode: 2054, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000009, mae: 0.002073, mean_q: 0.020284
 20496/100000: episode: 2055, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000017, mae: 0.002237, mean_q: 0.020575
 20506/100000: episode: 2056, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000032, mae: 0.001856, mean_q: 0.020826
 20516/100000: episode: 2057, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000061, mae: 0.003045, mean_q: 0.021611
 20526/100000: episode: 2058, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000078, mae: 0.003005, mean_q: 0.020903
 20536/100000: episode: 2059, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000033, mae: 0.001775, mean_q: 0.021491
 20546/100000: episode: 2060, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000058, mae: 0.002934, mean_q: 0.021615
 20556/100000: episode: 2061, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000030, mae: 0.002660, mean_q: 0.019778
 20566/100000: episode: 2062, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000098, mae: 0.003603, mean_q: 0.020785
 20576/100000: episode: 2063, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000263, mae: 0.004699, mean_q: 0.022306
 20586/100000: episode: 2064, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000404, mae: 0.005068, mean_q: 0.022874
 20596/100000: episode: 2065, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000074, mae: 0.003664, mean_q: 0.022738
 20606/100000: episode: 2066, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000053, mae: 0.002966, mean_q: 0.020341
[Info] 1-TH LEVEL FOUND: 0.022849466651678085, Considering 100/100 traces
 20616/100000: episode: 2067, duration: 0.720s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000072, mae: 0.002676, mean_q: 0.020998
[Info] 2-TH LEVEL FOUND: 0.02457771636545658, Considering 100/100 traces
 20626/100000: episode: 2068, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000071, mae: 0.006360, mean_q: 0.025233
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02457771636545658
2
 20636/100000: episode: 2069, duration: 0.500s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000022, mae: 0.003133, mean_q: 0.021768
 20646/100000: episode: 2070, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000053, mae: 0.003419, mean_q: 0.019812
 20656/100000: episode: 2071, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000030, mae: 0.001663, mean_q: 0.021794
 20666/100000: episode: 2072, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002057, mean_q: 0.021404
 20676/100000: episode: 2073, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000056, mae: 0.002534, mean_q: 0.021117
 20686/100000: episode: 2074, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000054, mae: 0.002269, mean_q: 0.021442
 20696/100000: episode: 2075, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000202, mae: 0.002834, mean_q: 0.021126
 20706/100000: episode: 2076, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.001984, mean_q: 0.021380
 20716/100000: episode: 2077, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000081, mae: 0.003288, mean_q: 0.021012
 20726/100000: episode: 2078, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000223, mae: 0.003394, mean_q: 0.021876
 20736/100000: episode: 2079, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000054, mae: 0.002551, mean_q: 0.022166
 20746/100000: episode: 2080, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.002611, mean_q: 0.021781
 20756/100000: episode: 2081, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000059, mae: 0.003085, mean_q: 0.020808
 20766/100000: episode: 2082, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000069, mae: 0.002307, mean_q: 0.021440
 20776/100000: episode: 2083, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000239, mae: 0.003733, mean_q: 0.022168
 20786/100000: episode: 2084, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000051, mae: 0.002191, mean_q: 0.021421
 20796/100000: episode: 2085, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000013, mae: 0.002131, mean_q: 0.020657
 20806/100000: episode: 2086, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000075, mae: 0.003027, mean_q: 0.020713
 20816/100000: episode: 2087, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000033, mae: 0.002397, mean_q: 0.022254
 20826/100000: episode: 2088, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000264, mae: 0.003947, mean_q: 0.021088
 20836/100000: episode: 2089, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000012, mae: 0.001795, mean_q: 0.022115
 20846/100000: episode: 2090, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000073, mae: 0.002828, mean_q: 0.020909
 20856/100000: episode: 2091, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000036, mae: 0.002321, mean_q: 0.021030
 20866/100000: episode: 2092, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000015, mae: 0.002224, mean_q: 0.020577
 20876/100000: episode: 2093, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002413, mean_q: 0.020621
 20886/100000: episode: 2094, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000035, mae: 0.002043, mean_q: 0.021078
 20896/100000: episode: 2095, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000037, mae: 0.002422, mean_q: 0.020756
 20906/100000: episode: 2096, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000285, mae: 0.004777, mean_q: 0.021650
 20916/100000: episode: 2097, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000056, mae: 0.003442, mean_q: 0.022436
 20926/100000: episode: 2098, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000058, mae: 0.002970, mean_q: 0.020677
 20936/100000: episode: 2099, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000037, mae: 0.002256, mean_q: 0.020893
 20946/100000: episode: 2100, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000031, mae: 0.001641, mean_q: 0.021062
 20956/100000: episode: 2101, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000056, mae: 0.002498, mean_q: 0.020973
 20966/100000: episode: 2102, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000261, mae: 0.003989, mean_q: 0.021575
 20976/100000: episode: 2103, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000052, mae: 0.002870, mean_q: 0.022263
 20986/100000: episode: 2104, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000009, mae: 0.002096, mean_q: 0.020058
 20996/100000: episode: 2105, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.003272, mean_q: 0.019913
Step 21000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.21000.hdf5
 21006/100000: episode: 2106, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000100, mae: 0.003691, mean_q: 0.021700
 21016/100000: episode: 2107, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000243, mae: 0.004324, mean_q: 0.022399
 21026/100000: episode: 2108, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000211, mae: 0.004498, mean_q: 0.022666
 21036/100000: episode: 2109, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000114, mae: 0.003527, mean_q: 0.020867
 21046/100000: episode: 2110, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000007, mae: 0.000862, mean_q: 0.021424
 21056/100000: episode: 2111, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000029, mae: 0.001875, mean_q: 0.020806
 21066/100000: episode: 2112, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000200, mae: 0.002757, mean_q: 0.021118
 21076/100000: episode: 2113, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000071, mae: 0.002497, mean_q: 0.021716
 21086/100000: episode: 2114, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000058, mae: 0.002481, mean_q: 0.021159
 21096/100000: episode: 2115, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000035, mae: 0.002199, mean_q: 0.021669
 21106/100000: episode: 2116, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000034, mae: 0.002635, mean_q: 0.020534
 21116/100000: episode: 2117, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000056, mae: 0.002876, mean_q: 0.020283
 21126/100000: episode: 2118, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000015, mae: 0.001933, mean_q: 0.021665
 21136/100000: episode: 2119, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000038, mae: 0.002609, mean_q: 0.020385
 21146/100000: episode: 2120, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000240, mae: 0.003234, mean_q: 0.021333
 21156/100000: episode: 2121, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000037, mae: 0.002770, mean_q: 0.022059
 21166/100000: episode: 2122, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002183, mean_q: 0.020699
 21176/100000: episode: 2123, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000010, mae: 0.001883, mean_q: 0.020314
 21186/100000: episode: 2124, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000078, mae: 0.003391, mean_q: 0.020215
 21196/100000: episode: 2125, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000031, mae: 0.002172, mean_q: 0.021734
 21206/100000: episode: 2126, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000017, mae: 0.002357, mean_q: 0.020338
 21216/100000: episode: 2127, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000039, mae: 0.003303, mean_q: 0.019399
 21226/100000: episode: 2128, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000033, mae: 0.002142, mean_q: 0.020373
 21236/100000: episode: 2129, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000222, mae: 0.003476, mean_q: 0.019989
 21246/100000: episode: 2130, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000051, mae: 0.002500, mean_q: 0.021366
 21256/100000: episode: 2131, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000221, mae: 0.003114, mean_q: 0.020606
 21266/100000: episode: 2132, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000033, mae: 0.002132, mean_q: 0.021044
 21276/100000: episode: 2133, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000077, mae: 0.002970, mean_q: 0.020607
 21286/100000: episode: 2134, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000228, mae: 0.003517, mean_q: 0.020496
 21296/100000: episode: 2135, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002187, mean_q: 0.021233
 21306/100000: episode: 2136, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000051, mae: 0.001899, mean_q: 0.020805
 21316/100000: episode: 2137, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000013, mae: 0.002176, mean_q: 0.019851
 21326/100000: episode: 2138, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000013, mae: 0.002246, mean_q: 0.019860
 21336/100000: episode: 2139, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.003010, mean_q: 0.019399
 21346/100000: episode: 2140, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000013, mae: 0.001530, mean_q: 0.020736
 21356/100000: episode: 2141, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000040, mae: 0.002891, mean_q: 0.019646
 21366/100000: episode: 2142, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000033, mae: 0.001980, mean_q: 0.020065
 21376/100000: episode: 2143, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.002296, mean_q: 0.019651
 21386/100000: episode: 2144, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000409, mae: 0.004343, mean_q: 0.020514
 21396/100000: episode: 2145, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000054, mae: 0.004304, mean_q: 0.023151
 21406/100000: episode: 2146, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000092, mae: 0.002575, mean_q: 0.020767
 21416/100000: episode: 2147, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000074, mae: 0.002414, mean_q: 0.020557
 21426/100000: episode: 2148, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000033, mae: 0.002167, mean_q: 0.021039
 21436/100000: episode: 2149, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.002897, mean_q: 0.019639
 21446/100000: episode: 2150, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000030, mae: 0.001858, mean_q: 0.020999
 21456/100000: episode: 2151, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000017, mae: 0.002177, mean_q: 0.020005
 21466/100000: episode: 2152, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000012, mae: 0.002015, mean_q: 0.019742
 21476/100000: episode: 2153, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000056, mae: 0.002695, mean_q: 0.019891
 21486/100000: episode: 2154, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000012, mae: 0.001696, mean_q: 0.020023
 21496/100000: episode: 2155, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000220, mae: 0.003187, mean_q: 0.020310
 21506/100000: episode: 2156, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000097, mae: 0.003893, mean_q: 0.021474
 21516/100000: episode: 2157, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000246, mae: 0.003907, mean_q: 0.021209
 21526/100000: episode: 2158, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.002748, mean_q: 0.021208
 21536/100000: episode: 2159, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000073, mae: 0.003285, mean_q: 0.019423
 21546/100000: episode: 2160, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000092, mae: 0.002751, mean_q: 0.020729
 21556/100000: episode: 2161, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000069, mae: 0.003163, mean_q: 0.021986
 21566/100000: episode: 2162, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.003245, mean_q: 0.020744
 21576/100000: episode: 2163, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000051, mae: 0.002581, mean_q: 0.019551
 21586/100000: episode: 2164, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000059, mae: 0.003516, mean_q: 0.021748
 21596/100000: episode: 2165, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000263, mae: 0.003863, mean_q: 0.020157
 21606/100000: episode: 2166, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000262, mae: 0.004982, mean_q: 0.022266
 21616/100000: episode: 2167, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.003088, mean_q: 0.021405
 21626/100000: episode: 2168, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000396, mae: 0.004887, mean_q: 0.019537
[Info] 1-TH LEVEL FOUND: 0.021554794162511826, Considering 100/100 traces
 21636/100000: episode: 2169, duration: 0.679s, episode steps: 10, steps per second: 15, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000080, mae: 0.004346, mean_q: 0.022318
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021554794162511826
1
 21646/100000: episode: 2170, duration: 0.490s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000050, mae: 0.002001, mean_q: 0.020859
 21656/100000: episode: 2171, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000035, mae: 0.002205, mean_q: 0.020362
 21666/100000: episode: 2172, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000050, mae: 0.002043, mean_q: 0.021184
 21676/100000: episode: 2173, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000225, mae: 0.003321, mean_q: 0.021151
 21686/100000: episode: 2174, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.002373, mean_q: 0.020981
 21696/100000: episode: 2175, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000037, mae: 0.003408, mean_q: 0.019206
 21706/100000: episode: 2176, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000058, mae: 0.003254, mean_q: 0.019780
 21716/100000: episode: 2177, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000030, mae: 0.001834, mean_q: 0.020220
 21726/100000: episode: 2178, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000201, mae: 0.002859, mean_q: 0.020831
 21736/100000: episode: 2179, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000032, mae: 0.002166, mean_q: 0.020016
 21746/100000: episode: 2180, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000220, mae: 0.003353, mean_q: 0.019573
 21756/100000: episode: 2181, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000054, mae: 0.003028, mean_q: 0.021619
 21766/100000: episode: 2182, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000075, mae: 0.002837, mean_q: 0.020589
 21776/100000: episode: 2183, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000029, mae: 0.001897, mean_q: 0.019917
 21786/100000: episode: 2184, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000016, mae: 0.002066, mean_q: 0.019898
 21796/100000: episode: 2185, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000012, mae: 0.001895, mean_q: 0.019685
 21806/100000: episode: 2186, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000200, mae: 0.002613, mean_q: 0.020061
 21816/100000: episode: 2187, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000035, mae: 0.002722, mean_q: 0.021551
 21826/100000: episode: 2188, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000050, mae: 0.002181, mean_q: 0.020090
 21836/100000: episode: 2189, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000013, mae: 0.002068, mean_q: 0.019603
 21846/100000: episode: 2190, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000033, mae: 0.002071, mean_q: 0.019700
 21856/100000: episode: 2191, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000017, mae: 0.001909, mean_q: 0.020063
 21866/100000: episode: 2192, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000395, mae: 0.004234, mean_q: 0.019664
 21876/100000: episode: 2193, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000227, mae: 0.005732, mean_q: 0.023374
 21886/100000: episode: 2194, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000248, mae: 0.003889, mean_q: 0.020890
 21896/100000: episode: 2195, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.002274, mean_q: 0.019988
 21906/100000: episode: 2196, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002515, mean_q: 0.020088
 21916/100000: episode: 2197, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000015, mae: 0.001680, mean_q: 0.020380
 21926/100000: episode: 2198, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000056, mae: 0.002382, mean_q: 0.020252
 21936/100000: episode: 2199, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000090, mae: 0.002561, mean_q: 0.020544
 21946/100000: episode: 2200, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000038, mae: 0.002557, mean_q: 0.021004
 21956/100000: episode: 2201, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000098, mae: 0.004113, mean_q: 0.018978
 21966/100000: episode: 2202, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000013, mae: 0.001882, mean_q: 0.020734
 21976/100000: episode: 2203, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000094, mae: 0.002701, mean_q: 0.020336
 21986/100000: episode: 2204, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000010, mae: 0.001694, mean_q: 0.019856
 21996/100000: episode: 2205, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.003151, mean_q: 0.018741
Step 22000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.22000.hdf5
 22006/100000: episode: 2206, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000202, mae: 0.002778, mean_q: 0.020101
 22016/100000: episode: 2207, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000054, mae: 0.002367, mean_q: 0.020373
 22026/100000: episode: 2208, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000096, mae: 0.003301, mean_q: 0.020668
 22036/100000: episode: 2209, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000013, mae: 0.002138, mean_q: 0.020672
 22046/100000: episode: 2210, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000053, mae: 0.003398, mean_q: 0.018407
 22056/100000: episode: 2211, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000056, mae: 0.002682, mean_q: 0.020244
 22066/100000: episode: 2212, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000222, mae: 0.002949, mean_q: 0.020184
 22076/100000: episode: 2213, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000054, mae: 0.002839, mean_q: 0.020887
 22086/100000: episode: 2214, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000076, mae: 0.003090, mean_q: 0.019398
 22096/100000: episode: 2215, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000015, mae: 0.001597, mean_q: 0.020255
 22106/100000: episode: 2216, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000010, mae: 0.001560, mean_q: 0.019581
 22116/100000: episode: 2217, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000225, mae: 0.003433, mean_q: 0.019698
 22126/100000: episode: 2218, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000015, mae: 0.002722, mean_q: 0.021569
 22136/100000: episode: 2219, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000009, mae: 0.001694, mean_q: 0.019289
 22146/100000: episode: 2220, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002382, mean_q: 0.019692
 22156/100000: episode: 2221, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000015, mae: 0.001682, mean_q: 0.019882
 22166/100000: episode: 2222, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000096, mae: 0.003335, mean_q: 0.019352
 22176/100000: episode: 2223, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000011, mae: 0.001533, mean_q: 0.020205
 22186/100000: episode: 2224, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000034, mae: 0.002809, mean_q: 0.018754
 22196/100000: episode: 2225, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002394, mean_q: 0.019203
 22206/100000: episode: 2226, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000033, mae: 0.001780, mean_q: 0.019596
 22216/100000: episode: 2227, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002173, mean_q: 0.019152
 22226/100000: episode: 2228, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000076, mae: 0.002849, mean_q: 0.019827
 22236/100000: episode: 2229, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000029, mae: 0.001899, mean_q: 0.018977
 22246/100000: episode: 2230, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000035, mae: 0.002358, mean_q: 0.018876
 22256/100000: episode: 2231, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000223, mae: 0.003624, mean_q: 0.020564
 22266/100000: episode: 2232, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000037, mae: 0.002666, mean_q: 0.019994
 22276/100000: episode: 2233, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000060, mae: 0.002758, mean_q: 0.019267
 22286/100000: episode: 2234, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.002288, mean_q: 0.020479
 22296/100000: episode: 2235, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000082, mae: 0.003245, mean_q: 0.019210
 22306/100000: episode: 2236, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000079, mae: 0.003482, mean_q: 0.020441
 22316/100000: episode: 2237, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000031, mae: 0.001858, mean_q: 0.019418
 22326/100000: episode: 2238, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000283, mae: 0.004288, mean_q: 0.020253
 22336/100000: episode: 2239, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000118, mae: 0.005153, mean_q: 0.021939
 22346/100000: episode: 2240, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000036, mae: 0.002349, mean_q: 0.020317
 22356/100000: episode: 2241, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000077, mae: 0.003055, mean_q: 0.019574
 22366/100000: episode: 2242, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000055, mae: 0.002836, mean_q: 0.020759
 22376/100000: episode: 2243, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000076, mae: 0.003023, mean_q: 0.020611
 22386/100000: episode: 2244, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000219, mae: 0.002739, mean_q: 0.020060
 22396/100000: episode: 2245, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000078, mae: 0.002994, mean_q: 0.020366
 22406/100000: episode: 2246, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000206, mae: 0.003589, mean_q: 0.021129
 22416/100000: episode: 2247, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002253, mean_q: 0.019825
 22426/100000: episode: 2248, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000029, mae: 0.002177, mean_q: 0.019157
 22436/100000: episode: 2249, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.002471, mean_q: 0.020009
 22446/100000: episode: 2250, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000055, mae: 0.002713, mean_q: 0.020755
 22456/100000: episode: 2251, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000058, mae: 0.002899, mean_q: 0.019342
 22466/100000: episode: 2252, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000031, mae: 0.001579, mean_q: 0.019882
 22476/100000: episode: 2253, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000035, mae: 0.002320, mean_q: 0.019593
 22486/100000: episode: 2254, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000245, mae: 0.003734, mean_q: 0.019282
 22496/100000: episode: 2255, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000097, mae: 0.005353, mean_q: 0.022788
 22506/100000: episode: 2256, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000042, mae: 0.003391, mean_q: 0.019847
 22516/100000: episode: 2257, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000078, mae: 0.003292, mean_q: 0.019302
 22526/100000: episode: 2258, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.003260, mean_q: 0.021665
 22536/100000: episode: 2259, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000038, mae: 0.002735, mean_q: 0.019512
 22546/100000: episode: 2260, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000094, mae: 0.003303, mean_q: 0.019365
 22556/100000: episode: 2261, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000011, mae: 0.002149, mean_q: 0.021304
 22566/100000: episode: 2262, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000033, mae: 0.002907, mean_q: 0.018813
 22576/100000: episode: 2263, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000028, mae: 0.002359, mean_q: 0.018701
 22586/100000: episode: 2264, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000010, mae: 0.001274, mean_q: 0.019728
 22596/100000: episode: 2265, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000012, mae: 0.001683, mean_q: 0.019445
 22606/100000: episode: 2266, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [0.000, 10.000], loss: 0.000224, mae: 0.003334, mean_q: 0.019297
 22616/100000: episode: 2267, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000038, mae: 0.002451, mean_q: 0.020463
 22626/100000: episode: 2268, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000033, mae: 0.001818, mean_q: 0.019686
 22636/100000: episode: 2269, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000092, mae: 0.003026, mean_q: 0.020454
[Info] 1-TH LEVEL FOUND: 0.01892935112118721, Considering 100/100 traces
 22646/100000: episode: 2270, duration: 0.709s, episode steps: 10, steps per second: 14, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000008, mae: 0.001466, mean_q: 0.020206
[Info] 2-TH LEVEL FOUND: 0.01980094611644745, Considering 100/100 traces
 22656/100000: episode: 2271, duration: 0.661s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.002927, mean_q: 0.018844
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.01980094611644745
2
 22666/100000: episode: 2272, duration: 0.489s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000017, mae: 0.001829, mean_q: 0.019891
 22676/100000: episode: 2273, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000246, mae: 0.003572, mean_q: 0.019467
 22686/100000: episode: 2274, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.002966, mean_q: 0.020668
 22696/100000: episode: 2275, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000204, mae: 0.003084, mean_q: 0.019115
 22706/100000: episode: 2276, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000035, mae: 0.001935, mean_q: 0.020015
 22716/100000: episode: 2277, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000246, mae: 0.004027, mean_q: 0.020597
 22726/100000: episode: 2278, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000223, mae: 0.004036, mean_q: 0.021411
 22736/100000: episode: 2279, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000013, mae: 0.002542, mean_q: 0.018958
 22746/100000: episode: 2280, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002753, mean_q: 0.018540
 22756/100000: episode: 2281, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000035, mae: 0.002411, mean_q: 0.020545
 22766/100000: episode: 2282, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000227, mae: 0.003588, mean_q: 0.019140
 22776/100000: episode: 2283, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000031, mae: 0.002201, mean_q: 0.020686
 22786/100000: episode: 2284, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000012, mae: 0.001929, mean_q: 0.019101
 22796/100000: episode: 2285, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002646, mean_q: 0.019011
 22806/100000: episode: 2286, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000050, mae: 0.001901, mean_q: 0.019691
 22816/100000: episode: 2287, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000281, mae: 0.004576, mean_q: 0.021016
 22826/100000: episode: 2288, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000016, mae: 0.002742, mean_q: 0.020339
 22836/100000: episode: 2289, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.003181, mean_q: 0.017748
 22846/100000: episode: 2290, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000055, mae: 0.002499, mean_q: 0.020154
 22856/100000: episode: 2291, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002817, mean_q: 0.020082
 22866/100000: episode: 2292, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000097, mae: 0.003344, mean_q: 0.019335
 22876/100000: episode: 2293, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000054, mae: 0.002279, mean_q: 0.020133
 22886/100000: episode: 2294, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000071, mae: 0.002110, mean_q: 0.019737
 22896/100000: episode: 2295, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000012, mae: 0.001886, mean_q: 0.020369
 22906/100000: episode: 2296, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000029, mae: 0.002126, mean_q: 0.018908
 22916/100000: episode: 2297, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000206, mae: 0.003250, mean_q: 0.019003
 22926/100000: episode: 2298, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000096, mae: 0.004027, mean_q: 0.021160
 22936/100000: episode: 2299, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000015, mae: 0.002149, mean_q: 0.019559
 22946/100000: episode: 2300, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000207, mae: 0.003531, mean_q: 0.018959
 22956/100000: episode: 2301, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000053, mae: 0.002014, mean_q: 0.019922
 22966/100000: episode: 2302, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000229, mae: 0.003555, mean_q: 0.020009
 22976/100000: episode: 2303, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000038, mae: 0.002259, mean_q: 0.020000
 22986/100000: episode: 2304, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000123, mae: 0.004154, mean_q: 0.020166
 22996/100000: episode: 2305, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000041, mae: 0.003483, mean_q: 0.021372
Step 23000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.23000.hdf5
 23006/100000: episode: 2306, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000220, mae: 0.002762, mean_q: 0.020190
 23016/100000: episode: 2307, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000051, mae: 0.002091, mean_q: 0.020226
 23026/100000: episode: 2308, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000225, mae: 0.003287, mean_q: 0.019630
 23036/100000: episode: 2309, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000009, mae: 0.001586, mean_q: 0.020697
 23046/100000: episode: 2310, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000052, mae: 0.001948, mean_q: 0.020170
 23056/100000: episode: 2311, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000070, mae: 0.002345, mean_q: 0.020510
 23066/100000: episode: 2312, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000054, mae: 0.002196, mean_q: 0.020473
 23076/100000: episode: 2313, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000077, mae: 0.003240, mean_q: 0.020941
 23086/100000: episode: 2314, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000072, mae: 0.003026, mean_q: 0.019362
 23096/100000: episode: 2315, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000245, mae: 0.004443, mean_q: 0.021342
 23106/100000: episode: 2316, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000289, mae: 0.005601, mean_q: 0.021691
 23116/100000: episode: 2317, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.002147, mean_q: 0.020313
 23126/100000: episode: 2318, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000222, mae: 0.003368, mean_q: 0.019616
 23136/100000: episode: 2319, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000077, mae: 0.003388, mean_q: 0.021112
 23146/100000: episode: 2320, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000055, mae: 0.002444, mean_q: 0.019961
 23156/100000: episode: 2321, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000083, mae: 0.003537, mean_q: 0.020784
 23166/100000: episode: 2322, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000218, mae: 0.002964, mean_q: 0.021078
 23176/100000: episode: 2323, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000084, mae: 0.003603, mean_q: 0.020824
 23186/100000: episode: 2324, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000220, mae: 0.002946, mean_q: 0.020682
 23196/100000: episode: 2325, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000050, mae: 0.002106, mean_q: 0.021081
 23206/100000: episode: 2326, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000078, mae: 0.002719, mean_q: 0.020646
 23216/100000: episode: 2327, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000097, mae: 0.003657, mean_q: 0.021401
 23226/100000: episode: 2328, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000263, mae: 0.004454, mean_q: 0.021799
 23236/100000: episode: 2329, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000224, mae: 0.004205, mean_q: 0.022232
 23246/100000: episode: 2330, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000248, mae: 0.003855, mean_q: 0.020675
 23256/100000: episode: 2331, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000011, mae: 0.001832, mean_q: 0.021708
 23266/100000: episode: 2332, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002690, mean_q: 0.020277
 23276/100000: episode: 2333, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000017, mae: 0.002415, mean_q: 0.019987
 23286/100000: episode: 2334, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000221, mae: 0.003210, mean_q: 0.021142
 23296/100000: episode: 2335, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000062, mae: 0.003041, mean_q: 0.021145
 23306/100000: episode: 2336, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000030, mae: 0.002180, mean_q: 0.020064
 23316/100000: episode: 2337, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000054, mae: 0.002408, mean_q: 0.020514
 23326/100000: episode: 2338, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000058, mae: 0.002492, mean_q: 0.020865
 23336/100000: episode: 2339, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000202, mae: 0.002984, mean_q: 0.021554
 23346/100000: episode: 2340, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000018, mae: 0.002574, mean_q: 0.021715
 23356/100000: episode: 2341, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000053, mae: 0.003020, mean_q: 0.019585
 23366/100000: episode: 2342, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000219, mae: 0.003025, mean_q: 0.020163
 23376/100000: episode: 2343, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000246, mae: 0.005031, mean_q: 0.022717
 23386/100000: episode: 2344, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000039, mae: 0.003041, mean_q: 0.021763
 23396/100000: episode: 2345, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000438, mae: 0.005450, mean_q: 0.020671
 23406/100000: episode: 2346, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000221, mae: 0.004405, mean_q: 0.022997
 23416/100000: episode: 2347, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000058, mae: 0.002790, mean_q: 0.021321
 23426/100000: episode: 2348, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000034, mae: 0.002967, mean_q: 0.019798
 23436/100000: episode: 2349, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000229, mae: 0.004287, mean_q: 0.021356
 23446/100000: episode: 2350, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.003024, mean_q: 0.022076
 23456/100000: episode: 2351, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000080, mae: 0.004257, mean_q: 0.019266
 23466/100000: episode: 2352, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000012, mae: 0.001470, mean_q: 0.020905
 23476/100000: episode: 2353, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000016, mae: 0.002607, mean_q: 0.019979
 23486/100000: episode: 2354, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000205, mae: 0.003547, mean_q: 0.019806
 23496/100000: episode: 2355, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000060, mae: 0.002766, mean_q: 0.020552
 23506/100000: episode: 2356, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000030, mae: 0.002013, mean_q: 0.021393
 23516/100000: episode: 2357, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000008, mae: 0.001667, mean_q: 0.019972
 23526/100000: episode: 2358, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000056, mae: 0.003421, mean_q: 0.019323
 23536/100000: episode: 2359, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000140, mae: 0.004701, mean_q: 0.021319
 23546/100000: episode: 2360, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000033, mae: 0.002449, mean_q: 0.021111
 23556/100000: episode: 2361, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000220, mae: 0.003817, mean_q: 0.019025
 23566/100000: episode: 2362, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000056, mae: 0.003673, mean_q: 0.022402
 23576/100000: episode: 2363, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000053, mae: 0.002442, mean_q: 0.020914
 23586/100000: episode: 2364, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002822, mean_q: 0.020052
 23596/100000: episode: 2365, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000012, mae: 0.001667, mean_q: 0.020147
 23606/100000: episode: 2366, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000016, mae: 0.002107, mean_q: 0.019922
 23616/100000: episode: 2367, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000078, mae: 0.002931, mean_q: 0.020433
 23626/100000: episode: 2368, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002117, mean_q: 0.020786
 23636/100000: episode: 2369, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000013, mae: 0.002066, mean_q: 0.019590
 23646/100000: episode: 2370, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000017, mae: 0.002694, mean_q: 0.019267
 23656/100000: episode: 2371, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000221, mae: 0.003408, mean_q: 0.019335
[Info] 1-TH LEVEL FOUND: 0.020649103447794914, Considering 100/100 traces
 23666/100000: episode: 2372, duration: 0.687s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000055, mae: 0.002969, mean_q: 0.021135
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020649103447794914
1
 23676/100000: episode: 2373, duration: 0.497s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000049, mae: 0.002060, mean_q: 0.019880
 23686/100000: episode: 2374, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000033, mae: 0.002280, mean_q: 0.019504
 23696/100000: episode: 2375, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000017, mae: 0.002071, mean_q: 0.019839
 23706/100000: episode: 2376, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000226, mae: 0.003886, mean_q: 0.021182
 23716/100000: episode: 2377, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000034, mae: 0.003349, mean_q: 0.022317
 23726/100000: episode: 2378, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000009, mae: 0.002181, mean_q: 0.019235
 23736/100000: episode: 2379, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000036, mae: 0.002748, mean_q: 0.019152
 23746/100000: episode: 2380, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000225, mae: 0.003641, mean_q: 0.020989
 23756/100000: episode: 2381, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000226, mae: 0.003498, mean_q: 0.020645
 23766/100000: episode: 2382, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000208, mae: 0.003409, mean_q: 0.019930
 23776/100000: episode: 2383, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002077, mean_q: 0.020443
 23786/100000: episode: 2384, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000030, mae: 0.001398, mean_q: 0.020182
 23796/100000: episode: 2385, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000057, mae: 0.002520, mean_q: 0.019859
 23806/100000: episode: 2386, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000229, mae: 0.003378, mean_q: 0.020289
 23816/100000: episode: 2387, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000417, mae: 0.005525, mean_q: 0.021473
 23826/100000: episode: 2388, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000225, mae: 0.004798, mean_q: 0.022362
 23836/100000: episode: 2389, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.002446, mean_q: 0.019407
 23846/100000: episode: 2390, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000602, mae: 0.006178, mean_q: 0.020041
 23856/100000: episode: 2391, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000206, mae: 0.005973, mean_q: 0.024461
 23866/100000: episode: 2392, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000078, mae: 0.003391, mean_q: 0.020377
 23876/100000: episode: 2393, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000035, mae: 0.002555, mean_q: 0.019915
 23886/100000: episode: 2394, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000201, mae: 0.002559, mean_q: 0.021016
 23896/100000: episode: 2395, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000039, mae: 0.002933, mean_q: 0.021491
 23906/100000: episode: 2396, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000020, mae: 0.003172, mean_q: 0.019243
 23916/100000: episode: 2397, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.002133, mean_q: 0.021087
 23926/100000: episode: 2398, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000036, mae: 0.002527, mean_q: 0.020046
 23936/100000: episode: 2399, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000227, mae: 0.004038, mean_q: 0.021011
 23946/100000: episode: 2400, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000618, mae: 0.007757, mean_q: 0.023325
 23956/100000: episode: 2401, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000033, mae: 0.003004, mean_q: 0.022316
 23966/100000: episode: 2402, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000062, mae: 0.004472, mean_q: 0.018743
 23976/100000: episode: 2403, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000207, mae: 0.003841, mean_q: 0.021685
 23986/100000: episode: 2404, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002469, mean_q: 0.021166
 23996/100000: episode: 2405, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000050, mae: 0.002528, mean_q: 0.019977
Step 24000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.24000.hdf5
 24006/100000: episode: 2406, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000050, mae: 0.002145, mean_q: 0.021305
 24016/100000: episode: 2407, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000265, mae: 0.003946, mean_q: 0.020786
 24026/100000: episode: 2408, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000225, mae: 0.004139, mean_q: 0.022073
 24036/100000: episode: 2409, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000055, mae: 0.002516, mean_q: 0.021368
 24046/100000: episode: 2410, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000075, mae: 0.003277, mean_q: 0.021838
 24056/100000: episode: 2411, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000206, mae: 0.003470, mean_q: 0.021728
 24066/100000: episode: 2412, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002371, mean_q: 0.020166
 24076/100000: episode: 2413, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000222, mae: 0.003079, mean_q: 0.020916
 24086/100000: episode: 2414, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000241, mae: 0.003984, mean_q: 0.022073
 24096/100000: episode: 2415, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.003276, mean_q: 0.021869
 24106/100000: episode: 2416, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000057, mae: 0.002561, mean_q: 0.020946
 24116/100000: episode: 2417, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000202, mae: 0.002763, mean_q: 0.020974
 24126/100000: episode: 2418, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002349, mean_q: 0.021778
 24136/100000: episode: 2419, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.002326, mean_q: 0.020903
 24146/100000: episode: 2420, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.002222, mean_q: 0.020457
 24156/100000: episode: 2421, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000016, mae: 0.002206, mean_q: 0.020456
 24166/100000: episode: 2422, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000270, mae: 0.004693, mean_q: 0.021377
 24176/100000: episode: 2423, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000080, mae: 0.003499, mean_q: 0.021510
 24186/100000: episode: 2424, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000244, mae: 0.003841, mean_q: 0.021218
 24196/100000: episode: 2425, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000054, mae: 0.002387, mean_q: 0.021094
 24206/100000: episode: 2426, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000073, mae: 0.002612, mean_q: 0.021203
 24216/100000: episode: 2427, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002058, mean_q: 0.020940
 24226/100000: episode: 2428, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000222, mae: 0.003232, mean_q: 0.020853
 24236/100000: episode: 2429, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000227, mae: 0.003658, mean_q: 0.020880
 24246/100000: episode: 2430, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000269, mae: 0.005047, mean_q: 0.022193
 24256/100000: episode: 2431, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000243, mae: 0.004405, mean_q: 0.022408
 24266/100000: episode: 2432, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000055, mae: 0.002813, mean_q: 0.022045
 24276/100000: episode: 2433, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002226, mean_q: 0.020791
 24286/100000: episode: 2434, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000014, mae: 0.001861, mean_q: 0.020776
 24296/100000: episode: 2435, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000019, mae: 0.002644, mean_q: 0.020443
 24306/100000: episode: 2436, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000053, mae: 0.003260, mean_q: 0.019520
 24316/100000: episode: 2437, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000037, mae: 0.002337, mean_q: 0.021259
 24326/100000: episode: 2438, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000037, mae: 0.002293, mean_q: 0.020643
 24336/100000: episode: 2439, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000075, mae: 0.002983, mean_q: 0.020381
 24346/100000: episode: 2440, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000019, mae: 0.002172, mean_q: 0.020913
 24356/100000: episode: 2441, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000241, mae: 0.003657, mean_q: 0.020153
 24366/100000: episode: 2442, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000077, mae: 0.003631, mean_q: 0.021972
 24376/100000: episode: 2443, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000013, mae: 0.001702, mean_q: 0.020755
 24386/100000: episode: 2444, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000037, mae: 0.003152, mean_q: 0.019431
 24396/100000: episode: 2445, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000077, mae: 0.002981, mean_q: 0.020849
 24406/100000: episode: 2446, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.002180, mean_q: 0.020825
 24416/100000: episode: 2447, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000393, mae: 0.004376, mean_q: 0.019956
 24426/100000: episode: 2448, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000258, mae: 0.005123, mean_q: 0.022991
 24436/100000: episode: 2449, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000204, mae: 0.003187, mean_q: 0.021700
 24446/100000: episode: 2450, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000080, mae: 0.002993, mean_q: 0.020985
 24456/100000: episode: 2451, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000052, mae: 0.002365, mean_q: 0.020521
 24466/100000: episode: 2452, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000077, mae: 0.003012, mean_q: 0.020636
 24476/100000: episode: 2453, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000260, mae: 0.003360, mean_q: 0.020789
 24486/100000: episode: 2454, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000229, mae: 0.004814, mean_q: 0.022552
 24496/100000: episode: 2455, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.002305, mean_q: 0.021180
 24506/100000: episode: 2456, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000098, mae: 0.003602, mean_q: 0.020183
 24516/100000: episode: 2457, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000055, mae: 0.003386, mean_q: 0.022458
 24526/100000: episode: 2458, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.002295, mean_q: 0.020505
 24536/100000: episode: 2459, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000051, mae: 0.003163, mean_q: 0.019331
 24546/100000: episode: 2460, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000038, mae: 0.002495, mean_q: 0.021364
 24556/100000: episode: 2461, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000075, mae: 0.002701, mean_q: 0.020744
 24566/100000: episode: 2462, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000625, mae: 0.007402, mean_q: 0.022499
 24576/100000: episode: 2463, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.003755, mean_q: 0.023098
 24586/100000: episode: 2464, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000013, mae: 0.003118, mean_q: 0.019131
 24596/100000: episode: 2465, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000227, mae: 0.004103, mean_q: 0.019831
 24606/100000: episode: 2466, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000057, mae: 0.004101, mean_q: 0.023210
 24616/100000: episode: 2467, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000014, mae: 0.002269, mean_q: 0.020829
 24626/100000: episode: 2468, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000073, mae: 0.003338, mean_q: 0.019708
 24636/100000: episode: 2469, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000013, mae: 0.001921, mean_q: 0.021822
 24646/100000: episode: 2470, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000012, mae: 0.002117, mean_q: 0.020059
 24656/100000: episode: 2471, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000055, mae: 0.002672, mean_q: 0.020298
 24666/100000: episode: 2472, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000039, mae: 0.002727, mean_q: 0.020308
[Info] 1-TH LEVEL FOUND: 0.023183530196547508, Considering 100/100 traces
 24676/100000: episode: 2473, duration: 0.713s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000288, mae: 0.005123, mean_q: 0.021425
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023183530196547508
1
 24686/100000: episode: 2474, duration: 0.590s, episode steps: 10, steps per second: 17, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000041, mae: 0.003355, mean_q: 0.021358
 24696/100000: episode: 2475, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000033, mae: 0.002547, mean_q: 0.019745
 24706/100000: episode: 2476, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000051, mae: 0.001949, mean_q: 0.020809
 24716/100000: episode: 2477, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000078, mae: 0.002816, mean_q: 0.020689
 24726/100000: episode: 2478, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000226, mae: 0.003327, mean_q: 0.020695
 24736/100000: episode: 2479, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000041, mae: 0.002793, mean_q: 0.020465
 24746/100000: episode: 2480, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000060, mae: 0.003459, mean_q: 0.019766
 24756/100000: episode: 2481, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002384, mean_q: 0.019986
 24766/100000: episode: 2482, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.002304, mean_q: 0.019331
 24776/100000: episode: 2483, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000018, mae: 0.002286, mean_q: 0.019822
 24786/100000: episode: 2484, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000224, mae: 0.003541, mean_q: 0.019631
 24796/100000: episode: 2485, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000201, mae: 0.004441, mean_q: 0.022940
 24806/100000: episode: 2486, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000032, mae: 0.002637, mean_q: 0.020145
 24816/100000: episode: 2487, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.003275, mean_q: 0.019327
 24826/100000: episode: 2488, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000037, mae: 0.002536, mean_q: 0.021028
 24836/100000: episode: 2489, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.003062, mean_q: 0.019390
 24846/100000: episode: 2490, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000036, mae: 0.002050, mean_q: 0.020115
 24856/100000: episode: 2491, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000053, mae: 0.002197, mean_q: 0.020603
 24866/100000: episode: 2492, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000056, mae: 0.002654, mean_q: 0.019717
 24876/100000: episode: 2493, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000015, mae: 0.001862, mean_q: 0.019847
 24886/100000: episode: 2494, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000053, mae: 0.002165, mean_q: 0.020343
 24896/100000: episode: 2495, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000014, mae: 0.001682, mean_q: 0.020605
 24906/100000: episode: 2496, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000010, mae: 0.001763, mean_q: 0.019447
 24916/100000: episode: 2497, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000243, mae: 0.003795, mean_q: 0.020577
 24926/100000: episode: 2498, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000053, mae: 0.002443, mean_q: 0.020654
 24936/100000: episode: 2499, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.002514, mean_q: 0.019240
 24946/100000: episode: 2500, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002232, mean_q: 0.019476
 24956/100000: episode: 2501, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000013, mae: 0.002425, mean_q: 0.018849
 24966/100000: episode: 2502, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000057, mae: 0.002583, mean_q: 0.019990
 24976/100000: episode: 2503, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000054, mae: 0.002338, mean_q: 0.019923
 24986/100000: episode: 2504, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000037, mae: 0.002071, mean_q: 0.019850
 24996/100000: episode: 2505, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000039, mae: 0.002744, mean_q: 0.019305
Step 25000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.25000.hdf5
 25006/100000: episode: 2506, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000037, mae: 0.002800, mean_q: 0.018856
 25016/100000: episode: 2507, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000029, mae: 0.001772, mean_q: 0.019318
 25026/100000: episode: 2508, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000037, mae: 0.002100, mean_q: 0.019896
 25036/100000: episode: 2509, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000028, mae: 0.001578, mean_q: 0.019316
 25046/100000: episode: 2510, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000227, mae: 0.003777, mean_q: 0.020665
 25056/100000: episode: 2511, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002797, mean_q: 0.019486
 25066/100000: episode: 2512, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000010, mae: 0.002169, mean_q: 0.018595
 25076/100000: episode: 2513, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000053, mae: 0.002250, mean_q: 0.019388
 25086/100000: episode: 2514, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000200, mae: 0.002484, mean_q: 0.019312
 25096/100000: episode: 2515, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000221, mae: 0.002645, mean_q: 0.019423
 25106/100000: episode: 2516, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000033, mae: 0.002044, mean_q: 0.020036
 25116/100000: episode: 2517, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000019, mae: 0.002569, mean_q: 0.018764
 25126/100000: episode: 2518, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000027, mae: 0.001832, mean_q: 0.018625
 25136/100000: episode: 2519, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000036, mae: 0.002031, mean_q: 0.019618
 25146/100000: episode: 2520, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000016, mae: 0.001928, mean_q: 0.018983
 25156/100000: episode: 2521, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.001522, mean_q: 0.019187
 25166/100000: episode: 2522, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000050, mae: 0.001576, mean_q: 0.019450
 25176/100000: episode: 2523, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000050, mae: 0.002053, mean_q: 0.019896
 25186/100000: episode: 2524, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000030, mae: 0.001780, mean_q: 0.019024
 25196/100000: episode: 2525, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000012, mae: 0.002019, mean_q: 0.018596
 25206/100000: episode: 2526, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000205, mae: 0.003502, mean_q: 0.018095
 25216/100000: episode: 2527, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000007, mae: 0.001217, mean_q: 0.019544
 25226/100000: episode: 2528, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000013, mae: 0.001532, mean_q: 0.019504
 25236/100000: episode: 2529, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000056, mae: 0.002466, mean_q: 0.018773
 25246/100000: episode: 2530, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000075, mae: 0.002696, mean_q: 0.019600
 25256/100000: episode: 2531, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000078, mae: 0.002833, mean_q: 0.019309
 25266/100000: episode: 2532, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002274, mean_q: 0.018720
 25276/100000: episode: 2533, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000013, mae: 0.001983, mean_q: 0.018364
 25286/100000: episode: 2534, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000068, mae: 0.002167, mean_q: 0.019522
 25296/100000: episode: 2535, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000018, mae: 0.002140, mean_q: 0.018785
 25306/100000: episode: 2536, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000231, mae: 0.003966, mean_q: 0.019429
 25316/100000: episode: 2537, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000075, mae: 0.003236, mean_q: 0.020334
 25326/100000: episode: 2538, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000059, mae: 0.002491, mean_q: 0.019315
 25336/100000: episode: 2539, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000016, mae: 0.001427, mean_q: 0.019173
 25346/100000: episode: 2540, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000054, mae: 0.002078, mean_q: 0.019374
 25356/100000: episode: 2541, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000052, mae: 0.002014, mean_q: 0.019679
 25366/100000: episode: 2542, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000205, mae: 0.003264, mean_q: 0.020049
 25376/100000: episode: 2543, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000042, mae: 0.003053, mean_q: 0.018611
 25386/100000: episode: 2544, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000099, mae: 0.003800, mean_q: 0.018313
 25396/100000: episode: 2545, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000053, mae: 0.003138, mean_q: 0.020669
 25406/100000: episode: 2546, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000011, mae: 0.001932, mean_q: 0.019266
 25416/100000: episode: 2547, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000013, mae: 0.003205, mean_q: 0.017189
 25426/100000: episode: 2548, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000018, mae: 0.002215, mean_q: 0.018645
 25436/100000: episode: 2549, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000205, mae: 0.002617, mean_q: 0.019132
 25446/100000: episode: 2550, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002407, mean_q: 0.019710
 25456/100000: episode: 2551, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000012, mae: 0.002163, mean_q: 0.018074
 25466/100000: episode: 2552, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000062, mae: 0.003288, mean_q: 0.018244
 25476/100000: episode: 2553, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000011, mae: 0.001652, mean_q: 0.019395
 25486/100000: episode: 2554, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000080, mae: 0.002874, mean_q: 0.018905
 25496/100000: episode: 2555, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000051, mae: 0.002170, mean_q: 0.019512
 25506/100000: episode: 2556, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000045, mae: 0.002902, mean_q: 0.018604
 25516/100000: episode: 2557, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000031, mae: 0.001738, mean_q: 0.018631
 25526/100000: episode: 2558, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000205, mae: 0.002840, mean_q: 0.018644
 25536/100000: episode: 2559, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000011, mae: 0.002292, mean_q: 0.020315
 25546/100000: episode: 2560, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000014, mae: 0.002649, mean_q: 0.017657
 25556/100000: episode: 2561, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000208, mae: 0.003688, mean_q: 0.019024
 25566/100000: episode: 2562, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000039, mae: 0.003263, mean_q: 0.020004
 25576/100000: episode: 2563, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000209, mae: 0.003881, mean_q: 0.017594
 25586/100000: episode: 2564, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000078, mae: 0.004093, mean_q: 0.020734
 25596/100000: episode: 2565, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000052, mae: 0.002265, mean_q: 0.019094
 25606/100000: episode: 2566, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000039, mae: 0.002282, mean_q: 0.019051
 25616/100000: episode: 2567, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000227, mae: 0.003118, mean_q: 0.019054
 25626/100000: episode: 2568, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000075, mae: 0.003398, mean_q: 0.020226
 25636/100000: episode: 2569, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000054, mae: 0.002268, mean_q: 0.018736
 25646/100000: episode: 2570, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000227, mae: 0.003744, mean_q: 0.019980
 25656/100000: episode: 2571, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000032, mae: 0.002401, mean_q: 0.020143
 25666/100000: episode: 2572, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000229, mae: 0.003593, mean_q: 0.018786
 25676/100000: episode: 2573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000260, mae: 0.005081, mean_q: 0.021528
[Info] 1-TH LEVEL FOUND: 0.018155260011553764, Considering 100/100 traces
 25686/100000: episode: 2574, duration: 0.681s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.002577, mean_q: 0.020517
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.018155260011553764
1
 25696/100000: episode: 2575, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000018, mae: 0.003601, mean_q: 0.017382
 25706/100000: episode: 2576, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000057, mae: 0.003003, mean_q: 0.019454
 25716/100000: episode: 2577, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000207, mae: 0.004647, mean_q: 0.021605
 25726/100000: episode: 2578, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000229, mae: 0.003914, mean_q: 0.018780
 25736/100000: episode: 2579, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000032, mae: 0.001688, mean_q: 0.019334
 25746/100000: episode: 2580, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000225, mae: 0.003130, mean_q: 0.018988
 25756/100000: episode: 2581, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000018, mae: 0.002897, mean_q: 0.020844
 25766/100000: episode: 2582, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000037, mae: 0.003175, mean_q: 0.018081
 25776/100000: episode: 2583, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000204, mae: 0.002972, mean_q: 0.018918
 25786/100000: episode: 2584, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000009, mae: 0.001647, mean_q: 0.020181
 25796/100000: episode: 2585, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000034, mae: 0.001710, mean_q: 0.019574
 25806/100000: episode: 2586, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000016, mae: 0.001635, mean_q: 0.019356
 25816/100000: episode: 2587, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000222, mae: 0.002707, mean_q: 0.019274
 25826/100000: episode: 2588, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000041, mae: 0.002656, mean_q: 0.019904
 25836/100000: episode: 2589, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000266, mae: 0.004377, mean_q: 0.020423
 25846/100000: episode: 2590, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000264, mae: 0.004381, mean_q: 0.020633
 25856/100000: episode: 2591, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000263, mae: 0.005118, mean_q: 0.021504
 25866/100000: episode: 2592, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000034, mae: 0.002787, mean_q: 0.020329
 25876/100000: episode: 2593, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000272, mae: 0.004806, mean_q: 0.018836
 25886/100000: episode: 2594, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.003608, mean_q: 0.022398
 25896/100000: episode: 2595, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000015, mae: 0.002649, mean_q: 0.018679
 25906/100000: episode: 2596, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000052, mae: 0.002705, mean_q: 0.018773
 25916/100000: episode: 2597, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000227, mae: 0.003934, mean_q: 0.020609
 25926/100000: episode: 2598, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000079, mae: 0.002846, mean_q: 0.020036
 25936/100000: episode: 2599, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000077, mae: 0.002879, mean_q: 0.019634
 25946/100000: episode: 2600, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.002657, mean_q: 0.018863
 25956/100000: episode: 2601, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000018, mae: 0.002512, mean_q: 0.018907
 25966/100000: episode: 2602, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.002222, mean_q: 0.019216
 25976/100000: episode: 2603, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000010, mae: 0.001438, mean_q: 0.019178
 25986/100000: episode: 2604, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000228, mae: 0.003673, mean_q: 0.019678
 25996/100000: episode: 2605, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000048, mae: 0.001870, mean_q: 0.019808
Step 26000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.26000.hdf5
 26006/100000: episode: 2606, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000032, mae: 0.001978, mean_q: 0.019203
 26016/100000: episode: 2607, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000251, mae: 0.003909, mean_q: 0.019340
 26026/100000: episode: 2608, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000016, mae: 0.001933, mean_q: 0.020036
 26036/100000: episode: 2609, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000032, mae: 0.002239, mean_q: 0.018812
 26046/100000: episode: 2610, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000063, mae: 0.003149, mean_q: 0.019460
 26056/100000: episode: 2611, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000053, mae: 0.003275, mean_q: 0.021122
 26066/100000: episode: 2612, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000053, mae: 0.002344, mean_q: 0.019526
 26076/100000: episode: 2613, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000205, mae: 0.004225, mean_q: 0.017612
 26086/100000: episode: 2614, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000060, mae: 0.003422, mean_q: 0.019967
 26096/100000: episode: 2615, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000395, mae: 0.003927, mean_q: 0.019546
 26106/100000: episode: 2616, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000248, mae: 0.004828, mean_q: 0.020995
 26116/100000: episode: 2617, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000224, mae: 0.003000, mean_q: 0.020032
 26126/100000: episode: 2618, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.002349, mean_q: 0.019597
 26136/100000: episode: 2619, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000073, mae: 0.002885, mean_q: 0.018805
 26146/100000: episode: 2620, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000226, mae: 0.003766, mean_q: 0.020502
 26156/100000: episode: 2621, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000075, mae: 0.003017, mean_q: 0.020069
 26166/100000: episode: 2622, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000457, mae: 0.005878, mean_q: 0.020524
 26176/100000: episode: 2623, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000245, mae: 0.004861, mean_q: 0.021720
 26186/100000: episode: 2624, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000038, mae: 0.002575, mean_q: 0.019943
 26196/100000: episode: 2625, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002953, mean_q: 0.018609
 26206/100000: episode: 2626, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000033, mae: 0.002031, mean_q: 0.019728
 26216/100000: episode: 2627, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000049, mae: 0.001639, mean_q: 0.019953
 26226/100000: episode: 2628, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.001959, mean_q: 0.019654
 26236/100000: episode: 2629, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.002546, mean_q: 0.019622
 26246/100000: episode: 2630, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000009, mae: 0.001342, mean_q: 0.019579
 26256/100000: episode: 2631, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000204, mae: 0.003087, mean_q: 0.020127
 26266/100000: episode: 2632, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002585, mean_q: 0.019207
 26276/100000: episode: 2633, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000029, mae: 0.001983, mean_q: 0.018992
 26286/100000: episode: 2634, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000016, mae: 0.002238, mean_q: 0.020435
 26296/100000: episode: 2635, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000031, mae: 0.002002, mean_q: 0.019107
 26306/100000: episode: 2636, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000077, mae: 0.003068, mean_q: 0.020238
 26316/100000: episode: 2637, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000011, mae: 0.001899, mean_q: 0.019603
 26326/100000: episode: 2638, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000016, mae: 0.002870, mean_q: 0.018370
 26336/100000: episode: 2639, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000229, mae: 0.003881, mean_q: 0.019877
 26346/100000: episode: 2640, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000247, mae: 0.004596, mean_q: 0.020959
 26356/100000: episode: 2641, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000014, mae: 0.001952, mean_q: 0.019605
 26366/100000: episode: 2642, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000208, mae: 0.003596, mean_q: 0.018675
 26376/100000: episode: 2643, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000037, mae: 0.002491, mean_q: 0.020157
 26386/100000: episode: 2644, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000209, mae: 0.003118, mean_q: 0.019368
 26396/100000: episode: 2645, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000249, mae: 0.003688, mean_q: 0.019558
 26406/100000: episode: 2646, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000081, mae: 0.004039, mean_q: 0.021041
 26416/100000: episode: 2647, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000009, mae: 0.001935, mean_q: 0.019291
 26426/100000: episode: 2648, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000058, mae: 0.003288, mean_q: 0.018477
 26436/100000: episode: 2649, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000203, mae: 0.003353, mean_q: 0.020628
 26446/100000: episode: 2650, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000011, mae: 0.002246, mean_q: 0.018439
 26456/100000: episode: 2651, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000030, mae: 0.001761, mean_q: 0.019075
 26466/100000: episode: 2652, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000025, mae: 0.002603, mean_q: 0.019646
 26476/100000: episode: 2653, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000251, mae: 0.003967, mean_q: 0.019536
 26486/100000: episode: 2654, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000104, mae: 0.003770, mean_q: 0.020038
 26496/100000: episode: 2655, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000014, mae: 0.001909, mean_q: 0.020180
 26506/100000: episode: 2656, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000275, mae: 0.004737, mean_q: 0.019843
 26516/100000: episode: 2657, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000055, mae: 0.002719, mean_q: 0.019759
 26526/100000: episode: 2658, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000060, mae: 0.002631, mean_q: 0.019271
 26536/100000: episode: 2659, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000031, mae: 0.001705, mean_q: 0.019916
 26546/100000: episode: 2660, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000051, mae: 0.001852, mean_q: 0.019566
 26556/100000: episode: 2661, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000013, mae: 0.002160, mean_q: 0.018791
 26566/100000: episode: 2662, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000007, mae: 0.002050, mean_q: 0.018267
 26576/100000: episode: 2663, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000224, mae: 0.003112, mean_q: 0.019426
 26586/100000: episode: 2664, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000228, mae: 0.003942, mean_q: 0.020429
 26596/100000: episode: 2665, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000078, mae: 0.003473, mean_q: 0.020444
 26606/100000: episode: 2666, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000225, mae: 0.003075, mean_q: 0.019801
 26616/100000: episode: 2667, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000039, mae: 0.002402, mean_q: 0.019704
 26626/100000: episode: 2668, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000055, mae: 0.002216, mean_q: 0.019327
 26636/100000: episode: 2669, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.002578, mean_q: 0.019315
 26646/100000: episode: 2670, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000037, mae: 0.002696, mean_q: 0.018559
 26656/100000: episode: 2671, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000015, mae: 0.001672, mean_q: 0.019332
 26666/100000: episode: 2672, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000438, mae: 0.005852, mean_q: 0.020846
 26676/100000: episode: 2673, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000055, mae: 0.002893, mean_q: 0.020066
 26686/100000: episode: 2674, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000036, mae: 0.002185, mean_q: 0.019101
[Info] 1-TH LEVEL FOUND: 0.019877564162015915, Considering 100/100 traces
 26696/100000: episode: 2675, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000203, mae: 0.002906, mean_q: 0.020121
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.019877564162015915
1
 26706/100000: episode: 2676, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000032, mae: 0.001960, mean_q: 0.020049
 26716/100000: episode: 2677, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000011, mae: 0.001445, mean_q: 0.019248
 26726/100000: episode: 2678, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000039, mae: 0.002346, mean_q: 0.019252
 26736/100000: episode: 2679, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002248, mean_q: 0.019315
 26746/100000: episode: 2680, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000052, mae: 0.002445, mean_q: 0.018796
 26756/100000: episode: 2681, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000202, mae: 0.003114, mean_q: 0.020331
 26766/100000: episode: 2682, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000012, mae: 0.001824, mean_q: 0.019096
 26776/100000: episode: 2683, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000227, mae: 0.003904, mean_q: 0.018507
 26786/100000: episode: 2684, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000015, mae: 0.001576, mean_q: 0.019340
 26796/100000: episode: 2685, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000205, mae: 0.003297, mean_q: 0.020087
 26806/100000: episode: 2686, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000231, mae: 0.003937, mean_q: 0.019944
 26816/100000: episode: 2687, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000204, mae: 0.002623, mean_q: 0.019488
 26826/100000: episode: 2688, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000031, mae: 0.001739, mean_q: 0.019512
 26836/100000: episode: 2689, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000404, mae: 0.005035, mean_q: 0.019841
 26846/100000: episode: 2690, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000057, mae: 0.003962, mean_q: 0.021651
 26856/100000: episode: 2691, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000076, mae: 0.002718, mean_q: 0.019507
 26866/100000: episode: 2692, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000058, mae: 0.002528, mean_q: 0.019862
 26876/100000: episode: 2693, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000040, mae: 0.002496, mean_q: 0.019297
 26886/100000: episode: 2694, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000056, mae: 0.002505, mean_q: 0.019373
 26896/100000: episode: 2695, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000056, mae: 0.002455, mean_q: 0.019989
 26906/100000: episode: 2696, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000223, mae: 0.003135, mean_q: 0.019249
 26916/100000: episode: 2697, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000270, mae: 0.004577, mean_q: 0.020296
 26926/100000: episode: 2698, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000039, mae: 0.003636, mean_q: 0.021565
 26936/100000: episode: 2699, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.002428, mean_q: 0.019154
 26946/100000: episode: 2700, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000012, mae: 0.001438, mean_q: 0.019462
 26956/100000: episode: 2701, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000221, mae: 0.003108, mean_q: 0.020308
 26966/100000: episode: 2702, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000040, mae: 0.003003, mean_q: 0.019475
 26976/100000: episode: 2703, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.002513, mean_q: 0.018395
 26986/100000: episode: 2704, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002307, mean_q: 0.019899
 26996/100000: episode: 2705, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002827, mean_q: 0.020595
Step 27000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.27000.hdf5
 27006/100000: episode: 2706, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000013, mae: 0.002110, mean_q: 0.019008
 27016/100000: episode: 2707, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000056, mae: 0.003130, mean_q: 0.018318
 27026/100000: episode: 2708, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000072, mae: 0.003671, mean_q: 0.021310
 27036/100000: episode: 2709, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000012, mae: 0.002241, mean_q: 0.019249
 27046/100000: episode: 2710, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000250, mae: 0.004154, mean_q: 0.019052
 27056/100000: episode: 2711, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000031, mae: 0.002106, mean_q: 0.020316
 27066/100000: episode: 2712, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000061, mae: 0.003031, mean_q: 0.019130
 27076/100000: episode: 2713, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000045, mae: 0.002805, mean_q: 0.019759
 27086/100000: episode: 2714, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.002405, mean_q: 0.019307
 27096/100000: episode: 2715, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000231, mae: 0.004146, mean_q: 0.018882
 27106/100000: episode: 2716, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000222, mae: 0.003461, mean_q: 0.020431
 27116/100000: episode: 2717, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000021, mae: 0.002482, mean_q: 0.019995
 27126/100000: episode: 2718, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000268, mae: 0.004214, mean_q: 0.019987
 27136/100000: episode: 2719, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000433, mae: 0.005664, mean_q: 0.021323
 27146/100000: episode: 2720, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000221, mae: 0.004458, mean_q: 0.022037
 27156/100000: episode: 2721, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000033, mae: 0.002127, mean_q: 0.019884
 27166/100000: episode: 2722, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000200, mae: 0.003006, mean_q: 0.018892
 27176/100000: episode: 2723, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000053, mae: 0.002700, mean_q: 0.020898
 27186/100000: episode: 2724, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000438, mae: 0.005361, mean_q: 0.020962
 27196/100000: episode: 2725, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000210, mae: 0.005002, mean_q: 0.022522
 27206/100000: episode: 2726, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000057, mae: 0.003009, mean_q: 0.020067
 27216/100000: episode: 2727, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000034, mae: 0.002745, mean_q: 0.019235
 27226/100000: episode: 2728, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000013, mae: 0.002421, mean_q: 0.019293
 27236/100000: episode: 2729, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000041, mae: 0.003203, mean_q: 0.018988
 27246/100000: episode: 2730, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000011, mae: 0.001176, mean_q: 0.019958
 27256/100000: episode: 2731, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000287, mae: 0.004204, mean_q: 0.019992
 27266/100000: episode: 2732, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000051, mae: 0.003351, mean_q: 0.021835
 27276/100000: episode: 2733, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000075, mae: 0.002545, mean_q: 0.020406
 27286/100000: episode: 2734, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000011, mae: 0.001863, mean_q: 0.019613
 27296/100000: episode: 2735, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000033, mae: 0.002774, mean_q: 0.018649
 27306/100000: episode: 2736, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000031, mae: 0.001697, mean_q: 0.020024
 27316/100000: episode: 2737, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000013, mae: 0.001790, mean_q: 0.019443
 27326/100000: episode: 2738, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000014, mae: 0.002291, mean_q: 0.018953
 27336/100000: episode: 2739, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000225, mae: 0.003622, mean_q: 0.019017
 27346/100000: episode: 2740, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000200, mae: 0.003421, mean_q: 0.021126
 27356/100000: episode: 2741, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000011, mae: 0.002210, mean_q: 0.019512
 27366/100000: episode: 2742, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000059, mae: 0.003693, mean_q: 0.018269
 27376/100000: episode: 2743, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000031, mae: 0.001936, mean_q: 0.020195
 27386/100000: episode: 2744, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000011, mae: 0.001960, mean_q: 0.018826
 27396/100000: episode: 2745, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000245, mae: 0.004021, mean_q: 0.020370
 27406/100000: episode: 2746, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000015, mae: 0.002154, mean_q: 0.020389
 27416/100000: episode: 2747, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000009, mae: 0.001721, mean_q: 0.018993
 27426/100000: episode: 2748, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000061, mae: 0.003354, mean_q: 0.018782
 27436/100000: episode: 2749, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000093, mae: 0.003004, mean_q: 0.019969
 27446/100000: episode: 2750, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000011, mae: 0.001905, mean_q: 0.020525
 27456/100000: episode: 2751, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000014, mae: 0.002564, mean_q: 0.018335
 27466/100000: episode: 2752, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000036, mae: 0.002212, mean_q: 0.019228
 27476/100000: episode: 2753, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000011, mae: 0.001830, mean_q: 0.018967
 27486/100000: episode: 2754, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000015, mae: 0.002388, mean_q: 0.018312
 27496/100000: episode: 2755, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000035, mae: 0.001938, mean_q: 0.019346
 27506/100000: episode: 2756, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.001980, mean_q: 0.019397
 27516/100000: episode: 2757, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.001962, mean_q: 0.019278
 27526/100000: episode: 2758, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000011, mae: 0.001403, mean_q: 0.019567
 27536/100000: episode: 2759, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002315, mean_q: 0.018901
 27546/100000: episode: 2760, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000059, mae: 0.002754, mean_q: 0.019903
 27556/100000: episode: 2761, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000057, mae: 0.002677, mean_q: 0.019893
 27566/100000: episode: 2762, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000016, mae: 0.001982, mean_q: 0.018864
 27576/100000: episode: 2763, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.002256, mean_q: 0.020036
 27586/100000: episode: 2764, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000206, mae: 0.003198, mean_q: 0.018921
 27596/100000: episode: 2765, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000040, mae: 0.002574, mean_q: 0.018958
 27606/100000: episode: 2766, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000009, mae: 0.001574, mean_q: 0.018964
 27616/100000: episode: 2767, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000219, mae: 0.003080, mean_q: 0.018354
 27626/100000: episode: 2768, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000281, mae: 0.005406, mean_q: 0.021365
 27636/100000: episode: 2769, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000057, mae: 0.003360, mean_q: 0.020244
 27646/100000: episode: 2770, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000081, mae: 0.003634, mean_q: 0.018614
 27656/100000: episode: 2771, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000208, mae: 0.003825, mean_q: 0.020563
 27666/100000: episode: 2772, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000246, mae: 0.004007, mean_q: 0.018452
 27676/100000: episode: 2773, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000052, mae: 0.002982, mean_q: 0.020750
 27686/100000: episode: 2774, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.002420, mean_q: 0.019261
 27696/100000: episode: 2775, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000247, mae: 0.004125, mean_q: 0.019640
[Info] 1-TH LEVEL FOUND: 0.01891964115202427, Considering 100/100 traces
 27706/100000: episode: 2776, duration: 0.717s, episode steps: 10, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000085, mae: 0.004694, mean_q: 0.021099
[Info] 2-TH LEVEL FOUND: 0.02022191509604454, Considering 100/100 traces
 27716/100000: episode: 2777, duration: 0.671s, episode steps: 10, steps per second: 15, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000225, mae: 0.003330, mean_q: 0.018999
[Info] 3-TH LEVEL FOUND: 0.020656311884522438, Considering 100/100 traces
 27726/100000: episode: 2778, duration: 0.723s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000096, mae: 0.003426, mean_q: 0.020454
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020656311884522438
3
 27736/100000: episode: 2779, duration: 0.498s, episode steps: 10, steps per second: 20, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000016, mae: 0.001962, mean_q: 0.020143
 27746/100000: episode: 2780, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000054, mae: 0.002682, mean_q: 0.018955
 27756/100000: episode: 2781, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000057, mae: 0.002593, mean_q: 0.019449
 27766/100000: episode: 2782, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000056, mae: 0.002390, mean_q: 0.019863
 27776/100000: episode: 2783, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000052, mae: 0.002402, mean_q: 0.019157
 27786/100000: episode: 2784, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000053, mae: 0.002625, mean_q: 0.018874
 27796/100000: episode: 2785, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000013, mae: 0.001898, mean_q: 0.018838
 27806/100000: episode: 2786, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000030, mae: 0.002191, mean_q: 0.018612
 27816/100000: episode: 2787, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000074, mae: 0.002711, mean_q: 0.019572
 27826/100000: episode: 2788, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.002385, mean_q: 0.019139
 27836/100000: episode: 2789, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000013, mae: 0.002146, mean_q: 0.018285
 27846/100000: episode: 2790, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000610, mae: 0.006942, mean_q: 0.020656
 27856/100000: episode: 2791, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000058, mae: 0.003543, mean_q: 0.020887
 27866/100000: episode: 2792, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000227, mae: 0.003466, mean_q: 0.019011
 27876/100000: episode: 2793, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000208, mae: 0.003357, mean_q: 0.020059
 27886/100000: episode: 2794, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000080, mae: 0.003628, mean_q: 0.020548
 27896/100000: episode: 2795, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000206, mae: 0.003166, mean_q: 0.020301
 27906/100000: episode: 2796, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000011, mae: 0.001385, mean_q: 0.019709
 27916/100000: episode: 2797, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000224, mae: 0.003491, mean_q: 0.018930
 27926/100000: episode: 2798, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000054, mae: 0.002368, mean_q: 0.019852
 27936/100000: episode: 2799, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000014, mae: 0.001721, mean_q: 0.019316
 27946/100000: episode: 2800, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000223, mae: 0.003326, mean_q: 0.019832
 27956/100000: episode: 2801, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000051, mae: 0.002229, mean_q: 0.019902
 27966/100000: episode: 2802, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000098, mae: 0.003258, mean_q: 0.019298
 27976/100000: episode: 2803, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000032, mae: 0.001902, mean_q: 0.020070
 27986/100000: episode: 2804, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000029, mae: 0.001308, mean_q: 0.019546
 27996/100000: episode: 2805, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000228, mae: 0.003286, mean_q: 0.019873
Step 28000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.28000.hdf5
 28006/100000: episode: 2806, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002300, mean_q: 0.020253
 28016/100000: episode: 2807, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000038, mae: 0.002699, mean_q: 0.018884
 28026/100000: episode: 2808, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000437, mae: 0.005509, mean_q: 0.020587
 28036/100000: episode: 2809, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000014, mae: 0.002631, mean_q: 0.021016
 28046/100000: episode: 2810, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000016, mae: 0.003182, mean_q: 0.017838
 28056/100000: episode: 2811, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000015, mae: 0.002132, mean_q: 0.018944
 28066/100000: episode: 2812, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.002427, mean_q: 0.020649
 28076/100000: episode: 2813, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000042, mae: 0.002953, mean_q: 0.018893
 28086/100000: episode: 2814, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000205, mae: 0.003462, mean_q: 0.020246
 28096/100000: episode: 2815, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000269, mae: 0.004153, mean_q: 0.020140
 28106/100000: episode: 2816, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.002314, mean_q: 0.020397
 28116/100000: episode: 2817, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000224, mae: 0.003429, mean_q: 0.018867
 28126/100000: episode: 2818, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000056, mae: 0.003110, mean_q: 0.020736
 28136/100000: episode: 2819, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000210, mae: 0.003439, mean_q: 0.019087
 28146/100000: episode: 2820, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001692, mean_q: 0.019926
 28156/100000: episode: 2821, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000610, mae: 0.006099, mean_q: 0.019401
 28166/100000: episode: 2822, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000082, mae: 0.005938, mean_q: 0.023505
 28176/100000: episode: 2823, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000455, mae: 0.005178, mean_q: 0.020515
 28186/100000: episode: 2824, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000221, mae: 0.003766, mean_q: 0.021403
 28196/100000: episode: 2825, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.002668, mean_q: 0.020956
 28206/100000: episode: 2826, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.002762, mean_q: 0.019836
 28216/100000: episode: 2827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000263, mae: 0.004289, mean_q: 0.021146
 28226/100000: episode: 2828, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000056, mae: 0.003149, mean_q: 0.021342
 28236/100000: episode: 2829, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.002711, mean_q: 0.019231
 28246/100000: episode: 2830, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000035, mae: 0.002485, mean_q: 0.019458
 28256/100000: episode: 2831, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000054, mae: 0.002415, mean_q: 0.019831
 28266/100000: episode: 2832, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000074, mae: 0.002639, mean_q: 0.020293
 28276/100000: episode: 2833, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000078, mae: 0.002822, mean_q: 0.020142
 28286/100000: episode: 2834, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000015, mae: 0.002077, mean_q: 0.019487
 28296/100000: episode: 2835, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000201, mae: 0.002832, mean_q: 0.019674
 28306/100000: episode: 2836, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000056, mae: 0.002718, mean_q: 0.020798
 28316/100000: episode: 2837, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000397, mae: 0.005363, mean_q: 0.021594
 28326/100000: episode: 2838, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.002326, mean_q: 0.019449
 28336/100000: episode: 2839, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002944, mean_q: 0.019080
 28346/100000: episode: 2840, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000095, mae: 0.003126, mean_q: 0.019522
 28356/100000: episode: 2841, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000017, mae: 0.002514, mean_q: 0.021267
 28366/100000: episode: 2842, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000072, mae: 0.002547, mean_q: 0.019912
 28376/100000: episode: 2843, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000240, mae: 0.003277, mean_q: 0.020351
 28386/100000: episode: 2844, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000202, mae: 0.003647, mean_q: 0.021469
 28396/100000: episode: 2845, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000035, mae: 0.002105, mean_q: 0.020134
 28406/100000: episode: 2846, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000054, mae: 0.002285, mean_q: 0.019898
 28416/100000: episode: 2847, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000053, mae: 0.002111, mean_q: 0.020488
 28426/100000: episode: 2848, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000115, mae: 0.003424, mean_q: 0.020607
 28436/100000: episode: 2849, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000264, mae: 0.005726, mean_q: 0.022754
 28446/100000: episode: 2850, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000241, mae: 0.004717, mean_q: 0.022438
 28456/100000: episode: 2851, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000061, mae: 0.003214, mean_q: 0.019951
 28466/100000: episode: 2852, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002656, mean_q: 0.020160
 28476/100000: episode: 2853, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000033, mae: 0.001880, mean_q: 0.020344
 28486/100000: episode: 2854, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000267, mae: 0.004237, mean_q: 0.020050
 28496/100000: episode: 2855, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000224, mae: 0.004640, mean_q: 0.022531
 28506/100000: episode: 2856, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000010, mae: 0.001967, mean_q: 0.021180
 28516/100000: episode: 2857, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000013, mae: 0.003036, mean_q: 0.018785
 28526/100000: episode: 2858, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002356, mean_q: 0.020176
 28536/100000: episode: 2859, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000074, mae: 0.002527, mean_q: 0.020391
 28546/100000: episode: 2860, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000036, mae: 0.002280, mean_q: 0.020141
 28556/100000: episode: 2861, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000099, mae: 0.003497, mean_q: 0.019961
 28566/100000: episode: 2862, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000219, mae: 0.003096, mean_q: 0.021071
 28576/100000: episode: 2863, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000029, mae: 0.001698, mean_q: 0.020990
 28586/100000: episode: 2864, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000061, mae: 0.003278, mean_q: 0.019795
 28596/100000: episode: 2865, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000225, mae: 0.003464, mean_q: 0.020234
 28606/100000: episode: 2866, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000225, mae: 0.003625, mean_q: 0.021161
 28616/100000: episode: 2867, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000058, mae: 0.002724, mean_q: 0.020889
 28626/100000: episode: 2868, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000039, mae: 0.002266, mean_q: 0.020304
 28636/100000: episode: 2869, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000033, mae: 0.002364, mean_q: 0.019749
 28646/100000: episode: 2870, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000075, mae: 0.002893, mean_q: 0.019908
 28656/100000: episode: 2871, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000063, mae: 0.003161, mean_q: 0.020860
 28666/100000: episode: 2872, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000077, mae: 0.002877, mean_q: 0.020281
 28676/100000: episode: 2873, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000208, mae: 0.003365, mean_q: 0.020999
 28686/100000: episode: 2874, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000427, mae: 0.004663, mean_q: 0.021276
 28696/100000: episode: 2875, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000220, mae: 0.004739, mean_q: 0.022965
 28706/100000: episode: 2876, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000458, mae: 0.005444, mean_q: 0.020334
 28716/100000: episode: 2877, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.003483, mean_q: 0.022866
 28726/100000: episode: 2878, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000243, mae: 0.004250, mean_q: 0.022210
[Info] 1-TH LEVEL FOUND: 0.020280223339796066, Considering 100/100 traces
 28736/100000: episode: 2879, duration: 0.715s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000060, mae: 0.003136, mean_q: 0.020594
[Info] 2-TH LEVEL FOUND: 0.02173713594675064, Considering 100/100 traces
 28746/100000: episode: 2880, duration: 0.696s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000206, mae: 0.003169, mean_q: 0.020709
[Info] 3-TH LEVEL FOUND: 0.022926338016986847, Considering 100/100 traces
 28756/100000: episode: 2881, duration: 0.668s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000281, mae: 0.004759, mean_q: 0.022071
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022926338016986847
3
 28766/100000: episode: 2882, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000078, mae: 0.003918, mean_q: 0.022544
 28776/100000: episode: 2883, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000038, mae: 0.002563, mean_q: 0.020632
 28786/100000: episode: 2884, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.002837, mean_q: 0.020377
 28796/100000: episode: 2885, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000265, mae: 0.004847, mean_q: 0.022389
 28806/100000: episode: 2886, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000244, mae: 0.004587, mean_q: 0.022554
 28816/100000: episode: 2887, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000034, mae: 0.002338, mean_q: 0.022212
 28826/100000: episode: 2888, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000220, mae: 0.002984, mean_q: 0.021770
 28836/100000: episode: 2889, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000077, mae: 0.003247, mean_q: 0.021437
 28846/100000: episode: 2890, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000102, mae: 0.003869, mean_q: 0.021365
 28856/100000: episode: 2891, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000060, mae: 0.003115, mean_q: 0.021508
 28866/100000: episode: 2892, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000244, mae: 0.003920, mean_q: 0.020717
 28876/100000: episode: 2893, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000079, mae: 0.004454, mean_q: 0.023372
 28886/100000: episode: 2894, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000038, mae: 0.002774, mean_q: 0.021143
 28896/100000: episode: 2895, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000055, mae: 0.003560, mean_q: 0.019833
 28906/100000: episode: 2896, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000057, mae: 0.002622, mean_q: 0.021516
 28916/100000: episode: 2897, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000054, mae: 0.002475, mean_q: 0.021844
 28926/100000: episode: 2898, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.001872, mean_q: 0.020936
 28936/100000: episode: 2899, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000078, mae: 0.003095, mean_q: 0.021213
 28946/100000: episode: 2900, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000056, mae: 0.002819, mean_q: 0.021009
 28956/100000: episode: 2901, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002004, mean_q: 0.020774
 28966/100000: episode: 2902, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002093, mean_q: 0.021108
 28976/100000: episode: 2903, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000059, mae: 0.002530, mean_q: 0.021226
 28986/100000: episode: 2904, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002508, mean_q: 0.021662
 28996/100000: episode: 2905, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000073, mae: 0.002645, mean_q: 0.020943
Step 29000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.29000.hdf5
 29006/100000: episode: 2906, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000014, mae: 0.001574, mean_q: 0.021318
 29016/100000: episode: 2907, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000202, mae: 0.002780, mean_q: 0.021459
 29026/100000: episode: 2908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000219, mae: 0.003143, mean_q: 0.021830
 29036/100000: episode: 2909, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000243, mae: 0.004895, mean_q: 0.023235
 29046/100000: episode: 2910, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000053, mae: 0.002540, mean_q: 0.021449
 29056/100000: episode: 2911, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000012, mae: 0.002147, mean_q: 0.020537
 29066/100000: episode: 2912, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000014, mae: 0.002078, mean_q: 0.020514
 29076/100000: episode: 2913, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000221, mae: 0.002839, mean_q: 0.021335
 29086/100000: episode: 2914, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000206, mae: 0.003967, mean_q: 0.022685
 29096/100000: episode: 2915, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000245, mae: 0.004741, mean_q: 0.022959
 29106/100000: episode: 2916, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000034, mae: 0.002453, mean_q: 0.021508
 29116/100000: episode: 2917, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000304, mae: 0.005124, mean_q: 0.020937
 29126/100000: episode: 2918, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000419, mae: 0.007287, mean_q: 0.024361
 29136/100000: episode: 2919, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000223, mae: 0.003760, mean_q: 0.022622
 29146/100000: episode: 2920, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000040, mae: 0.002663, mean_q: 0.021467
 29156/100000: episode: 2921, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002414, mean_q: 0.021926
 29166/100000: episode: 2922, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000074, mae: 0.002724, mean_q: 0.021563
 29176/100000: episode: 2923, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000077, mae: 0.003076, mean_q: 0.022095
 29186/100000: episode: 2924, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000055, mae: 0.002474, mean_q: 0.021386
 29196/100000: episode: 2925, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000580, mae: 0.006285, mean_q: 0.023244
 29206/100000: episode: 2926, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000022, mae: 0.003524, mean_q: 0.023623
 29216/100000: episode: 2927, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000015, mae: 0.003469, mean_q: 0.019554
 29226/100000: episode: 2928, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000236, mae: 0.003548, mean_q: 0.022026
 29236/100000: episode: 2929, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000079, mae: 0.004801, mean_q: 0.024131
 29246/100000: episode: 2930, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000137, mae: 0.004257, mean_q: 0.021431
 29256/100000: episode: 2931, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000391, mae: 0.004126, mean_q: 0.022248
 29266/100000: episode: 2932, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000250, mae: 0.005720, mean_q: 0.024192
 29276/100000: episode: 2933, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000057, mae: 0.003424, mean_q: 0.021949
 29286/100000: episode: 2934, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000414, mae: 0.005438, mean_q: 0.020882
 29296/100000: episode: 2935, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000229, mae: 0.006118, mean_q: 0.025148
 29306/100000: episode: 2936, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000226, mae: 0.004110, mean_q: 0.022759
 29316/100000: episode: 2937, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000225, mae: 0.003744, mean_q: 0.021853
 29326/100000: episode: 2938, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000059, mae: 0.002998, mean_q: 0.022951
 29336/100000: episode: 2939, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000262, mae: 0.004457, mean_q: 0.023309
 29346/100000: episode: 2940, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000038, mae: 0.002631, mean_q: 0.022660
 29356/100000: episode: 2941, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000286, mae: 0.004917, mean_q: 0.022310
 29366/100000: episode: 2942, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.003004, mean_q: 0.023878
 29376/100000: episode: 2943, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000244, mae: 0.003956, mean_q: 0.022077
 29386/100000: episode: 2944, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000259, mae: 0.004498, mean_q: 0.023799
 29396/100000: episode: 2945, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000245, mae: 0.005340, mean_q: 0.024754
 29406/100000: episode: 2946, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000131, mae: 0.003679, mean_q: 0.023114
 29416/100000: episode: 2947, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000037, mae: 0.002169, mean_q: 0.022843
 29426/100000: episode: 2948, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000031, mae: 0.001673, mean_q: 0.022734
 29436/100000: episode: 2949, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000039, mae: 0.002682, mean_q: 0.022362
 29446/100000: episode: 2950, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000056, mae: 0.003113, mean_q: 0.022063
 29456/100000: episode: 2951, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000073, mae: 0.002831, mean_q: 0.022248
 29466/100000: episode: 2952, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000098, mae: 0.003598, mean_q: 0.023024
 29476/100000: episode: 2953, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000228, mae: 0.003875, mean_q: 0.023159
 29486/100000: episode: 2954, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002281, mean_q: 0.022832
 29496/100000: episode: 2955, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000224, mae: 0.003422, mean_q: 0.022717
 29506/100000: episode: 2956, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000245, mae: 0.004286, mean_q: 0.023561
 29516/100000: episode: 2957, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000109, mae: 0.003936, mean_q: 0.024066
 29526/100000: episode: 2958, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000031, mae: 0.002087, mean_q: 0.023198
 29536/100000: episode: 2959, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000226, mae: 0.003988, mean_q: 0.022067
 29546/100000: episode: 2960, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000470, mae: 0.007302, mean_q: 0.024935
 29556/100000: episode: 2961, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000060, mae: 0.003660, mean_q: 0.024310
 29566/100000: episode: 2962, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000039, mae: 0.003118, mean_q: 0.022145
 29576/100000: episode: 2963, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000053, mae: 0.002970, mean_q: 0.022114
 29586/100000: episode: 2964, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000020, mae: 0.002236, mean_q: 0.022613
 29596/100000: episode: 2965, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000388, mae: 0.004422, mean_q: 0.023572
 29606/100000: episode: 2966, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000059, mae: 0.003195, mean_q: 0.022636
 29616/100000: episode: 2967, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000043, mae: 0.003569, mean_q: 0.021844
 29626/100000: episode: 2968, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000050, mae: 0.002275, mean_q: 0.022282
 29636/100000: episode: 2969, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000018, mae: 0.002313, mean_q: 0.022364
 29646/100000: episode: 2970, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000445, mae: 0.005323, mean_q: 0.023168
 29656/100000: episode: 2971, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000062, mae: 0.004531, mean_q: 0.024889
 29666/100000: episode: 2972, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000058, mae: 0.003647, mean_q: 0.021604
 29676/100000: episode: 2973, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000414, mae: 0.005922, mean_q: 0.024045
 29686/100000: episode: 2974, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000151, mae: 0.005324, mean_q: 0.024716
 29696/100000: episode: 2975, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000123, mae: 0.004713, mean_q: 0.023868
 29706/100000: episode: 2976, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000223, mae: 0.003206, mean_q: 0.023191
 29716/100000: episode: 2977, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000076, mae: 0.003365, mean_q: 0.023955
 29726/100000: episode: 2978, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000240, mae: 0.003557, mean_q: 0.023563
 29736/100000: episode: 2979, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000018, mae: 0.002393, mean_q: 0.022813
 29746/100000: episode: 2980, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000225, mae: 0.004583, mean_q: 0.021518
 29756/100000: episode: 2981, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000451, mae: 0.006265, mean_q: 0.024208
[Info] 1-TH LEVEL FOUND: 0.022713936865329742, Considering 100/100 traces
 29766/100000: episode: 2982, duration: 0.749s, episode steps: 10, steps per second: 13, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000035, mae: 0.003447, mean_q: 0.025443
[Info] 2-TH LEVEL FOUND: 0.023585092276334763, Considering 100/100 traces
 29776/100000: episode: 2983, duration: 0.665s, episode steps: 10, steps per second: 15, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000095, mae: 0.003634, mean_q: 0.022520
[Info] 3-TH LEVEL FOUND: 0.027104079723358154, Considering 100/100 traces
 29786/100000: episode: 2984, duration: 0.675s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000423, mae: 0.005182, mean_q: 0.024456
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.027104079723358154
3
 29796/100000: episode: 2985, duration: 0.493s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000042, mae: 0.004664, mean_q: 0.026444
 29806/100000: episode: 2986, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000205, mae: 0.003707, mean_q: 0.022829
 29816/100000: episode: 2987, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000018, mae: 0.002092, mean_q: 0.023557
 29826/100000: episode: 2988, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002599, mean_q: 0.023359
 29836/100000: episode: 2989, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000093, mae: 0.003446, mean_q: 0.022803
 29846/100000: episode: 2990, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.002622, mean_q: 0.023595
 29856/100000: episode: 2991, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000245, mae: 0.004226, mean_q: 0.023998
 29866/100000: episode: 2992, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000017, mae: 0.002449, mean_q: 0.022766
 29876/100000: episode: 2993, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000120, mae: 0.004574, mean_q: 0.022544
 29886/100000: episode: 2994, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000011, mae: 0.002271, mean_q: 0.024616
 29896/100000: episode: 2995, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000080, mae: 0.003541, mean_q: 0.022769
 29906/100000: episode: 2996, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000062, mae: 0.003109, mean_q: 0.023205
 29916/100000: episode: 2997, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000245, mae: 0.003990, mean_q: 0.023187
 29926/100000: episode: 2998, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002670, mean_q: 0.022995
 29936/100000: episode: 2999, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000444, mae: 0.005363, mean_q: 0.024075
 29946/100000: episode: 3000, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000038, mae: 0.003713, mean_q: 0.025420
 29956/100000: episode: 3001, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.003573, mean_q: 0.021706
 29966/100000: episode: 3002, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000096, mae: 0.003761, mean_q: 0.023055
 29976/100000: episode: 3003, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000038, mae: 0.002797, mean_q: 0.024088
 29986/100000: episode: 3004, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000035, mae: 0.002973, mean_q: 0.022060
 29996/100000: episode: 3005, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000260, mae: 0.004134, mean_q: 0.023037
Step 30000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.30000.hdf5
 30006/100000: episode: 3006, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.003888, mean_q: 0.025008
 30016/100000: episode: 3007, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000242, mae: 0.003688, mean_q: 0.023269
 30026/100000: episode: 3008, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000227, mae: 0.003734, mean_q: 0.023201
 30036/100000: episode: 3009, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000076, mae: 0.002716, mean_q: 0.023307
 30046/100000: episode: 3010, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000016, mae: 0.001722, mean_q: 0.023431
 30056/100000: episode: 3011, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000042, mae: 0.003280, mean_q: 0.022425
 30066/100000: episode: 3012, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000219, mae: 0.003562, mean_q: 0.022240
 30076/100000: episode: 3013, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000239, mae: 0.004669, mean_q: 0.024800
 30086/100000: episode: 3014, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.002856, mean_q: 0.023518
 30096/100000: episode: 3015, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000218, mae: 0.003785, mean_q: 0.021820
 30106/100000: episode: 3016, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000079, mae: 0.004170, mean_q: 0.024681
 30116/100000: episode: 3017, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000197, mae: 0.002479, mean_q: 0.023265
 30126/100000: episode: 3018, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000078, mae: 0.003534, mean_q: 0.022350
 30136/100000: episode: 3019, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000226, mae: 0.003960, mean_q: 0.023674
 30146/100000: episode: 3020, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000058, mae: 0.003374, mean_q: 0.024142
 30156/100000: episode: 3021, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000387, mae: 0.004281, mean_q: 0.022350
 30166/100000: episode: 3022, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000426, mae: 0.007268, mean_q: 0.026763
 30176/100000: episode: 3023, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000061, mae: 0.003905, mean_q: 0.024376
 30186/100000: episode: 3024, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.003027, mean_q: 0.022264
 30196/100000: episode: 3025, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000469, mae: 0.006663, mean_q: 0.024773
 30206/100000: episode: 3026, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000020, mae: 0.003068, mean_q: 0.024832
 30216/100000: episode: 3027, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.004548, mean_q: 0.021032
 30226/100000: episode: 3028, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000015, mae: 0.002139, mean_q: 0.023435
 30236/100000: episode: 3029, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000263, mae: 0.004531, mean_q: 0.024004
 30246/100000: episode: 3030, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000070, mae: 0.002540, mean_q: 0.023424
 30256/100000: episode: 3031, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000238, mae: 0.003337, mean_q: 0.023535
 30266/100000: episode: 3032, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000018, mae: 0.002452, mean_q: 0.024401
 30276/100000: episode: 3033, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000011, mae: 0.002190, mean_q: 0.022338
 30286/100000: episode: 3034, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000051, mae: 0.002324, mean_q: 0.023327
 30296/100000: episode: 3035, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000217, mae: 0.002986, mean_q: 0.023901
 30306/100000: episode: 3036, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000058, mae: 0.002785, mean_q: 0.023825
 30316/100000: episode: 3037, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000032, mae: 0.001827, mean_q: 0.023412
 30326/100000: episode: 3038, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000222, mae: 0.003656, mean_q: 0.022843
 30336/100000: episode: 3039, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000021, mae: 0.002719, mean_q: 0.024248
 30346/100000: episode: 3040, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000039, mae: 0.002978, mean_q: 0.022658
 30356/100000: episode: 3041, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.003341, mean_q: 0.022101
 30366/100000: episode: 3042, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000019, mae: 0.002281, mean_q: 0.022856
 30376/100000: episode: 3043, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000228, mae: 0.003980, mean_q: 0.023186
 30386/100000: episode: 3044, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000076, mae: 0.003814, mean_q: 0.024492
 30396/100000: episode: 3045, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000020, mae: 0.002508, mean_q: 0.022761
 30406/100000: episode: 3046, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.003159, mean_q: 0.021908
 30416/100000: episode: 3047, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000243, mae: 0.004080, mean_q: 0.023382
 30426/100000: episode: 3048, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000039, mae: 0.002700, mean_q: 0.023339
 30436/100000: episode: 3049, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000246, mae: 0.004190, mean_q: 0.022944
 30446/100000: episode: 3050, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000257, mae: 0.004208, mean_q: 0.024053
 30456/100000: episode: 3051, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000039, mae: 0.002743, mean_q: 0.023810
 30466/100000: episode: 3052, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.003358, mean_q: 0.022146
 30476/100000: episode: 3053, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.002413, mean_q: 0.022354
 30486/100000: episode: 3054, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000115, mae: 0.003784, mean_q: 0.022773
 30496/100000: episode: 3055, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000076, mae: 0.003878, mean_q: 0.024426
 30506/100000: episode: 3056, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000013, mae: 0.001943, mean_q: 0.022562
 30516/100000: episode: 3057, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000038, mae: 0.003233, mean_q: 0.021705
 30526/100000: episode: 3058, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.002752, mean_q: 0.023114
 30536/100000: episode: 3059, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.002399, mean_q: 0.022359
 30546/100000: episode: 3060, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.002472, mean_q: 0.022790
 30556/100000: episode: 3061, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000258, mae: 0.003701, mean_q: 0.023154
 30566/100000: episode: 3062, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000204, mae: 0.003547, mean_q: 0.023728
 30576/100000: episode: 3063, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000043, mae: 0.002963, mean_q: 0.022541
 30586/100000: episode: 3064, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000034, mae: 0.002337, mean_q: 0.022268
 30596/100000: episode: 3065, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000032, mae: 0.002310, mean_q: 0.022078
 30606/100000: episode: 3066, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000264, mae: 0.004274, mean_q: 0.022638
 30616/100000: episode: 3067, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000442, mae: 0.006490, mean_q: 0.025028
 30626/100000: episode: 3068, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000226, mae: 0.004659, mean_q: 0.024582
 30636/100000: episode: 3069, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000054, mae: 0.002701, mean_q: 0.022494
 30646/100000: episode: 3070, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000015, mae: 0.001934, mean_q: 0.022578
 30656/100000: episode: 3071, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000034, mae: 0.002248, mean_q: 0.022415
 30666/100000: episode: 3072, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000036, mae: 0.002304, mean_q: 0.022647
 30676/100000: episode: 3073, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000211, mae: 0.003742, mean_q: 0.022739
 30686/100000: episode: 3074, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000054, mae: 0.002585, mean_q: 0.023417
 30696/100000: episode: 3075, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000448, mae: 0.005713, mean_q: 0.023719
 30706/100000: episode: 3076, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000062, mae: 0.003812, mean_q: 0.024276
 30716/100000: episode: 3077, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000051, mae: 0.002695, mean_q: 0.022138
 30726/100000: episode: 3078, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000607, mae: 0.006128, mean_q: 0.024273
 30736/100000: episode: 3079, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000047, mae: 0.005198, mean_q: 0.026583
 30746/100000: episode: 3080, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000035, mae: 0.003318, mean_q: 0.021714
 30756/100000: episode: 3081, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.002577, mean_q: 0.021961
 30766/100000: episode: 3082, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000079, mae: 0.003314, mean_q: 0.022874
 30776/100000: episode: 3083, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000077, mae: 0.003100, mean_q: 0.023558
 30786/100000: episode: 3084, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000032, mae: 0.002277, mean_q: 0.024000
[Info] 1-TH LEVEL FOUND: 0.021468868479132652, Considering 100/100 traces
 30796/100000: episode: 3085, duration: 0.716s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000038, mae: 0.003140, mean_q: 0.022033
[Info] 2-TH LEVEL FOUND: 0.022369645535945892, Considering 100/100 traces
 30806/100000: episode: 3086, duration: 0.704s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.002457, mean_q: 0.021918
[Info] 3-TH LEVEL FOUND: 0.024899104610085487, Considering 100/100 traces
 30816/100000: episode: 3087, duration: 0.675s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000317, mae: 0.004980, mean_q: 0.023307
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024899104610085487
3
 30826/100000: episode: 3088, duration: 0.488s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000058, mae: 0.003963, mean_q: 0.024782
 30836/100000: episode: 3089, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000222, mae: 0.003341, mean_q: 0.022952
 30846/100000: episode: 3090, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000052, mae: 0.002486, mean_q: 0.022577
 30856/100000: episode: 3091, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000219, mae: 0.003187, mean_q: 0.023526
 30866/100000: episode: 3092, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000200, mae: 0.002763, mean_q: 0.023577
 30876/100000: episode: 3093, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000037, mae: 0.002232, mean_q: 0.023075
 30886/100000: episode: 3094, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000074, mae: 0.003264, mean_q: 0.022314
 30896/100000: episode: 3095, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000048, mae: 0.001790, mean_q: 0.023045
 30906/100000: episode: 3096, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000220, mae: 0.003420, mean_q: 0.022254
 30916/100000: episode: 3097, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000033, mae: 0.002534, mean_q: 0.023880
 30926/100000: episode: 3098, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000208, mae: 0.003556, mean_q: 0.023058
 30936/100000: episode: 3099, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000073, mae: 0.003383, mean_q: 0.021848
 30946/100000: episode: 3100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000204, mae: 0.003231, mean_q: 0.023365
 30956/100000: episode: 3101, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000091, mae: 0.003583, mean_q: 0.024115
 30966/100000: episode: 3102, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000061, mae: 0.003250, mean_q: 0.022983
 30976/100000: episode: 3103, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000076, mae: 0.003033, mean_q: 0.022888
 30986/100000: episode: 3104, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000283, mae: 0.005193, mean_q: 0.024042
 30996/100000: episode: 3105, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000095, mae: 0.003594, mean_q: 0.022914
Step 31000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.31000.hdf5
 31006/100000: episode: 3106, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000031, mae: 0.001973, mean_q: 0.022652
 31016/100000: episode: 3107, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000468, mae: 0.005779, mean_q: 0.023575
 31026/100000: episode: 3108, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000039, mae: 0.003365, mean_q: 0.024515
 31036/100000: episode: 3109, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000022, mae: 0.002961, mean_q: 0.022090
 31046/100000: episode: 3110, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000240, mae: 0.004221, mean_q: 0.024157
 31056/100000: episode: 3111, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000058, mae: 0.003250, mean_q: 0.023622
 31066/100000: episode: 3112, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.003401, mean_q: 0.022017
 31076/100000: episode: 3113, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000074, mae: 0.003029, mean_q: 0.022535
 31086/100000: episode: 3114, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000074, mae: 0.003389, mean_q: 0.024062
 31096/100000: episode: 3115, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000263, mae: 0.004148, mean_q: 0.023350
 31106/100000: episode: 3116, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000425, mae: 0.005313, mean_q: 0.024413
 31116/100000: episode: 3117, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000234, mae: 0.004111, mean_q: 0.024841
 31126/100000: episode: 3118, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000018, mae: 0.002573, mean_q: 0.024021
 31136/100000: episode: 3119, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000009, mae: 0.002477, mean_q: 0.021738
 31146/100000: episode: 3120, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000018, mae: 0.002068, mean_q: 0.022884
 31156/100000: episode: 3121, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000077, mae: 0.003177, mean_q: 0.022885
 31166/100000: episode: 3122, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002552, mean_q: 0.022861
 31176/100000: episode: 3123, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000015, mae: 0.002767, mean_q: 0.021729
 31186/100000: episode: 3124, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000092, mae: 0.003448, mean_q: 0.022239
 31196/100000: episode: 3125, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000221, mae: 0.004230, mean_q: 0.024460
 31206/100000: episode: 3126, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000300, mae: 0.004755, mean_q: 0.023644
 31216/100000: episode: 3127, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000102, mae: 0.005379, mean_q: 0.025304
 31226/100000: episode: 3128, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000096, mae: 0.003594, mean_q: 0.023218
 31236/100000: episode: 3129, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002303, mean_q: 0.023153
 31246/100000: episode: 3130, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000073, mae: 0.003040, mean_q: 0.022669
 31256/100000: episode: 3131, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000064, mae: 0.003210, mean_q: 0.023001
 31266/100000: episode: 3132, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000036, mae: 0.002218, mean_q: 0.023282
 31276/100000: episode: 3133, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.002348, mean_q: 0.022899
 31286/100000: episode: 3134, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000075, mae: 0.002716, mean_q: 0.023277
 31296/100000: episode: 3135, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000096, mae: 0.003406, mean_q: 0.023375
 31306/100000: episode: 3136, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000008, mae: 0.001441, mean_q: 0.022630
 31316/100000: episode: 3137, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000032, mae: 0.002876, mean_q: 0.021693
 31326/100000: episode: 3138, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000058, mae: 0.002873, mean_q: 0.023013
 31336/100000: episode: 3139, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000092, mae: 0.002910, mean_q: 0.022916
 31346/100000: episode: 3140, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000019, mae: 0.002347, mean_q: 0.022562
 31356/100000: episode: 3141, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000033, mae: 0.002828, mean_q: 0.021544
 31366/100000: episode: 3142, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000019, mae: 0.002239, mean_q: 0.022424
 31376/100000: episode: 3143, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000011, mae: 0.002020, mean_q: 0.021848
 31386/100000: episode: 3144, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002704, mean_q: 0.022093
 31396/100000: episode: 3145, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000036, mae: 0.002432, mean_q: 0.023015
 31406/100000: episode: 3146, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000132, mae: 0.003786, mean_q: 0.022598
 31416/100000: episode: 3147, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000036, mae: 0.002350, mean_q: 0.022414
 31426/100000: episode: 3148, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000114, mae: 0.003867, mean_q: 0.021773
 31436/100000: episode: 3149, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000034, mae: 0.002794, mean_q: 0.023794
 31446/100000: episode: 3150, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000020, mae: 0.002603, mean_q: 0.022167
 31456/100000: episode: 3151, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000020, mae: 0.003352, mean_q: 0.020848
 31466/100000: episode: 3152, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000042, mae: 0.002852, mean_q: 0.021890
 31476/100000: episode: 3153, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000033, mae: 0.001668, mean_q: 0.022247
 31486/100000: episode: 3154, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000074, mae: 0.002507, mean_q: 0.022355
 31496/100000: episode: 3155, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000076, mae: 0.003087, mean_q: 0.022813
 31506/100000: episode: 3156, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000030, mae: 0.002345, mean_q: 0.021335
 31516/100000: episode: 3157, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000300, mae: 0.004762, mean_q: 0.021968
 31526/100000: episode: 3158, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000221, mae: 0.005508, mean_q: 0.025378
 31536/100000: episode: 3159, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002604, mean_q: 0.022357
 31546/100000: episode: 3160, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000034, mae: 0.002433, mean_q: 0.021750
 31556/100000: episode: 3161, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000230, mae: 0.004491, mean_q: 0.023549
 31566/100000: episode: 3162, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000200, mae: 0.003119, mean_q: 0.023242
 31576/100000: episode: 3163, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000075, mae: 0.002833, mean_q: 0.022187
 31586/100000: episode: 3164, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000471, mae: 0.007494, mean_q: 0.024920
 31596/100000: episode: 3165, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000056, mae: 0.003835, mean_q: 0.024446
 31606/100000: episode: 3166, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000015, mae: 0.002648, mean_q: 0.021698
 31616/100000: episode: 3167, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000032, mae: 0.001918, mean_q: 0.022556
 31626/100000: episode: 3168, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000034, mae: 0.002456, mean_q: 0.023593
 31636/100000: episode: 3169, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002498, mean_q: 0.022266
 31646/100000: episode: 3170, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000042, mae: 0.003473, mean_q: 0.021721
 31656/100000: episode: 3171, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000225, mae: 0.003802, mean_q: 0.022102
 31666/100000: episode: 3172, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000018, mae: 0.002156, mean_q: 0.022524
 31676/100000: episode: 3173, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000244, mae: 0.004333, mean_q: 0.021383
 31686/100000: episode: 3174, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000014, mae: 0.002390, mean_q: 0.023773
 31696/100000: episode: 3175, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000054, mae: 0.002787, mean_q: 0.021843
 31706/100000: episode: 3176, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.002351, mean_q: 0.021990
 31716/100000: episode: 3177, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000205, mae: 0.003126, mean_q: 0.022123
 31726/100000: episode: 3178, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000016, mae: 0.001783, mean_q: 0.022378
 31736/100000: episode: 3179, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000059, mae: 0.003149, mean_q: 0.021731
 31746/100000: episode: 3180, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000395, mae: 0.004787, mean_q: 0.022658
 31756/100000: episode: 3181, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000031, mae: 0.002502, mean_q: 0.023550
 31766/100000: episode: 3182, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000269, mae: 0.004693, mean_q: 0.021955
 31776/100000: episode: 3183, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000056, mae: 0.003136, mean_q: 0.023442
 31786/100000: episode: 3184, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000056, mae: 0.002792, mean_q: 0.021908
 31796/100000: episode: 3185, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000054, mae: 0.002688, mean_q: 0.021777
 31806/100000: episode: 3186, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000036, mae: 0.002396, mean_q: 0.021930
 31816/100000: episode: 3187, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002780, mean_q: 0.021407
[Info] 1-TH LEVEL FOUND: 0.024368807673454285, Considering 100/100 traces
 31826/100000: episode: 3188, duration: 0.727s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000223, mae: 0.004074, mean_q: 0.023367
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024368807673454285
1
 31836/100000: episode: 3189, duration: 0.493s, episode steps: 10, steps per second: 20, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000012, mae: 0.002222, mean_q: 0.022721
 31846/100000: episode: 3190, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000241, mae: 0.003955, mean_q: 0.021434
 31856/100000: episode: 3191, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000053, mae: 0.004090, mean_q: 0.024948
 31866/100000: episode: 3192, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000059, mae: 0.003347, mean_q: 0.022241
 31876/100000: episode: 3193, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000116, mae: 0.004101, mean_q: 0.021890
 31886/100000: episode: 3194, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000264, mae: 0.005907, mean_q: 0.024799
 31896/100000: episode: 3195, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000055, mae: 0.003143, mean_q: 0.023273
 31906/100000: episode: 3196, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000203, mae: 0.004012, mean_q: 0.020895
 31916/100000: episode: 3197, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000057, mae: 0.002767, mean_q: 0.022731
 31926/100000: episode: 3198, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002407, mean_q: 0.021989
 31936/100000: episode: 3199, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000220, mae: 0.003254, mean_q: 0.022082
 31946/100000: episode: 3200, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000036, mae: 0.002990, mean_q: 0.023787
 31956/100000: episode: 3201, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000016, mae: 0.002343, mean_q: 0.021873
 31966/100000: episode: 3202, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000031, mae: 0.002308, mean_q: 0.021657
 31976/100000: episode: 3203, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000222, mae: 0.003363, mean_q: 0.022473
 31986/100000: episode: 3204, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000017, mae: 0.002321, mean_q: 0.021936
 31996/100000: episode: 3205, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000247, mae: 0.004668, mean_q: 0.021258
Step 32000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.32000.hdf5
 32006/100000: episode: 3206, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000068, mae: 0.003058, mean_q: 0.023543
 32016/100000: episode: 3207, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000261, mae: 0.004679, mean_q: 0.023624
 32026/100000: episode: 3208, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000073, mae: 0.004071, mean_q: 0.024463
 32036/100000: episode: 3209, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.002775, mean_q: 0.022660
 32046/100000: episode: 3210, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.002214, mean_q: 0.021897
 32056/100000: episode: 3211, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000018, mae: 0.002077, mean_q: 0.022213
 32066/100000: episode: 3212, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000267, mae: 0.004793, mean_q: 0.023169
 32076/100000: episode: 3213, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000072, mae: 0.002810, mean_q: 0.023248
 32086/100000: episode: 3214, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000057, mae: 0.002574, mean_q: 0.022901
 32096/100000: episode: 3215, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000055, mae: 0.002540, mean_q: 0.022300
 32106/100000: episode: 3216, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.001944, mean_q: 0.022591
 32116/100000: episode: 3217, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000239, mae: 0.003161, mean_q: 0.022682
 32126/100000: episode: 3218, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000219, mae: 0.003503, mean_q: 0.023491
 32136/100000: episode: 3219, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000076, mae: 0.003242, mean_q: 0.023364
 32146/100000: episode: 3220, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000073, mae: 0.002848, mean_q: 0.022469
 32156/100000: episode: 3221, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000029, mae: 0.001448, mean_q: 0.022691
 32166/100000: episode: 3222, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000077, mae: 0.002871, mean_q: 0.022701
 32176/100000: episode: 3223, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000073, mae: 0.002890, mean_q: 0.023200
 32186/100000: episode: 3224, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000241, mae: 0.003471, mean_q: 0.022390
 32196/100000: episode: 3225, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000225, mae: 0.005028, mean_q: 0.024851
 32206/100000: episode: 3226, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000282, mae: 0.004541, mean_q: 0.023072
 32216/100000: episode: 3227, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000266, mae: 0.004975, mean_q: 0.023810
 32226/100000: episode: 3228, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000040, mae: 0.003145, mean_q: 0.023739
 32236/100000: episode: 3229, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000035, mae: 0.002990, mean_q: 0.021770
 32246/100000: episode: 3230, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.002644, mean_q: 0.022469
 32256/100000: episode: 3231, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002244, mean_q: 0.022582
 32266/100000: episode: 3232, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000220, mae: 0.003290, mean_q: 0.022755
 32276/100000: episode: 3233, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000220, mae: 0.003738, mean_q: 0.024013
 32286/100000: episode: 3234, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000260, mae: 0.005260, mean_q: 0.024827
 32296/100000: episode: 3235, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000206, mae: 0.003699, mean_q: 0.023727
 32306/100000: episode: 3236, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.003502, mean_q: 0.021811
 32316/100000: episode: 3237, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000055, mae: 0.003550, mean_q: 0.021533
 32326/100000: episode: 3238, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000016, mae: 0.001976, mean_q: 0.022508
 32336/100000: episode: 3239, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000039, mae: 0.003134, mean_q: 0.021766
 32346/100000: episode: 3240, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.002835, mean_q: 0.021697
 32356/100000: episode: 3241, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000244, mae: 0.003706, mean_q: 0.022889
 32366/100000: episode: 3242, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000030, mae: 0.002135, mean_q: 0.023566
 32376/100000: episode: 3243, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000134, mae: 0.003899, mean_q: 0.022751
 32386/100000: episode: 3244, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000017, mae: 0.002697, mean_q: 0.024053
 32396/100000: episode: 3245, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000063, mae: 0.003714, mean_q: 0.021697
 32406/100000: episode: 3246, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000015, mae: 0.001676, mean_q: 0.022811
 32416/100000: episode: 3247, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000443, mae: 0.004932, mean_q: 0.023087
 32426/100000: episode: 3248, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000059, mae: 0.004584, mean_q: 0.025293
 32436/100000: episode: 3249, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000052, mae: 0.002590, mean_q: 0.022330
 32446/100000: episode: 3250, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000061, mae: 0.003016, mean_q: 0.022826
 32456/100000: episode: 3251, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000412, mae: 0.004791, mean_q: 0.022646
 32466/100000: episode: 3252, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000039, mae: 0.002801, mean_q: 0.023665
 32476/100000: episode: 3253, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000411, mae: 0.005040, mean_q: 0.023801
 32486/100000: episode: 3254, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000078, mae: 0.003727, mean_q: 0.024019
 32496/100000: episode: 3255, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000245, mae: 0.005094, mean_q: 0.024869
 32506/100000: episode: 3256, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000209, mae: 0.003858, mean_q: 0.023158
 32516/100000: episode: 3257, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000035, mae: 0.002409, mean_q: 0.022689
 32526/100000: episode: 3258, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000015, mae: 0.002366, mean_q: 0.022474
 32536/100000: episode: 3259, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000203, mae: 0.003285, mean_q: 0.022494
 32546/100000: episode: 3260, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000210, mae: 0.004413, mean_q: 0.024440
 32556/100000: episode: 3261, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000077, mae: 0.003803, mean_q: 0.021978
 32566/100000: episode: 3262, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000050, mae: 0.002174, mean_q: 0.022779
 32576/100000: episode: 3263, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000263, mae: 0.004156, mean_q: 0.022812
 32586/100000: episode: 3264, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000222, mae: 0.004495, mean_q: 0.024946
 32596/100000: episode: 3265, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002991, mean_q: 0.022522
 32606/100000: episode: 3266, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000009, mae: 0.002761, mean_q: 0.020955
 32616/100000: episode: 3267, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000036, mae: 0.002181, mean_q: 0.022648
 32626/100000: episode: 3268, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000062, mae: 0.002979, mean_q: 0.022657
 32636/100000: episode: 3269, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000059, mae: 0.002844, mean_q: 0.022454
 32646/100000: episode: 3270, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000216, mae: 0.002504, mean_q: 0.022945
 32656/100000: episode: 3271, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000241, mae: 0.004262, mean_q: 0.023897
 32666/100000: episode: 3272, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.002846, mean_q: 0.023132
 32676/100000: episode: 3273, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000058, mae: 0.002553, mean_q: 0.022850
 32686/100000: episode: 3274, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000074, mae: 0.003345, mean_q: 0.023907
 32696/100000: episode: 3275, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000033, mae: 0.002042, mean_q: 0.022934
 32706/100000: episode: 3276, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000392, mae: 0.004659, mean_q: 0.023063
 32716/100000: episode: 3277, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000059, mae: 0.003558, mean_q: 0.024080
 32726/100000: episode: 3278, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000100, mae: 0.004327, mean_q: 0.021809
 32736/100000: episode: 3279, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000226, mae: 0.004516, mean_q: 0.024494
 32746/100000: episode: 3280, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000242, mae: 0.003845, mean_q: 0.023391
 32756/100000: episode: 3281, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000227, mae: 0.003648, mean_q: 0.023146
 32766/100000: episode: 3282, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000034, mae: 0.002074, mean_q: 0.022926
 32776/100000: episode: 3283, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.003426, mean_q: 0.022045
 32786/100000: episode: 3284, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000201, mae: 0.002939, mean_q: 0.023380
 32796/100000: episode: 3285, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000222, mae: 0.003249, mean_q: 0.023435
 32806/100000: episode: 3286, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000243, mae: 0.005913, mean_q: 0.026056
 32816/100000: episode: 3287, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000037, mae: 0.003117, mean_q: 0.023746
 32826/100000: episode: 3288, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000203, mae: 0.004389, mean_q: 0.021074
[Info] 1-TH LEVEL FOUND: 0.02263675071299076, Considering 100/100 traces
 32836/100000: episode: 3289, duration: 0.715s, episode steps: 10, steps per second: 14, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000017, mae: 0.001929, mean_q: 0.023519
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02263675071299076
1
 32846/100000: episode: 3290, duration: 0.491s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000056, mae: 0.003363, mean_q: 0.021982
 32856/100000: episode: 3291, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000222, mae: 0.003684, mean_q: 0.023737
 32866/100000: episode: 3292, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000016, mae: 0.002366, mean_q: 0.023005
 32876/100000: episode: 3293, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.003273, mean_q: 0.021253
 32886/100000: episode: 3294, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000030, mae: 0.001937, mean_q: 0.022240
 32896/100000: episode: 3295, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000036, mae: 0.002284, mean_q: 0.023173
 32906/100000: episode: 3296, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000033, mae: 0.002014, mean_q: 0.022409
 32916/100000: episode: 3297, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000242, mae: 0.003665, mean_q: 0.022827
 32926/100000: episode: 3298, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000227, mae: 0.004669, mean_q: 0.024201
 32936/100000: episode: 3299, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000071, mae: 0.002892, mean_q: 0.022326
 32946/100000: episode: 3300, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000403, mae: 0.004431, mean_q: 0.022987
 32956/100000: episode: 3301, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000281, mae: 0.007082, mean_q: 0.026373
 32966/100000: episode: 3302, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000039, mae: 0.003330, mean_q: 0.023144
 32976/100000: episode: 3303, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000009, mae: 0.002182, mean_q: 0.021889
 32986/100000: episode: 3304, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000388, mae: 0.004132, mean_q: 0.023244
 32996/100000: episode: 3305, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000198, mae: 0.002951, mean_q: 0.023894
Step 33000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.33000.hdf5
 33006/100000: episode: 3306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000035, mae: 0.001848, mean_q: 0.023136
 33016/100000: episode: 3307, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000262, mae: 0.004384, mean_q: 0.023818
 33026/100000: episode: 3308, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000051, mae: 0.002231, mean_q: 0.023516
 33036/100000: episode: 3309, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000223, mae: 0.003557, mean_q: 0.022724
 33046/100000: episode: 3310, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000018, mae: 0.002344, mean_q: 0.023925
 33056/100000: episode: 3311, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000221, mae: 0.003320, mean_q: 0.022792
 33066/100000: episode: 3312, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000221, mae: 0.004627, mean_q: 0.025303
 33076/100000: episode: 3313, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000075, mae: 0.003790, mean_q: 0.024722
 33086/100000: episode: 3314, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000058, mae: 0.003297, mean_q: 0.022441
 33096/100000: episode: 3315, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000064, mae: 0.002364, mean_q: 0.024347
 33106/100000: episode: 3316, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000223, mae: 0.004340, mean_q: 0.024874
 33116/100000: episode: 3317, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.003266, mean_q: 0.022862
 33126/100000: episode: 3318, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000407, mae: 0.004788, mean_q: 0.023955
 33136/100000: episode: 3319, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000028, mae: 0.003451, mean_q: 0.024527
 33146/100000: episode: 3320, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.003719, mean_q: 0.021714
 33156/100000: episode: 3321, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.001785, mean_q: 0.023487
 33166/100000: episode: 3322, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000205, mae: 0.002982, mean_q: 0.023440
 33176/100000: episode: 3323, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000017, mae: 0.001850, mean_q: 0.023886
 33186/100000: episode: 3324, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000206, mae: 0.003766, mean_q: 0.022509
 33196/100000: episode: 3325, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000239, mae: 0.003885, mean_q: 0.023967
 33206/100000: episode: 3326, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000200, mae: 0.003929, mean_q: 0.025135
 33216/100000: episode: 3327, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000059, mae: 0.002740, mean_q: 0.023635
 33226/100000: episode: 3328, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000200, mae: 0.002666, mean_q: 0.023876
 33236/100000: episode: 3329, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000244, mae: 0.004413, mean_q: 0.024531
 33246/100000: episode: 3330, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.002399, mean_q: 0.025012
 33256/100000: episode: 3331, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000242, mae: 0.003892, mean_q: 0.023461
 33266/100000: episode: 3332, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.002698, mean_q: 0.024332
 33276/100000: episode: 3333, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000019, mae: 0.002366, mean_q: 0.023310
 33286/100000: episode: 3334, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000038, mae: 0.003383, mean_q: 0.022240
 33296/100000: episode: 3335, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000203, mae: 0.003219, mean_q: 0.023203
 33306/100000: episode: 3336, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000119, mae: 0.004582, mean_q: 0.024480
 33316/100000: episode: 3337, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000200, mae: 0.003420, mean_q: 0.024708
 33326/100000: episode: 3338, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000204, mae: 0.003310, mean_q: 0.023276
 33336/100000: episode: 3339, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000201, mae: 0.003245, mean_q: 0.023027
 33346/100000: episode: 3340, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000206, mae: 0.003617, mean_q: 0.024278
 33356/100000: episode: 3341, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000075, mae: 0.003537, mean_q: 0.022613
 33366/100000: episode: 3342, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000060, mae: 0.003361, mean_q: 0.022690
 33376/100000: episode: 3343, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000037, mae: 0.002580, mean_q: 0.023105
 33386/100000: episode: 3344, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000012, mae: 0.002662, mean_q: 0.021822
 33396/100000: episode: 3345, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000036, mae: 0.002349, mean_q: 0.022821
 33406/100000: episode: 3346, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002044, mean_q: 0.023299
 33416/100000: episode: 3347, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002396, mean_q: 0.022414
 33426/100000: episode: 3348, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000428, mae: 0.005334, mean_q: 0.023611
 33436/100000: episode: 3349, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000201, mae: 0.004575, mean_q: 0.025554
 33446/100000: episode: 3350, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000035, mae: 0.002645, mean_q: 0.022951
 33456/100000: episode: 3351, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000204, mae: 0.003205, mean_q: 0.022844
 33466/100000: episode: 3352, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000016, mae: 0.002305, mean_q: 0.022718
 33476/100000: episode: 3353, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000227, mae: 0.004225, mean_q: 0.022110
 33486/100000: episode: 3354, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002764, mean_q: 0.024250
 33496/100000: episode: 3355, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000014, mae: 0.002355, mean_q: 0.022196
 33506/100000: episode: 3356, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000015, mae: 0.002473, mean_q: 0.021900
 33516/100000: episode: 3357, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000020, mae: 0.002568, mean_q: 0.022325
 33526/100000: episode: 3358, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.002312, mean_q: 0.021805
 33536/100000: episode: 3359, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000039, mae: 0.002982, mean_q: 0.021941
 33546/100000: episode: 3360, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000033, mae: 0.001828, mean_q: 0.022634
 33556/100000: episode: 3361, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000034, mae: 0.001908, mean_q: 0.022933
 33566/100000: episode: 3362, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002587, mean_q: 0.022369
 33576/100000: episode: 3363, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000038, mae: 0.002662, mean_q: 0.022043
 33586/100000: episode: 3364, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000013, mae: 0.001974, mean_q: 0.021946
 33596/100000: episode: 3365, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000032, mae: 0.002432, mean_q: 0.021464
 33606/100000: episode: 3366, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000034, mae: 0.002072, mean_q: 0.022649
 33616/100000: episode: 3367, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000221, mae: 0.003655, mean_q: 0.021438
 33626/100000: episode: 3368, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000030, mae: 0.002036, mean_q: 0.022901
 33636/100000: episode: 3369, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.002100, mean_q: 0.022310
 33646/100000: episode: 3370, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000244, mae: 0.004119, mean_q: 0.021447
 33656/100000: episode: 3371, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000034, mae: 0.002803, mean_q: 0.023465
 33666/100000: episode: 3372, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.002459, mean_q: 0.021476
 33676/100000: episode: 3373, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000039, mae: 0.003442, mean_q: 0.020700
 33686/100000: episode: 3374, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000072, mae: 0.002568, mean_q: 0.022390
 33696/100000: episode: 3375, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000015, mae: 0.002072, mean_q: 0.022086
 33706/100000: episode: 3376, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000013, mae: 0.002589, mean_q: 0.020591
 33716/100000: episode: 3377, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000204, mae: 0.003177, mean_q: 0.021727
 33726/100000: episode: 3378, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000042, mae: 0.003143, mean_q: 0.022438
 33736/100000: episode: 3379, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002610, mean_q: 0.021444
 33746/100000: episode: 3380, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000073, mae: 0.002517, mean_q: 0.021878
 33756/100000: episode: 3381, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002475, mean_q: 0.022211
 33766/100000: episode: 3382, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000037, mae: 0.003106, mean_q: 0.020382
 33776/100000: episode: 3383, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000284, mae: 0.005252, mean_q: 0.023070
 33786/100000: episode: 3384, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000224, mae: 0.003913, mean_q: 0.022931
 33796/100000: episode: 3385, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.001758, mean_q: 0.022061
 33806/100000: episode: 3386, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000038, mae: 0.002640, mean_q: 0.021530
 33816/100000: episode: 3387, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000227, mae: 0.003653, mean_q: 0.021360
 33826/100000: episode: 3388, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000014, mae: 0.001812, mean_q: 0.022422
 33836/100000: episode: 3389, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000041, mae: 0.002874, mean_q: 0.021196
[Info] 1-TH LEVEL FOUND: 0.021744508296251297, Considering 100/100 traces
 33846/100000: episode: 3390, duration: 0.721s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000203, mae: 0.003278, mean_q: 0.022279
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021744508296251297
1
 33856/100000: episode: 3391, duration: 0.570s, episode steps: 10, steps per second: 18, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000053, mae: 0.002694, mean_q: 0.021052
 33866/100000: episode: 3392, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000013, mae: 0.002241, mean_q: 0.020859
 33876/100000: episode: 3393, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000028, mae: 0.001964, mean_q: 0.020944
 33886/100000: episode: 3394, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000030, mae: 0.001681, mean_q: 0.021864
 33896/100000: episode: 3395, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000243, mae: 0.003621, mean_q: 0.022111
 33906/100000: episode: 3396, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000018, mae: 0.002410, mean_q: 0.021422
 33916/100000: episode: 3397, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000225, mae: 0.003709, mean_q: 0.021175
 33926/100000: episode: 3398, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000034, mae: 0.001978, mean_q: 0.021959
 33936/100000: episode: 3399, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000037, mae: 0.002438, mean_q: 0.021196
 33946/100000: episode: 3400, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000015, mae: 0.001659, mean_q: 0.021280
 33956/100000: episode: 3401, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000051, mae: 0.002082, mean_q: 0.021366
 33966/100000: episode: 3402, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000080, mae: 0.003035, mean_q: 0.021422
 33976/100000: episode: 3403, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000247, mae: 0.005343, mean_q: 0.023481
 33986/100000: episode: 3404, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000016, mae: 0.002423, mean_q: 0.022480
 33996/100000: episode: 3405, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000244, mae: 0.003957, mean_q: 0.021942
Step 34000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.34000.hdf5
 34006/100000: episode: 3406, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000017, mae: 0.002599, mean_q: 0.022850
 34016/100000: episode: 3407, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000035, mae: 0.003342, mean_q: 0.020210
 34026/100000: episode: 3408, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000013, mae: 0.002421, mean_q: 0.020438
 34036/100000: episode: 3409, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000014, mae: 0.002141, mean_q: 0.020906
 34046/100000: episode: 3410, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000033, mae: 0.002545, mean_q: 0.020420
 34056/100000: episode: 3411, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000055, mae: 0.002739, mean_q: 0.020638
 34066/100000: episode: 3412, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000082, mae: 0.003851, mean_q: 0.022122
 34076/100000: episode: 3413, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000222, mae: 0.003538, mean_q: 0.022220
 34086/100000: episode: 3414, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000230, mae: 0.003892, mean_q: 0.022030
 34096/100000: episode: 3415, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000208, mae: 0.004538, mean_q: 0.023292
 34106/100000: episode: 3416, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000010, mae: 0.001687, mean_q: 0.021472
 34116/100000: episode: 3417, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000024, mae: 0.003556, mean_q: 0.020075
 34126/100000: episode: 3418, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000014, mae: 0.002261, mean_q: 0.020527
 34136/100000: episode: 3419, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000285, mae: 0.004763, mean_q: 0.021746
 34146/100000: episode: 3420, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000058, mae: 0.003723, mean_q: 0.023108
 34156/100000: episode: 3421, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000018, mae: 0.002209, mean_q: 0.020951
 34166/100000: episode: 3422, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000037, mae: 0.002611, mean_q: 0.020864
 34176/100000: episode: 3423, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000018, mae: 0.002070, mean_q: 0.021092
 34186/100000: episode: 3424, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000052, mae: 0.002171, mean_q: 0.021017
 34196/100000: episode: 3425, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000039, mae: 0.002231, mean_q: 0.021420
 34206/100000: episode: 3426, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000012, mae: 0.002121, mean_q: 0.020407
 34216/100000: episode: 3427, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000036, mae: 0.002441, mean_q: 0.021644
 34226/100000: episode: 3428, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000053, mae: 0.002908, mean_q: 0.022302
 34236/100000: episode: 3429, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000014, mae: 0.002606, mean_q: 0.020047
 34246/100000: episode: 3430, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000010, mae: 0.002263, mean_q: 0.019821
 34256/100000: episode: 3431, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000224, mae: 0.003336, mean_q: 0.021176
 34266/100000: episode: 3432, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000038, mae: 0.003288, mean_q: 0.022737
 34276/100000: episode: 3433, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000034, mae: 0.002592, mean_q: 0.020130
 34286/100000: episode: 3434, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000247, mae: 0.004025, mean_q: 0.021136
 34296/100000: episode: 3435, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000022, mae: 0.003181, mean_q: 0.022386
 34306/100000: episode: 3436, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000411, mae: 0.004717, mean_q: 0.020245
 34316/100000: episode: 3437, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000018, mae: 0.002671, mean_q: 0.022235
 34326/100000: episode: 3438, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000208, mae: 0.003924, mean_q: 0.020024
 34336/100000: episode: 3439, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000208, mae: 0.003520, mean_q: 0.021774
 34346/100000: episode: 3440, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002641, mean_q: 0.021510
 34356/100000: episode: 3441, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.003073, mean_q: 0.020091
 34366/100000: episode: 3442, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000240, mae: 0.004551, mean_q: 0.022948
 34376/100000: episode: 3443, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000074, mae: 0.003469, mean_q: 0.022333
 34386/100000: episode: 3444, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000056, mae: 0.002970, mean_q: 0.020468
 34396/100000: episode: 3445, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000597, mae: 0.005811, mean_q: 0.021999
 34406/100000: episode: 3446, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000042, mae: 0.004169, mean_q: 0.023735
 34416/100000: episode: 3447, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.002673, mean_q: 0.019857
 34426/100000: episode: 3448, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000409, mae: 0.004681, mean_q: 0.021216
 34436/100000: episode: 3449, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000047, mae: 0.004372, mean_q: 0.023381
 34446/100000: episode: 3450, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.003011, mean_q: 0.020501
 34456/100000: episode: 3451, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000011, mae: 0.002363, mean_q: 0.020075
 34466/100000: episode: 3452, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000037, mae: 0.002215, mean_q: 0.021142
 34476/100000: episode: 3453, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000031, mae: 0.001881, mean_q: 0.020995
 34486/100000: episode: 3454, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000057, mae: 0.003535, mean_q: 0.019752
 34496/100000: episode: 3455, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002434, mean_q: 0.021367
 34506/100000: episode: 3456, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000220, mae: 0.003432, mean_q: 0.021962
 34516/100000: episode: 3457, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.002045, mean_q: 0.021593
 34526/100000: episode: 3458, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000013, mae: 0.002485, mean_q: 0.019847
 34536/100000: episode: 3459, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000015, mae: 0.002169, mean_q: 0.020303
 34546/100000: episode: 3460, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000058, mae: 0.002680, mean_q: 0.021232
 34556/100000: episode: 3461, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000221, mae: 0.003016, mean_q: 0.021566
 34566/100000: episode: 3462, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.002244, mean_q: 0.021723
 34576/100000: episode: 3463, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000406, mae: 0.003914, mean_q: 0.021524
 34586/100000: episode: 3464, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000017, mae: 0.002802, mean_q: 0.022538
[Info] FALSIFICATION!
 34596/100000: episode: 3465, duration: 0.291s, episode steps: 10, steps per second: 34, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000038, mae: 0.002821, mean_q: 0.020332
 34606/100000: episode: 3466, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000054, mae: 0.002356, mean_q: 0.020823
 34616/100000: episode: 3467, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000221, mae: 0.003707, mean_q: 0.022267
 34626/100000: episode: 3468, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000052, mae: 0.002663, mean_q: 0.022264
 34636/100000: episode: 3469, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000013, mae: 0.002106, mean_q: 0.020543
 34646/100000: episode: 3470, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000203, mae: 0.003374, mean_q: 0.020254
 34656/100000: episode: 3471, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000020, mae: 0.002920, mean_q: 0.022463
 34666/100000: episode: 3472, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002489, mean_q: 0.020322
 34676/100000: episode: 3473, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000023, mae: 0.003293, mean_q: 0.019617
 34686/100000: episode: 3474, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001700, mae: 0.007744, mean_q: 0.023514
 34696/100000: episode: 3475, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000057, mae: 0.004338, mean_q: 0.023584
 34706/100000: episode: 3476, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000036, mae: 0.003723, mean_q: 0.019047
 34716/100000: episode: 3477, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000011, mae: 0.001608, mean_q: 0.021024
 34726/100000: episode: 3478, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000034, mae: 0.001914, mean_q: 0.021349
 34736/100000: episode: 3479, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000096, mae: 0.003271, mean_q: 0.021419
 34746/100000: episode: 3480, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000042, mae: 0.003123, mean_q: 0.022029
 34756/100000: episode: 3481, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000202, mae: 0.002850, mean_q: 0.020856
 34766/100000: episode: 3482, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000032, mae: 0.001820, mean_q: 0.020954
 34776/100000: episode: 3483, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000014, mae: 0.001796, mean_q: 0.020832
 34786/100000: episode: 3484, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002839, mean_q: 0.020217
 34796/100000: episode: 3485, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000021, mae: 0.002264, mean_q: 0.021446
 34806/100000: episode: 3486, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000052, mae: 0.001900, mean_q: 0.021015
 34816/100000: episode: 3487, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000026, mae: 0.002741, mean_q: 0.020860
 34826/100000: episode: 3488, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000246, mae: 0.004031, mean_q: 0.020440
 34836/100000: episode: 3489, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000014, mae: 0.001903, mean_q: 0.021690
 34846/100000: episode: 3490, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.002881, mean_q: 0.019621
[Info] Complete ISplit Iteration
[Info] Levels: [0.019814942]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 34856/100000: episode: 3491, duration: 0.841s, episode steps: 10, steps per second: 12, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000015, mae: 0.002926, mean_q: 0.019253
 34866/100000: episode: 3492, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001551, mae: 0.005841, mean_q: 0.021162
 34876/100000: episode: 3493, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000042, mae: 0.005135, mean_q: 0.024497
 34886/100000: episode: 3494, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.003200, mean_q: 0.019525
 34896/100000: episode: 3495, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000014, mae: 0.001987, mean_q: 0.020172
 34906/100000: episode: 3496, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000034, mae: 0.002149, mean_q: 0.020699
 34916/100000: episode: 3497, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000014, mae: 0.002500, mean_q: 0.019693
 34926/100000: episode: 3498, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002702, mean_q: 0.019518
 34936/100000: episode: 3499, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000009, mae: 0.001423, mean_q: 0.020198
 34946/100000: episode: 3500, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000016, mae: 0.002317, mean_q: 0.019963
 34956/100000: episode: 3501, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000038, mae: 0.002834, mean_q: 0.019356
 34966/100000: episode: 3502, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.001526, mae: 0.006057, mean_q: 0.022305
 34976/100000: episode: 3503, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000021, mae: 0.004160, mean_q: 0.023662
 34986/100000: episode: 3504, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000037, mae: 0.003538, mean_q: 0.018761
 34996/100000: episode: 3505, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000021, mae: 0.002718, mean_q: 0.021218
Step 35000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.35000.hdf5
 35006/100000: episode: 3506, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001513, mae: 0.006061, mean_q: 0.022613
 35016/100000: episode: 3507, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000016, mae: 0.002468, mean_q: 0.021149
 35026/100000: episode: 3508, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000242, mae: 0.004094, mean_q: 0.019564
 35036/100000: episode: 3509, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000203, mae: 0.003175, mean_q: 0.021591
 35046/100000: episode: 3510, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000013, mae: 0.001948, mean_q: 0.020823
 35056/100000: episode: 3511, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000039, mae: 0.002989, mean_q: 0.019771
 35066/100000: episode: 3512, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000036, mae: 0.002055, mean_q: 0.020918
 35076/100000: episode: 3513, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000016, mae: 0.001678, mean_q: 0.020751
 35086/100000: episode: 3514, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000022, mae: 0.002801, mean_q: 0.019982
 35096/100000: episode: 3515, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000008, mae: 0.001965, mean_q: 0.019526
 35106/100000: episode: 3516, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000031, mae: 0.001992, mean_q: 0.019819
 35116/100000: episode: 3517, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000033, mae: 0.001899, mean_q: 0.020758
 35126/100000: episode: 3518, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000032, mae: 0.002462, mean_q: 0.019496
 35136/100000: episode: 3519, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002044, mean_q: 0.020175
 35146/100000: episode: 3520, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001510, mae: 0.004865, mean_q: 0.021102
 35156/100000: episode: 3521, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000200, mae: 0.004202, mean_q: 0.022728
 35166/100000: episode: 3522, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000071, mae: 0.002638, mean_q: 0.020526
 35176/100000: episode: 3523, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000036, mae: 0.002170, mean_q: 0.020381
 35186/100000: episode: 3524, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.001524, mae: 0.006627, mean_q: 0.022175
 35196/100000: episode: 3525, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000031, mae: 0.002245, mean_q: 0.020956
 35206/100000: episode: 3526, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000010, mae: 0.001426, mean_q: 0.020359
 35216/100000: episode: 3527, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000009, mae: 0.000924, mean_q: 0.020693
 35226/100000: episode: 3528, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000019, mae: 0.002258, mean_q: 0.020255
 35236/100000: episode: 3529, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000017, mae: 0.002029, mean_q: 0.020186
 35246/100000: episode: 3530, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.002174, mean_q: 0.020163
 35256/100000: episode: 3531, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000035, mae: 0.002014, mean_q: 0.020130
 35266/100000: episode: 3532, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002233, mean_q: 0.021124
 35276/100000: episode: 3533, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000100, mae: 0.003918, mean_q: 0.019748
 35286/100000: episode: 3534, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000019, mae: 0.002115, mean_q: 0.020205
 35296/100000: episode: 3535, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000008, mae: 0.001248, mean_q: 0.020142
 35306/100000: episode: 3536, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000202, mae: 0.002673, mean_q: 0.020449
 35316/100000: episode: 3537, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000227, mae: 0.003536, mean_q: 0.020748
 35326/100000: episode: 3538, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002003, mean_q: 0.020504
 35336/100000: episode: 3539, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000244, mae: 0.003234, mean_q: 0.020367
 35346/100000: episode: 3540, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000036, mae: 0.002746, mean_q: 0.021466
 35356/100000: episode: 3541, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000034, mae: 0.001823, mean_q: 0.020256
 35366/100000: episode: 3542, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000038, mae: 0.002227, mean_q: 0.020785
 35376/100000: episode: 3543, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000031, mae: 0.001554, mean_q: 0.020435
 35386/100000: episode: 3544, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000225, mae: 0.003593, mean_q: 0.019853
 35396/100000: episode: 3545, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000035, mae: 0.002787, mean_q: 0.021565
 35406/100000: episode: 3546, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000205, mae: 0.002883, mean_q: 0.020238
 35416/100000: episode: 3547, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.001536, mae: 0.006384, mean_q: 0.022123
 35426/100000: episode: 3548, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000017, mae: 0.002974, mean_q: 0.022250
 35436/100000: episode: 3549, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000202, mae: 0.003431, mean_q: 0.019435
 35446/100000: episode: 3550, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000203, mae: 0.003222, mean_q: 0.021648
 35456/100000: episode: 3551, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000204, mae: 0.002903, mean_q: 0.021109
 35466/100000: episode: 3552, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000228, mae: 0.003516, mean_q: 0.021218
 35476/100000: episode: 3553, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000018, mae: 0.002456, mean_q: 0.021345
 35486/100000: episode: 3554, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000247, mae: 0.004270, mean_q: 0.019664
 35496/100000: episode: 3555, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000205, mae: 0.004100, mean_q: 0.022311
 35506/100000: episode: 3556, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002277, mean_q: 0.021211
 35516/100000: episode: 3557, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000038, mae: 0.003276, mean_q: 0.019522
 35526/100000: episode: 3558, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000017, mae: 0.002536, mean_q: 0.020006
 35536/100000: episode: 3559, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000033, mae: 0.002247, mean_q: 0.019896
 35546/100000: episode: 3560, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.001878, mean_q: 0.020293
 35556/100000: episode: 3561, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000221, mae: 0.003027, mean_q: 0.020534
 35566/100000: episode: 3562, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000200, mae: 0.002834, mean_q: 0.020015
 35576/100000: episode: 3563, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002042, mean_q: 0.020602
 35586/100000: episode: 3564, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.002038, mean_q: 0.020700
 35596/100000: episode: 3565, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000017, mae: 0.002052, mean_q: 0.019937
 35606/100000: episode: 3566, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000032, mae: 0.001776, mean_q: 0.020224
 35616/100000: episode: 3567, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.001539, mae: 0.006470, mean_q: 0.021581
 35626/100000: episode: 3568, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000228, mae: 0.005517, mean_q: 0.023489
 35636/100000: episode: 3569, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000209, mae: 0.003397, mean_q: 0.021397
 35646/100000: episode: 3570, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000017, mae: 0.001697, mean_q: 0.021058
 35656/100000: episode: 3571, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000010, mae: 0.001514, mean_q: 0.020464
 35666/100000: episode: 3572, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002716, mean_q: 0.019911
 35676/100000: episode: 3573, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000013, mae: 0.002324, mean_q: 0.019637
 35686/100000: episode: 3574, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000199, mae: 0.002963, mean_q: 0.019845
 35696/100000: episode: 3575, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.001885, mean_q: 0.020952
 35706/100000: episode: 3576, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000011, mae: 0.002030, mean_q: 0.019717
 35716/100000: episode: 3577, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000036, mae: 0.002625, mean_q: 0.019443
 35726/100000: episode: 3578, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000019, mae: 0.001764, mean_q: 0.020443
 35736/100000: episode: 3579, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000015, mae: 0.001827, mean_q: 0.020061
 35746/100000: episode: 3580, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000030, mae: 0.002047, mean_q: 0.019512
 35756/100000: episode: 3581, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000034, mae: 0.002296, mean_q: 0.019680
 35766/100000: episode: 3582, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000030, mae: 0.001704, mean_q: 0.019900
 35776/100000: episode: 3583, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000233, mae: 0.004008, mean_q: 0.020427
 35786/100000: episode: 3584, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000390, mae: 0.005187, mean_q: 0.022090
 35796/100000: episode: 3585, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000035, mae: 0.002589, mean_q: 0.019941
 35806/100000: episode: 3586, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000055, mae: 0.002970, mean_q: 0.019174
 35816/100000: episode: 3587, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000017, mae: 0.001995, mean_q: 0.020806
 35826/100000: episode: 3588, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000021, mae: 0.002359, mean_q: 0.020059
 35836/100000: episode: 3589, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000013, mae: 0.002335, mean_q: 0.019265
 35846/100000: episode: 3590, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000035, mae: 0.002521, mean_q: 0.019161
[Info] 1-TH LEVEL FOUND: 0.020116912201046944, Considering 100/100 traces
 35856/100000: episode: 3591, duration: 0.713s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000041, mae: 0.002487, mean_q: 0.020332
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020116912201046944
1
 35866/100000: episode: 3592, duration: 0.487s, episode steps: 10, steps per second: 21, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000015, mae: 0.001817, mean_q: 0.019679
 35876/100000: episode: 3593, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000042, mae: 0.002430, mean_q: 0.019792
 35886/100000: episode: 3594, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000015, mae: 0.001862, mean_q: 0.019949
 35896/100000: episode: 3595, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000040, mae: 0.002910, mean_q: 0.019020
 35906/100000: episode: 3596, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002114, mean_q: 0.019532
 35916/100000: episode: 3597, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000390, mae: 0.004575, mean_q: 0.021297
 35926/100000: episode: 3598, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [0.000, 10.000], loss: 0.001516, mae: 0.005692, mean_q: 0.021331
 35936/100000: episode: 3599, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000055, mae: 0.003555, mean_q: 0.021709
 35946/100000: episode: 3600, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000245, mae: 0.003662, mean_q: 0.020588
 35956/100000: episode: 3601, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002197, mean_q: 0.020011
 35966/100000: episode: 3602, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000209, mae: 0.003414, mean_q: 0.019784
 35976/100000: episode: 3603, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000393, mae: 0.004076, mean_q: 0.020690
 35986/100000: episode: 3604, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000221, mae: 0.003991, mean_q: 0.021738
 35996/100000: episode: 3605, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002299, mean_q: 0.020943
Step 36000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.36000.hdf5
 36006/100000: episode: 3606, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000201, mae: 0.003220, mean_q: 0.019338
 36016/100000: episode: 3607, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001512, mae: 0.005349, mean_q: 0.021355
 36026/100000: episode: 3608, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000059, mae: 0.004908, mean_q: 0.023659
 36036/100000: episode: 3609, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000021, mae: 0.003239, mean_q: 0.019291
 36046/100000: episode: 3610, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000019, mae: 0.002882, mean_q: 0.019176
 36056/100000: episode: 3611, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000034, mae: 0.002088, mean_q: 0.020971
 36066/100000: episode: 3612, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000035, mae: 0.001972, mean_q: 0.020365
 36076/100000: episode: 3613, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000017, mae: 0.002068, mean_q: 0.020027
 36086/100000: episode: 3614, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.002783, mean_q: 0.019711
 36096/100000: episode: 3615, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002434, mean_q: 0.019686
 36106/100000: episode: 3616, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000018, mae: 0.001949, mean_q: 0.020005
 36116/100000: episode: 3617, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000018, mae: 0.002053, mean_q: 0.019996
 36126/100000: episode: 3618, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000053, mae: 0.002404, mean_q: 0.019567
 36136/100000: episode: 3619, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000034, mae: 0.001832, mean_q: 0.020142
 36146/100000: episode: 3620, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001522, mae: 0.005402, mean_q: 0.019803
 36156/100000: episode: 3621, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000023, mae: 0.004082, mean_q: 0.022655
 36166/100000: episode: 3622, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000248, mae: 0.003977, mean_q: 0.020027
 36176/100000: episode: 3623, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.001785, mean_q: 0.020439
 36186/100000: episode: 3624, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000033, mae: 0.002083, mean_q: 0.019741
 36196/100000: episode: 3625, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000056, mae: 0.002710, mean_q: 0.019666
 36206/100000: episode: 3626, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000015, mae: 0.001828, mean_q: 0.019981
 36216/100000: episode: 3627, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000014, mae: 0.002025, mean_q: 0.019213
 36226/100000: episode: 3628, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000031, mae: 0.001752, mean_q: 0.020267
 36236/100000: episode: 3629, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000057, mae: 0.002681, mean_q: 0.019626
 36246/100000: episode: 3630, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000199, mae: 0.002348, mean_q: 0.019748
 36256/100000: episode: 3631, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000204, mae: 0.003590, mean_q: 0.021246
 36266/100000: episode: 3632, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001540, mae: 0.008298, mean_q: 0.023644
 36276/100000: episode: 3633, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000052, mae: 0.003137, mean_q: 0.021512
 36286/100000: episode: 3634, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000038, mae: 0.003335, mean_q: 0.019048
 36296/100000: episode: 3635, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000018, mae: 0.002205, mean_q: 0.019916
 36306/100000: episode: 3636, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000036, mae: 0.002038, mean_q: 0.020731
 36316/100000: episode: 3637, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000056, mae: 0.002330, mean_q: 0.020670
 36326/100000: episode: 3638, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000014, mae: 0.001837, mean_q: 0.020027
 36336/100000: episode: 3639, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000017, mae: 0.002561, mean_q: 0.019215
 36346/100000: episode: 3640, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000013, mae: 0.001744, mean_q: 0.019762
 36356/100000: episode: 3641, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000035, mae: 0.001969, mean_q: 0.019929
 36366/100000: episode: 3642, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000009, mae: 0.001644, mean_q: 0.019592
 36376/100000: episode: 3643, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000052, mae: 0.002302, mean_q: 0.019511
 36386/100000: episode: 3644, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000057, mae: 0.002892, mean_q: 0.020784
 36396/100000: episode: 3645, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000035, mae: 0.002386, mean_q: 0.019624
 36406/100000: episode: 3646, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000056, mae: 0.002905, mean_q: 0.019133
 36416/100000: episode: 3647, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002381, mean_q: 0.019552
 36426/100000: episode: 3648, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000038, mae: 0.002467, mean_q: 0.020481
 36436/100000: episode: 3649, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000403, mae: 0.004765, mean_q: 0.020231
 36446/100000: episode: 3650, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.002551, mean_q: 0.021394
 36456/100000: episode: 3651, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002904, mean_q: 0.018876
 36466/100000: episode: 3652, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000017, mae: 0.002432, mean_q: 0.019099
 36476/100000: episode: 3653, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000057, mae: 0.002440, mean_q: 0.020055
 36486/100000: episode: 3654, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000224, mae: 0.003110, mean_q: 0.020253
 36496/100000: episode: 3655, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000070, mae: 0.003024, mean_q: 0.021201
 36506/100000: episode: 3656, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000056, mae: 0.002584, mean_q: 0.020570
 36516/100000: episode: 3657, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000036, mae: 0.002290, mean_q: 0.019690
 36526/100000: episode: 3658, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000035, mae: 0.002219, mean_q: 0.019594
 36536/100000: episode: 3659, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001519, mae: 0.006202, mean_q: 0.021598
 36546/100000: episode: 3660, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000035, mae: 0.002842, mean_q: 0.021393
 36556/100000: episode: 3661, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000201, mae: 0.002970, mean_q: 0.019205
 36566/100000: episode: 3662, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000056, mae: 0.003183, mean_q: 0.021153
 36576/100000: episode: 3663, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000058, mae: 0.002792, mean_q: 0.019755
 36586/100000: episode: 3664, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000031, mae: 0.001557, mean_q: 0.020163
 36596/100000: episode: 3665, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000013, mae: 0.001821, mean_q: 0.019884
 36606/100000: episode: 3666, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000244, mae: 0.003652, mean_q: 0.019680
 36616/100000: episode: 3667, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000013, mae: 0.001911, mean_q: 0.020809
 36626/100000: episode: 3668, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000012, mae: 0.002323, mean_q: 0.018947
 36636/100000: episode: 3669, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002362, mean_q: 0.019281
 36646/100000: episode: 3670, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000220, mae: 0.002718, mean_q: 0.020280
 36656/100000: episode: 3671, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000205, mae: 0.003894, mean_q: 0.021594
 36666/100000: episode: 3672, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000014, mae: 0.002099, mean_q: 0.020348
 36676/100000: episode: 3673, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000011, mae: 0.002749, mean_q: 0.018394
 36686/100000: episode: 3674, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.002461, mean_q: 0.019439
 36696/100000: episode: 3675, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000030, mae: 0.001647, mean_q: 0.020212
 36706/100000: episode: 3676, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000220, mae: 0.002892, mean_q: 0.019670
 36716/100000: episode: 3677, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000044, mae: 0.002821, mean_q: 0.020213
 36726/100000: episode: 3678, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000224, mae: 0.003338, mean_q: 0.020494
 36736/100000: episode: 3679, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000204, mae: 0.003485, mean_q: 0.020988
 36746/100000: episode: 3680, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000013, mae: 0.001782, mean_q: 0.019515
 36756/100000: episode: 3681, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000012, mae: 0.002135, mean_q: 0.019065
 36766/100000: episode: 3682, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000029, mae: 0.001888, mean_q: 0.019242
 36776/100000: episode: 3683, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.002169, mean_q: 0.019628
 36786/100000: episode: 3684, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.002334, mean_q: 0.019516
 36796/100000: episode: 3685, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000020, mae: 0.004106, mean_q: 0.022651
 36806/100000: episode: 3686, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000014, mae: 0.002531, mean_q: 0.019755
 36816/100000: episode: 3687, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000037, mae: 0.003593, mean_q: 0.017946
 36826/100000: episode: 3688, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000021, mae: 0.002382, mean_q: 0.019489
 36836/100000: episode: 3689, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000012, mae: 0.001763, mean_q: 0.019273
 36846/100000: episode: 3690, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.001560, mae: 0.006735, mean_q: 0.020828
 36856/100000: episode: 3691, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000023, mae: 0.003505, mean_q: 0.021702
[Info] 1-TH LEVEL FOUND: 0.018187833949923515, Considering 100/100 traces
 36866/100000: episode: 3692, duration: 0.720s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000034, mae: 0.003309, mean_q: 0.018038
[Info] 2-TH LEVEL FOUND: 0.01985342986881733, Considering 100/100 traces
 36876/100000: episode: 3693, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000011, mae: 0.001775, mean_q: 0.019120
[Info] 3-TH LEVEL FOUND: 0.020589526742696762, Considering 100/100 traces
 36886/100000: episode: 3694, duration: 0.737s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000204, mae: 0.002926, mean_q: 0.020231
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020589526742696762
3
 36896/100000: episode: 3695, duration: 0.515s, episode steps: 10, steps per second: 19, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000009, mae: 0.001338, mean_q: 0.019818
 36906/100000: episode: 3696, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000011, mae: 0.001958, mean_q: 0.018806
 36916/100000: episode: 3697, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000029, mae: 0.001736, mean_q: 0.019241
 36926/100000: episode: 3698, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.001730, mean_q: 0.019340
 36936/100000: episode: 3699, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000013, mae: 0.001812, mean_q: 0.019194
 36946/100000: episode: 3700, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000032, mae: 0.002200, mean_q: 0.018643
 36956/100000: episode: 3701, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000225, mae: 0.003486, mean_q: 0.020082
 36966/100000: episode: 3702, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000016, mae: 0.001891, mean_q: 0.019801
 36976/100000: episode: 3703, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000077, mae: 0.002983, mean_q: 0.018952
 36986/100000: episode: 3704, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000033, mae: 0.002048, mean_q: 0.020030
 36996/100000: episode: 3705, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000019, mae: 0.001956, mean_q: 0.019516
Step 37000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.37000.hdf5
 37006/100000: episode: 3706, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000080, mae: 0.003191, mean_q: 0.019778
 37016/100000: episode: 3707, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000039, mae: 0.002720, mean_q: 0.020040
 37026/100000: episode: 3708, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000032, mae: 0.002535, mean_q: 0.018453
 37036/100000: episode: 3709, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000071, mae: 0.002501, mean_q: 0.019403
 37046/100000: episode: 3710, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.003280, mean_q: 0.020904
 37056/100000: episode: 3711, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000010, mae: 0.001793, mean_q: 0.018847
 37066/100000: episode: 3712, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000018, mae: 0.002449, mean_q: 0.018534
 37076/100000: episode: 3713, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000096, mae: 0.003103, mean_q: 0.019961
 37086/100000: episode: 3714, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001559, mae: 0.007354, mean_q: 0.021807
 37096/100000: episode: 3715, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000044, mae: 0.004137, mean_q: 0.021979
 37106/100000: episode: 3716, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000045, mae: 0.003261, mean_q: 0.018897
 37116/100000: episode: 3717, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000013, mae: 0.001493, mean_q: 0.019642
 37126/100000: episode: 3718, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000037, mae: 0.002129, mean_q: 0.019840
 37136/100000: episode: 3719, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000014, mae: 0.001567, mean_q: 0.019555
 37146/100000: episode: 3720, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.002473, mean_q: 0.019219
 37156/100000: episode: 3721, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000032, mae: 0.001657, mean_q: 0.019958
 37166/100000: episode: 3722, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000202, mae: 0.002492, mean_q: 0.019944
 37176/100000: episode: 3723, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.002160, mean_q: 0.020215
 37186/100000: episode: 3724, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000222, mae: 0.003190, mean_q: 0.020411
 37196/100000: episode: 3725, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000055, mae: 0.002590, mean_q: 0.020212
 37206/100000: episode: 3726, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000012, mae: 0.002048, mean_q: 0.019001
 37216/100000: episode: 3727, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000036, mae: 0.002616, mean_q: 0.019105
 37226/100000: episode: 3728, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000035, mae: 0.002188, mean_q: 0.019182
 37236/100000: episode: 3729, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000222, mae: 0.002800, mean_q: 0.019852
 37246/100000: episode: 3730, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000082, mae: 0.004571, mean_q: 0.021601
 37256/100000: episode: 3731, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.001552, mae: 0.005192, mean_q: 0.020274
 37266/100000: episode: 3732, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000037, mae: 0.003957, mean_q: 0.022437
 37276/100000: episode: 3733, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000013, mae: 0.002232, mean_q: 0.019796
 37286/100000: episode: 3734, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000015, mae: 0.002895, mean_q: 0.018299
 37296/100000: episode: 3735, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000048, mae: 0.001997, mean_q: 0.020186
 37306/100000: episode: 3736, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000026, mae: 0.001247, mean_q: 0.020061
 37316/100000: episode: 3737, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000032, mae: 0.002266, mean_q: 0.019176
 37326/100000: episode: 3738, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000074, mae: 0.002707, mean_q: 0.019998
 37336/100000: episode: 3739, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000039, mae: 0.002752, mean_q: 0.020871
 37346/100000: episode: 3740, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.001805, mean_q: 0.020054
 37356/100000: episode: 3741, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000060, mae: 0.002783, mean_q: 0.019691
 37366/100000: episode: 3742, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000020, mae: 0.002177, mean_q: 0.020237
 37376/100000: episode: 3743, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000034, mae: 0.002691, mean_q: 0.018979
 37386/100000: episode: 3744, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000056, mae: 0.002566, mean_q: 0.019476
 37396/100000: episode: 3745, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000035, mae: 0.002245, mean_q: 0.020099
 37406/100000: episode: 3746, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.002091, mean_q: 0.019133
 37416/100000: episode: 3747, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000055, mae: 0.002745, mean_q: 0.018971
 37426/100000: episode: 3748, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.001927, mean_q: 0.020284
 37436/100000: episode: 3749, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000058, mae: 0.002412, mean_q: 0.019851
 37446/100000: episode: 3750, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000051, mae: 0.002100, mean_q: 0.020274
 37456/100000: episode: 3751, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000013, mae: 0.001778, mean_q: 0.019718
 37466/100000: episode: 3752, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000010, mae: 0.001822, mean_q: 0.018900
 37476/100000: episode: 3753, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000036, mae: 0.002024, mean_q: 0.019454
 37486/100000: episode: 3754, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000015, mae: 0.001785, mean_q: 0.019588
 37496/100000: episode: 3755, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.002758, mean_q: 0.018419
 37506/100000: episode: 3756, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000012, mae: 0.002114, mean_q: 0.018766
 37516/100000: episode: 3757, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000056, mae: 0.002460, mean_q: 0.018875
 37526/100000: episode: 3758, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000022, mae: 0.002467, mean_q: 0.020055
 37536/100000: episode: 3759, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000016, mae: 0.002147, mean_q: 0.019034
 37546/100000: episode: 3760, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000011, mae: 0.001929, mean_q: 0.018437
 37556/100000: episode: 3761, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000013, mae: 0.001354, mean_q: 0.019293
 37566/100000: episode: 3762, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.001625, mean_q: 0.019127
 37576/100000: episode: 3763, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000078, mae: 0.002708, mean_q: 0.019196
 37586/100000: episode: 3764, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000058, mae: 0.002951, mean_q: 0.020066
 37596/100000: episode: 3765, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000060, mae: 0.002737, mean_q: 0.019740
 37606/100000: episode: 3766, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000060, mae: 0.003241, mean_q: 0.020383
 37616/100000: episode: 3767, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000012, mae: 0.001784, mean_q: 0.019812
 37626/100000: episode: 3768, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.002197, mean_q: 0.018423
 37636/100000: episode: 3769, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000010, mae: 0.001136, mean_q: 0.019294
 37646/100000: episode: 3770, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.001539, mae: 0.005778, mean_q: 0.020313
 37656/100000: episode: 3771, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000063, mae: 0.004513, mean_q: 0.021878
 37666/100000: episode: 3772, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000019, mae: 0.001768, mean_q: 0.019839
 37676/100000: episode: 3773, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000040, mae: 0.002762, mean_q: 0.019043
 37686/100000: episode: 3774, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000057, mae: 0.002529, mean_q: 0.019370
 37696/100000: episode: 3775, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000016, mae: 0.002199, mean_q: 0.019470
 37706/100000: episode: 3776, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000009, mae: 0.002317, mean_q: 0.018160
 37716/100000: episode: 3777, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000050, mae: 0.002792, mean_q: 0.018139
 37726/100000: episode: 3778, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000033, mae: 0.002229, mean_q: 0.019985
 37736/100000: episode: 3779, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000243, mae: 0.003053, mean_q: 0.019532
 37746/100000: episode: 3780, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000033, mae: 0.002330, mean_q: 0.020328
 37756/100000: episode: 3781, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000015, mae: 0.002004, mean_q: 0.019055
 37766/100000: episode: 3782, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000052, mae: 0.002618, mean_q: 0.018334
 37776/100000: episode: 3783, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000042, mae: 0.002515, mean_q: 0.019732
 37786/100000: episode: 3784, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001512, mae: 0.004423, mean_q: 0.019697
 37796/100000: episode: 3785, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000017, mae: 0.003455, mean_q: 0.021719
 37806/100000: episode: 3786, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000012, mae: 0.002281, mean_q: 0.018363
 37816/100000: episode: 3787, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.002174, mean_q: 0.018548
 37826/100000: episode: 3788, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000014, mae: 0.001407, mean_q: 0.019463
 37836/100000: episode: 3789, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000037, mae: 0.001962, mean_q: 0.019357
 37846/100000: episode: 3790, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.001638, mean_q: 0.019267
 37856/100000: episode: 3791, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000062, mae: 0.003209, mean_q: 0.018494
 37866/100000: episode: 3792, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000032, mae: 0.001776, mean_q: 0.019562
 37876/100000: episode: 3793, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000243, mae: 0.003378, mean_q: 0.019886
 37886/100000: episode: 3794, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.002167, mean_q: 0.020442
[Info] 1-TH LEVEL FOUND: 0.018730569630861282, Considering 100/100 traces
 37896/100000: episode: 3795, duration: 0.676s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000010, mae: 0.001663, mean_q: 0.018708
[Info] 2-TH LEVEL FOUND: 0.019821781665086746, Considering 100/100 traces
 37906/100000: episode: 3796, duration: 0.660s, episode steps: 10, steps per second: 15, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000055, mae: 0.002336, mean_q: 0.019401
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.019821781665086746
2
 37916/100000: episode: 3797, duration: 0.497s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000020, mae: 0.002321, mean_q: 0.019614
 37926/100000: episode: 3798, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001515, mae: 0.005235, mean_q: 0.019690
 37936/100000: episode: 3799, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000058, mae: 0.004241, mean_q: 0.021766
 37946/100000: episode: 3800, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.002753, mean_q: 0.018169
 37956/100000: episode: 3801, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000034, mae: 0.002699, mean_q: 0.018185
 37966/100000: episode: 3802, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000233, mae: 0.004485, mean_q: 0.020546
 37976/100000: episode: 3803, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000014, mae: 0.001702, mean_q: 0.019777
 37986/100000: episode: 3804, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000012, mae: 0.002449, mean_q: 0.018333
 37996/100000: episode: 3805, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000034, mae: 0.002323, mean_q: 0.018661
Step 38000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.38000.hdf5
 38006/100000: episode: 3806, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000034, mae: 0.001907, mean_q: 0.019095
 38016/100000: episode: 3807, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000012, mae: 0.001659, mean_q: 0.018791
 38026/100000: episode: 3808, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000039, mae: 0.002366, mean_q: 0.018745
 38036/100000: episode: 3809, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000056, mae: 0.002624, mean_q: 0.019974
 38046/100000: episode: 3810, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001578, mae: 0.006680, mean_q: 0.020816
 38056/100000: episode: 3811, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000248, mae: 0.007115, mean_q: 0.023971
 38066/100000: episode: 3812, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000012, mae: 0.002117, mean_q: 0.019771
 38076/100000: episode: 3813, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001540, mae: 0.006097, mean_q: 0.020173
 38086/100000: episode: 3814, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.003584, mean_q: 0.022037
 38096/100000: episode: 3815, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.002667, mean_q: 0.018747
 38106/100000: episode: 3816, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000036, mae: 0.002567, mean_q: 0.019230
 38116/100000: episode: 3817, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000030, mae: 0.001568, mean_q: 0.020132
 38126/100000: episode: 3818, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000036, mae: 0.002082, mean_q: 0.019595
 38136/100000: episode: 3819, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000010, mae: 0.001555, mean_q: 0.019394
 38146/100000: episode: 3820, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000030, mae: 0.001580, mean_q: 0.019607
 38156/100000: episode: 3821, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000028, mae: 0.001583, mean_q: 0.019501
 38166/100000: episode: 3822, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.001511, mae: 0.004480, mean_q: 0.019159
 38176/100000: episode: 3823, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000054, mae: 0.004182, mean_q: 0.022280
 38186/100000: episode: 3824, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000023, mae: 0.002763, mean_q: 0.019887
 38196/100000: episode: 3825, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000014, mae: 0.002147, mean_q: 0.019012
 38206/100000: episode: 3826, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000052, mae: 0.002061, mean_q: 0.019605
 38216/100000: episode: 3827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000012, mae: 0.001366, mean_q: 0.019898
 38226/100000: episode: 3828, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000016, mae: 0.002374, mean_q: 0.018976
 38236/100000: episode: 3829, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000080, mae: 0.003176, mean_q: 0.019420
 38246/100000: episode: 3830, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000035, mae: 0.002386, mean_q: 0.020207
 38256/100000: episode: 3831, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.002945, mean_q: 0.018407
 38266/100000: episode: 3832, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000051, mae: 0.002400, mean_q: 0.018811
 38276/100000: episode: 3833, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000013, mae: 0.001460, mean_q: 0.019445
 38286/100000: episode: 3834, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000013, mae: 0.002233, mean_q: 0.018395
 38296/100000: episode: 3835, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000011, mae: 0.001565, mean_q: 0.018836
 38306/100000: episode: 3836, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000039, mae: 0.002237, mean_q: 0.019147
 38316/100000: episode: 3837, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000031, mae: 0.001640, mean_q: 0.019072
 38326/100000: episode: 3838, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000011, mae: 0.001820, mean_q: 0.018683
 38336/100000: episode: 3839, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000010, mae: 0.002288, mean_q: 0.017908
 38346/100000: episode: 3840, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000030, mae: 0.001687, mean_q: 0.018724
 38356/100000: episode: 3841, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000007, mae: 0.001029, mean_q: 0.019022
 38366/100000: episode: 3842, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000037, mae: 0.002121, mean_q: 0.019029
 38376/100000: episode: 3843, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000011, mae: 0.001540, mean_q: 0.018456
 38386/100000: episode: 3844, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000204, mae: 0.003148, mean_q: 0.019737
 38396/100000: episode: 3845, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001734, mae: 0.008944, mean_q: 0.022259
 38406/100000: episode: 3846, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001515, mae: 0.007335, mean_q: 0.022445
 38416/100000: episode: 3847, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000033, mae: 0.002188, mean_q: 0.019770
 38426/100000: episode: 3848, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000017, mae: 0.002420, mean_q: 0.018678
 38436/100000: episode: 3849, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000038, mae: 0.002191, mean_q: 0.019480
 38446/100000: episode: 3850, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001538, mae: 0.005594, mean_q: 0.020115
 38456/100000: episode: 3851, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000038, mae: 0.003642, mean_q: 0.021795
 38466/100000: episode: 3852, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000094, mae: 0.002965, mean_q: 0.020223
 38476/100000: episode: 3853, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000018, mae: 0.002425, mean_q: 0.019357
 38486/100000: episode: 3854, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.002273, mean_q: 0.018859
 38496/100000: episode: 3855, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002386, mean_q: 0.020409
 38506/100000: episode: 3856, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001518, mae: 0.006438, mean_q: 0.021904
 38516/100000: episode: 3857, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000080, mae: 0.004045, mean_q: 0.021519
 38526/100000: episode: 3858, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000019, mae: 0.002707, mean_q: 0.019107
 38536/100000: episode: 3859, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000010, mae: 0.002084, mean_q: 0.018871
 38546/100000: episode: 3860, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.001694, mean_q: 0.019849
 38556/100000: episode: 3861, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000013, mae: 0.002189, mean_q: 0.018969
 38566/100000: episode: 3862, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002478, mean_q: 0.019429
 38576/100000: episode: 3863, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000094, mae: 0.003114, mean_q: 0.020351
 38586/100000: episode: 3864, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000225, mae: 0.004092, mean_q: 0.021146
 38596/100000: episode: 3865, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000032, mae: 0.001921, mean_q: 0.019698
 38606/100000: episode: 3866, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002101, mean_q: 0.019474
 38616/100000: episode: 3867, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000055, mae: 0.002390, mean_q: 0.020017
 38626/100000: episode: 3868, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000057, mae: 0.002830, mean_q: 0.020532
 38636/100000: episode: 3869, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000035, mae: 0.002256, mean_q: 0.020448
 38646/100000: episode: 3870, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000010, mae: 0.001730, mean_q: 0.019143
 38656/100000: episode: 3871, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000037, mae: 0.002135, mean_q: 0.019610
 38666/100000: episode: 3872, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000075, mae: 0.003184, mean_q: 0.020756
 38676/100000: episode: 3873, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000013, mae: 0.001724, mean_q: 0.020537
 38686/100000: episode: 3874, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000075, mae: 0.003217, mean_q: 0.019130
 38696/100000: episode: 3875, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000079, mae: 0.003005, mean_q: 0.019872
 38706/100000: episode: 3876, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000032, mae: 0.002095, mean_q: 0.020772
 38716/100000: episode: 3877, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000057, mae: 0.002228, mean_q: 0.020064
 38726/100000: episode: 3878, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000011, mae: 0.001591, mean_q: 0.020328
 38736/100000: episode: 3879, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000016, mae: 0.002121, mean_q: 0.019190
 38746/100000: episode: 3880, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000075, mae: 0.002876, mean_q: 0.020434
 38756/100000: episode: 3881, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002231, mean_q: 0.020619
 38766/100000: episode: 3882, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000038, mae: 0.002829, mean_q: 0.019229
 38776/100000: episode: 3883, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.002250, mean_q: 0.019077
 38786/100000: episode: 3884, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000033, mae: 0.001979, mean_q: 0.019406
 38796/100000: episode: 3885, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000082, mae: 0.003392, mean_q: 0.020391
 38806/100000: episode: 3886, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000221, mae: 0.003583, mean_q: 0.021001
 38816/100000: episode: 3887, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.001558, mae: 0.006851, mean_q: 0.021572
 38826/100000: episode: 3888, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000207, mae: 0.005397, mean_q: 0.023281
 38836/100000: episode: 3889, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000034, mae: 0.002340, mean_q: 0.019772
 38846/100000: episode: 3890, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000015, mae: 0.002245, mean_q: 0.019557
 38856/100000: episode: 3891, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000015, mae: 0.001994, mean_q: 0.019756
 38866/100000: episode: 3892, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000053, mae: 0.002461, mean_q: 0.019685
 38876/100000: episode: 3893, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000012, mae: 0.001341, mean_q: 0.020082
 38886/100000: episode: 3894, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000305, mae: 0.004479, mean_q: 0.020211
 38896/100000: episode: 3895, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000083, mae: 0.004871, mean_q: 0.022383
 38906/100000: episode: 3896, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000021, mae: 0.002706, mean_q: 0.020233
[Info] 1-TH LEVEL FOUND: 0.01997123286128044, Considering 100/100 traces
 38916/100000: episode: 3897, duration: 0.723s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000037, mae: 0.002915, mean_q: 0.018968
[Info] 2-TH LEVEL FOUND: 0.02242511883378029, Considering 100/100 traces
 38926/100000: episode: 3898, duration: 0.787s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000220, mae: 0.003147, mean_q: 0.020959
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02242511883378029
2
 38936/100000: episode: 3899, duration: 0.498s, episode steps: 10, steps per second: 20, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000267, mae: 0.005947, mean_q: 0.022739
 38946/100000: episode: 3900, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000017, mae: 0.002500, mean_q: 0.021766
 38956/100000: episode: 3901, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001554, mae: 0.005827, mean_q: 0.021043
 38966/100000: episode: 3902, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000041, mae: 0.004399, mean_q: 0.023573
 38976/100000: episode: 3903, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000013, mae: 0.002069, mean_q: 0.020434
 38986/100000: episode: 3904, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000013, mae: 0.002484, mean_q: 0.019576
 38996/100000: episode: 3905, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000075, mae: 0.002707, mean_q: 0.020626
Step 39000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.39000.hdf5
 39006/100000: episode: 3906, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.003027, mae: 0.011000, mean_q: 0.024469
 39016/100000: episode: 3907, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000063, mae: 0.005913, mean_q: 0.025705
 39026/100000: episode: 3908, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002244, mean_q: 0.022110
 39036/100000: episode: 3909, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.003028, mae: 0.010140, mean_q: 0.023724
 39046/100000: episode: 3910, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000084, mae: 0.006317, mean_q: 0.026088
 39056/100000: episode: 3911, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000054, mae: 0.002663, mean_q: 0.021839
 39066/100000: episode: 3912, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000052, mae: 0.002357, mean_q: 0.021691
 39076/100000: episode: 3913, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000202, mae: 0.002787, mean_q: 0.022533
 39086/100000: episode: 3914, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000051, mae: 0.002386, mean_q: 0.022879
 39096/100000: episode: 3915, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001506, mae: 0.004644, mean_q: 0.021679
 39106/100000: episode: 3916, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000094, mae: 0.004668, mean_q: 0.024409
 39116/100000: episode: 3917, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000204, mae: 0.004071, mean_q: 0.023964
 39126/100000: episode: 3918, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000207, mae: 0.003508, mean_q: 0.021825
 39136/100000: episode: 3919, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000032, mae: 0.001910, mean_q: 0.022762
 39146/100000: episode: 3920, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000033, mae: 0.002014, mean_q: 0.022219
 39156/100000: episode: 3921, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000072, mae: 0.002878, mean_q: 0.021862
 39166/100000: episode: 3922, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000032, mae: 0.001863, mean_q: 0.022836
 39176/100000: episode: 3923, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000017, mae: 0.001905, mean_q: 0.022188
 39186/100000: episode: 3924, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000196, mae: 0.002243, mean_q: 0.022142
 39196/100000: episode: 3925, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000017, mae: 0.002313, mean_q: 0.021740
 39206/100000: episode: 3926, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000015, mae: 0.002525, mean_q: 0.021151
 39216/100000: episode: 3927, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.002300, mean_q: 0.021374
 39226/100000: episode: 3928, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002479, mean_q: 0.021247
 39236/100000: episode: 3929, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000239, mae: 0.003451, mean_q: 0.021477
 39246/100000: episode: 3930, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000039, mae: 0.003555, mean_q: 0.023669
 39256/100000: episode: 3931, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002271, mean_q: 0.022370
 39266/100000: episode: 3932, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000079, mae: 0.003646, mean_q: 0.021150
 39276/100000: episode: 3933, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000021, mae: 0.002297, mean_q: 0.022215
 39286/100000: episode: 3934, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002788, mean_q: 0.021181
 39296/100000: episode: 3935, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000037, mae: 0.002463, mean_q: 0.021426
 39306/100000: episode: 3936, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000016, mae: 0.001916, mean_q: 0.021457
 39316/100000: episode: 3937, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000034, mae: 0.002062, mean_q: 0.021455
 39326/100000: episode: 3938, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000078, mae: 0.003263, mean_q: 0.021451
 39336/100000: episode: 3939, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002176, mean_q: 0.021098
 39346/100000: episode: 3940, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000063, mae: 0.003231, mean_q: 0.022116
 39356/100000: episode: 3941, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000053, mae: 0.002395, mean_q: 0.021594
 39366/100000: episode: 3942, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000040, mae: 0.002402, mean_q: 0.021449
 39376/100000: episode: 3943, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000038, mae: 0.002211, mean_q: 0.021792
 39386/100000: episode: 3944, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000012, mae: 0.001937, mean_q: 0.020956
 39396/100000: episode: 3945, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.002527, mean_q: 0.021046
 39406/100000: episode: 3946, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000033, mae: 0.002086, mean_q: 0.021035
 39416/100000: episode: 3947, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.001836, mean_q: 0.021418
 39426/100000: episode: 3948, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000010, mae: 0.001635, mean_q: 0.020961
 39436/100000: episode: 3949, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000225, mae: 0.003784, mean_q: 0.020452
 39446/100000: episode: 3950, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000031, mae: 0.002312, mean_q: 0.022350
 39456/100000: episode: 3951, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000033, mae: 0.002191, mean_q: 0.021358
 39466/100000: episode: 3952, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002864, mean_q: 0.020083
 39476/100000: episode: 3953, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000243, mae: 0.003841, mean_q: 0.021406
 39486/100000: episode: 3954, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002929, mean_q: 0.022484
 39496/100000: episode: 3955, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002614, mean_q: 0.020961
 39506/100000: episode: 3956, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000095, mae: 0.003106, mean_q: 0.021043
 39516/100000: episode: 3957, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000202, mae: 0.003065, mean_q: 0.021913
 39526/100000: episode: 3958, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000025, mae: 0.002841, mean_q: 0.021336
 39536/100000: episode: 3959, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000030, mae: 0.002224, mean_q: 0.020417
 39546/100000: episode: 3960, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000013, mae: 0.001727, mean_q: 0.020713
 39556/100000: episode: 3961, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000056, mae: 0.002435, mean_q: 0.021118
 39566/100000: episode: 3962, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000229, mae: 0.004140, mean_q: 0.022025
 39576/100000: episode: 3963, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000010, mae: 0.001633, mean_q: 0.021232
 39586/100000: episode: 3964, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000229, mae: 0.003888, mean_q: 0.020673
 39596/100000: episode: 3965, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000055, mae: 0.003424, mean_q: 0.022726
 39606/100000: episode: 3966, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000011, mae: 0.001800, mean_q: 0.020888
 39616/100000: episode: 3967, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000029, mae: 0.002478, mean_q: 0.020023
 39626/100000: episode: 3968, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002124, mean_q: 0.020426
 39636/100000: episode: 3969, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000034, mae: 0.001989, mean_q: 0.021475
 39646/100000: episode: 3970, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002109, mean_q: 0.021255
 39656/100000: episode: 3971, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000057, mae: 0.002732, mean_q: 0.020529
 39666/100000: episode: 3972, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000036, mae: 0.002128, mean_q: 0.021406
 39676/100000: episode: 3973, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000008, mae: 0.001423, mean_q: 0.020412
 39686/100000: episode: 3974, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000038, mae: 0.002316, mean_q: 0.021156
 39696/100000: episode: 3975, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000053, mae: 0.002188, mean_q: 0.021206
 39706/100000: episode: 3976, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002099, mean_q: 0.021012
 39716/100000: episode: 3977, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000032, mae: 0.001634, mean_q: 0.021108
 39726/100000: episode: 3978, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000204, mae: 0.002842, mean_q: 0.021011
 39736/100000: episode: 3979, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.002477, mean_q: 0.020791
 39746/100000: episode: 3980, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000054, mae: 0.002792, mean_q: 0.020200
 39756/100000: episode: 3981, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000223, mae: 0.003360, mean_q: 0.021280
 39766/100000: episode: 3982, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000076, mae: 0.003258, mean_q: 0.021745
 39776/100000: episode: 3983, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000062, mae: 0.002930, mean_q: 0.021315
 39786/100000: episode: 3984, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000033, mae: 0.002083, mean_q: 0.020547
 39796/100000: episode: 3985, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000011, mae: 0.001989, mean_q: 0.020147
 39806/100000: episode: 3986, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000031, mae: 0.001979, mean_q: 0.020225
 39816/100000: episode: 3987, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.002302, mean_q: 0.020891
 39826/100000: episode: 3988, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000038, mae: 0.003071, mean_q: 0.022056
 39836/100000: episode: 3989, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000011, mae: 0.001727, mean_q: 0.020529
 39846/100000: episode: 3990, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000034, mae: 0.002849, mean_q: 0.019563
 39856/100000: episode: 3991, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000030, mae: 0.001875, mean_q: 0.020213
 39866/100000: episode: 3992, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.002290, mean_q: 0.020369
 39876/100000: episode: 3993, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000266, mae: 0.004382, mean_q: 0.021169
 39886/100000: episode: 3994, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000095, mae: 0.003670, mean_q: 0.021773
 39896/100000: episode: 3995, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000061, mae: 0.002928, mean_q: 0.021204
 39906/100000: episode: 3996, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000057, mae: 0.002854, mean_q: 0.020272
 39916/100000: episode: 3997, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000224, mae: 0.003699, mean_q: 0.021636
 39926/100000: episode: 3998, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.002198, mean_q: 0.020739
[Info] 1-TH LEVEL FOUND: 0.020964765921235085, Considering 100/100 traces
 39936/100000: episode: 3999, duration: 0.715s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000052, mae: 0.002518, mean_q: 0.020038
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020964765921235085
1
 39946/100000: episode: 4000, duration: 0.528s, episode steps: 10, steps per second: 19, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000039, mae: 0.002252, mean_q: 0.020986
 39956/100000: episode: 4001, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000075, mae: 0.002420, mean_q: 0.021018
 39966/100000: episode: 4002, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000235, mae: 0.003464, mean_q: 0.021907
 39976/100000: episode: 4003, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.002461, mean_q: 0.022433
 39986/100000: episode: 4004, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000431, mae: 0.004824, mean_q: 0.021615
 39996/100000: episode: 4005, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000202, mae: 0.003781, mean_q: 0.022589
Step 40000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.40000.hdf5
 40006/100000: episode: 4006, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000053, mae: 0.002232, mean_q: 0.020998
 40016/100000: episode: 4007, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000163, mae: 0.004934, mean_q: 0.021461
 40026/100000: episode: 4008, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000041, mae: 0.003219, mean_q: 0.022349
 40036/100000: episode: 4009, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.003733, mean_q: 0.019780
 40046/100000: episode: 4010, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000018, mae: 0.002397, mean_q: 0.020378
 40056/100000: episode: 4011, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000007, mae: 0.001038, mean_q: 0.020990
 40066/100000: episode: 4012, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000050, mae: 0.002434, mean_q: 0.020198
 40076/100000: episode: 4013, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000027, mae: 0.001307, mean_q: 0.021148
 40086/100000: episode: 4014, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000037, mae: 0.002398, mean_q: 0.020644
 40096/100000: episode: 4015, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000078, mae: 0.003250, mean_q: 0.020243
 40106/100000: episode: 4016, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000013, mae: 0.002385, mean_q: 0.022192
 40116/100000: episode: 4017, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000054, mae: 0.002480, mean_q: 0.021381
 40126/100000: episode: 4018, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000073, mae: 0.002623, mean_q: 0.020643
 40136/100000: episode: 4019, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000098, mae: 0.004044, mean_q: 0.022289
 40146/100000: episode: 4020, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000015, mae: 0.002309, mean_q: 0.021082
 40156/100000: episode: 4021, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.003420, mean_q: 0.019349
 40166/100000: episode: 4022, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002423, mean_q: 0.021332
 40176/100000: episode: 4023, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000016, mae: 0.002248, mean_q: 0.021005
 40186/100000: episode: 4024, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000018, mae: 0.002911, mean_q: 0.019415
 40196/100000: episode: 4025, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000073, mae: 0.002809, mean_q: 0.021216
 40206/100000: episode: 4026, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000243, mae: 0.003526, mean_q: 0.021095
 40216/100000: episode: 4027, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002070, mean_q: 0.020951
 40226/100000: episode: 4028, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000230, mae: 0.004108, mean_q: 0.021556
 40236/100000: episode: 4029, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000034, mae: 0.002227, mean_q: 0.020519
 40246/100000: episode: 4030, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000030, mae: 0.001924, mean_q: 0.020124
 40256/100000: episode: 4031, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000032, mae: 0.001681, mean_q: 0.020787
 40266/100000: episode: 4032, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.002503, mean_q: 0.020218
 40276/100000: episode: 4033, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000007, mae: 0.001001, mean_q: 0.020612
 40286/100000: episode: 4034, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000028, mae: 0.001647, mean_q: 0.020369
 40296/100000: episode: 4035, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000077, mae: 0.003036, mean_q: 0.021117
 40306/100000: episode: 4036, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000054, mae: 0.002600, mean_q: 0.021400
 40316/100000: episode: 4037, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000073, mae: 0.002653, mean_q: 0.020558
 40326/100000: episode: 4038, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000074, mae: 0.002756, mean_q: 0.021084
 40336/100000: episode: 4039, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000243, mae: 0.004459, mean_q: 0.022304
 40346/100000: episode: 4040, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000064, mae: 0.003436, mean_q: 0.021165
 40356/100000: episode: 4041, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000053, mae: 0.002259, mean_q: 0.020641
 40366/100000: episode: 4042, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000071, mae: 0.002255, mean_q: 0.020958
 40376/100000: episode: 4043, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000014, mae: 0.001800, mean_q: 0.020873
 40386/100000: episode: 4044, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000049, mae: 0.002005, mean_q: 0.020353
 40396/100000: episode: 4045, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000032, mae: 0.001717, mean_q: 0.020678
 40406/100000: episode: 4046, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000057, mae: 0.002495, mean_q: 0.020871
 40416/100000: episode: 4047, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000076, mae: 0.002578, mean_q: 0.021034
 40426/100000: episode: 4048, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.002218, mean_q: 0.021184
 40436/100000: episode: 4049, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000036, mae: 0.002522, mean_q: 0.020059
 40446/100000: episode: 4050, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000072, mae: 0.002288, mean_q: 0.020997
 40456/100000: episode: 4051, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000056, mae: 0.003649, mean_q: 0.022590
 40466/100000: episode: 4052, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000076, mae: 0.002875, mean_q: 0.020902
 40476/100000: episode: 4053, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000036, mae: 0.002182, mean_q: 0.021098
 40486/100000: episode: 4054, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.001847, mean_q: 0.020625
 40496/100000: episode: 4055, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000014, mae: 0.002059, mean_q: 0.020288
 40506/100000: episode: 4056, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000201, mae: 0.003407, mean_q: 0.022003
 40516/100000: episode: 4057, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000031, mae: 0.002288, mean_q: 0.021766
 40526/100000: episode: 4058, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000016, mae: 0.002759, mean_q: 0.019591
 40536/100000: episode: 4059, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000016, mae: 0.002273, mean_q: 0.020136
 40546/100000: episode: 4060, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000250, mae: 0.004038, mean_q: 0.020996
 40556/100000: episode: 4061, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000093, mae: 0.004414, mean_q: 0.022965
 40566/100000: episode: 4062, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002795, mean_q: 0.021249
 40576/100000: episode: 4063, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000246, mae: 0.004413, mean_q: 0.019852
 40586/100000: episode: 4064, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000035, mae: 0.002577, mean_q: 0.021903
 40596/100000: episode: 4065, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000094, mae: 0.002934, mean_q: 0.021030
 40606/100000: episode: 4066, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000037, mae: 0.002351, mean_q: 0.020729
 40616/100000: episode: 4067, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000032, mae: 0.002010, mean_q: 0.020501
 40626/100000: episode: 4068, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000013, mae: 0.001847, mean_q: 0.020279
 40636/100000: episode: 4069, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000077, mae: 0.002994, mean_q: 0.020545
 40646/100000: episode: 4070, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000031, mae: 0.001799, mean_q: 0.021226
 40656/100000: episode: 4071, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000034, mae: 0.002048, mean_q: 0.020534
 40666/100000: episode: 4072, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000055, mae: 0.002245, mean_q: 0.020872
 40676/100000: episode: 4073, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000055, mae: 0.002329, mean_q: 0.021218
 40686/100000: episode: 4074, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000038, mae: 0.002321, mean_q: 0.021171
 40696/100000: episode: 4075, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000033, mae: 0.001844, mean_q: 0.020629
 40706/100000: episode: 4076, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000057, mae: 0.002741, mean_q: 0.020373
 40716/100000: episode: 4077, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000072, mae: 0.002452, mean_q: 0.020867
 40726/100000: episode: 4078, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000095, mae: 0.003069, mean_q: 0.021095
 40736/100000: episode: 4079, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000056, mae: 0.002215, mean_q: 0.020952
 40746/100000: episode: 4080, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000136, mae: 0.004482, mean_q: 0.021916
 40756/100000: episode: 4081, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000041, mae: 0.003048, mean_q: 0.020832
 40766/100000: episode: 4082, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000286, mae: 0.004971, mean_q: 0.019756
 40776/100000: episode: 4083, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000039, mae: 0.004110, mean_q: 0.023211
 40786/100000: episode: 4084, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000057, mae: 0.003159, mean_q: 0.019713
 40796/100000: episode: 4085, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000051, mae: 0.002431, mean_q: 0.021276
 40806/100000: episode: 4086, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002710, mean_q: 0.022056
 40816/100000: episode: 4087, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000263, mae: 0.003942, mean_q: 0.020554
 40826/100000: episode: 4088, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000284, mae: 0.005674, mean_q: 0.022829
 40836/100000: episode: 4089, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000096, mae: 0.004458, mean_q: 0.023082
 40846/100000: episode: 4090, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000089, mae: 0.002498, mean_q: 0.021745
 40856/100000: episode: 4091, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002429, mean_q: 0.021577
 40866/100000: episode: 4092, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000052, mae: 0.002576, mean_q: 0.022155
 40876/100000: episode: 4093, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000053, mae: 0.002536, mean_q: 0.021735
 40886/100000: episode: 4094, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000034, mae: 0.002156, mean_q: 0.020989
 40896/100000: episode: 4095, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000075, mae: 0.002942, mean_q: 0.020914
 40906/100000: episode: 4096, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000013, mae: 0.001730, mean_q: 0.021772
 40916/100000: episode: 4097, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000015, mae: 0.002107, mean_q: 0.020818
 40926/100000: episode: 4098, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000018, mae: 0.002957, mean_q: 0.019907
 40936/100000: episode: 4099, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000054, mae: 0.002357, mean_q: 0.020776
[Info] 1-TH LEVEL FOUND: 0.02170942910015583, Considering 100/100 traces
 40946/100000: episode: 4100, duration: 0.715s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000055, mae: 0.002901, mean_q: 0.021977
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02170942910015583
1
 40956/100000: episode: 4101, duration: 0.498s, episode steps: 10, steps per second: 20, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000013, mae: 0.001625, mean_q: 0.021109
 40966/100000: episode: 4102, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000032, mae: 0.002480, mean_q: 0.020094
 40976/100000: episode: 4103, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000006, mae: 0.001220, mean_q: 0.020647
[Info] FALSIFICATION!
 40986/100000: episode: 4104, duration: 0.291s, episode steps: 10, steps per second: 34, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000073, mae: 0.002585, mean_q: 0.021066
 40996/100000: episode: 4105, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000247, mae: 0.003803, mean_q: 0.020766
Step 41000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.41000.hdf5
 41006/100000: episode: 4106, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000012, mae: 0.001780, mean_q: 0.021311
 41016/100000: episode: 4107, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000056, mae: 0.002985, mean_q: 0.020093
 41026/100000: episode: 4108, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000076, mae: 0.003159, mean_q: 0.021574
 41036/100000: episode: 4109, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000054, mae: 0.002693, mean_q: 0.021446
 41046/100000: episode: 4110, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000076, mae: 0.003025, mean_q: 0.020373
 41056/100000: episode: 4111, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.001781, mean_q: 0.021258
 41066/100000: episode: 4112, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000411, mae: 0.004933, mean_q: 0.022243
 41076/100000: episode: 4113, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000098, mae: 0.004318, mean_q: 0.022330
 41086/100000: episode: 4114, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000218, mae: 0.003544, mean_q: 0.019843
 41096/100000: episode: 4115, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000060, mae: 0.003779, mean_q: 0.022458
 41106/100000: episode: 4116, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000037, mae: 0.002573, mean_q: 0.021204
 41116/100000: episode: 4117, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001595, mae: 0.008350, mean_q: 0.023895
 41126/100000: episode: 4118, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000060, mae: 0.004238, mean_q: 0.023311
 41136/100000: episode: 4119, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000096, mae: 0.004074, mean_q: 0.019919
 41146/100000: episode: 4120, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000060, mae: 0.003770, mean_q: 0.023015
 41156/100000: episode: 4121, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001893, mae: 0.010669, mean_q: 0.025757
 41166/100000: episode: 4122, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000140, mae: 0.005564, mean_q: 0.023748
 41176/100000: episode: 4123, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000071, mae: 0.002770, mean_q: 0.021606
 41186/100000: episode: 4124, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000038, mae: 0.002245, mean_q: 0.022347
 41196/100000: episode: 4125, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000276, mae: 0.004569, mean_q: 0.023359
 41206/100000: episode: 4126, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000013, mae: 0.002121, mean_q: 0.023090
 41216/100000: episode: 4127, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001530, mae: 0.005860, mean_q: 0.023025
 41226/100000: episode: 4128, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001567, mae: 0.007727, mean_q: 0.024659
 41236/100000: episode: 4129, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000270, mae: 0.006028, mean_q: 0.024535
 41246/100000: episode: 4130, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000072, mae: 0.002717, mean_q: 0.022631
 41256/100000: episode: 4131, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000035, mae: 0.002303, mean_q: 0.022322
 41266/100000: episode: 4132, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000037, mae: 0.002409, mean_q: 0.022432
 41276/100000: episode: 4133, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000073, mae: 0.002593, mean_q: 0.022562
 41286/100000: episode: 4134, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000052, mae: 0.002478, mean_q: 0.023264
 41296/100000: episode: 4135, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000075, mae: 0.003244, mean_q: 0.022065
 41306/100000: episode: 4136, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000028, mae: 0.001442, mean_q: 0.022754
 41316/100000: episode: 4137, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.002406, mean_q: 0.022621
 41326/100000: episode: 4138, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000034, mae: 0.002199, mean_q: 0.022069
 41336/100000: episode: 4139, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.001841, mean_q: 0.022832
 41346/100000: episode: 4140, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000118, mae: 0.004121, mean_q: 0.023235
 41356/100000: episode: 4141, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000056, mae: 0.002697, mean_q: 0.022938
 41366/100000: episode: 4142, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000055, mae: 0.002886, mean_q: 0.021968
 41376/100000: episode: 4143, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001536, mae: 0.007367, mean_q: 0.025049
 41386/100000: episode: 4144, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000038, mae: 0.003503, mean_q: 0.024660
 41396/100000: episode: 4145, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000058, mae: 0.004121, mean_q: 0.020900
 41406/100000: episode: 4146, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000053, mae: 0.002537, mean_q: 0.022421
 41416/100000: episode: 4147, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000037, mae: 0.002310, mean_q: 0.022775
 41426/100000: episode: 4148, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001528, mae: 0.005283, mean_q: 0.022323
 41436/100000: episode: 4149, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.004593, mean_q: 0.025108
 41446/100000: episode: 4150, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000031, mae: 0.002063, mean_q: 0.022860
 41456/100000: episode: 4151, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000220, mae: 0.003401, mean_q: 0.022089
 41466/100000: episode: 4152, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000044, mae: 0.003016, mean_q: 0.023329
 41476/100000: episode: 4153, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000033, mae: 0.002419, mean_q: 0.022153
 41486/100000: episode: 4154, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000029, mae: 0.001779, mean_q: 0.022437
 41496/100000: episode: 4155, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000039, mae: 0.002292, mean_q: 0.022988
 41506/100000: episode: 4156, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000036, mae: 0.002581, mean_q: 0.022098
 41516/100000: episode: 4157, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001588, mae: 0.007121, mean_q: 0.023216
 41526/100000: episode: 4158, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000062, mae: 0.005472, mean_q: 0.026308
 41536/100000: episode: 4159, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000034, mae: 0.002900, mean_q: 0.021819
 41546/100000: episode: 4160, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000035, mae: 0.003223, mean_q: 0.021424
 41556/100000: episode: 4161, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000031, mae: 0.001766, mean_q: 0.022680
 41566/100000: episode: 4162, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000415, mae: 0.005372, mean_q: 0.023552
 41576/100000: episode: 4163, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000030, mae: 0.002274, mean_q: 0.023880
 41586/100000: episode: 4164, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000112, mae: 0.003428, mean_q: 0.022609
 41596/100000: episode: 4165, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.001905, mean_q: 0.023429
 41606/100000: episode: 4166, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000092, mae: 0.002895, mean_q: 0.022886
 41616/100000: episode: 4167, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000088, mae: 0.002770, mean_q: 0.023436
 41626/100000: episode: 4168, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000081, mae: 0.003534, mean_q: 0.023573
 41636/100000: episode: 4169, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000231, mae: 0.003909, mean_q: 0.022917
 41646/100000: episode: 4170, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000034, mae: 0.002350, mean_q: 0.022490
 41656/100000: episode: 4171, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002613, mean_q: 0.022153
 41666/100000: episode: 4172, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000096, mae: 0.003482, mean_q: 0.022477
 41676/100000: episode: 4173, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000242, mae: 0.003706, mean_q: 0.023331
 41686/100000: episode: 4174, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002200, mean_q: 0.023039
 41696/100000: episode: 4175, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000014, mae: 0.002465, mean_q: 0.021680
 41706/100000: episode: 4176, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000033, mae: 0.002226, mean_q: 0.022123
 41716/100000: episode: 4177, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000036, mae: 0.002268, mean_q: 0.022392
 41726/100000: episode: 4178, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002356, mean_q: 0.022054
 41736/100000: episode: 4179, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000033, mae: 0.001902, mean_q: 0.022707
 41746/100000: episode: 4180, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000034, mae: 0.002188, mean_q: 0.022910
 41756/100000: episode: 4181, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000012, mae: 0.002347, mean_q: 0.021401
 41766/100000: episode: 4182, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000248, mae: 0.004188, mean_q: 0.022118
 41776/100000: episode: 4183, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000081, mae: 0.003871, mean_q: 0.023372
 41786/100000: episode: 4184, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000019, mae: 0.002660, mean_q: 0.021634
 41796/100000: episode: 4185, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000071, mae: 0.003284, mean_q: 0.021050
 41806/100000: episode: 4186, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000117, mae: 0.004178, mean_q: 0.023116
 41816/100000: episode: 4187, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000053, mae: 0.002854, mean_q: 0.023278
 41826/100000: episode: 4188, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002377, mean_q: 0.021734
 41836/100000: episode: 4189, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000019, mae: 0.002466, mean_q: 0.021601
 41846/100000: episode: 4190, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000239, mae: 0.003408, mean_q: 0.022185
 41856/100000: episode: 4191, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000059, mae: 0.003128, mean_q: 0.023079
 41866/100000: episode: 4192, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.001681, mean_q: 0.022347
 41876/100000: episode: 4193, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000011, mae: 0.002017, mean_q: 0.021320
 41886/100000: episode: 4194, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.002261, mean_q: 0.021593
 41896/100000: episode: 4195, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000040, mae: 0.002536, mean_q: 0.022010
 41906/100000: episode: 4196, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000037, mae: 0.002974, mean_q: 0.021018
 41916/100000: episode: 4197, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002587, mean_q: 0.021355
 41926/100000: episode: 4198, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000056, mae: 0.002758, mean_q: 0.021258
 41936/100000: episode: 4199, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000015, mae: 0.001706, mean_q: 0.022013
 41946/100000: episode: 4200, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000038, mae: 0.002790, mean_q: 0.020922
[Info] Complete ISplit Iteration
[Info] Levels: [0.024606215]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 41956/100000: episode: 4201, duration: 0.832s, episode steps: 10, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000430, mae: 0.005355, mean_q: 0.022823
 41966/100000: episode: 4202, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000225, mae: 0.005583, mean_q: 0.024772
 41976/100000: episode: 4203, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001558, mae: 0.006408, mean_q: 0.021168
 41986/100000: episode: 4204, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000018, mae: 0.003293, mean_q: 0.024085
 41996/100000: episode: 4205, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000033, mae: 0.002295, mean_q: 0.022199
Step 42000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.42000.hdf5
 42006/100000: episode: 4206, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000035, mae: 0.002809, mean_q: 0.020972
 42016/100000: episode: 4207, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.002386, mean_q: 0.022028
 42026/100000: episode: 4208, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000221, mae: 0.003312, mean_q: 0.022675
 42036/100000: episode: 4209, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000037, mae: 0.002433, mean_q: 0.021837
 42046/100000: episode: 4210, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.002663, mean_q: 0.021258
 42056/100000: episode: 4211, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001550, mae: 0.006828, mean_q: 0.023747
 42066/100000: episode: 4212, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000055, mae: 0.003728, mean_q: 0.024106
 42076/100000: episode: 4213, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000016, mae: 0.002827, mean_q: 0.020897
 42086/100000: episode: 4214, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000057, mae: 0.003071, mean_q: 0.021443
 42096/100000: episode: 4215, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000651, mae: 0.007483, mean_q: 0.023831
 42106/100000: episode: 4216, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000017, mae: 0.003429, mean_q: 0.024878
 42116/100000: episode: 4217, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000071, mae: 0.003479, mean_q: 0.020969
 42126/100000: episode: 4218, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000120, mae: 0.004209, mean_q: 0.022471
 42136/100000: episode: 4219, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002690, mean_q: 0.023480
 42146/100000: episode: 4220, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000281, mae: 0.004288, mean_q: 0.022237
 42156/100000: episode: 4221, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000077, mae: 0.003815, mean_q: 0.023823
 42166/100000: episode: 4222, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000195, mae: 0.002607, mean_q: 0.023393
 42176/100000: episode: 4223, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000073, mae: 0.002536, mean_q: 0.022480
 42186/100000: episode: 4224, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000033, mae: 0.002196, mean_q: 0.022202
 42196/100000: episode: 4225, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000052, mae: 0.002402, mean_q: 0.022147
 42206/100000: episode: 4226, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000058, mae: 0.002635, mean_q: 0.022590
 42216/100000: episode: 4227, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000054, mae: 0.003015, mean_q: 0.021535
 42226/100000: episode: 4228, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000031, mae: 0.002214, mean_q: 0.021686
 42236/100000: episode: 4229, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002107, mean_q: 0.022080
 42246/100000: episode: 4230, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000076, mae: 0.002688, mean_q: 0.022435
 42256/100000: episode: 4231, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000037, mae: 0.002600, mean_q: 0.021975
 42266/100000: episode: 4232, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.003023, mae: 0.009235, mean_q: 0.022369
 42276/100000: episode: 4233, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000042, mae: 0.006062, mean_q: 0.027441
 42286/100000: episode: 4234, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000061, mae: 0.003790, mean_q: 0.022212
 42296/100000: episode: 4235, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.004356, mean_q: 0.020262
 42306/100000: episode: 4236, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000033, mae: 0.002148, mean_q: 0.022828
 42316/100000: episode: 4237, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000029, mae: 0.001555, mean_q: 0.022743
 42326/100000: episode: 4238, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000032, mae: 0.002052, mean_q: 0.021981
[Info] FALSIFICATION!
 42336/100000: episode: 4239, duration: 0.305s, episode steps: 10, steps per second: 33, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000035, mae: 0.002778, mean_q: 0.021497
 42346/100000: episode: 4240, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000102, mae: 0.004125, mean_q: 0.022519
 42356/100000: episode: 4241, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000221, mae: 0.003411, mean_q: 0.022941
 42366/100000: episode: 4242, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000034, mae: 0.001897, mean_q: 0.022408
 42376/100000: episode: 4243, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000059, mae: 0.003048, mean_q: 0.021964
 42386/100000: episode: 4244, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002332, mean_q: 0.021932
 42396/100000: episode: 4245, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000055, mae: 0.002528, mean_q: 0.022702
 42406/100000: episode: 4246, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.001532, mae: 0.006105, mean_q: 0.023318
 42416/100000: episode: 4247, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000034, mae: 0.003006, mean_q: 0.024068
 42426/100000: episode: 4248, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.001510, mae: 0.005111, mean_q: 0.022156
 42436/100000: episode: 4249, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001568, mae: 0.009188, mean_q: 0.026496
 42446/100000: episode: 4250, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000073, mae: 0.003891, mean_q: 0.024725
 42456/100000: episode: 4251, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000244, mae: 0.003842, mean_q: 0.022816
 42466/100000: episode: 4252, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000053, mae: 0.002361, mean_q: 0.023170
 42476/100000: episode: 4253, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.002109, mean_q: 0.022412
 42486/100000: episode: 4254, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000099, mae: 0.003721, mean_q: 0.022690
 42496/100000: episode: 4255, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000113, mae: 0.003847, mean_q: 0.023822
 42506/100000: episode: 4256, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000393, mae: 0.005041, mean_q: 0.024031
 42516/100000: episode: 4257, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000034, mae: 0.002069, mean_q: 0.023458
 42526/100000: episode: 4258, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000241, mae: 0.003731, mean_q: 0.022704
 42536/100000: episode: 4259, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000094, mae: 0.003804, mean_q: 0.024165
 42546/100000: episode: 4260, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000056, mae: 0.002906, mean_q: 0.024022
 42556/100000: episode: 4261, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000010, mae: 0.001467, mean_q: 0.023064
 42566/100000: episode: 4262, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000021, mae: 0.003091, mean_q: 0.021916
 42576/100000: episode: 4263, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000031, mae: 0.002324, mean_q: 0.022218
 42586/100000: episode: 4264, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000030, mae: 0.001491, mean_q: 0.023041
 42596/100000: episode: 4265, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.001508, mae: 0.004961, mean_q: 0.023217
 42606/100000: episode: 4266, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.001523, mae: 0.007144, mean_q: 0.025397
 42616/100000: episode: 4267, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000227, mae: 0.005981, mean_q: 0.026440
 42626/100000: episode: 4268, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000072, mae: 0.002674, mean_q: 0.023531
 42636/100000: episode: 4269, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002497, mean_q: 0.023233
 42646/100000: episode: 4270, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000198, mae: 0.002576, mean_q: 0.023361
 42656/100000: episode: 4271, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000119, mae: 0.004762, mean_q: 0.024820
 42666/100000: episode: 4272, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000064, mae: 0.003698, mean_q: 0.024171
 42676/100000: episode: 4273, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000051, mae: 0.003015, mean_q: 0.022331
 42686/100000: episode: 4274, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.001566, mae: 0.006963, mean_q: 0.024663
 42696/100000: episode: 4275, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000064, mae: 0.004935, mean_q: 0.026307
 42706/100000: episode: 4276, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.003200, mean_q: 0.022071
 42716/100000: episode: 4277, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000016, mae: 0.002570, mean_q: 0.022466
 42726/100000: episode: 4278, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.001508, mae: 0.005087, mean_q: 0.023081
 42736/100000: episode: 4279, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.004031, mean_q: 0.026037
 42746/100000: episode: 4280, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000195, mae: 0.003001, mean_q: 0.024642
 42756/100000: episode: 4281, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000079, mae: 0.004387, mean_q: 0.022107
 42766/100000: episode: 4282, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001505, mae: 0.005637, mean_q: 0.024433
 42776/100000: episode: 4283, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000040, mae: 0.004087, mean_q: 0.026290
 42786/100000: episode: 4284, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000115, mae: 0.004126, mean_q: 0.023200
 42796/100000: episode: 4285, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002607, mean_q: 0.023664
 42806/100000: episode: 4286, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.003056, mae: 0.012092, mean_q: 0.027523
 42816/100000: episode: 4287, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000078, mae: 0.006274, mean_q: 0.028512
 42826/100000: episode: 4288, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000045, mae: 0.004020, mean_q: 0.022989
 42836/100000: episode: 4289, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000018, mae: 0.003489, mean_q: 0.022483
 42846/100000: episode: 4290, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000199, mae: 0.003058, mean_q: 0.023664
 42856/100000: episode: 4291, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000030, mae: 0.004332, mean_q: 0.026381
 42866/100000: episode: 4292, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000060, mae: 0.004420, mean_q: 0.026640
 42876/100000: episode: 4293, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000055, mae: 0.002864, mean_q: 0.024164
 42886/100000: episode: 4294, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000076, mae: 0.003392, mean_q: 0.023727
 42896/100000: episode: 4295, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000037, mae: 0.002379, mean_q: 0.024861
 42906/100000: episode: 4296, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000257, mae: 0.005953, mean_q: 0.027467
 42916/100000: episode: 4297, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001524, mae: 0.006523, mean_q: 0.026617
 42926/100000: episode: 4298, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000036, mae: 0.002444, mean_q: 0.025702
 42936/100000: episode: 4299, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000018, mae: 0.002620, mean_q: 0.024161
 42946/100000: episode: 4300, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000096, mae: 0.004118, mean_q: 0.023821
[Info] Complete ISplit Iteration
[Info] Levels: [0.023694642]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 42956/100000: episode: 4301, duration: 0.742s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000020, mae: 0.002514, mean_q: 0.024195
 42966/100000: episode: 4302, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.003199, mean_q: 0.023776
 42976/100000: episode: 4303, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000244, mae: 0.004212, mean_q: 0.024212
 42986/100000: episode: 4304, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002170, mean_q: 0.024983
 42996/100000: episode: 4305, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.002169, mean_q: 0.024268
Step 43000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.43000.hdf5
 43006/100000: episode: 4306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.002387, mean_q: 0.023749
 43016/100000: episode: 4307, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000059, mae: 0.002815, mean_q: 0.024272
 43026/100000: episode: 4308, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.001699, mean_q: 0.024240
 43036/100000: episode: 4309, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.001567, mae: 0.006660, mean_q: 0.023764
 43046/100000: episode: 4310, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000075, mae: 0.004514, mean_q: 0.026509
 43056/100000: episode: 4311, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000283, mae: 0.005153, mean_q: 0.025362
 43066/100000: episode: 4312, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000038, mae: 0.002430, mean_q: 0.024820
 43076/100000: episode: 4313, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000037, mae: 0.002609, mean_q: 0.023978
 43086/100000: episode: 4314, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000017, mae: 0.002425, mean_q: 0.023834
 43096/100000: episode: 4315, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000013, mae: 0.002478, mean_q: 0.023147
 43106/100000: episode: 4316, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.003144, mean_q: 0.023401
 43116/100000: episode: 4317, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000255, mae: 0.003726, mean_q: 0.024655
 43126/100000: episode: 4318, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000221, mae: 0.004518, mean_q: 0.025974
 43136/100000: episode: 4319, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000063, mae: 0.003397, mean_q: 0.024174
 43146/100000: episode: 4320, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001524, mae: 0.005677, mean_q: 0.023777
 43156/100000: episode: 4321, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000046, mae: 0.004277, mean_q: 0.026228
 43166/100000: episode: 4322, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000216, mae: 0.002896, mean_q: 0.024374
 43176/100000: episode: 4323, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000021, mae: 0.002822, mean_q: 0.023565
 43186/100000: episode: 4324, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.003379, mean_q: 0.022653
 43196/100000: episode: 4325, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002868, mean_q: 0.023019
 43206/100000: episode: 4326, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000012, mae: 0.001660, mean_q: 0.023526
 43216/100000: episode: 4327, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.001498, mae: 0.004904, mean_q: 0.024869
 43226/100000: episode: 4328, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000080, mae: 0.004849, mean_q: 0.026364
 43236/100000: episode: 4329, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000215, mae: 0.003056, mean_q: 0.024917
 43246/100000: episode: 4330, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000019, mae: 0.002099, mean_q: 0.024686
 43256/100000: episode: 4331, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000017, mae: 0.002578, mean_q: 0.023385
 43266/100000: episode: 4332, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000051, mae: 0.002388, mean_q: 0.023701
 43276/100000: episode: 4333, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000075, mae: 0.002957, mean_q: 0.024564
 43286/100000: episode: 4334, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000216, mae: 0.002788, mean_q: 0.024237
 43296/100000: episode: 4335, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000246, mae: 0.004159, mean_q: 0.024609
 43306/100000: episode: 4336, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000441, mae: 0.006151, mean_q: 0.025917
 43316/100000: episode: 4337, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000044, mae: 0.003676, mean_q: 0.025920
 43326/100000: episode: 4338, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000042, mae: 0.003563, mean_q: 0.023264
 43336/100000: episode: 4339, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000016, mae: 0.002922, mean_q: 0.022878
 43346/100000: episode: 4340, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000017, mae: 0.002774, mean_q: 0.022952
 43356/100000: episode: 4341, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000019, mae: 0.002787, mean_q: 0.023060
 43366/100000: episode: 4342, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000016, mae: 0.002493, mean_q: 0.022918
 43376/100000: episode: 4343, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000057, mae: 0.003045, mean_q: 0.023222
 43386/100000: episode: 4344, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000037, mae: 0.002265, mean_q: 0.024067
 43396/100000: episode: 4345, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000021, mae: 0.002698, mean_q: 0.023088
 43406/100000: episode: 4346, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.001597, mae: 0.007892, mean_q: 0.023821
 43416/100000: episode: 4347, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000039, mae: 0.003877, mean_q: 0.025964
 43426/100000: episode: 4348, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000014, mae: 0.002531, mean_q: 0.022725
 43436/100000: episode: 4349, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000202, mae: 0.004010, mean_q: 0.022126
 43446/100000: episode: 4350, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000060, mae: 0.003049, mean_q: 0.023980
 43456/100000: episode: 4351, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001543, mae: 0.006290, mean_q: 0.025001
 43466/100000: episode: 4352, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000059, mae: 0.003561, mean_q: 0.025129
 43476/100000: episode: 4353, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000015, mae: 0.002904, mean_q: 0.022327
 43486/100000: episode: 4354, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000017, mae: 0.003115, mean_q: 0.021990
 43496/100000: episode: 4355, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000028, mae: 0.001924, mean_q: 0.022850
 43506/100000: episode: 4356, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002012, mean_q: 0.023437
 43516/100000: episode: 4357, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000033, mae: 0.002342, mean_q: 0.022916
 43526/100000: episode: 4358, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000016, mae: 0.003023, mean_q: 0.021791
 43536/100000: episode: 4359, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000039, mae: 0.003142, mean_q: 0.022201
 43546/100000: episode: 4360, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000050, mae: 0.002049, mean_q: 0.023037
 43556/100000: episode: 4361, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000056, mae: 0.002639, mean_q: 0.023269
 43566/100000: episode: 4362, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000012, mae: 0.002015, mean_q: 0.022259
 43576/100000: episode: 4363, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000244, mae: 0.004401, mean_q: 0.021976
 43586/100000: episode: 4364, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000038, mae: 0.002265, mean_q: 0.022908
 43596/100000: episode: 4365, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000010, mae: 0.001614, mean_q: 0.022280
 43606/100000: episode: 4366, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000223, mae: 0.003787, mean_q: 0.022010
 43616/100000: episode: 4367, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000221, mae: 0.003475, mean_q: 0.023446
 43626/100000: episode: 4368, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000019, mae: 0.002488, mean_q: 0.022899
 43636/100000: episode: 4369, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.001739, mae: 0.007582, mean_q: 0.022523
 43646/100000: episode: 4370, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000039, mae: 0.004566, mean_q: 0.026012
 43656/100000: episode: 4371, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000223, mae: 0.003698, mean_q: 0.023167
 43666/100000: episode: 4372, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.003013, mae: 0.010362, mean_q: 0.025085
 43676/100000: episode: 4373, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000045, mae: 0.004604, mean_q: 0.026043
 43686/100000: episode: 4374, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000055, mae: 0.003667, mean_q: 0.021830
 43696/100000: episode: 4375, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000053, mae: 0.003157, mean_q: 0.022089
 43706/100000: episode: 4376, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000052, mae: 0.002397, mean_q: 0.023711
 43716/100000: episode: 4377, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.001585, mae: 0.006533, mean_q: 0.023761
 43726/100000: episode: 4378, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.001690, mae: 0.008814, mean_q: 0.026715
 43736/100000: episode: 4379, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000062, mae: 0.004349, mean_q: 0.025788
 43746/100000: episode: 4380, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.003459, mean_q: 0.022178
 43756/100000: episode: 4381, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000201, mae: 0.003498, mean_q: 0.022451
 43766/100000: episode: 4382, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000217, mae: 0.003022, mean_q: 0.024053
 43776/100000: episode: 4383, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000022, mae: 0.002537, mean_q: 0.024222
 43786/100000: episode: 4384, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.003040, mean_q: 0.022652
 43796/100000: episode: 4385, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.002121, mean_q: 0.022580
 43806/100000: episode: 4386, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000118, mae: 0.004004, mean_q: 0.023506
 43816/100000: episode: 4387, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000222, mae: 0.004171, mean_q: 0.024747
 43826/100000: episode: 4388, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000051, mae: 0.002271, mean_q: 0.023399
 43836/100000: episode: 4389, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.001534, mae: 0.006713, mean_q: 0.024702
 43846/100000: episode: 4390, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000075, mae: 0.004520, mean_q: 0.025970
 43856/100000: episode: 4391, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.001501, mae: 0.004523, mean_q: 0.024011
 43866/100000: episode: 4392, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000019, mae: 0.002726, mean_q: 0.025201
 43876/100000: episode: 4393, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000016, mae: 0.002524, mean_q: 0.022913
 43886/100000: episode: 4394, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000222, mae: 0.003966, mean_q: 0.022767
 43896/100000: episode: 4395, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000015, mae: 0.001674, mean_q: 0.024099
 43906/100000: episode: 4396, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000403, mae: 0.004058, mean_q: 0.023626
 43916/100000: episode: 4397, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000016, mae: 0.002150, mean_q: 0.024413
 43926/100000: episode: 4398, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001505, mae: 0.005085, mean_q: 0.024571
 43936/100000: episode: 4399, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000017, mae: 0.002255, mean_q: 0.023654
 43946/100000: episode: 4400, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.001513, mae: 0.005645, mean_q: 0.023635
[Info] 1-TH LEVEL FOUND: 0.024370873346924782, Considering 100/100 traces
 43956/100000: episode: 4401, duration: 0.669s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000092, mae: 0.003760, mean_q: 0.024959
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024370873346924782
1
 43966/100000: episode: 4402, duration: 0.493s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000058, mae: 0.002838, mean_q: 0.023830
 43976/100000: episode: 4403, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000223, mae: 0.003643, mean_q: 0.023309
 43986/100000: episode: 4404, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001748, mae: 0.008102, mean_q: 0.025265
 43996/100000: episode: 4405, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000042, mae: 0.004600, mean_q: 0.026895
Step 44000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.44000.hdf5
 44006/100000: episode: 4406, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000040, mae: 0.003665, mean_q: 0.022747
 44016/100000: episode: 4407, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.001529, mae: 0.005996, mean_q: 0.023769
 44026/100000: episode: 4408, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000242, mae: 0.005460, mean_q: 0.026303
 44036/100000: episode: 4409, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000036, mae: 0.002717, mean_q: 0.024156
 44046/100000: episode: 4410, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.001541, mae: 0.007580, mean_q: 0.022104
 44056/100000: episode: 4411, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000017, mae: 0.002513, mean_q: 0.024846
 44066/100000: episode: 4412, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000205, mae: 0.003613, mean_q: 0.024880
 44076/100000: episode: 4413, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000042, mae: 0.003297, mean_q: 0.023182
 44086/100000: episode: 4414, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002663, mean_q: 0.023202
 44096/100000: episode: 4415, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000222, mae: 0.003452, mean_q: 0.024355
 44106/100000: episode: 4416, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000250, mae: 0.004565, mean_q: 0.024490
 44116/100000: episode: 4417, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000201, mae: 0.002835, mean_q: 0.023891
 44126/100000: episode: 4418, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000407, mae: 0.004431, mean_q: 0.023786
 44136/100000: episode: 4419, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000017, mae: 0.002177, mean_q: 0.024721
 44146/100000: episode: 4420, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000201, mae: 0.003266, mean_q: 0.023347
 44156/100000: episode: 4421, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000053, mae: 0.004614, mean_q: 0.022618
 44166/100000: episode: 4422, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000071, mae: 0.003160, mean_q: 0.022710
 44176/100000: episode: 4423, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000198, mae: 0.002333, mean_q: 0.023818
 44186/100000: episode: 4424, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000057, mae: 0.002732, mean_q: 0.023866
 44196/100000: episode: 4425, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000041, mae: 0.002534, mean_q: 0.023462
 44206/100000: episode: 4426, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000076, mae: 0.002976, mean_q: 0.023385
 44216/100000: episode: 4427, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000223, mae: 0.003495, mean_q: 0.024036
 44226/100000: episode: 4428, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000238, mae: 0.003344, mean_q: 0.023536
 44236/100000: episode: 4429, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000110, mae: 0.003650, mean_q: 0.024419
 44246/100000: episode: 4430, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.001565, mae: 0.007132, mean_q: 0.025143
 44256/100000: episode: 4431, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000226, mae: 0.005091, mean_q: 0.025992
 44266/100000: episode: 4432, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000038, mae: 0.002788, mean_q: 0.023793
 44276/100000: episode: 4433, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.003900, mean_q: 0.022299
 44286/100000: episode: 4434, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000075, mae: 0.003140, mean_q: 0.023263
 44296/100000: episode: 4435, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000239, mae: 0.003743, mean_q: 0.024454
 44306/100000: episode: 4436, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000038, mae: 0.002468, mean_q: 0.024384
 44316/100000: episode: 4437, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.001550, mae: 0.006927, mean_q: 0.025222
 44326/100000: episode: 4438, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000022, mae: 0.002799, mean_q: 0.024395
 44336/100000: episode: 4439, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000071, mae: 0.003503, mean_q: 0.022698
 44346/100000: episode: 4440, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.002774, mean_q: 0.023156
 44356/100000: episode: 4441, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.001534, mae: 0.006473, mean_q: 0.024306
 44366/100000: episode: 4442, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000045, mae: 0.003707, mean_q: 0.025290
 44376/100000: episode: 4443, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000036, mae: 0.002599, mean_q: 0.023312
 44386/100000: episode: 4444, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000043, mae: 0.003641, mean_q: 0.022662
 44396/100000: episode: 4445, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000014, mae: 0.002476, mean_q: 0.022483
 44406/100000: episode: 4446, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000062, mae: 0.003399, mean_q: 0.023057
 44416/100000: episode: 4447, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000202, mae: 0.003086, mean_q: 0.023133
 44426/100000: episode: 4448, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.001524, mae: 0.005032, mean_q: 0.023282
 44436/100000: episode: 4449, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000057, mae: 0.004287, mean_q: 0.025839
 44446/100000: episode: 4450, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000013, mae: 0.002010, mean_q: 0.023910
 44456/100000: episode: 4451, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000222, mae: 0.003887, mean_q: 0.022563
 44466/100000: episode: 4452, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000442, mae: 0.006059, mean_q: 0.025253
 44476/100000: episode: 4453, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000020, mae: 0.003453, mean_q: 0.025878
 44486/100000: episode: 4454, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000038, mae: 0.003392, mean_q: 0.022568
 44496/100000: episode: 4455, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000415, mae: 0.005542, mean_q: 0.022895
 44506/100000: episode: 4456, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000055, mae: 0.003736, mean_q: 0.025531
 44516/100000: episode: 4457, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000043, mae: 0.003140, mean_q: 0.023990
 44526/100000: episode: 4458, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000016, mae: 0.002936, mean_q: 0.022236
 44536/100000: episode: 4459, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000033, mae: 0.002595, mean_q: 0.022750
 44546/100000: episode: 4460, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000040, mae: 0.003147, mean_q: 0.022685
 44556/100000: episode: 4461, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000042, mae: 0.002953, mean_q: 0.023063
 44566/100000: episode: 4462, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000217, mae: 0.002850, mean_q: 0.023599
 44576/100000: episode: 4463, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000014, mae: 0.001890, mean_q: 0.023737
 44586/100000: episode: 4464, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000034, mae: 0.003198, mean_q: 0.021750
 44596/100000: episode: 4465, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000082, mae: 0.003885, mean_q: 0.022650
 44606/100000: episode: 4466, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000200, mae: 0.002986, mean_q: 0.023877
 44616/100000: episode: 4467, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000203, mae: 0.003257, mean_q: 0.023878
 44626/100000: episode: 4468, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000222, mae: 0.003927, mean_q: 0.022211
 44636/100000: episode: 4469, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000032, mae: 0.002075, mean_q: 0.022739
 44646/100000: episode: 4470, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.002201, mean_q: 0.023965
 44656/100000: episode: 4471, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000057, mae: 0.002743, mean_q: 0.023169
 44666/100000: episode: 4472, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.001575, mae: 0.007215, mean_q: 0.023825
 44676/100000: episode: 4473, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.001710, mae: 0.008748, mean_q: 0.025970
 44686/100000: episode: 4474, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000017, mae: 0.003272, mean_q: 0.025795
 44696/100000: episode: 4475, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000021, mae: 0.002963, mean_q: 0.022682
 44706/100000: episode: 4476, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000211, mae: 0.004381, mean_q: 0.022406
 44716/100000: episode: 4477, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000061, mae: 0.002898, mean_q: 0.023532
 44726/100000: episode: 4478, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000389, mae: 0.004852, mean_q: 0.024570
 44736/100000: episode: 4479, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001523, mae: 0.006195, mean_q: 0.025158
 44746/100000: episode: 4480, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000043, mae: 0.003965, mean_q: 0.025601
 44756/100000: episode: 4481, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000232, mae: 0.002846, mean_q: 0.024199
 44766/100000: episode: 4482, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000039, mae: 0.002431, mean_q: 0.024123
 44776/100000: episode: 4483, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000225, mae: 0.004122, mean_q: 0.023203
 44786/100000: episode: 4484, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000227, mae: 0.004177, mean_q: 0.023003
 44796/100000: episode: 4485, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000038, mae: 0.002542, mean_q: 0.024245
 44806/100000: episode: 4486, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000017, mae: 0.002327, mean_q: 0.023290
 44816/100000: episode: 4487, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000220, mae: 0.004250, mean_q: 0.022114
 44826/100000: episode: 4488, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001550, mae: 0.006574, mean_q: 0.024048
 44836/100000: episode: 4489, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000041, mae: 0.004252, mean_q: 0.026232
 44846/100000: episode: 4490, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000072, mae: 0.002768, mean_q: 0.024180
 44856/100000: episode: 4491, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.003203, mean_q: 0.022816
 44866/100000: episode: 4492, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002456, mean_q: 0.022918
 44876/100000: episode: 4493, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000264, mae: 0.004406, mean_q: 0.023426
 44886/100000: episode: 4494, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000083, mae: 0.004074, mean_q: 0.024575
 44896/100000: episode: 4495, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000020, mae: 0.002419, mean_q: 0.023733
 44906/100000: episode: 4496, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000224, mae: 0.003532, mean_q: 0.023127
 44916/100000: episode: 4497, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000061, mae: 0.003294, mean_q: 0.023208
 44926/100000: episode: 4498, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.001573, mae: 0.007393, mean_q: 0.024770
 44936/100000: episode: 4499, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000243, mae: 0.005145, mean_q: 0.025619
 44946/100000: episode: 4500, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000040, mae: 0.002801, mean_q: 0.023821
 44956/100000: episode: 4501, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000076, mae: 0.003858, mean_q: 0.022430
[Info] 1-TH LEVEL FOUND: 0.02410014718770981, Considering 100/100 traces
 44966/100000: episode: 4502, duration: 0.792s, episode steps: 10, steps per second: 13, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000218, mae: 0.003241, mean_q: 0.023219
[Info] 2-TH LEVEL FOUND: 0.024734001606702805, Considering 100/100 traces
 44976/100000: episode: 4503, duration: 0.705s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000248, mae: 0.004952, mean_q: 0.024706
[Info] 3-TH LEVEL FOUND: 0.025453027337789536, Considering 100/100 traces
 44986/100000: episode: 4504, duration: 0.728s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.001511, mae: 0.005709, mean_q: 0.024580
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025453027337789536
3
 44996/100000: episode: 4505, duration: 0.498s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000038, mae: 0.002992, mean_q: 0.025028
Step 45000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.45000.hdf5
 45006/100000: episode: 4506, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000052, mae: 0.002664, mean_q: 0.023175
 45016/100000: episode: 4507, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000395, mae: 0.004860, mean_q: 0.023923
 45026/100000: episode: 4508, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000040, mae: 0.003131, mean_q: 0.024957
 45036/100000: episode: 4509, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000385, mae: 0.004567, mean_q: 0.025033
 45046/100000: episode: 4510, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000024, mae: 0.003673, mean_q: 0.026002
 45056/100000: episode: 4511, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000428, mae: 0.004994, mean_q: 0.023672
 45066/100000: episode: 4512, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001887, mae: 0.008221, mean_q: 0.025396
 45076/100000: episode: 4513, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000059, mae: 0.005463, mean_q: 0.028182
 45086/100000: episode: 4514, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000019, mae: 0.002725, mean_q: 0.025085
 45096/100000: episode: 4515, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000200, mae: 0.004042, mean_q: 0.022712
 45106/100000: episode: 4516, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000044, mae: 0.002739, mean_q: 0.024525
 45116/100000: episode: 4517, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000053, mae: 0.002536, mean_q: 0.024112
 45126/100000: episode: 4518, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.001530, mae: 0.006045, mean_q: 0.024445
 45136/100000: episode: 4519, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001562, mae: 0.008527, mean_q: 0.027359
 45146/100000: episode: 4520, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000212, mae: 0.005496, mean_q: 0.027243
 45156/100000: episode: 4521, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000021, mae: 0.003071, mean_q: 0.023732
 45166/100000: episode: 4522, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000075, mae: 0.004220, mean_q: 0.022949
 45176/100000: episode: 4523, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001525, mae: 0.005528, mean_q: 0.024608
 45186/100000: episode: 4524, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000244, mae: 0.006015, mean_q: 0.027373
 45196/100000: episode: 4525, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000249, mae: 0.005124, mean_q: 0.026072
 45206/100000: episode: 4526, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000058, mae: 0.003201, mean_q: 0.024295
 45216/100000: episode: 4527, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000211, mae: 0.004391, mean_q: 0.023838
 45226/100000: episode: 4528, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000014, mae: 0.001753, mean_q: 0.024379
 45236/100000: episode: 4529, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000032, mae: 0.001704, mean_q: 0.024680
 45246/100000: episode: 4530, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000061, mae: 0.003286, mean_q: 0.024332
 45256/100000: episode: 4531, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000038, mae: 0.003251, mean_q: 0.023435
 45266/100000: episode: 4532, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000222, mae: 0.004019, mean_q: 0.023562
 45276/100000: episode: 4533, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000242, mae: 0.003600, mean_q: 0.024460
 45286/100000: episode: 4534, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000018, mae: 0.002045, mean_q: 0.024901
 45296/100000: episode: 4535, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000575, mae: 0.005773, mean_q: 0.025037
 45306/100000: episode: 4536, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.003277, mean_q: 0.026124
 45316/100000: episode: 4537, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000243, mae: 0.003721, mean_q: 0.024798
 45326/100000: episode: 4538, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000027, mae: 0.002870, mean_q: 0.024323
 45336/100000: episode: 4539, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.003414, mae: 0.013393, mean_q: 0.026402
 45346/100000: episode: 4540, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.001533, mae: 0.010231, mean_q: 0.029952
 45356/100000: episode: 4541, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.001556, mae: 0.007921, mean_q: 0.027802
 45366/100000: episode: 4542, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000209, mae: 0.004198, mean_q: 0.026413
 45376/100000: episode: 4543, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000038, mae: 0.003199, mean_q: 0.024434
 45386/100000: episode: 4544, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000073, mae: 0.003313, mean_q: 0.024653
 45396/100000: episode: 4545, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.001955, mean_q: 0.025389
 45406/100000: episode: 4546, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000205, mae: 0.003461, mean_q: 0.024972
 45416/100000: episode: 4547, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000077, mae: 0.003076, mean_q: 0.025156
 45426/100000: episode: 4548, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000057, mae: 0.002742, mean_q: 0.025762
 45436/100000: episode: 4549, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000036, mae: 0.002480, mean_q: 0.024858
 45446/100000: episode: 4550, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000205, mae: 0.004037, mean_q: 0.024079
 45456/100000: episode: 4551, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000075, mae: 0.003012, mean_q: 0.025057
 45466/100000: episode: 4552, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000051, mae: 0.003545, mean_q: 0.024846
 45476/100000: episode: 4553, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000220, mae: 0.003994, mean_q: 0.023930
 45486/100000: episode: 4554, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000393, mae: 0.004843, mean_q: 0.025385
 45496/100000: episode: 4555, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000235, mae: 0.003599, mean_q: 0.025729
 45506/100000: episode: 4556, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000018, mae: 0.001902, mean_q: 0.025159
 45516/100000: episode: 4557, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000041, mae: 0.003560, mean_q: 0.023747
 45526/100000: episode: 4558, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000042, mae: 0.003572, mean_q: 0.023633
 45536/100000: episode: 4559, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000038, mae: 0.002673, mean_q: 0.024143
 45546/100000: episode: 4560, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000032, mae: 0.001968, mean_q: 0.024406
 45556/100000: episode: 4561, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000023, mae: 0.002871, mean_q: 0.023797
 45566/100000: episode: 4562, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000220, mae: 0.003238, mean_q: 0.024313
 45576/100000: episode: 4563, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000015, mae: 0.001548, mean_q: 0.024766
 45586/100000: episode: 4564, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000220, mae: 0.003061, mean_q: 0.024676
 45596/100000: episode: 4565, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000037, mae: 0.002207, mean_q: 0.024552
 45606/100000: episode: 4566, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000285, mae: 0.005130, mean_q: 0.024943
 45616/100000: episode: 4567, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000217, mae: 0.004095, mean_q: 0.026076
 45626/100000: episode: 4568, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000037, mae: 0.002680, mean_q: 0.024144
 45636/100000: episode: 4569, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000034, mae: 0.002909, mean_q: 0.023473
 45646/100000: episode: 4570, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000120, mae: 0.004292, mean_q: 0.024271
 45656/100000: episode: 4571, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000021, mae: 0.002424, mean_q: 0.024386
 45666/100000: episode: 4572, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000061, mae: 0.003926, mean_q: 0.023006
 45676/100000: episode: 4573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000020, mae: 0.002851, mean_q: 0.023334
 45686/100000: episode: 4574, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000036, mae: 0.002948, mean_q: 0.022991
 45696/100000: episode: 4575, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000021, mae: 0.002794, mean_q: 0.023250
 45706/100000: episode: 4576, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000392, mae: 0.005063, mean_q: 0.022530
 45716/100000: episode: 4577, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000087, mae: 0.004369, mean_q: 0.024795
 45726/100000: episode: 4578, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000097, mae: 0.003774, mean_q: 0.024661
 45736/100000: episode: 4579, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000019, mae: 0.002335, mean_q: 0.023599
 45746/100000: episode: 4580, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.001534, mae: 0.006183, mean_q: 0.023755
 45756/100000: episode: 4581, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000015, mae: 0.002106, mean_q: 0.024575
 45766/100000: episode: 4582, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.002418, mean_q: 0.022617
 45776/100000: episode: 4583, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000035, mae: 0.002810, mean_q: 0.022682
 45786/100000: episode: 4584, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000261, mae: 0.004177, mean_q: 0.023301
 45796/100000: episode: 4585, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000056, mae: 0.003067, mean_q: 0.024505
 45806/100000: episode: 4586, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000199, mae: 0.002691, mean_q: 0.023439
 45816/100000: episode: 4587, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000061, mae: 0.003055, mean_q: 0.023501
 45826/100000: episode: 4588, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000199, mae: 0.002987, mean_q: 0.023075
 45836/100000: episode: 4589, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000201, mae: 0.002828, mean_q: 0.023325
 45846/100000: episode: 4590, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000409, mae: 0.004818, mean_q: 0.024104
 45856/100000: episode: 4591, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000035, mae: 0.002526, mean_q: 0.024503
 45866/100000: episode: 4592, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001901, mae: 0.008536, mean_q: 0.024837
 45876/100000: episode: 4593, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000090, mae: 0.007568, mean_q: 0.028930
 45886/100000: episode: 4594, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000202, mae: 0.004183, mean_q: 0.025752
 45896/100000: episode: 4595, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000227, mae: 0.004880, mean_q: 0.022436
 45906/100000: episode: 4596, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000423, mae: 0.005306, mean_q: 0.025063
 45916/100000: episode: 4597, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000046, mae: 0.004265, mean_q: 0.026429
 45926/100000: episode: 4598, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000210, mae: 0.004364, mean_q: 0.023350
 45936/100000: episode: 4599, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.001529, mae: 0.006466, mean_q: 0.022863
 45946/100000: episode: 4600, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000066, mae: 0.004827, mean_q: 0.026425
 45956/100000: episode: 4601, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000059, mae: 0.003440, mean_q: 0.025398
 45966/100000: episode: 4602, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000262, mae: 0.004441, mean_q: 0.023863
 45976/100000: episode: 4603, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000261, mae: 0.004309, mean_q: 0.025003
 45986/100000: episode: 4604, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000023, mae: 0.002809, mean_q: 0.025443
[Info] 1-TH LEVEL FOUND: 0.023937484249472618, Considering 100/100 traces
 45996/100000: episode: 4605, duration: 0.727s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000243, mae: 0.004554, mean_q: 0.023444
Step 46000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.46000.hdf5
[Info] 2-TH LEVEL FOUND: 0.026060890406370163, Considering 100/100 traces
 46006/100000: episode: 4606, duration: 0.717s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000224, mae: 0.004169, mean_q: 0.025437
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.026060890406370163
2
 46016/100000: episode: 4607, duration: 0.490s, episode steps: 10, steps per second: 20, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000055, mae: 0.002899, mean_q: 0.025364
 46026/100000: episode: 4608, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000205, mae: 0.004038, mean_q: 0.023261
 46036/100000: episode: 4609, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000204, mae: 0.004421, mean_q: 0.026131
 46046/100000: episode: 4610, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000405, mae: 0.006202, mean_q: 0.026941
 46056/100000: episode: 4611, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000058, mae: 0.002979, mean_q: 0.024773
 46066/100000: episode: 4612, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000019, mae: 0.002808, mean_q: 0.023700
 46076/100000: episode: 4613, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000036, mae: 0.003080, mean_q: 0.023447
 46086/100000: episode: 4614, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000076, mae: 0.003375, mean_q: 0.023871
 46096/100000: episode: 4615, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000278, mae: 0.004659, mean_q: 0.025142
 46106/100000: episode: 4616, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000403, mae: 0.005069, mean_q: 0.025850
 46116/100000: episode: 4617, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000117, mae: 0.004403, mean_q: 0.025599
 46126/100000: episode: 4618, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.002224, mean_q: 0.024131
 46136/100000: episode: 4619, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000041, mae: 0.003651, mean_q: 0.023098
 46146/100000: episode: 4620, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001517, mae: 0.006626, mean_q: 0.023164
 46156/100000: episode: 4621, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000221, mae: 0.004401, mean_q: 0.025954
 46166/100000: episode: 4622, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000422, mae: 0.005619, mean_q: 0.025933
 46176/100000: episode: 4623, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000042, mae: 0.002993, mean_q: 0.025379
 46186/100000: episode: 4624, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000038, mae: 0.003374, mean_q: 0.023301
 46196/100000: episode: 4625, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000469, mae: 0.006375, mean_q: 0.023704
 46206/100000: episode: 4626, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000058, mae: 0.003771, mean_q: 0.026129
 46216/100000: episode: 4627, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000057, mae: 0.003185, mean_q: 0.023995
 46226/100000: episode: 4628, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000386, mae: 0.004073, mean_q: 0.024422
 46236/100000: episode: 4629, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000148, mae: 0.004991, mean_q: 0.025957
 46246/100000: episode: 4630, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000226, mae: 0.004272, mean_q: 0.025638
 46256/100000: episode: 4631, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000221, mae: 0.003076, mean_q: 0.024924
 46266/100000: episode: 4632, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000062, mae: 0.003666, mean_q: 0.023884
 46276/100000: episode: 4633, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000035, mae: 0.002918, mean_q: 0.023456
 46286/100000: episode: 4634, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000056, mae: 0.002736, mean_q: 0.024113
 46296/100000: episode: 4635, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.003077, mean_q: 0.023952
 46306/100000: episode: 4636, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000248, mae: 0.004221, mean_q: 0.024502
 46316/100000: episode: 4637, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000015, mae: 0.001741, mean_q: 0.024501
 46326/100000: episode: 4638, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000278, mae: 0.004430, mean_q: 0.023920
 46336/100000: episode: 4639, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000061, mae: 0.004201, mean_q: 0.026154
 46346/100000: episode: 4640, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000409, mae: 0.004970, mean_q: 0.024064
 46356/100000: episode: 4641, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000038, mae: 0.002404, mean_q: 0.024677
 46366/100000: episode: 4642, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000222, mae: 0.003722, mean_q: 0.025256
 46376/100000: episode: 4643, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.002656, mean_q: 0.024389
 46386/100000: episode: 4644, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000245, mae: 0.004759, mean_q: 0.023374
 46396/100000: episode: 4645, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000038, mae: 0.002445, mean_q: 0.024555
 46406/100000: episode: 4646, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000022, mae: 0.002930, mean_q: 0.023610
 46416/100000: episode: 4647, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000035, mae: 0.003090, mean_q: 0.022976
 46426/100000: episode: 4648, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000206, mae: 0.003269, mean_q: 0.024284
 46436/100000: episode: 4649, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000020, mae: 0.002208, mean_q: 0.024153
 46446/100000: episode: 4650, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000269, mae: 0.004944, mean_q: 0.023598
 46456/100000: episode: 4651, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000052, mae: 0.002343, mean_q: 0.024491
 46466/100000: episode: 4652, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.002770, mean_q: 0.023494
 46476/100000: episode: 4653, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.002675, mean_q: 0.023930
 46486/100000: episode: 4654, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000243, mae: 0.004581, mean_q: 0.025299
 46496/100000: episode: 4655, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000230, mae: 0.003921, mean_q: 0.024232
 46506/100000: episode: 4656, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000075, mae: 0.002767, mean_q: 0.024028
 46516/100000: episode: 4657, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000405, mae: 0.004155, mean_q: 0.024051
 46526/100000: episode: 4658, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000243, mae: 0.004570, mean_q: 0.025251
 46536/100000: episode: 4659, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000219, mae: 0.004052, mean_q: 0.025617
 46546/100000: episode: 4660, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.002312, mean_q: 0.024913
 46556/100000: episode: 4661, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000031, mae: 0.002711, mean_q: 0.023159
 46566/100000: episode: 4662, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000054, mae: 0.003012, mean_q: 0.023304
 46576/100000: episode: 4663, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000273, mae: 0.003976, mean_q: 0.024390
 46586/100000: episode: 4664, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.003722, mean_q: 0.025995
 46596/100000: episode: 4665, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000217, mae: 0.003147, mean_q: 0.024193
 46606/100000: episode: 4666, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000281, mae: 0.004653, mean_q: 0.024136
 46616/100000: episode: 4667, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000238, mae: 0.004585, mean_q: 0.025814
 46626/100000: episode: 4668, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000260, mae: 0.005592, mean_q: 0.026431
 46636/100000: episode: 4669, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000059, mae: 0.003168, mean_q: 0.024944
 46646/100000: episode: 4670, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000041, mae: 0.002969, mean_q: 0.024066
 46656/100000: episode: 4671, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000058, mae: 0.002790, mean_q: 0.024832
 46666/100000: episode: 4672, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000057, mae: 0.002857, mean_q: 0.024370
 46676/100000: episode: 4673, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000235, mae: 0.003453, mean_q: 0.024169
 46686/100000: episode: 4674, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000062, mae: 0.003510, mean_q: 0.025290
 46696/100000: episode: 4675, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000020, mae: 0.002721, mean_q: 0.023693
 46706/100000: episode: 4676, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.003352, mean_q: 0.022975
 46716/100000: episode: 4677, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000264, mae: 0.004500, mean_q: 0.024149
 46726/100000: episode: 4678, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000061, mae: 0.003925, mean_q: 0.025672
 46736/100000: episode: 4679, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000058, mae: 0.002706, mean_q: 0.024540
 46746/100000: episode: 4680, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000402, mae: 0.004283, mean_q: 0.024852
 46756/100000: episode: 4681, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000058, mae: 0.003695, mean_q: 0.025995
 46766/100000: episode: 4682, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000216, mae: 0.003417, mean_q: 0.025520
 46776/100000: episode: 4683, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000018, mae: 0.002078, mean_q: 0.024500
 46786/100000: episode: 4684, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000096, mae: 0.004242, mean_q: 0.023353
 46796/100000: episode: 4685, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000221, mae: 0.003557, mean_q: 0.024800
 46806/100000: episode: 4686, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000057, mae: 0.003141, mean_q: 0.025439
 46816/100000: episode: 4687, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000568, mae: 0.005083, mean_q: 0.024730
 46826/100000: episode: 4688, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000017, mae: 0.002597, mean_q: 0.025976
 46836/100000: episode: 4689, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.002498, mean_q: 0.024030
 46846/100000: episode: 4690, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000056, mae: 0.003233, mean_q: 0.023728
 46856/100000: episode: 4691, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000255, mae: 0.003639, mean_q: 0.024577
 46866/100000: episode: 4692, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000060, mae: 0.004617, mean_q: 0.026993
 46876/100000: episode: 4693, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.001533, mae: 0.006467, mean_q: 0.025173
 46886/100000: episode: 4694, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.002171, mean_q: 0.025867
 46896/100000: episode: 4695, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000080, mae: 0.003488, mean_q: 0.024970
 46906/100000: episode: 4696, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000244, mae: 0.004498, mean_q: 0.024074
 46916/100000: episode: 4697, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000057, mae: 0.002693, mean_q: 0.025124
 46926/100000: episode: 4698, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000018, mae: 0.002268, mean_q: 0.024259
 46936/100000: episode: 4699, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000432, mae: 0.005753, mean_q: 0.023938
 46946/100000: episode: 4700, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000418, mae: 0.006484, mean_q: 0.027365
 46956/100000: episode: 4701, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000222, mae: 0.005358, mean_q: 0.027578
 46966/100000: episode: 4702, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000040, mae: 0.003214, mean_q: 0.024239
 46976/100000: episode: 4703, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000057, mae: 0.003722, mean_q: 0.023740
 46986/100000: episode: 4704, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000216, mae: 0.004261, mean_q: 0.024738
 46996/100000: episode: 4705, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000063, mae: 0.003294, mean_q: 0.024626
Step 47000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.47000.hdf5
 47006/100000: episode: 4706, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000036, mae: 0.002811, mean_q: 0.024002
[Info] 1-TH LEVEL FOUND: 0.024424036964774132, Considering 100/100 traces
 47016/100000: episode: 4707, duration: 0.668s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000017, mae: 0.001904, mean_q: 0.024508
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024424036964774132
1
 47026/100000: episode: 4708, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000059, mae: 0.003162, mean_q: 0.024223
 47036/100000: episode: 4709, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000392, mae: 0.004669, mean_q: 0.024254
 47046/100000: episode: 4710, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002288, mean_q: 0.025163
 47056/100000: episode: 4711, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000044, mae: 0.003889, mean_q: 0.023230
 47066/100000: episode: 4712, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000220, mae: 0.004501, mean_q: 0.022532
 47076/100000: episode: 4713, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000244, mae: 0.004073, mean_q: 0.024717
 47086/100000: episode: 4714, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000052, mae: 0.002802, mean_q: 0.025268
 47096/100000: episode: 4715, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000265, mae: 0.004323, mean_q: 0.024386
 47106/100000: episode: 4716, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.003181, mean_q: 0.025614
 47116/100000: episode: 4717, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.003005, mean_q: 0.023810
 47126/100000: episode: 4718, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000015, mae: 0.002487, mean_q: 0.023300
 47136/100000: episode: 4719, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000019, mae: 0.002611, mean_q: 0.023379
 47146/100000: episode: 4720, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000229, mae: 0.004026, mean_q: 0.023789
 47156/100000: episode: 4721, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000223, mae: 0.004658, mean_q: 0.026000
 47166/100000: episode: 4722, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000038, mae: 0.002780, mean_q: 0.024718
 47176/100000: episode: 4723, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000091, mae: 0.003505, mean_q: 0.023373
 47186/100000: episode: 4724, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000018, mae: 0.001963, mean_q: 0.024641
 47196/100000: episode: 4725, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000204, mae: 0.003535, mean_q: 0.023589
 47206/100000: episode: 4726, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.002497, mean_q: 0.024807
 47216/100000: episode: 4727, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000236, mae: 0.003220, mean_q: 0.024472
 47226/100000: episode: 4728, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000020, mae: 0.002335, mean_q: 0.024327
 47236/100000: episode: 4729, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000226, mae: 0.004146, mean_q: 0.023317
 47246/100000: episode: 4730, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000016, mae: 0.002168, mean_q: 0.023472
 47256/100000: episode: 4731, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000407, mae: 0.004393, mean_q: 0.023751
 47266/100000: episode: 4732, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000013, mae: 0.001975, mean_q: 0.024940
 47276/100000: episode: 4733, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000045, mae: 0.003503, mean_q: 0.023385
 47286/100000: episode: 4734, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000023, mae: 0.003672, mean_q: 0.022313
 47296/100000: episode: 4735, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000238, mae: 0.003665, mean_q: 0.023264
 47306/100000: episode: 4736, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000198, mae: 0.002484, mean_q: 0.023688
 47316/100000: episode: 4737, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000016, mae: 0.001840, mean_q: 0.023550
 47326/100000: episode: 4738, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000034, mae: 0.002875, mean_q: 0.022678
 47336/100000: episode: 4739, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000241, mae: 0.003893, mean_q: 0.023164
 47346/100000: episode: 4740, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.002197, mean_q: 0.024167
 47356/100000: episode: 4741, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000057, mae: 0.002818, mean_q: 0.023084
 47366/100000: episode: 4742, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000240, mae: 0.003539, mean_q: 0.023835
 47376/100000: episode: 4743, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000043, mae: 0.003193, mean_q: 0.024393
 47386/100000: episode: 4744, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000204, mae: 0.003442, mean_q: 0.023123
 47396/100000: episode: 4745, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000205, mae: 0.004034, mean_q: 0.022229
 47406/100000: episode: 4746, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000022, mae: 0.002613, mean_q: 0.022970
 47416/100000: episode: 4747, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000057, mae: 0.002641, mean_q: 0.023234
 47426/100000: episode: 4748, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000074, mae: 0.002812, mean_q: 0.023362
 47436/100000: episode: 4749, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000097, mae: 0.003437, mean_q: 0.023455
 47446/100000: episode: 4750, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000071, mae: 0.002462, mean_q: 0.023520
 47456/100000: episode: 4751, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000597, mae: 0.006573, mean_q: 0.024465
 47466/100000: episode: 4752, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000018, mae: 0.003270, mean_q: 0.025510
 47476/100000: episode: 4753, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000080, mae: 0.004513, mean_q: 0.021810
 47486/100000: episode: 4754, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.002234, mean_q: 0.023174
 47496/100000: episode: 4755, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000041, mae: 0.003719, mean_q: 0.025206
 47506/100000: episode: 4756, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000023, mae: 0.003045, mean_q: 0.022911
 47516/100000: episode: 4757, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.002486, mean_q: 0.022089
 47526/100000: episode: 4758, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000409, mae: 0.004819, mean_q: 0.023879
 47536/100000: episode: 4759, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000024, mae: 0.003323, mean_q: 0.024791
 47546/100000: episode: 4760, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000017, mae: 0.003169, mean_q: 0.021771
 47556/100000: episode: 4761, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.002707, mean_q: 0.021971
 47566/100000: episode: 4762, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000200, mae: 0.002692, mean_q: 0.022996
 47576/100000: episode: 4763, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000020, mae: 0.002283, mean_q: 0.023091
 47586/100000: episode: 4764, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000060, mae: 0.003838, mean_q: 0.021760
 47596/100000: episode: 4765, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002127, mean_q: 0.022866
 47606/100000: episode: 4766, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000019, mae: 0.002497, mean_q: 0.022382
 47616/100000: episode: 4767, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000061, mae: 0.004164, mean_q: 0.021274
 47626/100000: episode: 4768, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002698, mean_q: 0.021999
 47636/100000: episode: 4769, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000037, mae: 0.002652, mean_q: 0.021985
 47646/100000: episode: 4770, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000037, mae: 0.002816, mean_q: 0.021742
 47656/100000: episode: 4771, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000017, mae: 0.002659, mean_q: 0.021500
 47666/100000: episode: 4772, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000037, mae: 0.002839, mean_q: 0.021415
 47676/100000: episode: 4773, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000023, mae: 0.002508, mean_q: 0.021934
 47686/100000: episode: 4774, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000223, mae: 0.003345, mean_q: 0.022333
 47696/100000: episode: 4775, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000254, mae: 0.005558, mean_q: 0.023812
 47706/100000: episode: 4776, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000413, mae: 0.005560, mean_q: 0.023683
 47716/100000: episode: 4777, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000016, mae: 0.002074, mean_q: 0.022869
 47726/100000: episode: 4778, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000055, mae: 0.003340, mean_q: 0.021196
 47736/100000: episode: 4779, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000055, mae: 0.002522, mean_q: 0.022297
 47746/100000: episode: 4780, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000265, mae: 0.004577, mean_q: 0.023228
 47756/100000: episode: 4781, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000046, mae: 0.003697, mean_q: 0.023545
 47766/100000: episode: 4782, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000038, mae: 0.002956, mean_q: 0.021459
 47776/100000: episode: 4783, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000244, mae: 0.004067, mean_q: 0.022675
 47786/100000: episode: 4784, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000062, mae: 0.003721, mean_q: 0.023628
 47796/100000: episode: 4785, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000247, mae: 0.004443, mean_q: 0.021506
 47806/100000: episode: 4786, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000243, mae: 0.004334, mean_q: 0.023604
 47816/100000: episode: 4787, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.003359, mean_q: 0.023696
 47826/100000: episode: 4788, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000076, mae: 0.003317, mean_q: 0.021867
 47836/100000: episode: 4789, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000059, mae: 0.003012, mean_q: 0.022232
 47846/100000: episode: 4790, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000036, mae: 0.002503, mean_q: 0.021886
 47856/100000: episode: 4791, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000056, mae: 0.002552, mean_q: 0.022720
 47866/100000: episode: 4792, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000034, mae: 0.002052, mean_q: 0.022405
 47876/100000: episode: 4793, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000388, mae: 0.003763, mean_q: 0.022424
 47886/100000: episode: 4794, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000097, mae: 0.003938, mean_q: 0.023437
 47896/100000: episode: 4795, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000036, mae: 0.002024, mean_q: 0.022568
 47906/100000: episode: 4796, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000207, mae: 0.003434, mean_q: 0.022832
 47916/100000: episode: 4797, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000222, mae: 0.003307, mean_q: 0.022438
 47926/100000: episode: 4798, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000061, mae: 0.003114, mean_q: 0.023053
 47936/100000: episode: 4799, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000080, mae: 0.003343, mean_q: 0.022375
 47946/100000: episode: 4800, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000036, mae: 0.002166, mean_q: 0.022555
 47956/100000: episode: 4801, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000038, mae: 0.002299, mean_q: 0.022881
 47966/100000: episode: 4802, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000033, mae: 0.002073, mean_q: 0.022226
 47976/100000: episode: 4803, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002820, mean_q: 0.021687
 47986/100000: episode: 4804, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000014, mae: 0.002108, mean_q: 0.021739
 47996/100000: episode: 4805, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000017, mae: 0.002706, mean_q: 0.021189
Step 48000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.48000.hdf5
 48006/100000: episode: 4806, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000053, mae: 0.002230, mean_q: 0.022043
 48016/100000: episode: 4807, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.002892, mean_q: 0.021593
[Info] 1-TH LEVEL FOUND: 0.021807169541716576, Considering 100/100 traces
 48026/100000: episode: 4808, duration: 0.717s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000041, mae: 0.002588, mean_q: 0.021792
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021807169541716576
1
 48036/100000: episode: 4809, duration: 0.488s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000017, mae: 0.002723, mean_q: 0.021013
 48046/100000: episode: 4810, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.002888, mean_q: 0.020943
 48056/100000: episode: 4811, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000262, mae: 0.004233, mean_q: 0.022483
 48066/100000: episode: 4812, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000042, mae: 0.003167, mean_q: 0.022637
 48076/100000: episode: 4813, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000223, mae: 0.003287, mean_q: 0.021763
 48086/100000: episode: 4814, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000017, mae: 0.002141, mean_q: 0.021610
 48096/100000: episode: 4815, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000244, mae: 0.003729, mean_q: 0.021533
 48106/100000: episode: 4816, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000032, mae: 0.001906, mean_q: 0.022127
 48116/100000: episode: 4817, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000203, mae: 0.003325, mean_q: 0.021043
 48126/100000: episode: 4818, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000012, mae: 0.001279, mean_q: 0.021862
 48136/100000: episode: 4819, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000038, mae: 0.002971, mean_q: 0.020945
 48146/100000: episode: 4820, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000083, mae: 0.003578, mean_q: 0.021375
 48156/100000: episode: 4821, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000059, mae: 0.003135, mean_q: 0.022310
 48166/100000: episode: 4822, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000266, mae: 0.004259, mean_q: 0.021684
 48176/100000: episode: 4823, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000056, mae: 0.003169, mean_q: 0.022813
 48186/100000: episode: 4824, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000055, mae: 0.002659, mean_q: 0.021226
 48196/100000: episode: 4825, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000202, mae: 0.003058, mean_q: 0.021380
 48206/100000: episode: 4826, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000282, mae: 0.005307, mean_q: 0.023128
 48216/100000: episode: 4827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000038, mae: 0.003883, mean_q: 0.024301
 48226/100000: episode: 4828, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000060, mae: 0.003410, mean_q: 0.021360
 48236/100000: episode: 4829, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000283, mae: 0.004766, mean_q: 0.021821
 48246/100000: episode: 4830, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000040, mae: 0.003263, mean_q: 0.023190
 48256/100000: episode: 4831, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000084, mae: 0.003914, mean_q: 0.021198
 48266/100000: episode: 4832, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002256, mean_q: 0.022522
 48276/100000: episode: 4833, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000432, mae: 0.005259, mean_q: 0.022764
 48286/100000: episode: 4834, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000244, mae: 0.004661, mean_q: 0.023428
 48296/100000: episode: 4835, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000031, mae: 0.001956, mean_q: 0.021927
 48306/100000: episode: 4836, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000395, mae: 0.004733, mean_q: 0.021359
 48316/100000: episode: 4837, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000016, mae: 0.002438, mean_q: 0.023194
 48326/100000: episode: 4838, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000096, mae: 0.003485, mean_q: 0.021656
 48336/100000: episode: 4839, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000231, mae: 0.003772, mean_q: 0.022064
 48346/100000: episode: 4840, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000248, mae: 0.004550, mean_q: 0.022961
 48356/100000: episode: 4841, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001391, mean_q: 0.022384
 48366/100000: episode: 4842, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000033, mae: 0.001859, mean_q: 0.021979
 48376/100000: episode: 4843, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000037, mae: 0.002755, mean_q: 0.021561
 48386/100000: episode: 4844, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000055, mae: 0.002807, mean_q: 0.021282
 48396/100000: episode: 4845, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000020, mae: 0.001980, mean_q: 0.021905
 48406/100000: episode: 4846, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000230, mae: 0.003821, mean_q: 0.021859
 48416/100000: episode: 4847, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000056, mae: 0.002766, mean_q: 0.022698
 48426/100000: episode: 4848, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000019, mae: 0.002148, mean_q: 0.022344
 48436/100000: episode: 4849, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000201, mae: 0.002606, mean_q: 0.021824
 48446/100000: episode: 4850, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000035, mae: 0.002126, mean_q: 0.021835
 48456/100000: episode: 4851, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000039, mae: 0.002709, mean_q: 0.021290
 48466/100000: episode: 4852, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000039, mae: 0.002258, mean_q: 0.021781
 48476/100000: episode: 4853, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000030, mae: 0.002036, mean_q: 0.021274
 48486/100000: episode: 4854, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000053, mae: 0.002385, mean_q: 0.021555
 48496/100000: episode: 4855, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000112, mae: 0.003066, mean_q: 0.021918
 48506/100000: episode: 4856, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000037, mae: 0.002486, mean_q: 0.022378
 48516/100000: episode: 4857, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000039, mae: 0.003271, mean_q: 0.020611
 48526/100000: episode: 4858, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000052, mae: 0.002266, mean_q: 0.021428
 48536/100000: episode: 4859, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000212, mae: 0.003582, mean_q: 0.021959
 48546/100000: episode: 4860, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002528, mean_q: 0.022132
 48556/100000: episode: 4861, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000014, mae: 0.002201, mean_q: 0.020936
 48566/100000: episode: 4862, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000226, mae: 0.003801, mean_q: 0.021057
 48576/100000: episode: 4863, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000044, mae: 0.004079, mean_q: 0.023381
 48586/100000: episode: 4864, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000098, mae: 0.003712, mean_q: 0.021015
 48596/100000: episode: 4865, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000039, mae: 0.002509, mean_q: 0.021856
 48606/100000: episode: 4866, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000223, mae: 0.003884, mean_q: 0.022738
 48616/100000: episode: 4867, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000060, mae: 0.003091, mean_q: 0.022065
 48626/100000: episode: 4868, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000034, mae: 0.002629, mean_q: 0.020853
 48636/100000: episode: 4869, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000238, mae: 0.003440, mean_q: 0.021781
 48646/100000: episode: 4870, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000056, mae: 0.003028, mean_q: 0.022651
 48656/100000: episode: 4871, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002193, mean_q: 0.021457
 48666/100000: episode: 4872, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000017, mae: 0.001817, mean_q: 0.021562
 48676/100000: episode: 4873, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000057, mae: 0.002594, mean_q: 0.021463
 48686/100000: episode: 4874, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000224, mae: 0.003518, mean_q: 0.021024
 48696/100000: episode: 4875, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000009, mae: 0.001767, mean_q: 0.022582
 48706/100000: episode: 4876, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.002062, mean_q: 0.021307
 48716/100000: episode: 4877, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000057, mae: 0.003150, mean_q: 0.020756
 48726/100000: episode: 4878, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000091, mae: 0.003135, mean_q: 0.022213
 48736/100000: episode: 4879, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000231, mae: 0.004335, mean_q: 0.022452
 48746/100000: episode: 4880, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000227, mae: 0.003871, mean_q: 0.022436
 48756/100000: episode: 4881, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000019, mae: 0.002967, mean_q: 0.023176
 48766/100000: episode: 4882, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000093, mae: 0.003202, mean_q: 0.021340
 48776/100000: episode: 4883, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000263, mae: 0.003992, mean_q: 0.022171
 48786/100000: episode: 4884, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000061, mae: 0.003545, mean_q: 0.022935
 48796/100000: episode: 4885, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000064, mae: 0.003697, mean_q: 0.021056
 48806/100000: episode: 4886, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000119, mae: 0.004320, mean_q: 0.022448
 48816/100000: episode: 4887, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.002682, mean_q: 0.022670
 48826/100000: episode: 4888, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000039, mae: 0.003719, mean_q: 0.020039
 48836/100000: episode: 4889, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000242, mae: 0.003780, mean_q: 0.021859
 48846/100000: episode: 4890, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000018, mae: 0.002546, mean_q: 0.022917
 48856/100000: episode: 4891, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000053, mae: 0.002763, mean_q: 0.021108
 48866/100000: episode: 4892, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000032, mae: 0.002117, mean_q: 0.021305
 48876/100000: episode: 4893, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000030, mae: 0.002010, mean_q: 0.021096
 48886/100000: episode: 4894, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000056, mae: 0.002544, mean_q: 0.021814
 48896/100000: episode: 4895, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000051, mae: 0.002258, mean_q: 0.021117
 48906/100000: episode: 4896, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000031, mae: 0.001675, mean_q: 0.021706
 48916/100000: episode: 4897, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000054, mae: 0.002433, mean_q: 0.021321
 48926/100000: episode: 4898, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000207, mae: 0.003505, mean_q: 0.021027
 48936/100000: episode: 4899, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000036, mae: 0.002062, mean_q: 0.021756
 48946/100000: episode: 4900, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000077, mae: 0.002828, mean_q: 0.021434
 48956/100000: episode: 4901, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000020, mae: 0.002167, mean_q: 0.021701
 48966/100000: episode: 4902, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000051, mae: 0.002098, mean_q: 0.021338
 48976/100000: episode: 4903, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000074, mae: 0.002754, mean_q: 0.021135
 48986/100000: episode: 4904, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000056, mae: 0.002386, mean_q: 0.021485
 48996/100000: episode: 4905, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000035, mae: 0.002079, mean_q: 0.021571
Step 49000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.49000.hdf5
 49006/100000: episode: 4906, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000035, mae: 0.002765, mean_q: 0.020359
 49016/100000: episode: 4907, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.002602, mean_q: 0.021606
 49026/100000: episode: 4908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000021, mae: 0.002922, mean_q: 0.020976
[Info] 1-TH LEVEL FOUND: 0.021351682022213936, Considering 100/100 traces
 49036/100000: episode: 4909, duration: 0.712s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000059, mae: 0.003579, mean_q: 0.019899
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021351682022213936
1
 49046/100000: episode: 4910, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000034, mae: 0.002165, mean_q: 0.021607
 49056/100000: episode: 4911, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000035, mae: 0.002290, mean_q: 0.020716
 49066/100000: episode: 4912, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000596, mae: 0.005418, mean_q: 0.021200
 49076/100000: episode: 4913, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000063, mae: 0.005158, mean_q: 0.024270
 49086/100000: episode: 4914, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000229, mae: 0.003835, mean_q: 0.021254
 49096/100000: episode: 4915, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000226, mae: 0.004073, mean_q: 0.022431
 49106/100000: episode: 4916, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000451, mae: 0.006806, mean_q: 0.023670
 49116/100000: episode: 4917, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000031, mae: 0.002325, mean_q: 0.022814
 49126/100000: episode: 4918, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000244, mae: 0.003834, mean_q: 0.021383
 49136/100000: episode: 4919, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000040, mae: 0.003070, mean_q: 0.022769
 49146/100000: episode: 4920, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000120, mae: 0.004089, mean_q: 0.021803
 49156/100000: episode: 4921, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000019, mae: 0.002362, mean_q: 0.022638
 49166/100000: episode: 4922, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000016, mae: 0.002641, mean_q: 0.020790
 49176/100000: episode: 4923, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000036, mae: 0.002922, mean_q: 0.020592
 49186/100000: episode: 4924, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000057, mae: 0.002614, mean_q: 0.021795
 49196/100000: episode: 4925, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000063, mae: 0.003082, mean_q: 0.021998
 49206/100000: episode: 4926, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000224, mae: 0.003524, mean_q: 0.022167
 49216/100000: episode: 4927, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000039, mae: 0.002436, mean_q: 0.022073
 49226/100000: episode: 4928, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000058, mae: 0.003079, mean_q: 0.021098
 49236/100000: episode: 4929, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000037, mae: 0.002509, mean_q: 0.021192
 49246/100000: episode: 4930, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000058, mae: 0.002819, mean_q: 0.021136
 49256/100000: episode: 4931, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000053, mae: 0.002179, mean_q: 0.021746
 49266/100000: episode: 4932, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000053, mae: 0.002355, mean_q: 0.021291
 49276/100000: episode: 4933, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000061, mae: 0.003148, mean_q: 0.021048
 49286/100000: episode: 4934, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000241, mae: 0.003391, mean_q: 0.021754
 49296/100000: episode: 4935, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000098, mae: 0.004719, mean_q: 0.023409
 49306/100000: episode: 4936, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000058, mae: 0.002903, mean_q: 0.021573
 49316/100000: episode: 4937, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000063, mae: 0.003509, mean_q: 0.020788
 49326/100000: episode: 4938, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.001961, mean_q: 0.021712
 49336/100000: episode: 4939, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000079, mae: 0.002979, mean_q: 0.021836
 49346/100000: episode: 4940, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000081, mae: 0.003386, mean_q: 0.021392
 49356/100000: episode: 4941, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000056, mae: 0.002784, mean_q: 0.022159
 49366/100000: episode: 4942, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000225, mae: 0.003402, mean_q: 0.021448
 49376/100000: episode: 4943, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000040, mae: 0.002624, mean_q: 0.021331
 49386/100000: episode: 4944, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000016, mae: 0.002540, mean_q: 0.020502
 49396/100000: episode: 4945, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000427, mae: 0.004684, mean_q: 0.021048
 49406/100000: episode: 4946, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000058, mae: 0.004692, mean_q: 0.024284
 49416/100000: episode: 4947, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000240, mae: 0.003473, mean_q: 0.021363
 49426/100000: episode: 4948, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000014, mae: 0.001378, mean_q: 0.021700
 49436/100000: episode: 4949, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000060, mae: 0.002894, mean_q: 0.021219
 49446/100000: episode: 4950, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000096, mae: 0.003325, mean_q: 0.022148
 49456/100000: episode: 4951, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000019, mae: 0.002291, mean_q: 0.021755
 49466/100000: episode: 4952, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000055, mae: 0.002464, mean_q: 0.021471
 49476/100000: episode: 4953, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000076, mae: 0.003099, mean_q: 0.021066
 49486/100000: episode: 4954, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000074, mae: 0.002893, mean_q: 0.022100
 49496/100000: episode: 4955, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000305, mae: 0.005510, mean_q: 0.022789
 49506/100000: episode: 4956, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000057, mae: 0.003293, mean_q: 0.022686
 49516/100000: episode: 4957, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000079, mae: 0.003690, mean_q: 0.020976
 49526/100000: episode: 4958, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000037, mae: 0.002213, mean_q: 0.021564
 49536/100000: episode: 4959, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002364, mean_q: 0.021313
 49546/100000: episode: 4960, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000012, mae: 0.002330, mean_q: 0.020268
 49556/100000: episode: 4961, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000056, mae: 0.002571, mean_q: 0.021144
 49566/100000: episode: 4962, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000037, mae: 0.002380, mean_q: 0.021569
 49576/100000: episode: 4963, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000455, mae: 0.005748, mean_q: 0.020791
 49586/100000: episode: 4964, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000080, mae: 0.005739, mean_q: 0.024967
 49596/100000: episode: 4965, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000059, mae: 0.003455, mean_q: 0.021504
 49606/100000: episode: 4966, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000017, mae: 0.003083, mean_q: 0.019993
 49616/100000: episode: 4967, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000036, mae: 0.002284, mean_q: 0.021633
 49626/100000: episode: 4968, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000009, mae: 0.001585, mean_q: 0.020906
 49636/100000: episode: 4969, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000056, mae: 0.002855, mean_q: 0.020729
 49646/100000: episode: 4970, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000225, mae: 0.003554, mean_q: 0.021856
 49656/100000: episode: 4971, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000016, mae: 0.002102, mean_q: 0.021424
 49666/100000: episode: 4972, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000052, mae: 0.002835, mean_q: 0.020264
 49676/100000: episode: 4973, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000037, mae: 0.002369, mean_q: 0.021082
 49686/100000: episode: 4974, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.002516, mean_q: 0.021106
 49696/100000: episode: 4975, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000058, mae: 0.002542, mean_q: 0.021142
 49706/100000: episode: 4976, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000011, mae: 0.001468, mean_q: 0.020820
 49716/100000: episode: 4977, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000078, mae: 0.003215, mean_q: 0.020706
 49726/100000: episode: 4978, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000221, mae: 0.003512, mean_q: 0.022040
 49736/100000: episode: 4979, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000035, mae: 0.002360, mean_q: 0.021953
 49746/100000: episode: 4980, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000058, mae: 0.003202, mean_q: 0.020366
 49756/100000: episode: 4981, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000055, mae: 0.002581, mean_q: 0.021431
 49766/100000: episode: 4982, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000074, mae: 0.002598, mean_q: 0.021171
 49776/100000: episode: 4983, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000041, mae: 0.003120, mean_q: 0.022014
 49786/100000: episode: 4984, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000250, mae: 0.004437, mean_q: 0.020251
 49796/100000: episode: 4985, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000244, mae: 0.005281, mean_q: 0.023478
 49806/100000: episode: 4986, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000033, mae: 0.002722, mean_q: 0.022117
 49816/100000: episode: 4987, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000060, mae: 0.004169, mean_q: 0.019358
 49826/100000: episode: 4988, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000071, mae: 0.002882, mean_q: 0.021947
 49836/100000: episode: 4989, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000062, mae: 0.003248, mean_q: 0.021710
 49846/100000: episode: 4990, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000074, mae: 0.002911, mean_q: 0.020777
 49856/100000: episode: 4991, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000114, mae: 0.004098, mean_q: 0.022469
 49866/100000: episode: 4992, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000074, mae: 0.004002, mean_q: 0.023349
 49876/100000: episode: 4993, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000248, mae: 0.004284, mean_q: 0.020955
 49886/100000: episode: 4994, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000282, mae: 0.005064, mean_q: 0.022736
 49896/100000: episode: 4995, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000055, mae: 0.003053, mean_q: 0.022344
 49906/100000: episode: 4996, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000227, mae: 0.003774, mean_q: 0.021211
 49916/100000: episode: 4997, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000075, mae: 0.003149, mean_q: 0.022408
 49926/100000: episode: 4998, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000391, mae: 0.003959, mean_q: 0.022104
 49936/100000: episode: 4999, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000035, mae: 0.003623, mean_q: 0.024075
 49946/100000: episode: 5000, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000075, mae: 0.003079, mean_q: 0.021615
 49956/100000: episode: 5001, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000039, mae: 0.003391, mean_q: 0.020562
 49966/100000: episode: 5002, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000056, mae: 0.003255, mean_q: 0.020634
 49976/100000: episode: 5003, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000032, mae: 0.002200, mean_q: 0.021123
 49986/100000: episode: 5004, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000076, mae: 0.002971, mean_q: 0.021180
 49996/100000: episode: 5005, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000056, mae: 0.003211, mean_q: 0.022731
Step 50000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.50000.hdf5
 50006/100000: episode: 5006, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000056, mae: 0.002523, mean_q: 0.021376
 50016/100000: episode: 5007, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002384, mean_q: 0.021633
 50026/100000: episode: 5008, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000056, mae: 0.002847, mean_q: 0.020985
 50036/100000: episode: 5009, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000096, mae: 0.003318, mean_q: 0.021402
[Info] 1-TH LEVEL FOUND: 0.02172842063009739, Considering 100/100 traces
 50046/100000: episode: 5010, duration: 0.767s, episode steps: 10, steps per second: 13, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000074, mae: 0.002594, mean_q: 0.021587
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02172842063009739
1
 50056/100000: episode: 5011, duration: 0.620s, episode steps: 10, steps per second: 16, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000076, mae: 0.002918, mean_q: 0.021797
 50066/100000: episode: 5012, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000206, mae: 0.003082, mean_q: 0.021270
 50076/100000: episode: 5013, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000017, mae: 0.002085, mean_q: 0.021229
 50086/100000: episode: 5014, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000029, mae: 0.001801, mean_q: 0.020934
 50096/100000: episode: 5015, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000013, mae: 0.001322, mean_q: 0.021563
 50106/100000: episode: 5016, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000076, mae: 0.002812, mean_q: 0.021245
 50116/100000: episode: 5017, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000020, mae: 0.001988, mean_q: 0.021419
 50126/100000: episode: 5018, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000038, mae: 0.002781, mean_q: 0.020534
 50136/100000: episode: 5019, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000435, mae: 0.005802, mean_q: 0.022257
 50146/100000: episode: 5020, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000203, mae: 0.004208, mean_q: 0.023364
 50156/100000: episode: 5021, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000058, mae: 0.003422, mean_q: 0.020299
 50166/100000: episode: 5022, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000450, mae: 0.005782, mean_q: 0.022174
 50176/100000: episode: 5023, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000061, mae: 0.004555, mean_q: 0.023951
 50186/100000: episode: 5024, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000039, mae: 0.003408, mean_q: 0.020257
 50196/100000: episode: 5025, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000204, mae: 0.003527, mean_q: 0.020630
 50206/100000: episode: 5026, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000015, mae: 0.002135, mean_q: 0.022247
 50216/100000: episode: 5027, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000098, mae: 0.004026, mean_q: 0.020438
 50226/100000: episode: 5028, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000034, mae: 0.002032, mean_q: 0.021335
 50236/100000: episode: 5029, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000017, mae: 0.002310, mean_q: 0.020869
 50246/100000: episode: 5030, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000285, mae: 0.004849, mean_q: 0.020295
 50256/100000: episode: 5031, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000034, mae: 0.003538, mean_q: 0.023615
 50266/100000: episode: 5032, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.002294, mean_q: 0.021010
 50276/100000: episode: 5033, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000055, mae: 0.002875, mean_q: 0.020429
 50286/100000: episode: 5034, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000071, mae: 0.002867, mean_q: 0.022321
 50296/100000: episode: 5035, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000203, mae: 0.003161, mean_q: 0.022148
 50306/100000: episode: 5036, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000391, mae: 0.003986, mean_q: 0.021392
 50316/100000: episode: 5037, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000041, mae: 0.002821, mean_q: 0.022020
 50326/100000: episode: 5038, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000060, mae: 0.002868, mean_q: 0.021323
 50336/100000: episode: 5039, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000028, mae: 0.001566, mean_q: 0.021192
 50346/100000: episode: 5040, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000222, mae: 0.003309, mean_q: 0.021065
 50356/100000: episode: 5041, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000434, mae: 0.005820, mean_q: 0.022741
 50366/100000: episode: 5042, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000039, mae: 0.003558, mean_q: 0.023251
 50376/100000: episode: 5043, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000247, mae: 0.004381, mean_q: 0.020712
 50386/100000: episode: 5044, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000057, mae: 0.002503, mean_q: 0.021758
 50396/100000: episode: 5045, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000221, mae: 0.003254, mean_q: 0.022265
 50406/100000: episode: 5046, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000410, mae: 0.005692, mean_q: 0.023641
 50416/100000: episode: 5047, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.002882, mean_q: 0.023012
 50426/100000: episode: 5048, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000038, mae: 0.003203, mean_q: 0.020571
 50436/100000: episode: 5049, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000245, mae: 0.004310, mean_q: 0.022519
 50446/100000: episode: 5050, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000220, mae: 0.005569, mean_q: 0.025226
 50456/100000: episode: 5051, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000036, mae: 0.002994, mean_q: 0.021924
 50466/100000: episode: 5052, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000247, mae: 0.005019, mean_q: 0.020324
 50476/100000: episode: 5053, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000207, mae: 0.004351, mean_q: 0.023494
 50486/100000: episode: 5054, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000074, mae: 0.002983, mean_q: 0.021882
 50496/100000: episode: 5055, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000203, mae: 0.003003, mean_q: 0.022103
 50506/100000: episode: 5056, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000056, mae: 0.003492, mean_q: 0.023624
 50516/100000: episode: 5057, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000038, mae: 0.002932, mean_q: 0.021487
 50526/100000: episode: 5058, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000035, mae: 0.002485, mean_q: 0.021412
 50536/100000: episode: 5059, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000057, mae: 0.002429, mean_q: 0.022220
 50546/100000: episode: 5060, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000224, mae: 0.003368, mean_q: 0.022023
 50556/100000: episode: 5061, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000041, mae: 0.003021, mean_q: 0.022895
 50566/100000: episode: 5062, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000075, mae: 0.002826, mean_q: 0.021961
 50576/100000: episode: 5063, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000055, mae: 0.003134, mean_q: 0.021052
 50586/100000: episode: 5064, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000054, mae: 0.002613, mean_q: 0.021570
 50596/100000: episode: 5065, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000078, mae: 0.003253, mean_q: 0.022529
 50606/100000: episode: 5066, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000030, mae: 0.001511, mean_q: 0.022060
 50616/100000: episode: 5067, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000095, mae: 0.003341, mean_q: 0.021481
 50626/100000: episode: 5068, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000060, mae: 0.002953, mean_q: 0.022160
 50636/100000: episode: 5069, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000071, mae: 0.002635, mean_q: 0.021341
 50646/100000: episode: 5070, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.001653, mean_q: 0.021771
 50656/100000: episode: 5071, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000263, mae: 0.003920, mean_q: 0.021692
 50666/100000: episode: 5072, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000028, mae: 0.004850, mean_q: 0.024964
 50676/100000: episode: 5073, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.003500, mean_q: 0.021548
 50686/100000: episode: 5074, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000079, mae: 0.003612, mean_q: 0.021146
 50696/100000: episode: 5075, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000053, mae: 0.002285, mean_q: 0.022337
 50706/100000: episode: 5076, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000223, mae: 0.003059, mean_q: 0.021879
 50716/100000: episode: 5077, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000036, mae: 0.002674, mean_q: 0.022979
 50726/100000: episode: 5078, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000096, mae: 0.003138, mean_q: 0.022222
 50736/100000: episode: 5079, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000017, mae: 0.001958, mean_q: 0.022308
 50746/100000: episode: 5080, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000039, mae: 0.003457, mean_q: 0.020541
 50756/100000: episode: 5081, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000043, mae: 0.002898, mean_q: 0.021402
 50766/100000: episode: 5082, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002268, mean_q: 0.022179
 50776/100000: episode: 5083, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000053, mae: 0.002346, mean_q: 0.021528
 50786/100000: episode: 5084, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000059, mae: 0.002858, mean_q: 0.022264
 50796/100000: episode: 5085, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000226, mae: 0.003668, mean_q: 0.022494
 50806/100000: episode: 5086, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000038, mae: 0.002181, mean_q: 0.021892
 50816/100000: episode: 5087, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002300, mean_q: 0.021433
 50826/100000: episode: 5088, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000204, mae: 0.003324, mean_q: 0.022628
 50836/100000: episode: 5089, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000075, mae: 0.002734, mean_q: 0.022234
 50846/100000: episode: 5090, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000017, mae: 0.002491, mean_q: 0.020935
 50856/100000: episode: 5091, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000011, mae: 0.001590, mean_q: 0.021391
 50866/100000: episode: 5092, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000053, mae: 0.002745, mean_q: 0.020920
 50876/100000: episode: 5093, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000038, mae: 0.002513, mean_q: 0.021312
 50886/100000: episode: 5094, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000434, mae: 0.006531, mean_q: 0.023986
 50896/100000: episode: 5095, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000014, mae: 0.002979, mean_q: 0.022358
 50906/100000: episode: 5096, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000040, mae: 0.004548, mean_q: 0.018943
 50916/100000: episode: 5097, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000033, mae: 0.002042, mean_q: 0.021813
 50926/100000: episode: 5098, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000779, mae: 0.008982, mean_q: 0.025047
 50936/100000: episode: 5099, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000082, mae: 0.005131, mean_q: 0.024531
 50946/100000: episode: 5100, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000075, mae: 0.003836, mean_q: 0.020470
 50956/100000: episode: 5101, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000013, mae: 0.001503, mean_q: 0.021992
 50966/100000: episode: 5102, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000032, mae: 0.002391, mean_q: 0.021263
 50976/100000: episode: 5103, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000075, mae: 0.003425, mean_q: 0.020872
 50986/100000: episode: 5104, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000448, mae: 0.006438, mean_q: 0.023848
 50996/100000: episode: 5105, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000053, mae: 0.003136, mean_q: 0.023505
Step 51000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.51000.hdf5
 51006/100000: episode: 5106, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000051, mae: 0.002750, mean_q: 0.021124
 51016/100000: episode: 5107, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000035, mae: 0.002569, mean_q: 0.021275
 51026/100000: episode: 5108, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000411, mae: 0.004704, mean_q: 0.022416
 51036/100000: episode: 5109, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000099, mae: 0.004255, mean_q: 0.023235
 51046/100000: episode: 5110, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000094, mae: 0.003041, mean_q: 0.021971
[Info] 1-TH LEVEL FOUND: 0.02245495840907097, Considering 100/100 traces
 51056/100000: episode: 5111, duration: 0.771s, episode steps: 10, steps per second: 13, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000058, mae: 0.003159, mean_q: 0.022993
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02245495840907097
1
 51066/100000: episode: 5112, duration: 0.589s, episode steps: 10, steps per second: 17, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000014, mae: 0.002164, mean_q: 0.021431
 51076/100000: episode: 5113, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000249, mae: 0.004397, mean_q: 0.021571
 51086/100000: episode: 5114, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000073, mae: 0.002661, mean_q: 0.022309
 51096/100000: episode: 5115, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000261, mae: 0.003698, mean_q: 0.021791
 51106/100000: episode: 5116, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000269, mae: 0.006270, mean_q: 0.024585
 51116/100000: episode: 5117, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000205, mae: 0.003909, mean_q: 0.022962
 51126/100000: episode: 5118, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000062, mae: 0.003380, mean_q: 0.022316
 51136/100000: episode: 5119, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000226, mae: 0.004179, mean_q: 0.023448
 51146/100000: episode: 5120, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000264, mae: 0.004004, mean_q: 0.022599
 51156/100000: episode: 5121, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000054, mae: 0.002563, mean_q: 0.022813
 51166/100000: episode: 5122, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000032, mae: 0.002472, mean_q: 0.021548
 51176/100000: episode: 5123, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000261, mae: 0.004176, mean_q: 0.022800
 51186/100000: episode: 5124, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000246, mae: 0.004829, mean_q: 0.023805
 51196/100000: episode: 5125, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000037, mae: 0.002502, mean_q: 0.022975
 51206/100000: episode: 5126, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000415, mae: 0.005388, mean_q: 0.021461
 51216/100000: episode: 5127, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000057, mae: 0.003226, mean_q: 0.023403
 51226/100000: episode: 5128, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000038, mae: 0.002455, mean_q: 0.022220
 51236/100000: episode: 5129, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000244, mae: 0.003969, mean_q: 0.022062
 51246/100000: episode: 5130, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000055, mae: 0.002473, mean_q: 0.022906
 51256/100000: episode: 5131, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000030, mae: 0.001783, mean_q: 0.022276
 51266/100000: episode: 5132, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000031, mae: 0.002351, mean_q: 0.021570
 51276/100000: episode: 5133, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000052, mae: 0.002399, mean_q: 0.022847
 51286/100000: episode: 5134, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000040, mae: 0.002748, mean_q: 0.022483
 51296/100000: episode: 5135, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.002815, mean_q: 0.021450
 51306/100000: episode: 5136, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000039, mae: 0.002369, mean_q: 0.022167
 51316/100000: episode: 5137, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000054, mae: 0.002403, mean_q: 0.021946
 51326/100000: episode: 5138, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000203, mae: 0.002921, mean_q: 0.022036
 51336/100000: episode: 5139, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000200, mae: 0.002823, mean_q: 0.022785
 51346/100000: episode: 5140, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002682, mean_q: 0.022237
 51356/100000: episode: 5141, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002439, mean_q: 0.021572
 51366/100000: episode: 5142, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000019, mae: 0.002025, mean_q: 0.022517
 51376/100000: episode: 5143, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000074, mae: 0.003064, mean_q: 0.021459
 51386/100000: episode: 5144, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000248, mae: 0.004005, mean_q: 0.022196
 51396/100000: episode: 5145, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.002698, mean_q: 0.022839
 51406/100000: episode: 5146, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.002645, mean_q: 0.021317
 51416/100000: episode: 5147, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000450, mae: 0.006246, mean_q: 0.023724
 51426/100000: episode: 5148, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000060, mae: 0.004204, mean_q: 0.024309
 51436/100000: episode: 5149, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000264, mae: 0.004234, mean_q: 0.021992
 51446/100000: episode: 5150, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000220, mae: 0.003639, mean_q: 0.023338
 51456/100000: episode: 5151, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000091, mae: 0.003863, mean_q: 0.023900
 51466/100000: episode: 5152, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000274, mae: 0.004536, mean_q: 0.023928
 51476/100000: episode: 5153, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000079, mae: 0.003445, mean_q: 0.023245
 51486/100000: episode: 5154, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000055, mae: 0.002715, mean_q: 0.022256
 51496/100000: episode: 5155, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.001976, mean_q: 0.022876
 51506/100000: episode: 5156, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000076, mae: 0.003657, mean_q: 0.021623
 51516/100000: episode: 5157, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000015, mae: 0.002182, mean_q: 0.021945
 51526/100000: episode: 5158, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002358, mean_q: 0.022148
 51536/100000: episode: 5159, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000036, mae: 0.002204, mean_q: 0.022141
 51546/100000: episode: 5160, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.001989, mean_q: 0.022316
 51556/100000: episode: 5161, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000249, mae: 0.004431, mean_q: 0.021869
 51566/100000: episode: 5162, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000077, mae: 0.003103, mean_q: 0.022888
 51576/100000: episode: 5163, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000204, mae: 0.003121, mean_q: 0.022701
 51586/100000: episode: 5164, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000329, mae: 0.005769, mean_q: 0.022471
 51596/100000: episode: 5165, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000400, mae: 0.004954, mean_q: 0.024214
 51606/100000: episode: 5166, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000056, mae: 0.003409, mean_q: 0.023877
 51616/100000: episode: 5167, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000056, mae: 0.003633, mean_q: 0.021079
 51626/100000: episode: 5168, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000267, mae: 0.004748, mean_q: 0.022618
 51636/100000: episode: 5169, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000220, mae: 0.004004, mean_q: 0.023970
 51646/100000: episode: 5170, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000407, mae: 0.004194, mean_q: 0.022949
 51656/100000: episode: 5171, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000017, mae: 0.002462, mean_q: 0.023424
 51666/100000: episode: 5172, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000054, mae: 0.003388, mean_q: 0.021416
 51676/100000: episode: 5173, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000279, mae: 0.004253, mean_q: 0.022664
 51686/100000: episode: 5174, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000063, mae: 0.004838, mean_q: 0.025057
 51696/100000: episode: 5175, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000013, mae: 0.002481, mean_q: 0.021729
 51706/100000: episode: 5176, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002863, mean_q: 0.022920
 51716/100000: episode: 5177, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000052, mae: 0.002714, mean_q: 0.023784
 51726/100000: episode: 5178, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000019, mae: 0.003097, mean_q: 0.021512
 51736/100000: episode: 5179, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000112, mae: 0.003810, mean_q: 0.021832
 51746/100000: episode: 5180, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000218, mae: 0.003887, mean_q: 0.023978
 51756/100000: episode: 5181, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000040, mae: 0.003177, mean_q: 0.022169
 51766/100000: episode: 5182, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000101, mae: 0.004660, mean_q: 0.021048
 51776/100000: episode: 5183, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.003076, mean_q: 0.023601
 51786/100000: episode: 5184, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000038, mae: 0.002771, mean_q: 0.021996
 51796/100000: episode: 5185, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000507, mae: 0.007100, mean_q: 0.023560
 51806/100000: episode: 5186, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000045, mae: 0.004978, mean_q: 0.025876
 51816/100000: episode: 5187, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000060, mae: 0.003772, mean_q: 0.021532
 51826/100000: episode: 5188, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000009, mae: 0.001615, mean_q: 0.022127
 51836/100000: episode: 5189, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000037, mae: 0.002253, mean_q: 0.022560
 51846/100000: episode: 5190, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000593, mae: 0.005526, mean_q: 0.022118
 51856/100000: episode: 5191, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000227, mae: 0.006160, mean_q: 0.026081
 51866/100000: episode: 5192, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000279, mae: 0.004277, mean_q: 0.023189
 51876/100000: episode: 5193, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000038, mae: 0.002455, mean_q: 0.023231
 51886/100000: episode: 5194, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.002258, mean_q: 0.022841
 51896/100000: episode: 5195, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000034, mae: 0.002219, mean_q: 0.022504
 51906/100000: episode: 5196, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000246, mae: 0.004389, mean_q: 0.023727
 51916/100000: episode: 5197, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000202, mae: 0.003605, mean_q: 0.024170
 51926/100000: episode: 5198, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000054, mae: 0.002330, mean_q: 0.023076
 51936/100000: episode: 5199, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000097, mae: 0.003954, mean_q: 0.022191
 51946/100000: episode: 5200, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000051, mae: 0.002729, mean_q: 0.024080
 51956/100000: episode: 5201, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000227, mae: 0.004035, mean_q: 0.023790
 51966/100000: episode: 5202, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000033, mae: 0.002187, mean_q: 0.022630
 51976/100000: episode: 5203, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000261, mae: 0.004345, mean_q: 0.022310
 51986/100000: episode: 5204, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000225, mae: 0.004472, mean_q: 0.024495
 51996/100000: episode: 5205, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000240, mae: 0.004621, mean_q: 0.024882
Step 52000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.52000.hdf5
 52006/100000: episode: 5206, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000052, mae: 0.002686, mean_q: 0.024040
 52016/100000: episode: 5207, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000019, mae: 0.003716, mean_q: 0.021414
 52026/100000: episode: 5208, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000043, mae: 0.003381, mean_q: 0.022211
 52036/100000: episode: 5209, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.001775, mean_q: 0.022646
 52046/100000: episode: 5210, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000013, mae: 0.002130, mean_q: 0.022250
 52056/100000: episode: 5211, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000060, mae: 0.003716, mean_q: 0.021760
[Info] 1-TH LEVEL FOUND: 0.02394862100481987, Considering 100/100 traces
 52066/100000: episode: 5212, duration: 0.746s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000073, mae: 0.002836, mean_q: 0.022576
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02394862100481987
1
 52076/100000: episode: 5213, duration: 0.511s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000041, mae: 0.003815, mean_q: 0.024590
 52086/100000: episode: 5214, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000059, mae: 0.003338, mean_q: 0.022256
 52096/100000: episode: 5215, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000053, mae: 0.002894, mean_q: 0.021846
 52106/100000: episode: 5216, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000017, mae: 0.001827, mean_q: 0.022631
 52116/100000: episode: 5217, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000058, mae: 0.003148, mean_q: 0.021941
 52126/100000: episode: 5218, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000243, mae: 0.003995, mean_q: 0.022542
 52136/100000: episode: 5219, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000225, mae: 0.004032, mean_q: 0.023658
 52146/100000: episode: 5220, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000095, mae: 0.003299, mean_q: 0.022899
 52156/100000: episode: 5221, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000057, mae: 0.002920, mean_q: 0.022192
 52166/100000: episode: 5222, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000015, mae: 0.001695, mean_q: 0.022415
 52176/100000: episode: 5223, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000073, mae: 0.002904, mean_q: 0.022129
 52186/100000: episode: 5224, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000076, mae: 0.003260, mean_q: 0.023362
 52196/100000: episode: 5225, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000058, mae: 0.003008, mean_q: 0.022459
 52206/100000: episode: 5226, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000240, mae: 0.003820, mean_q: 0.021782
 52216/100000: episode: 5227, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000151, mae: 0.005325, mean_q: 0.024269
 52226/100000: episode: 5228, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000019, mae: 0.002856, mean_q: 0.023525
 52236/100000: episode: 5229, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000391, mae: 0.004733, mean_q: 0.021642
 52246/100000: episode: 5230, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000087, mae: 0.003741, mean_q: 0.024425
 52256/100000: episode: 5231, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000077, mae: 0.003436, mean_q: 0.023061
 52266/100000: episode: 5232, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000267, mae: 0.004702, mean_q: 0.022943
 52276/100000: episode: 5233, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000072, mae: 0.003190, mean_q: 0.023931
 52286/100000: episode: 5234, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000202, mae: 0.002858, mean_q: 0.022998
 52296/100000: episode: 5235, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000042, mae: 0.003171, mean_q: 0.022190
 52306/100000: episode: 5236, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000247, mae: 0.004285, mean_q: 0.022274
 52316/100000: episode: 5237, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000057, mae: 0.002993, mean_q: 0.023541
 52326/100000: episode: 5238, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000032, mae: 0.002370, mean_q: 0.021963
 52336/100000: episode: 5239, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000018, mae: 0.002740, mean_q: 0.021723
 52346/100000: episode: 5240, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000013, mae: 0.002516, mean_q: 0.021373
 52356/100000: episode: 5241, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000030, mae: 0.002069, mean_q: 0.021700
 52366/100000: episode: 5242, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000013, mae: 0.001597, mean_q: 0.022243
 52376/100000: episode: 5243, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000014, mae: 0.002424, mean_q: 0.021258
 52386/100000: episode: 5244, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000075, mae: 0.003021, mean_q: 0.021697
 52396/100000: episode: 5245, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000405, mae: 0.005104, mean_q: 0.023691
 52406/100000: episode: 5246, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000018, mae: 0.003210, mean_q: 0.024347
 52416/100000: episode: 5247, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000016, mae: 0.003112, mean_q: 0.020737
 52426/100000: episode: 5248, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000035, mae: 0.003084, mean_q: 0.020894
 52436/100000: episode: 5249, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000037, mae: 0.002259, mean_q: 0.022006
 52446/100000: episode: 5250, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000073, mae: 0.002632, mean_q: 0.022491
 52456/100000: episode: 5251, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000021, mae: 0.002243, mean_q: 0.022306
 52466/100000: episode: 5252, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000051, mae: 0.002311, mean_q: 0.021746
 52476/100000: episode: 5253, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000408, mae: 0.004768, mean_q: 0.023073
 52486/100000: episode: 5254, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000031, mae: 0.002532, mean_q: 0.023465
 52496/100000: episode: 5255, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000390, mae: 0.004421, mean_q: 0.021237
 52506/100000: episode: 5256, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000059, mae: 0.004417, mean_q: 0.024642
 52516/100000: episode: 5257, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000077, mae: 0.003386, mean_q: 0.022737
 52526/100000: episode: 5258, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000037, mae: 0.002479, mean_q: 0.021986
 52536/100000: episode: 5259, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000246, mae: 0.004151, mean_q: 0.022950
 52546/100000: episode: 5260, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000202, mae: 0.003782, mean_q: 0.023930
 52556/100000: episode: 5261, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000031, mae: 0.002100, mean_q: 0.021917
 52566/100000: episode: 5262, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000056, mae: 0.002994, mean_q: 0.021831
 52576/100000: episode: 5263, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000052, mae: 0.002089, mean_q: 0.022309
 52586/100000: episode: 5264, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000076, mae: 0.003050, mean_q: 0.023117
 52596/100000: episode: 5265, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000035, mae: 0.002581, mean_q: 0.023159
 52606/100000: episode: 5266, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000052, mae: 0.003360, mean_q: 0.020803
 52616/100000: episode: 5267, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000056, mae: 0.002848, mean_q: 0.021848
 52626/100000: episode: 5268, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000070, mae: 0.002246, mean_q: 0.022495
 52636/100000: episode: 5269, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000248, mae: 0.004858, mean_q: 0.023480
 52646/100000: episode: 5270, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000055, mae: 0.002812, mean_q: 0.021875
 52656/100000: episode: 5271, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000235, mae: 0.003373, mean_q: 0.021919
 52666/100000: episode: 5272, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000415, mae: 0.006297, mean_q: 0.024252
 52676/100000: episode: 5273, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000054, mae: 0.002880, mean_q: 0.023381
 52686/100000: episode: 5274, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002256, mean_q: 0.021872
 52696/100000: episode: 5275, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000037, mae: 0.002327, mean_q: 0.022135
 52706/100000: episode: 5276, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000223, mae: 0.003458, mean_q: 0.022861
 52716/100000: episode: 5277, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.002098, mean_q: 0.022611
 52726/100000: episode: 5278, duration: 0.084s, episode steps: 10, steps per second: 120, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000222, mae: 0.003408, mean_q: 0.021937
 52736/100000: episode: 5279, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000014, mae: 0.001931, mean_q: 0.022939
 52746/100000: episode: 5280, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000036, mae: 0.002865, mean_q: 0.021438
 52756/100000: episode: 5281, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000012, mae: 0.002747, mean_q: 0.020784
 52766/100000: episode: 5282, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000282, mae: 0.004760, mean_q: 0.022191
 52776/100000: episode: 5283, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000213, mae: 0.006494, mean_q: 0.026184
 52786/100000: episode: 5284, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000227, mae: 0.004329, mean_q: 0.022104
 52796/100000: episode: 5285, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.002741, mean_q: 0.021324
 52806/100000: episode: 5286, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000652, mae: 0.007302, mean_q: 0.023591
 52816/100000: episode: 5287, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000095, mae: 0.006306, mean_q: 0.026819
 52826/100000: episode: 5288, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000462, mae: 0.005780, mean_q: 0.024158
 52836/100000: episode: 5289, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000055, mae: 0.002662, mean_q: 0.023247
 52846/100000: episode: 5290, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000093, mae: 0.003229, mean_q: 0.023083
 52856/100000: episode: 5291, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000077, mae: 0.003412, mean_q: 0.023660
 52866/100000: episode: 5292, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000265, mae: 0.004416, mean_q: 0.022833
 52876/100000: episode: 5293, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000073, mae: 0.002679, mean_q: 0.023204
 52886/100000: episode: 5294, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000031, mae: 0.002253, mean_q: 0.022350
 52896/100000: episode: 5295, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000051, mae: 0.002411, mean_q: 0.022418
 52906/100000: episode: 5296, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000092, mae: 0.002943, mean_q: 0.023340
 52916/100000: episode: 5297, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000577, mae: 0.006169, mean_q: 0.024217
 52926/100000: episode: 5298, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000231, mae: 0.005541, mean_q: 0.025178
 52936/100000: episode: 5299, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000056, mae: 0.003295, mean_q: 0.022087
 52946/100000: episode: 5300, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000099, mae: 0.003952, mean_q: 0.022489
 52956/100000: episode: 5301, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000407, mae: 0.005165, mean_q: 0.024282
 52966/100000: episode: 5302, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000405, mae: 0.006575, mean_q: 0.026294
 52976/100000: episode: 5303, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000241, mae: 0.005210, mean_q: 0.025640
 52986/100000: episode: 5304, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000224, mae: 0.004343, mean_q: 0.022129
 52996/100000: episode: 5305, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000053, mae: 0.002258, mean_q: 0.023503
Step 53000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.53000.hdf5
 53006/100000: episode: 5306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000036, mae: 0.002262, mean_q: 0.023478
 53016/100000: episode: 5307, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000037, mae: 0.002249, mean_q: 0.023650
 53026/100000: episode: 5308, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000244, mae: 0.003897, mean_q: 0.023847
 53036/100000: episode: 5309, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000095, mae: 0.003213, mean_q: 0.023744
 53046/100000: episode: 5310, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000139, mae: 0.004698, mean_q: 0.024219
 53056/100000: episode: 5311, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000406, mae: 0.004666, mean_q: 0.024328
 53066/100000: episode: 5312, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002656, mean_q: 0.024700
[Info] 1-TH LEVEL FOUND: 0.02422974817454815, Considering 100/100 traces
 53076/100000: episode: 5313, duration: 0.660s, episode steps: 10, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000235, mae: 0.003081, mean_q: 0.023749
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02422974817454815
1
 53086/100000: episode: 5314, duration: 0.502s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000220, mae: 0.003587, mean_q: 0.024612
 53096/100000: episode: 5315, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000073, mae: 0.002653, mean_q: 0.023979
 53106/100000: episode: 5316, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000021, mae: 0.002840, mean_q: 0.022905
 53116/100000: episode: 5317, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000051, mae: 0.003034, mean_q: 0.022377
 53126/100000: episode: 5318, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000056, mae: 0.002994, mean_q: 0.024370
 53136/100000: episode: 5319, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000089, mae: 0.002816, mean_q: 0.023561
 53146/100000: episode: 5320, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000039, mae: 0.002758, mean_q: 0.023113
 53156/100000: episode: 5321, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000245, mae: 0.004130, mean_q: 0.023308
 53166/100000: episode: 5322, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000258, mae: 0.004083, mean_q: 0.024189
 53176/100000: episode: 5323, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000074, mae: 0.003651, mean_q: 0.024969
 53186/100000: episode: 5324, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000224, mae: 0.003464, mean_q: 0.023572
 53196/100000: episode: 5325, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000052, mae: 0.002151, mean_q: 0.023807
 53206/100000: episode: 5326, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000032, mae: 0.002262, mean_q: 0.023063
 53216/100000: episode: 5327, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000238, mae: 0.003647, mean_q: 0.023724
 53226/100000: episode: 5328, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000467, mae: 0.006841, mean_q: 0.025243
 53236/100000: episode: 5329, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000013, mae: 0.002683, mean_q: 0.025660
 53246/100000: episode: 5330, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000137, mae: 0.004894, mean_q: 0.022909
 53256/100000: episode: 5331, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000236, mae: 0.003954, mean_q: 0.025020
 53266/100000: episode: 5332, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000239, mae: 0.005097, mean_q: 0.026222
 53276/100000: episode: 5333, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000033, mae: 0.002502, mean_q: 0.024034
 53286/100000: episode: 5334, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000448, mae: 0.005975, mean_q: 0.024083
 53296/100000: episode: 5335, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000055, mae: 0.003489, mean_q: 0.025723
 53306/100000: episode: 5336, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000073, mae: 0.002811, mean_q: 0.023998
 53316/100000: episode: 5337, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000035, mae: 0.002015, mean_q: 0.024320
 53326/100000: episode: 5338, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000031, mae: 0.002508, mean_q: 0.023267
 53336/100000: episode: 5339, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000015, mae: 0.002332, mean_q: 0.023178
 53346/100000: episode: 5340, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000197, mae: 0.002638, mean_q: 0.023980
 53356/100000: episode: 5341, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000222, mae: 0.003492, mean_q: 0.024615
 53366/100000: episode: 5342, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000238, mae: 0.003564, mean_q: 0.024602
 53376/100000: episode: 5343, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000075, mae: 0.002979, mean_q: 0.024099
 53386/100000: episode: 5344, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000032, mae: 0.002522, mean_q: 0.023188
 53396/100000: episode: 5345, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000485, mae: 0.006337, mean_q: 0.023830
 53406/100000: episode: 5346, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000220, mae: 0.005632, mean_q: 0.027214
 53416/100000: episode: 5347, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000037, mae: 0.002690, mean_q: 0.024756
 53426/100000: episode: 5348, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000053, mae: 0.003080, mean_q: 0.023247
 53436/100000: episode: 5349, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000058, mae: 0.002835, mean_q: 0.023956
 53446/100000: episode: 5350, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000241, mae: 0.003748, mean_q: 0.024295
 53456/100000: episode: 5351, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000201, mae: 0.004297, mean_q: 0.026219
 53466/100000: episode: 5352, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000018, mae: 0.002349, mean_q: 0.024436
 53476/100000: episode: 5353, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000079, mae: 0.003813, mean_q: 0.023236
 53486/100000: episode: 5354, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000201, mae: 0.003255, mean_q: 0.024995
 53496/100000: episode: 5355, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000236, mae: 0.003311, mean_q: 0.024015
 53506/100000: episode: 5356, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000057, mae: 0.002665, mean_q: 0.024546
 53516/100000: episode: 5357, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000225, mae: 0.004167, mean_q: 0.023201
 53526/100000: episode: 5358, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.003027, mean_q: 0.025111
 53536/100000: episode: 5359, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000080, mae: 0.003518, mean_q: 0.024686
 53546/100000: episode: 5360, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000012, mae: 0.001546, mean_q: 0.023956
 53556/100000: episode: 5361, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000052, mae: 0.002576, mean_q: 0.023642
 53566/100000: episode: 5362, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000019, mae: 0.001958, mean_q: 0.024098
 53576/100000: episode: 5363, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.002051, mean_q: 0.023445
 53586/100000: episode: 5364, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000054, mae: 0.003193, mean_q: 0.022775
 53596/100000: episode: 5365, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000252, mae: 0.003940, mean_q: 0.024931
 53606/100000: episode: 5366, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000226, mae: 0.004924, mean_q: 0.025849
 53616/100000: episode: 5367, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000058, mae: 0.003099, mean_q: 0.023908
 53626/100000: episode: 5368, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000226, mae: 0.004678, mean_q: 0.022693
 53636/100000: episode: 5369, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000222, mae: 0.003951, mean_q: 0.025084
 53646/100000: episode: 5370, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.002253, mean_q: 0.024508
 53656/100000: episode: 5371, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.003086, mean_q: 0.022884
 53666/100000: episode: 5372, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000388, mae: 0.004910, mean_q: 0.025190
 53676/100000: episode: 5373, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000221, mae: 0.003686, mean_q: 0.024862
 53686/100000: episode: 5374, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000019, mae: 0.002603, mean_q: 0.023414
 53696/100000: episode: 5375, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000054, mae: 0.003427, mean_q: 0.022615
 53706/100000: episode: 5376, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000232, mae: 0.004788, mean_q: 0.025048
 53716/100000: episode: 5377, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.003031, mean_q: 0.024095
 53726/100000: episode: 5378, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000077, mae: 0.003158, mean_q: 0.023695
 53736/100000: episode: 5379, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000240, mae: 0.003815, mean_q: 0.024483
 53746/100000: episode: 5380, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.003094, mean_q: 0.025027
 53756/100000: episode: 5381, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002661, mean_q: 0.023789
 53766/100000: episode: 5382, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000076, mae: 0.003491, mean_q: 0.023262
 53776/100000: episode: 5383, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000218, mae: 0.003137, mean_q: 0.024214
 53786/100000: episode: 5384, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.002450, mean_q: 0.024473
 53796/100000: episode: 5385, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000224, mae: 0.003752, mean_q: 0.023507
 53806/100000: episode: 5386, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000047, mae: 0.001994, mean_q: 0.024471
 53816/100000: episode: 5387, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000071, mae: 0.002659, mean_q: 0.023738
 53826/100000: episode: 5388, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000241, mae: 0.003873, mean_q: 0.024446
 53836/100000: episode: 5389, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000223, mae: 0.003899, mean_q: 0.024827
 53846/100000: episode: 5390, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000578, mae: 0.005785, mean_q: 0.024173
 53856/100000: episode: 5391, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000020, mae: 0.002958, mean_q: 0.025551
 53866/100000: episode: 5392, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000410, mae: 0.005348, mean_q: 0.022702
 53876/100000: episode: 5393, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000779, mae: 0.009716, mean_q: 0.027383
 53886/100000: episode: 5394, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000245, mae: 0.006281, mean_q: 0.027658
 53896/100000: episode: 5395, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000219, mae: 0.003680, mean_q: 0.023696
 53906/100000: episode: 5396, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.002631, mean_q: 0.024868
 53916/100000: episode: 5397, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000041, mae: 0.003168, mean_q: 0.023926
 53926/100000: episode: 5398, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000410, mae: 0.005193, mean_q: 0.023822
 53936/100000: episode: 5399, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000051, mae: 0.003089, mean_q: 0.025938
 53946/100000: episode: 5400, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000037, mae: 0.002657, mean_q: 0.024245
 53956/100000: episode: 5401, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000081, mae: 0.004456, mean_q: 0.022987
 53966/100000: episode: 5402, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000264, mae: 0.004395, mean_q: 0.024541
 53976/100000: episode: 5403, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000017, mae: 0.002339, mean_q: 0.025406
 53986/100000: episode: 5404, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000059, mae: 0.003680, mean_q: 0.023178
 53996/100000: episode: 5405, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000017, mae: 0.001811, mean_q: 0.024115
Step 54000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.54000.hdf5
 54006/100000: episode: 5406, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000034, mae: 0.002343, mean_q: 0.023631
 54016/100000: episode: 5407, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000465, mae: 0.005761, mean_q: 0.024193
 54026/100000: episode: 5408, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000077, mae: 0.004420, mean_q: 0.026184
 54036/100000: episode: 5409, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000221, mae: 0.003573, mean_q: 0.025037
 54046/100000: episode: 5410, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000092, mae: 0.003128, mean_q: 0.024214
 54056/100000: episode: 5411, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000014, mae: 0.001537, mean_q: 0.024333
 54066/100000: episode: 5412, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000221, mae: 0.003485, mean_q: 0.024073
 54076/100000: episode: 5413, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000245, mae: 0.004215, mean_q: 0.024872
[Info] 1-TH LEVEL FOUND: 0.023857779800891876, Considering 100/100 traces
 54086/100000: episode: 5414, duration: 0.661s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000035, mae: 0.002007, mean_q: 0.024650
[Info] 2-TH LEVEL FOUND: 0.026566216722130775, Considering 100/100 traces
 54096/100000: episode: 5415, duration: 0.681s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000444, mae: 0.005235, mean_q: 0.024869
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.026566216722130775
2
 54106/100000: episode: 5416, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000285, mae: 0.006429, mean_q: 0.026590
 54116/100000: episode: 5417, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000204, mae: 0.003568, mean_q: 0.025172
 54126/100000: episode: 5418, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000034, mae: 0.002960, mean_q: 0.023505
 54136/100000: episode: 5419, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000408, mae: 0.005095, mean_q: 0.024679
 54146/100000: episode: 5420, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000387, mae: 0.005956, mean_q: 0.027003
 54156/100000: episode: 5421, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000241, mae: 0.003987, mean_q: 0.024424
 54166/100000: episode: 5422, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000060, mae: 0.002857, mean_q: 0.025027
 54176/100000: episode: 5423, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000218, mae: 0.003022, mean_q: 0.024758
 54186/100000: episode: 5424, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000021, mae: 0.002603, mean_q: 0.025641
 54196/100000: episode: 5425, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000220, mae: 0.003501, mean_q: 0.024398
 54206/100000: episode: 5426, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000032, mae: 0.001808, mean_q: 0.024652
 54216/100000: episode: 5427, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000400, mae: 0.003872, mean_q: 0.024784
 54226/100000: episode: 5428, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.004220, mean_q: 0.027213
 54236/100000: episode: 5429, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000075, mae: 0.003600, mean_q: 0.024919
 54246/100000: episode: 5430, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000196, mae: 0.002943, mean_q: 0.024233
 54256/100000: episode: 5431, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000387, mae: 0.004810, mean_q: 0.025972
 54266/100000: episode: 5432, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000079, mae: 0.004085, mean_q: 0.026193
 54276/100000: episode: 5433, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000021, mae: 0.003051, mean_q: 0.023978
 54286/100000: episode: 5434, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000264, mae: 0.005043, mean_q: 0.023969
 54296/100000: episode: 5435, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000040, mae: 0.003164, mean_q: 0.025925
 54306/100000: episode: 5436, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000041, mae: 0.003661, mean_q: 0.023517
 54316/100000: episode: 5437, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000092, mae: 0.003580, mean_q: 0.024089
 54326/100000: episode: 5438, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000055, mae: 0.002431, mean_q: 0.025078
 54336/100000: episode: 5439, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000019, mae: 0.002125, mean_q: 0.024511
 54346/100000: episode: 5440, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000012, mae: 0.002171, mean_q: 0.023714
 54356/100000: episode: 5441, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000016, mae: 0.002531, mean_q: 0.023547
 54366/100000: episode: 5442, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000053, mae: 0.003032, mean_q: 0.023503
 54376/100000: episode: 5443, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000036, mae: 0.002496, mean_q: 0.023870
 54386/100000: episode: 5444, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000058, mae: 0.002939, mean_q: 0.023833
 54396/100000: episode: 5445, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000304, mae: 0.005181, mean_q: 0.024111
 54406/100000: episode: 5446, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000092, mae: 0.004896, mean_q: 0.026786
 54416/100000: episode: 5447, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000032, mae: 0.002382, mean_q: 0.025097
 54426/100000: episode: 5448, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000281, mae: 0.004842, mean_q: 0.023983
 54436/100000: episode: 5449, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000293, mae: 0.006842, mean_q: 0.027596
 54446/100000: episode: 5450, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.003234, mean_q: 0.026103
 54456/100000: episode: 5451, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000038, mae: 0.003161, mean_q: 0.023746
 54466/100000: episode: 5452, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000215, mae: 0.002908, mean_q: 0.024658
 54476/100000: episode: 5453, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000043, mae: 0.003140, mean_q: 0.024136
 54486/100000: episode: 5454, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.003240, mean_q: 0.023131
 54496/100000: episode: 5455, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000054, mae: 0.002563, mean_q: 0.024361
 54506/100000: episode: 5456, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000222, mae: 0.003303, mean_q: 0.024418
 54516/100000: episode: 5457, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000245, mae: 0.004140, mean_q: 0.024709
 54526/100000: episode: 5458, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002356, mean_q: 0.023773
 54536/100000: episode: 5459, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000216, mae: 0.003060, mean_q: 0.024021
 54546/100000: episode: 5460, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000317, mae: 0.006426, mean_q: 0.026132
 54556/100000: episode: 5461, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000417, mae: 0.005536, mean_q: 0.026397
 54566/100000: episode: 5462, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000056, mae: 0.003537, mean_q: 0.026185
 54576/100000: episode: 5463, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000222, mae: 0.003361, mean_q: 0.024745
 54586/100000: episode: 5464, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000078, mae: 0.003268, mean_q: 0.024553
 54596/100000: episode: 5465, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000055, mae: 0.002595, mean_q: 0.024539
 54606/100000: episode: 5466, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000260, mae: 0.004466, mean_q: 0.023989
 54616/100000: episode: 5467, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000040, mae: 0.002928, mean_q: 0.025555
 54626/100000: episode: 5468, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000053, mae: 0.003052, mean_q: 0.023729
 54636/100000: episode: 5469, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000056, mae: 0.002908, mean_q: 0.024630
 54646/100000: episode: 5470, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000260, mae: 0.004000, mean_q: 0.024834
 54656/100000: episode: 5471, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000018, mae: 0.002036, mean_q: 0.024805
 54666/100000: episode: 5472, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000064, mae: 0.004079, mean_q: 0.023455
 54676/100000: episode: 5473, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000203, mae: 0.003232, mean_q: 0.024320
 54686/100000: episode: 5474, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000054, mae: 0.002493, mean_q: 0.024598
 54696/100000: episode: 5475, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000095, mae: 0.003740, mean_q: 0.023643
 54706/100000: episode: 5476, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000200, mae: 0.003582, mean_q: 0.025546
 54716/100000: episode: 5477, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000221, mae: 0.003203, mean_q: 0.024719
 54726/100000: episode: 5478, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000265, mae: 0.005343, mean_q: 0.025740
 54736/100000: episode: 5479, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000071, mae: 0.002920, mean_q: 0.024980
 54746/100000: episode: 5480, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000424, mae: 0.005403, mean_q: 0.025562
 54756/100000: episode: 5481, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000406, mae: 0.005296, mean_q: 0.026054
 54766/100000: episode: 5482, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000282, mae: 0.005968, mean_q: 0.026458
 54776/100000: episode: 5483, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000381, mae: 0.004588, mean_q: 0.026415
 54786/100000: episode: 5484, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000059, mae: 0.003195, mean_q: 0.025524
 54796/100000: episode: 5485, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000119, mae: 0.004818, mean_q: 0.023936
 54806/100000: episode: 5486, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000033, mae: 0.001829, mean_q: 0.025059
 54816/100000: episode: 5487, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000240, mae: 0.003604, mean_q: 0.024997
 54826/100000: episode: 5488, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000035, mae: 0.002295, mean_q: 0.025617
 54836/100000: episode: 5489, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000074, mae: 0.003806, mean_q: 0.023725
 54846/100000: episode: 5490, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000056, mae: 0.002915, mean_q: 0.025247
 54856/100000: episode: 5491, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000057, mae: 0.002909, mean_q: 0.024774
 54866/100000: episode: 5492, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.003858, mean_q: 0.023307
 54876/100000: episode: 5493, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000039, mae: 0.002908, mean_q: 0.024084
 54886/100000: episode: 5494, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000260, mae: 0.004394, mean_q: 0.023991
 54896/100000: episode: 5495, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000261, mae: 0.005519, mean_q: 0.026613
 54906/100000: episode: 5496, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000425, mae: 0.005906, mean_q: 0.026311
 54916/100000: episode: 5497, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000074, mae: 0.003056, mean_q: 0.024882
 54926/100000: episode: 5498, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000222, mae: 0.003425, mean_q: 0.024682
 54936/100000: episode: 5499, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000082, mae: 0.003766, mean_q: 0.025551
 54946/100000: episode: 5500, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000110, mae: 0.003546, mean_q: 0.025468
 54956/100000: episode: 5501, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000014, mae: 0.001813, mean_q: 0.025335
 54966/100000: episode: 5502, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000256, mae: 0.003908, mean_q: 0.024780
 54976/100000: episode: 5503, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000044, mae: 0.003228, mean_q: 0.025454
 54986/100000: episode: 5504, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000228, mae: 0.004911, mean_q: 0.023525
 54996/100000: episode: 5505, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002448, mean_q: 0.025044
Step 55000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.55000.hdf5
 55006/100000: episode: 5506, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000081, mae: 0.004007, mean_q: 0.024000
 55016/100000: episode: 5507, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000060, mae: 0.003004, mean_q: 0.024427
 55026/100000: episode: 5508, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000410, mae: 0.005352, mean_q: 0.025764
 55036/100000: episode: 5509, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002804, mean_q: 0.024837
 55046/100000: episode: 5510, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000245, mae: 0.004390, mean_q: 0.024073
 55056/100000: episode: 5511, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000028, mae: 0.001541, mean_q: 0.024616
 55066/100000: episode: 5512, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000039, mae: 0.003220, mean_q: 0.023643
 55076/100000: episode: 5513, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000216, mae: 0.003420, mean_q: 0.023632
 55086/100000: episode: 5514, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000421, mae: 0.005621, mean_q: 0.025935
 55096/100000: episode: 5515, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000259, mae: 0.005109, mean_q: 0.026263
[Info] 1-TH LEVEL FOUND: 0.025090739130973816, Considering 100/100 traces
 55106/100000: episode: 5516, duration: 0.691s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000220, mae: 0.003605, mean_q: 0.025591
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025090739130973816
1
 55116/100000: episode: 5517, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000038, mae: 0.002658, mean_q: 0.024226
 55126/100000: episode: 5518, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000198, mae: 0.002855, mean_q: 0.024760
 55136/100000: episode: 5519, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000066, mae: 0.004006, mean_q: 0.023959
 55146/100000: episode: 5520, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000276, mae: 0.004693, mean_q: 0.023802
 55156/100000: episode: 5521, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000022, mae: 0.003438, mean_q: 0.026456
 55166/100000: episode: 5522, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000206, mae: 0.003786, mean_q: 0.024155
 55176/100000: episode: 5523, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000037, mae: 0.003505, mean_q: 0.022867
 55186/100000: episode: 5524, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000260, mae: 0.004495, mean_q: 0.025028
 55196/100000: episode: 5525, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000261, mae: 0.005349, mean_q: 0.026265
 55206/100000: episode: 5526, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.002042, mean_q: 0.024285
 55216/100000: episode: 5527, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000394, mae: 0.005134, mean_q: 0.023868
 55226/100000: episode: 5528, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000237, mae: 0.003917, mean_q: 0.025540
 55236/100000: episode: 5529, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000204, mae: 0.003334, mean_q: 0.025022
 55246/100000: episode: 5530, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000052, mae: 0.003058, mean_q: 0.023411
 55256/100000: episode: 5531, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000056, mae: 0.002516, mean_q: 0.024535
 55266/100000: episode: 5532, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000217, mae: 0.003516, mean_q: 0.025482
 55276/100000: episode: 5533, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000285, mae: 0.004911, mean_q: 0.024562
 55286/100000: episode: 5534, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000226, mae: 0.004695, mean_q: 0.026078
 55296/100000: episode: 5535, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000026, mae: 0.003019, mean_q: 0.024971
 55306/100000: episode: 5536, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000040, mae: 0.004276, mean_q: 0.022290
 55316/100000: episode: 5537, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000064, mae: 0.003648, mean_q: 0.023827
 55326/100000: episode: 5538, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002761, mean_q: 0.023907
 55336/100000: episode: 5539, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000049, mae: 0.002191, mean_q: 0.023911
 55346/100000: episode: 5540, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.001793, mean_q: 0.024535
 55356/100000: episode: 5541, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000063, mae: 0.003934, mean_q: 0.023156
 55366/100000: episode: 5542, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002779, mean_q: 0.023512
 55376/100000: episode: 5543, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000032, mae: 0.001864, mean_q: 0.024392
 55386/100000: episode: 5544, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000018, mae: 0.002388, mean_q: 0.023326
 55396/100000: episode: 5545, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000034, mae: 0.002535, mean_q: 0.023253
 55406/100000: episode: 5546, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000080, mae: 0.003786, mean_q: 0.023102
 55416/100000: episode: 5547, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000029, mae: 0.001615, mean_q: 0.023647
 55426/100000: episode: 5548, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000019, mae: 0.002697, mean_q: 0.022999
 55436/100000: episode: 5549, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000429, mae: 0.005685, mean_q: 0.022182
 55446/100000: episode: 5550, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000046, mae: 0.004926, mean_q: 0.026592
 55456/100000: episode: 5551, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000409, mae: 0.004718, mean_q: 0.023637
 55466/100000: episode: 5552, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000052, mae: 0.002212, mean_q: 0.023889
 55476/100000: episode: 5553, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000053, mae: 0.002499, mean_q: 0.023506
 55486/100000: episode: 5554, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000077, mae: 0.003424, mean_q: 0.023234
 55496/100000: episode: 5555, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000056, mae: 0.002584, mean_q: 0.023522
 55506/100000: episode: 5556, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002461, mean_q: 0.024516
 55516/100000: episode: 5557, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000019, mae: 0.002585, mean_q: 0.022852
 55526/100000: episode: 5558, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000031, mae: 0.002265, mean_q: 0.022747
 55536/100000: episode: 5559, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000398, mae: 0.005350, mean_q: 0.024521
 55546/100000: episode: 5560, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000247, mae: 0.004670, mean_q: 0.024626
 55556/100000: episode: 5561, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000076, mae: 0.002913, mean_q: 0.024204
 55566/100000: episode: 5562, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000410, mae: 0.004489, mean_q: 0.023814
 55576/100000: episode: 5563, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000078, mae: 0.004511, mean_q: 0.025738
 55586/100000: episode: 5564, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000224, mae: 0.003440, mean_q: 0.024032
 55596/100000: episode: 5565, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000011, mae: 0.001363, mean_q: 0.024224
 55606/100000: episode: 5566, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000043, mae: 0.003242, mean_q: 0.023231
 55616/100000: episode: 5567, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000239, mae: 0.003644, mean_q: 0.023535
 55626/100000: episode: 5568, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000225, mae: 0.004878, mean_q: 0.025835
 55636/100000: episode: 5569, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000018, mae: 0.002791, mean_q: 0.023411
 55646/100000: episode: 5570, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000020, mae: 0.003881, mean_q: 0.021433
 55656/100000: episode: 5571, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000018, mae: 0.002342, mean_q: 0.023067
 55666/100000: episode: 5572, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000414, mae: 0.005239, mean_q: 0.023257
 55676/100000: episode: 5573, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000260, mae: 0.004898, mean_q: 0.024899
 55686/100000: episode: 5574, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000241, mae: 0.003851, mean_q: 0.024020
 55696/100000: episode: 5575, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000054, mae: 0.002428, mean_q: 0.023809
 55706/100000: episode: 5576, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000032, mae: 0.001880, mean_q: 0.024122
 55716/100000: episode: 5577, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000045, mae: 0.002100, mean_q: 0.022903
 55726/100000: episode: 5578, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000054, mae: 0.002445, mean_q: 0.023846
 55736/100000: episode: 5579, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000040, mae: 0.002888, mean_q: 0.023158
 55746/100000: episode: 5580, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000244, mae: 0.004204, mean_q: 0.023103
 55756/100000: episode: 5581, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000424, mae: 0.005756, mean_q: 0.025285
 55766/100000: episode: 5582, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.002492, mean_q: 0.024551
 55776/100000: episode: 5583, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000574, mae: 0.005569, mean_q: 0.023548
 55786/100000: episode: 5584, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000449, mae: 0.006654, mean_q: 0.025441
 55796/100000: episode: 5585, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000201, mae: 0.003998, mean_q: 0.025624
 55806/100000: episode: 5586, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000016, mae: 0.002943, mean_q: 0.022615
 55816/100000: episode: 5587, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000017, mae: 0.003068, mean_q: 0.022238
 55826/100000: episode: 5588, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000033, mae: 0.001929, mean_q: 0.023540
 55836/100000: episode: 5589, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000203, mae: 0.003359, mean_q: 0.022989
 55846/100000: episode: 5590, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000018, mae: 0.002203, mean_q: 0.023152
 55856/100000: episode: 5591, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.002103, mean_q: 0.023069
 55866/100000: episode: 5592, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000036, mae: 0.002881, mean_q: 0.022411
 55876/100000: episode: 5593, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.001836, mean_q: 0.023553
 55886/100000: episode: 5594, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000222, mae: 0.003426, mean_q: 0.023058
 55896/100000: episode: 5595, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000054, mae: 0.002471, mean_q: 0.023837
 55906/100000: episode: 5596, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000199, mae: 0.002568, mean_q: 0.023264
 55916/100000: episode: 5597, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.002182, mean_q: 0.023098
 55926/100000: episode: 5598, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000240, mae: 0.003643, mean_q: 0.023601
 55936/100000: episode: 5599, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000014, mae: 0.002006, mean_q: 0.023092
 55946/100000: episode: 5600, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000052, mae: 0.003222, mean_q: 0.021896
 55956/100000: episode: 5601, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000389, mae: 0.004362, mean_q: 0.023336
 55966/100000: episode: 5602, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000221, mae: 0.004432, mean_q: 0.024957
 55976/100000: episode: 5603, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000200, mae: 0.003581, mean_q: 0.024725
 55986/100000: episode: 5604, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000465, mae: 0.005854, mean_q: 0.024355
 55996/100000: episode: 5605, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000270, mae: 0.005493, mean_q: 0.024884
Step 56000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.56000.hdf5
 56006/100000: episode: 5606, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000014, mae: 0.001695, mean_q: 0.023702
 56016/100000: episode: 5607, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000036, mae: 0.003071, mean_q: 0.022507
 56026/100000: episode: 5608, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000038, mae: 0.002806, mean_q: 0.022717
 56036/100000: episode: 5609, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.002571, mean_q: 0.022966
 56046/100000: episode: 5610, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000410, mae: 0.005057, mean_q: 0.022277
 56056/100000: episode: 5611, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000206, mae: 0.004696, mean_q: 0.025349
 56066/100000: episode: 5612, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000224, mae: 0.004039, mean_q: 0.023163
 56076/100000: episode: 5613, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000196, mae: 0.002874, mean_q: 0.022611
 56086/100000: episode: 5614, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000262, mae: 0.005230, mean_q: 0.024983
 56096/100000: episode: 5615, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000037, mae: 0.003062, mean_q: 0.024035
 56106/100000: episode: 5616, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000201, mae: 0.003480, mean_q: 0.022490
[Info] 1-TH LEVEL FOUND: 0.023557599633932114, Considering 100/100 traces
 56116/100000: episode: 5617, duration: 0.688s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000015, mae: 0.001730, mean_q: 0.023380
[Info] 2-TH LEVEL FOUND: 0.024824708700180054, Considering 100/100 traces
 56126/100000: episode: 5618, duration: 0.649s, episode steps: 10, steps per second: 15, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000219, mae: 0.003520, mean_q: 0.024281
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024824708700180054
2
 56136/100000: episode: 5619, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000037, mae: 0.002527, mean_q: 0.023862
 56146/100000: episode: 5620, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000221, mae: 0.003490, mean_q: 0.022878
 56156/100000: episode: 5621, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000038, mae: 0.002499, mean_q: 0.023977
 56166/100000: episode: 5622, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000203, mae: 0.002980, mean_q: 0.023733
 56176/100000: episode: 5623, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000093, mae: 0.003054, mean_q: 0.023789
 56186/100000: episode: 5624, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000042, mae: 0.003107, mean_q: 0.023086
 56196/100000: episode: 5625, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000057, mae: 0.003383, mean_q: 0.022215
 56206/100000: episode: 5626, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000019, mae: 0.001924, mean_q: 0.023354
 56216/100000: episode: 5627, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000019, mae: 0.002832, mean_q: 0.022249
 56226/100000: episode: 5628, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000061, mae: 0.003403, mean_q: 0.022392
 56236/100000: episode: 5629, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.001874, mean_q: 0.023266
 56246/100000: episode: 5630, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000202, mae: 0.003084, mean_q: 0.022672
 56256/100000: episode: 5631, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000056, mae: 0.002750, mean_q: 0.022659
 56266/100000: episode: 5632, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000232, mae: 0.005361, mean_q: 0.024959
 56276/100000: episode: 5633, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000021, mae: 0.003129, mean_q: 0.022958
 56286/100000: episode: 5634, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000397, mae: 0.005564, mean_q: 0.021442
 56296/100000: episode: 5635, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000218, mae: 0.004152, mean_q: 0.024746
 56306/100000: episode: 5636, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000450, mae: 0.005644, mean_q: 0.023942
 56316/100000: episode: 5637, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000020, mae: 0.002512, mean_q: 0.023955
 56326/100000: episode: 5638, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000061, mae: 0.003646, mean_q: 0.022087
 56336/100000: episode: 5639, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000281, mae: 0.004581, mean_q: 0.023189
 56346/100000: episode: 5640, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000041, mae: 0.003865, mean_q: 0.025136
 56356/100000: episode: 5641, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000243, mae: 0.004281, mean_q: 0.022302
 56366/100000: episode: 5642, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000204, mae: 0.003730, mean_q: 0.024333
 56376/100000: episode: 5643, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000224, mae: 0.004246, mean_q: 0.024620
 56386/100000: episode: 5644, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000227, mae: 0.003864, mean_q: 0.022863
 56396/100000: episode: 5645, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000236, mae: 0.004238, mean_q: 0.024876
 56406/100000: episode: 5646, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000244, mae: 0.004494, mean_q: 0.024508
 56416/100000: episode: 5647, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000058, mae: 0.002710, mean_q: 0.023844
 56426/100000: episode: 5648, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000448, mae: 0.006705, mean_q: 0.025598
 56436/100000: episode: 5649, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000386, mae: 0.005266, mean_q: 0.025560
 56446/100000: episode: 5650, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000018, mae: 0.002479, mean_q: 0.023201
 56456/100000: episode: 5651, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000062, mae: 0.004183, mean_q: 0.022351
 56466/100000: episode: 5652, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000297, mae: 0.004929, mean_q: 0.024186
 56476/100000: episode: 5653, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000442, mae: 0.006932, mean_q: 0.026275
 56486/100000: episode: 5654, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000266, mae: 0.005943, mean_q: 0.025964
 56496/100000: episode: 5655, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000054, mae: 0.002764, mean_q: 0.023702
 56506/100000: episode: 5656, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000071, mae: 0.003248, mean_q: 0.023061
 56516/100000: episode: 5657, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000410, mae: 0.006033, mean_q: 0.026164
 56526/100000: episode: 5658, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000038, mae: 0.002963, mean_q: 0.024658
 56536/100000: episode: 5659, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000010, mae: 0.002889, mean_q: 0.022134
 56546/100000: episode: 5660, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000410, mae: 0.005119, mean_q: 0.023960
 56556/100000: episode: 5661, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000403, mae: 0.005867, mean_q: 0.026259
 56566/100000: episode: 5662, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000255, mae: 0.004569, mean_q: 0.025695
 56576/100000: episode: 5663, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.002873, mean_q: 0.024928
 56586/100000: episode: 5664, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000320, mae: 0.005466, mean_q: 0.024510
 56596/100000: episode: 5665, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000213, mae: 0.005615, mean_q: 0.027099
 56606/100000: episode: 5666, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000020, mae: 0.002666, mean_q: 0.023977
 56616/100000: episode: 5667, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000037, mae: 0.002772, mean_q: 0.023828
 56626/100000: episode: 5668, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000411, mae: 0.005679, mean_q: 0.025930
 56636/100000: episode: 5669, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000203, mae: 0.003517, mean_q: 0.025404
 56646/100000: episode: 5670, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000028, mae: 0.003955, mean_q: 0.022974
 56656/100000: episode: 5671, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000054, mae: 0.002893, mean_q: 0.023862
 56666/100000: episode: 5672, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000247, mae: 0.004803, mean_q: 0.025428
 56676/100000: episode: 5673, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000227, mae: 0.004622, mean_q: 0.025806
 56686/100000: episode: 5674, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000040, mae: 0.002977, mean_q: 0.024038
 56696/100000: episode: 5675, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000054, mae: 0.003315, mean_q: 0.023161
 56706/100000: episode: 5676, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000046, mae: 0.003199, mean_q: 0.024040
 56716/100000: episode: 5677, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000410, mae: 0.005167, mean_q: 0.024708
 56726/100000: episode: 5678, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000027, mae: 0.003137, mean_q: 0.024771
 56736/100000: episode: 5679, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000031, mae: 0.001923, mean_q: 0.024077
 56746/100000: episode: 5680, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000037, mae: 0.002372, mean_q: 0.024225
 56756/100000: episode: 5681, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000242, mae: 0.004161, mean_q: 0.023609
 56766/100000: episode: 5682, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000063, mae: 0.003317, mean_q: 0.024819
 56776/100000: episode: 5683, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000410, mae: 0.004912, mean_q: 0.024762
 56786/100000: episode: 5684, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000018, mae: 0.002429, mean_q: 0.025089
 56796/100000: episode: 5685, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000428, mae: 0.005602, mean_q: 0.023158
 56806/100000: episode: 5686, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000099, mae: 0.005428, mean_q: 0.026621
 56816/100000: episode: 5687, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000033, mae: 0.003412, mean_q: 0.026796
 56826/100000: episode: 5688, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000202, mae: 0.003191, mean_q: 0.024310
 56836/100000: episode: 5689, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000039, mae: 0.002850, mean_q: 0.024052
 56846/100000: episode: 5690, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000083, mae: 0.003582, mean_q: 0.024480
 56856/100000: episode: 5691, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000052, mae: 0.002239, mean_q: 0.024365
 56866/100000: episode: 5692, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000237, mae: 0.004013, mean_q: 0.025519
 56876/100000: episode: 5693, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000061, mae: 0.003531, mean_q: 0.025523
 56886/100000: episode: 5694, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000038, mae: 0.002556, mean_q: 0.024189
 56896/100000: episode: 5695, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000244, mae: 0.004258, mean_q: 0.024100
 56906/100000: episode: 5696, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000386, mae: 0.004946, mean_q: 0.025855
 56916/100000: episode: 5697, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000079, mae: 0.004609, mean_q: 0.026609
 56926/100000: episode: 5698, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000034, mae: 0.003163, mean_q: 0.023314
 56936/100000: episode: 5699, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000017, mae: 0.002135, mean_q: 0.024559
 56946/100000: episode: 5700, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000571, mae: 0.006059, mean_q: 0.025953
 56956/100000: episode: 5701, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000228, mae: 0.004918, mean_q: 0.026341
 56966/100000: episode: 5702, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000061, mae: 0.003531, mean_q: 0.024055
 56976/100000: episode: 5703, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000241, mae: 0.004221, mean_q: 0.024046
 56986/100000: episode: 5704, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000042, mae: 0.003030, mean_q: 0.025594
 56996/100000: episode: 5705, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000217, mae: 0.003052, mean_q: 0.025115
Step 57000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.57000.hdf5
 57006/100000: episode: 5706, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000018, mae: 0.002116, mean_q: 0.024716
 57016/100000: episode: 5707, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000225, mae: 0.004096, mean_q: 0.023987
 57026/100000: episode: 5708, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000046, mae: 0.003865, mean_q: 0.026184
 57036/100000: episode: 5709, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000052, mae: 0.002670, mean_q: 0.024398
 57046/100000: episode: 5710, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000220, mae: 0.003507, mean_q: 0.024461
 57056/100000: episode: 5711, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000239, mae: 0.003973, mean_q: 0.025529
 57066/100000: episode: 5712, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000039, mae: 0.002513, mean_q: 0.025213
 57076/100000: episode: 5713, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000407, mae: 0.004840, mean_q: 0.024838
 57086/100000: episode: 5714, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000132, mae: 0.005004, mean_q: 0.026384
 57096/100000: episode: 5715, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000208, mae: 0.004397, mean_q: 0.026364
 57106/100000: episode: 5716, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000039, mae: 0.003312, mean_q: 0.023925
 57116/100000: episode: 5717, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000056, mae: 0.003413, mean_q: 0.023769
 57126/100000: episode: 5718, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000043, mae: 0.002744, mean_q: 0.024717
[Info] 1-TH LEVEL FOUND: 0.025171130895614624, Considering 100/100 traces
 57136/100000: episode: 5719, duration: 0.749s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000202, mae: 0.003092, mean_q: 0.024725
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025171130895614624
1
 57146/100000: episode: 5720, duration: 0.494s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.003016, mean_q: 0.025293
 57156/100000: episode: 5721, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000243, mae: 0.003966, mean_q: 0.025217
 57166/100000: episode: 5722, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000044, mae: 0.003330, mean_q: 0.024157
 57176/100000: episode: 5723, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000052, mae: 0.003014, mean_q: 0.023635
 57186/100000: episode: 5724, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000484, mae: 0.006914, mean_q: 0.025893
 57196/100000: episode: 5725, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000252, mae: 0.004823, mean_q: 0.026674
 57206/100000: episode: 5726, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000062, mae: 0.003453, mean_q: 0.025297
 57216/100000: episode: 5727, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000038, mae: 0.003499, mean_q: 0.023393
 57226/100000: episode: 5728, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000042, mae: 0.003300, mean_q: 0.023815
 57236/100000: episode: 5729, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000428, mae: 0.005230, mean_q: 0.025104
 57246/100000: episode: 5730, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000068, mae: 0.004937, mean_q: 0.026801
 57256/100000: episode: 5731, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000225, mae: 0.003982, mean_q: 0.024413
 57266/100000: episode: 5732, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002315, mean_q: 0.024400
 57276/100000: episode: 5733, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.003087, mean_q: 0.023950
 57286/100000: episode: 5734, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000572, mae: 0.005518, mean_q: 0.024329
 57296/100000: episode: 5735, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000049, mae: 0.005146, mean_q: 0.027599
 57306/100000: episode: 5736, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000752, mae: 0.007238, mean_q: 0.026457
 57316/100000: episode: 5737, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000391, mae: 0.005990, mean_q: 0.027065
 57326/100000: episode: 5738, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000065, mae: 0.003851, mean_q: 0.026308
 57336/100000: episode: 5739, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000052, mae: 0.002616, mean_q: 0.024777
 57346/100000: episode: 5740, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000219, mae: 0.003463, mean_q: 0.024659
 57356/100000: episode: 5741, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000062, mae: 0.003054, mean_q: 0.025457
 57366/100000: episode: 5742, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000218, mae: 0.003012, mean_q: 0.025294
 57376/100000: episode: 5743, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000057, mae: 0.002539, mean_q: 0.025297
 57386/100000: episode: 5744, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000053, mae: 0.002633, mean_q: 0.025763
 57396/100000: episode: 5745, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002242, mean_q: 0.024963
 57406/100000: episode: 5746, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000017, mae: 0.002843, mean_q: 0.023916
 57416/100000: episode: 5747, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000057, mae: 0.003573, mean_q: 0.023823
 57426/100000: episode: 5748, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000205, mae: 0.003489, mean_q: 0.024343
 57436/100000: episode: 5749, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000241, mae: 0.004310, mean_q: 0.025857
 57446/100000: episode: 5750, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000022, mae: 0.002782, mean_q: 0.024819
 57456/100000: episode: 5751, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000074, mae: 0.003786, mean_q: 0.023510
 57466/100000: episode: 5752, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.001967, mean_q: 0.024652
 57476/100000: episode: 5753, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000016, mae: 0.001959, mean_q: 0.024381
 57486/100000: episode: 5754, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000053, mae: 0.003246, mean_q: 0.023445
 57496/100000: episode: 5755, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000041, mae: 0.002893, mean_q: 0.025160
 57506/100000: episode: 5756, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000059, mae: 0.003131, mean_q: 0.024236
 57516/100000: episode: 5757, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000077, mae: 0.003419, mean_q: 0.024141
 57526/100000: episode: 5758, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000037, mae: 0.002798, mean_q: 0.023808
 57536/100000: episode: 5759, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002668, mean_q: 0.023906
 57546/100000: episode: 5760, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000387, mae: 0.004245, mean_q: 0.024847
 57556/100000: episode: 5761, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000019, mae: 0.002389, mean_q: 0.024107
 57566/100000: episode: 5762, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000223, mae: 0.003794, mean_q: 0.023468
 57576/100000: episode: 5763, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000056, mae: 0.002917, mean_q: 0.025015
 57586/100000: episode: 5764, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000018, mae: 0.002590, mean_q: 0.023468
 57596/100000: episode: 5765, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000206, mae: 0.004297, mean_q: 0.022726
 57606/100000: episode: 5766, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000225, mae: 0.003561, mean_q: 0.024147
 57616/100000: episode: 5767, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000039, mae: 0.003774, mean_q: 0.026139
 57626/100000: episode: 5768, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000039, mae: 0.002749, mean_q: 0.024115
 57636/100000: episode: 5769, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000038, mae: 0.002497, mean_q: 0.024066
 57646/100000: episode: 5770, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000053, mae: 0.002652, mean_q: 0.023655
 57656/100000: episode: 5771, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000208, mae: 0.003669, mean_q: 0.023688
 57666/100000: episode: 5772, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000039, mae: 0.002653, mean_q: 0.023871
 57676/100000: episode: 5773, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000204, mae: 0.004264, mean_q: 0.022261
 57686/100000: episode: 5774, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000112, mae: 0.004076, mean_q: 0.024510
 57696/100000: episode: 5775, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000241, mae: 0.004443, mean_q: 0.025058
 57706/100000: episode: 5776, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000058, mae: 0.002868, mean_q: 0.023629
 57716/100000: episode: 5777, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000022, mae: 0.003166, mean_q: 0.022756
 57726/100000: episode: 5778, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000040, mae: 0.003039, mean_q: 0.023080
 57736/100000: episode: 5779, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000222, mae: 0.003401, mean_q: 0.024010
 57746/100000: episode: 5780, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000202, mae: 0.003676, mean_q: 0.025035
 57756/100000: episode: 5781, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000209, mae: 0.003846, mean_q: 0.023111
 57766/100000: episode: 5782, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000218, mae: 0.003318, mean_q: 0.024531
 57776/100000: episode: 5783, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000200, mae: 0.003052, mean_q: 0.024455
 57786/100000: episode: 5784, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000243, mae: 0.004004, mean_q: 0.023404
 57796/100000: episode: 5785, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000200, mae: 0.002919, mean_q: 0.024452
 57806/100000: episode: 5786, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000243, mae: 0.003733, mean_q: 0.024125
 57816/100000: episode: 5787, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000223, mae: 0.004918, mean_q: 0.026181
 57826/100000: episode: 5788, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000015, mae: 0.002295, mean_q: 0.024403
 57836/100000: episode: 5789, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000204, mae: 0.004348, mean_q: 0.022210
 57846/100000: episode: 5790, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000020, mae: 0.002309, mean_q: 0.023574
 57856/100000: episode: 5791, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000217, mae: 0.003284, mean_q: 0.023054
 57866/100000: episode: 5792, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000278, mae: 0.005161, mean_q: 0.025136
 57876/100000: episode: 5793, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.004195, mean_q: 0.025602
 57886/100000: episode: 5794, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000021, mae: 0.002726, mean_q: 0.023425
 57896/100000: episode: 5795, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002901, mean_q: 0.022714
 57906/100000: episode: 5796, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000039, mae: 0.002470, mean_q: 0.023717
 57916/100000: episode: 5797, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000202, mae: 0.002897, mean_q: 0.023896
 57926/100000: episode: 5798, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000228, mae: 0.003857, mean_q: 0.024105
 57936/100000: episode: 5799, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000044, mae: 0.003587, mean_q: 0.022722
 57946/100000: episode: 5800, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000248, mae: 0.004525, mean_q: 0.023492
 57956/100000: episode: 5801, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000202, mae: 0.003037, mean_q: 0.024375
 57966/100000: episode: 5802, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.001926, mean_q: 0.023799
 57976/100000: episode: 5803, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000229, mae: 0.004055, mean_q: 0.023185
 57986/100000: episode: 5804, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000022, mae: 0.002544, mean_q: 0.024290
 57996/100000: episode: 5805, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000244, mae: 0.004539, mean_q: 0.022509
Step 58000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.58000.hdf5
 58006/100000: episode: 5806, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000035, mae: 0.002640, mean_q: 0.024473
 58016/100000: episode: 5807, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000228, mae: 0.003945, mean_q: 0.024016
 58026/100000: episode: 5808, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000037, mae: 0.002736, mean_q: 0.022995
 58036/100000: episode: 5809, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000246, mae: 0.004130, mean_q: 0.023275
 58046/100000: episode: 5810, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000608, mae: 0.006409, mean_q: 0.024872
 58056/100000: episode: 5811, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000024, mae: 0.004061, mean_q: 0.026449
 58066/100000: episode: 5812, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000016, mae: 0.003387, mean_q: 0.021747
 58076/100000: episode: 5813, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000201, mae: 0.003566, mean_q: 0.022570
 58086/100000: episode: 5814, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000036, mae: 0.002765, mean_q: 0.024670
 58096/100000: episode: 5815, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000020, mae: 0.002670, mean_q: 0.022958
 58106/100000: episode: 5816, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000032, mae: 0.003048, mean_q: 0.022055
 58116/100000: episode: 5817, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000196, mae: 0.002715, mean_q: 0.023945
 58126/100000: episode: 5818, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000057, mae: 0.002618, mean_q: 0.023685
 58136/100000: episode: 5819, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000203, mae: 0.003841, mean_q: 0.022178
[Info] 1-TH LEVEL FOUND: 0.024585925042629242, Considering 100/100 traces
 58146/100000: episode: 5820, duration: 0.680s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000200, mae: 0.002993, mean_q: 0.023308
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024585925042629242
1
 58156/100000: episode: 5821, duration: 0.517s, episode steps: 10, steps per second: 19, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000061, mae: 0.003516, mean_q: 0.024355
 58166/100000: episode: 5822, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000264, mae: 0.004545, mean_q: 0.022723
 58176/100000: episode: 5823, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000058, mae: 0.003596, mean_q: 0.024692
 58186/100000: episode: 5824, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000224, mae: 0.003680, mean_q: 0.023361
 58196/100000: episode: 5825, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000245, mae: 0.004394, mean_q: 0.024209
 58206/100000: episode: 5826, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000042, mae: 0.003421, mean_q: 0.024705
 58216/100000: episode: 5827, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000020, mae: 0.003792, mean_q: 0.021399
 58226/100000: episode: 5828, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000208, mae: 0.004198, mean_q: 0.022130
 58236/100000: episode: 5829, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000224, mae: 0.003693, mean_q: 0.023876
 58246/100000: episode: 5830, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000227, mae: 0.003615, mean_q: 0.023451
 58256/100000: episode: 5831, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000232, mae: 0.004388, mean_q: 0.023863
 58266/100000: episode: 5832, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.001913, mean_q: 0.023704
 58276/100000: episode: 5833, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000226, mae: 0.004044, mean_q: 0.022524
 58286/100000: episode: 5834, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000014, mae: 0.001609, mean_q: 0.023647
 58296/100000: episode: 5835, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000389, mae: 0.004440, mean_q: 0.024043
 58306/100000: episode: 5836, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000039, mae: 0.002902, mean_q: 0.023079
 58316/100000: episode: 5837, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000030, mae: 0.002973, mean_q: 0.021604
 58326/100000: episode: 5838, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000223, mae: 0.003544, mean_q: 0.023097
 58336/100000: episode: 5839, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000022, mae: 0.002803, mean_q: 0.024256
 58346/100000: episode: 5840, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000040, mae: 0.003334, mean_q: 0.022056
 58356/100000: episode: 5841, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000074, mae: 0.003094, mean_q: 0.022513
 58366/100000: episode: 5842, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000227, mae: 0.003816, mean_q: 0.023348
 58376/100000: episode: 5843, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000266, mae: 0.004484, mean_q: 0.023376
 58386/100000: episode: 5844, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000388, mae: 0.005672, mean_q: 0.025409
 58396/100000: episode: 5845, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000203, mae: 0.003261, mean_q: 0.023771
 58406/100000: episode: 5846, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000038, mae: 0.003242, mean_q: 0.022170
 58416/100000: episode: 5847, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002967, mean_q: 0.021814
 58426/100000: episode: 5848, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000245, mae: 0.004329, mean_q: 0.023114
 58436/100000: episode: 5849, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000040, mae: 0.002780, mean_q: 0.023071
 58446/100000: episode: 5850, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000220, mae: 0.003546, mean_q: 0.023507
 58456/100000: episode: 5851, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000019, mae: 0.002438, mean_q: 0.023218
 58466/100000: episode: 5852, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000010, mae: 0.002592, mean_q: 0.021304
 58476/100000: episode: 5853, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000219, mae: 0.003401, mean_q: 0.022167
 58486/100000: episode: 5854, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000020, mae: 0.002457, mean_q: 0.023628
 58496/100000: episode: 5855, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000054, mae: 0.002790, mean_q: 0.022276
 58506/100000: episode: 5856, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000013, mae: 0.001735, mean_q: 0.022425
 58516/100000: episode: 5857, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000200, mae: 0.002826, mean_q: 0.022393
 58526/100000: episode: 5858, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000018, mae: 0.002139, mean_q: 0.023066
 58536/100000: episode: 5859, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.003254, mean_q: 0.021576
 58546/100000: episode: 5860, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000016, mae: 0.001822, mean_q: 0.022692
 58556/100000: episode: 5861, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000081, mae: 0.003805, mean_q: 0.021785
 58566/100000: episode: 5862, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.001990, mean_q: 0.022746
 58576/100000: episode: 5863, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000038, mae: 0.002727, mean_q: 0.021908
 58586/100000: episode: 5864, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000016, mae: 0.002586, mean_q: 0.021433
 58596/100000: episode: 5865, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.002739, mean_q: 0.021516
 58606/100000: episode: 5866, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000013, mae: 0.001544, mean_q: 0.022103
 58616/100000: episode: 5867, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000039, mae: 0.003229, mean_q: 0.021163
 58626/100000: episode: 5868, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000015, mae: 0.002428, mean_q: 0.021026
 58636/100000: episode: 5869, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000009, mae: 0.001630, mean_q: 0.021337
 58646/100000: episode: 5870, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002044, mean_q: 0.021559
 58656/100000: episode: 5871, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000207, mae: 0.003349, mean_q: 0.021299
 58666/100000: episode: 5872, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000018, mae: 0.002248, mean_q: 0.022250
 58676/100000: episode: 5873, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000056, mae: 0.003236, mean_q: 0.020836
 58686/100000: episode: 5874, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000018, mae: 0.002586, mean_q: 0.020772
 58696/100000: episode: 5875, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000220, mae: 0.003290, mean_q: 0.020913
 58706/100000: episode: 5876, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000022, mae: 0.002452, mean_q: 0.021891
 58716/100000: episode: 5877, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000060, mae: 0.002728, mean_q: 0.021635
 58726/100000: episode: 5878, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000219, mae: 0.002822, mean_q: 0.021730
 58736/100000: episode: 5879, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000017, mae: 0.002224, mean_q: 0.021567
 58746/100000: episode: 5880, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000201, mae: 0.003760, mean_q: 0.019993
 58756/100000: episode: 5881, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000279, mae: 0.004868, mean_q: 0.022563
 58766/100000: episode: 5882, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000057, mae: 0.003797, mean_q: 0.023214
 58776/100000: episode: 5883, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000036, mae: 0.003471, mean_q: 0.019869
 58786/100000: episode: 5884, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.002412, mean_q: 0.020522
 58796/100000: episode: 5885, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000119, mae: 0.003837, mean_q: 0.021072
 58806/100000: episode: 5886, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000074, mae: 0.003356, mean_q: 0.022468
 58816/100000: episode: 5887, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000033, mae: 0.001940, mean_q: 0.021370
 58826/100000: episode: 5888, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000018, mae: 0.002280, mean_q: 0.020913
 58836/100000: episode: 5889, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000037, mae: 0.002672, mean_q: 0.020628
 58846/100000: episode: 5890, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.001836, mean_q: 0.020943
 58856/100000: episode: 5891, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000221, mae: 0.003540, mean_q: 0.020213
 58866/100000: episode: 5892, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000260, mae: 0.004866, mean_q: 0.022947
 58876/100000: episode: 5893, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000017, mae: 0.002793, mean_q: 0.021809
 58886/100000: episode: 5894, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000207, mae: 0.004622, mean_q: 0.019071
 58896/100000: episode: 5895, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000077, mae: 0.003616, mean_q: 0.022296
 58906/100000: episode: 5896, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000222, mae: 0.003510, mean_q: 0.021905
 58916/100000: episode: 5897, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000204, mae: 0.003403, mean_q: 0.022164
 58926/100000: episode: 5898, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000393, mae: 0.004688, mean_q: 0.022270
 58936/100000: episode: 5899, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002083, mean_q: 0.021006
 58946/100000: episode: 5900, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000032, mae: 0.002457, mean_q: 0.020513
 58956/100000: episode: 5901, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000010, mae: 0.001298, mean_q: 0.021078
 58966/100000: episode: 5902, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000226, mae: 0.003589, mean_q: 0.020903
 58976/100000: episode: 5903, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000034, mae: 0.002157, mean_q: 0.021114
 58986/100000: episode: 5904, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000010, mae: 0.002134, mean_q: 0.020076
 58996/100000: episode: 5905, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000015, mae: 0.002194, mean_q: 0.020475
Step 59000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.59000.hdf5
 59006/100000: episode: 5906, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000032, mae: 0.002303, mean_q: 0.020056
 59016/100000: episode: 5907, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000040, mae: 0.002397, mean_q: 0.021212
 59026/100000: episode: 5908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000054, mae: 0.002197, mean_q: 0.020868
 59036/100000: episode: 5909, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000051, mae: 0.002117, mean_q: 0.020545
 59046/100000: episode: 5910, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000034, mae: 0.001968, mean_q: 0.020987
 59056/100000: episode: 5911, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000057, mae: 0.002515, mean_q: 0.021012
 59066/100000: episode: 5912, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000324, mae: 0.005495, mean_q: 0.021844
 59076/100000: episode: 5913, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000208, mae: 0.004958, mean_q: 0.023364
 59086/100000: episode: 5914, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000229, mae: 0.003967, mean_q: 0.020309
 59096/100000: episode: 5915, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000015, mae: 0.001800, mean_q: 0.021557
 59106/100000: episode: 5916, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000225, mae: 0.003263, mean_q: 0.020922
 59116/100000: episode: 5917, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000223, mae: 0.003557, mean_q: 0.021858
 59126/100000: episode: 5918, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000219, mae: 0.003993, mean_q: 0.022822
 59136/100000: episode: 5919, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000577, mae: 0.005667, mean_q: 0.022533
 59146/100000: episode: 5920, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000058, mae: 0.003128, mean_q: 0.022158
[Info] 1-TH LEVEL FOUND: 0.023466141894459724, Considering 100/100 traces
 59156/100000: episode: 5921, duration: 0.694s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000409, mae: 0.004299, mean_q: 0.021358
[Info] 2-TH LEVEL FOUND: 0.02463419921696186, Considering 100/100 traces
 59166/100000: episode: 5922, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000265, mae: 0.006125, mean_q: 0.024214
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02463419921696186
2
 59176/100000: episode: 5923, duration: 0.523s, episode steps: 10, steps per second: 19, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000040, mae: 0.003418, mean_q: 0.022846
 59186/100000: episode: 5924, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000079, mae: 0.003546, mean_q: 0.020873
 59196/100000: episode: 5925, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000036, mae: 0.002339, mean_q: 0.022345
 59206/100000: episode: 5926, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000033, mae: 0.002183, mean_q: 0.021350
 59216/100000: episode: 5927, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.002885, mean_q: 0.020769
 59226/100000: episode: 5928, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000078, mae: 0.003099, mean_q: 0.021805
 59236/100000: episode: 5929, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000224, mae: 0.003917, mean_q: 0.022724
 59246/100000: episode: 5930, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000202, mae: 0.003068, mean_q: 0.022524
 59256/100000: episode: 5931, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000059, mae: 0.002851, mean_q: 0.021671
 59266/100000: episode: 5932, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000037, mae: 0.002496, mean_q: 0.021320
 59276/100000: episode: 5933, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000011, mae: 0.001813, mean_q: 0.021212
 59286/100000: episode: 5934, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000203, mae: 0.003896, mean_q: 0.020087
 59296/100000: episode: 5935, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002474, mean_q: 0.021694
 59306/100000: episode: 5936, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000036, mae: 0.002072, mean_q: 0.021735
 59316/100000: episode: 5937, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000013, mae: 0.001948, mean_q: 0.020856
 59326/100000: episode: 5938, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000076, mae: 0.003047, mean_q: 0.021341
 59336/100000: episode: 5939, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000200, mae: 0.002517, mean_q: 0.021567
 59346/100000: episode: 5940, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000059, mae: 0.003176, mean_q: 0.022229
 59356/100000: episode: 5941, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000032, mae: 0.002028, mean_q: 0.021161
 59366/100000: episode: 5942, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000054, mae: 0.002705, mean_q: 0.020823
 59376/100000: episode: 5943, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000060, mae: 0.002821, mean_q: 0.021682
 59386/100000: episode: 5944, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000225, mae: 0.003374, mean_q: 0.021215
 59396/100000: episode: 5945, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000020, mae: 0.001924, mean_q: 0.021641
 59406/100000: episode: 5946, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000040, mae: 0.002883, mean_q: 0.020714
 59416/100000: episode: 5947, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000092, mae: 0.003111, mean_q: 0.021665
 59426/100000: episode: 5948, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000037, mae: 0.002697, mean_q: 0.022156
 59436/100000: episode: 5949, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000223, mae: 0.003387, mean_q: 0.020682
 59446/100000: episode: 5950, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000015, mae: 0.002138, mean_q: 0.022036
 59456/100000: episode: 5951, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000204, mae: 0.003631, mean_q: 0.020027
 59466/100000: episode: 5952, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000223, mae: 0.003843, mean_q: 0.022243
 59476/100000: episode: 5953, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000053, mae: 0.002741, mean_q: 0.021477
 59486/100000: episode: 5954, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000010, mae: 0.002284, mean_q: 0.019999
 59496/100000: episode: 5955, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.001813, mean_q: 0.021141
 59506/100000: episode: 5956, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002479, mean_q: 0.020592
 59516/100000: episode: 5957, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000222, mae: 0.003711, mean_q: 0.019954
 59526/100000: episode: 5958, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.002464, mean_q: 0.021867
 59536/100000: episode: 5959, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002699, mean_q: 0.020011
 59546/100000: episode: 5960, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000205, mae: 0.003269, mean_q: 0.020383
 59556/100000: episode: 5961, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000205, mae: 0.003924, mean_q: 0.022386
 59566/100000: episode: 5962, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000020, mae: 0.002871, mean_q: 0.020443
 59576/100000: episode: 5963, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000015, mae: 0.003289, mean_q: 0.018880
 59586/100000: episode: 5964, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000224, mae: 0.003435, mean_q: 0.020497
 59596/100000: episode: 5965, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000265, mae: 0.005075, mean_q: 0.022333
 59606/100000: episode: 5966, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002740, mean_q: 0.021851
 59616/100000: episode: 5967, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000038, mae: 0.003307, mean_q: 0.019525
 59626/100000: episode: 5968, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000077, mae: 0.002984, mean_q: 0.020483
 59636/100000: episode: 5969, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000081, mae: 0.003897, mean_q: 0.021989
 59646/100000: episode: 5970, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000578, mae: 0.005814, mean_q: 0.022197
 59656/100000: episode: 5971, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000219, mae: 0.003776, mean_q: 0.022613
 59666/100000: episode: 5972, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000014, mae: 0.001855, mean_q: 0.021607
 59676/100000: episode: 5973, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000041, mae: 0.003358, mean_q: 0.019932
 59686/100000: episode: 5974, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000244, mae: 0.003887, mean_q: 0.021340
 59696/100000: episode: 5975, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000243, mae: 0.005578, mean_q: 0.023780
 59706/100000: episode: 5976, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000605, mae: 0.005980, mean_q: 0.021177
 59716/100000: episode: 5977, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000020, mae: 0.003127, mean_q: 0.022828
 59726/100000: episode: 5978, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000014, mae: 0.003166, mean_q: 0.019460
 59736/100000: episode: 5979, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000085, mae: 0.004467, mean_q: 0.019832
 59746/100000: episode: 5980, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.002556, mean_q: 0.021817
 59756/100000: episode: 5981, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000035, mae: 0.002981, mean_q: 0.020016
 59766/100000: episode: 5982, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000055, mae: 0.002550, mean_q: 0.020775
 59776/100000: episode: 5983, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000080, mae: 0.003011, mean_q: 0.020911
 59786/100000: episode: 5984, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000600, mae: 0.007045, mean_q: 0.023335
 59796/100000: episode: 5985, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000226, mae: 0.004929, mean_q: 0.023227
 59806/100000: episode: 5986, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000010, mae: 0.002638, mean_q: 0.019617
 59816/100000: episode: 5987, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000248, mae: 0.004443, mean_q: 0.021298
 59826/100000: episode: 5988, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002554, mean_q: 0.022053
 59836/100000: episode: 5989, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000203, mae: 0.003505, mean_q: 0.020221
 59846/100000: episode: 5990, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000013, mae: 0.001491, mean_q: 0.021242
 59856/100000: episode: 5991, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000035, mae: 0.002627, mean_q: 0.020196
 59866/100000: episode: 5992, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000015, mae: 0.002070, mean_q: 0.020601
 59876/100000: episode: 5993, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000409, mae: 0.004219, mean_q: 0.021092
 59886/100000: episode: 5994, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000036, mae: 0.002917, mean_q: 0.022220
 59896/100000: episode: 5995, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000078, mae: 0.003524, mean_q: 0.020224
 59906/100000: episode: 5996, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000248, mae: 0.004230, mean_q: 0.020644
 59916/100000: episode: 5997, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000039, mae: 0.002623, mean_q: 0.021127
 59926/100000: episode: 5998, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000055, mae: 0.002668, mean_q: 0.020452
 59936/100000: episode: 5999, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000231, mae: 0.004555, mean_q: 0.022365
 59946/100000: episode: 6000, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000076, mae: 0.003610, mean_q: 0.022398
 59956/100000: episode: 6001, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000018, mae: 0.002351, mean_q: 0.020532
 59966/100000: episode: 6002, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.002144, mean_q: 0.020263
 59976/100000: episode: 6003, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000020, mae: 0.002081, mean_q: 0.020855
 59986/100000: episode: 6004, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000033, mae: 0.002190, mean_q: 0.020435
 59996/100000: episode: 6005, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000013, mae: 0.001311, mean_q: 0.020899
Step 60000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.60000.hdf5
 60006/100000: episode: 6006, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000054, mae: 0.002472, mean_q: 0.020489
 60016/100000: episode: 6007, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000060, mae: 0.002945, mean_q: 0.021242
 60026/100000: episode: 6008, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000075, mae: 0.002678, mean_q: 0.021297
 60036/100000: episode: 6009, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000073, mae: 0.002832, mean_q: 0.021609
 60046/100000: episode: 6010, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000013, mae: 0.001875, mean_q: 0.020701
 60056/100000: episode: 6011, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.002280, mean_q: 0.019657
 60066/100000: episode: 6012, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000021, mae: 0.002575, mean_q: 0.021530
 60076/100000: episode: 6013, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000240, mae: 0.003239, mean_q: 0.021349
 60086/100000: episode: 6014, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000038, mae: 0.002527, mean_q: 0.020587
 60096/100000: episode: 6015, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000016, mae: 0.002637, mean_q: 0.019617
 60106/100000: episode: 6016, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000109, mae: 0.002819, mean_q: 0.020698
 60116/100000: episode: 6017, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000013, mae: 0.002687, mean_q: 0.022446
 60126/100000: episode: 6018, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000230, mae: 0.003787, mean_q: 0.020231
 60136/100000: episode: 6019, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000055, mae: 0.003648, mean_q: 0.022629
 60146/100000: episode: 6020, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002328, mean_q: 0.020789
 60156/100000: episode: 6021, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000038, mae: 0.002617, mean_q: 0.020212
 60166/100000: episode: 6022, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000017, mae: 0.002037, mean_q: 0.021251
[Info] 1-TH LEVEL FOUND: 0.01973704807460308, Considering 100/100 traces
 60176/100000: episode: 6023, duration: 0.692s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000056, mae: 0.003394, mean_q: 0.019635
[Info] 2-TH LEVEL FOUND: 0.022456765174865723, Considering 100/100 traces
 60186/100000: episode: 6024, duration: 0.670s, episode steps: 10, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [0.000, 10.000], loss: 0.000245, mae: 0.004209, mean_q: 0.021639
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022456765174865723
2
 60196/100000: episode: 6025, duration: 0.495s, episode steps: 10, steps per second: 20, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.003058, mean_q: 0.021275
 60206/100000: episode: 6026, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000415, mae: 0.004662, mean_q: 0.020716
 60216/100000: episode: 6027, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000228, mae: 0.004962, mean_q: 0.022794
 60226/100000: episode: 6028, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000245, mae: 0.003845, mean_q: 0.020921
 60236/100000: episode: 6029, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000284, mae: 0.004712, mean_q: 0.021770
 60246/100000: episode: 6030, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000210, mae: 0.004313, mean_q: 0.022539
 60256/100000: episode: 6031, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000242, mae: 0.003622, mean_q: 0.021468
 60266/100000: episode: 6032, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.000393, mae: 0.004195, mean_q: 0.020679
 60276/100000: episode: 6033, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000226, mae: 0.004423, mean_q: 0.022633
 60286/100000: episode: 6034, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000040, mae: 0.003124, mean_q: 0.022142
 60296/100000: episode: 6035, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002963, mean_q: 0.020115
 60306/100000: episode: 6036, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000071, mae: 0.002646, mean_q: 0.021417
 60316/100000: episode: 6037, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000059, mae: 0.003311, mean_q: 0.022293
 60326/100000: episode: 6038, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000240, mae: 0.003276, mean_q: 0.021672
 60336/100000: episode: 6039, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000055, mae: 0.002429, mean_q: 0.021854
 60346/100000: episode: 6040, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.001795, mean_q: 0.021019
 60356/100000: episode: 6041, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002645, mean_q: 0.020539
 60366/100000: episode: 6042, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000040, mae: 0.002390, mean_q: 0.021528
 60376/100000: episode: 6043, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000419, mae: 0.006240, mean_q: 0.023376
 60386/100000: episode: 6044, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000231, mae: 0.005396, mean_q: 0.023748
 60396/100000: episode: 6045, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000057, mae: 0.003181, mean_q: 0.020900
 60406/100000: episode: 6046, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.002687, mean_q: 0.020753
 60416/100000: episode: 6047, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000062, mae: 0.003071, mean_q: 0.022112
 60426/100000: episode: 6048, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.001991, mean_q: 0.021821
 60436/100000: episode: 6049, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000075, mae: 0.002773, mean_q: 0.021182
 60446/100000: episode: 6050, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000263, mae: 0.004689, mean_q: 0.022802
 60456/100000: episode: 6051, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000039, mae: 0.002921, mean_q: 0.022409
 60466/100000: episode: 6052, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000061, mae: 0.003702, mean_q: 0.020381
 60476/100000: episode: 6053, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000074, mae: 0.002641, mean_q: 0.021803
 60486/100000: episode: 6054, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000080, mae: 0.003920, mean_q: 0.022832
 60496/100000: episode: 6055, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000058, mae: 0.003099, mean_q: 0.020980
 60506/100000: episode: 6056, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000054, mae: 0.002617, mean_q: 0.020991
 60516/100000: episode: 6057, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000043, mae: 0.002811, mean_q: 0.021805
 60526/100000: episode: 6058, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000412, mae: 0.004691, mean_q: 0.022118
 60536/100000: episode: 6059, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000242, mae: 0.004055, mean_q: 0.022644
 60546/100000: episode: 6060, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000061, mae: 0.003363, mean_q: 0.022562
 60556/100000: episode: 6061, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000226, mae: 0.003389, mean_q: 0.021763
 60566/100000: episode: 6062, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000225, mae: 0.003388, mean_q: 0.021507
 60576/100000: episode: 6063, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000057, mae: 0.002814, mean_q: 0.022178
 60586/100000: episode: 6064, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000031, mae: 0.001758, mean_q: 0.021582
 60596/100000: episode: 6065, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000456, mae: 0.006106, mean_q: 0.022388
 60606/100000: episode: 6066, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000058, mae: 0.003860, mean_q: 0.023671
 60616/100000: episode: 6067, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000037, mae: 0.003031, mean_q: 0.020949
 60626/100000: episode: 6068, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.002407, mean_q: 0.021032
 60636/100000: episode: 6069, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000082, mae: 0.003681, mean_q: 0.022306
 60646/100000: episode: 6070, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000228, mae: 0.003691, mean_q: 0.021399
 60656/100000: episode: 6071, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000079, mae: 0.003965, mean_q: 0.023169
 60666/100000: episode: 6072, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000072, mae: 0.002639, mean_q: 0.022064
 60676/100000: episode: 6073, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000094, mae: 0.003242, mean_q: 0.022302
 60686/100000: episode: 6074, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000034, mae: 0.002556, mean_q: 0.022974
 60696/100000: episode: 6075, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000055, mae: 0.002793, mean_q: 0.021462
 60706/100000: episode: 6076, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000432, mae: 0.005234, mean_q: 0.021813
 60716/100000: episode: 6077, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000064, mae: 0.005292, mean_q: 0.024876
 60726/100000: episode: 6078, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.003341, mean_q: 0.021031
 60736/100000: episode: 6079, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000059, mae: 0.003373, mean_q: 0.020927
 60746/100000: episode: 6080, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000246, mae: 0.004908, mean_q: 0.023500
 60756/100000: episode: 6081, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000012, mae: 0.001786, mean_q: 0.021818
 60766/100000: episode: 6082, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000051, mae: 0.002483, mean_q: 0.021315
 60776/100000: episode: 6083, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000221, mae: 0.003927, mean_q: 0.023307
 60786/100000: episode: 6084, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000035, mae: 0.002434, mean_q: 0.022049
 60796/100000: episode: 6085, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000078, mae: 0.003539, mean_q: 0.021236
 60806/100000: episode: 6086, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000205, mae: 0.003503, mean_q: 0.022949
 60816/100000: episode: 6087, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000078, mae: 0.004026, mean_q: 0.023627
 60826/100000: episode: 6088, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000403, mae: 0.005198, mean_q: 0.024284
 60836/100000: episode: 6089, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000019, mae: 0.002598, mean_q: 0.022517
 60846/100000: episode: 6090, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000223, mae: 0.003782, mean_q: 0.021655
 60856/100000: episode: 6091, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000036, mae: 0.002736, mean_q: 0.023303
 60866/100000: episode: 6092, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000017, mae: 0.002877, mean_q: 0.020991
 60876/100000: episode: 6093, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002492, mean_q: 0.022125
 60886/100000: episode: 6094, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000239, mae: 0.005450, mean_q: 0.025131
 60896/100000: episode: 6095, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000074, mae: 0.003200, mean_q: 0.022722
 60906/100000: episode: 6096, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.003170, mean_q: 0.022722
 60916/100000: episode: 6097, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000041, mae: 0.002692, mean_q: 0.022920
 60926/100000: episode: 6098, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000207, mae: 0.003410, mean_q: 0.022138
 60936/100000: episode: 6099, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000054, mae: 0.003072, mean_q: 0.023767
 60946/100000: episode: 6100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000051, mae: 0.002366, mean_q: 0.022227
 60956/100000: episode: 6101, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000034, mae: 0.002361, mean_q: 0.022041
 60966/100000: episode: 6102, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000061, mae: 0.003247, mean_q: 0.021987
 60976/100000: episode: 6103, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000318, mae: 0.004929, mean_q: 0.022743
 60986/100000: episode: 6104, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000097, mae: 0.005506, mean_q: 0.025378
 60996/100000: episode: 6105, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000030, mae: 0.002011, mean_q: 0.022423
Step 61000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.61000.hdf5
 61006/100000: episode: 6106, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000225, mae: 0.003798, mean_q: 0.023272
 61016/100000: episode: 6107, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000613, mae: 0.006899, mean_q: 0.024290
 61026/100000: episode: 6108, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000062, mae: 0.005299, mean_q: 0.026212
 61036/100000: episode: 6109, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000204, mae: 0.003693, mean_q: 0.022468
 61046/100000: episode: 6110, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000077, mae: 0.003471, mean_q: 0.022246
 61056/100000: episode: 6111, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000211, mae: 0.004531, mean_q: 0.024527
 61066/100000: episode: 6112, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000224, mae: 0.003963, mean_q: 0.024069
 61076/100000: episode: 6113, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002596, mean_q: 0.022436
 61086/100000: episode: 6114, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000263, mae: 0.004642, mean_q: 0.023669
 61096/100000: episode: 6115, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.003323, mean_q: 0.024734
 61106/100000: episode: 6116, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000070, mae: 0.002474, mean_q: 0.023147
 61116/100000: episode: 6117, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000034, mae: 0.002520, mean_q: 0.022423
 61126/100000: episode: 6118, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000242, mae: 0.003986, mean_q: 0.023764
 61136/100000: episode: 6119, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000595, mae: 0.008061, mean_q: 0.026423
 61146/100000: episode: 6120, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000039, mae: 0.003256, mean_q: 0.024118
 61156/100000: episode: 6121, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000019, mae: 0.003991, mean_q: 0.021045
 61166/100000: episode: 6122, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000042, mae: 0.003244, mean_q: 0.023267
 61176/100000: episode: 6123, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000018, mae: 0.002468, mean_q: 0.022991
 61186/100000: episode: 6124, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000033, mae: 0.002953, mean_q: 0.021806
[Info] 1-TH LEVEL FOUND: 0.02453956939280033, Considering 100/100 traces
 61196/100000: episode: 6125, duration: 0.688s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.003014, mean_q: 0.024101
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02453956939280033
1
 61206/100000: episode: 6126, duration: 0.500s, episode steps: 10, steps per second: 20, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000010, mae: 0.001575, mean_q: 0.023692
 61216/100000: episode: 6127, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.002249, mean_q: 0.022425
 61226/100000: episode: 6128, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000017, mae: 0.002490, mean_q: 0.022212
 61236/100000: episode: 6129, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000207, mae: 0.003404, mean_q: 0.023160
 61246/100000: episode: 6130, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000034, mae: 0.002659, mean_q: 0.024182
 61256/100000: episode: 6131, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.002055, mean_q: 0.022539
 61266/100000: episode: 6132, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000261, mae: 0.004126, mean_q: 0.022769
 61276/100000: episode: 6133, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000036, mae: 0.002956, mean_q: 0.024466
 61286/100000: episode: 6134, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000201, mae: 0.002943, mean_q: 0.023036
 61296/100000: episode: 6135, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000247, mae: 0.004262, mean_q: 0.022763
 61306/100000: episode: 6136, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000070, mae: 0.002455, mean_q: 0.023462
 61316/100000: episode: 6137, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000035, mae: 0.002561, mean_q: 0.023874
 61326/100000: episode: 6138, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000098, mae: 0.004122, mean_q: 0.022191
 61336/100000: episode: 6139, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000076, mae: 0.003270, mean_q: 0.023697
 61346/100000: episode: 6140, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000108, mae: 0.004827, mean_q: 0.023605
 61356/100000: episode: 6141, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000229, mae: 0.003908, mean_q: 0.023339
 61366/100000: episode: 6142, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000053, mae: 0.002801, mean_q: 0.024098
 61376/100000: episode: 6143, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000038, mae: 0.002390, mean_q: 0.023676
 61386/100000: episode: 6144, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000205, mae: 0.003770, mean_q: 0.022250
 61396/100000: episode: 6145, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000222, mae: 0.003685, mean_q: 0.023653
 61406/100000: episode: 6146, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002770, mean_q: 0.023321
 61416/100000: episode: 6147, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000593, mae: 0.005944, mean_q: 0.022819
 61426/100000: episode: 6148, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000083, mae: 0.005462, mean_q: 0.025970
 61436/100000: episode: 6149, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000052, mae: 0.002624, mean_q: 0.022917
 61446/100000: episode: 6150, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000037, mae: 0.002924, mean_q: 0.022467
 61456/100000: episode: 6151, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000105, mae: 0.004125, mean_q: 0.023010
 61466/100000: episode: 6152, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000035, mae: 0.002103, mean_q: 0.023075
 61476/100000: episode: 6153, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000038, mae: 0.002702, mean_q: 0.022644
 61486/100000: episode: 6154, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000054, mae: 0.003064, mean_q: 0.022201
 61496/100000: episode: 6155, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000036, mae: 0.002280, mean_q: 0.022582
 61506/100000: episode: 6156, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000014, mae: 0.001957, mean_q: 0.022628
 61516/100000: episode: 6157, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000033, mae: 0.002804, mean_q: 0.021639
 61526/100000: episode: 6158, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000032, mae: 0.001950, mean_q: 0.022496
 61536/100000: episode: 6159, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000063, mae: 0.003058, mean_q: 0.022621
 61546/100000: episode: 6160, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002667, mean_q: 0.022469
 61556/100000: episode: 6161, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000079, mae: 0.003337, mean_q: 0.022216
 61566/100000: episode: 6162, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000262, mae: 0.005290, mean_q: 0.024594
 61576/100000: episode: 6163, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000264, mae: 0.004856, mean_q: 0.023637
 61586/100000: episode: 6164, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000040, mae: 0.002495, mean_q: 0.023023
 61596/100000: episode: 6165, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000010, mae: 0.001377, mean_q: 0.023098
 61606/100000: episode: 6166, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000016, mae: 0.002523, mean_q: 0.021937
 61616/100000: episode: 6167, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000219, mae: 0.003857, mean_q: 0.021465
 61626/100000: episode: 6168, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002707, mean_q: 0.023369
 61636/100000: episode: 6169, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000074, mae: 0.002672, mean_q: 0.022529
 61646/100000: episode: 6170, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000053, mae: 0.002182, mean_q: 0.022644
 61656/100000: episode: 6171, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000075, mae: 0.003027, mean_q: 0.022252
 61666/100000: episode: 6172, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000053, mae: 0.002273, mean_q: 0.022400
 61676/100000: episode: 6173, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000056, mae: 0.002562, mean_q: 0.022354
 61686/100000: episode: 6174, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000073, mae: 0.002850, mean_q: 0.023041
 61696/100000: episode: 6175, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000220, mae: 0.003455, mean_q: 0.023505
 61706/100000: episode: 6176, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000241, mae: 0.004199, mean_q: 0.023831
 61716/100000: episode: 6177, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000038, mae: 0.002858, mean_q: 0.022964
 61726/100000: episode: 6178, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000247, mae: 0.004651, mean_q: 0.021580
 61736/100000: episode: 6179, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000432, mae: 0.006673, mean_q: 0.025017
 61746/100000: episode: 6180, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000069, mae: 0.002528, mean_q: 0.023529
 61756/100000: episode: 6181, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.001982, mean_q: 0.022852
 61766/100000: episode: 6182, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000222, mae: 0.003191, mean_q: 0.022872
 61776/100000: episode: 6183, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000093, mae: 0.002947, mean_q: 0.023136
 61786/100000: episode: 6184, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000039, mae: 0.002671, mean_q: 0.022475
 61796/100000: episode: 6185, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000223, mae: 0.003633, mean_q: 0.022735
 61806/100000: episode: 6186, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000057, mae: 0.002520, mean_q: 0.022934
 61816/100000: episode: 6187, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000114, mae: 0.003786, mean_q: 0.023279
 61826/100000: episode: 6188, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000011, mae: 0.001834, mean_q: 0.023607
 61836/100000: episode: 6189, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.002429, mean_q: 0.022086
 61846/100000: episode: 6190, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000218, mae: 0.003506, mean_q: 0.023943
 61856/100000: episode: 6191, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000243, mae: 0.003894, mean_q: 0.023530
 61866/100000: episode: 6192, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000093, mae: 0.003650, mean_q: 0.024101
 61876/100000: episode: 6193, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.002735, mean_q: 0.023440
 61886/100000: episode: 6194, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000054, mae: 0.002575, mean_q: 0.022963
 61896/100000: episode: 6195, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000052, mae: 0.002339, mean_q: 0.022881
 61906/100000: episode: 6196, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000057, mae: 0.003190, mean_q: 0.022269
 61916/100000: episode: 6197, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000034, mae: 0.002567, mean_q: 0.022166
 61926/100000: episode: 6198, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000224, mae: 0.003448, mean_q: 0.023194
 61936/100000: episode: 6199, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000034, mae: 0.002088, mean_q: 0.023203
 61946/100000: episode: 6200, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000070, mae: 0.002627, mean_q: 0.023599
 61956/100000: episode: 6201, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000017, mae: 0.002129, mean_q: 0.023036
 61966/100000: episode: 6202, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000263, mae: 0.004327, mean_q: 0.022635
 61976/100000: episode: 6203, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002829, mean_q: 0.023938
 61986/100000: episode: 6204, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.002808, mean_q: 0.021736
 61996/100000: episode: 6205, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000201, mae: 0.003243, mean_q: 0.022086
Step 62000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.62000.hdf5
 62006/100000: episode: 6206, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000054, mae: 0.002811, mean_q: 0.023602
 62016/100000: episode: 6207, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000055, mae: 0.002880, mean_q: 0.022030
 62026/100000: episode: 6208, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000038, mae: 0.002747, mean_q: 0.022119
 62036/100000: episode: 6209, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000052, mae: 0.002419, mean_q: 0.022311
 62046/100000: episode: 6210, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000058, mae: 0.002630, mean_q: 0.022761
 62056/100000: episode: 6211, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000018, mae: 0.002528, mean_q: 0.021886
 62066/100000: episode: 6212, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.003121, mean_q: 0.021274
 62076/100000: episode: 6213, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000058, mae: 0.002742, mean_q: 0.022155
 62086/100000: episode: 6214, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000013, mae: 0.001839, mean_q: 0.021832
 62096/100000: episode: 6215, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002686, mean_q: 0.021366
 62106/100000: episode: 6216, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000049, mae: 0.002134, mean_q: 0.022612
 62116/100000: episode: 6217, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.001696, mean_q: 0.022011
 62126/100000: episode: 6218, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000089, mae: 0.002915, mean_q: 0.022741
 62136/100000: episode: 6219, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000223, mae: 0.004364, mean_q: 0.023736
 62146/100000: episode: 6220, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000265, mae: 0.004488, mean_q: 0.021551
 62156/100000: episode: 6221, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000014, mae: 0.001922, mean_q: 0.022820
 62166/100000: episode: 6222, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000074, mae: 0.003368, mean_q: 0.021220
 62176/100000: episode: 6223, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000204, mae: 0.003828, mean_q: 0.023551
 62186/100000: episode: 6224, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000054, mae: 0.002473, mean_q: 0.022036
 62196/100000: episode: 6225, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000006, mae: 0.001237, mean_q: 0.021787
[Info] 1-TH LEVEL FOUND: 0.02202560007572174, Considering 100/100 traces
 62206/100000: episode: 6226, duration: 0.641s, episode steps: 10, steps per second: 16, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002570, mean_q: 0.021955
[Info] 2-TH LEVEL FOUND: 0.02209797501564026, Considering 100/100 traces
 62216/100000: episode: 6227, duration: 0.751s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000079, mae: 0.003105, mean_q: 0.022354
[Info] 3-TH LEVEL FOUND: 0.022693399339914322, Considering 100/100 traces
 62226/100000: episode: 6228, duration: 0.702s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000075, mae: 0.002911, mean_q: 0.021772
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022693399339914322
3
 62236/100000: episode: 6229, duration: 0.504s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000021, mae: 0.002794, mean_q: 0.023158
 62246/100000: episode: 6230, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.002834, mean_q: 0.021239
 62256/100000: episode: 6231, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000093, mae: 0.003102, mean_q: 0.022153
 62266/100000: episode: 6232, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000031, mae: 0.002494, mean_q: 0.023364
 62276/100000: episode: 6233, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000246, mae: 0.004024, mean_q: 0.022579
 62286/100000: episode: 6234, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000072, mae: 0.002693, mean_q: 0.022111
 62296/100000: episode: 6235, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000015, mae: 0.001719, mean_q: 0.022115
 62306/100000: episode: 6236, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000032, mae: 0.002288, mean_q: 0.021480
 62316/100000: episode: 6237, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000060, mae: 0.003427, mean_q: 0.021302
 62326/100000: episode: 6238, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000036, mae: 0.002278, mean_q: 0.022338
 62336/100000: episode: 6239, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002488, mean_q: 0.021867
 62346/100000: episode: 6240, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000052, mae: 0.002167, mean_q: 0.021697
 62356/100000: episode: 6241, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000056, mae: 0.003056, mean_q: 0.023025
 62366/100000: episode: 6242, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000100, mae: 0.003874, mean_q: 0.022337
 62376/100000: episode: 6243, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000037, mae: 0.002703, mean_q: 0.021369
 62386/100000: episode: 6244, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000037, mae: 0.002280, mean_q: 0.021680
 62396/100000: episode: 6245, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000053, mae: 0.002276, mean_q: 0.022322
 62406/100000: episode: 6246, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000051, mae: 0.002155, mean_q: 0.022193
 62416/100000: episode: 6247, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000091, mae: 0.003110, mean_q: 0.022708
 62426/100000: episode: 6248, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002485, mean_q: 0.022444
 62436/100000: episode: 6249, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000034, mae: 0.002647, mean_q: 0.020985
 62446/100000: episode: 6250, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.002879, mean_q: 0.022341
 62456/100000: episode: 6251, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002433, mean_q: 0.022153
 62466/100000: episode: 6252, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000010, mae: 0.002706, mean_q: 0.020233
 62476/100000: episode: 6253, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000010, mae: 0.001900, mean_q: 0.020874
 62486/100000: episode: 6254, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000078, mae: 0.003005, mean_q: 0.021779
 62496/100000: episode: 6255, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000039, mae: 0.003025, mean_q: 0.020811
 62506/100000: episode: 6256, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000054, mae: 0.002593, mean_q: 0.021186
 62516/100000: episode: 6257, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000038, mae: 0.002453, mean_q: 0.021609
 62526/100000: episode: 6258, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000016, mae: 0.002982, mean_q: 0.020022
 62536/100000: episode: 6259, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000016, mae: 0.002119, mean_q: 0.020786
 62546/100000: episode: 6260, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.001820, mean_q: 0.021435
 62556/100000: episode: 6261, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000228, mae: 0.003586, mean_q: 0.021523
 62566/100000: episode: 6262, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000099, mae: 0.003574, mean_q: 0.021268
 62576/100000: episode: 6263, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000032, mae: 0.001819, mean_q: 0.021629
 62586/100000: episode: 6264, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002512, mean_q: 0.020896
 62596/100000: episode: 6265, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000055, mae: 0.002577, mean_q: 0.021703
 62606/100000: episode: 6266, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000035, mae: 0.002234, mean_q: 0.020994
 62616/100000: episode: 6267, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000055, mae: 0.002889, mean_q: 0.020409
 62626/100000: episode: 6268, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000228, mae: 0.003628, mean_q: 0.020983
 62636/100000: episode: 6269, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000112, mae: 0.004244, mean_q: 0.022695
 62646/100000: episode: 6270, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000056, mae: 0.003172, mean_q: 0.021917
 62656/100000: episode: 6271, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000034, mae: 0.002917, mean_q: 0.019863
 62666/100000: episode: 6272, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000057, mae: 0.002598, mean_q: 0.021293
 62676/100000: episode: 6273, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000019, mae: 0.001974, mean_q: 0.021069
 62686/100000: episode: 6274, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000014, mae: 0.002674, mean_q: 0.019943
 62696/100000: episode: 6275, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000012, mae: 0.002285, mean_q: 0.019689
 62706/100000: episode: 6276, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.003103, mean_q: 0.021919
 62716/100000: episode: 6277, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000055, mae: 0.002491, mean_q: 0.021121
 62726/100000: episode: 6278, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000035, mae: 0.002587, mean_q: 0.019976
 62736/100000: episode: 6279, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000058, mae: 0.003071, mean_q: 0.021557
 62746/100000: episode: 6280, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000037, mae: 0.002601, mean_q: 0.020310
 62756/100000: episode: 6281, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000241, mae: 0.004092, mean_q: 0.021950
 62766/100000: episode: 6282, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000016, mae: 0.002184, mean_q: 0.021453
 62776/100000: episode: 6283, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000244, mae: 0.003943, mean_q: 0.020815
 62786/100000: episode: 6284, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000038, mae: 0.004105, mean_q: 0.023643
 62796/100000: episode: 6285, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000198, mae: 0.002456, mean_q: 0.021055
 62806/100000: episode: 6286, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000052, mae: 0.002253, mean_q: 0.021494
 62816/100000: episode: 6287, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000028, mae: 0.001814, mean_q: 0.020594
 62826/100000: episode: 6288, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000012, mae: 0.002243, mean_q: 0.020141
 62836/100000: episode: 6289, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000032, mae: 0.002048, mean_q: 0.020516
 62846/100000: episode: 6290, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000033, mae: 0.001705, mean_q: 0.021037
 62856/100000: episode: 6291, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000016, mae: 0.002234, mean_q: 0.020301
 62866/100000: episode: 6292, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000032, mae: 0.001806, mean_q: 0.020659
 62876/100000: episode: 6293, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000054, mae: 0.002531, mean_q: 0.020479
 62886/100000: episode: 6294, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000088, mae: 0.002916, mean_q: 0.021515
 62896/100000: episode: 6295, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000039, mae: 0.002616, mean_q: 0.021186
 62906/100000: episode: 6296, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000032, mae: 0.002398, mean_q: 0.019975
 62916/100000: episode: 6297, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000026, mae: 0.001286, mean_q: 0.020776
 62926/100000: episode: 6298, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000016, mae: 0.002410, mean_q: 0.019943
 62936/100000: episode: 6299, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000056, mae: 0.002723, mean_q: 0.020359
 62946/100000: episode: 6300, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002017, mean_q: 0.021148
 62956/100000: episode: 6301, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000222, mae: 0.003068, mean_q: 0.020654
 62966/100000: episode: 6302, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000037, mae: 0.002366, mean_q: 0.020942
 62976/100000: episode: 6303, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000245, mae: 0.003923, mean_q: 0.021064
 62986/100000: episode: 6304, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000224, mae: 0.003285, mean_q: 0.021191
 62996/100000: episode: 6305, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000050, mae: 0.002651, mean_q: 0.021953
Step 63000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.63000.hdf5
 63006/100000: episode: 6306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000014, mae: 0.001842, mean_q: 0.020551
 63016/100000: episode: 6307, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000078, mae: 0.002991, mean_q: 0.020859
 63026/100000: episode: 6308, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.001961, mean_q: 0.020870
 63036/100000: episode: 6309, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000010, mae: 0.002510, mean_q: 0.019151
 63046/100000: episode: 6310, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002674, mean_q: 0.020884
 63056/100000: episode: 6311, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000014, mae: 0.001914, mean_q: 0.020361
 63066/100000: episode: 6312, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000079, mae: 0.003413, mean_q: 0.019977
 63076/100000: episode: 6313, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000070, mae: 0.002248, mean_q: 0.020808
 63086/100000: episode: 6314, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000029, mae: 0.001595, mean_q: 0.020958
 63096/100000: episode: 6315, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000077, mae: 0.002857, mean_q: 0.020641
 63106/100000: episode: 6316, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000081, mae: 0.003375, mean_q: 0.020468
 63116/100000: episode: 6317, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.001528, mean_q: 0.020704
 63126/100000: episode: 6318, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000035, mae: 0.002362, mean_q: 0.020089
 63136/100000: episode: 6319, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000079, mae: 0.002971, mean_q: 0.020343
 63146/100000: episode: 6320, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000052, mae: 0.002611, mean_q: 0.021367
 63156/100000: episode: 6321, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000053, mae: 0.002352, mean_q: 0.020302
 63166/100000: episode: 6322, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000055, mae: 0.002568, mean_q: 0.020827
 63176/100000: episode: 6323, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000079, mae: 0.003353, mean_q: 0.020083
 63186/100000: episode: 6324, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000039, mae: 0.002518, mean_q: 0.019975
 63196/100000: episode: 6325, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.001724, mean_q: 0.020649
 63206/100000: episode: 6326, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000059, mae: 0.002943, mean_q: 0.020095
 63216/100000: episode: 6327, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000260, mae: 0.003471, mean_q: 0.020577
 63226/100000: episode: 6328, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000012, mae: 0.002219, mean_q: 0.021661
[Info] 1-TH LEVEL FOUND: 0.02045874483883381, Considering 100/100 traces
 63236/100000: episode: 6329, duration: 0.700s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000058, mae: 0.002804, mean_q: 0.020028
[Info] 2-TH LEVEL FOUND: 0.021451016888022423, Considering 100/100 traces
 63246/100000: episode: 6330, duration: 0.680s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000056, mae: 0.002335, mean_q: 0.020677
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021451016888022423
2
 63256/100000: episode: 6331, duration: 0.514s, episode steps: 10, steps per second: 19, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000050, mae: 0.002407, mean_q: 0.021389
 63266/100000: episode: 6332, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000230, mae: 0.003744, mean_q: 0.020403
 63276/100000: episode: 6333, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.003109, mean_q: 0.021942
 63286/100000: episode: 6334, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000059, mae: 0.002848, mean_q: 0.020364
 63296/100000: episode: 6335, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002306, mean_q: 0.020716
 63306/100000: episode: 6336, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000016, mae: 0.002148, mean_q: 0.021329
 63316/100000: episode: 6337, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000036, mae: 0.002602, mean_q: 0.019918
 63326/100000: episode: 6338, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000014, mae: 0.002411, mean_q: 0.019579
 63336/100000: episode: 6339, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000015, mae: 0.002046, mean_q: 0.019887
 63346/100000: episode: 6340, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000034, mae: 0.001974, mean_q: 0.020238
 63356/100000: episode: 6341, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000033, mae: 0.001862, mean_q: 0.020261
 63366/100000: episode: 6342, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.003052, mean_q: 0.019602
 63376/100000: episode: 6343, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000057, mae: 0.002817, mean_q: 0.019810
 63386/100000: episode: 6344, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000038, mae: 0.002583, mean_q: 0.020740
 63396/100000: episode: 6345, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000040, mae: 0.002814, mean_q: 0.019387
 63406/100000: episode: 6346, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000010, mae: 0.001162, mean_q: 0.020152
 63416/100000: episode: 6347, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000038, mae: 0.002740, mean_q: 0.019325
 63426/100000: episode: 6348, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000036, mae: 0.002157, mean_q: 0.019963
 63436/100000: episode: 6349, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000063, mae: 0.002830, mean_q: 0.020075
 63446/100000: episode: 6350, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000016, mae: 0.001668, mean_q: 0.020212
 63456/100000: episode: 6351, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000012, mae: 0.001970, mean_q: 0.019308
 63466/100000: episode: 6352, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000057, mae: 0.002706, mean_q: 0.019544
 63476/100000: episode: 6353, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000036, mae: 0.002239, mean_q: 0.020507
 63486/100000: episode: 6354, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000015, mae: 0.001870, mean_q: 0.019574
 63496/100000: episode: 6355, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000219, mae: 0.002798, mean_q: 0.019567
 63506/100000: episode: 6356, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000034, mae: 0.003089, mean_q: 0.021599
 63516/100000: episode: 6357, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000040, mae: 0.002887, mean_q: 0.019290
 63526/100000: episode: 6358, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000077, mae: 0.002986, mean_q: 0.019694
 63536/100000: episode: 6359, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000014, mae: 0.001681, mean_q: 0.019871
 63546/100000: episode: 6360, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000030, mae: 0.001831, mean_q: 0.019508
 63556/100000: episode: 6361, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000056, mae: 0.003191, mean_q: 0.018763
 63566/100000: episode: 6362, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000080, mae: 0.003538, mean_q: 0.020752
 63576/100000: episode: 6363, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000073, mae: 0.002915, mean_q: 0.019703
 63586/100000: episode: 6364, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.002416, mean_q: 0.019463
 63596/100000: episode: 6365, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000032, mae: 0.001954, mean_q: 0.020349
 63606/100000: episode: 6366, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000041, mae: 0.002576, mean_q: 0.019470
 63616/100000: episode: 6367, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000011, mae: 0.001790, mean_q: 0.019176
 63626/100000: episode: 6368, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000033, mae: 0.001980, mean_q: 0.019999
 63636/100000: episode: 6369, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000054, mae: 0.002474, mean_q: 0.019250
 63646/100000: episode: 6370, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000202, mae: 0.003044, mean_q: 0.020456
 63656/100000: episode: 6371, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000016, mae: 0.001868, mean_q: 0.019781
 63666/100000: episode: 6372, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000050, mae: 0.002491, mean_q: 0.018813
 63676/100000: episode: 6373, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000060, mae: 0.002835, mean_q: 0.019289
 63686/100000: episode: 6374, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000031, mae: 0.001651, mean_q: 0.019869
[Info] FALSIFICATION!
 63696/100000: episode: 6375, duration: 0.411s, episode steps: 10, steps per second: 24, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000056, mae: 0.002437, mean_q: 0.020220
 63706/100000: episode: 6376, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000042, mae: 0.002938, mean_q: 0.020373
 63716/100000: episode: 6377, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.002624, mean_q: 0.018804
 63726/100000: episode: 6378, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000078, mae: 0.003005, mean_q: 0.020210
 63736/100000: episode: 6379, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000032, mae: 0.002485, mean_q: 0.020945
 63746/100000: episode: 6380, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000058, mae: 0.002625, mean_q: 0.019325
 63756/100000: episode: 6381, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000054, mae: 0.002358, mean_q: 0.020152
 63766/100000: episode: 6382, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.001766, mae: 0.007723, mean_q: 0.020973
 63776/100000: episode: 6383, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000046, mae: 0.005753, mean_q: 0.024205
 63786/100000: episode: 6384, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000081, mae: 0.004875, mean_q: 0.017370
 63796/100000: episode: 6385, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000076, mae: 0.003371, mean_q: 0.020952
 63806/100000: episode: 6386, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000410, mae: 0.005926, mean_q: 0.022557
 63816/100000: episode: 6387, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000031, mae: 0.002191, mean_q: 0.020240
 63826/100000: episode: 6388, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000312, mae: 0.005933, mean_q: 0.018759
 63836/100000: episode: 6389, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000244, mae: 0.005272, mean_q: 0.022462
 63846/100000: episode: 6390, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000064, mae: 0.004049, mean_q: 0.021765
 63856/100000: episode: 6391, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000077, mae: 0.003829, mean_q: 0.019098
 63866/100000: episode: 6392, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000265, mae: 0.004335, mean_q: 0.020929
 63876/100000: episode: 6393, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000035, mae: 0.003262, mean_q: 0.022190
 63886/100000: episode: 6394, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000204, mae: 0.003900, mean_q: 0.018831
 63896/100000: episode: 6395, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000220, mae: 0.003428, mean_q: 0.021394
 63906/100000: episode: 6396, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000015, mae: 0.002128, mean_q: 0.021125
 63916/100000: episode: 6397, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000078, mae: 0.003350, mean_q: 0.019732
 63926/100000: episode: 6398, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000057, mae: 0.002697, mean_q: 0.020916
 63936/100000: episode: 6399, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000074, mae: 0.002492, mean_q: 0.020473
 63946/100000: episode: 6400, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000035, mae: 0.002710, mean_q: 0.021571
 63956/100000: episode: 6401, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000054, mae: 0.002440, mean_q: 0.020163
 63966/100000: episode: 6402, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000033, mae: 0.002171, mean_q: 0.019932
 63976/100000: episode: 6403, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000202, mae: 0.002812, mean_q: 0.020593
 63986/100000: episode: 6404, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000057, mae: 0.002529, mean_q: 0.020748
 63996/100000: episode: 6405, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000030, mae: 0.001486, mean_q: 0.020319
Step 64000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.64000.hdf5
 64006/100000: episode: 6406, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000039, mae: 0.002412, mean_q: 0.020748
 64016/100000: episode: 6407, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002241, mean_q: 0.020024
 64026/100000: episode: 6408, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000032, mae: 0.002109, mean_q: 0.021102
 64036/100000: episode: 6409, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.001979, mean_q: 0.019947
 64046/100000: episode: 6410, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000073, mae: 0.003051, mean_q: 0.019458
 64056/100000: episode: 6411, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000098, mae: 0.004257, mean_q: 0.021755
 64066/100000: episode: 6412, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000245, mae: 0.003883, mean_q: 0.020867
 64076/100000: episode: 6413, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000073, mae: 0.002478, mean_q: 0.020357
 64086/100000: episode: 6414, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002191, mean_q: 0.020700
 64096/100000: episode: 6415, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000055, mae: 0.002752, mean_q: 0.019874
 64106/100000: episode: 6416, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000052, mae: 0.002241, mean_q: 0.020419
 64116/100000: episode: 6417, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002448, mean_q: 0.021332
 64126/100000: episode: 6418, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000224, mae: 0.003126, mean_q: 0.020755
 64136/100000: episode: 6419, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000285, mae: 0.005390, mean_q: 0.022059
 64146/100000: episode: 6420, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000013, mae: 0.002396, mean_q: 0.021893
 64156/100000: episode: 6421, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000015, mae: 0.002939, mean_q: 0.019102
 64166/100000: episode: 6422, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000031, mae: 0.002333, mean_q: 0.019804
 64176/100000: episode: 6423, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000052, mae: 0.002162, mean_q: 0.020278
 64186/100000: episode: 6424, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000013, mae: 0.001603, mean_q: 0.020235
 64196/100000: episode: 6425, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000035, mae: 0.002658, mean_q: 0.019461
 64206/100000: episode: 6426, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000080, mae: 0.003559, mean_q: 0.020999
 64216/100000: episode: 6427, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000013, mae: 0.002000, mean_q: 0.020469
 64226/100000: episode: 6428, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000033, mae: 0.003074, mean_q: 0.018659
 64236/100000: episode: 6429, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000011, mae: 0.001563, mean_q: 0.020610
 64246/100000: episode: 6430, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000033, mae: 0.002032, mean_q: 0.019784
[Info] Complete ISplit Iteration
[Info] Levels: [0.021507949]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 64256/100000: episode: 6431, duration: 0.796s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000096, mae: 0.003139, mean_q: 0.020347
 64266/100000: episode: 6432, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002230, mean_q: 0.021061
 64276/100000: episode: 6433, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000013, mae: 0.001819, mean_q: 0.019824
 64286/100000: episode: 6434, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000035, mae: 0.002105, mean_q: 0.019904
 64296/100000: episode: 6435, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.001534, mae: 0.005103, mean_q: 0.020478
 64306/100000: episode: 6436, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000018, mae: 0.003588, mean_q: 0.022687
 64316/100000: episode: 6437, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000055, mae: 0.003218, mean_q: 0.019163
 64326/100000: episode: 6438, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000030, mae: 0.002328, mean_q: 0.019204
 64336/100000: episode: 6439, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000037, mae: 0.002578, mean_q: 0.019676
 64346/100000: episode: 6440, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000201, mae: 0.002946, mean_q: 0.019404
 64356/100000: episode: 6441, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000269, mae: 0.005042, mean_q: 0.021313
 64366/100000: episode: 6442, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000040, mae: 0.003315, mean_q: 0.021111
 64376/100000: episode: 6443, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.003434, mean_q: 0.018854
 64386/100000: episode: 6444, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.002552, mean_q: 0.019518
 64396/100000: episode: 6445, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000052, mae: 0.002029, mean_q: 0.020233
 64406/100000: episode: 6446, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.001536, mae: 0.007620, mean_q: 0.023378
 64416/100000: episode: 6447, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000010, mae: 0.002355, mean_q: 0.020216
 64426/100000: episode: 6448, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.003560, mean_q: 0.018363
 64436/100000: episode: 6449, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000036, mae: 0.003270, mean_q: 0.021904
 64446/100000: episode: 6450, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000222, mae: 0.003419, mean_q: 0.019395
 64456/100000: episode: 6451, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000014, mae: 0.002041, mean_q: 0.020849
 64466/100000: episode: 6452, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000220, mae: 0.003158, mean_q: 0.019629
 64476/100000: episode: 6453, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000038, mae: 0.002418, mean_q: 0.020460
 64486/100000: episode: 6454, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000012, mae: 0.002119, mean_q: 0.019208
 64496/100000: episode: 6455, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000027, mae: 0.001739, mean_q: 0.019464
 64506/100000: episode: 6456, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.001684, mean_q: 0.019629
 64516/100000: episode: 6457, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000031, mae: 0.001742, mean_q: 0.019731
 64526/100000: episode: 6458, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002243, mean_q: 0.019331
 64536/100000: episode: 6459, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000011, mae: 0.001599, mean_q: 0.019504
 64546/100000: episode: 6460, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000246, mae: 0.003872, mean_q: 0.020026
 64556/100000: episode: 6461, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000094, mae: 0.003919, mean_q: 0.021372
 64566/100000: episode: 6462, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000009, mae: 0.001391, mean_q: 0.019819
 64576/100000: episode: 6463, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000016, mae: 0.001820, mean_q: 0.019481
 64586/100000: episode: 6464, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002149, mean_q: 0.019858
 64596/100000: episode: 6465, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001532, mae: 0.004800, mean_q: 0.019670
 64606/100000: episode: 6466, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000041, mae: 0.004439, mean_q: 0.022670
 64616/100000: episode: 6467, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000093, mae: 0.002817, mean_q: 0.020469
 64626/100000: episode: 6468, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000053, mae: 0.002834, mean_q: 0.021112
 64636/100000: episode: 6469, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000046, mae: 0.001736, mean_q: 0.020674
 64646/100000: episode: 6470, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000244, mae: 0.003450, mean_q: 0.020564
 64656/100000: episode: 6471, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000010, mae: 0.001864, mean_q: 0.021228
 64666/100000: episode: 6472, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000030, mae: 0.002596, mean_q: 0.019017
 64676/100000: episode: 6473, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.002946, mean_q: 0.019415
 64686/100000: episode: 6474, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000117, mae: 0.003850, mean_q: 0.021070
 64696/100000: episode: 6475, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000039, mae: 0.002997, mean_q: 0.021369
 64706/100000: episode: 6476, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000080, mae: 0.003225, mean_q: 0.020075
 64716/100000: episode: 6477, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000095, mae: 0.002940, mean_q: 0.020360
 64726/100000: episode: 6478, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000057, mae: 0.004103, mean_q: 0.022669
 64736/100000: episode: 6479, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000034, mae: 0.002478, mean_q: 0.020163
 64746/100000: episode: 6480, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000241, mae: 0.003694, mean_q: 0.020025
 64756/100000: episode: 6481, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000075, mae: 0.004212, mean_q: 0.022640
 64766/100000: episode: 6482, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000095, mae: 0.003149, mean_q: 0.020579
 64776/100000: episode: 6483, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000073, mae: 0.002484, mean_q: 0.020796
 64786/100000: episode: 6484, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000038, mae: 0.002450, mean_q: 0.020832
 64796/100000: episode: 6485, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000052, mae: 0.003057, mean_q: 0.019355
 64806/100000: episode: 6486, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.002600, mean_q: 0.020463
 64816/100000: episode: 6487, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.001510, mae: 0.004593, mean_q: 0.021109
 64826/100000: episode: 6488, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000058, mae: 0.004508, mean_q: 0.023284
 64836/100000: episode: 6489, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000097, mae: 0.003560, mean_q: 0.021412
 64846/100000: episode: 6490, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000077, mae: 0.003149, mean_q: 0.021460
 64856/100000: episode: 6491, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000011, mae: 0.001955, mean_q: 0.020353
 64866/100000: episode: 6492, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.003099, mean_q: 0.019652
 64876/100000: episode: 6493, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000032, mae: 0.001924, mean_q: 0.021193
 64886/100000: episode: 6494, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002533, mean_q: 0.020398
 64896/100000: episode: 6495, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000075, mae: 0.002722, mean_q: 0.020812
 64906/100000: episode: 6496, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000078, mae: 0.002924, mean_q: 0.021021
 64916/100000: episode: 6497, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000074, mae: 0.002955, mean_q: 0.021531
 64926/100000: episode: 6498, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000035, mae: 0.002486, mean_q: 0.020437
 64936/100000: episode: 6499, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000053, mae: 0.002423, mean_q: 0.020618
 64946/100000: episode: 6500, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000073, mae: 0.002539, mean_q: 0.021110
 64956/100000: episode: 6501, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000080, mae: 0.002991, mean_q: 0.020848
 64966/100000: episode: 6502, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000032, mae: 0.001928, mean_q: 0.020529
 64976/100000: episode: 6503, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000039, mae: 0.002622, mean_q: 0.020319
 64986/100000: episode: 6504, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000036, mae: 0.002235, mean_q: 0.020400
 64996/100000: episode: 6505, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000018, mae: 0.001922, mean_q: 0.020646
Step 65000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.65000.hdf5
 65006/100000: episode: 6506, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000029, mae: 0.001933, mean_q: 0.019964
 65016/100000: episode: 6507, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.001547, mean_q: 0.020597
 65026/100000: episode: 6508, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000012, mae: 0.001923, mean_q: 0.019770
 65036/100000: episode: 6509, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000033, mae: 0.002103, mean_q: 0.019998
 65046/100000: episode: 6510, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000010, mae: 0.001717, mean_q: 0.019875
 65056/100000: episode: 6511, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000035, mae: 0.002358, mean_q: 0.019646
 65066/100000: episode: 6512, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000010, mae: 0.001178, mean_q: 0.020324
 65076/100000: episode: 6513, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000011, mae: 0.002199, mean_q: 0.019044
 65086/100000: episode: 6514, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000043, mae: 0.003083, mean_q: 0.019432
 65096/100000: episode: 6515, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000074, mae: 0.003010, mean_q: 0.021009
 65106/100000: episode: 6516, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002290, mean_q: 0.020440
 65116/100000: episode: 6517, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002496, mean_q: 0.019840
 65126/100000: episode: 6518, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000072, mae: 0.002513, mean_q: 0.020398
 65136/100000: episode: 6519, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000096, mae: 0.003143, mean_q: 0.020528
 65146/100000: episode: 6520, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000032, mae: 0.002168, mean_q: 0.020932
 65156/100000: episode: 6521, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000009, mae: 0.001604, mean_q: 0.019519
 65166/100000: episode: 6522, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000036, mae: 0.002723, mean_q: 0.019276
 65176/100000: episode: 6523, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.002509, mean_q: 0.019582
 65186/100000: episode: 6524, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000056, mae: 0.002504, mean_q: 0.020338
 65196/100000: episode: 6525, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000038, mae: 0.002776, mean_q: 0.021018
 65206/100000: episode: 6526, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000017, mae: 0.002933, mean_q: 0.018718
 65216/100000: episode: 6527, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000034, mae: 0.002401, mean_q: 0.019065
 65226/100000: episode: 6528, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000055, mae: 0.003065, mean_q: 0.021116
 65236/100000: episode: 6529, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.002274, mean_q: 0.019604
 65246/100000: episode: 6530, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000055, mae: 0.002985, mean_q: 0.018868
[Info] 1-TH LEVEL FOUND: 0.02100057527422905, Considering 100/100 traces
 65256/100000: episode: 6531, duration: 0.790s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000097, mae: 0.003445, mean_q: 0.020670
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02100057527422905
1
 65266/100000: episode: 6532, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000057, mae: 0.002777, mean_q: 0.020671
 65276/100000: episode: 6533, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001571, mae: 0.006674, mean_q: 0.021910
 65286/100000: episode: 6534, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000015, mae: 0.003237, mean_q: 0.022388
 65296/100000: episode: 6535, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000075, mae: 0.003524, mean_q: 0.018875
 65306/100000: episode: 6536, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000029, mae: 0.002183, mean_q: 0.021281
 65316/100000: episode: 6537, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000227, mae: 0.003359, mean_q: 0.020296
 65326/100000: episode: 6538, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.001891, mean_q: 0.021010
 65336/100000: episode: 6539, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.001537, mae: 0.005711, mean_q: 0.020594
 65346/100000: episode: 6540, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000223, mae: 0.004690, mean_q: 0.022534
 65356/100000: episode: 6541, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000220, mae: 0.003113, mean_q: 0.020352
 65366/100000: episode: 6542, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002514, mean_q: 0.020114
 65376/100000: episode: 6543, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000075, mae: 0.003021, mean_q: 0.021182
 65386/100000: episode: 6544, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002780, mean_q: 0.021399
 65396/100000: episode: 6545, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000031, mae: 0.002462, mean_q: 0.019618
 65406/100000: episode: 6546, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000030, mae: 0.002211, mean_q: 0.019595
 65416/100000: episode: 6547, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000058, mae: 0.002665, mean_q: 0.020692
 65426/100000: episode: 6548, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000030, mae: 0.001726, mean_q: 0.020090
 65436/100000: episode: 6549, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000248, mae: 0.004146, mean_q: 0.020996
 65446/100000: episode: 6550, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000012, mae: 0.001893, mean_q: 0.021137
 65456/100000: episode: 6551, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000033, mae: 0.002702, mean_q: 0.019327
 65466/100000: episode: 6552, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000246, mae: 0.003833, mean_q: 0.020336
 65476/100000: episode: 6553, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000029, mae: 0.002194, mean_q: 0.021539
 65486/100000: episode: 6554, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000028, mae: 0.001544, mean_q: 0.020152
 65496/100000: episode: 6555, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000072, mae: 0.002539, mean_q: 0.020583
 65506/100000: episode: 6556, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000012, mae: 0.001416, mean_q: 0.020727
 65516/100000: episode: 6557, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000033, mae: 0.002060, mean_q: 0.020033
 65526/100000: episode: 6558, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000074, mae: 0.002656, mean_q: 0.020483
 65536/100000: episode: 6559, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000077, mae: 0.003180, mean_q: 0.021064
 65546/100000: episode: 6560, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000098, mae: 0.003480, mean_q: 0.019890
 65556/100000: episode: 6561, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000052, mae: 0.002306, mean_q: 0.020957
 65566/100000: episode: 6562, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000078, mae: 0.003019, mean_q: 0.020357
 65576/100000: episode: 6563, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.001990, mean_q: 0.020435
 65586/100000: episode: 6564, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000052, mae: 0.002507, mean_q: 0.019873
 65596/100000: episode: 6565, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000238, mae: 0.003603, mean_q: 0.021243
 65606/100000: episode: 6566, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000033, mae: 0.002425, mean_q: 0.020669
 65616/100000: episode: 6567, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000052, mae: 0.002677, mean_q: 0.019368
 65626/100000: episode: 6568, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000030, mae: 0.001575, mean_q: 0.020721
 65636/100000: episode: 6569, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000034, mae: 0.002023, mean_q: 0.020223
 65646/100000: episode: 6570, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000076, mae: 0.002962, mean_q: 0.019772
 65656/100000: episode: 6571, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000074, mae: 0.003443, mean_q: 0.021602
 65666/100000: episode: 6572, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000048, mae: 0.002008, mean_q: 0.020960
 65676/100000: episode: 6573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000077, mae: 0.002924, mean_q: 0.020180
 65686/100000: episode: 6574, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000015, mae: 0.001900, mean_q: 0.020926
 65696/100000: episode: 6575, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000108, mae: 0.002834, mean_q: 0.020615
 65706/100000: episode: 6576, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000072, mae: 0.003114, mean_q: 0.021597
 65716/100000: episode: 6577, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000077, mae: 0.003104, mean_q: 0.021195
 65726/100000: episode: 6578, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000093, mae: 0.003080, mean_q: 0.020088
 65736/100000: episode: 6579, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000014, mae: 0.001958, mean_q: 0.021069
 65746/100000: episode: 6580, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.003255, mean_q: 0.019593
 65756/100000: episode: 6581, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000049, mae: 0.001977, mean_q: 0.020604
 65766/100000: episode: 6582, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000094, mae: 0.003109, mean_q: 0.021170
 65776/100000: episode: 6583, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002433, mean_q: 0.020725
 65786/100000: episode: 6584, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000061, mae: 0.002894, mean_q: 0.020935
 65796/100000: episode: 6585, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000031, mae: 0.001912, mean_q: 0.021259
 65806/100000: episode: 6586, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000029, mae: 0.002027, mean_q: 0.019882
 65816/100000: episode: 6587, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000033, mae: 0.002596, mean_q: 0.019377
 65826/100000: episode: 6588, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002193, mean_q: 0.020262
 65836/100000: episode: 6589, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000017, mae: 0.001932, mean_q: 0.020060
 65846/100000: episode: 6590, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000048, mae: 0.001643, mean_q: 0.020593
 65856/100000: episode: 6591, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000074, mae: 0.002694, mean_q: 0.020086
 65866/100000: episode: 6592, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000115, mae: 0.004295, mean_q: 0.021760
 65876/100000: episode: 6593, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000076, mae: 0.003564, mean_q: 0.021157
 65886/100000: episode: 6594, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000032, mae: 0.002162, mean_q: 0.019648
 65896/100000: episode: 6595, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000036, mae: 0.002206, mean_q: 0.020893
 65906/100000: episode: 6596, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.001938, mean_q: 0.020077
 65916/100000: episode: 6597, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000052, mae: 0.002308, mean_q: 0.020113
 65926/100000: episode: 6598, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000032, mae: 0.002049, mean_q: 0.019887
 65936/100000: episode: 6599, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.002148, mean_q: 0.019837
 65946/100000: episode: 6600, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000271, mae: 0.004685, mean_q: 0.020864
 65956/100000: episode: 6601, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.002456, mean_q: 0.021671
 65966/100000: episode: 6602, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000203, mae: 0.003157, mean_q: 0.019628
 65976/100000: episode: 6603, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000094, mae: 0.003660, mean_q: 0.021475
 65986/100000: episode: 6604, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000034, mae: 0.002359, mean_q: 0.020658
 65996/100000: episode: 6605, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000054, mae: 0.003101, mean_q: 0.019162
Step 66000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.66000.hdf5
 66006/100000: episode: 6606, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000035, mae: 0.002077, mean_q: 0.020604
 66016/100000: episode: 6607, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000011, mae: 0.001565, mean_q: 0.020086
 66026/100000: episode: 6608, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000012, mae: 0.002445, mean_q: 0.019067
 66036/100000: episode: 6609, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000053, mae: 0.002821, mean_q: 0.019191
 66046/100000: episode: 6610, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000031, mae: 0.001561, mean_q: 0.020163
 66056/100000: episode: 6611, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000030, mae: 0.001442, mean_q: 0.019929
 66066/100000: episode: 6612, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000006, mae: 0.000836, mean_q: 0.019968
 66076/100000: episode: 6613, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000090, mae: 0.002620, mean_q: 0.019730
 66086/100000: episode: 6614, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000093, mae: 0.002701, mean_q: 0.020147
 66096/100000: episode: 6615, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000016, mae: 0.001975, mean_q: 0.020657
 66106/100000: episode: 6616, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000024, mae: 0.003311, mean_q: 0.018855
 66116/100000: episode: 6617, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000055, mae: 0.002687, mean_q: 0.019180
 66126/100000: episode: 6618, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000038, mae: 0.002608, mean_q: 0.020233
 66136/100000: episode: 6619, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000073, mae: 0.002379, mean_q: 0.019706
 66146/100000: episode: 6620, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002172, mean_q: 0.020467
 66156/100000: episode: 6621, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002178, mean_q: 0.019287
 66166/100000: episode: 6622, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000225, mae: 0.003459, mean_q: 0.019424
 66176/100000: episode: 6623, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000037, mae: 0.002915, mean_q: 0.021033
 66186/100000: episode: 6624, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001554, mae: 0.005412, mean_q: 0.019188
 66196/100000: episode: 6625, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000044, mae: 0.005029, mean_q: 0.023130
 66206/100000: episode: 6626, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000031, mae: 0.002190, mean_q: 0.019978
 66216/100000: episode: 6627, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001566, mae: 0.007452, mean_q: 0.021337
 66226/100000: episode: 6628, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000293, mae: 0.007196, mean_q: 0.023377
 66236/100000: episode: 6629, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000060, mae: 0.003001, mean_q: 0.021130
 66246/100000: episode: 6630, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002187, mean_q: 0.020146
 66256/100000: episode: 6631, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000035, mae: 0.001951, mean_q: 0.020544
[Info] 1-TH LEVEL FOUND: 0.01989760249853134, Considering 100/100 traces
 66266/100000: episode: 6632, duration: 0.808s, episode steps: 10, steps per second: 12, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000031, mae: 0.002144, mean_q: 0.019768
[Info] 2-TH LEVEL FOUND: 0.020609542727470398, Considering 100/100 traces
 66276/100000: episode: 6633, duration: 0.715s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000032, mae: 0.001850, mean_q: 0.020145
[Info] 3-TH LEVEL FOUND: 0.021246183663606644, Considering 100/100 traces
 66286/100000: episode: 6634, duration: 0.789s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000078, mae: 0.003385, mean_q: 0.021281
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021246183663606644
3
 66296/100000: episode: 6635, duration: 0.494s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000013, mae: 0.001971, mean_q: 0.020378
 66306/100000: episode: 6636, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000060, mae: 0.003836, mean_q: 0.018869
 66316/100000: episode: 6637, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000036, mae: 0.002207, mean_q: 0.020084
 66326/100000: episode: 6638, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.002276, mean_q: 0.019791
 66336/100000: episode: 6639, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000048, mae: 0.001818, mean_q: 0.019968
 66346/100000: episode: 6640, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.001576, mae: 0.006371, mean_q: 0.020945
 66356/100000: episode: 6641, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000202, mae: 0.005308, mean_q: 0.023809
 66366/100000: episode: 6642, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000012, mae: 0.002459, mean_q: 0.019833
 66376/100000: episode: 6643, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000012, mae: 0.002914, mean_q: 0.018594
 66386/100000: episode: 6644, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000036, mae: 0.002182, mean_q: 0.020425
 66396/100000: episode: 6645, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000219, mae: 0.003481, mean_q: 0.021593
 66406/100000: episode: 6646, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000053, mae: 0.002504, mean_q: 0.020318
 66416/100000: episode: 6647, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000033, mae: 0.002054, mean_q: 0.019964
 66426/100000: episode: 6648, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000220, mae: 0.003243, mean_q: 0.021263
 66436/100000: episode: 6649, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002127, mean_q: 0.020394
 66446/100000: episode: 6650, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000205, mae: 0.003645, mean_q: 0.019198
 66456/100000: episode: 6651, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000017, mae: 0.002237, mean_q: 0.021200
 66466/100000: episode: 6652, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000062, mae: 0.003144, mean_q: 0.020746
 66476/100000: episode: 6653, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.001512, mae: 0.004850, mean_q: 0.019584
 66486/100000: episode: 6654, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000203, mae: 0.005271, mean_q: 0.023720
 66496/100000: episode: 6655, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000016, mae: 0.002913, mean_q: 0.020700
 66506/100000: episode: 6656, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000043, mae: 0.004511, mean_q: 0.017876
 66516/100000: episode: 6657, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000104, mae: 0.004213, mean_q: 0.020786
 66526/100000: episode: 6658, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000018, mae: 0.002485, mean_q: 0.021062
 66536/100000: episode: 6659, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000036, mae: 0.003131, mean_q: 0.018965
 66546/100000: episode: 6660, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000453, mae: 0.005162, mean_q: 0.020165
 66556/100000: episode: 6661, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000250, mae: 0.006291, mean_q: 0.023296
 66566/100000: episode: 6662, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000015, mae: 0.002183, mean_q: 0.020634
 66576/100000: episode: 6663, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000233, mae: 0.004508, mean_q: 0.021158
 66586/100000: episode: 6664, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.002116, mean_q: 0.020543
 66596/100000: episode: 6665, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000011, mae: 0.002650, mean_q: 0.018881
 66606/100000: episode: 6666, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000037, mae: 0.002585, mean_q: 0.019806
 66616/100000: episode: 6667, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000093, mae: 0.002889, mean_q: 0.020036
 66626/100000: episode: 6668, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000037, mae: 0.002684, mean_q: 0.021201
 66636/100000: episode: 6669, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000014, mae: 0.002019, mean_q: 0.019868
 66646/100000: episode: 6670, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000010, mae: 0.002084, mean_q: 0.019339
 66656/100000: episode: 6671, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000036, mae: 0.002124, mean_q: 0.019889
 66666/100000: episode: 6672, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.001927, mean_q: 0.019992
 66676/100000: episode: 6673, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000055, mae: 0.002442, mean_q: 0.019767
 66686/100000: episode: 6674, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000049, mae: 0.001902, mean_q: 0.020436
 66696/100000: episode: 6675, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002302, mean_q: 0.019740
 66706/100000: episode: 6676, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000035, mae: 0.002825, mean_q: 0.018930
 66716/100000: episode: 6677, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000032, mae: 0.001722, mean_q: 0.019778
 66726/100000: episode: 6678, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000051, mae: 0.002103, mean_q: 0.019672
 66736/100000: episode: 6679, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000011, mae: 0.001594, mean_q: 0.019527
 66746/100000: episode: 6680, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000202, mae: 0.002782, mean_q: 0.019408
 66756/100000: episode: 6681, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000036, mae: 0.003544, mean_q: 0.021890
 66766/100000: episode: 6682, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000013, mae: 0.001965, mean_q: 0.019793
 66776/100000: episode: 6683, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000035, mae: 0.002675, mean_q: 0.018911
 66786/100000: episode: 6684, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000019, mae: 0.001896, mean_q: 0.019985
 66796/100000: episode: 6685, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000227, mae: 0.003469, mean_q: 0.019545
 66806/100000: episode: 6686, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000437, mae: 0.005759, mean_q: 0.020993
 66816/100000: episode: 6687, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000057, mae: 0.003477, mean_q: 0.021472
 66826/100000: episode: 6688, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000035, mae: 0.003080, mean_q: 0.018588
 66836/100000: episode: 6689, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000055, mae: 0.002622, mean_q: 0.019767
 66846/100000: episode: 6690, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000054, mae: 0.002650, mean_q: 0.020629
 66856/100000: episode: 6691, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000031, mae: 0.002539, mean_q: 0.019005
 66866/100000: episode: 6692, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000077, mae: 0.003082, mean_q: 0.019376
 66876/100000: episode: 6693, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.001539, mae: 0.006642, mean_q: 0.021633
 66886/100000: episode: 6694, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000058, mae: 0.004299, mean_q: 0.022284
 66896/100000: episode: 6695, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000035, mae: 0.003138, mean_q: 0.018550
 66906/100000: episode: 6696, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000038, mae: 0.002510, mean_q: 0.019642
 66916/100000: episode: 6697, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000015, mae: 0.001967, mean_q: 0.020367
 66926/100000: episode: 6698, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000011, mae: 0.002173, mean_q: 0.018844
 66936/100000: episode: 6699, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000074, mae: 0.002615, mean_q: 0.019598
 66946/100000: episode: 6700, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.002744, mean_q: 0.020483
 66956/100000: episode: 6701, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000224, mae: 0.003434, mean_q: 0.020719
 66966/100000: episode: 6702, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000080, mae: 0.003694, mean_q: 0.021021
 66976/100000: episode: 6703, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000073, mae: 0.002397, mean_q: 0.019887
 66986/100000: episode: 6704, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000021, mae: 0.002551, mean_q: 0.019649
 66996/100000: episode: 6705, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000225, mae: 0.003900, mean_q: 0.018843
Step 67000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.67000.hdf5
 67006/100000: episode: 6706, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000009, mae: 0.001097, mean_q: 0.019945
 67016/100000: episode: 6707, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000015, mae: 0.002335, mean_q: 0.019055
 67026/100000: episode: 6708, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000010, mae: 0.001802, mean_q: 0.018917
 67036/100000: episode: 6709, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002336, mean_q: 0.019234
 67046/100000: episode: 6710, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001543, mae: 0.005870, mean_q: 0.019616
 67056/100000: episode: 6711, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000206, mae: 0.005756, mean_q: 0.023513
 67066/100000: episode: 6712, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000054, mae: 0.003062, mean_q: 0.019549
 67076/100000: episode: 6713, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000013, mae: 0.003004, mean_q: 0.018021
 67086/100000: episode: 6714, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002352, mean_q: 0.020286
 67096/100000: episode: 6715, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000013, mae: 0.001970, mean_q: 0.019545
 67106/100000: episode: 6716, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000033, mae: 0.002814, mean_q: 0.018366
 67116/100000: episode: 6717, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000055, mae: 0.002393, mean_q: 0.019540
 67126/100000: episode: 6718, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000219, mae: 0.003353, mean_q: 0.020781
 67136/100000: episode: 6719, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000012, mae: 0.001820, mean_q: 0.019693
 67146/100000: episode: 6720, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000276, mae: 0.006694, mean_q: 0.022585
 67156/100000: episode: 6721, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001512, mae: 0.007451, mean_q: 0.023335
 67166/100000: episode: 6722, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001550, mae: 0.007365, mean_q: 0.023083
 67176/100000: episode: 6723, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000033, mae: 0.002260, mean_q: 0.021055
 67186/100000: episode: 6724, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000034, mae: 0.002564, mean_q: 0.019698
 67196/100000: episode: 6725, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000198, mae: 0.002529, mean_q: 0.020119
 67206/100000: episode: 6726, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000098, mae: 0.003696, mean_q: 0.021310
 67216/100000: episode: 6727, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000035, mae: 0.002056, mean_q: 0.020810
 67226/100000: episode: 6728, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000074, mae: 0.002719, mean_q: 0.020264
 67236/100000: episode: 6729, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001700, mae: 0.005794, mean_q: 0.020815
 67246/100000: episode: 6730, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000223, mae: 0.005766, mean_q: 0.024092
 67256/100000: episode: 6731, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000016, mae: 0.002368, mean_q: 0.021481
 67266/100000: episode: 6732, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.002681, mean_q: 0.019365
 67276/100000: episode: 6733, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.001909, mean_q: 0.020407
 67286/100000: episode: 6734, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000054, mae: 0.002618, mean_q: 0.021389
[Info] 1-TH LEVEL FOUND: 0.023327616974711418, Considering 100/100 traces
 67296/100000: episode: 6735, duration: 0.781s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.001529, mae: 0.005393, mean_q: 0.021758
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023327616974711418
1
 67306/100000: episode: 6736, duration: 0.506s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000074, mae: 0.004119, mean_q: 0.023017
 67316/100000: episode: 6737, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000221, mae: 0.002887, mean_q: 0.021274
 67326/100000: episode: 6738, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000075, mae: 0.002912, mean_q: 0.020602
 67336/100000: episode: 6739, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000061, mae: 0.002838, mean_q: 0.021031
 67346/100000: episode: 6740, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000031, mae: 0.002069, mean_q: 0.020486
 67356/100000: episode: 6741, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000031, mae: 0.002295, mean_q: 0.020021
 67366/100000: episode: 6742, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000030, mae: 0.001616, mean_q: 0.020759
 67376/100000: episode: 6743, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000201, mae: 0.002439, mean_q: 0.020868
 67386/100000: episode: 6744, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000278, mae: 0.004250, mean_q: 0.021802
 67396/100000: episode: 6745, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000051, mae: 0.002843, mean_q: 0.022212
 67406/100000: episode: 6746, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000036, mae: 0.002222, mean_q: 0.020611
 67416/100000: episode: 6747, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000200, mae: 0.002811, mean_q: 0.020641
 67426/100000: episode: 6748, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000089, mae: 0.002717, mean_q: 0.021512
 67436/100000: episode: 6749, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000012, mae: 0.001722, mean_q: 0.021385
 67446/100000: episode: 6750, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000055, mae: 0.003358, mean_q: 0.019507
 67456/100000: episode: 6751, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000038, mae: 0.002859, mean_q: 0.020212
 67466/100000: episode: 6752, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000031, mae: 0.002011, mean_q: 0.020282
 67476/100000: episode: 6753, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000009, mae: 0.001031, mean_q: 0.020712
 67486/100000: episode: 6754, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000203, mae: 0.003289, mean_q: 0.019868
 67496/100000: episode: 6755, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000018, mae: 0.002091, mean_q: 0.020473
 67506/100000: episode: 6756, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002421, mean_q: 0.020355
 67516/100000: episode: 6757, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000006, mae: 0.000785, mean_q: 0.020435
 67526/100000: episode: 6758, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000427, mae: 0.004235, mean_q: 0.020651
 67536/100000: episode: 6759, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000078, mae: 0.004343, mean_q: 0.022564
 67546/100000: episode: 6760, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000220, mae: 0.002897, mean_q: 0.021099
 67556/100000: episode: 6761, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000017, mae: 0.002120, mean_q: 0.020265
 67566/100000: episode: 6762, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000038, mae: 0.002654, mean_q: 0.020016
 67576/100000: episode: 6763, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000032, mae: 0.001828, mean_q: 0.020699
 67586/100000: episode: 6764, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000057, mae: 0.002914, mean_q: 0.021313
 67596/100000: episode: 6765, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000221, mae: 0.002778, mean_q: 0.020875
 67606/100000: episode: 6766, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000008, mae: 0.001125, mean_q: 0.020928
 67616/100000: episode: 6767, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000093, mae: 0.002967, mean_q: 0.020317
 67626/100000: episode: 6768, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000040, mae: 0.002725, mean_q: 0.021135
 67636/100000: episode: 6769, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000035, mae: 0.002535, mean_q: 0.020089
 67646/100000: episode: 6770, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000032, mae: 0.002275, mean_q: 0.019778
 67656/100000: episode: 6771, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000015, mae: 0.001559, mean_q: 0.020690
 67666/100000: episode: 6772, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000018, mae: 0.002513, mean_q: 0.021456
 67676/100000: episode: 6773, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001536, mae: 0.005413, mean_q: 0.019977
 67686/100000: episode: 6774, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000058, mae: 0.003828, mean_q: 0.022287
 67696/100000: episode: 6775, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000013, mae: 0.001846, mean_q: 0.020553
 67706/100000: episode: 6776, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000228, mae: 0.003607, mean_q: 0.020492
 67716/100000: episode: 6777, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000055, mae: 0.002276, mean_q: 0.020730
 67726/100000: episode: 6778, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000015, mae: 0.001874, mean_q: 0.020221
 67736/100000: episode: 6779, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000198, mae: 0.002348, mean_q: 0.020510
 67746/100000: episode: 6780, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000030, mae: 0.001459, mean_q: 0.020727
 67756/100000: episode: 6781, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000244, mae: 0.003363, mean_q: 0.020501
 67766/100000: episode: 6782, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000230, mae: 0.004650, mean_q: 0.021978
 67776/100000: episode: 6783, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000060, mae: 0.002883, mean_q: 0.020694
 67786/100000: episode: 6784, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000093, mae: 0.003182, mean_q: 0.021226
 67796/100000: episode: 6785, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000076, mae: 0.003498, mean_q: 0.021992
 67806/100000: episode: 6786, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001726, mae: 0.007095, mean_q: 0.021584
 67816/100000: episode: 6787, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000033, mae: 0.003818, mean_q: 0.023749
 67826/100000: episode: 6788, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.001560, mae: 0.006037, mean_q: 0.020839
 67836/100000: episode: 6789, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000064, mae: 0.004776, mean_q: 0.023527
 67846/100000: episode: 6790, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000075, mae: 0.003024, mean_q: 0.021090
 67856/100000: episode: 6791, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.002767, mean_q: 0.020516
 67866/100000: episode: 6792, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000393, mae: 0.004439, mean_q: 0.021771
 67876/100000: episode: 6793, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000043, mae: 0.003298, mean_q: 0.022188
 67886/100000: episode: 6794, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000016, mae: 0.002974, mean_q: 0.019738
 67896/100000: episode: 6795, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000038, mae: 0.002881, mean_q: 0.020332
 67906/100000: episode: 6796, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000012, mae: 0.001775, mean_q: 0.020625
 67916/100000: episode: 6797, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000057, mae: 0.003137, mean_q: 0.020103
 67926/100000: episode: 6798, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000015, mae: 0.001689, mean_q: 0.020723
 67936/100000: episode: 6799, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000033, mae: 0.001923, mean_q: 0.020743
 67946/100000: episode: 6800, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000015, mae: 0.002828, mean_q: 0.022597
 67956/100000: episode: 6801, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000073, mae: 0.003123, mean_q: 0.022064
 67966/100000: episode: 6802, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.002392, mean_q: 0.021016
 67976/100000: episode: 6803, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002180, mean_q: 0.020606
 67986/100000: episode: 6804, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000059, mae: 0.002831, mean_q: 0.020922
 67996/100000: episode: 6805, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000039, mae: 0.002507, mean_q: 0.020549
Step 68000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.68000.hdf5
 68006/100000: episode: 6806, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000207, mae: 0.003238, mean_q: 0.020567
 68016/100000: episode: 6807, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000224, mae: 0.003118, mean_q: 0.021065
 68026/100000: episode: 6808, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000210, mae: 0.003539, mean_q: 0.021417
 68036/100000: episode: 6809, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000076, mae: 0.002904, mean_q: 0.021394
 68046/100000: episode: 6810, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.001596, mae: 0.007113, mean_q: 0.022122
 68056/100000: episode: 6811, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000228, mae: 0.006500, mean_q: 0.025107
 68066/100000: episode: 6812, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002625, mean_q: 0.021200
 68076/100000: episode: 6813, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000249, mae: 0.004555, mean_q: 0.020227
 68086/100000: episode: 6814, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000015, mae: 0.002452, mean_q: 0.022492
 68096/100000: episode: 6815, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000056, mae: 0.002815, mean_q: 0.022047
 68106/100000: episode: 6816, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000198, mae: 0.002479, mean_q: 0.021209
 68116/100000: episode: 6817, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000052, mae: 0.002037, mean_q: 0.021342
 68126/100000: episode: 6818, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.002069, mae: 0.007913, mean_q: 0.021603
 68136/100000: episode: 6819, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000048, mae: 0.006636, mean_q: 0.027023
 68146/100000: episode: 6820, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000053, mae: 0.003362, mean_q: 0.022504
 68156/100000: episode: 6821, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000033, mae: 0.003681, mean_q: 0.019675
 68166/100000: episode: 6822, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000035, mae: 0.002775, mean_q: 0.020723
 68176/100000: episode: 6823, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.001803, mean_q: 0.021829
 68186/100000: episode: 6824, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000037, mae: 0.002195, mean_q: 0.021778
 68196/100000: episode: 6825, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002999, mean_q: 0.020759
 68206/100000: episode: 6826, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000076, mae: 0.002861, mean_q: 0.021506
 68216/100000: episode: 6827, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000030, mae: 0.001586, mean_q: 0.021763
 68226/100000: episode: 6828, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000059, mae: 0.003039, mean_q: 0.020792
 68236/100000: episode: 6829, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000245, mae: 0.003853, mean_q: 0.021939
 68246/100000: episode: 6830, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000059, mae: 0.003416, mean_q: 0.022773
 68256/100000: episode: 6831, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000076, mae: 0.002918, mean_q: 0.021601
 68266/100000: episode: 6832, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.001727, mean_q: 0.021053
 68276/100000: episode: 6833, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000415, mae: 0.004971, mean_q: 0.021609
 68286/100000: episode: 6834, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000204, mae: 0.003901, mean_q: 0.022985
 68296/100000: episode: 6835, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000007, mae: 0.001661, mean_q: 0.020863
[Info] 1-TH LEVEL FOUND: 0.021902751177549362, Considering 100/100 traces
 68306/100000: episode: 6836, duration: 0.753s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000078, mae: 0.003514, mean_q: 0.020550
[Info] 2-TH LEVEL FOUND: 0.022824876010417938, Considering 100/100 traces
 68316/100000: episode: 6837, duration: 0.791s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000053, mae: 0.002595, mean_q: 0.022323
[Info] 3-TH LEVEL FOUND: 0.022876733914017677, Considering 100/100 traces
 68326/100000: episode: 6838, duration: 0.843s, episode steps: 10, steps per second: 12, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000246, mae: 0.004495, mean_q: 0.022716
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022876733914017677
3
 68336/100000: episode: 6839, duration: 0.494s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000033, mae: 0.002167, mean_q: 0.022261
 68346/100000: episode: 6840, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000416, mae: 0.005272, mean_q: 0.020577
 68356/100000: episode: 6841, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.003006, mean_q: 0.022823
 68366/100000: episode: 6842, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000240, mae: 0.003284, mean_q: 0.021645
 68376/100000: episode: 6843, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000201, mae: 0.003345, mean_q: 0.022859
 68386/100000: episode: 6844, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000039, mae: 0.003019, mean_q: 0.022613
 68396/100000: episode: 6845, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000079, mae: 0.003703, mean_q: 0.020841
 68406/100000: episode: 6846, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000038, mae: 0.002201, mean_q: 0.021712
 68416/100000: episode: 6847, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000013, mae: 0.001962, mean_q: 0.021006
 68426/100000: episode: 6848, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000036, mae: 0.002713, mean_q: 0.020866
 68436/100000: episode: 6849, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000248, mae: 0.004248, mean_q: 0.020996
 68446/100000: episode: 6850, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002121, mean_q: 0.021960
 68456/100000: episode: 6851, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000203, mae: 0.002953, mean_q: 0.021140
 68466/100000: episode: 6852, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000223, mae: 0.003134, mean_q: 0.021834
 68476/100000: episode: 6853, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000227, mae: 0.003921, mean_q: 0.022265
 68486/100000: episode: 6854, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000057, mae: 0.002616, mean_q: 0.021492
 68496/100000: episode: 6855, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000099, mae: 0.003542, mean_q: 0.021253
 68506/100000: episode: 6856, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000079, mae: 0.002938, mean_q: 0.021594
 68516/100000: episode: 6857, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000237, mae: 0.003262, mean_q: 0.022131
 68526/100000: episode: 6858, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000057, mae: 0.003043, mean_q: 0.022302
 68536/100000: episode: 6859, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000034, mae: 0.002467, mean_q: 0.020790
 68546/100000: episode: 6860, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000041, mae: 0.002793, mean_q: 0.021096
 68556/100000: episode: 6861, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000041, mae: 0.002441, mean_q: 0.021564
 68566/100000: episode: 6862, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000071, mae: 0.002390, mean_q: 0.021311
 68576/100000: episode: 6863, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000036, mae: 0.002094, mean_q: 0.021239
 68586/100000: episode: 6864, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000224, mae: 0.003240, mean_q: 0.021377
 68596/100000: episode: 6865, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000014, mae: 0.001863, mean_q: 0.022017
 68606/100000: episode: 6866, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.002512, mean_q: 0.020290
 68616/100000: episode: 6867, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000204, mae: 0.003517, mean_q: 0.020325
 68626/100000: episode: 6868, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000058, mae: 0.002994, mean_q: 0.021813
 68636/100000: episode: 6869, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000201, mae: 0.003415, mean_q: 0.022412
 68646/100000: episode: 6870, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000045, mae: 0.003509, mean_q: 0.020645
 68656/100000: episode: 6871, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000288, mae: 0.004948, mean_q: 0.021366
 68666/100000: episode: 6872, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000014, mae: 0.001947, mean_q: 0.022040
 68676/100000: episode: 6873, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000203, mae: 0.002565, mean_q: 0.021419
 68686/100000: episode: 6874, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000060, mae: 0.003296, mean_q: 0.022254
 68696/100000: episode: 6875, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000037, mae: 0.002630, mean_q: 0.020841
 68706/100000: episode: 6876, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000036, mae: 0.002770, mean_q: 0.020570
 68716/100000: episode: 6877, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000251, mae: 0.004363, mean_q: 0.020891
 68726/100000: episode: 6878, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000080, mae: 0.003448, mean_q: 0.021806
 68736/100000: episode: 6879, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000057, mae: 0.002883, mean_q: 0.020675
 68746/100000: episode: 6880, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000018, mae: 0.002273, mean_q: 0.020823
 68756/100000: episode: 6881, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000031, mae: 0.002169, mean_q: 0.020370
 68766/100000: episode: 6882, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000078, mae: 0.003098, mean_q: 0.020566
 68776/100000: episode: 6883, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000036, mae: 0.002584, mean_q: 0.021913
 68786/100000: episode: 6884, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000225, mae: 0.003332, mean_q: 0.021455
 68796/100000: episode: 6885, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000033, mae: 0.001939, mean_q: 0.021051
 68806/100000: episode: 6886, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000033, mae: 0.002059, mean_q: 0.021474
 68816/100000: episode: 6887, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000134, mae: 0.004149, mean_q: 0.022087
 68826/100000: episode: 6888, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000077, mae: 0.003165, mean_q: 0.021217
 68836/100000: episode: 6889, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000033, mae: 0.002222, mean_q: 0.020632
 68846/100000: episode: 6890, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000431, mae: 0.005352, mean_q: 0.022246
 68856/100000: episode: 6891, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000230, mae: 0.004769, mean_q: 0.022640
 68866/100000: episode: 6892, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000249, mae: 0.004048, mean_q: 0.021071
 68876/100000: episode: 6893, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000201, mae: 0.002582, mean_q: 0.021453
 68886/100000: episode: 6894, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000014, mae: 0.001638, mean_q: 0.021618
 68896/100000: episode: 6895, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000073, mae: 0.002858, mean_q: 0.020836
 68906/100000: episode: 6896, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000055, mae: 0.002478, mean_q: 0.021434
 68916/100000: episode: 6897, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000014, mae: 0.001957, mean_q: 0.020761
 68926/100000: episode: 6898, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000011, mae: 0.001788, mean_q: 0.020451
 68936/100000: episode: 6899, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000240, mae: 0.003334, mean_q: 0.021559
 68946/100000: episode: 6900, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000017, mae: 0.002307, mean_q: 0.022123
 68956/100000: episode: 6901, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000223, mae: 0.003181, mean_q: 0.021554
 68966/100000: episode: 6902, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000036, mae: 0.002505, mean_q: 0.021471
 68976/100000: episode: 6903, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000197, mae: 0.002669, mean_q: 0.020485
 68986/100000: episode: 6904, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000206, mae: 0.003205, mean_q: 0.021725
 68996/100000: episode: 6905, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000012, mae: 0.001775, mean_q: 0.021652
Step 69000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.69000.hdf5
 69006/100000: episode: 6906, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000224, mae: 0.003448, mean_q: 0.020774
 69016/100000: episode: 6907, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000251, mae: 0.004134, mean_q: 0.021239
 69026/100000: episode: 6908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000078, mae: 0.003252, mean_q: 0.021918
 69036/100000: episode: 6909, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000202, mae: 0.002777, mean_q: 0.021409
 69046/100000: episode: 6910, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000226, mae: 0.003409, mean_q: 0.021340
 69056/100000: episode: 6911, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000058, mae: 0.002861, mean_q: 0.021870
 69066/100000: episode: 6912, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000410, mae: 0.004391, mean_q: 0.021837
 69076/100000: episode: 6913, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000243, mae: 0.004789, mean_q: 0.023103
 69086/100000: episode: 6914, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000042, mae: 0.003165, mean_q: 0.022166
 69096/100000: episode: 6915, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.003237, mean_q: 0.020124
 69106/100000: episode: 6916, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000206, mae: 0.003449, mean_q: 0.020688
 69116/100000: episode: 6917, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.002683, mean_q: 0.021926
 69126/100000: episode: 6918, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000222, mae: 0.003763, mean_q: 0.022574
 69136/100000: episode: 6919, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000265, mae: 0.004595, mean_q: 0.022428
 69146/100000: episode: 6920, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000038, mae: 0.002502, mean_q: 0.021770
 69156/100000: episode: 6921, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000394, mae: 0.004424, mean_q: 0.021495
 69166/100000: episode: 6922, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000035, mae: 0.002401, mean_q: 0.022259
 69176/100000: episode: 6923, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000223, mae: 0.003551, mean_q: 0.020968
 69186/100000: episode: 6924, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000050, mae: 0.001929, mean_q: 0.021784
 69196/100000: episode: 6925, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000022, mae: 0.002154, mean_q: 0.021692
 69206/100000: episode: 6926, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.002429, mean_q: 0.020622
 69216/100000: episode: 6927, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000039, mae: 0.003169, mean_q: 0.020471
 69226/100000: episode: 6928, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000265, mae: 0.004212, mean_q: 0.021180
 69236/100000: episode: 6929, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000228, mae: 0.003959, mean_q: 0.022091
 69246/100000: episode: 6930, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.002023, mean_q: 0.021562
 69256/100000: episode: 6931, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000057, mae: 0.003118, mean_q: 0.020596
 69266/100000: episode: 6932, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.002702, mean_q: 0.020723
 69276/100000: episode: 6933, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000431, mae: 0.005263, mean_q: 0.022140
 69286/100000: episode: 6934, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000066, mae: 0.004195, mean_q: 0.022818
 69296/100000: episode: 6935, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000224, mae: 0.003528, mean_q: 0.020842
 69306/100000: episode: 6936, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000412, mae: 0.004294, mean_q: 0.021598
 69316/100000: episode: 6937, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000078, mae: 0.004431, mean_q: 0.023391
 69326/100000: episode: 6938, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000054, mae: 0.002386, mean_q: 0.021857
[Info] 1-TH LEVEL FOUND: 0.02142728678882122, Considering 100/100 traces
 69336/100000: episode: 6939, duration: 0.718s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000045, mae: 0.002951, mean_q: 0.021173
[Info] 2-TH LEVEL FOUND: 0.021588094532489777, Considering 100/100 traces
 69346/100000: episode: 6940, duration: 0.700s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000037, mae: 0.002082, mean_q: 0.021512
[Info] 3-TH LEVEL FOUND: 0.021707575768232346, Considering 100/100 traces
 69356/100000: episode: 6941, duration: 0.722s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000029, mae: 0.001345, mean_q: 0.021680
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021707575768232346
3
 69366/100000: episode: 6942, duration: 0.498s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000038, mae: 0.002108, mean_q: 0.021667
 69376/100000: episode: 6943, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000282, mae: 0.004448, mean_q: 0.021636
 69386/100000: episode: 6944, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000039, mae: 0.003385, mean_q: 0.023129
 69396/100000: episode: 6945, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000032, mae: 0.002669, mean_q: 0.020282
 69406/100000: episode: 6946, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000058, mae: 0.002793, mean_q: 0.021463
 69416/100000: episode: 6947, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000020, mae: 0.002549, mean_q: 0.021479
 69426/100000: episode: 6948, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000074, mae: 0.003293, mean_q: 0.020390
 69436/100000: episode: 6949, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000020, mae: 0.002361, mean_q: 0.021097
 69446/100000: episode: 6950, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000019, mae: 0.003196, mean_q: 0.019799
 69456/100000: episode: 6951, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000037, mae: 0.002647, mean_q: 0.021531
 69466/100000: episode: 6952, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000228, mae: 0.003791, mean_q: 0.021899
 69476/100000: episode: 6953, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000052, mae: 0.002230, mean_q: 0.021183
 69486/100000: episode: 6954, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000017, mae: 0.002301, mean_q: 0.020706
 69496/100000: episode: 6955, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000239, mae: 0.003123, mean_q: 0.020901
 69506/100000: episode: 6956, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002809, mean_q: 0.022007
 69516/100000: episode: 6957, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000262, mae: 0.003932, mean_q: 0.021763
 69526/100000: episode: 6958, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000038, mae: 0.002762, mean_q: 0.022165
 69536/100000: episode: 6959, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000220, mae: 0.002978, mean_q: 0.021050
 69546/100000: episode: 6960, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000412, mae: 0.005579, mean_q: 0.022896
 69556/100000: episode: 6961, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000226, mae: 0.004831, mean_q: 0.023491
 69566/100000: episode: 6962, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000076, mae: 0.003179, mean_q: 0.021542
 69576/100000: episode: 6963, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000035, mae: 0.002474, mean_q: 0.020872
 69586/100000: episode: 6964, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.002417, mean_q: 0.021321
 69596/100000: episode: 6965, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.002455, mean_q: 0.020538
 69606/100000: episode: 6966, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000011, mae: 0.002253, mean_q: 0.020115
 69616/100000: episode: 6967, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000076, mae: 0.002930, mean_q: 0.021367
 69626/100000: episode: 6968, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000029, mae: 0.001766, mean_q: 0.021757
 69636/100000: episode: 6969, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000053, mae: 0.002348, mean_q: 0.021704
 69646/100000: episode: 6970, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000034, mae: 0.002413, mean_q: 0.021065
 69656/100000: episode: 6971, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000033, mae: 0.002162, mean_q: 0.020809
 69666/100000: episode: 6972, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000060, mae: 0.002998, mean_q: 0.020901
 69676/100000: episode: 6973, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000061, mae: 0.002970, mean_q: 0.020854
 69686/100000: episode: 6974, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000011, mae: 0.001762, mean_q: 0.020745
 69696/100000: episode: 6975, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000037, mae: 0.003341, mean_q: 0.019635
 69706/100000: episode: 6976, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000393, mae: 0.004422, mean_q: 0.020783
 69716/100000: episode: 6977, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000209, mae: 0.004728, mean_q: 0.022910
 69726/100000: episode: 6978, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000015, mae: 0.002095, mean_q: 0.020761
 69736/100000: episode: 6979, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000212, mae: 0.003819, mean_q: 0.020307
 69746/100000: episode: 6980, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000018, mae: 0.002629, mean_q: 0.020336
 69756/100000: episode: 6981, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002950, mean_q: 0.019461
 69766/100000: episode: 6982, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000014, mae: 0.001601, mean_q: 0.020608
 69776/100000: episode: 6983, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000222, mae: 0.003229, mean_q: 0.020201
 69786/100000: episode: 6984, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.002356, mean_q: 0.021031
 69796/100000: episode: 6985, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000052, mae: 0.002249, mean_q: 0.020244
 69806/100000: episode: 6986, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000033, mae: 0.001739, mean_q: 0.020860
 69816/100000: episode: 6987, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000034, mae: 0.001954, mean_q: 0.020529
 69826/100000: episode: 6988, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000013, mae: 0.002139, mean_q: 0.019804
 69836/100000: episode: 6989, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000035, mae: 0.001967, mean_q: 0.020247
 69846/100000: episode: 6990, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000053, mae: 0.002353, mean_q: 0.020216
 69856/100000: episode: 6991, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002389, mean_q: 0.020027
 69866/100000: episode: 6992, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000268, mae: 0.004385, mean_q: 0.019759
 69876/100000: episode: 6993, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000013, mae: 0.002161, mean_q: 0.021301
 69886/100000: episode: 6994, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000071, mae: 0.002958, mean_q: 0.019296
 69896/100000: episode: 6995, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000222, mae: 0.003530, mean_q: 0.021276
 69906/100000: episode: 6996, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000060, mae: 0.003262, mean_q: 0.021243
 69916/100000: episode: 6997, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000164, mae: 0.004835, mean_q: 0.020797
 69926/100000: episode: 6998, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000427, mae: 0.005331, mean_q: 0.022042
 69936/100000: episode: 6999, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000244, mae: 0.005430, mean_q: 0.023314
 69946/100000: episode: 7000, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000217, mae: 0.002836, mean_q: 0.021146
 69956/100000: episode: 7001, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000013, mae: 0.001683, mean_q: 0.020678
 69966/100000: episode: 7002, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000264, mae: 0.004437, mean_q: 0.019812
 69976/100000: episode: 7003, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000016, mae: 0.002932, mean_q: 0.022612
 69986/100000: episode: 7004, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000078, mae: 0.003153, mean_q: 0.020550
 69996/100000: episode: 7005, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000035, mae: 0.002014, mean_q: 0.020867
Step 70000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.70000.hdf5
 70006/100000: episode: 7006, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000056, mae: 0.002337, mean_q: 0.021049
 70016/100000: episode: 7007, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000037, mae: 0.001914, mean_q: 0.020911
 70026/100000: episode: 7008, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000041, mae: 0.002708, mean_q: 0.020590
 70036/100000: episode: 7009, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000036, mae: 0.002486, mean_q: 0.020173
 70046/100000: episode: 7010, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000055, mae: 0.002666, mean_q: 0.020205
 70056/100000: episode: 7011, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000019, mae: 0.002288, mean_q: 0.020293
 70066/100000: episode: 7012, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000034, mae: 0.002166, mean_q: 0.020186
 70076/100000: episode: 7013, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000019, mae: 0.002316, mean_q: 0.020067
 70086/100000: episode: 7014, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000035, mae: 0.002081, mean_q: 0.020271
 70096/100000: episode: 7015, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000060, mae: 0.002748, mean_q: 0.020551
 70106/100000: episode: 7016, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000015, mae: 0.001732, mean_q: 0.020224
 70116/100000: episode: 7017, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000205, mae: 0.003130, mean_q: 0.020769
 70126/100000: episode: 7018, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000011, mae: 0.001765, mean_q: 0.020007
 70136/100000: episode: 7019, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000031, mae: 0.002475, mean_q: 0.019236
 70146/100000: episode: 7020, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000205, mae: 0.002815, mean_q: 0.020271
 70156/100000: episode: 7021, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000058, mae: 0.002597, mean_q: 0.020789
 70166/100000: episode: 7022, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000238, mae: 0.003676, mean_q: 0.021402
 70176/100000: episode: 7023, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.002217, mean_q: 0.020720
 70186/100000: episode: 7024, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000204, mae: 0.003252, mean_q: 0.019558
 70196/100000: episode: 7025, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000038, mae: 0.002249, mean_q: 0.020673
 70206/100000: episode: 7026, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000008, mae: 0.001270, mean_q: 0.020054
 70216/100000: episode: 7027, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000054, mae: 0.002509, mean_q: 0.019622
 70226/100000: episode: 7028, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.001599, mean_q: 0.020643
 70236/100000: episode: 7029, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001502, mean_q: 0.020227
 70246/100000: episode: 7030, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000033, mae: 0.002497, mean_q: 0.019244
 70256/100000: episode: 7031, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000202, mae: 0.002874, mean_q: 0.019995
 70266/100000: episode: 7032, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000241, mae: 0.003746, mean_q: 0.021144
 70276/100000: episode: 7033, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000065, mae: 0.003277, mean_q: 0.020761
 70286/100000: episode: 7034, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000096, mae: 0.002990, mean_q: 0.020090
 70296/100000: episode: 7035, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.002733, mean_q: 0.020950
 70306/100000: episode: 7036, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000082, mae: 0.003202, mean_q: 0.020095
 70316/100000: episode: 7037, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000012, mae: 0.001642, mean_q: 0.019952
 70326/100000: episode: 7038, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000033, mae: 0.002954, mean_q: 0.018650
 70336/100000: episode: 7039, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000019, mae: 0.002103, mean_q: 0.019994
 70346/100000: episode: 7040, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000230, mae: 0.003483, mean_q: 0.020010
 70356/100000: episode: 7041, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.001920, mean_q: 0.019642
[Info] 1-TH LEVEL FOUND: 0.02026502788066864, Considering 100/100 traces
 70366/100000: episode: 7042, duration: 0.787s, episode steps: 10, steps per second: 13, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.003000, mean_q: 0.019038
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02026502788066864
1
 70376/100000: episode: 7043, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000037, mae: 0.002433, mean_q: 0.020503
 70386/100000: episode: 7044, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000205, mae: 0.002951, mean_q: 0.019681
 70396/100000: episode: 7045, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000048, mae: 0.001703, mean_q: 0.019683
 70406/100000: episode: 7046, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000008, mae: 0.001297, mean_q: 0.020457
 70416/100000: episode: 7047, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000056, mae: 0.002172, mean_q: 0.019937
 70426/100000: episode: 7048, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000012, mae: 0.001609, mean_q: 0.019510
 70436/100000: episode: 7049, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000050, mae: 0.001870, mean_q: 0.019752
 70446/100000: episode: 7050, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000100, mae: 0.003439, mean_q: 0.020394
 70456/100000: episode: 7051, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002827, mean_q: 0.020944
 70466/100000: episode: 7052, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000018, mae: 0.002816, mean_q: 0.018501
 70476/100000: episode: 7053, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000224, mae: 0.003722, mean_q: 0.020650
 70486/100000: episode: 7054, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000036, mae: 0.002425, mean_q: 0.020094
 70496/100000: episode: 7055, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000224, mae: 0.003177, mean_q: 0.019660
 70506/100000: episode: 7056, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000060, mae: 0.003009, mean_q: 0.020570
 70516/100000: episode: 7057, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000057, mae: 0.002668, mean_q: 0.020573
 70526/100000: episode: 7058, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000014, mae: 0.001801, mean_q: 0.020483
 70536/100000: episode: 7059, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000056, mae: 0.002443, mean_q: 0.019815
 70546/100000: episode: 7060, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000054, mae: 0.002278, mean_q: 0.020040
 70556/100000: episode: 7061, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000038, mae: 0.002802, mean_q: 0.019115
 70566/100000: episode: 7062, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000226, mae: 0.003608, mean_q: 0.020426
 70576/100000: episode: 7063, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000014, mae: 0.002030, mean_q: 0.019926
 70586/100000: episode: 7064, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000226, mae: 0.003506, mean_q: 0.019941
 70596/100000: episode: 7065, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000225, mae: 0.003422, mean_q: 0.020647
 70606/100000: episode: 7066, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000032, mae: 0.002165, mean_q: 0.019936
 70616/100000: episode: 7067, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000267, mae: 0.004263, mean_q: 0.019743
 70626/100000: episode: 7068, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000230, mae: 0.005248, mean_q: 0.022286
 70636/100000: episode: 7069, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000031, mae: 0.002051, mean_q: 0.020421
 70646/100000: episode: 7070, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000203, mae: 0.002926, mean_q: 0.019921
 70656/100000: episode: 7071, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002376, mean_q: 0.020627
 70666/100000: episode: 7072, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.001984, mean_q: 0.019828
 70676/100000: episode: 7073, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000042, mae: 0.003068, mean_q: 0.019405
 70686/100000: episode: 7074, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000015, mae: 0.001930, mean_q: 0.019770
 70696/100000: episode: 7075, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000226, mae: 0.003537, mean_q: 0.019852
 70706/100000: episode: 7076, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000013, mae: 0.001789, mean_q: 0.020048
 70716/100000: episode: 7077, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000019, mae: 0.002505, mean_q: 0.019012
 70726/100000: episode: 7078, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000036, mae: 0.002444, mean_q: 0.020109
 70736/100000: episode: 7079, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000207, mae: 0.003712, mean_q: 0.018484
 70746/100000: episode: 7080, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000032, mae: 0.002648, mean_q: 0.021210
 70756/100000: episode: 7081, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000033, mae: 0.001997, mean_q: 0.019815
 70766/100000: episode: 7082, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000054, mae: 0.002786, mean_q: 0.019013
 70776/100000: episode: 7083, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000228, mae: 0.003400, mean_q: 0.020038
 70786/100000: episode: 7084, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000035, mae: 0.003254, mean_q: 0.021599
 70796/100000: episode: 7085, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.001936, mean_q: 0.020440
 70806/100000: episode: 7086, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000076, mae: 0.002770, mean_q: 0.019732
 70816/100000: episode: 7087, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000037, mae: 0.002153, mean_q: 0.020035
 70826/100000: episode: 7088, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000034, mae: 0.002103, mean_q: 0.019529
 70836/100000: episode: 7089, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000241, mae: 0.004435, mean_q: 0.021724
 70846/100000: episode: 7090, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000012, mae: 0.002209, mean_q: 0.021215
 70856/100000: episode: 7091, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000017, mae: 0.002509, mean_q: 0.019154
 70866/100000: episode: 7092, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000050, mae: 0.002082, mean_q: 0.019740
 70876/100000: episode: 7093, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000204, mae: 0.003812, mean_q: 0.021595
 70886/100000: episode: 7094, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000077, mae: 0.002908, mean_q: 0.020418
 70896/100000: episode: 7095, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.002807, mean_q: 0.020280
 70906/100000: episode: 7096, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000015, mae: 0.002294, mean_q: 0.019497
 70916/100000: episode: 7097, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000014, mae: 0.002422, mean_q: 0.018715
 70926/100000: episode: 7098, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000391, mae: 0.003688, mean_q: 0.020152
 70936/100000: episode: 7099, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000038, mae: 0.003753, mean_q: 0.022272
 70946/100000: episode: 7100, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000014, mae: 0.002045, mean_q: 0.019677
 70956/100000: episode: 7101, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000077, mae: 0.003273, mean_q: 0.019351
 70966/100000: episode: 7102, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000054, mae: 0.002331, mean_q: 0.020535
 70976/100000: episode: 7103, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002685, mean_q: 0.020920
 70986/100000: episode: 7104, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000254, mae: 0.004273, mean_q: 0.019941
 70996/100000: episode: 7105, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000209, mae: 0.004019, mean_q: 0.021465
Step 71000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.71000.hdf5
 71006/100000: episode: 7106, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000010, mae: 0.001378, mean_q: 0.020782
 71016/100000: episode: 7107, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000051, mae: 0.002288, mean_q: 0.019688
 71026/100000: episode: 7108, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000223, mae: 0.003291, mean_q: 0.020800
 71036/100000: episode: 7109, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.001722, mean_q: 0.020267
 71046/100000: episode: 7110, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000075, mae: 0.003016, mean_q: 0.019468
 71056/100000: episode: 7111, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000207, mae: 0.004177, mean_q: 0.021777
 71066/100000: episode: 7112, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000096, mae: 0.003037, mean_q: 0.020744
 71076/100000: episode: 7113, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000036, mae: 0.002461, mean_q: 0.020779
 71086/100000: episode: 7114, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000058, mae: 0.003571, mean_q: 0.018935
 71096/100000: episode: 7115, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000035, mae: 0.002351, mean_q: 0.019665
 71106/100000: episode: 7116, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000054, mae: 0.001992, mean_q: 0.020197
 71116/100000: episode: 7117, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000058, mae: 0.002710, mean_q: 0.020616
 71126/100000: episode: 7118, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000223, mae: 0.003160, mean_q: 0.020229
 71136/100000: episode: 7119, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000054, mae: 0.002253, mean_q: 0.020471
 71146/100000: episode: 7120, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000037, mae: 0.002872, mean_q: 0.019302
 71156/100000: episode: 7121, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002374, mean_q: 0.019270
 71166/100000: episode: 7122, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000035, mae: 0.002205, mean_q: 0.020575
 71176/100000: episode: 7123, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000204, mae: 0.002713, mean_q: 0.020151
 71186/100000: episode: 7124, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000081, mae: 0.004037, mean_q: 0.021507
 71196/100000: episode: 7125, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000036, mae: 0.002550, mean_q: 0.020436
 71206/100000: episode: 7126, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000074, mae: 0.002828, mean_q: 0.019936
 71216/100000: episode: 7127, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000054, mae: 0.002505, mean_q: 0.020569
 71226/100000: episode: 7128, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000393, mae: 0.004113, mean_q: 0.020474
 71236/100000: episode: 7129, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000037, mae: 0.003713, mean_q: 0.022502
 71246/100000: episode: 7130, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000013, mae: 0.002007, mean_q: 0.019929
 71256/100000: episode: 7131, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000055, mae: 0.003201, mean_q: 0.019000
 71266/100000: episode: 7132, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000226, mae: 0.004000, mean_q: 0.021339
 71276/100000: episode: 7133, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000204, mae: 0.003098, mean_q: 0.020774
 71286/100000: episode: 7134, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000036, mae: 0.001966, mean_q: 0.020427
 71296/100000: episode: 7135, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000035, mae: 0.002173, mean_q: 0.020782
 71306/100000: episode: 7136, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000014, mae: 0.002551, mean_q: 0.019121
 71316/100000: episode: 7137, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000223, mae: 0.003184, mean_q: 0.020085
 71326/100000: episode: 7138, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000279, mae: 0.005764, mean_q: 0.022847
 71336/100000: episode: 7139, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000082, mae: 0.004342, mean_q: 0.022308
 71346/100000: episode: 7140, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002575, mean_q: 0.020130
 71356/100000: episode: 7141, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000032, mae: 0.001858, mean_q: 0.020327
 71366/100000: episode: 7142, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000063, mae: 0.003086, mean_q: 0.020362
[Info] 1-TH LEVEL FOUND: 0.02115417644381523, Considering 100/100 traces
 71376/100000: episode: 7143, duration: 0.771s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000251, mae: 0.004515, mean_q: 0.021423
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02115417644381523
1
 71386/100000: episode: 7144, duration: 0.586s, episode steps: 10, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000079, mae: 0.003219, mean_q: 0.020232
 71396/100000: episode: 7145, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000041, mae: 0.002514, mean_q: 0.020513
 71406/100000: episode: 7146, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000034, mae: 0.002058, mean_q: 0.020189
 71416/100000: episode: 7147, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000244, mae: 0.003692, mean_q: 0.020734
 71426/100000: episode: 7148, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000201, mae: 0.002962, mean_q: 0.021033
 71436/100000: episode: 7149, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000228, mae: 0.003623, mean_q: 0.020871
 71446/100000: episode: 7150, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000034, mae: 0.002318, mean_q: 0.020306
 71456/100000: episode: 7151, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000052, mae: 0.002485, mean_q: 0.020011
 71466/100000: episode: 7152, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000051, mae: 0.001859, mean_q: 0.020431
 71476/100000: episode: 7153, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000019, mae: 0.002003, mean_q: 0.020260
 71486/100000: episode: 7154, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000062, mae: 0.002783, mean_q: 0.020383
 71496/100000: episode: 7155, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000207, mae: 0.003256, mean_q: 0.021059
 71506/100000: episode: 7156, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000016, mae: 0.002043, mean_q: 0.020748
 71516/100000: episode: 7157, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000084, mae: 0.003868, mean_q: 0.019515
 71526/100000: episode: 7158, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000033, mae: 0.002264, mean_q: 0.021146
 71536/100000: episode: 7159, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000057, mae: 0.002838, mean_q: 0.019961
 71546/100000: episode: 7160, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000030, mae: 0.001473, mean_q: 0.020364
 71556/100000: episode: 7161, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000049, mae: 0.001773, mean_q: 0.020318
 71566/100000: episode: 7162, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000080, mae: 0.003009, mean_q: 0.020318
 71576/100000: episode: 7163, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000040, mae: 0.002731, mean_q: 0.020162
 71586/100000: episode: 7164, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000052, mae: 0.002554, mean_q: 0.019335
 71596/100000: episode: 7165, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000081, mae: 0.003899, mean_q: 0.021577
 71606/100000: episode: 7166, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000040, mae: 0.002880, mean_q: 0.019913
 71616/100000: episode: 7167, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000229, mae: 0.003997, mean_q: 0.019809
 71626/100000: episode: 7168, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000230, mae: 0.005612, mean_q: 0.023064
 71636/100000: episode: 7169, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000229, mae: 0.004001, mean_q: 0.020167
 71646/100000: episode: 7170, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000016, mae: 0.001980, mean_q: 0.020492
 71656/100000: episode: 7171, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000223, mae: 0.003060, mean_q: 0.020316
 71666/100000: episode: 7172, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000214, mae: 0.003988, mean_q: 0.021260
 71676/100000: episode: 7173, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000033, mae: 0.002090, mean_q: 0.021123
 71686/100000: episode: 7174, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000224, mae: 0.003623, mean_q: 0.019669
 71696/100000: episode: 7175, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000217, mae: 0.003404, mean_q: 0.021581
 71706/100000: episode: 7176, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000222, mae: 0.003608, mean_q: 0.021620
 71716/100000: episode: 7177, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000015, mae: 0.001877, mean_q: 0.021257
 71726/100000: episode: 7178, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000018, mae: 0.002532, mean_q: 0.020018
 71736/100000: episode: 7179, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.002722, mean_q: 0.019146
 71746/100000: episode: 7180, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002503, mean_q: 0.020107
 71756/100000: episode: 7181, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000209, mae: 0.004165, mean_q: 0.021852
 71766/100000: episode: 7182, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002216, mean_q: 0.020499
 71776/100000: episode: 7183, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000264, mae: 0.004147, mean_q: 0.020521
 71786/100000: episode: 7184, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000035, mae: 0.002781, mean_q: 0.021835
 71796/100000: episode: 7185, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000082, mae: 0.004008, mean_q: 0.019379
 71806/100000: episode: 7186, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000042, mae: 0.002830, mean_q: 0.021087
 71816/100000: episode: 7187, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000036, mae: 0.002317, mean_q: 0.020835
 71826/100000: episode: 7188, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000061, mae: 0.003567, mean_q: 0.019380
 71836/100000: episode: 7189, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000221, mae: 0.002782, mean_q: 0.020617
 71846/100000: episode: 7190, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000205, mae: 0.004317, mean_q: 0.022373
 71856/100000: episode: 7191, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000020, mae: 0.002742, mean_q: 0.020015
 71866/100000: episode: 7192, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.002814, mean_q: 0.019593
 71876/100000: episode: 7193, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000242, mae: 0.003258, mean_q: 0.020289
 71886/100000: episode: 7194, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000245, mae: 0.004781, mean_q: 0.022044
 71896/100000: episode: 7195, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000036, mae: 0.002454, mean_q: 0.020518
 71906/100000: episode: 7196, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000018, mae: 0.002184, mean_q: 0.020111
 71916/100000: episode: 7197, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000244, mae: 0.003909, mean_q: 0.021242
 71926/100000: episode: 7198, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000243, mae: 0.003575, mean_q: 0.021170
 71936/100000: episode: 7199, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000225, mae: 0.003507, mean_q: 0.021284
 71946/100000: episode: 7200, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000395, mae: 0.005393, mean_q: 0.022300
 71956/100000: episode: 7201, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000035, mae: 0.002652, mean_q: 0.021642
 71966/100000: episode: 7202, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000055, mae: 0.002635, mean_q: 0.021217
 71976/100000: episode: 7203, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000202, mae: 0.002873, mean_q: 0.021467
 71986/100000: episode: 7204, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000016, mae: 0.001990, mean_q: 0.020950
 71996/100000: episode: 7205, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000055, mae: 0.003402, mean_q: 0.019520
Step 72000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.72000.hdf5
 72006/100000: episode: 7206, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000015, mae: 0.001730, mean_q: 0.020659
 72016/100000: episode: 7207, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000017, mae: 0.002090, mean_q: 0.020370
 72026/100000: episode: 7208, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.002702, mean_q: 0.020027
 72036/100000: episode: 7209, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000078, mae: 0.003678, mean_q: 0.021791
 72046/100000: episode: 7210, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000037, mae: 0.002538, mean_q: 0.020429
 72056/100000: episode: 7211, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000040, mae: 0.002543, mean_q: 0.020507
 72066/100000: episode: 7212, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000014, mae: 0.001846, mean_q: 0.020455
 72076/100000: episode: 7213, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000081, mae: 0.003954, mean_q: 0.019362
 72086/100000: episode: 7214, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000055, mae: 0.002671, mean_q: 0.021225
 72096/100000: episode: 7215, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000264, mae: 0.004597, mean_q: 0.021757
 72106/100000: episode: 7216, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.002618, mean_q: 0.021336
 72116/100000: episode: 7217, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000225, mae: 0.003980, mean_q: 0.019472
 72126/100000: episode: 7218, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000097, mae: 0.004463, mean_q: 0.022425
 72136/100000: episode: 7219, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000017, mae: 0.002536, mean_q: 0.020401
 72146/100000: episode: 7220, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000032, mae: 0.003148, mean_q: 0.018857
 72156/100000: episode: 7221, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000060, mae: 0.003517, mean_q: 0.021573
 72166/100000: episode: 7222, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000009, mae: 0.001490, mean_q: 0.020785
 72176/100000: episode: 7223, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000018, mae: 0.003087, mean_q: 0.019051
 72186/100000: episode: 7224, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000399, mae: 0.005185, mean_q: 0.021321
 72196/100000: episode: 7225, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000018, mae: 0.002737, mean_q: 0.021513
 72206/100000: episode: 7226, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000221, mae: 0.003263, mean_q: 0.019963
 72216/100000: episode: 7227, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000013, mae: 0.001710, mean_q: 0.020788
 72226/100000: episode: 7228, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000224, mae: 0.003348, mean_q: 0.020119
 72236/100000: episode: 7229, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000060, mae: 0.003634, mean_q: 0.021935
 72246/100000: episode: 7230, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000018, mae: 0.001983, mean_q: 0.020549
 72256/100000: episode: 7231, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000227, mae: 0.003680, mean_q: 0.020821
 72266/100000: episode: 7232, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000055, mae: 0.002642, mean_q: 0.020996
 72276/100000: episode: 7233, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000015, mae: 0.002204, mean_q: 0.019954
 72286/100000: episode: 7234, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000202, mae: 0.002935, mean_q: 0.020136
 72296/100000: episode: 7235, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000036, mae: 0.002345, mean_q: 0.020829
 72306/100000: episode: 7236, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000405, mae: 0.005417, mean_q: 0.021301
 72316/100000: episode: 7237, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000221, mae: 0.004153, mean_q: 0.022511
 72326/100000: episode: 7238, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000203, mae: 0.002801, mean_q: 0.021049
 72336/100000: episode: 7239, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000013, mae: 0.001769, mean_q: 0.020474
 72346/100000: episode: 7240, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000063, mae: 0.003008, mean_q: 0.020670
 72356/100000: episode: 7241, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000013, mae: 0.001893, mean_q: 0.020422
 72366/100000: episode: 7242, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000226, mae: 0.003790, mean_q: 0.019738
 72376/100000: episode: 7243, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000095, mae: 0.004282, mean_q: 0.022464
[Info] 1-TH LEVEL FOUND: 0.01912611909210682, Considering 100/100 traces
 72386/100000: episode: 7244, duration: 0.739s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.002405, mean_q: 0.020600
[Info] 2-TH LEVEL FOUND: 0.021157901734113693, Considering 100/100 traces
 72396/100000: episode: 7245, duration: 0.716s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000056, mae: 0.003081, mean_q: 0.019647
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021157901734113693
2
 72406/100000: episode: 7246, duration: 0.496s, episode steps: 10, steps per second: 20, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000036, mae: 0.002558, mean_q: 0.021533
 72416/100000: episode: 7247, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000032, mae: 0.002042, mean_q: 0.020153
 72426/100000: episode: 7248, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000223, mae: 0.003188, mean_q: 0.020465
 72436/100000: episode: 7249, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000079, mae: 0.004417, mean_q: 0.022655
 72446/100000: episode: 7250, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000223, mae: 0.003217, mean_q: 0.021369
 72456/100000: episode: 7251, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000231, mae: 0.004127, mean_q: 0.021570
 72466/100000: episode: 7252, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000245, mae: 0.003792, mean_q: 0.021596
 72476/100000: episode: 7253, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000223, mae: 0.003917, mean_q: 0.022213
 72486/100000: episode: 7254, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000053, mae: 0.002323, mean_q: 0.021491
 72496/100000: episode: 7255, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000032, mae: 0.001765, mean_q: 0.021286
 72506/100000: episode: 7256, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000034, mae: 0.002266, mean_q: 0.020723
 72516/100000: episode: 7257, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000031, mae: 0.001821, mean_q: 0.020773
 72526/100000: episode: 7258, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000433, mae: 0.005250, mean_q: 0.021887
 72536/100000: episode: 7259, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000040, mae: 0.003977, mean_q: 0.023427
 72546/100000: episode: 7260, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000052, mae: 0.003080, mean_q: 0.020034
 72556/100000: episode: 7261, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000012, mae: 0.001881, mean_q: 0.020487
 72566/100000: episode: 7262, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000204, mae: 0.002944, mean_q: 0.021462
 72576/100000: episode: 7263, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000197, mae: 0.002559, mean_q: 0.021702
 72586/100000: episode: 7264, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000287, mae: 0.004787, mean_q: 0.021829
 72596/100000: episode: 7265, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000450, mae: 0.007859, mean_q: 0.024741
 72606/100000: episode: 7266, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.003263, mean_q: 0.023211
 72616/100000: episode: 7267, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000284, mae: 0.004605, mean_q: 0.021738
 72626/100000: episode: 7268, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000037, mae: 0.002944, mean_q: 0.023044
 72636/100000: episode: 7269, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000203, mae: 0.003500, mean_q: 0.020790
 72646/100000: episode: 7270, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000058, mae: 0.002877, mean_q: 0.021925
 72656/100000: episode: 7271, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000204, mae: 0.002851, mean_q: 0.022008
 72666/100000: episode: 7272, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000020, mae: 0.002032, mean_q: 0.022029
 72676/100000: episode: 7273, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000222, mae: 0.003119, mean_q: 0.021792
 72686/100000: episode: 7274, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000058, mae: 0.002876, mean_q: 0.022212
 72696/100000: episode: 7275, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000058, mae: 0.003556, mean_q: 0.020535
 72706/100000: episode: 7276, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000015, mae: 0.001800, mean_q: 0.021725
 72716/100000: episode: 7277, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000039, mae: 0.002467, mean_q: 0.021839
 72726/100000: episode: 7278, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000013, mae: 0.002286, mean_q: 0.020816
 72736/100000: episode: 7279, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000223, mae: 0.003681, mean_q: 0.020863
 72746/100000: episode: 7280, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000014, mae: 0.002149, mean_q: 0.022296
 72756/100000: episode: 7281, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000014, mae: 0.002544, mean_q: 0.020228
 72766/100000: episode: 7282, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000227, mae: 0.003767, mean_q: 0.021657
 72776/100000: episode: 7283, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000059, mae: 0.003616, mean_q: 0.022916
 72786/100000: episode: 7284, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000057, mae: 0.002798, mean_q: 0.021046
 72796/100000: episode: 7285, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000032, mae: 0.001877, mean_q: 0.021262
 72806/100000: episode: 7286, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000012, mae: 0.001917, mean_q: 0.020856
 72816/100000: episode: 7287, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000053, mae: 0.002559, mean_q: 0.020861
 72826/100000: episode: 7288, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000016, mae: 0.002005, mean_q: 0.020908
 72836/100000: episode: 7289, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.002246, mean_q: 0.020952
 72846/100000: episode: 7290, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.001861, mean_q: 0.021305
 72856/100000: episode: 7291, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000204, mae: 0.003340, mean_q: 0.020493
 72866/100000: episode: 7292, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000225, mae: 0.003861, mean_q: 0.022118
 72876/100000: episode: 7293, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000030, mae: 0.002072, mean_q: 0.022228
 72886/100000: episode: 7294, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000098, mae: 0.003798, mean_q: 0.020455
 72896/100000: episode: 7295, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000102, mae: 0.004957, mean_q: 0.023249
 72906/100000: episode: 7296, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000012, mae: 0.001928, mean_q: 0.020993
 72916/100000: episode: 7297, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000243, mae: 0.004068, mean_q: 0.020460
 72926/100000: episode: 7298, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000206, mae: 0.004615, mean_q: 0.023493
 72936/100000: episode: 7299, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000031, mae: 0.002360, mean_q: 0.021053
 72946/100000: episode: 7300, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000264, mae: 0.004413, mean_q: 0.021177
 72956/100000: episode: 7301, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000057, mae: 0.003391, mean_q: 0.022833
 72966/100000: episode: 7302, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000037, mae: 0.002387, mean_q: 0.021815
 72976/100000: episode: 7303, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000013, mae: 0.002629, mean_q: 0.020184
 72986/100000: episode: 7304, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000080, mae: 0.003452, mean_q: 0.020940
 72996/100000: episode: 7305, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000037, mae: 0.002348, mean_q: 0.021412
Step 73000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.73000.hdf5
 73006/100000: episode: 7306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000228, mae: 0.003893, mean_q: 0.020620
 73016/100000: episode: 7307, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000013, mae: 0.001732, mean_q: 0.021880
 73026/100000: episode: 7308, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000120, mae: 0.003962, mean_q: 0.020846
 73036/100000: episode: 7309, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000064, mae: 0.003281, mean_q: 0.021799
 73046/100000: episode: 7310, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000057, mae: 0.002569, mean_q: 0.021661
 73056/100000: episode: 7311, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000247, mae: 0.003817, mean_q: 0.021229
 73066/100000: episode: 7312, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000039, mae: 0.003051, mean_q: 0.022423
 73076/100000: episode: 7313, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000202, mae: 0.003173, mean_q: 0.020589
 73086/100000: episode: 7314, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000040, mae: 0.002568, mean_q: 0.021121
 73096/100000: episode: 7315, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000021, mae: 0.002757, mean_q: 0.020380
 73106/100000: episode: 7316, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000043, mae: 0.002856, mean_q: 0.020717
 73116/100000: episode: 7317, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000226, mae: 0.003438, mean_q: 0.021203
 73126/100000: episode: 7318, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000204, mae: 0.003180, mean_q: 0.021858
 73136/100000: episode: 7319, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000079, mae: 0.003773, mean_q: 0.022452
 73146/100000: episode: 7320, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.001828, mean_q: 0.021791
 73156/100000: episode: 7321, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000014, mae: 0.002479, mean_q: 0.020358
 73166/100000: episode: 7322, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.003114, mean_q: 0.019786
 73176/100000: episode: 7323, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000394, mae: 0.004253, mean_q: 0.021042
 73186/100000: episode: 7324, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000637, mae: 0.010309, mean_q: 0.026455
 73196/100000: episode: 7325, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000271, mae: 0.006087, mean_q: 0.024004
 73206/100000: episode: 7326, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000032, mae: 0.002410, mean_q: 0.020940
 73216/100000: episode: 7327, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000246, mae: 0.004106, mean_q: 0.022284
 73226/100000: episode: 7328, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000406, mae: 0.004158, mean_q: 0.022351
 73236/100000: episode: 7329, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000058, mae: 0.003585, mean_q: 0.023387
 73246/100000: episode: 7330, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000080, mae: 0.003625, mean_q: 0.021559
 73256/100000: episode: 7331, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000055, mae: 0.002440, mean_q: 0.021731
 73266/100000: episode: 7332, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000059, mae: 0.002618, mean_q: 0.022140
 73276/100000: episode: 7333, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000244, mae: 0.003724, mean_q: 0.022075
 73286/100000: episode: 7334, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000052, mae: 0.002445, mean_q: 0.022700
 73296/100000: episode: 7335, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000035, mae: 0.002319, mean_q: 0.021609
 73306/100000: episode: 7336, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000082, mae: 0.003813, mean_q: 0.021067
 73316/100000: episode: 7337, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000289, mae: 0.006603, mean_q: 0.024389
 73326/100000: episode: 7338, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000222, mae: 0.004391, mean_q: 0.024008
 73336/100000: episode: 7339, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [0.000, 10.000], loss: 0.000017, mae: 0.002279, mean_q: 0.021685
 73346/100000: episode: 7340, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000016, mae: 0.002357, mean_q: 0.021264
 73356/100000: episode: 7341, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000017, mae: 0.002149, mean_q: 0.021655
 73366/100000: episode: 7342, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000222, mae: 0.003281, mean_q: 0.022350
 73376/100000: episode: 7343, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000055, mae: 0.003291, mean_q: 0.023457
 73386/100000: episode: 7344, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000016, mae: 0.002408, mean_q: 0.021316
 73396/100000: episode: 7345, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000204, mae: 0.003000, mean_q: 0.021756
[Info] 1-TH LEVEL FOUND: 0.02502923086285591, Considering 100/100 traces
 73406/100000: episode: 7346, duration: 0.743s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000415, mae: 0.006306, mean_q: 0.024155
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02502923086285591
1
 73416/100000: episode: 7347, duration: 0.515s, episode steps: 10, steps per second: 19, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000470, mae: 0.006727, mean_q: 0.023973
 73426/100000: episode: 7348, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000240, mae: 0.004499, mean_q: 0.024121
 73436/100000: episode: 7349, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000414, mae: 0.005075, mean_q: 0.023459
 73446/100000: episode: 7350, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000034, mae: 0.002993, mean_q: 0.024197
 73456/100000: episode: 7351, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000059, mae: 0.002729, mean_q: 0.022839
 73466/100000: episode: 7352, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000077, mae: 0.003302, mean_q: 0.022059
 73476/100000: episode: 7353, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000244, mae: 0.004829, mean_q: 0.024311
 73486/100000: episode: 7354, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000264, mae: 0.005713, mean_q: 0.024946
 73496/100000: episode: 7355, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000031, mae: 0.001935, mean_q: 0.022828
 73506/100000: episode: 7356, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000245, mae: 0.003991, mean_q: 0.022843
 73516/100000: episode: 7357, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002966, mean_q: 0.023830
 73526/100000: episode: 7358, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000201, mae: 0.002832, mean_q: 0.022741
 73536/100000: episode: 7359, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000022, mae: 0.002406, mean_q: 0.023495
 73546/100000: episode: 7360, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.002941, mean_q: 0.021767
 73556/100000: episode: 7361, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000055, mae: 0.003056, mean_q: 0.021950
 73566/100000: episode: 7362, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000017, mae: 0.001897, mean_q: 0.023087
 73576/100000: episode: 7363, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000248, mae: 0.004074, mean_q: 0.023143
 73586/100000: episode: 7364, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000035, mae: 0.002202, mean_q: 0.023236
 73596/100000: episode: 7365, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000080, mae: 0.003272, mean_q: 0.022676
 73606/100000: episode: 7366, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000014, mae: 0.001675, mean_q: 0.022558
 73616/100000: episode: 7367, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000452, mae: 0.005551, mean_q: 0.022973
 73626/100000: episode: 7368, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000042, mae: 0.003769, mean_q: 0.024579
 73636/100000: episode: 7369, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000030, mae: 0.002661, mean_q: 0.021655
 73646/100000: episode: 7370, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000060, mae: 0.003619, mean_q: 0.021621
 73656/100000: episode: 7371, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000246, mae: 0.004636, mean_q: 0.023835
 73666/100000: episode: 7372, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000035, mae: 0.002246, mean_q: 0.022914
 73676/100000: episode: 7373, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000020, mae: 0.002173, mean_q: 0.023071
 73686/100000: episode: 7374, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000392, mae: 0.004012, mean_q: 0.022903
 73696/100000: episode: 7375, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000421, mae: 0.004006, mean_q: 0.023167
 73706/100000: episode: 7376, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000075, mae: 0.004875, mean_q: 0.025744
 73716/100000: episode: 7377, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000054, mae: 0.003117, mean_q: 0.022727
 73726/100000: episode: 7378, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000033, mae: 0.003098, mean_q: 0.021405
 73736/100000: episode: 7379, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000021, mae: 0.002369, mean_q: 0.022728
 73746/100000: episode: 7380, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000246, mae: 0.004493, mean_q: 0.021860
 73756/100000: episode: 7381, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000429, mae: 0.005436, mean_q: 0.023745
 73766/100000: episode: 7382, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000039, mae: 0.003333, mean_q: 0.024576
 73776/100000: episode: 7383, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000116, mae: 0.003814, mean_q: 0.022672
 73786/100000: episode: 7384, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000200, mae: 0.002763, mean_q: 0.023169
 73796/100000: episode: 7385, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000058, mae: 0.002695, mean_q: 0.023224
 73806/100000: episode: 7386, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.002203, mean_q: 0.022234
 73816/100000: episode: 7387, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000051, mae: 0.002368, mean_q: 0.022637
 73826/100000: episode: 7388, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000247, mae: 0.004695, mean_q: 0.024107
 73836/100000: episode: 7389, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000059, mae: 0.002916, mean_q: 0.023408
 73846/100000: episode: 7390, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000227, mae: 0.003948, mean_q: 0.022309
 73856/100000: episode: 7391, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000245, mae: 0.004706, mean_q: 0.024191
 73866/100000: episode: 7392, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000428, mae: 0.005963, mean_q: 0.024711
 73876/100000: episode: 7393, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000220, mae: 0.003441, mean_q: 0.023970
 73886/100000: episode: 7394, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000018, mae: 0.002078, mean_q: 0.023057
 73896/100000: episode: 7395, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000034, mae: 0.002508, mean_q: 0.022461
 73906/100000: episode: 7396, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000057, mae: 0.002666, mean_q: 0.023081
 73916/100000: episode: 7397, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000036, mae: 0.002710, mean_q: 0.022331
 73926/100000: episode: 7398, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000200, mae: 0.002749, mean_q: 0.023201
 73936/100000: episode: 7399, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000079, mae: 0.003244, mean_q: 0.022855
 73946/100000: episode: 7400, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000014, mae: 0.001997, mean_q: 0.022484
 73956/100000: episode: 7401, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002869, mean_q: 0.022077
 73966/100000: episode: 7402, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000095, mae: 0.003474, mean_q: 0.023509
 73976/100000: episode: 7403, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000039, mae: 0.002872, mean_q: 0.022912
 73986/100000: episode: 7404, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000035, mae: 0.002979, mean_q: 0.021622
 73996/100000: episode: 7405, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000040, mae: 0.002521, mean_q: 0.022540
Step 74000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.74000.hdf5
 74006/100000: episode: 7406, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000446, mae: 0.005657, mean_q: 0.023827
 74016/100000: episode: 7407, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000036, mae: 0.002936, mean_q: 0.024216
 74026/100000: episode: 7408, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000078, mae: 0.003602, mean_q: 0.021885
 74036/100000: episode: 7409, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000257, mae: 0.004209, mean_q: 0.023980
 74046/100000: episode: 7410, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000385, mae: 0.005898, mean_q: 0.025721
 74056/100000: episode: 7411, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000201, mae: 0.003062, mean_q: 0.023149
 74066/100000: episode: 7412, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000038, mae: 0.002934, mean_q: 0.022343
 74076/100000: episode: 7413, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000241, mae: 0.003814, mean_q: 0.023068
 74086/100000: episode: 7414, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000633, mae: 0.008778, mean_q: 0.026236
 74096/100000: episode: 7415, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000036, mae: 0.003340, mean_q: 0.025080
 74106/100000: episode: 7416, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000094, mae: 0.003930, mean_q: 0.022293
 74116/100000: episode: 7417, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002436, mean_q: 0.023170
 74126/100000: episode: 7418, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000040, mae: 0.002491, mean_q: 0.023220
 74136/100000: episode: 7419, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000054, mae: 0.002663, mean_q: 0.022871
 74146/100000: episode: 7420, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000222, mae: 0.003597, mean_q: 0.022827
 74156/100000: episode: 7421, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000020, mae: 0.002498, mean_q: 0.023019
 74166/100000: episode: 7422, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.003401, mean_q: 0.021816
 74176/100000: episode: 7423, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000414, mae: 0.006233, mean_q: 0.025344
 74186/100000: episode: 7424, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.003055, mean_q: 0.023240
 74196/100000: episode: 7425, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000245, mae: 0.004704, mean_q: 0.021693
 74206/100000: episode: 7426, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000432, mae: 0.006308, mean_q: 0.024686
 74216/100000: episode: 7427, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000628, mae: 0.008375, mean_q: 0.026174
 74226/100000: episode: 7428, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000065, mae: 0.004761, mean_q: 0.025885
 74236/100000: episode: 7429, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000228, mae: 0.004004, mean_q: 0.023668
 74246/100000: episode: 7430, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000042, mae: 0.003780, mean_q: 0.022145
 74256/100000: episode: 7431, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000078, mae: 0.003324, mean_q: 0.023080
 74266/100000: episode: 7432, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000427, mae: 0.005216, mean_q: 0.024335
 74276/100000: episode: 7433, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.003521, mean_q: 0.024835
 74286/100000: episode: 7434, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000033, mae: 0.002068, mean_q: 0.023437
 74296/100000: episode: 7435, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000057, mae: 0.003079, mean_q: 0.023024
 74306/100000: episode: 7436, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000037, mae: 0.002450, mean_q: 0.023204
 74316/100000: episode: 7437, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000016, mae: 0.002459, mean_q: 0.022654
 74326/100000: episode: 7438, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.003039, mean_q: 0.022319
 74336/100000: episode: 7439, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000015, mae: 0.002119, mean_q: 0.022647
 74346/100000: episode: 7440, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000058, mae: 0.003553, mean_q: 0.022129
 74356/100000: episode: 7441, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.002708, mean_q: 0.022170
 74366/100000: episode: 7442, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000018, mae: 0.002376, mean_q: 0.022356
 74376/100000: episode: 7443, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000099, mae: 0.003974, mean_q: 0.023545
 74386/100000: episode: 7444, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000017, mae: 0.002203, mean_q: 0.023705
 74396/100000: episode: 7445, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000389, mae: 0.004424, mean_q: 0.022190
 74406/100000: episode: 7446, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000039, mae: 0.002603, mean_q: 0.023611
[Info] 1-TH LEVEL FOUND: 0.02246747724711895, Considering 100/100 traces
 74416/100000: episode: 7447, duration: 0.746s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000022, mae: 0.002357, mean_q: 0.023307
[Info] 2-TH LEVEL FOUND: 0.02288377843797207, Considering 100/100 traces
 74426/100000: episode: 7448, duration: 0.746s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000202, mae: 0.003052, mean_q: 0.022475
[Info] 3-TH LEVEL FOUND: 0.024397438392043114, Considering 100/100 traces
 74436/100000: episode: 7449, duration: 0.718s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000217, mae: 0.002675, mean_q: 0.023097
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024397438392043114
3
 74446/100000: episode: 7450, duration: 0.495s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000072, mae: 0.003685, mean_q: 0.024644
 74456/100000: episode: 7451, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000033, mae: 0.002156, mean_q: 0.023090
 74466/100000: episode: 7452, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000267, mae: 0.004759, mean_q: 0.023469
 74476/100000: episode: 7453, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000231, mae: 0.004886, mean_q: 0.024468
 74486/100000: episode: 7454, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000024, mae: 0.003657, mean_q: 0.021641
 74496/100000: episode: 7455, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000031, mae: 0.002287, mean_q: 0.022136
 74506/100000: episode: 7456, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000056, mae: 0.002487, mean_q: 0.023256
 74516/100000: episode: 7457, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000037, mae: 0.002333, mean_q: 0.022853
 74526/100000: episode: 7458, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.002322, mean_q: 0.022189
 74536/100000: episode: 7459, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000223, mae: 0.003371, mean_q: 0.022564
 74546/100000: episode: 7460, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000021, mae: 0.002510, mean_q: 0.023607
 74556/100000: episode: 7461, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000061, mae: 0.003352, mean_q: 0.022322
 74566/100000: episode: 7462, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000014, mae: 0.002616, mean_q: 0.021448
 74576/100000: episode: 7463, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000036, mae: 0.002777, mean_q: 0.021822
 74586/100000: episode: 7464, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000258, mae: 0.004040, mean_q: 0.023174
 74596/100000: episode: 7465, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000018, mae: 0.002400, mean_q: 0.023058
 74606/100000: episode: 7466, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000599, mae: 0.006460, mean_q: 0.021131
 74616/100000: episode: 7467, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000210, mae: 0.005739, mean_q: 0.025703
 74626/100000: episode: 7468, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002852, mean_q: 0.022935
 74636/100000: episode: 7469, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000031, mae: 0.002103, mean_q: 0.022202
 74646/100000: episode: 7470, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000015, mae: 0.001723, mean_q: 0.022610
 74656/100000: episode: 7471, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000029, mae: 0.002193, mean_q: 0.021792
 74666/100000: episode: 7472, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000019, mae: 0.002656, mean_q: 0.021747
 74676/100000: episode: 7473, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.002933, mean_q: 0.021779
 74686/100000: episode: 7474, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000074, mae: 0.003147, mean_q: 0.023274
 74696/100000: episode: 7475, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000390, mae: 0.003988, mean_q: 0.022784
 74706/100000: episode: 7476, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000576, mae: 0.005965, mean_q: 0.023680
 74716/100000: episode: 7477, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000233, mae: 0.007166, mean_q: 0.026878
 74726/100000: episode: 7478, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000207, mae: 0.003907, mean_q: 0.022842
 74736/100000: episode: 7479, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000392, mae: 0.004628, mean_q: 0.023313
 74746/100000: episode: 7480, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000060, mae: 0.003349, mean_q: 0.023707
 74756/100000: episode: 7481, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000389, mae: 0.004228, mean_q: 0.022969
 74766/100000: episode: 7482, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000050, mae: 0.002642, mean_q: 0.024186
 74776/100000: episode: 7483, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000054, mae: 0.002661, mean_q: 0.022773
 74786/100000: episode: 7484, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000248, mae: 0.004579, mean_q: 0.022236
 74796/100000: episode: 7485, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000388, mae: 0.004997, mean_q: 0.024420
 74806/100000: episode: 7486, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000198, mae: 0.002914, mean_q: 0.023671
 74816/100000: episode: 7487, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002570, mean_q: 0.022444
 74826/100000: episode: 7488, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000198, mae: 0.002583, mean_q: 0.023092
 74836/100000: episode: 7489, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000228, mae: 0.003924, mean_q: 0.023529
 74846/100000: episode: 7490, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000018, mae: 0.001961, mean_q: 0.022989
 74856/100000: episode: 7491, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000033, mae: 0.002098, mean_q: 0.022681
 74866/100000: episode: 7492, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000204, mae: 0.003358, mean_q: 0.022548
 74876/100000: episode: 7493, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000078, mae: 0.003275, mean_q: 0.023215
 74886/100000: episode: 7494, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000207, mae: 0.003639, mean_q: 0.022481
 74896/100000: episode: 7495, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000228, mae: 0.003953, mean_q: 0.022864
 74906/100000: episode: 7496, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000018, mae: 0.002327, mean_q: 0.022982
 74916/100000: episode: 7497, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000038, mae: 0.003788, mean_q: 0.021052
 74926/100000: episode: 7498, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000034, mae: 0.002352, mean_q: 0.022124
 74936/100000: episode: 7499, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000239, mae: 0.003694, mean_q: 0.023476
 74946/100000: episode: 7500, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000056, mae: 0.002947, mean_q: 0.023507
 74956/100000: episode: 7501, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000429, mae: 0.004977, mean_q: 0.021998
 74966/100000: episode: 7502, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000201, mae: 0.004117, mean_q: 0.024604
 74976/100000: episode: 7503, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000431, mae: 0.005144, mean_q: 0.023332
 74986/100000: episode: 7504, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000240, mae: 0.004393, mean_q: 0.024269
 74996/100000: episode: 7505, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000019, mae: 0.002522, mean_q: 0.023737
Step 75000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.75000.hdf5
 75006/100000: episode: 7506, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000220, mae: 0.003368, mean_q: 0.022443
 75016/100000: episode: 7507, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000018, mae: 0.001945, mean_q: 0.022962
 75026/100000: episode: 7508, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000015, mae: 0.003125, mean_q: 0.021239
 75036/100000: episode: 7509, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000243, mae: 0.004086, mean_q: 0.022077
 75046/100000: episode: 7510, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000037, mae: 0.003367, mean_q: 0.024406
 75056/100000: episode: 7511, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000049, mae: 0.002185, mean_q: 0.022354
 75066/100000: episode: 7512, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000220, mae: 0.003115, mean_q: 0.022674
 75076/100000: episode: 7513, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000076, mae: 0.003359, mean_q: 0.023764
 75086/100000: episode: 7514, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000098, mae: 0.003819, mean_q: 0.023698
 75096/100000: episode: 7515, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000295, mae: 0.004609, mean_q: 0.023823
 75106/100000: episode: 7516, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000039, mae: 0.003206, mean_q: 0.024411
 75116/100000: episode: 7517, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000205, mae: 0.004112, mean_q: 0.021720
 75126/100000: episode: 7518, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000204, mae: 0.003339, mean_q: 0.022548
 75136/100000: episode: 7519, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000210, mae: 0.004972, mean_q: 0.025135
 75146/100000: episode: 7520, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000061, mae: 0.003666, mean_q: 0.022596
 75156/100000: episode: 7521, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000034, mae: 0.002879, mean_q: 0.021772
 75166/100000: episode: 7522, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000019, mae: 0.001981, mean_q: 0.022811
 75176/100000: episode: 7523, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000225, mae: 0.003749, mean_q: 0.022212
 75186/100000: episode: 7524, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000035, mae: 0.002499, mean_q: 0.023748
 75196/100000: episode: 7525, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.002218, mean_q: 0.022888
 75206/100000: episode: 7526, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000014, mae: 0.002324, mean_q: 0.021921
 75216/100000: episode: 7527, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000086, mae: 0.003967, mean_q: 0.022152
 75226/100000: episode: 7528, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000057, mae: 0.002794, mean_q: 0.023190
 75236/100000: episode: 7529, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000033, mae: 0.002199, mean_q: 0.022367
 75246/100000: episode: 7530, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000261, mae: 0.004445, mean_q: 0.021622
 75256/100000: episode: 7531, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000032, mae: 0.002486, mean_q: 0.023653
 75266/100000: episode: 7532, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000060, mae: 0.003414, mean_q: 0.021776
 75276/100000: episode: 7533, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000408, mae: 0.004492, mean_q: 0.022664
 75286/100000: episode: 7534, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000205, mae: 0.004115, mean_q: 0.024184
 75296/100000: episode: 7535, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000054, mae: 0.002540, mean_q: 0.022547
 75306/100000: episode: 7536, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000018, mae: 0.003128, mean_q: 0.021074
 75316/100000: episode: 7537, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.002245, mean_q: 0.021768
 75326/100000: episode: 7538, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000034, mae: 0.002429, mean_q: 0.021745
 75336/100000: episode: 7539, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000205, mae: 0.003078, mean_q: 0.022528
 75346/100000: episode: 7540, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000201, mae: 0.002717, mean_q: 0.022066
 75356/100000: episode: 7541, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000201, mae: 0.003395, mean_q: 0.023435
 75366/100000: episode: 7542, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000035, mae: 0.002630, mean_q: 0.022191
 75376/100000: episode: 7543, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000221, mae: 0.003674, mean_q: 0.021366
 75386/100000: episode: 7544, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000252, mae: 0.005708, mean_q: 0.024351
 75396/100000: episode: 7545, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000039, mae: 0.002834, mean_q: 0.022769
 75406/100000: episode: 7546, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000204, mae: 0.003646, mean_q: 0.021463
 75416/100000: episode: 7547, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000204, mae: 0.003333, mean_q: 0.021759
 75426/100000: episode: 7548, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000016, mae: 0.002039, mean_q: 0.021900
 75436/100000: episode: 7549, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000031, mae: 0.002106, mean_q: 0.021578
[Info] 1-TH LEVEL FOUND: 0.022082993760704994, Considering 100/100 traces
 75446/100000: episode: 7550, duration: 0.750s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000074, mae: 0.002901, mean_q: 0.021673
[Info] 2-TH LEVEL FOUND: 0.02273821458220482, Considering 100/100 traces
 75456/100000: episode: 7551, duration: 0.648s, episode steps: 10, steps per second: 15, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000043, mae: 0.002888, mean_q: 0.022832
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02273821458220482
2
 75466/100000: episode: 7552, duration: 0.578s, episode steps: 10, steps per second: 17, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000017, mae: 0.002125, mean_q: 0.021774
 75476/100000: episode: 7553, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000205, mae: 0.003511, mean_q: 0.021337
 75486/100000: episode: 7554, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000226, mae: 0.003558, mean_q: 0.022453
 75496/100000: episode: 7555, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000046, mae: 0.003486, mean_q: 0.022945
 75506/100000: episode: 7556, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000243, mae: 0.003985, mean_q: 0.021279
 75516/100000: episode: 7557, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000018, mae: 0.002622, mean_q: 0.023222
 75526/100000: episode: 7558, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000048, mae: 0.001718, mean_q: 0.022482
 75536/100000: episode: 7559, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000248, mae: 0.004042, mean_q: 0.022256
 75546/100000: episode: 7560, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000019, mae: 0.002247, mean_q: 0.022236
 75556/100000: episode: 7561, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000059, mae: 0.003004, mean_q: 0.021535
 75566/100000: episode: 7562, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000205, mae: 0.003391, mean_q: 0.022762
 75576/100000: episode: 7563, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000205, mae: 0.003340, mean_q: 0.021410
 75586/100000: episode: 7564, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000016, mae: 0.001857, mean_q: 0.022242
 75596/100000: episode: 7565, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000242, mae: 0.003725, mean_q: 0.022463
 75606/100000: episode: 7566, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.002488, mean_q: 0.022497
 75616/100000: episode: 7567, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000244, mae: 0.004013, mean_q: 0.022879
 75626/100000: episode: 7568, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000071, mae: 0.002826, mean_q: 0.022988
 75636/100000: episode: 7569, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.002096, mean_q: 0.021547
 75646/100000: episode: 7570, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000038, mae: 0.002896, mean_q: 0.021475
 75656/100000: episode: 7571, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.002216, mean_q: 0.021291
 75666/100000: episode: 7572, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000621, mae: 0.006911, mean_q: 0.023052
 75676/100000: episode: 7573, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000041, mae: 0.003984, mean_q: 0.024423
 75686/100000: episode: 7574, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000222, mae: 0.003656, mean_q: 0.021469
 75696/100000: episode: 7575, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000042, mae: 0.002973, mean_q: 0.022729
 75706/100000: episode: 7576, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000246, mae: 0.004217, mean_q: 0.021563
 75716/100000: episode: 7577, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000057, mae: 0.002576, mean_q: 0.022482
 75726/100000: episode: 7578, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000033, mae: 0.001969, mean_q: 0.022148
 75736/100000: episode: 7579, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002966, mean_q: 0.021510
 75746/100000: episode: 7580, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000241, mae: 0.003454, mean_q: 0.022270
 75756/100000: episode: 7581, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000226, mae: 0.004645, mean_q: 0.023815
 75766/100000: episode: 7582, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000611, mae: 0.006893, mean_q: 0.024057
 75776/100000: episode: 7583, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000398, mae: 0.008308, mean_q: 0.027289
 75786/100000: episode: 7584, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000237, mae: 0.003926, mean_q: 0.023947
 75796/100000: episode: 7585, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000075, mae: 0.003631, mean_q: 0.021908
 75806/100000: episode: 7586, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000040, mae: 0.003031, mean_q: 0.022038
 75816/100000: episode: 7587, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000452, mae: 0.006248, mean_q: 0.023996
 75826/100000: episode: 7588, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000204, mae: 0.003812, mean_q: 0.024463
 75836/100000: episode: 7589, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000225, mae: 0.003633, mean_q: 0.022647
 75846/100000: episode: 7590, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000206, mae: 0.003051, mean_q: 0.023298
 75856/100000: episode: 7591, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000205, mae: 0.003990, mean_q: 0.024327
 75866/100000: episode: 7592, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000021, mae: 0.002441, mean_q: 0.023968
 75876/100000: episode: 7593, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000014, mae: 0.002569, mean_q: 0.021989
 75886/100000: episode: 7594, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000207, mae: 0.003744, mean_q: 0.022234
 75896/100000: episode: 7595, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000039, mae: 0.002625, mean_q: 0.023565
 75906/100000: episode: 7596, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000036, mae: 0.003004, mean_q: 0.021893
 75916/100000: episode: 7597, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000093, mae: 0.003413, mean_q: 0.022615
 75926/100000: episode: 7598, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000072, mae: 0.003718, mean_q: 0.024592
 75936/100000: episode: 7599, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000052, mae: 0.002405, mean_q: 0.023058
 75946/100000: episode: 7600, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000221, mae: 0.003314, mean_q: 0.023483
 75956/100000: episode: 7601, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000018, mae: 0.002293, mean_q: 0.022841
 75966/100000: episode: 7602, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000076, mae: 0.003472, mean_q: 0.022087
 75976/100000: episode: 7603, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000039, mae: 0.002168, mean_q: 0.023046
 75986/100000: episode: 7604, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000038, mae: 0.002320, mean_q: 0.022820
 75996/100000: episode: 7605, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000053, mae: 0.002491, mean_q: 0.022454
Step 76000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.76000.hdf5
 76006/100000: episode: 7606, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000205, mae: 0.003222, mean_q: 0.023313
 76016/100000: episode: 7607, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000017, mae: 0.002184, mean_q: 0.022584
 76026/100000: episode: 7608, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000007, mae: 0.001656, mean_q: 0.021956
 76036/100000: episode: 7609, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000016, mae: 0.002296, mean_q: 0.022028
 76046/100000: episode: 7610, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000057, mae: 0.003105, mean_q: 0.021814
 76056/100000: episode: 7611, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000059, mae: 0.002617, mean_q: 0.022617
 76066/100000: episode: 7612, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000227, mae: 0.004362, mean_q: 0.023782
 76076/100000: episode: 7613, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000221, mae: 0.003362, mean_q: 0.023322
 76086/100000: episode: 7614, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000205, mae: 0.003641, mean_q: 0.023649
 76096/100000: episode: 7615, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000220, mae: 0.003175, mean_q: 0.023419
 76106/100000: episode: 7616, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000043, mae: 0.002936, mean_q: 0.022648
 76116/100000: episode: 7617, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000242, mae: 0.003786, mean_q: 0.022449
 76126/100000: episode: 7618, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.002816, mean_q: 0.023003
 76136/100000: episode: 7619, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000017, mae: 0.002562, mean_q: 0.021848
 76146/100000: episode: 7620, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000056, mae: 0.002973, mean_q: 0.021910
 76156/100000: episode: 7621, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000090, mae: 0.002850, mean_q: 0.023028
 76166/100000: episode: 7622, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000016, mae: 0.002692, mean_q: 0.024074
 76176/100000: episode: 7623, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000389, mae: 0.004153, mean_q: 0.023272
 76186/100000: episode: 7624, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000224, mae: 0.003723, mean_q: 0.022446
 76196/100000: episode: 7625, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000615, mae: 0.007338, mean_q: 0.024860
 76206/100000: episode: 7626, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000041, mae: 0.004193, mean_q: 0.025633
 76216/100000: episode: 7627, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000032, mae: 0.002977, mean_q: 0.021587
 76226/100000: episode: 7628, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000262, mae: 0.004567, mean_q: 0.023759
 76236/100000: episode: 7629, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000205, mae: 0.003597, mean_q: 0.023755
 76246/100000: episode: 7630, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000419, mae: 0.005599, mean_q: 0.022792
 76256/100000: episode: 7631, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000202, mae: 0.003745, mean_q: 0.024447
 76266/100000: episode: 7632, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000015, mae: 0.002288, mean_q: 0.022389
 76276/100000: episode: 7633, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000248, mae: 0.005428, mean_q: 0.025169
 76286/100000: episode: 7634, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000059, mae: 0.003850, mean_q: 0.024710
 76296/100000: episode: 7635, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000290, mae: 0.005713, mean_q: 0.022116
 76306/100000: episode: 7636, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000041, mae: 0.003173, mean_q: 0.024399
 76316/100000: episode: 7637, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000221, mae: 0.003195, mean_q: 0.023221
 76326/100000: episode: 7638, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000021, mae: 0.002465, mean_q: 0.024091
 76336/100000: episode: 7639, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000042, mae: 0.003722, mean_q: 0.022003
 76346/100000: episode: 7640, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000062, mae: 0.003793, mean_q: 0.022109
 76356/100000: episode: 7641, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000018, mae: 0.001940, mean_q: 0.023034
 76366/100000: episode: 7642, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000203, mae: 0.003117, mean_q: 0.022853
 76376/100000: episode: 7643, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000010, mae: 0.001369, mean_q: 0.023226
 76386/100000: episode: 7644, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000430, mae: 0.005385, mean_q: 0.021982
 76396/100000: episode: 7645, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000067, mae: 0.004357, mean_q: 0.024437
 76406/100000: episode: 7646, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000074, mae: 0.002747, mean_q: 0.023070
 76416/100000: episode: 7647, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000209, mae: 0.003718, mean_q: 0.022510
 76426/100000: episode: 7648, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000052, mae: 0.002128, mean_q: 0.023360
 76436/100000: episode: 7649, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000036, mae: 0.002309, mean_q: 0.023659
 76446/100000: episode: 7650, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000199, mae: 0.002496, mean_q: 0.022938
 76456/100000: episode: 7651, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000388, mae: 0.004624, mean_q: 0.024294
[Info] 1-TH LEVEL FOUND: 0.024560879915952682, Considering 100/100 traces
 76466/100000: episode: 7652, duration: 0.796s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000224, mae: 0.005010, mean_q: 0.025376
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024560879915952682
1
 76476/100000: episode: 7653, duration: 0.505s, episode steps: 10, steps per second: 20, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000052, mae: 0.002423, mean_q: 0.023469
 76486/100000: episode: 7654, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000425, mae: 0.004961, mean_q: 0.023978
 76496/100000: episode: 7655, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000042, mae: 0.003546, mean_q: 0.024906
 76506/100000: episode: 7656, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000054, mae: 0.003059, mean_q: 0.022552
 76516/100000: episode: 7657, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000226, mae: 0.003886, mean_q: 0.022957
 76526/100000: episode: 7658, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000021, mae: 0.002168, mean_q: 0.023781
 76536/100000: episode: 7659, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002498, mean_q: 0.023345
 76546/100000: episode: 7660, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000052, mae: 0.002545, mean_q: 0.022713
 76556/100000: episode: 7661, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000391, mae: 0.004282, mean_q: 0.023466
 76566/100000: episode: 7662, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000204, mae: 0.004557, mean_q: 0.025420
 76576/100000: episode: 7663, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000226, mae: 0.004235, mean_q: 0.022503
 76586/100000: episode: 7664, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000243, mae: 0.004548, mean_q: 0.024321
 76596/100000: episode: 7665, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000064, mae: 0.003762, mean_q: 0.024207
 76606/100000: episode: 7666, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000246, mae: 0.004204, mean_q: 0.023153
 76616/100000: episode: 7667, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000201, mae: 0.003443, mean_q: 0.024652
 76626/100000: episode: 7668, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000040, mae: 0.002746, mean_q: 0.023775
 76636/100000: episode: 7669, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000224, mae: 0.003679, mean_q: 0.023459
 76646/100000: episode: 7670, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000031, mae: 0.001746, mean_q: 0.023776
 76656/100000: episode: 7671, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000216, mae: 0.003106, mean_q: 0.023130
 76666/100000: episode: 7672, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000038, mae: 0.002453, mean_q: 0.023374
 76676/100000: episode: 7673, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000202, mae: 0.002960, mean_q: 0.023527
 76686/100000: episode: 7674, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000611, mae: 0.006263, mean_q: 0.024311
 76696/100000: episode: 7675, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000226, mae: 0.004819, mean_q: 0.025521
 76706/100000: episode: 7676, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000208, mae: 0.004262, mean_q: 0.025088
 76716/100000: episode: 7677, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002633, mean_q: 0.022971
 76726/100000: episode: 7678, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000024, mae: 0.003489, mean_q: 0.022522
 76736/100000: episode: 7679, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000386, mae: 0.004496, mean_q: 0.023901
 76746/100000: episode: 7680, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000026, mae: 0.003339, mean_q: 0.024746
 76756/100000: episode: 7681, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000018, mae: 0.003426, mean_q: 0.021734
 76766/100000: episode: 7682, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000021, mae: 0.002794, mean_q: 0.022490
 76776/100000: episode: 7683, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000390, mae: 0.004236, mean_q: 0.023271
 76786/100000: episode: 7684, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000237, mae: 0.004388, mean_q: 0.024984
 76796/100000: episode: 7685, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000200, mae: 0.003688, mean_q: 0.024931
 76806/100000: episode: 7686, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000015, mae: 0.002403, mean_q: 0.022735
 76816/100000: episode: 7687, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000051, mae: 0.003044, mean_q: 0.022319
 76826/100000: episode: 7688, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000222, mae: 0.004040, mean_q: 0.024615
 76836/100000: episode: 7689, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000057, mae: 0.003340, mean_q: 0.024698
 76846/100000: episode: 7690, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000035, mae: 0.002504, mean_q: 0.023046
 76856/100000: episode: 7691, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000214, mae: 0.002940, mean_q: 0.022888
 76866/100000: episode: 7692, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000211, mae: 0.004310, mean_q: 0.024575
 76876/100000: episode: 7693, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000015, mae: 0.002191, mean_q: 0.022887
 76886/100000: episode: 7694, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000037, mae: 0.003028, mean_q: 0.022391
 76896/100000: episode: 7695, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000224, mae: 0.003360, mean_q: 0.023389
 76906/100000: episode: 7696, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000218, mae: 0.003990, mean_q: 0.024849
 76916/100000: episode: 7697, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000111, mae: 0.004544, mean_q: 0.025192
 76926/100000: episode: 7698, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000201, mae: 0.002801, mean_q: 0.023530
 76936/100000: episode: 7699, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000223, mae: 0.003645, mean_q: 0.024247
 76946/100000: episode: 7700, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000242, mae: 0.003757, mean_q: 0.024177
 76956/100000: episode: 7701, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000203, mae: 0.002929, mean_q: 0.023731
 76966/100000: episode: 7702, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000074, mae: 0.002723, mean_q: 0.024098
 76976/100000: episode: 7703, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000043, mae: 0.002889, mean_q: 0.023919
 76986/100000: episode: 7704, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.003178, mean_q: 0.022523
 76996/100000: episode: 7705, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000217, mae: 0.003216, mean_q: 0.023449
Step 77000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.77000.hdf5
 77006/100000: episode: 7706, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000041, mae: 0.002944, mean_q: 0.024360
 77016/100000: episode: 7707, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000038, mae: 0.002282, mean_q: 0.023589
 77026/100000: episode: 7708, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000039, mae: 0.003328, mean_q: 0.022445
 77036/100000: episode: 7709, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000209, mae: 0.003970, mean_q: 0.022577
 77046/100000: episode: 7710, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000242, mae: 0.004403, mean_q: 0.024612
 77056/100000: episode: 7711, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000020, mae: 0.002735, mean_q: 0.023632
 77066/100000: episode: 7712, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000032, mae: 0.003154, mean_q: 0.021710
 77076/100000: episode: 7713, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000264, mae: 0.004441, mean_q: 0.023445
 77086/100000: episode: 7714, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000227, mae: 0.005206, mean_q: 0.025472
 77096/100000: episode: 7715, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000037, mae: 0.002627, mean_q: 0.023290
 77106/100000: episode: 7716, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000019, mae: 0.002973, mean_q: 0.022184
 77116/100000: episode: 7717, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000015, mae: 0.001831, mean_q: 0.022910
 77126/100000: episode: 7718, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000059, mae: 0.002841, mean_q: 0.023569
 77136/100000: episode: 7719, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000018, mae: 0.002632, mean_q: 0.022422
 77146/100000: episode: 7720, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000015, mae: 0.003206, mean_q: 0.021240
 77156/100000: episode: 7721, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000210, mae: 0.004099, mean_q: 0.022012
 77166/100000: episode: 7722, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000035, mae: 0.002641, mean_q: 0.023968
 77176/100000: episode: 7723, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000205, mae: 0.003346, mean_q: 0.022620
 77186/100000: episode: 7724, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000015, mae: 0.002363, mean_q: 0.022070
 77196/100000: episode: 7725, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000016, mae: 0.002927, mean_q: 0.021354
 77206/100000: episode: 7726, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000084, mae: 0.003976, mean_q: 0.021995
 77216/100000: episode: 7727, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000231, mae: 0.005270, mean_q: 0.024603
 77226/100000: episode: 7728, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.002528, mean_q: 0.022223
 77236/100000: episode: 7729, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000034, mae: 0.002137, mean_q: 0.022205
 77246/100000: episode: 7730, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000032, mae: 0.001845, mean_q: 0.022553
 77256/100000: episode: 7731, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000054, mae: 0.002722, mean_q: 0.021984
 77266/100000: episode: 7732, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000036, mae: 0.002133, mean_q: 0.022890
 77276/100000: episode: 7733, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.002244, mean_q: 0.022693
 77286/100000: episode: 7734, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000223, mae: 0.003327, mean_q: 0.022355
 77296/100000: episode: 7735, duration: 0.073s, episode steps: 10, steps per second: 136, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000017, mae: 0.001982, mean_q: 0.022957
 77306/100000: episode: 7736, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000037, mae: 0.002481, mean_q: 0.022095
 77316/100000: episode: 7737, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000014, mae: 0.001836, mean_q: 0.022067
 77326/100000: episode: 7738, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000016, mae: 0.001726, mean_q: 0.022400
 77336/100000: episode: 7739, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000032, mae: 0.002107, mean_q: 0.021763
 77346/100000: episode: 7740, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.002685, mean_q: 0.021887
 77356/100000: episode: 7741, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000203, mae: 0.002889, mean_q: 0.022178
 77366/100000: episode: 7742, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000393, mae: 0.005373, mean_q: 0.023723
 77376/100000: episode: 7743, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000468, mae: 0.006411, mean_q: 0.023907
 77386/100000: episode: 7744, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000017, mae: 0.003209, mean_q: 0.024633
 77396/100000: episode: 7745, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000053, mae: 0.003233, mean_q: 0.021406
 77406/100000: episode: 7746, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000022, mae: 0.003110, mean_q: 0.021317
 77416/100000: episode: 7747, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000012, mae: 0.001736, mean_q: 0.022093
 77426/100000: episode: 7748, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000014, mae: 0.002661, mean_q: 0.021168
 77436/100000: episode: 7749, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000056, mae: 0.002723, mean_q: 0.021623
 77446/100000: episode: 7750, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000039, mae: 0.002576, mean_q: 0.021992
 77456/100000: episode: 7751, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000281, mae: 0.004378, mean_q: 0.022057
 77466/100000: episode: 7752, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000037, mae: 0.003301, mean_q: 0.023869
[Info] 1-TH LEVEL FOUND: 0.022383498027920723, Considering 100/100 traces
 77476/100000: episode: 7753, duration: 0.720s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000215, mae: 0.002753, mean_q: 0.022865
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022383498027920723
1
 77486/100000: episode: 7754, duration: 0.527s, episode steps: 10, steps per second: 19, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000044, mae: 0.002791, mean_q: 0.022106
 77496/100000: episode: 7755, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000031, mae: 0.001423, mean_q: 0.022357
 77506/100000: episode: 7756, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000058, mae: 0.003038, mean_q: 0.021736
 77516/100000: episode: 7757, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000011, mae: 0.001444, mean_q: 0.022015
 77526/100000: episode: 7758, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000043, mae: 0.003416, mean_q: 0.021261
 77536/100000: episode: 7759, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000201, mae: 0.003076, mean_q: 0.021447
 77546/100000: episode: 7760, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000059, mae: 0.003271, mean_q: 0.023094
 77556/100000: episode: 7761, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000016, mae: 0.002293, mean_q: 0.021888
 77566/100000: episode: 7762, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000227, mae: 0.004343, mean_q: 0.020607
 77576/100000: episode: 7763, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000281, mae: 0.006764, mean_q: 0.025331
 77586/100000: episode: 7764, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000226, mae: 0.005369, mean_q: 0.024820
 77596/100000: episode: 7765, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000430, mae: 0.004967, mean_q: 0.022483
 77606/100000: episode: 7766, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000069, mae: 0.003675, mean_q: 0.024475
 77616/100000: episode: 7767, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000013, mae: 0.002029, mean_q: 0.022385
 77626/100000: episode: 7768, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000223, mae: 0.003789, mean_q: 0.021731
 77636/100000: episode: 7769, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000020, mae: 0.002370, mean_q: 0.023138
 77646/100000: episode: 7770, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000043, mae: 0.003214, mean_q: 0.021706
 77656/100000: episode: 7771, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000119, mae: 0.004259, mean_q: 0.023197
 77666/100000: episode: 7772, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000075, mae: 0.003443, mean_q: 0.023655
 77676/100000: episode: 7773, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.002409, mean_q: 0.023205
 77686/100000: episode: 7774, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000259, mae: 0.003937, mean_q: 0.023317
 77696/100000: episode: 7775, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000018, mae: 0.002213, mean_q: 0.023559
 77706/100000: episode: 7776, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000224, mae: 0.003886, mean_q: 0.022059
 77716/100000: episode: 7777, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000206, mae: 0.003155, mean_q: 0.022857
 77726/100000: episode: 7778, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000012, mae: 0.001570, mean_q: 0.023210
 77736/100000: episode: 7779, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000208, mae: 0.003813, mean_q: 0.021964
 77746/100000: episode: 7780, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000014, mae: 0.001662, mean_q: 0.022544
 77756/100000: episode: 7781, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000033, mae: 0.002165, mean_q: 0.022186
 77766/100000: episode: 7782, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000062, mae: 0.003450, mean_q: 0.021780
 77776/100000: episode: 7783, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000014, mae: 0.001584, mean_q: 0.022848
 77786/100000: episode: 7784, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002676, mean_q: 0.021584
 77796/100000: episode: 7785, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000211, mae: 0.003942, mean_q: 0.022759
 77806/100000: episode: 7786, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000224, mae: 0.003506, mean_q: 0.022901
 77816/100000: episode: 7787, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000415, mae: 0.004945, mean_q: 0.022881
 77826/100000: episode: 7788, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000243, mae: 0.004365, mean_q: 0.023724
 77836/100000: episode: 7789, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002307, mean_q: 0.022947
 77846/100000: episode: 7790, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000227, mae: 0.004181, mean_q: 0.023772
 77856/100000: episode: 7791, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000038, mae: 0.002932, mean_q: 0.021943
 77866/100000: episode: 7792, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000055, mae: 0.002885, mean_q: 0.022818
 77876/100000: episode: 7793, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000224, mae: 0.003946, mean_q: 0.023678
 77886/100000: episode: 7794, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000238, mae: 0.003587, mean_q: 0.023476
 77896/100000: episode: 7795, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000240, mae: 0.003516, mean_q: 0.023292
 77906/100000: episode: 7796, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000224, mae: 0.003982, mean_q: 0.023809
 77916/100000: episode: 7797, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000057, mae: 0.002696, mean_q: 0.023232
 77926/100000: episode: 7798, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000036, mae: 0.002808, mean_q: 0.021875
 77936/100000: episode: 7799, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000054, mae: 0.002360, mean_q: 0.022731
 77946/100000: episode: 7800, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000205, mae: 0.003194, mean_q: 0.022473
 77956/100000: episode: 7801, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000232, mae: 0.004715, mean_q: 0.024001
 77966/100000: episode: 7802, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000033, mae: 0.002195, mean_q: 0.023214
 77976/100000: episode: 7803, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000243, mae: 0.004606, mean_q: 0.021356
 77986/100000: episode: 7804, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000056, mae: 0.003455, mean_q: 0.024214
 77996/100000: episode: 7805, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000019, mae: 0.002573, mean_q: 0.022710
Step 78000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.78000.hdf5
 78006/100000: episode: 7806, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000244, mae: 0.004219, mean_q: 0.021817
 78016/100000: episode: 7807, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000242, mae: 0.004833, mean_q: 0.024461
 78026/100000: episode: 7808, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000080, mae: 0.004147, mean_q: 0.024248
 78036/100000: episode: 7809, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000204, mae: 0.003852, mean_q: 0.021669
 78046/100000: episode: 7810, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000070, mae: 0.002541, mean_q: 0.022981
 78056/100000: episode: 7811, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002294, mean_q: 0.023628
 78066/100000: episode: 7812, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000217, mae: 0.002716, mean_q: 0.023087
 78076/100000: episode: 7813, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000014, mae: 0.001636, mean_q: 0.022794
 78086/100000: episode: 7814, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000074, mae: 0.002833, mean_q: 0.022926
 78096/100000: episode: 7815, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000243, mae: 0.004691, mean_q: 0.024409
 78106/100000: episode: 7816, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000076, mae: 0.003120, mean_q: 0.023144
 78116/100000: episode: 7817, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000017, mae: 0.002541, mean_q: 0.022166
[Info] FALSIFICATION!
 78126/100000: episode: 7818, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000036, mae: 0.002902, mean_q: 0.021825
 78136/100000: episode: 7819, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000393, mae: 0.004377, mean_q: 0.022505
 78146/100000: episode: 7820, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000052, mae: 0.002985, mean_q: 0.023992
 78156/100000: episode: 7821, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000018, mae: 0.002408, mean_q: 0.022419
 78166/100000: episode: 7822, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000062, mae: 0.003740, mean_q: 0.021694
 78176/100000: episode: 7823, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000237, mae: 0.003092, mean_q: 0.022796
 78186/100000: episode: 7824, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000122, mae: 0.005106, mean_q: 0.024187
 78196/100000: episode: 7825, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000243, mae: 0.005700, mean_q: 0.025589
 78206/100000: episode: 7826, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000402, mae: 0.004125, mean_q: 0.023793
 78216/100000: episode: 7827, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000080, mae: 0.003398, mean_q: 0.023330
 78226/100000: episode: 7828, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000092, mae: 0.003162, mean_q: 0.023637
 78236/100000: episode: 7829, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000055, mae: 0.002786, mean_q: 0.023870
 78246/100000: episode: 7830, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000062, mae: 0.003342, mean_q: 0.022794
 78256/100000: episode: 7831, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000076, mae: 0.002981, mean_q: 0.023104
 78266/100000: episode: 7832, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000042, mae: 0.002831, mean_q: 0.023492
 78276/100000: episode: 7833, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000041, mae: 0.003458, mean_q: 0.021802
 78286/100000: episode: 7834, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000058, mae: 0.004830, mean_q: 0.026109
 78296/100000: episode: 7835, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000199, mae: 0.004139, mean_q: 0.025297
 78306/100000: episode: 7836, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000014, mae: 0.003250, mean_q: 0.021357
 78316/100000: episode: 7837, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000011, mae: 0.002140, mean_q: 0.022213
 78326/100000: episode: 7838, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000016, mae: 0.001778, mean_q: 0.022920
 78336/100000: episode: 7839, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000219, mae: 0.002917, mean_q: 0.023135
 78346/100000: episode: 7840, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000078, mae: 0.004578, mean_q: 0.025294
 78356/100000: episode: 7841, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000031, mae: 0.002398, mean_q: 0.022803
 78366/100000: episode: 7842, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000056, mae: 0.002887, mean_q: 0.022689
 78376/100000: episode: 7843, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.001666, mean_q: 0.023093
 78386/100000: episode: 7844, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000055, mae: 0.002956, mean_q: 0.022536
 78396/100000: episode: 7845, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000298, mae: 0.004806, mean_q: 0.023596
 78406/100000: episode: 7846, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000064, mae: 0.003958, mean_q: 0.024507
 78416/100000: episode: 7847, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000220, mae: 0.003019, mean_q: 0.023181
 78426/100000: episode: 7848, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000017, mae: 0.001927, mean_q: 0.023072
 78436/100000: episode: 7849, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000432, mae: 0.005476, mean_q: 0.022415
 78446/100000: episode: 7850, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001707, mae: 0.007384, mean_q: 0.025020
 78456/100000: episode: 7851, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000529, mae: 0.011808, mean_q: 0.029239
 78466/100000: episode: 7852, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000265, mae: 0.007121, mean_q: 0.027610
 78476/100000: episode: 7853, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000033, mae: 0.002064, mean_q: 0.024177
[Info] Complete ISplit Iteration
[Info] Levels: [0.02483807]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 78486/100000: episode: 7854, duration: 0.755s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000240, mae: 0.003760, mean_q: 0.024609
 78496/100000: episode: 7855, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000082, mae: 0.003476, mean_q: 0.024271
 78506/100000: episode: 7856, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.002363, mean_q: 0.023568
 78516/100000: episode: 7857, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000039, mae: 0.003446, mean_q: 0.022940
 78526/100000: episode: 7858, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000062, mae: 0.004063, mean_q: 0.022538
 78536/100000: episode: 7859, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000258, mae: 0.004016, mean_q: 0.023972
 78546/100000: episode: 7860, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000202, mae: 0.004189, mean_q: 0.025735
 78556/100000: episode: 7861, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002660, mean_q: 0.023803
 78566/100000: episode: 7862, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000019, mae: 0.002531, mean_q: 0.023168
 78576/100000: episode: 7863, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000050, mae: 0.002110, mean_q: 0.023797
 78586/100000: episode: 7864, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002381, mean_q: 0.023520
 78596/100000: episode: 7865, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000020, mae: 0.002616, mean_q: 0.023112
 78606/100000: episode: 7866, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000054, mae: 0.002985, mean_q: 0.022870
 78616/100000: episode: 7867, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000017, mae: 0.001741, mean_q: 0.023568
 78626/100000: episode: 7868, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000223, mae: 0.003455, mean_q: 0.023330
 78636/100000: episode: 7869, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000207, mae: 0.003642, mean_q: 0.024351
 78646/100000: episode: 7870, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000022, mae: 0.002505, mean_q: 0.023343
 78656/100000: episode: 7871, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000098, mae: 0.004133, mean_q: 0.022509
 78666/100000: episode: 7872, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000046, mae: 0.003246, mean_q: 0.024011
 78676/100000: episode: 7873, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000079, mae: 0.003887, mean_q: 0.022531
 78686/100000: episode: 7874, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001569, mae: 0.006811, mean_q: 0.023425
 78696/100000: episode: 7875, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000041, mae: 0.004588, mean_q: 0.026521
 78706/100000: episode: 7876, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000424, mae: 0.004546, mean_q: 0.024150
 78716/100000: episode: 7877, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000044, mae: 0.002966, mean_q: 0.024297
 78726/100000: episode: 7878, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000206, mae: 0.003673, mean_q: 0.023066
 78736/100000: episode: 7879, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000049, mae: 0.002028, mean_q: 0.023822
 78746/100000: episode: 7880, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000022, mae: 0.002634, mean_q: 0.023171
 78756/100000: episode: 7881, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000033, mae: 0.002336, mean_q: 0.022817
 78766/100000: episode: 7882, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000036, mae: 0.002091, mean_q: 0.023726
 78776/100000: episode: 7883, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000055, mae: 0.002766, mean_q: 0.022867
 78786/100000: episode: 7884, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000032, mae: 0.001804, mean_q: 0.023320
 78796/100000: episode: 7885, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000031, mae: 0.002204, mean_q: 0.022790
 78806/100000: episode: 7886, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000033, mae: 0.002007, mean_q: 0.023071
 78816/100000: episode: 7887, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.002407, mean_q: 0.023228
 78826/100000: episode: 7888, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000225, mae: 0.003481, mean_q: 0.023056
 78836/100000: episode: 7889, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000050, mae: 0.002305, mean_q: 0.023944
 78846/100000: episode: 7890, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000224, mae: 0.003661, mean_q: 0.023866
 78856/100000: episode: 7891, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000258, mae: 0.003725, mean_q: 0.023670
 78866/100000: episode: 7892, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.001839, mean_q: 0.024232
 78876/100000: episode: 7893, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000075, mae: 0.003561, mean_q: 0.022430
 78886/100000: episode: 7894, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000078, mae: 0.003098, mean_q: 0.023349
 78896/100000: episode: 7895, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000020, mae: 0.002138, mean_q: 0.023612
 78906/100000: episode: 7896, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000058, mae: 0.003697, mean_q: 0.021950
 78916/100000: episode: 7897, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000051, mae: 0.002265, mean_q: 0.023131
 78926/100000: episode: 7898, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000049, mae: 0.002033, mean_q: 0.023148
 78936/100000: episode: 7899, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000032, mae: 0.001647, mean_q: 0.023106
 78946/100000: episode: 7900, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000015, mae: 0.001742, mean_q: 0.022954
 78956/100000: episode: 7901, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000057, mae: 0.003730, mean_q: 0.021562
 78966/100000: episode: 7902, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000052, mae: 0.002377, mean_q: 0.022435
 78976/100000: episode: 7903, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000071, mae: 0.002770, mean_q: 0.023538
 78986/100000: episode: 7904, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000056, mae: 0.002482, mean_q: 0.023083
 78996/100000: episode: 7905, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000018, mae: 0.002451, mean_q: 0.022265
Step 79000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.79000.hdf5
 79006/100000: episode: 7906, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.003070, mean_q: 0.021933
 79016/100000: episode: 7907, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000040, mae: 0.002415, mean_q: 0.022760
 79026/100000: episode: 7908, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000052, mae: 0.002613, mean_q: 0.022000
 79036/100000: episode: 7909, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000228, mae: 0.004234, mean_q: 0.023413
 79046/100000: episode: 7910, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000396, mae: 0.005524, mean_q: 0.024033
 79056/100000: episode: 7911, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000072, mae: 0.002595, mean_q: 0.023196
 79066/100000: episode: 7912, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.002458, mean_q: 0.022115
 79076/100000: episode: 7913, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.001556, mae: 0.009116, mean_q: 0.026748
 79086/100000: episode: 7914, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000071, mae: 0.004846, mean_q: 0.026416
 79096/100000: episode: 7915, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.003775, mean_q: 0.021374
 79106/100000: episode: 7916, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000037, mae: 0.003245, mean_q: 0.021785
 79116/100000: episode: 7917, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000204, mae: 0.003275, mean_q: 0.023723
 79126/100000: episode: 7918, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000200, mae: 0.002769, mean_q: 0.022957
 79136/100000: episode: 7919, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000034, mae: 0.002159, mean_q: 0.023513
 79146/100000: episode: 7920, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001784, mae: 0.009578, mean_q: 0.026087
 79156/100000: episode: 7921, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000246, mae: 0.005426, mean_q: 0.025796
 79166/100000: episode: 7922, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000051, mae: 0.002153, mean_q: 0.023538
 79176/100000: episode: 7923, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000040, mae: 0.002655, mean_q: 0.023298
 79186/100000: episode: 7924, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000204, mae: 0.003483, mean_q: 0.022977
 79196/100000: episode: 7925, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.001528, mae: 0.005927, mean_q: 0.024003
 79206/100000: episode: 7926, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000034, mae: 0.003626, mean_q: 0.025968
 79216/100000: episode: 7927, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000094, mae: 0.003361, mean_q: 0.023463
 79226/100000: episode: 7928, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.002613, mean_q: 0.023344
 79236/100000: episode: 7929, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000039, mae: 0.003096, mean_q: 0.022738
 79246/100000: episode: 7930, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000039, mae: 0.002622, mean_q: 0.023199
 79256/100000: episode: 7931, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000059, mae: 0.002871, mean_q: 0.023186
 79266/100000: episode: 7932, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000202, mae: 0.003025, mean_q: 0.023841
 79276/100000: episode: 7933, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000093, mae: 0.003207, mean_q: 0.023358
 79286/100000: episode: 7934, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000019, mae: 0.002645, mean_q: 0.024523
 79296/100000: episode: 7935, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001766, mae: 0.008580, mean_q: 0.025545
 79306/100000: episode: 7936, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000056, mae: 0.004421, mean_q: 0.026536
 79316/100000: episode: 7937, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000461, mae: 0.005285, mean_q: 0.023521
 79326/100000: episode: 7938, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000045, mae: 0.003967, mean_q: 0.025631
 79336/100000: episode: 7939, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000018, mae: 0.002469, mean_q: 0.023861
 79346/100000: episode: 7940, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000472, mae: 0.006645, mean_q: 0.023280
 79356/100000: episode: 7941, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000054, mae: 0.004204, mean_q: 0.026584
 79366/100000: episode: 7942, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000443, mae: 0.006197, mean_q: 0.025787
 79376/100000: episode: 7943, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000056, mae: 0.003879, mean_q: 0.026317
 79386/100000: episode: 7944, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000040, mae: 0.002934, mean_q: 0.024004
 79396/100000: episode: 7945, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001581, mae: 0.006722, mean_q: 0.024277
 79406/100000: episode: 7946, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000097, mae: 0.006380, mean_q: 0.028537
 79416/100000: episode: 7947, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000241, mae: 0.004514, mean_q: 0.026053
 79426/100000: episode: 7948, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000012, mae: 0.002106, mean_q: 0.024015
 79436/100000: episode: 7949, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000240, mae: 0.003967, mean_q: 0.024648
 79446/100000: episode: 7950, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000135, mae: 0.004724, mean_q: 0.025874
 79456/100000: episode: 7951, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.001861, mean_q: 0.025329
 79466/100000: episode: 7952, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000096, mae: 0.003594, mean_q: 0.024523
 79476/100000: episode: 7953, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000057, mae: 0.002852, mean_q: 0.024635
[Info] 1-TH LEVEL FOUND: 0.023560605943202972, Considering 100/100 traces
 79486/100000: episode: 7954, duration: 0.736s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000061, mae: 0.003833, mean_q: 0.023755
[Info] 2-TH LEVEL FOUND: 0.025115838274359703, Considering 100/100 traces
 79496/100000: episode: 7955, duration: 0.665s, episode steps: 10, steps per second: 15, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000225, mae: 0.003734, mean_q: 0.024558
[Info] 3-TH LEVEL FOUND: 0.025857917964458466, Considering 100/100 traces
 79506/100000: episode: 7956, duration: 0.677s, episode steps: 10, steps per second: 15, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000239, mae: 0.003653, mean_q: 0.025142
[Info] 4-TH LEVEL FOUND: 0.02661001682281494, Considering 100/100 traces
 79516/100000: episode: 7957, duration: 0.741s, episode steps: 10, steps per second: 13, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001546, mae: 0.006380, mean_q: 0.025587
[Info] 5-TH LEVEL FOUND: 0.02844962477684021, Considering 100/100 traces
 79526/100000: episode: 7958, duration: 0.765s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000272, mae: 0.006632, mean_q: 0.028265
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02844962477684021
5
 79536/100000: episode: 7959, duration: 0.501s, episode steps: 10, steps per second: 20, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000061, mae: 0.003879, mean_q: 0.026360
 79546/100000: episode: 7960, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000023, mae: 0.004042, mean_q: 0.022980
 79556/100000: episode: 7961, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000207, mae: 0.004545, mean_q: 0.023234
 79566/100000: episode: 7962, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000382, mae: 0.004023, mean_q: 0.025421
 79576/100000: episode: 7963, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000081, mae: 0.004491, mean_q: 0.026500
 79586/100000: episode: 7964, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000075, mae: 0.003140, mean_q: 0.025429
 79596/100000: episode: 7965, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000074, mae: 0.003201, mean_q: 0.024383
 79606/100000: episode: 7966, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000052, mae: 0.002363, mean_q: 0.025333
 79616/100000: episode: 7967, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000050, mae: 0.002092, mean_q: 0.025081
 79626/100000: episode: 7968, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000077, mae: 0.003679, mean_q: 0.024153
 79636/100000: episode: 7969, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000015, mae: 0.002095, mean_q: 0.024308
 79646/100000: episode: 7970, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000040, mae: 0.003407, mean_q: 0.023581
 79656/100000: episode: 7971, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000037, mae: 0.002455, mean_q: 0.024292
 79666/100000: episode: 7972, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000389, mae: 0.004413, mean_q: 0.024313
 79676/100000: episode: 7973, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002991, mean_q: 0.025959
 79686/100000: episode: 7974, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000037, mae: 0.002659, mean_q: 0.025473
 79696/100000: episode: 7975, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000074, mae: 0.003070, mean_q: 0.024474
 79706/100000: episode: 7976, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000035, mae: 0.002194, mean_q: 0.024480
 79716/100000: episode: 7977, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000049, mae: 0.002436, mean_q: 0.024007
 79726/100000: episode: 7978, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000058, mae: 0.002640, mean_q: 0.024635
 79736/100000: episode: 7979, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000260, mae: 0.004178, mean_q: 0.024228
 79746/100000: episode: 7980, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002269, mean_q: 0.025169
 79756/100000: episode: 7981, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000041, mae: 0.002655, mean_q: 0.024407
 79766/100000: episode: 7982, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000242, mae: 0.003964, mean_q: 0.024672
 79776/100000: episode: 7983, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000032, mae: 0.002068, mean_q: 0.025017
 79786/100000: episode: 7984, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.001580, mean_q: 0.024621
 79796/100000: episode: 7985, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000077, mae: 0.003401, mean_q: 0.024128
 79806/100000: episode: 7986, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000012, mae: 0.001609, mean_q: 0.024113
 79816/100000: episode: 7987, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000221, mae: 0.003431, mean_q: 0.024070
 79826/100000: episode: 7988, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000080, mae: 0.003866, mean_q: 0.025495
 79836/100000: episode: 7989, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000247, mae: 0.004245, mean_q: 0.024354
 79846/100000: episode: 7990, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000222, mae: 0.003323, mean_q: 0.024646
 79856/100000: episode: 7991, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000203, mae: 0.003329, mean_q: 0.023889
 79866/100000: episode: 7992, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000011, mae: 0.001458, mean_q: 0.024751
 79876/100000: episode: 7993, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000015, mae: 0.002482, mean_q: 0.023443
 79886/100000: episode: 7994, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000061, mae: 0.004079, mean_q: 0.022712
 79896/100000: episode: 7995, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.001530, mae: 0.006889, mean_q: 0.025864
 79906/100000: episode: 7996, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.003980, mean_q: 0.025683
 79916/100000: episode: 7997, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000061, mae: 0.003981, mean_q: 0.023003
 79926/100000: episode: 7998, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.001524, mae: 0.005825, mean_q: 0.024640
 79936/100000: episode: 7999, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000023, mae: 0.003983, mean_q: 0.026900
 79946/100000: episode: 8000, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000263, mae: 0.004698, mean_q: 0.023755
 79956/100000: episode: 8001, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000051, mae: 0.002119, mean_q: 0.024597
 79966/100000: episode: 8002, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000047, mae: 0.001929, mean_q: 0.024964
 79976/100000: episode: 8003, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000083, mae: 0.003570, mean_q: 0.024451
 79986/100000: episode: 8004, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000222, mae: 0.003357, mean_q: 0.024615
 79996/100000: episode: 8005, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000039, mae: 0.002590, mean_q: 0.024136
Step 80000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.80000.hdf5
 80006/100000: episode: 8006, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000205, mae: 0.003362, mean_q: 0.024558
 80016/100000: episode: 8007, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.001739, mae: 0.007521, mean_q: 0.025841
 80026/100000: episode: 8008, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000049, mae: 0.006018, mean_q: 0.028828
 80036/100000: episode: 8009, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000253, mae: 0.003659, mean_q: 0.024993
 80046/100000: episode: 8010, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000260, mae: 0.004470, mean_q: 0.025614
 80056/100000: episode: 8011, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.001524, mae: 0.005426, mean_q: 0.025393
 80066/100000: episode: 8012, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000043, mae: 0.003432, mean_q: 0.026203
 80076/100000: episode: 8013, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000054, mae: 0.002984, mean_q: 0.024405
 80086/100000: episode: 8014, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000055, mae: 0.003218, mean_q: 0.024151
 80096/100000: episode: 8015, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000058, mae: 0.002937, mean_q: 0.025414
 80106/100000: episode: 8016, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000240, mae: 0.004077, mean_q: 0.025807
 80116/100000: episode: 8017, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000026, mae: 0.002930, mean_q: 0.024653
 80126/100000: episode: 8018, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000042, mae: 0.003964, mean_q: 0.023291
 80136/100000: episode: 8019, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000077, mae: 0.003482, mean_q: 0.024167
 80146/100000: episode: 8020, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000051, mae: 0.002191, mean_q: 0.025143
 80156/100000: episode: 8021, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000080, mae: 0.003380, mean_q: 0.024746
 80166/100000: episode: 8022, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.003254, mean_q: 0.023675
 80176/100000: episode: 8023, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000037, mae: 0.002913, mean_q: 0.023632
 80186/100000: episode: 8024, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000076, mae: 0.002883, mean_q: 0.024659
 80196/100000: episode: 8025, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000256, mae: 0.003839, mean_q: 0.024620
 80206/100000: episode: 8026, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.003234, mean_q: 0.025744
 80216/100000: episode: 8027, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000096, mae: 0.003484, mean_q: 0.024461
 80226/100000: episode: 8028, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000061, mae: 0.003073, mean_q: 0.024361
 80236/100000: episode: 8029, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000019, mae: 0.002271, mean_q: 0.023983
 80246/100000: episode: 8030, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000071, mae: 0.003146, mean_q: 0.023589
 80256/100000: episode: 8031, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000078, mae: 0.002999, mean_q: 0.024388
 80266/100000: episode: 8032, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000017, mae: 0.002217, mean_q: 0.023927
 80276/100000: episode: 8033, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000226, mae: 0.003784, mean_q: 0.024382
 80286/100000: episode: 8034, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000018, mae: 0.002059, mean_q: 0.024358
 80296/100000: episode: 8035, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000035, mae: 0.002845, mean_q: 0.023244
 80306/100000: episode: 8036, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000094, mae: 0.003388, mean_q: 0.023902
 80316/100000: episode: 8037, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000015, mae: 0.001788, mean_q: 0.024470
 80326/100000: episode: 8038, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000075, mae: 0.003355, mean_q: 0.023348
 80336/100000: episode: 8039, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000197, mae: 0.002607, mean_q: 0.024027
 80346/100000: episode: 8040, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000241, mae: 0.003552, mean_q: 0.024161
 80356/100000: episode: 8041, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000077, mae: 0.003601, mean_q: 0.025067
 80366/100000: episode: 8042, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000052, mae: 0.002366, mean_q: 0.024546
 80376/100000: episode: 8043, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000201, mae: 0.003355, mean_q: 0.023448
 80386/100000: episode: 8044, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.001526, mae: 0.006521, mean_q: 0.026026
 80396/100000: episode: 8045, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000080, mae: 0.004244, mean_q: 0.025848
 80406/100000: episode: 8046, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001697, mae: 0.006984, mean_q: 0.023478
 80416/100000: episode: 8047, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000084, mae: 0.005383, mean_q: 0.026951
 80426/100000: episode: 8048, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000015, mae: 0.002269, mean_q: 0.025065
 80436/100000: episode: 8049, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000055, mae: 0.003109, mean_q: 0.023678
 80446/100000: episode: 8050, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000018, mae: 0.002041, mean_q: 0.024452
 80456/100000: episode: 8051, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000082, mae: 0.004125, mean_q: 0.023565
 80466/100000: episode: 8052, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001532, mae: 0.006366, mean_q: 0.023602
 80476/100000: episode: 8053, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000221, mae: 0.004411, mean_q: 0.025791
 80486/100000: episode: 8054, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000240, mae: 0.004654, mean_q: 0.025835
 80496/100000: episode: 8055, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000224, mae: 0.003582, mean_q: 0.024750
 80506/100000: episode: 8056, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000034, mae: 0.001864, mean_q: 0.024652
 80516/100000: episode: 8057, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000020, mae: 0.002420, mean_q: 0.024080
 80526/100000: episode: 8058, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000038, mae: 0.002983, mean_q: 0.023535
[Info] 1-TH LEVEL FOUND: 0.023821193724870682, Considering 100/100 traces
 80536/100000: episode: 8059, duration: 0.722s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000039, mae: 0.002650, mean_q: 0.024047
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023821193724870682
1
 80546/100000: episode: 8060, duration: 0.585s, episode steps: 10, steps per second: 17, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000012, mae: 0.001785, mean_q: 0.023637
 80556/100000: episode: 8061, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000021, mae: 0.002601, mean_q: 0.023694
 80566/100000: episode: 8062, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000020, mae: 0.002338, mean_q: 0.023815
 80576/100000: episode: 8063, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000032, mae: 0.002020, mean_q: 0.023644
 80586/100000: episode: 8064, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000031, mae: 0.001582, mean_q: 0.024090
 80596/100000: episode: 8065, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000080, mae: 0.003179, mean_q: 0.024071
 80606/100000: episode: 8066, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000202, mae: 0.003303, mean_q: 0.024834
 80616/100000: episode: 8067, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000015, mae: 0.001860, mean_q: 0.024190
 80626/100000: episode: 8068, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000076, mae: 0.003517, mean_q: 0.023082
 80636/100000: episode: 8069, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000048, mae: 0.002511, mean_q: 0.025149
 80646/100000: episode: 8070, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000054, mae: 0.002587, mean_q: 0.024645
 80656/100000: episode: 8071, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000031, mae: 0.002074, mean_q: 0.023712
 80666/100000: episode: 8072, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000019, mae: 0.002777, mean_q: 0.023065
 80676/100000: episode: 8073, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.001537, mae: 0.006764, mean_q: 0.022814
 80686/100000: episode: 8074, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000049, mae: 0.004758, mean_q: 0.026206
 80696/100000: episode: 8075, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000011, mae: 0.002046, mean_q: 0.023571
 80706/100000: episode: 8076, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000405, mae: 0.004760, mean_q: 0.023577
 80716/100000: episode: 8077, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000076, mae: 0.003729, mean_q: 0.025106
 80726/100000: episode: 8078, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002892, mean_q: 0.023431
 80736/100000: episode: 8079, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000078, mae: 0.003428, mean_q: 0.023466
 80746/100000: episode: 8080, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000019, mae: 0.002032, mean_q: 0.023779
 80756/100000: episode: 8081, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000488, mae: 0.006855, mean_q: 0.024889
 80766/100000: episode: 8082, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.001529, mae: 0.007955, mean_q: 0.026868
 80776/100000: episode: 8083, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.002088, mae: 0.012047, mean_q: 0.028090
 80786/100000: episode: 8084, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000040, mae: 0.004954, mean_q: 0.028415
 80796/100000: episode: 8085, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000225, mae: 0.004288, mean_q: 0.023915
 80806/100000: episode: 8086, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000238, mae: 0.003815, mean_q: 0.024352
 80816/100000: episode: 8087, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000077, mae: 0.003903, mean_q: 0.025963
 80826/100000: episode: 8088, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000038, mae: 0.002797, mean_q: 0.024264
 80836/100000: episode: 8089, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000053, mae: 0.003157, mean_q: 0.023614
 80846/100000: episode: 8090, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.001998, mean_q: 0.024657
 80856/100000: episode: 8091, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000038, mae: 0.002840, mean_q: 0.024110
 80866/100000: episode: 8092, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000263, mae: 0.004392, mean_q: 0.024312
 80876/100000: episode: 8093, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000035, mae: 0.002214, mean_q: 0.024577
 80886/100000: episode: 8094, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000018, mae: 0.002650, mean_q: 0.023599
 80896/100000: episode: 8095, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000018, mae: 0.002921, mean_q: 0.023086
 80906/100000: episode: 8096, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000051, mae: 0.002574, mean_q: 0.023766
 80916/100000: episode: 8097, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000017, mae: 0.002263, mean_q: 0.023734
 80926/100000: episode: 8098, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000074, mae: 0.003637, mean_q: 0.022877
 80936/100000: episode: 8099, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000098, mae: 0.003792, mean_q: 0.024323
 80946/100000: episode: 8100, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.001689, mae: 0.007842, mean_q: 0.026517
 80956/100000: episode: 8101, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000220, mae: 0.004178, mean_q: 0.025803
 80966/100000: episode: 8102, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000407, mae: 0.004604, mean_q: 0.024448
 80976/100000: episode: 8103, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000201, mae: 0.003486, mean_q: 0.025436
 80986/100000: episode: 8104, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000038, mae: 0.002654, mean_q: 0.024026
 80996/100000: episode: 8105, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000020, mae: 0.002582, mean_q: 0.023709
Step 81000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.81000.hdf5
 81006/100000: episode: 8106, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000218, mae: 0.003253, mean_q: 0.024126
 81016/100000: episode: 8107, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000040, mae: 0.002523, mean_q: 0.024379
 81026/100000: episode: 8108, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000059, mae: 0.003189, mean_q: 0.023744
 81036/100000: episode: 8109, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000052, mae: 0.002377, mean_q: 0.023959
 81046/100000: episode: 8110, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000222, mae: 0.003286, mean_q: 0.024311
 81056/100000: episode: 8111, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000060, mae: 0.002989, mean_q: 0.024147
 81066/100000: episode: 8112, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002692, mean_q: 0.023208
 81076/100000: episode: 8113, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000056, mae: 0.002573, mean_q: 0.024254
 81086/100000: episode: 8114, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000060, mae: 0.003229, mean_q: 0.023643
 81096/100000: episode: 8115, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000205, mae: 0.003526, mean_q: 0.023423
 81106/100000: episode: 8116, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002897, mean_q: 0.024943
 81116/100000: episode: 8117, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000057, mae: 0.002960, mean_q: 0.023619
 81126/100000: episode: 8118, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000055, mae: 0.003276, mean_q: 0.022752
 81136/100000: episode: 8119, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000020, mae: 0.002550, mean_q: 0.023436
 81146/100000: episode: 8120, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000201, mae: 0.003080, mean_q: 0.023378
 81156/100000: episode: 8121, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000072, mae: 0.002780, mean_q: 0.024369
 81166/100000: episode: 8122, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000285, mae: 0.004786, mean_q: 0.023577
 81176/100000: episode: 8123, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000077, mae: 0.004187, mean_q: 0.025517
 81186/100000: episode: 8124, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000082, mae: 0.003843, mean_q: 0.024553
 81196/100000: episode: 8125, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000062, mae: 0.003581, mean_q: 0.023264
 81206/100000: episode: 8126, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.001543, mae: 0.006996, mean_q: 0.026032
 81216/100000: episode: 8127, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000066, mae: 0.004440, mean_q: 0.025909
 81226/100000: episode: 8128, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.001503, mae: 0.005137, mean_q: 0.024616
 81236/100000: episode: 8129, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000051, mae: 0.003405, mean_q: 0.026190
 81246/100000: episode: 8130, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000035, mae: 0.002396, mean_q: 0.023995
 81256/100000: episode: 8131, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000019, mae: 0.003007, mean_q: 0.023171
 81266/100000: episode: 8132, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000395, mae: 0.005069, mean_q: 0.023513
 81276/100000: episode: 8133, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000199, mae: 0.004048, mean_q: 0.026172
 81286/100000: episode: 8134, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000050, mae: 0.002578, mean_q: 0.025028
 81296/100000: episode: 8135, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001693, mae: 0.006532, mean_q: 0.023946
 81306/100000: episode: 8136, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000046, mae: 0.004634, mean_q: 0.026801
 81316/100000: episode: 8137, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.001712, mae: 0.007043, mean_q: 0.025405
 81326/100000: episode: 8138, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000423, mae: 0.006805, mean_q: 0.027348
 81336/100000: episode: 8139, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000221, mae: 0.004777, mean_q: 0.026895
 81346/100000: episode: 8140, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000017, mae: 0.002698, mean_q: 0.023965
 81356/100000: episode: 8141, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000023, mae: 0.003724, mean_q: 0.023035
 81366/100000: episode: 8142, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001901, mae: 0.009619, mean_q: 0.026726
 81376/100000: episode: 8143, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000228, mae: 0.006659, mean_q: 0.028773
 81386/100000: episode: 8144, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.001563, mae: 0.006513, mean_q: 0.024683
 81396/100000: episode: 8145, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000109, mae: 0.005040, mean_q: 0.027622
 81406/100000: episode: 8146, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000404, mae: 0.005892, mean_q: 0.027632
 81416/100000: episode: 8147, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000057, mae: 0.003243, mean_q: 0.026849
 81426/100000: episode: 8148, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000034, mae: 0.002244, mean_q: 0.025470
 81436/100000: episode: 8149, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000034, mae: 0.002626, mean_q: 0.024847
 81446/100000: episode: 8150, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001525, mae: 0.006022, mean_q: 0.025408
 81456/100000: episode: 8151, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000021, mae: 0.003251, mean_q: 0.027527
 81466/100000: episode: 8152, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000205, mae: 0.003470, mean_q: 0.025401
 81476/100000: episode: 8153, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.001513, mae: 0.006292, mean_q: 0.026510
 81486/100000: episode: 8154, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.001543, mae: 0.007710, mean_q: 0.027950
 81496/100000: episode: 8155, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000394, mae: 0.003975, mean_q: 0.026755
 81506/100000: episode: 8156, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000431, mae: 0.005822, mean_q: 0.026752
 81516/100000: episode: 8157, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000237, mae: 0.003505, mean_q: 0.026592
 81526/100000: episode: 8158, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000025, mae: 0.002603, mean_q: 0.026413
 81536/100000: episode: 8159, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000098, mae: 0.004149, mean_q: 0.025467
[Info] 1-TH LEVEL FOUND: 0.025397593155503273, Considering 100/100 traces
 81546/100000: episode: 8160, duration: 0.730s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000202, mae: 0.003504, mean_q: 0.025441
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025397593155503273
1
 81556/100000: episode: 8161, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002622, mean_q: 0.025244
 81566/100000: episode: 8162, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000039, mae: 0.002796, mean_q: 0.025287
 81576/100000: episode: 8163, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000242, mae: 0.004036, mean_q: 0.025490
 81586/100000: episode: 8164, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000044, mae: 0.004320, mean_q: 0.028083
 81596/100000: episode: 8165, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000216, mae: 0.004109, mean_q: 0.027590
 81606/100000: episode: 8166, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000424, mae: 0.004967, mean_q: 0.025754
 81616/100000: episode: 8167, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000041, mae: 0.002910, mean_q: 0.026634
 81626/100000: episode: 8168, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000037, mae: 0.002697, mean_q: 0.025496
 81636/100000: episode: 8169, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000039, mae: 0.003461, mean_q: 0.024708
 81646/100000: episode: 8170, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000036, mae: 0.002737, mean_q: 0.025104
 81656/100000: episode: 8171, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000017, mae: 0.002430, mean_q: 0.025048
 81666/100000: episode: 8172, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000059, mae: 0.003469, mean_q: 0.024890
 81676/100000: episode: 8173, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000051, mae: 0.002165, mean_q: 0.025629
 81686/100000: episode: 8174, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000071, mae: 0.003077, mean_q: 0.026373
 81696/100000: episode: 8175, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002286, mean_q: 0.025856
 81706/100000: episode: 8176, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000022, mae: 0.003589, mean_q: 0.023917
 81716/100000: episode: 8177, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000056, mae: 0.003882, mean_q: 0.023736
 81726/100000: episode: 8178, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000227, mae: 0.004006, mean_q: 0.025310
 81736/100000: episode: 8179, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001712, mae: 0.008728, mean_q: 0.027955
 81746/100000: episode: 8180, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001521, mae: 0.006354, mean_q: 0.027284
 81756/100000: episode: 8181, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000054, mae: 0.003784, mean_q: 0.027670
 81766/100000: episode: 8182, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000059, mae: 0.003322, mean_q: 0.025795
 81776/100000: episode: 8183, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000074, mae: 0.003963, mean_q: 0.024317
 81786/100000: episode: 8184, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.002372, mean_q: 0.025442
 81796/100000: episode: 8185, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000037, mae: 0.002563, mean_q: 0.025168
 81806/100000: episode: 8186, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000242, mae: 0.004216, mean_q: 0.024992
 81816/100000: episode: 8187, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000221, mae: 0.003583, mean_q: 0.026084
 81826/100000: episode: 8188, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.001566, mae: 0.007861, mean_q: 0.027243
 81836/100000: episode: 8189, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000229, mae: 0.004467, mean_q: 0.027793
 81846/100000: episode: 8190, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002579, mean_q: 0.026388
 81856/100000: episode: 8191, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000257, mae: 0.003962, mean_q: 0.026087
 81866/100000: episode: 8192, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.001518, mae: 0.005665, mean_q: 0.026830
 81876/100000: episode: 8193, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000043, mae: 0.003865, mean_q: 0.027911
 81886/100000: episode: 8194, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000043, mae: 0.003464, mean_q: 0.025249
 81896/100000: episode: 8195, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000041, mae: 0.004077, mean_q: 0.024033
 81906/100000: episode: 8196, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000039, mae: 0.002554, mean_q: 0.025512
 81916/100000: episode: 8197, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000238, mae: 0.003494, mean_q: 0.025753
 81926/100000: episode: 8198, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000020, mae: 0.002073, mean_q: 0.026281
 81936/100000: episode: 8199, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001685, mae: 0.006052, mean_q: 0.025897
 81946/100000: episode: 8200, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000384, mae: 0.006745, mean_q: 0.029397
 81956/100000: episode: 8201, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000043, mae: 0.003586, mean_q: 0.027349
 81966/100000: episode: 8202, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000017, mae: 0.003479, mean_q: 0.024063
 81976/100000: episode: 8203, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000034, mae: 0.002782, mean_q: 0.024890
 81986/100000: episode: 8204, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001543, mae: 0.007001, mean_q: 0.027475
 81996/100000: episode: 8205, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.002844, mean_q: 0.027549
Step 82000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.82000.hdf5
 82006/100000: episode: 8206, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000076, mae: 0.003527, mean_q: 0.025507
 82016/100000: episode: 8207, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.001759, mae: 0.007807, mean_q: 0.025580
 82026/100000: episode: 8208, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000084, mae: 0.007130, mean_q: 0.030016
 82036/100000: episode: 8209, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000042, mae: 0.003572, mean_q: 0.026272
 82046/100000: episode: 8210, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000202, mae: 0.004870, mean_q: 0.023645
 82056/100000: episode: 8211, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000040, mae: 0.002772, mean_q: 0.025690
 82066/100000: episode: 8212, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000059, mae: 0.002888, mean_q: 0.026059
 82076/100000: episode: 8213, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002611, mean_q: 0.025681
 82086/100000: episode: 8214, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000051, mae: 0.002367, mean_q: 0.025739
 82096/100000: episode: 8215, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000239, mae: 0.003610, mean_q: 0.025866
 82106/100000: episode: 8216, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000080, mae: 0.003443, mean_q: 0.026320
 82116/100000: episode: 8217, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000061, mae: 0.002965, mean_q: 0.026041
 82126/100000: episode: 8218, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000097, mae: 0.003699, mean_q: 0.025651
 82136/100000: episode: 8219, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000410, mae: 0.005068, mean_q: 0.025907
 82146/100000: episode: 8220, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000024, mae: 0.002950, mean_q: 0.026909
 82156/100000: episode: 8221, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001544, mae: 0.006465, mean_q: 0.026437
 82166/100000: episode: 8222, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000236, mae: 0.004001, mean_q: 0.026901
 82176/100000: episode: 8223, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000053, mae: 0.002491, mean_q: 0.025957
 82186/100000: episode: 8224, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002591, mean_q: 0.025290
 82196/100000: episode: 8225, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000093, mae: 0.003432, mean_q: 0.025995
 82206/100000: episode: 8226, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000055, mae: 0.002790, mean_q: 0.026606
 82216/100000: episode: 8227, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000261, mae: 0.005013, mean_q: 0.027154
 82226/100000: episode: 8228, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000039, mae: 0.002785, mean_q: 0.026454
 82236/100000: episode: 8229, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000042, mae: 0.004000, mean_q: 0.024309
 82246/100000: episode: 8230, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000090, mae: 0.003694, mean_q: 0.024871
 82256/100000: episode: 8231, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000055, mae: 0.002685, mean_q: 0.026174
 82266/100000: episode: 8232, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000038, mae: 0.002729, mean_q: 0.025312
 82276/100000: episode: 8233, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000039, mae: 0.003387, mean_q: 0.024432
 82286/100000: episode: 8234, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000054, mae: 0.002832, mean_q: 0.024947
 82296/100000: episode: 8235, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000059, mae: 0.003264, mean_q: 0.026308
 82306/100000: episode: 8236, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000063, mae: 0.003560, mean_q: 0.026034
 82316/100000: episode: 8237, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000059, mae: 0.003940, mean_q: 0.024055
 82326/100000: episode: 8238, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000237, mae: 0.003752, mean_q: 0.025593
 82336/100000: episode: 8239, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000057, mae: 0.003377, mean_q: 0.026652
 82346/100000: episode: 8240, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000059, mae: 0.003614, mean_q: 0.024504
 82356/100000: episode: 8241, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000022, mae: 0.002764, mean_q: 0.024608
 82366/100000: episode: 8242, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.002017, mean_q: 0.025207
 82376/100000: episode: 8243, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000252, mae: 0.003481, mean_q: 0.025708
 82386/100000: episode: 8244, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000219, mae: 0.004479, mean_q: 0.027125
 82396/100000: episode: 8245, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000054, mae: 0.002693, mean_q: 0.026087
 82406/100000: episode: 8246, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000039, mae: 0.003069, mean_q: 0.024732
 82416/100000: episode: 8247, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000060, mae: 0.003496, mean_q: 0.024456
 82426/100000: episode: 8248, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.001564, mean_q: 0.025151
 82436/100000: episode: 8249, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000275, mae: 0.004258, mean_q: 0.025020
 82446/100000: episode: 8250, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.001705, mae: 0.008873, mean_q: 0.028270
 82456/100000: episode: 8251, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000257, mae: 0.006420, mean_q: 0.028663
 82466/100000: episode: 8252, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000239, mae: 0.003927, mean_q: 0.026287
 82476/100000: episode: 8253, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000218, mae: 0.003524, mean_q: 0.025176
 82486/100000: episode: 8254, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000426, mae: 0.005956, mean_q: 0.026994
 82496/100000: episode: 8255, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000043, mae: 0.003390, mean_q: 0.027209
 82506/100000: episode: 8256, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002871, mean_q: 0.024830
 82516/100000: episode: 8257, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000089, mae: 0.003798, mean_q: 0.024551
 82526/100000: episode: 8258, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000240, mae: 0.004643, mean_q: 0.027251
 82536/100000: episode: 8259, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000115, mae: 0.004446, mean_q: 0.026868
 82546/100000: episode: 8260, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000116, mae: 0.003954, mean_q: 0.025796
[Info] 1-TH LEVEL FOUND: 0.028756558895111084, Considering 100/100 traces
 82556/100000: episode: 8261, duration: 0.732s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.001712, mae: 0.008218, mean_q: 0.027655
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.028756558895111084
1
 82566/100000: episode: 8262, duration: 0.502s, episode steps: 10, steps per second: 20, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000097, mae: 0.004989, mean_q: 0.028149
 82576/100000: episode: 8263, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000241, mae: 0.004187, mean_q: 0.025791
 82586/100000: episode: 8264, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000040, mae: 0.002915, mean_q: 0.025670
 82596/100000: episode: 8265, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000057, mae: 0.002905, mean_q: 0.025716
 82606/100000: episode: 8266, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000202, mae: 0.003366, mean_q: 0.025674
 82616/100000: episode: 8267, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000023, mae: 0.002736, mean_q: 0.025423
 82626/100000: episode: 8268, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000060, mae: 0.003706, mean_q: 0.024983
 82636/100000: episode: 8269, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000014, mae: 0.002112, mean_q: 0.025103
 82646/100000: episode: 8270, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000224, mae: 0.003697, mean_q: 0.025670
 82656/100000: episode: 8271, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000023, mae: 0.002426, mean_q: 0.025690
 82666/100000: episode: 8272, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.001530, mae: 0.006438, mean_q: 0.024960
 82676/100000: episode: 8273, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000040, mae: 0.003360, mean_q: 0.027047
 82686/100000: episode: 8274, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000220, mae: 0.003678, mean_q: 0.025067
 82696/100000: episode: 8275, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002211, mean_q: 0.025337
 82706/100000: episode: 8276, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000075, mae: 0.003122, mean_q: 0.026071
 82716/100000: episode: 8277, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000050, mae: 0.003956, mean_q: 0.024928
 82726/100000: episode: 8278, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000057, mae: 0.004025, mean_q: 0.023709
 82736/100000: episode: 8279, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000225, mae: 0.004116, mean_q: 0.025828
 82746/100000: episode: 8280, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000206, mae: 0.003575, mean_q: 0.026024
 82756/100000: episode: 8281, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000042, mae: 0.002527, mean_q: 0.025619
 82766/100000: episode: 8282, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000077, mae: 0.003740, mean_q: 0.024472
 82776/100000: episode: 8283, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000020, mae: 0.002286, mean_q: 0.024964
 82786/100000: episode: 8284, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000058, mae: 0.003256, mean_q: 0.024536
 82796/100000: episode: 8285, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000076, mae: 0.003345, mean_q: 0.024724
 82806/100000: episode: 8286, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000081, mae: 0.003265, mean_q: 0.025183
 82816/100000: episode: 8287, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000222, mae: 0.004100, mean_q: 0.024106
 82826/100000: episode: 8288, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000426, mae: 0.005661, mean_q: 0.025717
 82836/100000: episode: 8289, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000094, mae: 0.004428, mean_q: 0.026703
 82846/100000: episode: 8290, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000293, mae: 0.004666, mean_q: 0.025846
 82856/100000: episode: 8291, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000039, mae: 0.002996, mean_q: 0.026402
 82866/100000: episode: 8292, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000224, mae: 0.004405, mean_q: 0.024096
 82876/100000: episode: 8293, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000056, mae: 0.003117, mean_q: 0.024508
 82886/100000: episode: 8294, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000094, mae: 0.003611, mean_q: 0.024587
 82896/100000: episode: 8295, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000019, mae: 0.002123, mean_q: 0.024875
 82906/100000: episode: 8296, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000116, mae: 0.004545, mean_q: 0.023919
 82916/100000: episode: 8297, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000075, mae: 0.003381, mean_q: 0.025651
 82926/100000: episode: 8298, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000198, mae: 0.002830, mean_q: 0.024762
 82936/100000: episode: 8299, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000052, mae: 0.002221, mean_q: 0.024884
 82946/100000: episode: 8300, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000410, mae: 0.004867, mean_q: 0.025282
 82956/100000: episode: 8301, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000013, mae: 0.001874, mean_q: 0.024764
 82966/100000: episode: 8302, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000206, mae: 0.004324, mean_q: 0.023347
 82976/100000: episode: 8303, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000039, mae: 0.002477, mean_q: 0.024936
 82986/100000: episode: 8304, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000055, mae: 0.002374, mean_q: 0.024973
 82996/100000: episode: 8305, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000033, mae: 0.002714, mean_q: 0.023749
Step 83000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.83000.hdf5
 83006/100000: episode: 8306, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000042, mae: 0.003531, mean_q: 0.023424
 83016/100000: episode: 8307, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000042, mae: 0.003161, mean_q: 0.023846
 83026/100000: episode: 8308, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000232, mae: 0.003314, mean_q: 0.024956
 83036/100000: episode: 8309, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000050, mae: 0.002733, mean_q: 0.025599
 83046/100000: episode: 8310, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000232, mae: 0.004785, mean_q: 0.025557
 83056/100000: episode: 8311, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000080, mae: 0.003681, mean_q: 0.024146
 83066/100000: episode: 8312, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002433, mean_q: 0.024277
 83076/100000: episode: 8313, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000083, mae: 0.004002, mean_q: 0.023961
 83086/100000: episode: 8314, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000017, mae: 0.002252, mean_q: 0.023983
 83096/100000: episode: 8315, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000055, mae: 0.003325, mean_q: 0.023267
 83106/100000: episode: 8316, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000246, mae: 0.004352, mean_q: 0.024459
 83116/100000: episode: 8317, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000246, mae: 0.004644, mean_q: 0.025207
 83126/100000: episode: 8318, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000261, mae: 0.004588, mean_q: 0.025226
 83136/100000: episode: 8319, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000042, mae: 0.003002, mean_q: 0.025228
 83146/100000: episode: 8320, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000013, mae: 0.002318, mean_q: 0.023448
 83156/100000: episode: 8321, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000032, mae: 0.002758, mean_q: 0.023057
 83166/100000: episode: 8322, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000037, mae: 0.002936, mean_q: 0.025287
 83176/100000: episode: 8323, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000245, mae: 0.004206, mean_q: 0.024395
 83186/100000: episode: 8324, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000035, mae: 0.002210, mean_q: 0.023990
 83196/100000: episode: 8325, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000205, mae: 0.003232, mean_q: 0.024195
 83206/100000: episode: 8326, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000060, mae: 0.003347, mean_q: 0.023717
 83216/100000: episode: 8327, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002835, mean_q: 0.023560
 83226/100000: episode: 8328, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000036, mae: 0.002151, mean_q: 0.023990
 83236/100000: episode: 8329, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000033, mae: 0.002250, mean_q: 0.023581
 83246/100000: episode: 8330, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000053, mae: 0.002830, mean_q: 0.023236
 83256/100000: episode: 8331, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000223, mae: 0.003455, mean_q: 0.024004
 83266/100000: episode: 8332, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000057, mae: 0.002937, mean_q: 0.023804
 83276/100000: episode: 8333, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000071, mae: 0.002697, mean_q: 0.023571
 83286/100000: episode: 8334, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000091, mae: 0.003299, mean_q: 0.024753
 83296/100000: episode: 8335, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000012, mae: 0.001569, mean_q: 0.024252
 83306/100000: episode: 8336, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.003275, mean_q: 0.022632
 83316/100000: episode: 8337, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000064, mae: 0.003917, mean_q: 0.022858
 83326/100000: episode: 8338, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000205, mae: 0.004005, mean_q: 0.025052
 83336/100000: episode: 8339, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000057, mae: 0.003335, mean_q: 0.025050
 83346/100000: episode: 8340, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000013, mae: 0.001447, mean_q: 0.024206
 83356/100000: episode: 8341, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000030, mae: 0.002376, mean_q: 0.023029
 83366/100000: episode: 8342, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.002547, mean_q: 0.022961
 83376/100000: episode: 8343, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000054, mae: 0.002356, mean_q: 0.023888
 83386/100000: episode: 8344, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000056, mae: 0.002633, mean_q: 0.023845
 83396/100000: episode: 8345, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000023, mae: 0.003860, mean_q: 0.021741
 83406/100000: episode: 8346, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000199, mae: 0.003821, mean_q: 0.021656
 83416/100000: episode: 8347, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000042, mae: 0.003058, mean_q: 0.024218
 83426/100000: episode: 8348, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.002589, mean_q: 0.022862
 83436/100000: episode: 8349, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000076, mae: 0.003103, mean_q: 0.022777
 83446/100000: episode: 8350, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000054, mae: 0.002394, mean_q: 0.023632
 83456/100000: episode: 8351, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002303, mean_q: 0.023205
 83466/100000: episode: 8352, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000245, mae: 0.004099, mean_q: 0.022936
 83476/100000: episode: 8353, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000393, mae: 0.005006, mean_q: 0.024214
 83486/100000: episode: 8354, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000099, mae: 0.004020, mean_q: 0.024016
 83496/100000: episode: 8355, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002096, mean_q: 0.023034
 83506/100000: episode: 8356, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000036, mae: 0.002625, mean_q: 0.022711
 83516/100000: episode: 8357, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000033, mae: 0.002468, mean_q: 0.022294
 83526/100000: episode: 8358, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000043, mae: 0.002845, mean_q: 0.022899
 83536/100000: episode: 8359, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000223, mae: 0.003718, mean_q: 0.022421
 83546/100000: episode: 8360, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000053, mae: 0.002232, mean_q: 0.023103
 83556/100000: episode: 8361, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002182, mean_q: 0.022954
[Info] 1-TH LEVEL FOUND: 0.021680552512407303, Considering 100/100 traces
 83566/100000: episode: 8362, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000011, mae: 0.001807, mean_q: 0.022429
[Info] 2-TH LEVEL FOUND: 0.02201877161860466, Considering 100/100 traces
 83576/100000: episode: 8363, duration: 0.709s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000058, mae: 0.003460, mean_q: 0.021790
[Info] 3-TH LEVEL FOUND: 0.023143945261836052, Considering 100/100 traces
 83586/100000: episode: 8364, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000056, mae: 0.002682, mean_q: 0.022385
[Info] 4-TH LEVEL FOUND: 0.024421000853180885, Considering 100/100 traces
 83596/100000: episode: 8365, duration: 0.753s, episode steps: 10, steps per second: 13, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000269, mae: 0.005145, mean_q: 0.023682
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024421000853180885
4
 83606/100000: episode: 8366, duration: 0.593s, episode steps: 10, steps per second: 17, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000042, mae: 0.003339, mean_q: 0.024025
 83616/100000: episode: 8367, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000014, mae: 0.002136, mean_q: 0.022214
 83626/100000: episode: 8368, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000220, mae: 0.003780, mean_q: 0.021621
 83636/100000: episode: 8369, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000077, mae: 0.003509, mean_q: 0.023734
 83646/100000: episode: 8370, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000054, mae: 0.002684, mean_q: 0.022753
 83656/100000: episode: 8371, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000044, mae: 0.003702, mean_q: 0.021612
 83666/100000: episode: 8372, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000244, mae: 0.003969, mean_q: 0.022229
 83676/100000: episode: 8373, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000050, mae: 0.002440, mean_q: 0.023439
 83686/100000: episode: 8374, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000038, mae: 0.002375, mean_q: 0.022602
 83696/100000: episode: 8375, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000196, mae: 0.002538, mean_q: 0.022131
 83706/100000: episode: 8376, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000425, mae: 0.006402, mean_q: 0.025067
 83716/100000: episode: 8377, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000059, mae: 0.004269, mean_q: 0.025017
 83726/100000: episode: 8378, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000245, mae: 0.005038, mean_q: 0.020915
 83736/100000: episode: 8379, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000407, mae: 0.005566, mean_q: 0.024536
 83746/100000: episode: 8380, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000203, mae: 0.005459, mean_q: 0.026376
 83756/100000: episode: 8381, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000034, mae: 0.002669, mean_q: 0.022580
 83766/100000: episode: 8382, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000202, mae: 0.003554, mean_q: 0.022224
 83776/100000: episode: 8383, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000201, mae: 0.003677, mean_q: 0.024552
 83786/100000: episode: 8384, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000032, mae: 0.002070, mean_q: 0.023021
 83796/100000: episode: 8385, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000072, mae: 0.003144, mean_q: 0.022306
 83806/100000: episode: 8386, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000078, mae: 0.003123, mean_q: 0.023561
 83816/100000: episode: 8387, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000018, mae: 0.002203, mean_q: 0.023706
 83826/100000: episode: 8388, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000221, mae: 0.003903, mean_q: 0.021901
 83836/100000: episode: 8389, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000222, mae: 0.003676, mean_q: 0.023706
 83846/100000: episode: 8390, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002600, mean_q: 0.024135
 83856/100000: episode: 8391, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000202, mae: 0.003812, mean_q: 0.021606
 83866/100000: episode: 8392, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000057, mae: 0.002721, mean_q: 0.022941
 83876/100000: episode: 8393, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000207, mae: 0.003557, mean_q: 0.023674
 83886/100000: episode: 8394, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000387, mae: 0.004342, mean_q: 0.024057
 83896/100000: episode: 8395, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.002295, mean_q: 0.022996
 83906/100000: episode: 8396, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.003117, mean_q: 0.021860
 83916/100000: episode: 8397, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000199, mae: 0.003063, mean_q: 0.022172
 83926/100000: episode: 8398, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000018, mae: 0.002145, mean_q: 0.023356
 83936/100000: episode: 8399, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000244, mae: 0.003896, mean_q: 0.022501
 83946/100000: episode: 8400, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000013, mae: 0.001663, mean_q: 0.023200
 83956/100000: episode: 8401, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000226, mae: 0.004148, mean_q: 0.021809
 83966/100000: episode: 8402, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000057, mae: 0.002781, mean_q: 0.023335
 83976/100000: episode: 8403, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000008, mae: 0.001320, mean_q: 0.022703
 83986/100000: episode: 8404, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.002587, mean_q: 0.021896
 83996/100000: episode: 8405, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000037, mae: 0.002217, mean_q: 0.022812
Step 84000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.84000.hdf5
 84006/100000: episode: 8406, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000223, mae: 0.003913, mean_q: 0.021712
 84016/100000: episode: 8407, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000014, mae: 0.001717, mean_q: 0.022336
 84026/100000: episode: 8408, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000239, mae: 0.003515, mean_q: 0.022741
 84036/100000: episode: 8409, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000019, mae: 0.002393, mean_q: 0.022654
 84046/100000: episode: 8410, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.003428, mean_q: 0.020946
 84056/100000: episode: 8411, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000036, mae: 0.002329, mean_q: 0.022108
 84066/100000: episode: 8412, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000444, mae: 0.005144, mean_q: 0.023268
 84076/100000: episode: 8413, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000037, mae: 0.003561, mean_q: 0.024442
 84086/100000: episode: 8414, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.003145, mean_q: 0.021325
 84096/100000: episode: 8415, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.002094, mean_q: 0.021752
 84106/100000: episode: 8416, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000037, mae: 0.002662, mean_q: 0.021683
 84116/100000: episode: 8417, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000220, mae: 0.003176, mean_q: 0.022592
 84126/100000: episode: 8418, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000013, mae: 0.002055, mean_q: 0.023095
 84136/100000: episode: 8419, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.000095, mae: 0.003567, mean_q: 0.021641
 84146/100000: episode: 8420, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000269, mae: 0.005932, mean_q: 0.024237
 84156/100000: episode: 8421, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000017, mae: 0.002623, mean_q: 0.023656
 84166/100000: episode: 8422, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000202, mae: 0.003916, mean_q: 0.021050
 84176/100000: episode: 8423, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000074, mae: 0.003003, mean_q: 0.022436
 84186/100000: episode: 8424, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000021, mae: 0.002724, mean_q: 0.023219
 84196/100000: episode: 8425, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000241, mae: 0.003853, mean_q: 0.021577
 84206/100000: episode: 8426, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000035, mae: 0.002734, mean_q: 0.023601
 84216/100000: episode: 8427, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000057, mae: 0.003074, mean_q: 0.021778
 84226/100000: episode: 8428, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000246, mae: 0.003929, mean_q: 0.022283
 84236/100000: episode: 8429, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000262, mae: 0.005575, mean_q: 0.024547
 84246/100000: episode: 8430, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000243, mae: 0.003818, mean_q: 0.023071
 84256/100000: episode: 8431, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000201, mae: 0.002632, mean_q: 0.022659
 84266/100000: episode: 8432, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000019, mae: 0.002189, mean_q: 0.022416
 84276/100000: episode: 8433, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000020, mae: 0.002830, mean_q: 0.021689
 84286/100000: episode: 8434, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000228, mae: 0.004436, mean_q: 0.021375
 84296/100000: episode: 8435, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000224, mae: 0.003693, mean_q: 0.023118
 84306/100000: episode: 8436, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000058, mae: 0.003551, mean_q: 0.023810
 84316/100000: episode: 8437, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000035, mae: 0.002981, mean_q: 0.021389
 84326/100000: episode: 8438, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000052, mae: 0.002897, mean_q: 0.021450
 84336/100000: episode: 8439, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000041, mae: 0.003154, mean_q: 0.023414
 84346/100000: episode: 8440, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.002240, mean_q: 0.022301
 84356/100000: episode: 8441, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000013, mae: 0.002344, mean_q: 0.021203
 84366/100000: episode: 8442, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000037, mae: 0.002483, mean_q: 0.021785
 84376/100000: episode: 8443, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000019, mae: 0.001968, mean_q: 0.022097
 84386/100000: episode: 8444, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000011, mae: 0.001649, mean_q: 0.021659
 84396/100000: episode: 8445, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000074, mae: 0.003282, mean_q: 0.021221
 84406/100000: episode: 8446, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000031, mae: 0.001839, mean_q: 0.022481
 84416/100000: episode: 8447, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002795, mean_q: 0.021636
 84426/100000: episode: 8448, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000435, mae: 0.005342, mean_q: 0.021566
 84436/100000: episode: 8449, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000055, mae: 0.003823, mean_q: 0.024007
 84446/100000: episode: 8450, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000219, mae: 0.002914, mean_q: 0.022342
 84456/100000: episode: 8451, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000248, mae: 0.004333, mean_q: 0.022499
 84466/100000: episode: 8452, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000016, mae: 0.002150, mean_q: 0.022243
 84476/100000: episode: 8453, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000034, mae: 0.003026, mean_q: 0.020739
 84486/100000: episode: 8454, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000010, mae: 0.001489, mean_q: 0.021701
 84496/100000: episode: 8455, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000059, mae: 0.003267, mean_q: 0.021241
 84506/100000: episode: 8456, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000016, mae: 0.002283, mean_q: 0.021308
 84516/100000: episode: 8457, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000209, mae: 0.004135, mean_q: 0.020603
 84526/100000: episode: 8458, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000057, mae: 0.002830, mean_q: 0.021287
 84536/100000: episode: 8459, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000388, mae: 0.003639, mean_q: 0.021478
 84546/100000: episode: 8460, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000018, mae: 0.002588, mean_q: 0.022773
 84556/100000: episode: 8461, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000406, mae: 0.004117, mean_q: 0.021100
 84566/100000: episode: 8462, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000200, mae: 0.003931, mean_q: 0.023540
 84576/100000: episode: 8463, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000118, mae: 0.004478, mean_q: 0.022962
 84586/100000: episode: 8464, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000076, mae: 0.003219, mean_q: 0.021583
 84596/100000: episode: 8465, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000058, mae: 0.003187, mean_q: 0.021211
[Info] 1-TH LEVEL FOUND: 0.021545303985476494, Considering 100/100 traces
 84606/100000: episode: 8466, duration: 0.747s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000055, mae: 0.002572, mean_q: 0.021524
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021545303985476494
1
 84616/100000: episode: 8467, duration: 0.493s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000016, mae: 0.002130, mean_q: 0.021212
 84626/100000: episode: 8468, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000034, mae: 0.002889, mean_q: 0.020452
 84636/100000: episode: 8469, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000033, mae: 0.002068, mean_q: 0.021117
 84646/100000: episode: 8470, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000205, mae: 0.003502, mean_q: 0.020794
 84656/100000: episode: 8471, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000093, mae: 0.003375, mean_q: 0.021958
 84666/100000: episode: 8472, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000014, mae: 0.002044, mean_q: 0.021841
 84676/100000: episode: 8473, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000414, mae: 0.004903, mean_q: 0.021470
 84686/100000: episode: 8474, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000204, mae: 0.005324, mean_q: 0.024800
 84696/100000: episode: 8475, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002821, mean_q: 0.021062
 84706/100000: episode: 8476, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000015, mae: 0.003261, mean_q: 0.019560
 84716/100000: episode: 8477, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000222, mae: 0.003571, mean_q: 0.022066
 84726/100000: episode: 8478, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000034, mae: 0.002591, mean_q: 0.022487
 84736/100000: episode: 8479, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000018, mae: 0.002725, mean_q: 0.020559
 84746/100000: episode: 8480, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.002401, mean_q: 0.020526
 84756/100000: episode: 8481, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000050, mae: 0.002183, mean_q: 0.020954
 84766/100000: episode: 8482, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000035, mae: 0.001977, mean_q: 0.021208
 84776/100000: episode: 8483, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.002116, mean_q: 0.021326
 84786/100000: episode: 8484, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000221, mae: 0.003300, mean_q: 0.020741
 84796/100000: episode: 8485, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000207, mae: 0.003241, mean_q: 0.021373
 84806/100000: episode: 8486, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000406, mae: 0.003868, mean_q: 0.021602
 84816/100000: episode: 8487, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000028, mae: 0.002124, mean_q: 0.022483
 84826/100000: episode: 8488, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000034, mae: 0.002249, mean_q: 0.020897
 84836/100000: episode: 8489, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000243, mae: 0.003874, mean_q: 0.021841
 84846/100000: episode: 8490, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000041, mae: 0.002929, mean_q: 0.021713
 84856/100000: episode: 8491, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [1.000, 10.000], loss: 0.000263, mae: 0.004273, mean_q: 0.021382
 84866/100000: episode: 8492, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000430, mae: 0.007311, mean_q: 0.024796
 84876/100000: episode: 8493, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000039, mae: 0.003695, mean_q: 0.022917
 84886/100000: episode: 8494, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000249, mae: 0.005405, mean_q: 0.019454
 84896/100000: episode: 8495, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000033, mae: 0.002562, mean_q: 0.022722
 84906/100000: episode: 8496, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002461, mean_q: 0.022049
 84916/100000: episode: 8497, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000072, mae: 0.002808, mean_q: 0.020975
 84926/100000: episode: 8498, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000225, mae: 0.003622, mean_q: 0.022176
 84936/100000: episode: 8499, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000262, mae: 0.004869, mean_q: 0.023190
 84946/100000: episode: 8500, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000053, mae: 0.002548, mean_q: 0.022203
 84956/100000: episode: 8501, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000062, mae: 0.003398, mean_q: 0.021220
 84966/100000: episode: 8502, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000434, mae: 0.005695, mean_q: 0.022807
 84976/100000: episode: 8503, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000111, mae: 0.004621, mean_q: 0.023962
 84986/100000: episode: 8504, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000240, mae: 0.003393, mean_q: 0.022318
 84996/100000: episode: 8505, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000267, mae: 0.004962, mean_q: 0.023146
Step 85000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.85000.hdf5
 85006/100000: episode: 8506, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000031, mae: 0.002253, mean_q: 0.023166
 85016/100000: episode: 8507, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000415, mae: 0.004910, mean_q: 0.022602
 85026/100000: episode: 8508, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000078, mae: 0.003669, mean_q: 0.023466
 85036/100000: episode: 8509, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000243, mae: 0.003993, mean_q: 0.023139
 85046/100000: episode: 8510, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000092, mae: 0.002879, mean_q: 0.022555
 85056/100000: episode: 8511, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000242, mae: 0.003632, mean_q: 0.022089
 85066/100000: episode: 8512, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000074, mae: 0.003734, mean_q: 0.023980
 85076/100000: episode: 8513, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000035, mae: 0.002439, mean_q: 0.022519
 85086/100000: episode: 8514, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000014, mae: 0.002603, mean_q: 0.021245
 85096/100000: episode: 8515, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000057, mae: 0.002955, mean_q: 0.021709
 85106/100000: episode: 8516, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002192, mean_q: 0.022342
 85116/100000: episode: 8517, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000099, mae: 0.003698, mean_q: 0.022825
 85126/100000: episode: 8518, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.002408, mean_q: 0.022060
 85136/100000: episode: 8519, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000198, mae: 0.002847, mean_q: 0.021636
 85146/100000: episode: 8520, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000272, mae: 0.004978, mean_q: 0.022849
 85156/100000: episode: 8521, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.004032, mean_q: 0.025456
 85166/100000: episode: 8522, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000238, mae: 0.003101, mean_q: 0.022774
 85176/100000: episode: 8523, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000015, mae: 0.002162, mean_q: 0.021984
 85186/100000: episode: 8524, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000207, mae: 0.003662, mean_q: 0.023102
 85196/100000: episode: 8525, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.002623, mean_q: 0.022693
 85206/100000: episode: 8526, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000037, mae: 0.002561, mean_q: 0.021961
 85216/100000: episode: 8527, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000227, mae: 0.003789, mean_q: 0.022243
 85226/100000: episode: 8528, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000265, mae: 0.004148, mean_q: 0.022832
 85236/100000: episode: 8529, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000214, mae: 0.003000, mean_q: 0.023394
 85246/100000: episode: 8530, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000225, mae: 0.003488, mean_q: 0.023052
 85256/100000: episode: 8531, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000266, mae: 0.005098, mean_q: 0.023859
 85266/100000: episode: 8532, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000034, mae: 0.002474, mean_q: 0.023591
 85276/100000: episode: 8533, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000040, mae: 0.003287, mean_q: 0.021685
 85286/100000: episode: 8534, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000220, mae: 0.003689, mean_q: 0.023489
 85296/100000: episode: 8535, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000057, mae: 0.003089, mean_q: 0.023638
 85306/100000: episode: 8536, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000038, mae: 0.002926, mean_q: 0.021971
 85316/100000: episode: 8537, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000241, mae: 0.003909, mean_q: 0.023050
 85326/100000: episode: 8538, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000197, mae: 0.002652, mean_q: 0.023344
 85336/100000: episode: 8539, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000073, mae: 0.002834, mean_q: 0.022399
 85346/100000: episode: 8540, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000399, mae: 0.005998, mean_q: 0.024335
 85356/100000: episode: 8541, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000036, mae: 0.002773, mean_q: 0.023796
 85366/100000: episode: 8542, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000033, mae: 0.003102, mean_q: 0.021352
 85376/100000: episode: 8543, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000287, mae: 0.005178, mean_q: 0.023063
 85386/100000: episode: 8544, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000078, mae: 0.003564, mean_q: 0.023812
 85396/100000: episode: 8545, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000218, mae: 0.002976, mean_q: 0.022945
 85406/100000: episode: 8546, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000200, mae: 0.002613, mean_q: 0.022912
 85416/100000: episode: 8547, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000015, mae: 0.001797, mean_q: 0.022914
 85426/100000: episode: 8548, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000054, mae: 0.002951, mean_q: 0.022032
 85436/100000: episode: 8549, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000246, mae: 0.004297, mean_q: 0.023436
 85446/100000: episode: 8550, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000056, mae: 0.002661, mean_q: 0.023228
 85456/100000: episode: 8551, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000222, mae: 0.003690, mean_q: 0.022025
 85466/100000: episode: 8552, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000038, mae: 0.002770, mean_q: 0.023637
 85476/100000: episode: 8553, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000031, mae: 0.001958, mean_q: 0.022640
 85486/100000: episode: 8554, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000051, mae: 0.002414, mean_q: 0.022213
 85496/100000: episode: 8555, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000281, mae: 0.005392, mean_q: 0.024453
 85506/100000: episode: 8556, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000038, mae: 0.003247, mean_q: 0.023541
 85516/100000: episode: 8557, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000226, mae: 0.003848, mean_q: 0.022061
 85526/100000: episode: 8558, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002273, mean_q: 0.023085
 85536/100000: episode: 8559, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000016, mae: 0.002219, mean_q: 0.022220
 85546/100000: episode: 8560, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000058, mae: 0.003413, mean_q: 0.021807
 85556/100000: episode: 8561, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000022, mae: 0.003152, mean_q: 0.021652
 85566/100000: episode: 8562, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000033, mae: 0.002761, mean_q: 0.021215
 85576/100000: episode: 8563, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000058, mae: 0.002656, mean_q: 0.022193
 85586/100000: episode: 8564, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000219, mae: 0.003341, mean_q: 0.023149
 85596/100000: episode: 8565, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000465, mae: 0.005362, mean_q: 0.023008
 85606/100000: episode: 8566, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000017, mae: 0.003064, mean_q: 0.024288
[Info] 1-TH LEVEL FOUND: 0.021670902147889137, Considering 100/100 traces
 85616/100000: episode: 8567, duration: 0.723s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000054, mae: 0.003153, mean_q: 0.021411
[Info] 2-TH LEVEL FOUND: 0.02192951738834381, Considering 100/100 traces
 85626/100000: episode: 8568, duration: 0.716s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000007, mae: 0.001561, mean_q: 0.021881
[Info] 3-TH LEVEL FOUND: 0.022209059447050095, Considering 100/100 traces
 85636/100000: episode: 8569, duration: 0.900s, episode steps: 10, steps per second: 11, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000051, mae: 0.002206, mean_q: 0.022105
[Info] 4-TH LEVEL FOUND: 0.022428225725889206, Considering 100/100 traces
 85646/100000: episode: 8570, duration: 0.840s, episode steps: 10, steps per second: 12, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000056, mae: 0.002602, mean_q: 0.022110
[Info] 5-TH LEVEL FOUND: 0.023724669590592384, Considering 100/100 traces
 85656/100000: episode: 8571, duration: 0.789s, episode steps: 10, steps per second: 13, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000219, mae: 0.002973, mean_q: 0.022880
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023724669590592384
5
 85666/100000: episode: 8572, duration: 0.596s, episode steps: 10, steps per second: 17, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000072, mae: 0.003306, mean_q: 0.023543
 85676/100000: episode: 8573, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000012, mae: 0.001784, mean_q: 0.022007
 85686/100000: episode: 8574, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000015, mae: 0.002774, mean_q: 0.021084
 85696/100000: episode: 8575, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000030, mae: 0.002085, mean_q: 0.021492
 85706/100000: episode: 8576, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000040, mae: 0.002357, mean_q: 0.022379
 85716/100000: episode: 8577, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000008, mae: 0.001658, mean_q: 0.021468
 85726/100000: episode: 8578, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000220, mae: 0.003198, mean_q: 0.021944
 85736/100000: episode: 8579, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000043, mae: 0.003077, mean_q: 0.022621
 85746/100000: episode: 8580, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000078, mae: 0.003032, mean_q: 0.022144
 85756/100000: episode: 8581, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000223, mae: 0.004125, mean_q: 0.023421
 85766/100000: episode: 8582, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000061, mae: 0.003474, mean_q: 0.022017
 85776/100000: episode: 8583, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000222, mae: 0.003411, mean_q: 0.022174
 85786/100000: episode: 8584, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000255, mae: 0.004220, mean_q: 0.023504
 85796/100000: episode: 8585, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000206, mae: 0.003433, mean_q: 0.022660
 85806/100000: episode: 8586, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000266, mae: 0.004329, mean_q: 0.022261
 85816/100000: episode: 8587, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000020, mae: 0.003016, mean_q: 0.023716
 85826/100000: episode: 8588, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000203, mae: 0.003716, mean_q: 0.021170
 85836/100000: episode: 8589, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000241, mae: 0.003992, mean_q: 0.022952
 85846/100000: episode: 8590, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000201, mae: 0.003274, mean_q: 0.023498
 85856/100000: episode: 8591, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002360, mean_q: 0.022635
 85866/100000: episode: 8592, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000228, mae: 0.003689, mean_q: 0.022338
 85876/100000: episode: 8593, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000057, mae: 0.002819, mean_q: 0.022259
 85886/100000: episode: 8594, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000073, mae: 0.003038, mean_q: 0.021609
 85896/100000: episode: 8595, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000034, mae: 0.002361, mean_q: 0.023043
 85906/100000: episode: 8596, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [1.000, 10.000], loss: 0.000220, mae: 0.003186, mean_q: 0.022882
 85916/100000: episode: 8597, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000388, mae: 0.005252, mean_q: 0.024339
 85926/100000: episode: 8598, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000085, mae: 0.004256, mean_q: 0.022376
 85936/100000: episode: 8599, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000032, mae: 0.002628, mean_q: 0.021433
 85946/100000: episode: 8600, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000226, mae: 0.003661, mean_q: 0.022145
 85956/100000: episode: 8601, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000264, mae: 0.005058, mean_q: 0.023682
 85966/100000: episode: 8602, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000198, mae: 0.002815, mean_q: 0.022838
 85976/100000: episode: 8603, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000017, mae: 0.002690, mean_q: 0.023877
 85986/100000: episode: 8604, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000060, mae: 0.003287, mean_q: 0.022309
 85996/100000: episode: 8605, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000037, mae: 0.003085, mean_q: 0.021398
Step 86000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.86000.hdf5
 86006/100000: episode: 8606, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.002439, mean_q: 0.022459
 86016/100000: episode: 8607, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000017, mae: 0.001948, mean_q: 0.022182
 86026/100000: episode: 8608, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000433, mae: 0.005383, mean_q: 0.022682
 86036/100000: episode: 8609, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000392, mae: 0.006566, mean_q: 0.025388
 86046/100000: episode: 8610, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000019, mae: 0.002627, mean_q: 0.022531
 86056/100000: episode: 8611, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000023, mae: 0.003559, mean_q: 0.021033
 86066/100000: episode: 8612, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002378, mean_q: 0.022280
 86076/100000: episode: 8613, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000057, mae: 0.002450, mean_q: 0.022782
 86086/100000: episode: 8614, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000230, mae: 0.003896, mean_q: 0.022912
 86096/100000: episode: 8615, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000044, mae: 0.003069, mean_q: 0.023072
 86106/100000: episode: 8616, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000222, mae: 0.003241, mean_q: 0.022622
 86116/100000: episode: 8617, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002111, mean_q: 0.022753
 86126/100000: episode: 8618, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000041, mae: 0.003098, mean_q: 0.021810
 86136/100000: episode: 8619, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000015, mae: 0.002479, mean_q: 0.021557
 86146/100000: episode: 8620, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000051, mae: 0.002618, mean_q: 0.021588
 86156/100000: episode: 8621, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000037, mae: 0.002725, mean_q: 0.023069
 86166/100000: episode: 8622, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002982, mean_q: 0.021431
 86176/100000: episode: 8623, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000039, mae: 0.003056, mean_q: 0.021385
 86186/100000: episode: 8624, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000033, mae: 0.002241, mean_q: 0.021380
 86196/100000: episode: 8625, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000058, mae: 0.003033, mean_q: 0.022794
 86206/100000: episode: 8626, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000241, mae: 0.003359, mean_q: 0.022015
 86216/100000: episode: 8627, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000059, mae: 0.003691, mean_q: 0.023490
 86226/100000: episode: 8628, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000034, mae: 0.002320, mean_q: 0.021756
 86236/100000: episode: 8629, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000223, mae: 0.003746, mean_q: 0.021146
 86246/100000: episode: 8630, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000079, mae: 0.003734, mean_q: 0.023156
 86256/100000: episode: 8631, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000033, mae: 0.002295, mean_q: 0.022905
 86266/100000: episode: 8632, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000016, mae: 0.002617, mean_q: 0.021174
 86276/100000: episode: 8633, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000057, mae: 0.003665, mean_q: 0.020618
 86286/100000: episode: 8634, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000038, mae: 0.002379, mean_q: 0.022147
 86296/100000: episode: 8635, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000032, mae: 0.002620, mean_q: 0.021001
 86306/100000: episode: 8636, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000225, mae: 0.003743, mean_q: 0.021501
 86316/100000: episode: 8637, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000412, mae: 0.004598, mean_q: 0.022277
 86326/100000: episode: 8638, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000052, mae: 0.002928, mean_q: 0.022993
 86336/100000: episode: 8639, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000212, mae: 0.003891, mean_q: 0.021381
 86346/100000: episode: 8640, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000241, mae: 0.003876, mean_q: 0.022327
 86356/100000: episode: 8641, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000600, mae: 0.007805, mean_q: 0.024555
 86366/100000: episode: 8642, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000042, mae: 0.003706, mean_q: 0.023353
 86376/100000: episode: 8643, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000271, mae: 0.005204, mean_q: 0.021237
 86386/100000: episode: 8644, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000216, mae: 0.003686, mean_q: 0.023679
 86396/100000: episode: 8645, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000076, mae: 0.003211, mean_q: 0.023089
 86406/100000: episode: 8646, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000018, mae: 0.002865, mean_q: 0.021147
 86416/100000: episode: 8647, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000032, mae: 0.002398, mean_q: 0.021491
 86426/100000: episode: 8648, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000013, mae: 0.001967, mean_q: 0.021588
 86436/100000: episode: 8649, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.002589, mean_q: 0.021355
 86446/100000: episode: 8650, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000074, mae: 0.002673, mean_q: 0.022133
 86456/100000: episode: 8651, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000240, mae: 0.004666, mean_q: 0.023825
 86466/100000: episode: 8652, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000033, mae: 0.002642, mean_q: 0.023391
 86476/100000: episode: 8653, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000055, mae: 0.003490, mean_q: 0.020682
 86486/100000: episode: 8654, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000057, mae: 0.002963, mean_q: 0.022613
 86496/100000: episode: 8655, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000426, mae: 0.005645, mean_q: 0.023811
 86506/100000: episode: 8656, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000016, mae: 0.002429, mean_q: 0.022915
 86516/100000: episode: 8657, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000058, mae: 0.003592, mean_q: 0.020865
 86526/100000: episode: 8658, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000018, mae: 0.002061, mean_q: 0.022109
 86536/100000: episode: 8659, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000034, mae: 0.002573, mean_q: 0.021289
 86546/100000: episode: 8660, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000217, mae: 0.003586, mean_q: 0.023504
 86556/100000: episode: 8661, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000020, mae: 0.002611, mean_q: 0.022083
 86566/100000: episode: 8662, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000211, mae: 0.004547, mean_q: 0.020681
 86576/100000: episode: 8663, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000057, mae: 0.003404, mean_q: 0.023404
 86586/100000: episode: 8664, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000078, mae: 0.003340, mean_q: 0.022224
 86596/100000: episode: 8665, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000021, mae: 0.002749, mean_q: 0.021384
 86606/100000: episode: 8666, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000220, mae: 0.003248, mean_q: 0.021888
 86616/100000: episode: 8667, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000279, mae: 0.005269, mean_q: 0.023812
 86626/100000: episode: 8668, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000222, mae: 0.003300, mean_q: 0.022727
 86636/100000: episode: 8669, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000037, mae: 0.002445, mean_q: 0.021888
 86646/100000: episode: 8670, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000040, mae: 0.002782, mean_q: 0.021798
 86656/100000: episode: 8671, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000042, mae: 0.003370, mean_q: 0.021164
[Info] 1-TH LEVEL FOUND: 0.0207240991294384, Considering 100/100 traces
 86666/100000: episode: 8672, duration: 0.738s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000035, mae: 0.003148, mean_q: 0.020673
[Info] 2-TH LEVEL FOUND: 0.022783461958169937, Considering 100/100 traces
 86676/100000: episode: 8673, duration: 0.665s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000205, mae: 0.003381, mean_q: 0.021610
[Info] 3-TH LEVEL FOUND: 0.023547539487481117, Considering 100/100 traces
 86686/100000: episode: 8674, duration: 0.731s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000281, mae: 0.004624, mean_q: 0.022735
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.023547539487481117
3
 86696/100000: episode: 8675, duration: 0.506s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000040, mae: 0.003505, mean_q: 0.023578
 86706/100000: episode: 8676, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000054, mae: 0.002799, mean_q: 0.021430
 86716/100000: episode: 8677, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000225, mae: 0.003722, mean_q: 0.022195
 86726/100000: episode: 8678, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000064, mae: 0.003686, mean_q: 0.022753
 86736/100000: episode: 8679, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000096, mae: 0.003945, mean_q: 0.020912
 86746/100000: episode: 8680, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000033, mae: 0.002270, mean_q: 0.022604
 86756/100000: episode: 8681, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000032, mae: 0.002397, mean_q: 0.021341
 86766/100000: episode: 8682, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000413, mae: 0.005031, mean_q: 0.021745
 86776/100000: episode: 8683, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000083, mae: 0.005014, mean_q: 0.024074
 86786/100000: episode: 8684, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000058, mae: 0.002963, mean_q: 0.021526
 86796/100000: episode: 8685, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000014, mae: 0.002143, mean_q: 0.021257
 86806/100000: episode: 8686, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000075, mae: 0.002885, mean_q: 0.021570
 86816/100000: episode: 8687, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000036, mae: 0.002455, mean_q: 0.022632
 86826/100000: episode: 8688, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000077, mae: 0.003126, mean_q: 0.021534
 86836/100000: episode: 8689, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000382, mae: 0.003497, mean_q: 0.022329
 86846/100000: episode: 8690, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000017, mae: 0.002867, mean_q: 0.023555
 86856/100000: episode: 8691, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000073, mae: 0.003043, mean_q: 0.021212
 86866/100000: episode: 8692, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000031, mae: 0.001896, mean_q: 0.021742
 86876/100000: episode: 8693, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000055, mae: 0.002559, mean_q: 0.022454
 86886/100000: episode: 8694, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000263, mae: 0.004691, mean_q: 0.023015
 86896/100000: episode: 8695, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000052, mae: 0.002248, mean_q: 0.022164
 86906/100000: episode: 8696, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000055, mae: 0.003454, mean_q: 0.020837
 86916/100000: episode: 8697, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000222, mae: 0.003501, mean_q: 0.021985
 86926/100000: episode: 8698, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000059, mae: 0.003013, mean_q: 0.022678
 86936/100000: episode: 8699, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000042, mae: 0.002984, mean_q: 0.021559
 86946/100000: episode: 8700, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000118, mae: 0.003989, mean_q: 0.021585
 86956/100000: episode: 8701, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000036, mae: 0.002684, mean_q: 0.022585
 86966/100000: episode: 8702, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000054, mae: 0.002898, mean_q: 0.021008
 86976/100000: episode: 8703, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000065, mae: 0.003831, mean_q: 0.022860
 86986/100000: episode: 8704, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000051, mae: 0.002412, mean_q: 0.022648
 86996/100000: episode: 8705, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000036, mae: 0.002632, mean_q: 0.021265
Step 87000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.87000.hdf5
 87006/100000: episode: 8706, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000223, mae: 0.003456, mean_q: 0.021747
 87016/100000: episode: 8707, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000402, mae: 0.004650, mean_q: 0.023287
 87026/100000: episode: 8708, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000059, mae: 0.003421, mean_q: 0.023116
 87036/100000: episode: 8709, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000037, mae: 0.003074, mean_q: 0.021018
 87046/100000: episode: 8710, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000264, mae: 0.004465, mean_q: 0.021897
 87056/100000: episode: 8711, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000032, mae: 0.002434, mean_q: 0.022943
 87066/100000: episode: 8712, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000010, mae: 0.002099, mean_q: 0.020931
 87076/100000: episode: 8713, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000060, mae: 0.003276, mean_q: 0.021208
 87086/100000: episode: 8714, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000032, mae: 0.001878, mean_q: 0.021700
 87096/100000: episode: 8715, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000015, mae: 0.002422, mean_q: 0.020927
 87106/100000: episode: 8716, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000060, mae: 0.003323, mean_q: 0.020922
 87116/100000: episode: 8717, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002067, mean_q: 0.021959
 87126/100000: episode: 8718, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000034, mae: 0.002243, mean_q: 0.021149
 87136/100000: episode: 8719, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000012, mae: 0.001972, mean_q: 0.020878
 87146/100000: episode: 8720, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000266, mae: 0.004507, mean_q: 0.021694
 87156/100000: episode: 8721, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000033, mae: 0.002397, mean_q: 0.021935
 87166/100000: episode: 8722, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000011, mae: 0.001568, mean_q: 0.021303
 87176/100000: episode: 8723, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000014, mae: 0.002442, mean_q: 0.020444
 87186/100000: episode: 8724, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000010, mae: 0.001562, mean_q: 0.020787
 87196/100000: episode: 8725, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000019, mae: 0.002325, mean_q: 0.020932
 87206/100000: episode: 8726, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000097, mae: 0.003726, mean_q: 0.020492
 87216/100000: episode: 8727, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000011, mae: 0.001296, mean_q: 0.021307
 87226/100000: episode: 8728, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000075, mae: 0.002716, mean_q: 0.021128
 87236/100000: episode: 8729, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000010, mae: 0.001524, mean_q: 0.020981
 87246/100000: episode: 8730, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000077, mae: 0.003083, mean_q: 0.020594
 87256/100000: episode: 8731, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000057, mae: 0.002826, mean_q: 0.021624
 87266/100000: episode: 8732, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000077, mae: 0.003027, mean_q: 0.020634
 87276/100000: episode: 8733, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000038, mae: 0.002373, mean_q: 0.020906
 87286/100000: episode: 8734, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000011, mae: 0.002078, mean_q: 0.020061
 87296/100000: episode: 8735, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000031, mae: 0.001828, mean_q: 0.020471
 87306/100000: episode: 8736, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000266, mae: 0.004691, mean_q: 0.021945
 87316/100000: episode: 8737, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000013, mae: 0.002231, mean_q: 0.021370
 87326/100000: episode: 8738, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000033, mae: 0.003218, mean_q: 0.019044
 87336/100000: episode: 8739, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000032, mae: 0.001996, mean_q: 0.021014
 87346/100000: episode: 8740, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000059, mae: 0.002806, mean_q: 0.021233
 87356/100000: episode: 8741, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000226, mae: 0.003836, mean_q: 0.021587
 87366/100000: episode: 8742, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000033, mae: 0.002116, mean_q: 0.020342
 87376/100000: episode: 8743, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000036, mae: 0.002630, mean_q: 0.020050
 87386/100000: episode: 8744, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000238, mae: 0.003045, mean_q: 0.020478
 87396/100000: episode: 8745, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000077, mae: 0.003693, mean_q: 0.021939
 87406/100000: episode: 8746, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000074, mae: 0.002693, mean_q: 0.021382
 87416/100000: episode: 8747, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000098, mae: 0.003328, mean_q: 0.020712
 87426/100000: episode: 8748, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000058, mae: 0.002818, mean_q: 0.021423
 87436/100000: episode: 8749, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000038, mae: 0.002516, mean_q: 0.020990
 87446/100000: episode: 8750, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000053, mae: 0.002656, mean_q: 0.019942
 87456/100000: episode: 8751, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000078, mae: 0.003368, mean_q: 0.021566
 87466/100000: episode: 8752, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000034, mae: 0.002203, mean_q: 0.021288
 87476/100000: episode: 8753, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000030, mae: 0.002374, mean_q: 0.019904
 87486/100000: episode: 8754, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000057, mae: 0.002756, mean_q: 0.020354
 87496/100000: episode: 8755, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.002201, mean_q: 0.020591
 87506/100000: episode: 8756, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000074, mae: 0.002770, mean_q: 0.021145
 87516/100000: episode: 8757, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000120, mae: 0.004516, mean_q: 0.021853
 87526/100000: episode: 8758, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.002220, mean_q: 0.020575
 87536/100000: episode: 8759, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000036, mae: 0.002876, mean_q: 0.019655
 87546/100000: episode: 8760, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000038, mae: 0.002513, mean_q: 0.020124
 87556/100000: episode: 8761, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000009, mae: 0.001239, mean_q: 0.020703
 87566/100000: episode: 8762, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000055, mae: 0.002550, mean_q: 0.020149
 87576/100000: episode: 8763, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000223, mae: 0.002981, mean_q: 0.020551
 87586/100000: episode: 8764, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000076, mae: 0.003494, mean_q: 0.021760
 87596/100000: episode: 8765, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000264, mae: 0.003992, mean_q: 0.021239
 87606/100000: episode: 8766, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000223, mae: 0.004441, mean_q: 0.022463
 87616/100000: episode: 8767, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000010, mae: 0.002054, mean_q: 0.019855
 87626/100000: episode: 8768, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000014, mae: 0.002744, mean_q: 0.019286
 87636/100000: episode: 8769, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000032, mae: 0.001813, mean_q: 0.020471
 87646/100000: episode: 8770, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000013, mae: 0.001724, mean_q: 0.020402
 87656/100000: episode: 8771, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000204, mae: 0.003233, mean_q: 0.019885
 87666/100000: episode: 8772, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000056, mae: 0.002603, mean_q: 0.020939
 87676/100000: episode: 8773, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000048, mae: 0.001707, mean_q: 0.020294
 87686/100000: episode: 8774, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000031, mae: 0.001692, mean_q: 0.020457
[Info] 1-TH LEVEL FOUND: 0.019445914775133133, Considering 100/100 traces
 87696/100000: episode: 8775, duration: 0.729s, episode steps: 10, steps per second: 14, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000013, mae: 0.002170, mean_q: 0.019572
[Info] 2-TH LEVEL FOUND: 0.022229453548789024, Considering 100/100 traces
 87706/100000: episode: 8776, duration: 0.762s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000245, mae: 0.004389, mean_q: 0.021358
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.022229453548789024
2
 87716/100000: episode: 8777, duration: 0.645s, episode steps: 10, steps per second: 16, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000243, mae: 0.004413, mean_q: 0.021900
 87726/100000: episode: 8778, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000012, mae: 0.001664, mean_q: 0.020369
 87736/100000: episode: 8779, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000035, mae: 0.002777, mean_q: 0.019485
 87746/100000: episode: 8780, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002437, mean_q: 0.019959
 87756/100000: episode: 8781, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000013, mae: 0.002459, mean_q: 0.019258
 87766/100000: episode: 8782, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000053, mae: 0.002579, mean_q: 0.019468
 87776/100000: episode: 8783, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000076, mae: 0.002905, mean_q: 0.020738
 87786/100000: episode: 8784, duration: 0.094s, episode steps: 10, steps per second: 107, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000028, mae: 0.001478, mean_q: 0.020577
 87796/100000: episode: 8785, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000052, mae: 0.002627, mean_q: 0.019486
 87806/100000: episode: 8786, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000054, mae: 0.002115, mean_q: 0.020204
 87816/100000: episode: 8787, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000033, mae: 0.002976, mean_q: 0.021824
 87826/100000: episode: 8788, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000014, mae: 0.001834, mean_q: 0.019962
 87836/100000: episode: 8789, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000060, mae: 0.002931, mean_q: 0.019947
 87846/100000: episode: 8790, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000038, mae: 0.002951, mean_q: 0.019355
 87856/100000: episode: 8791, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000012, mae: 0.002286, mean_q: 0.018955
 87866/100000: episode: 8792, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000055, mae: 0.002386, mean_q: 0.019977
 87876/100000: episode: 8793, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000030, mae: 0.002011, mean_q: 0.019415
 87886/100000: episode: 8794, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000060, mae: 0.003190, mean_q: 0.019139
 87896/100000: episode: 8795, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000052, mae: 0.002368, mean_q: 0.020526
 87906/100000: episode: 8796, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000053, mae: 0.002377, mean_q: 0.019646
 87916/100000: episode: 8797, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000056, mae: 0.002650, mean_q: 0.019248
 87926/100000: episode: 8798, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000055, mae: 0.002579, mean_q: 0.019945
 87936/100000: episode: 8799, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000052, mae: 0.002168, mean_q: 0.019555
 87946/100000: episode: 8800, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000061, mae: 0.003274, mean_q: 0.018858
 87956/100000: episode: 8801, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000071, mae: 0.003222, mean_q: 0.020993
 87966/100000: episode: 8802, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000037, mae: 0.002714, mean_q: 0.020480
 87976/100000: episode: 8803, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002837, mean_q: 0.018495
 87986/100000: episode: 8804, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000243, mae: 0.004091, mean_q: 0.020637
 87996/100000: episode: 8805, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000018, mae: 0.002478, mean_q: 0.020209
Step 88000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.88000.hdf5
 88006/100000: episode: 8806, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000035, mae: 0.003472, mean_q: 0.017814
 88016/100000: episode: 8807, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000017, mae: 0.002584, mean_q: 0.020533
 88026/100000: episode: 8808, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000014, mae: 0.002249, mean_q: 0.018862
 88036/100000: episode: 8809, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000221, mae: 0.003160, mean_q: 0.019688
 88046/100000: episode: 8810, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000079, mae: 0.003669, mean_q: 0.020855
 88056/100000: episode: 8811, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000040, mae: 0.002569, mean_q: 0.019464
 88066/100000: episode: 8812, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000049, mae: 0.001961, mean_q: 0.019283
 88076/100000: episode: 8813, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000247, mae: 0.004251, mean_q: 0.020734
 88086/100000: episode: 8814, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000057, mae: 0.003368, mean_q: 0.021093
 88096/100000: episode: 8815, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000054, mae: 0.002966, mean_q: 0.018832
 88106/100000: episode: 8816, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000055, mae: 0.002546, mean_q: 0.019752
 88116/100000: episode: 8817, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000030, mae: 0.001776, mean_q: 0.019520
 88126/100000: episode: 8818, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000034, mae: 0.002560, mean_q: 0.018825
 88136/100000: episode: 8819, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000091, mae: 0.002739, mean_q: 0.019199
 88146/100000: episode: 8820, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000034, mae: 0.002762, mean_q: 0.020766
 88156/100000: episode: 8821, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000052, mae: 0.002892, mean_q: 0.018425
 88166/100000: episode: 8822, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000010, mae: 0.001607, mean_q: 0.019186
 88176/100000: episode: 8823, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000053, mae: 0.002089, mean_q: 0.019336
 88186/100000: episode: 8824, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000008, mae: 0.001192, mean_q: 0.019170
 88196/100000: episode: 8825, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000202, mae: 0.002926, mean_q: 0.018921
 88206/100000: episode: 8826, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000013, mae: 0.001714, mean_q: 0.020022
 88216/100000: episode: 8827, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000034, mae: 0.002212, mean_q: 0.018871
 88226/100000: episode: 8828, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000268, mae: 0.004780, mean_q: 0.020550
 88236/100000: episode: 8829, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000208, mae: 0.004412, mean_q: 0.021312
 88246/100000: episode: 8830, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000015, mae: 0.003484, mean_q: 0.017555
 88256/100000: episode: 8831, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000010, mae: 0.002040, mean_q: 0.018617
 88266/100000: episode: 8832, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000009, mae: 0.001567, mean_q: 0.019197
 88276/100000: episode: 8833, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000054, mae: 0.003099, mean_q: 0.017933
 88286/100000: episode: 8834, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000033, mae: 0.002353, mean_q: 0.019916
 88296/100000: episode: 8835, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000223, mae: 0.003572, mean_q: 0.018153
 88306/100000: episode: 8836, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000012, mae: 0.001750, mean_q: 0.019862
 88316/100000: episode: 8837, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000011, mae: 0.001580, mean_q: 0.018634
 88326/100000: episode: 8838, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000056, mae: 0.002593, mean_q: 0.019352
 88336/100000: episode: 8839, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.002007, mean_q: 0.019679
 88346/100000: episode: 8840, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000032, mae: 0.002034, mean_q: 0.018485
 88356/100000: episode: 8841, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000056, mae: 0.002638, mean_q: 0.019720
 88366/100000: episode: 8842, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000221, mae: 0.003313, mean_q: 0.020123
 88376/100000: episode: 8843, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000035, mae: 0.002214, mean_q: 0.019189
 88386/100000: episode: 8844, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000030, mae: 0.001625, mean_q: 0.018936
 88396/100000: episode: 8845, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000055, mae: 0.002150, mean_q: 0.019247
 88406/100000: episode: 8846, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000116, mae: 0.003382, mean_q: 0.019763
 88416/100000: episode: 8847, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000057, mae: 0.003075, mean_q: 0.020186
 88426/100000: episode: 8848, duration: 0.090s, episode steps: 10, steps per second: 111, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000032, mae: 0.002507, mean_q: 0.018240
 88436/100000: episode: 8849, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000074, mae: 0.002706, mean_q: 0.019169
 88446/100000: episode: 8850, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000056, mae: 0.004022, mean_q: 0.021491
 88456/100000: episode: 8851, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000011, mae: 0.001809, mean_q: 0.019057
 88466/100000: episode: 8852, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000077, mae: 0.003328, mean_q: 0.018391
 88476/100000: episode: 8853, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000013, mae: 0.001782, mean_q: 0.020024
 88486/100000: episode: 8854, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000034, mae: 0.002301, mean_q: 0.018821
 88496/100000: episode: 8855, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000419, mae: 0.004923, mean_q: 0.018445
 88506/100000: episode: 8856, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000059, mae: 0.004720, mean_q: 0.022258
 88516/100000: episode: 8857, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000079, mae: 0.003315, mean_q: 0.019100
 88526/100000: episode: 8858, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000012, mae: 0.001979, mean_q: 0.018846
 88536/100000: episode: 8859, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000073, mae: 0.003076, mean_q: 0.018381
 88546/100000: episode: 8860, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000054, mae: 0.002728, mean_q: 0.020150
 88556/100000: episode: 8861, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000038, mae: 0.002603, mean_q: 0.019492
 88566/100000: episode: 8862, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000048, mae: 0.002087, mean_q: 0.018458
 88576/100000: episode: 8863, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000034, mae: 0.002063, mean_q: 0.019645
 88586/100000: episode: 8864, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.002245, mean_q: 0.018286
 88596/100000: episode: 8865, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000057, mae: 0.002619, mean_q: 0.018845
 88606/100000: episode: 8866, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000204, mae: 0.002930, mean_q: 0.019653
 88616/100000: episode: 8867, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000016, mae: 0.002144, mean_q: 0.019518
 88626/100000: episode: 8868, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000100, mae: 0.003995, mean_q: 0.018100
 88636/100000: episode: 8869, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000054, mae: 0.002930, mean_q: 0.020208
 88646/100000: episode: 8870, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000097, mae: 0.003218, mean_q: 0.019209
 88656/100000: episode: 8871, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000029, mae: 0.001907, mean_q: 0.018669
 88666/100000: episode: 8872, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000013, mae: 0.001822, mean_q: 0.018539
 88676/100000: episode: 8873, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000100, mae: 0.003605, mean_q: 0.019504
 88686/100000: episode: 8874, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.400 [1.000, 10.000], loss: 0.000058, mae: 0.002713, mean_q: 0.019003
 88696/100000: episode: 8875, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000271, mae: 0.004625, mean_q: 0.019412
 88706/100000: episode: 8876, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000098, mae: 0.004482, mean_q: 0.021117
[Info] 1-TH LEVEL FOUND: 0.021424053236842155, Considering 100/100 traces
 88716/100000: episode: 8877, duration: 0.721s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000289, mae: 0.005365, mean_q: 0.020777
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021424053236842155
1
 88726/100000: episode: 8878, duration: 0.507s, episode steps: 10, steps per second: 20, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000097, mae: 0.003441, mean_q: 0.020214
 88736/100000: episode: 8879, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000030, mae: 0.001714, mean_q: 0.019297
 88746/100000: episode: 8880, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000057, mae: 0.002960, mean_q: 0.018780
 88756/100000: episode: 8881, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000033, mae: 0.002035, mean_q: 0.019058
 88766/100000: episode: 8882, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000016, mae: 0.002074, mean_q: 0.018875
 88776/100000: episode: 8883, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000075, mae: 0.002794, mean_q: 0.019665
 88786/100000: episode: 8884, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002055, mean_q: 0.019290
 88796/100000: episode: 8885, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000062, mae: 0.003165, mean_q: 0.018707
 88806/100000: episode: 8886, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000436, mae: 0.004781, mean_q: 0.019356
 88816/100000: episode: 8887, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000060, mae: 0.004510, mean_q: 0.021733
 88826/100000: episode: 8888, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000079, mae: 0.003679, mean_q: 0.018039
 88836/100000: episode: 8889, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000014, mae: 0.001733, mean_q: 0.019650
 88846/100000: episode: 8890, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000031, mae: 0.002318, mean_q: 0.018444
 88856/100000: episode: 8891, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000031, mae: 0.002115, mean_q: 0.018567
 88866/100000: episode: 8892, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000071, mae: 0.002722, mean_q: 0.019954
 88876/100000: episode: 8893, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000075, mae: 0.002900, mean_q: 0.019981
 88886/100000: episode: 8894, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000057, mae: 0.002383, mean_q: 0.019288
 88896/100000: episode: 8895, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000034, mae: 0.002055, mean_q: 0.019199
 88906/100000: episode: 8896, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000266, mae: 0.003712, mean_q: 0.019091
 88916/100000: episode: 8897, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000035, mae: 0.003209, mean_q: 0.021026
 88926/100000: episode: 8898, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000412, mae: 0.004662, mean_q: 0.020533
 88936/100000: episode: 8899, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000031, mae: 0.001891, mean_q: 0.020173
 88946/100000: episode: 8900, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000059, mae: 0.002909, mean_q: 0.019128
 88956/100000: episode: 8901, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000125, mae: 0.004480, mean_q: 0.020455
 88966/100000: episode: 8902, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000226, mae: 0.004148, mean_q: 0.020998
 88976/100000: episode: 8903, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000032, mae: 0.002139, mean_q: 0.020152
 88986/100000: episode: 8904, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000267, mae: 0.004332, mean_q: 0.019193
 88996/100000: episode: 8905, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000094, mae: 0.004449, mean_q: 0.021859
Step 89000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.89000.hdf5
 89006/100000: episode: 8906, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000096, mae: 0.003382, mean_q: 0.019359
 89016/100000: episode: 8907, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000097, mae: 0.003270, mean_q: 0.019974
 89026/100000: episode: 8908, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000014, mae: 0.002136, mean_q: 0.020643
 89036/100000: episode: 8909, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000010, mae: 0.002528, mean_q: 0.018389
 89046/100000: episode: 8910, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000202, mae: 0.002979, mean_q: 0.019764
 89056/100000: episode: 8911, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000039, mae: 0.002381, mean_q: 0.019934
 89066/100000: episode: 8912, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000032, mae: 0.001657, mean_q: 0.019650
 89076/100000: episode: 8913, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000220, mae: 0.003709, mean_q: 0.021270
 89086/100000: episode: 8914, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000224, mae: 0.003431, mean_q: 0.019804
 89096/100000: episode: 8915, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000221, mae: 0.002968, mean_q: 0.020090
 89106/100000: episode: 8916, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002171, mean_q: 0.020412
 89116/100000: episode: 8917, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000032, mae: 0.001833, mean_q: 0.019642
 89126/100000: episode: 8918, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000054, mae: 0.002078, mean_q: 0.019989
 89136/100000: episode: 8919, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000098, mae: 0.003299, mean_q: 0.020149
 89146/100000: episode: 8920, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000033, mae: 0.001929, mean_q: 0.019626
 89156/100000: episode: 8921, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000286, mae: 0.004454, mean_q: 0.019914
 89166/100000: episode: 8922, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000013, mae: 0.002523, mean_q: 0.021493
 89176/100000: episode: 8923, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000013, mae: 0.002814, mean_q: 0.018352
 89186/100000: episode: 8924, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000034, mae: 0.002234, mean_q: 0.019950
 89196/100000: episode: 8925, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000076, mae: 0.003081, mean_q: 0.020648
 89206/100000: episode: 8926, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000073, mae: 0.002731, mean_q: 0.020557
 89216/100000: episode: 8927, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000033, mae: 0.002106, mean_q: 0.019489
 89226/100000: episode: 8928, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000035, mae: 0.001904, mean_q: 0.019738
 89236/100000: episode: 8929, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000412, mae: 0.005073, mean_q: 0.021278
 89246/100000: episode: 8930, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000034, mae: 0.003704, mean_q: 0.022571
 89256/100000: episode: 8931, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000241, mae: 0.003199, mean_q: 0.020218
 89266/100000: episode: 8932, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002306, mean_q: 0.019884
 89276/100000: episode: 8933, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000077, mae: 0.002785, mean_q: 0.020160
 89286/100000: episode: 8934, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000093, mae: 0.003710, mean_q: 0.021534
 89296/100000: episode: 8935, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002493, mean_q: 0.019713
 89306/100000: episode: 8936, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000009, mae: 0.002372, mean_q: 0.018866
 89316/100000: episode: 8937, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000281, mae: 0.004713, mean_q: 0.021163
 89326/100000: episode: 8938, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000079, mae: 0.004765, mean_q: 0.022626
 89336/100000: episode: 8939, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000057, mae: 0.003563, mean_q: 0.018914
 89346/100000: episode: 8940, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000036, mae: 0.002327, mean_q: 0.019957
 89356/100000: episode: 8941, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000035, mae: 0.002540, mean_q: 0.019537
 89366/100000: episode: 8942, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000054, mae: 0.002872, mean_q: 0.019318
 89376/100000: episode: 8943, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000011, mae: 0.001784, mean_q: 0.019430
 89386/100000: episode: 8944, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [0.000, 10.000], loss: 0.000057, mae: 0.002899, mean_q: 0.019230
 89396/100000: episode: 8945, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000246, mae: 0.003672, mean_q: 0.020360
 89406/100000: episode: 8946, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000051, mae: 0.002798, mean_q: 0.021117
 89416/100000: episode: 8947, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000017, mae: 0.001980, mean_q: 0.020126
 89426/100000: episode: 8948, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000030, mae: 0.002056, mean_q: 0.019174
 89436/100000: episode: 8949, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000077, mae: 0.003186, mean_q: 0.020657
 89446/100000: episode: 8950, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000057, mae: 0.002486, mean_q: 0.019951
 89456/100000: episode: 8951, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000029, mae: 0.001883, mean_q: 0.020633
 89466/100000: episode: 8952, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000055, mae: 0.002354, mean_q: 0.019834
 89476/100000: episode: 8953, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000205, mae: 0.003237, mean_q: 0.019383
 89486/100000: episode: 8954, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000055, mae: 0.003010, mean_q: 0.020859
 89496/100000: episode: 8955, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000247, mae: 0.004144, mean_q: 0.020851
 89506/100000: episode: 8956, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000060, mae: 0.003488, mean_q: 0.018984
 89516/100000: episode: 8957, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000038, mae: 0.002528, mean_q: 0.019584
 89526/100000: episode: 8958, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000032, mae: 0.001938, mean_q: 0.020387
 89536/100000: episode: 8959, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.002072, mean_q: 0.019320
 89546/100000: episode: 8960, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000058, mae: 0.003027, mean_q: 0.018962
 89556/100000: episode: 8961, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000050, mae: 0.002107, mean_q: 0.020285
 89566/100000: episode: 8962, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000038, mae: 0.002148, mean_q: 0.019769
 89576/100000: episode: 8963, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000208, mae: 0.003189, mean_q: 0.020217
 89586/100000: episode: 8964, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000012, mae: 0.002051, mean_q: 0.019491
 89596/100000: episode: 8965, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000037, mae: 0.003441, mean_q: 0.017835
 89606/100000: episode: 8966, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000014, mae: 0.001774, mean_q: 0.020039
 89616/100000: episode: 8967, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000049, mae: 0.002272, mean_q: 0.018910
 89626/100000: episode: 8968, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.001675, mean_q: 0.019267
 89636/100000: episode: 8969, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000055, mae: 0.002456, mean_q: 0.019175
 89646/100000: episode: 8970, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000020, mae: 0.001974, mean_q: 0.019497
 89656/100000: episode: 8971, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000034, mae: 0.002271, mean_q: 0.018998
 89666/100000: episode: 8972, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000416, mae: 0.004688, mean_q: 0.018591
 89676/100000: episode: 8973, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000014, mae: 0.003346, mean_q: 0.021823
 89686/100000: episode: 8974, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000202, mae: 0.002879, mean_q: 0.018962
 89696/100000: episode: 8975, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000030, mae: 0.001870, mean_q: 0.019940
 89706/100000: episode: 8976, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000440, mae: 0.004974, mean_q: 0.019926
 89716/100000: episode: 8977, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000035, mae: 0.003661, mean_q: 0.021923
[Info] 1-TH LEVEL FOUND: 0.018522460013628006, Considering 100/100 traces
 89726/100000: episode: 8978, duration: 0.806s, episode steps: 10, steps per second: 12, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000013, mae: 0.001929, mean_q: 0.019092
[Info] 2-TH LEVEL FOUND: 0.020704763010144234, Considering 100/100 traces
 89736/100000: episode: 8979, duration: 0.696s, episode steps: 10, steps per second: 14, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000092, mae: 0.003064, mean_q: 0.019237
[Info] 3-TH LEVEL FOUND: 0.020794618874788284, Considering 100/100 traces
 89746/100000: episode: 8980, duration: 0.697s, episode steps: 10, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000071, mae: 0.003127, mean_q: 0.021040
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.020794618874788284
3
 89756/100000: episode: 8981, duration: 0.583s, episode steps: 10, steps per second: 17, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000013, mae: 0.002263, mean_q: 0.019277
 89766/100000: episode: 8982, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000052, mae: 0.002559, mean_q: 0.018663
 89776/100000: episode: 8983, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000055, mae: 0.002860, mean_q: 0.020433
 89786/100000: episode: 8984, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.002481, mean_q: 0.019630
 89796/100000: episode: 8985, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000095, mae: 0.003186, mean_q: 0.020276
 89806/100000: episode: 8986, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002212, mean_q: 0.019964
 89816/100000: episode: 8987, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000230, mae: 0.003752, mean_q: 0.019129
 89826/100000: episode: 8988, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002736, mean_q: 0.020898
 89836/100000: episode: 8989, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002350, mean_q: 0.019339
 89846/100000: episode: 8990, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000266, mae: 0.004546, mean_q: 0.020387
 89856/100000: episode: 8991, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000261, mae: 0.004658, mean_q: 0.021467
 89866/100000: episode: 8992, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000034, mae: 0.002384, mean_q: 0.020447
 89876/100000: episode: 8993, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000058, mae: 0.002946, mean_q: 0.019430
 89886/100000: episode: 8994, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000012, mae: 0.001462, mean_q: 0.019817
 89896/100000: episode: 8995, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000079, mae: 0.003248, mean_q: 0.019211
 89906/100000: episode: 8996, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002334, mean_q: 0.020325
 89916/100000: episode: 8997, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000034, mae: 0.002450, mean_q: 0.019198
 89926/100000: episode: 8998, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000017, mae: 0.002349, mean_q: 0.018939
 89936/100000: episode: 8999, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000081, mae: 0.003094, mean_q: 0.019878
 89946/100000: episode: 9000, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000433, mae: 0.005369, mean_q: 0.021118
 89956/100000: episode: 9001, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000022, mae: 0.003630, mean_q: 0.021664
 89966/100000: episode: 9002, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000013, mae: 0.003263, mean_q: 0.017676
 89976/100000: episode: 9003, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.002309, mean_q: 0.019560
 89986/100000: episode: 9004, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000201, mae: 0.002562, mean_q: 0.020189
 89996/100000: episode: 9005, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000013, mae: 0.001596, mean_q: 0.020203
Step 90000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.90000.hdf5
 90006/100000: episode: 9006, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000272, mae: 0.005188, mean_q: 0.021102
 90016/100000: episode: 9007, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000203, mae: 0.002936, mean_q: 0.020301
 90026/100000: episode: 9008, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000019, mae: 0.002050, mean_q: 0.019956
[Info] FALSIFICATION!
 90036/100000: episode: 9009, duration: 0.315s, episode steps: 10, steps per second: 32, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000034, mae: 0.002984, mean_q: 0.018474
 90046/100000: episode: 9010, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000010, mae: 0.001353, mean_q: 0.019895
 90056/100000: episode: 9011, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000204, mae: 0.002754, mean_q: 0.019419
 90066/100000: episode: 9012, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000014, mae: 0.001816, mean_q: 0.019964
 90076/100000: episode: 9013, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.003054, mean_q: 0.018408
 90086/100000: episode: 9014, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.003061, mae: 0.010958, mean_q: 0.021720
 90096/100000: episode: 9015, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000069, mae: 0.006442, mean_q: 0.024603
 90106/100000: episode: 9016, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000038, mae: 0.003553, mean_q: 0.018401
 90116/100000: episode: 9017, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000036, mae: 0.002399, mean_q: 0.020173
 90126/100000: episode: 9018, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001600, mae: 0.008454, mean_q: 0.022419
 90136/100000: episode: 9019, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000061, mae: 0.004840, mean_q: 0.023224
 90146/100000: episode: 9020, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000013, mae: 0.003145, mean_q: 0.018569
 90156/100000: episode: 9021, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000229, mae: 0.004208, mean_q: 0.019504
 90166/100000: episode: 9022, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000056, mae: 0.003075, mean_q: 0.021391
 90176/100000: episode: 9023, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001582, mae: 0.006235, mean_q: 0.020080
 90186/100000: episode: 9024, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000087, mae: 0.006159, mean_q: 0.023970
 90196/100000: episode: 9025, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.002889, mean_q: 0.022104
 90206/100000: episode: 9026, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002933, mean_q: 0.019525
 90216/100000: episode: 9027, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000013, mae: 0.002066, mean_q: 0.021486
 90226/100000: episode: 9028, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000032, mae: 0.002274, mean_q: 0.020163
 90236/100000: episode: 9029, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000016, mae: 0.002668, mean_q: 0.019703
 90246/100000: episode: 9030, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000016, mae: 0.002378, mean_q: 0.019622
 90256/100000: episode: 9031, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.001922, mean_q: 0.020361
 90266/100000: episode: 9032, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000009, mae: 0.001260, mean_q: 0.020206
 90276/100000: episode: 9033, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000228, mae: 0.003537, mean_q: 0.020501
 90286/100000: episode: 9034, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000246, mae: 0.004851, mean_q: 0.022229
 90296/100000: episode: 9035, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001515, mae: 0.007118, mean_q: 0.023399
 90306/100000: episode: 9036, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002702, mean_q: 0.022050
 90316/100000: episode: 9037, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000033, mae: 0.002952, mean_q: 0.019594
 90326/100000: episode: 9038, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000016, mae: 0.002488, mean_q: 0.019682
 90336/100000: episode: 9039, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000037, mae: 0.002137, mean_q: 0.020578
[Info] FALSIFICATION!
 90346/100000: episode: 9040, duration: 0.240s, episode steps: 10, steps per second: 42, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.001516, mae: 0.005226, mean_q: 0.021136
 90356/100000: episode: 9041, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000098, mae: 0.005383, mean_q: 0.023619
 90366/100000: episode: 9042, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000021, mae: 0.002864, mean_q: 0.020468
 90376/100000: episode: 9043, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000012, mae: 0.002499, mean_q: 0.019509
 90386/100000: episode: 9044, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000015, mae: 0.001945, mean_q: 0.020417
 90396/100000: episode: 9045, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000010, mae: 0.001825, mean_q: 0.019984
 90406/100000: episode: 9046, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002812, mean_q: 0.019830
 90416/100000: episode: 9047, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.001534, mae: 0.006142, mean_q: 0.022087
 90426/100000: episode: 9048, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000016, mae: 0.003011, mean_q: 0.022594
 90436/100000: episode: 9049, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000200, mae: 0.003628, mean_q: 0.018972
 90446/100000: episode: 9050, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000308, mae: 0.006020, mean_q: 0.022129
 90456/100000: episode: 9051, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000098, mae: 0.004597, mean_q: 0.022795
 90466/100000: episode: 9052, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000094, mae: 0.003001, mean_q: 0.020872
 90476/100000: episode: 9053, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.001947, mean_q: 0.021225
 90486/100000: episode: 9054, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000033, mae: 0.001978, mean_q: 0.020691
 90496/100000: episode: 9055, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000036, mae: 0.002370, mean_q: 0.020562
 90506/100000: episode: 9056, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000053, mae: 0.002236, mean_q: 0.021053
 90516/100000: episode: 9057, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.002397, mean_q: 0.020836
 90526/100000: episode: 9058, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000032, mae: 0.002292, mean_q: 0.019945
 90536/100000: episode: 9059, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000206, mae: 0.003234, mean_q: 0.021229
 90546/100000: episode: 9060, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000082, mae: 0.003765, mean_q: 0.021690
 90556/100000: episode: 9061, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000053, mae: 0.002077, mean_q: 0.021162
 90566/100000: episode: 9062, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000055, mae: 0.002546, mean_q: 0.020594
 90576/100000: episode: 9063, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000040, mae: 0.002526, mean_q: 0.020809
 90586/100000: episode: 9064, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001577, mae: 0.006861, mean_q: 0.021748
 90596/100000: episode: 9065, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000060, mae: 0.004303, mean_q: 0.023228
 90606/100000: episode: 9066, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000135, mae: 0.003959, mean_q: 0.021782
 90616/100000: episode: 9067, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000011, mae: 0.001717, mean_q: 0.021947
 90626/100000: episode: 9068, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000036, mae: 0.003023, mean_q: 0.020204
 90636/100000: episode: 9069, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000038, mae: 0.003190, mean_q: 0.019835
 90646/100000: episode: 9070, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000045, mae: 0.003055, mean_q: 0.021527
 90656/100000: episode: 9071, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000076, mae: 0.003286, mean_q: 0.021915
 90666/100000: episode: 9072, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000014, mae: 0.001903, mean_q: 0.020745
 90676/100000: episode: 9073, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000037, mae: 0.002901, mean_q: 0.019984
 90686/100000: episode: 9074, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000056, mae: 0.002571, mean_q: 0.020600
 90696/100000: episode: 9075, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000075, mae: 0.002887, mean_q: 0.021417
 90706/100000: episode: 9076, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000011, mae: 0.001487, mean_q: 0.021040
 90716/100000: episode: 9077, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000096, mae: 0.003397, mean_q: 0.020273
 90726/100000: episode: 9078, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000031, mae: 0.001963, mean_q: 0.021502
 90736/100000: episode: 9079, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000039, mae: 0.002115, mean_q: 0.020891
 90746/100000: episode: 9080, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.001577, mae: 0.006472, mean_q: 0.021599
[Info] Complete ISplit Iteration
[Info] Levels: [0.020923259]
[Info] Cond. Prob: [0.02]
[Info] Error Prob: 0.02

 90756/100000: episode: 9081, duration: 0.843s, episode steps: 10, steps per second: 12, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000020, mae: 0.003683, mean_q: 0.023448
 90766/100000: episode: 9082, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000062, mae: 0.003801, mean_q: 0.019824
 90776/100000: episode: 9083, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000076, mae: 0.003060, mean_q: 0.020754
 90786/100000: episode: 9084, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001573, mae: 0.007222, mean_q: 0.022911
 90796/100000: episode: 9085, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000099, mae: 0.005167, mean_q: 0.023789
 90806/100000: episode: 9086, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000077, mae: 0.002923, mean_q: 0.021451
 90816/100000: episode: 9087, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001537, mae: 0.005852, mean_q: 0.020969
 90826/100000: episode: 9088, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000064, mae: 0.004294, mean_q: 0.023146
 90836/100000: episode: 9089, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000033, mae: 0.002217, mean_q: 0.021540
 90846/100000: episode: 9090, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000075, mae: 0.003549, mean_q: 0.020292
 90856/100000: episode: 9091, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000096, mae: 0.003352, mean_q: 0.021566
 90866/100000: episode: 9092, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000096, mae: 0.003472, mean_q: 0.022175
 90876/100000: episode: 9093, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000203, mae: 0.003197, mean_q: 0.022333
 90886/100000: episode: 9094, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000053, mae: 0.002358, mean_q: 0.022073
 90896/100000: episode: 9095, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000059, mae: 0.002473, mean_q: 0.021803
 90906/100000: episode: 9096, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001533, mae: 0.006249, mean_q: 0.023159
 90916/100000: episode: 9097, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000021, mae: 0.003356, mean_q: 0.023583
 90926/100000: episode: 9098, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000059, mae: 0.003814, mean_q: 0.020435
 90936/100000: episode: 9099, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000099, mae: 0.003691, mean_q: 0.021365
 90946/100000: episode: 9100, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000042, mae: 0.002853, mean_q: 0.022333
 90956/100000: episode: 9101, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000036, mae: 0.002201, mean_q: 0.021487
 90966/100000: episode: 9102, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000035, mae: 0.001989, mean_q: 0.021693
 90976/100000: episode: 9103, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000072, mae: 0.002648, mean_q: 0.021458
 90986/100000: episode: 9104, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000062, mae: 0.003125, mean_q: 0.021259
 90996/100000: episode: 9105, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000050, mae: 0.001890, mean_q: 0.021738
Step 91000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.91000.hdf5
 91006/100000: episode: 9106, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000096, mae: 0.003359, mean_q: 0.022083
 91016/100000: episode: 9107, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000072, mae: 0.002486, mean_q: 0.021720
 91026/100000: episode: 9108, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000078, mae: 0.002984, mean_q: 0.021784
 91036/100000: episode: 9109, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000054, mae: 0.002406, mean_q: 0.021463
 91046/100000: episode: 9110, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000058, mae: 0.002590, mean_q: 0.021474
 91056/100000: episode: 9111, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000054, mae: 0.002254, mean_q: 0.021657
 91066/100000: episode: 9112, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000061, mae: 0.002640, mean_q: 0.021745
 91076/100000: episode: 9113, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000034, mae: 0.002038, mean_q: 0.021523
 91086/100000: episode: 9114, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000243, mae: 0.003585, mean_q: 0.021666
 91096/100000: episode: 9115, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.001527, mae: 0.005972, mean_q: 0.023053
 91106/100000: episode: 9116, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000082, mae: 0.005233, mean_q: 0.024334
 91116/100000: episode: 9117, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001519, mae: 0.005824, mean_q: 0.021374
 91126/100000: episode: 9118, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000113, mae: 0.004412, mean_q: 0.023345
 91136/100000: episode: 9119, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000219, mae: 0.004051, mean_q: 0.023717
 91146/100000: episode: 9120, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001553, mae: 0.009462, mean_q: 0.026948
 91156/100000: episode: 9121, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000096, mae: 0.004532, mean_q: 0.024618
 91166/100000: episode: 9122, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000017, mae: 0.003014, mean_q: 0.021367
 91176/100000: episode: 9123, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000226, mae: 0.004047, mean_q: 0.021756
 91186/100000: episode: 9124, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000020, mae: 0.002990, mean_q: 0.024114
 91196/100000: episode: 9125, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000094, mae: 0.003281, mean_q: 0.022552
 91206/100000: episode: 9126, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000056, mae: 0.002518, mean_q: 0.022634
 91216/100000: episode: 9127, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.001555, mae: 0.007133, mean_q: 0.024013
 91226/100000: episode: 9128, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000018, mae: 0.003042, mean_q: 0.024657
 91236/100000: episode: 9129, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000216, mae: 0.002854, mean_q: 0.023481
 91246/100000: episode: 9130, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000042, mae: 0.002731, mean_q: 0.022991
 91256/100000: episode: 9131, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000080, mae: 0.003450, mean_q: 0.022532
 91266/100000: episode: 9132, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000053, mae: 0.002364, mean_q: 0.022796
 91276/100000: episode: 9133, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000038, mae: 0.002831, mean_q: 0.022267
 91286/100000: episode: 9134, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000240, mae: 0.003755, mean_q: 0.022211
 91296/100000: episode: 9135, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000057, mae: 0.003337, mean_q: 0.023851
 91306/100000: episode: 9136, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000036, mae: 0.002698, mean_q: 0.022194
 91316/100000: episode: 9137, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000035, mae: 0.002947, mean_q: 0.021531
 91326/100000: episode: 9138, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002631, mean_q: 0.022216
 91336/100000: episode: 9139, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001536, mae: 0.006196, mean_q: 0.022802
 91346/100000: episode: 9140, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000260, mae: 0.005325, mean_q: 0.024694
 91356/100000: episode: 9141, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000039, mae: 0.003191, mean_q: 0.024217
 91366/100000: episode: 9142, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000043, mae: 0.003101, mean_q: 0.022392
 91376/100000: episode: 9143, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000055, mae: 0.003197, mean_q: 0.021751
 91386/100000: episode: 9144, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000057, mae: 0.002729, mean_q: 0.022837
 91396/100000: episode: 9145, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000024, mae: 0.002763, mean_q: 0.022819
 91406/100000: episode: 9146, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000031, mae: 0.002400, mean_q: 0.021737
 91416/100000: episode: 9147, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000015, mae: 0.001877, mean_q: 0.022168
 91426/100000: episode: 9148, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000412, mae: 0.004754, mean_q: 0.022043
 91436/100000: episode: 9149, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000073, mae: 0.004220, mean_q: 0.024715
 91446/100000: episode: 9150, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000058, mae: 0.003427, mean_q: 0.023647
 91456/100000: episode: 9151, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000016, mae: 0.003115, mean_q: 0.021013
 91466/100000: episode: 9152, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000015, mae: 0.003065, mean_q: 0.021082
 91476/100000: episode: 9153, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000015, mae: 0.002589, mean_q: 0.021196
 91486/100000: episode: 9154, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.003035, mae: 0.011553, mean_q: 0.025846
 91496/100000: episode: 9155, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000062, mae: 0.004880, mean_q: 0.025547
 91506/100000: episode: 9156, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000040, mae: 0.003685, mean_q: 0.021325
 91516/100000: episode: 9157, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.001536, mae: 0.006375, mean_q: 0.023599
 91526/100000: episode: 9158, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000041, mae: 0.003469, mean_q: 0.024271
 91536/100000: episode: 9159, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000076, mae: 0.003803, mean_q: 0.021554
 91546/100000: episode: 9160, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000016, mae: 0.001911, mean_q: 0.022357
 91556/100000: episode: 9161, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000205, mae: 0.003012, mean_q: 0.022705
 91566/100000: episode: 9162, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000025, mae: 0.002613, mean_q: 0.022573
 91576/100000: episode: 9163, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000057, mae: 0.002936, mean_q: 0.022262
 91586/100000: episode: 9164, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000222, mae: 0.003131, mean_q: 0.022572
 91596/100000: episode: 9165, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000015, mae: 0.002047, mean_q: 0.022151
 91606/100000: episode: 9166, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.001772, mae: 0.008092, mean_q: 0.023320
 91616/100000: episode: 9167, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000213, mae: 0.006408, mean_q: 0.026385
 91626/100000: episode: 9168, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000222, mae: 0.003885, mean_q: 0.022062
 91636/100000: episode: 9169, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000118, mae: 0.004281, mean_q: 0.022225
 91646/100000: episode: 9170, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000290, mae: 0.006658, mean_q: 0.025057
 91656/100000: episode: 9171, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000205, mae: 0.003733, mean_q: 0.023827
 91666/100000: episode: 9172, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.001531, mae: 0.005647, mean_q: 0.022820
 91676/100000: episode: 9173, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000019, mae: 0.002767, mean_q: 0.024412
 91686/100000: episode: 9174, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000056, mae: 0.002681, mean_q: 0.022942
 91696/100000: episode: 9175, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.001570, mae: 0.006969, mean_q: 0.024236
 91706/100000: episode: 9176, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000039, mae: 0.004232, mean_q: 0.025952
 91716/100000: episode: 9177, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000019, mae: 0.003058, mean_q: 0.022300
 91726/100000: episode: 9178, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000203, mae: 0.004182, mean_q: 0.021432
 91736/100000: episode: 9179, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000060, mae: 0.003066, mean_q: 0.023548
 91746/100000: episode: 9180, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000202, mae: 0.003274, mean_q: 0.023996
[Info] 1-TH LEVEL FOUND: 0.025775577872991562, Considering 100/100 traces
 91756/100000: episode: 9181, duration: 0.688s, episode steps: 10, steps per second: 15, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.001540, mae: 0.005398, mean_q: 0.024001
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025775577872991562
1
 91766/100000: episode: 9182, duration: 0.518s, episode steps: 10, steps per second: 19, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000057, mae: 0.004169, mean_q: 0.025519
 91776/100000: episode: 9183, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.003042, mean_q: 0.022772
 91786/100000: episode: 9184, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000055, mae: 0.002819, mean_q: 0.023328
 91796/100000: episode: 9185, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000019, mae: 0.002141, mean_q: 0.023763
 91806/100000: episode: 9186, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000243, mae: 0.003928, mean_q: 0.022955
 91816/100000: episode: 9187, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000058, mae: 0.003231, mean_q: 0.024358
 91826/100000: episode: 9188, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000039, mae: 0.002420, mean_q: 0.023586
 91836/100000: episode: 9189, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000203, mae: 0.003290, mean_q: 0.022963
 91846/100000: episode: 9190, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000018, mae: 0.002339, mean_q: 0.022801
 91856/100000: episode: 9191, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000053, mae: 0.002876, mean_q: 0.022436
 91866/100000: episode: 9192, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000044, mae: 0.004210, mean_q: 0.025467
 91876/100000: episode: 9193, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000096, mae: 0.004612, mean_q: 0.025255
 91886/100000: episode: 9194, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000038, mae: 0.002534, mean_q: 0.023176
 91896/100000: episode: 9195, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000091, mae: 0.002941, mean_q: 0.023358
 91906/100000: episode: 9196, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000260, mae: 0.004662, mean_q: 0.024685
 91916/100000: episode: 9197, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001549, mae: 0.007515, mean_q: 0.025763
 91926/100000: episode: 9198, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000061, mae: 0.003668, mean_q: 0.025001
 91936/100000: episode: 9199, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002989, mean_q: 0.022688
 91946/100000: episode: 9200, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000052, mae: 0.002567, mean_q: 0.023193
 91956/100000: episode: 9201, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000058, mae: 0.002837, mean_q: 0.024126
 91966/100000: episode: 9202, duration: 0.085s, episode steps: 10, steps per second: 118, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000023, mae: 0.002429, mean_q: 0.023559
 91976/100000: episode: 9203, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000221, mae: 0.003685, mean_q: 0.022883
 91986/100000: episode: 9204, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000038, mae: 0.002542, mean_q: 0.024319
 91996/100000: episode: 9205, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000205, mae: 0.003594, mean_q: 0.024476
Step 92000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.92000.hdf5
 92006/100000: episode: 9206, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000296, mae: 0.004448, mean_q: 0.023651
 92016/100000: episode: 9207, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.001523, mae: 0.005736, mean_q: 0.024671
 92026/100000: episode: 9208, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [1.000, 10.000], loss: 0.000060, mae: 0.004491, mean_q: 0.026158
 92036/100000: episode: 9209, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000239, mae: 0.004491, mean_q: 0.025498
 92046/100000: episode: 9210, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.001504, mae: 0.004774, mean_q: 0.024198
 92056/100000: episode: 9211, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000066, mae: 0.004274, mean_q: 0.025513
 92066/100000: episode: 9212, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000018, mae: 0.002401, mean_q: 0.023655
 92076/100000: episode: 9213, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000054, mae: 0.003383, mean_q: 0.022804
 92086/100000: episode: 9214, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000058, mae: 0.003115, mean_q: 0.023483
 92096/100000: episode: 9215, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000056, mae: 0.002505, mean_q: 0.023934
 92106/100000: episode: 9216, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000058, mae: 0.002852, mean_q: 0.023760
 92116/100000: episode: 9217, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000066, mae: 0.004909, mean_q: 0.026124
 92126/100000: episode: 9218, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [1.000, 10.000], loss: 0.000052, mae: 0.003186, mean_q: 0.025691
 92136/100000: episode: 9219, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.001524, mae: 0.005454, mean_q: 0.024652
 92146/100000: episode: 9220, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000041, mae: 0.003462, mean_q: 0.025823
 92156/100000: episode: 9221, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000058, mae: 0.003579, mean_q: 0.023363
 92166/100000: episode: 9222, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000040, mae: 0.003417, mean_q: 0.022948
 92176/100000: episode: 9223, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000042, mae: 0.003164, mean_q: 0.023541
 92186/100000: episode: 9224, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.001510, mae: 0.005818, mean_q: 0.024872
 92196/100000: episode: 9225, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000054, mae: 0.003324, mean_q: 0.025572
 92206/100000: episode: 9226, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001542, mae: 0.006145, mean_q: 0.025418
 92216/100000: episode: 9227, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000019, mae: 0.002454, mean_q: 0.025172
 92226/100000: episode: 9228, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000057, mae: 0.003432, mean_q: 0.023328
 92236/100000: episode: 9229, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000080, mae: 0.003522, mean_q: 0.024202
 92246/100000: episode: 9230, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000264, mae: 0.004204, mean_q: 0.024341
 92256/100000: episode: 9231, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000034, mae: 0.001710, mean_q: 0.024490
 92266/100000: episode: 9232, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000059, mae: 0.002908, mean_q: 0.024065
 92276/100000: episode: 9233, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000099, mae: 0.004188, mean_q: 0.023401
 92286/100000: episode: 9234, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000056, mae: 0.002543, mean_q: 0.024380
 92296/100000: episode: 9235, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000052, mae: 0.002207, mean_q: 0.024306
 92306/100000: episode: 9236, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000023, mae: 0.002519, mean_q: 0.023828
 92316/100000: episode: 9237, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.001712, mae: 0.006880, mean_q: 0.023476
 92326/100000: episode: 9238, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.001565, mae: 0.009759, mean_q: 0.028486
 92336/100000: episode: 9239, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000029, mae: 0.004239, mean_q: 0.026782
 92346/100000: episode: 9240, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000040, mae: 0.004386, mean_q: 0.021833
 92356/100000: episode: 9241, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000018, mae: 0.002724, mean_q: 0.023355
 92366/100000: episode: 9242, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000012, mae: 0.001384, mean_q: 0.024146
 92376/100000: episode: 9243, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000077, mae: 0.002989, mean_q: 0.024340
 92386/100000: episode: 9244, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000200, mae: 0.003029, mean_q: 0.023734
 92396/100000: episode: 9245, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000096, mae: 0.003664, mean_q: 0.024548
 92406/100000: episode: 9246, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.001530, mae: 0.007717, mean_q: 0.026807
 92416/100000: episode: 9247, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000066, mae: 0.003913, mean_q: 0.024715
 92426/100000: episode: 9248, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000030, mae: 0.002692, mean_q: 0.023021
 92436/100000: episode: 9249, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000226, mae: 0.003923, mean_q: 0.023920
 92446/100000: episode: 9250, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000227, mae: 0.004454, mean_q: 0.025430
 92456/100000: episode: 9251, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.001523, mae: 0.005268, mean_q: 0.024661
 92466/100000: episode: 9252, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000055, mae: 0.003447, mean_q: 0.025967
 92476/100000: episode: 9253, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000038, mae: 0.002789, mean_q: 0.025337
 92486/100000: episode: 9254, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000018, mae: 0.002910, mean_q: 0.023286
 92496/100000: episode: 9255, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000039, mae: 0.002973, mean_q: 0.023693
 92506/100000: episode: 9256, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000022, mae: 0.002299, mean_q: 0.024221
 92516/100000: episode: 9257, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000042, mae: 0.003346, mean_q: 0.023366
 92526/100000: episode: 9258, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000039, mae: 0.002884, mean_q: 0.023480
 92536/100000: episode: 9259, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.001560, mae: 0.005778, mean_q: 0.024195
 92546/100000: episode: 9260, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000028, mae: 0.004232, mean_q: 0.026553
 92556/100000: episode: 9261, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000102, mae: 0.004085, mean_q: 0.024277
 92566/100000: episode: 9262, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000097, mae: 0.003541, mean_q: 0.024193
 92576/100000: episode: 9263, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000058, mae: 0.002725, mean_q: 0.024352
 92586/100000: episode: 9264, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000202, mae: 0.003489, mean_q: 0.023377
 92596/100000: episode: 9265, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000037, mae: 0.002335, mean_q: 0.024138
 92606/100000: episode: 9266, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001554, mae: 0.007148, mean_q: 0.025281
 92616/100000: episode: 9267, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000021, mae: 0.003000, mean_q: 0.025732
 92626/100000: episode: 9268, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000075, mae: 0.003249, mean_q: 0.023925
 92636/100000: episode: 9269, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000058, mae: 0.002727, mean_q: 0.024152
 92646/100000: episode: 9270, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000052, mae: 0.002353, mean_q: 0.023980
 92656/100000: episode: 9271, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000059, mae: 0.003077, mean_q: 0.023853
 92666/100000: episode: 9272, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.001989, mean_q: 0.024188
 92676/100000: episode: 9273, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000015, mae: 0.001688, mean_q: 0.024087
 92686/100000: episode: 9274, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000075, mae: 0.002940, mean_q: 0.024092
 92696/100000: episode: 9275, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000057, mae: 0.002725, mean_q: 0.024201
 92706/100000: episode: 9276, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.001526, mae: 0.006334, mean_q: 0.025599
 92716/100000: episode: 9277, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000018, mae: 0.002636, mean_q: 0.025462
 92726/100000: episode: 9278, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000085, mae: 0.004411, mean_q: 0.023378
 92736/100000: episode: 9279, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000040, mae: 0.002849, mean_q: 0.023751
 92746/100000: episode: 9280, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000243, mae: 0.003819, mean_q: 0.024083
 92756/100000: episode: 9281, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000043, mae: 0.003498, mean_q: 0.025413
[Info] 1-TH LEVEL FOUND: 0.023618929088115692, Considering 100/100 traces
 92766/100000: episode: 9282, duration: 0.857s, episode steps: 10, steps per second: 12, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000075, mae: 0.003048, mean_q: 0.024010
[Info] 2-TH LEVEL FOUND: 0.024072609841823578, Considering 100/100 traces
 92776/100000: episode: 9283, duration: 0.671s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000095, mae: 0.003392, mean_q: 0.024035
[Info] 3-TH LEVEL FOUND: 0.0245065875351429, Considering 100/100 traces
 92786/100000: episode: 9284, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000073, mae: 0.002630, mean_q: 0.024109
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.0245065875351429
3
 92796/100000: episode: 9285, duration: 0.503s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000016, mae: 0.001792, mean_q: 0.024166
 92806/100000: episode: 9286, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000018, mae: 0.002669, mean_q: 0.023223
 92816/100000: episode: 9287, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000040, mae: 0.003210, mean_q: 0.023040
 92826/100000: episode: 9288, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001555, mae: 0.006863, mean_q: 0.023800
 92836/100000: episode: 9289, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000050, mae: 0.005968, mean_q: 0.028030
 92846/100000: episode: 9290, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000102, mae: 0.004573, mean_q: 0.025310
 92856/100000: episode: 9291, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [0.000, 10.000], loss: 0.000078, mae: 0.003754, mean_q: 0.023529
 92866/100000: episode: 9292, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000032, mae: 0.002161, mean_q: 0.023717
 92876/100000: episode: 9293, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000071, mae: 0.002598, mean_q: 0.024580
 92886/100000: episode: 9294, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000034, mae: 0.002097, mean_q: 0.024723
 92896/100000: episode: 9295, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000062, mae: 0.003043, mean_q: 0.024489
 92906/100000: episode: 9296, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000056, mae: 0.002858, mean_q: 0.023852
 92916/100000: episode: 9297, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000056, mae: 0.003015, mean_q: 0.023520
 92926/100000: episode: 9298, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000046, mae: 0.003075, mean_q: 0.024618
 92936/100000: episode: 9299, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000258, mae: 0.003627, mean_q: 0.024180
 92946/100000: episode: 9300, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000058, mae: 0.003177, mean_q: 0.025067
 92956/100000: episode: 9301, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000056, mae: 0.002754, mean_q: 0.023907
 92966/100000: episode: 9302, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000015, mae: 0.002201, mean_q: 0.023447
 92976/100000: episode: 9303, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000077, mae: 0.003621, mean_q: 0.023268
 92986/100000: episode: 9304, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000016, mae: 0.001947, mean_q: 0.023732
 92996/100000: episode: 9305, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000098, mae: 0.003649, mean_q: 0.023595
Step 93000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.93000.hdf5
 93006/100000: episode: 9306, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000018, mae: 0.001805, mean_q: 0.024177
 93016/100000: episode: 9307, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000223, mae: 0.003388, mean_q: 0.023777
 93026/100000: episode: 9308, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000081, mae: 0.003168, mean_q: 0.024049
 93036/100000: episode: 9309, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000039, mae: 0.002625, mean_q: 0.023412
 93046/100000: episode: 9310, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000033, mae: 0.002072, mean_q: 0.023437
 93056/100000: episode: 9311, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000075, mae: 0.002876, mean_q: 0.023650
 93066/100000: episode: 9312, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.001525, mae: 0.007076, mean_q: 0.026279
 93076/100000: episode: 9313, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000095, mae: 0.003742, mean_q: 0.024835
 93086/100000: episode: 9314, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000019, mae: 0.002560, mean_q: 0.023213
 93096/100000: episode: 9315, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000053, mae: 0.002594, mean_q: 0.023426
 93106/100000: episode: 9316, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000222, mae: 0.003557, mean_q: 0.024371
 93116/100000: episode: 9317, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000246, mae: 0.003998, mean_q: 0.024332
 93126/100000: episode: 9318, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000020, mae: 0.002349, mean_q: 0.024064
 93136/100000: episode: 9319, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000044, mae: 0.003846, mean_q: 0.022499
 93146/100000: episode: 9320, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000041, mae: 0.003008, mean_q: 0.023246
 93156/100000: episode: 9321, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001533, mae: 0.006076, mean_q: 0.023858
 93166/100000: episode: 9322, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000045, mae: 0.004490, mean_q: 0.026139
 93176/100000: episode: 9323, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000041, mae: 0.003267, mean_q: 0.023209
 93186/100000: episode: 9324, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000040, mae: 0.003671, mean_q: 0.022265
 93196/100000: episode: 9325, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000016, mae: 0.002198, mean_q: 0.023056
 93206/100000: episode: 9326, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001565, mae: 0.006830, mean_q: 0.024758
 93216/100000: episode: 9327, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.001544, mae: 0.009208, mean_q: 0.028204
 93226/100000: episode: 9328, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000022, mae: 0.003187, mean_q: 0.025209
 93236/100000: episode: 9329, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000058, mae: 0.004229, mean_q: 0.021975
 93246/100000: episode: 9330, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000019, mae: 0.002463, mean_q: 0.023414
 93256/100000: episode: 9331, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000060, mae: 0.002793, mean_q: 0.023936
 93266/100000: episode: 9332, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000077, mae: 0.002975, mean_q: 0.023935
 93276/100000: episode: 9333, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000021, mae: 0.002218, mean_q: 0.024295
 93286/100000: episode: 9334, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000081, mae: 0.003620, mean_q: 0.023489
 93296/100000: episode: 9335, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000015, mae: 0.002164, mean_q: 0.023121
 93306/100000: episode: 9336, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000039, mae: 0.002822, mean_q: 0.023204
 93316/100000: episode: 9337, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000076, mae: 0.003182, mean_q: 0.023308
 93326/100000: episode: 9338, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.001953, mean_q: 0.023670
 93336/100000: episode: 9339, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000305, mae: 0.005259, mean_q: 0.024018
 93346/100000: episode: 9340, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001541, mae: 0.008496, mean_q: 0.027683
 93356/100000: episode: 9341, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000040, mae: 0.004052, mean_q: 0.026475
 93366/100000: episode: 9342, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000035, mae: 0.003581, mean_q: 0.022335
 93376/100000: episode: 9343, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000022, mae: 0.003328, mean_q: 0.022610
 93386/100000: episode: 9344, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000045, mae: 0.003404, mean_q: 0.023244
 93396/100000: episode: 9345, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000053, mae: 0.002417, mean_q: 0.023638
 93406/100000: episode: 9346, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000020, mae: 0.002314, mean_q: 0.023781
 93416/100000: episode: 9347, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000058, mae: 0.003607, mean_q: 0.022591
 93426/100000: episode: 9348, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000227, mae: 0.004151, mean_q: 0.022756
 93436/100000: episode: 9349, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000010, mae: 0.001824, mean_q: 0.024578
 93446/100000: episode: 9350, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000064, mae: 0.003350, mean_q: 0.023780
 93456/100000: episode: 9351, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002637, mean_q: 0.022741
 93466/100000: episode: 9352, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000033, mae: 0.002354, mean_q: 0.022881
 93476/100000: episode: 9353, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000227, mae: 0.003852, mean_q: 0.023751
 93486/100000: episode: 9354, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000053, mae: 0.002369, mean_q: 0.023752
 93496/100000: episode: 9355, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000041, mae: 0.002777, mean_q: 0.023035
 93506/100000: episode: 9356, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000018, mae: 0.001974, mean_q: 0.023219
 93516/100000: episode: 9357, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000019, mae: 0.002637, mean_q: 0.022525
 93526/100000: episode: 9358, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.001530, mae: 0.006305, mean_q: 0.024411
 93536/100000: episode: 9359, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000018, mae: 0.002710, mean_q: 0.024776
 93546/100000: episode: 9360, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000045, mae: 0.003843, mean_q: 0.022218
 93556/100000: episode: 9361, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000039, mae: 0.002812, mean_q: 0.022839
 93566/100000: episode: 9362, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000095, mae: 0.003158, mean_q: 0.023666
 93576/100000: episode: 9363, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000078, mae: 0.003597, mean_q: 0.024247
 93586/100000: episode: 9364, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000043, mae: 0.004913, mean_q: 0.026761
 93596/100000: episode: 9365, duration: 0.089s, episode steps: 10, steps per second: 112, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000079, mae: 0.003787, mean_q: 0.024620
 93606/100000: episode: 9366, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000247, mae: 0.004810, mean_q: 0.022678
 93616/100000: episode: 9367, duration: 0.094s, episode steps: 10, steps per second: 107, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000035, mae: 0.001959, mean_q: 0.023649
 93626/100000: episode: 9368, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000054, mae: 0.002512, mean_q: 0.023377
 93636/100000: episode: 9369, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000036, mae: 0.002522, mean_q: 0.023184
 93646/100000: episode: 9370, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000037, mae: 0.002532, mean_q: 0.022998
 93656/100000: episode: 9371, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.001532, mae: 0.007383, mean_q: 0.026095
 93666/100000: episode: 9372, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000019, mae: 0.002809, mean_q: 0.024885
 93676/100000: episode: 9373, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000268, mae: 0.005772, mean_q: 0.021929
 93686/100000: episode: 9374, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000095, mae: 0.003909, mean_q: 0.024415
 93696/100000: episode: 9375, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000015, mae: 0.002399, mean_q: 0.024960
 93706/100000: episode: 9376, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.002605, mean_q: 0.022921
 93716/100000: episode: 9377, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000049, mae: 0.002564, mean_q: 0.022790
 93726/100000: episode: 9378, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.001530, mae: 0.007130, mean_q: 0.025970
 93736/100000: episode: 9379, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000095, mae: 0.004417, mean_q: 0.025440
 93746/100000: episode: 9380, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.001527, mae: 0.005803, mean_q: 0.022969
 93756/100000: episode: 9381, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000227, mae: 0.005424, mean_q: 0.026479
 93766/100000: episode: 9382, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000236, mae: 0.004617, mean_q: 0.025968
 93776/100000: episode: 9383, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000221, mae: 0.004989, mean_q: 0.026718
 93786/100000: episode: 9384, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000201, mae: 0.003663, mean_q: 0.025930
[Info] 1-TH LEVEL FOUND: 0.02431429736316204, Considering 100/100 traces
 93796/100000: episode: 9385, duration: 0.810s, episode steps: 10, steps per second: 12, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000230, mae: 0.003930, mean_q: 0.024674
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02431429736316204
1
 93806/100000: episode: 9386, duration: 0.524s, episode steps: 10, steps per second: 19, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000210, mae: 0.004091, mean_q: 0.023941
 93816/100000: episode: 9387, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000039, mae: 0.002578, mean_q: 0.024270
 93826/100000: episode: 9388, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000079, mae: 0.003335, mean_q: 0.024327
 93836/100000: episode: 9389, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000056, mae: 0.002729, mean_q: 0.024138
 93846/100000: episode: 9390, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000223, mae: 0.003324, mean_q: 0.024529
 93856/100000: episode: 9391, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000207, mae: 0.004370, mean_q: 0.026077
 93866/100000: episode: 9392, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000035, mae: 0.002552, mean_q: 0.024264
 93876/100000: episode: 9393, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000207, mae: 0.003813, mean_q: 0.023836
 93886/100000: episode: 9394, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000067, mae: 0.003653, mean_q: 0.024034
 93896/100000: episode: 9395, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000057, mae: 0.002658, mean_q: 0.024313
 93906/100000: episode: 9396, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000100, mae: 0.003854, mean_q: 0.024886
 93916/100000: episode: 9397, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000068, mae: 0.004109, mean_q: 0.023766
 93926/100000: episode: 9398, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000060, mae: 0.003986, mean_q: 0.022830
 93936/100000: episode: 9399, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000198, mae: 0.002807, mean_q: 0.024121
 93946/100000: episode: 9400, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000055, mae: 0.002665, mean_q: 0.024563
 93956/100000: episode: 9401, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000098, mae: 0.003664, mean_q: 0.024025
 93966/100000: episode: 9402, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.001527, mae: 0.005625, mean_q: 0.023589
 93976/100000: episode: 9403, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001693, mae: 0.010140, mean_q: 0.029177
 93986/100000: episode: 9404, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000044, mae: 0.004370, mean_q: 0.027171
 93996/100000: episode: 9405, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [0.000, 10.000], loss: 0.000208, mae: 0.004323, mean_q: 0.023263
Step 94000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.94000.hdf5
 94006/100000: episode: 9406, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000281, mae: 0.004947, mean_q: 0.025247
 94016/100000: episode: 9407, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.001578, mae: 0.008050, mean_q: 0.026804
 94026/100000: episode: 9408, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000049, mae: 0.005128, mean_q: 0.027965
 94036/100000: episode: 9409, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000042, mae: 0.003286, mean_q: 0.024790
 94046/100000: episode: 9410, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.001709, mae: 0.007237, mean_q: 0.023868
 94056/100000: episode: 9411, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000207, mae: 0.005534, mean_q: 0.028003
 94066/100000: episode: 9412, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000152, mae: 0.004817, mean_q: 0.025944
 94076/100000: episode: 9413, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000096, mae: 0.003446, mean_q: 0.025482
 94086/100000: episode: 9414, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000035, mae: 0.002733, mean_q: 0.024529
 94096/100000: episode: 9415, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000020, mae: 0.003133, mean_q: 0.023916
 94106/100000: episode: 9416, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000041, mae: 0.003537, mean_q: 0.023868
 94116/100000: episode: 9417, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.003200, mae: 0.010796, mean_q: 0.026335
 94126/100000: episode: 9418, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000115, mae: 0.008185, mean_q: 0.030272
 94136/100000: episode: 9419, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000094, mae: 0.003922, mean_q: 0.026293
 94146/100000: episode: 9420, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000118, mae: 0.004963, mean_q: 0.024201
 94156/100000: episode: 9421, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000277, mae: 0.005568, mean_q: 0.027333
 94166/100000: episode: 9422, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000035, mae: 0.003153, mean_q: 0.027232
 94176/100000: episode: 9423, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000059, mae: 0.003659, mean_q: 0.024647
 94186/100000: episode: 9424, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000275, mae: 0.004642, mean_q: 0.025236
 94196/100000: episode: 9425, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000116, mae: 0.005354, mean_q: 0.027734
 94206/100000: episode: 9426, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000420, mae: 0.004886, mean_q: 0.026430
 94216/100000: episode: 9427, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000040, mae: 0.002617, mean_q: 0.026470
 94226/100000: episode: 9428, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.002984, mae: 0.009725, mean_q: 0.028027
 94236/100000: episode: 9429, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000070, mae: 0.005245, mean_q: 0.028728
 94246/100000: episode: 9430, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000018, mae: 0.003178, mean_q: 0.024708
 94256/100000: episode: 9431, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000036, mae: 0.003427, mean_q: 0.024378
 94266/100000: episode: 9432, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000053, mae: 0.002432, mean_q: 0.026244
 94276/100000: episode: 9433, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.001687, mae: 0.008082, mean_q: 0.028184
 94286/100000: episode: 9434, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000064, mae: 0.004061, mean_q: 0.027453
 94296/100000: episode: 9435, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000227, mae: 0.004807, mean_q: 0.025063
 94306/100000: episode: 9436, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000037, mae: 0.002889, mean_q: 0.025339
 94316/100000: episode: 9437, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000060, mae: 0.003506, mean_q: 0.025234
 94326/100000: episode: 9438, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000094, mae: 0.003738, mean_q: 0.025304
 94336/100000: episode: 9439, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000080, mae: 0.003304, mean_q: 0.026105
 94346/100000: episode: 9440, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000260, mae: 0.004537, mean_q: 0.026507
 94356/100000: episode: 9441, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000025, mae: 0.002688, mean_q: 0.026503
 94366/100000: episode: 9442, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000029, mae: 0.004150, mean_q: 0.024145
 94376/100000: episode: 9443, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000010, mae: 0.002072, mean_q: 0.024686
 94386/100000: episode: 9444, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000021, mae: 0.002232, mean_q: 0.025486
 94396/100000: episode: 9445, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.001568, mae: 0.007189, mean_q: 0.025528
 94406/100000: episode: 9446, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000042, mae: 0.003442, mean_q: 0.027031
 94416/100000: episode: 9447, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.001778, mae: 0.007833, mean_q: 0.026053
 94426/100000: episode: 9448, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.001539, mae: 0.009245, mean_q: 0.029985
 94436/100000: episode: 9449, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000217, mae: 0.005349, mean_q: 0.029326
 94446/100000: episode: 9450, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000065, mae: 0.003933, mean_q: 0.025948
 94456/100000: episode: 9451, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000079, mae: 0.004454, mean_q: 0.024823
 94466/100000: episode: 9452, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000036, mae: 0.002413, mean_q: 0.025828
 94476/100000: episode: 9453, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000225, mae: 0.003789, mean_q: 0.026007
 94486/100000: episode: 9454, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.001526, mae: 0.005808, mean_q: 0.026446
 94496/100000: episode: 9455, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000205, mae: 0.003959, mean_q: 0.027438
 94506/100000: episode: 9456, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000055, mae: 0.002661, mean_q: 0.026884
 94516/100000: episode: 9457, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.001722, mae: 0.008083, mean_q: 0.027892
 94526/100000: episode: 9458, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000060, mae: 0.004055, mean_q: 0.028454
 94536/100000: episode: 9459, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000071, mae: 0.002861, mean_q: 0.026245
 94546/100000: episode: 9460, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000037, mae: 0.002331, mean_q: 0.026260
 94556/100000: episode: 9461, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000073, mae: 0.002962, mean_q: 0.026211
 94566/100000: episode: 9462, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000057, mae: 0.002891, mean_q: 0.026200
 94576/100000: episode: 9463, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000083, mae: 0.004167, mean_q: 0.025804
 94586/100000: episode: 9464, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000286, mae: 0.005503, mean_q: 0.025936
 94596/100000: episode: 9465, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000200, mae: 0.003087, mean_q: 0.026971
 94606/100000: episode: 9466, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000021, mae: 0.002445, mean_q: 0.025993
 94616/100000: episode: 9467, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000075, mae: 0.003598, mean_q: 0.025373
 94626/100000: episode: 9468, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.000206, mae: 0.003379, mean_q: 0.026003
 94636/100000: episode: 9469, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000039, mae: 0.002355, mean_q: 0.026254
 94646/100000: episode: 9470, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000091, mae: 0.003226, mean_q: 0.025899
 94656/100000: episode: 9471, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000079, mae: 0.003300, mean_q: 0.026397
 94666/100000: episode: 9472, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000095, mae: 0.003473, mean_q: 0.026083
 94676/100000: episode: 9473, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000042, mae: 0.002714, mean_q: 0.026488
 94686/100000: episode: 9474, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000060, mae: 0.003155, mean_q: 0.025840
 94696/100000: episode: 9475, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [0.000, 10.000], loss: 0.000023, mae: 0.002974, mean_q: 0.025234
 94706/100000: episode: 9476, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000235, mae: 0.003697, mean_q: 0.026577
 94716/100000: episode: 9477, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000062, mae: 0.003816, mean_q: 0.027478
 94726/100000: episode: 9478, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000248, mae: 0.004429, mean_q: 0.026332
 94736/100000: episode: 9479, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000019, mae: 0.002764, mean_q: 0.025213
 94746/100000: episode: 9480, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.003464, mean_q: 0.024527
 94756/100000: episode: 9481, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000222, mae: 0.003923, mean_q: 0.025173
 94766/100000: episode: 9482, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000076, mae: 0.003149, mean_q: 0.026271
 94776/100000: episode: 9483, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.001528, mae: 0.006066, mean_q: 0.025568
 94786/100000: episode: 9484, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000071, mae: 0.003886, mean_q: 0.027547
 94796/100000: episode: 9485, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000116, mae: 0.004502, mean_q: 0.026938
[Info] 1-TH LEVEL FOUND: 0.026507068425416946, Considering 100/100 traces
 94806/100000: episode: 9486, duration: 0.726s, episode steps: 10, steps per second: 14, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000409, mae: 0.004917, mean_q: 0.026038
[Info] 2-TH LEVEL FOUND: 0.02715039812028408, Considering 100/100 traces
 94816/100000: episode: 9487, duration: 0.906s, episode steps: 10, steps per second: 11, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000263, mae: 0.004608, mean_q: 0.026479
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.02715039812028408
2
 94826/100000: episode: 9488, duration: 0.548s, episode steps: 10, steps per second: 18, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000027, mae: 0.003060, mean_q: 0.027131
 94836/100000: episode: 9489, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.001526, mae: 0.005992, mean_q: 0.025638
 94846/100000: episode: 9490, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [0.000, 10.000], loss: 0.000061, mae: 0.003453, mean_q: 0.027034
 94856/100000: episode: 9491, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000039, mae: 0.002558, mean_q: 0.025988
 94866/100000: episode: 9492, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000056, mae: 0.002975, mean_q: 0.025560
 94876/100000: episode: 9493, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000042, mae: 0.002694, mean_q: 0.025875
 94886/100000: episode: 9494, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000101, mae: 0.003849, mean_q: 0.026152
 94896/100000: episode: 9495, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000219, mae: 0.003336, mean_q: 0.025975
 94906/100000: episode: 9496, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000132, mae: 0.004689, mean_q: 0.026983
 94916/100000: episode: 9497, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.002698, mean_q: 0.026934
 94926/100000: episode: 9498, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000223, mae: 0.003829, mean_q: 0.025715
 94936/100000: episode: 9499, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000032, mae: 0.002256, mean_q: 0.025639
 94946/100000: episode: 9500, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000017, mae: 0.002522, mean_q: 0.025176
 94956/100000: episode: 9501, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000198, mae: 0.003451, mean_q: 0.024791
 94966/100000: episode: 9502, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [0.000, 10.000], loss: 0.000055, mae: 0.003025, mean_q: 0.026590
 94976/100000: episode: 9503, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000200, mae: 0.003219, mean_q: 0.026383
 94986/100000: episode: 9504, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000224, mae: 0.004015, mean_q: 0.025184
 94996/100000: episode: 9505, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000222, mae: 0.003415, mean_q: 0.025833
Step 95000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.95000.hdf5
 95006/100000: episode: 9506, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000235, mae: 0.003672, mean_q: 0.026568
 95016/100000: episode: 9507, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000216, mae: 0.003491, mean_q: 0.026922
 95026/100000: episode: 9508, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000024, mae: 0.002588, mean_q: 0.026221
 95036/100000: episode: 9509, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000218, mae: 0.003950, mean_q: 0.024642
 95046/100000: episode: 9510, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000027, mae: 0.002646, mean_q: 0.026053
 95056/100000: episode: 9511, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000225, mae: 0.004312, mean_q: 0.024773
 95066/100000: episode: 9512, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000016, mae: 0.002051, mean_q: 0.025304
 95076/100000: episode: 9513, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000208, mae: 0.003574, mean_q: 0.025499
 95086/100000: episode: 9514, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000076, mae: 0.003664, mean_q: 0.024673
 95096/100000: episode: 9515, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000055, mae: 0.002623, mean_q: 0.025742
 95106/100000: episode: 9516, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000079, mae: 0.003427, mean_q: 0.025599
 95116/100000: episode: 9517, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000092, mae: 0.003134, mean_q: 0.025408
 95126/100000: episode: 9518, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000204, mae: 0.003292, mean_q: 0.025845
 95136/100000: episode: 9519, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000297, mae: 0.005359, mean_q: 0.026387
 95146/100000: episode: 9520, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000240, mae: 0.005104, mean_q: 0.027519
 95156/100000: episode: 9521, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000294, mae: 0.006164, mean_q: 0.027776
 95166/100000: episode: 9522, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000040, mae: 0.003777, mean_q: 0.028028
 95176/100000: episode: 9523, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [1.000, 10.000], loss: 0.000233, mae: 0.004863, mean_q: 0.025327
 95186/100000: episode: 9524, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000034, mae: 0.002394, mean_q: 0.025460
 95196/100000: episode: 9525, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000036, mae: 0.002236, mean_q: 0.025784
 95206/100000: episode: 9526, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000018, mae: 0.002777, mean_q: 0.024843
 95216/100000: episode: 9527, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000408, mae: 0.005135, mean_q: 0.025164
 95226/100000: episode: 9528, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000230, mae: 0.005026, mean_q: 0.027236
 95236/100000: episode: 9529, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000230, mae: 0.004810, mean_q: 0.024822
 95246/100000: episode: 9530, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000209, mae: 0.004581, mean_q: 0.024356
 95256/100000: episode: 9531, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000268, mae: 0.005398, mean_q: 0.026541
 95266/100000: episode: 9532, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000062, mae: 0.003517, mean_q: 0.026245
 95276/100000: episode: 9533, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000042, mae: 0.003805, mean_q: 0.024198
 95286/100000: episode: 9534, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000035, mae: 0.002999, mean_q: 0.024420
 95296/100000: episode: 9535, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000265, mae: 0.004711, mean_q: 0.025761
 95306/100000: episode: 9536, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000033, mae: 0.002038, mean_q: 0.025769
 95316/100000: episode: 9537, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000032, mae: 0.002325, mean_q: 0.024736
 95326/100000: episode: 9538, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000019, mae: 0.002410, mean_q: 0.024765
 95336/100000: episode: 9539, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000610, mae: 0.006576, mean_q: 0.024289
 95346/100000: episode: 9540, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000022, mae: 0.003352, mean_q: 0.026929
 95356/100000: episode: 9541, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000055, mae: 0.002597, mean_q: 0.025194
 95366/100000: episode: 9542, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000041, mae: 0.003434, mean_q: 0.024096
 95376/100000: episode: 9543, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000038, mae: 0.003302, mean_q: 0.023910
 95386/100000: episode: 9544, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000071, mae: 0.003261, mean_q: 0.024031
 95396/100000: episode: 9545, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000040, mae: 0.002867, mean_q: 0.025577
 95406/100000: episode: 9546, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000075, mae: 0.002951, mean_q: 0.024888
 95416/100000: episode: 9547, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000056, mae: 0.002759, mean_q: 0.024606
 95426/100000: episode: 9548, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000464, mae: 0.006455, mean_q: 0.026085
 95436/100000: episode: 9549, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000028, mae: 0.003889, mean_q: 0.026989
 95446/100000: episode: 9550, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000219, mae: 0.004011, mean_q: 0.023993
 95456/100000: episode: 9551, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000050, mae: 0.002405, mean_q: 0.024447
 95466/100000: episode: 9552, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000036, mae: 0.001902, mean_q: 0.025154
 95476/100000: episode: 9553, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000073, mae: 0.003062, mean_q: 0.024405
 95486/100000: episode: 9554, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000237, mae: 0.003589, mean_q: 0.024506
 95496/100000: episode: 9555, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.002736, mean_q: 0.025883
 95506/100000: episode: 9556, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000053, mae: 0.002482, mean_q: 0.024748
 95516/100000: episode: 9557, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000059, mae: 0.002831, mean_q: 0.024787
 95526/100000: episode: 9558, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000298, mae: 0.005583, mean_q: 0.026147
 95536/100000: episode: 9559, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000223, mae: 0.004837, mean_q: 0.026889
 95546/100000: episode: 9560, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000060, mae: 0.003271, mean_q: 0.024942
 95556/100000: episode: 9561, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000080, mae: 0.004231, mean_q: 0.023779
 95566/100000: episode: 9562, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000238, mae: 0.004017, mean_q: 0.025824
 95576/100000: episode: 9563, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000220, mae: 0.003524, mean_q: 0.025364
 95586/100000: episode: 9564, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000827, mae: 0.008449, mean_q: 0.025941
 95596/100000: episode: 9565, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000025, mae: 0.004851, mean_q: 0.028840
 95606/100000: episode: 9566, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000017, mae: 0.002666, mean_q: 0.024687
 95616/100000: episode: 9567, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000205, mae: 0.004370, mean_q: 0.023702
 95626/100000: episode: 9568, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000256, mae: 0.004405, mean_q: 0.026194
 95636/100000: episode: 9569, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002514, mean_q: 0.026073
 95646/100000: episode: 9570, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000077, mae: 0.003732, mean_q: 0.024570
 95656/100000: episode: 9571, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000033, mae: 0.002255, mean_q: 0.024705
 95666/100000: episode: 9572, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000429, mae: 0.005532, mean_q: 0.024541
 95676/100000: episode: 9573, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000036, mae: 0.003089, mean_q: 0.026514
 95686/100000: episode: 9574, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000228, mae: 0.004466, mean_q: 0.024445
 95696/100000: episode: 9575, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000034, mae: 0.002877, mean_q: 0.024012
 95706/100000: episode: 9576, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000075, mae: 0.002969, mean_q: 0.024945
 95716/100000: episode: 9577, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000288, mae: 0.005270, mean_q: 0.024868
 95726/100000: episode: 9578, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.000204, mae: 0.003562, mean_q: 0.025831
 95736/100000: episode: 9579, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000239, mae: 0.003717, mean_q: 0.025565
 95746/100000: episode: 9580, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000039, mae: 0.002716, mean_q: 0.024716
 95756/100000: episode: 9581, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000042, mae: 0.003483, mean_q: 0.023958
 95766/100000: episode: 9582, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000040, mae: 0.003027, mean_q: 0.024240
 95776/100000: episode: 9583, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000256, mae: 0.004287, mean_q: 0.023968
 95786/100000: episode: 9584, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000041, mae: 0.003183, mean_q: 0.025793
 95796/100000: episode: 9585, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000035, mae: 0.002425, mean_q: 0.024454
 95806/100000: episode: 9586, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000063, mae: 0.004027, mean_q: 0.023490
 95816/100000: episode: 9587, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000282, mae: 0.004953, mean_q: 0.024947
[Info] 1-TH LEVEL FOUND: 0.024644453078508377, Considering 100/100 traces
 95826/100000: episode: 9588, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000034, mae: 0.002512, mean_q: 0.025667
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024644453078508377
1
 95836/100000: episode: 9589, duration: 0.502s, episode steps: 10, steps per second: 20, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000015, mae: 0.001931, mean_q: 0.024200
 95846/100000: episode: 9590, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000059, mae: 0.003304, mean_q: 0.024010
 95856/100000: episode: 9591, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000223, mae: 0.003597, mean_q: 0.024940
 95866/100000: episode: 9592, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000127, mae: 0.003691, mean_q: 0.025185
 95876/100000: episode: 9593, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000057, mae: 0.002997, mean_q: 0.025406
 95886/100000: episode: 9594, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000224, mae: 0.003508, mean_q: 0.024975
 95896/100000: episode: 9595, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002864, mean_q: 0.024205
 95906/100000: episode: 9596, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000240, mae: 0.003852, mean_q: 0.024559
 95916/100000: episode: 9597, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000203, mae: 0.003227, mean_q: 0.025274
 95926/100000: episode: 9598, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000038, mae: 0.002477, mean_q: 0.024773
 95936/100000: episode: 9599, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000199, mae: 0.003450, mean_q: 0.023655
 95946/100000: episode: 9600, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000241, mae: 0.004110, mean_q: 0.024964
 95956/100000: episode: 9601, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000080, mae: 0.004493, mean_q: 0.026348
 95966/100000: episode: 9602, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000057, mae: 0.003088, mean_q: 0.024090
 95976/100000: episode: 9603, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000219, mae: 0.003529, mean_q: 0.024621
 95986/100000: episode: 9604, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000035, mae: 0.002228, mean_q: 0.025311
 95996/100000: episode: 9605, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000054, mae: 0.002640, mean_q: 0.024318
Step 96000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.96000.hdf5
 96006/100000: episode: 9606, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000200, mae: 0.002945, mean_q: 0.025012
 96016/100000: episode: 9607, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000033, mae: 0.001968, mean_q: 0.024604
 96026/100000: episode: 9608, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000227, mae: 0.004312, mean_q: 0.024001
 96036/100000: episode: 9609, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002522, mean_q: 0.024874
 96046/100000: episode: 9610, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000039, mae: 0.002536, mean_q: 0.024280
 96056/100000: episode: 9611, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000016, mae: 0.001920, mean_q: 0.024213
 96066/100000: episode: 9612, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000055, mae: 0.002966, mean_q: 0.023843
 96076/100000: episode: 9613, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.002768, mean_q: 0.023791
 96086/100000: episode: 9614, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000014, mae: 0.002180, mean_q: 0.023653
 96096/100000: episode: 9615, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000022, mae: 0.003420, mean_q: 0.022742
 96106/100000: episode: 9616, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000077, mae: 0.003866, mean_q: 0.022979
 96116/100000: episode: 9617, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000039, mae: 0.002817, mean_q: 0.024792
 96126/100000: episode: 9618, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000220, mae: 0.003230, mean_q: 0.023636
 96136/100000: episode: 9619, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000019, mae: 0.002185, mean_q: 0.023712
 96146/100000: episode: 9620, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000037, mae: 0.003033, mean_q: 0.022813
 96156/100000: episode: 9621, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000447, mae: 0.006032, mean_q: 0.024835
 96166/100000: episode: 9622, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000023, mae: 0.003403, mean_q: 0.025715
 96176/100000: episode: 9623, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000204, mae: 0.003909, mean_q: 0.022837
 96186/100000: episode: 9624, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000023, mae: 0.003430, mean_q: 0.022722
 96196/100000: episode: 9625, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000075, mae: 0.003850, mean_q: 0.022240
 96206/100000: episode: 9626, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000039, mae: 0.002502, mean_q: 0.024097
 96216/100000: episode: 9627, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000281, mae: 0.004804, mean_q: 0.024450
 96226/100000: episode: 9628, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000056, mae: 0.003215, mean_q: 0.024864
 96236/100000: episode: 9629, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000015, mae: 0.002279, mean_q: 0.023086
 96246/100000: episode: 9630, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000039, mae: 0.003900, mean_q: 0.021806
 96256/100000: episode: 9631, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000059, mae: 0.003141, mean_q: 0.022959
 96266/100000: episode: 9632, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000261, mae: 0.004481, mean_q: 0.024343
 96276/100000: episode: 9633, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000060, mae: 0.003605, mean_q: 0.024578
 96286/100000: episode: 9634, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000205, mae: 0.003673, mean_q: 0.022668
 96296/100000: episode: 9635, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000387, mae: 0.004147, mean_q: 0.023406
 96306/100000: episode: 9636, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000060, mae: 0.003650, mean_q: 0.024823
 96316/100000: episode: 9637, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000200, mae: 0.003399, mean_q: 0.022479
 96326/100000: episode: 9638, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000056, mae: 0.002789, mean_q: 0.023741
 96336/100000: episode: 9639, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000038, mae: 0.002418, mean_q: 0.023492
 96346/100000: episode: 9640, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000423, mae: 0.004685, mean_q: 0.023501
 96356/100000: episode: 9641, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000061, mae: 0.003763, mean_q: 0.024942
 96366/100000: episode: 9642, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000023, mae: 0.003078, mean_q: 0.022674
 96376/100000: episode: 9643, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000079, mae: 0.003976, mean_q: 0.022319
 96386/100000: episode: 9644, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000301, mae: 0.005843, mean_q: 0.025127
 96396/100000: episode: 9645, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000061, mae: 0.003602, mean_q: 0.024477
 96406/100000: episode: 9646, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000071, mae: 0.003015, mean_q: 0.022877
 96416/100000: episode: 9647, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000040, mae: 0.003051, mean_q: 0.022926
 96426/100000: episode: 9648, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000075, mae: 0.002806, mean_q: 0.023281
 96436/100000: episode: 9649, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000209, mae: 0.003488, mean_q: 0.023338
 96446/100000: episode: 9650, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000055, mae: 0.002647, mean_q: 0.023038
 96456/100000: episode: 9651, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000200, mae: 0.002729, mean_q: 0.023280
 96466/100000: episode: 9652, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000039, mae: 0.002651, mean_q: 0.023713
 96476/100000: episode: 9653, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000017, mae: 0.002858, mean_q: 0.021992
 96486/100000: episode: 9654, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002385, mean_q: 0.023029
 96496/100000: episode: 9655, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000075, mae: 0.002894, mean_q: 0.023078
 96506/100000: episode: 9656, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000044, mae: 0.003442, mean_q: 0.022391
 96516/100000: episode: 9657, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000017, mae: 0.002045, mean_q: 0.023053
 96526/100000: episode: 9658, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000205, mae: 0.003309, mean_q: 0.023341
 96536/100000: episode: 9659, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000019, mae: 0.002663, mean_q: 0.022154
 96546/100000: episode: 9660, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000285, mae: 0.005023, mean_q: 0.022221
 96556/100000: episode: 9661, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000076, mae: 0.003984, mean_q: 0.024408
 96566/100000: episode: 9662, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000407, mae: 0.004857, mean_q: 0.023989
 96576/100000: episode: 9663, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000064, mae: 0.003328, mean_q: 0.023276
 96586/100000: episode: 9664, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000092, mae: 0.003576, mean_q: 0.022112
 96596/100000: episode: 9665, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000041, mae: 0.002538, mean_q: 0.023200
 96606/100000: episode: 9666, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000059, mae: 0.003222, mean_q: 0.022288
 96616/100000: episode: 9667, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000055, mae: 0.002442, mean_q: 0.022976
 96626/100000: episode: 9668, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [0.000, 10.000], loss: 0.000078, mae: 0.003325, mean_q: 0.023591
 96636/100000: episode: 9669, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000058, mae: 0.002966, mean_q: 0.023654
 96646/100000: episode: 9670, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000409, mae: 0.004852, mean_q: 0.022060
 96656/100000: episode: 9671, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000020, mae: 0.003323, mean_q: 0.024851
 96666/100000: episode: 9672, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000078, mae: 0.003711, mean_q: 0.022102
 96676/100000: episode: 9673, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.350 [1.000, 10.000], loss: 0.000033, mae: 0.002022, mean_q: 0.022877
 96686/100000: episode: 9674, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000227, mae: 0.004998, mean_q: 0.024986
 96696/100000: episode: 9675, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000018, mae: 0.002416, mean_q: 0.023272
 96706/100000: episode: 9676, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000019, mae: 0.003612, mean_q: 0.021127
 96716/100000: episode: 9677, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000209, mae: 0.003841, mean_q: 0.022635
 96726/100000: episode: 9678, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000062, mae: 0.003223, mean_q: 0.023195
 96736/100000: episode: 9679, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000425, mae: 0.004644, mean_q: 0.022416
 96746/100000: episode: 9680, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000063, mae: 0.005563, mean_q: 0.026486
 96756/100000: episode: 9681, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000245, mae: 0.004422, mean_q: 0.023023
 96766/100000: episode: 9682, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000061, mae: 0.003329, mean_q: 0.022731
 96776/100000: episode: 9683, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.300 [1.000, 10.000], loss: 0.000241, mae: 0.003589, mean_q: 0.023035
 96786/100000: episode: 9684, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000240, mae: 0.004630, mean_q: 0.024651
 96796/100000: episode: 9685, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000055, mae: 0.002662, mean_q: 0.023386
 96806/100000: episode: 9686, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000245, mae: 0.003988, mean_q: 0.023060
 96816/100000: episode: 9687, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000382, mae: 0.003553, mean_q: 0.023676
 96826/100000: episode: 9688, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000219, mae: 0.003547, mean_q: 0.024306
[Info] 1-TH LEVEL FOUND: 0.022369548678398132, Considering 100/100 traces
 96836/100000: episode: 9689, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000208, mae: 0.003725, mean_q: 0.023474
[Info] 2-TH LEVEL FOUND: 0.024774057790637016, Considering 100/100 traces
 96846/100000: episode: 9690, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000260, mae: 0.004065, mean_q: 0.023030
[Info] 3-TH LEVEL FOUND: 0.026259377598762512, Considering 100/100 traces
 96856/100000: episode: 9691, duration: 0.727s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000424, mae: 0.006475, mean_q: 0.025750
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.026259377598762512
3
 96866/100000: episode: 9692, duration: 0.494s, episode steps: 10, steps per second: 20, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000013, mae: 0.002292, mean_q: 0.024632
 96876/100000: episode: 9693, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000059, mae: 0.004028, mean_q: 0.021918
 96886/100000: episode: 9694, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000221, mae: 0.003465, mean_q: 0.022987
 96896/100000: episode: 9695, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000245, mae: 0.004948, mean_q: 0.024973
 96906/100000: episode: 9696, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000221, mae: 0.003794, mean_q: 0.024539
 96916/100000: episode: 9697, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000223, mae: 0.003496, mean_q: 0.023261
 96926/100000: episode: 9698, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000018, mae: 0.002191, mean_q: 0.023303
 96936/100000: episode: 9699, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000074, mae: 0.003451, mean_q: 0.022455
 96946/100000: episode: 9700, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000054, mae: 0.002323, mean_q: 0.023557
 96956/100000: episode: 9701, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000427, mae: 0.005426, mean_q: 0.024470
 96966/100000: episode: 9702, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000267, mae: 0.006750, mean_q: 0.026602
 96976/100000: episode: 9703, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000260, mae: 0.004646, mean_q: 0.024972
 96986/100000: episode: 9704, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000410, mae: 0.004808, mean_q: 0.024375
 96996/100000: episode: 9705, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000062, mae: 0.003780, mean_q: 0.025212
Step 97000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.97000.hdf5
 97006/100000: episode: 9706, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000018, mae: 0.002703, mean_q: 0.023018
 97016/100000: episode: 9707, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000023, mae: 0.002784, mean_q: 0.023337
 97026/100000: episode: 9708, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000236, mae: 0.003323, mean_q: 0.023600
 97036/100000: episode: 9709, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000426, mae: 0.005814, mean_q: 0.025321
 97046/100000: episode: 9710, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000241, mae: 0.005685, mean_q: 0.026622
 97056/100000: episode: 9711, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.002592, mean_q: 0.024187
 97066/100000: episode: 9712, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000081, mae: 0.003937, mean_q: 0.023272
 97076/100000: episode: 9713, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000031, mae: 0.001645, mean_q: 0.024467
 97086/100000: episode: 9714, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000038, mae: 0.002599, mean_q: 0.023645
 97096/100000: episode: 9715, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000020, mae: 0.002619, mean_q: 0.023263
 97106/100000: episode: 9716, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000018, mae: 0.003129, mean_q: 0.022508
 97116/100000: episode: 9717, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000057, mae: 0.003617, mean_q: 0.022343
 97126/100000: episode: 9718, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000402, mae: 0.004269, mean_q: 0.024204
 97136/100000: episode: 9719, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000098, mae: 0.004490, mean_q: 0.025178
 97146/100000: episode: 9720, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000019, mae: 0.002143, mean_q: 0.023624
 97156/100000: episode: 9721, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [1.000, 10.000], loss: 0.000467, mae: 0.005981, mean_q: 0.024086
 97166/100000: episode: 9722, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000260, mae: 0.005522, mean_q: 0.025847
 97176/100000: episode: 9723, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000038, mae: 0.002662, mean_q: 0.024836
 97186/100000: episode: 9724, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000232, mae: 0.004838, mean_q: 0.022840
 97196/100000: episode: 9725, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000017, mae: 0.001913, mean_q: 0.024095
 97206/100000: episode: 9726, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000202, mae: 0.002844, mean_q: 0.024199
 97216/100000: episode: 9727, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000075, mae: 0.003129, mean_q: 0.023412
 97226/100000: episode: 9728, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000074, mae: 0.003341, mean_q: 0.024767
 97236/100000: episode: 9729, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000096, mae: 0.003527, mean_q: 0.023734
 97246/100000: episode: 9730, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000203, mae: 0.003342, mean_q: 0.024749
 97256/100000: episode: 9731, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000060, mae: 0.003332, mean_q: 0.023513
 97266/100000: episode: 9732, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000224, mae: 0.004066, mean_q: 0.023071
 97276/100000: episode: 9733, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000032, mae: 0.001764, mean_q: 0.024152
 97286/100000: episode: 9734, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000058, mae: 0.002860, mean_q: 0.023584
 97296/100000: episode: 9735, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000019, mae: 0.001935, mean_q: 0.023987
 97306/100000: episode: 9736, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000248, mae: 0.004275, mean_q: 0.023750
 97316/100000: episode: 9737, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000055, mae: 0.002382, mean_q: 0.023886
 97326/100000: episode: 9738, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [0.000, 10.000], loss: 0.000036, mae: 0.002191, mean_q: 0.023576
 97336/100000: episode: 9739, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000074, mae: 0.002839, mean_q: 0.023752
 97346/100000: episode: 9740, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000054, mae: 0.002554, mean_q: 0.024336
 97356/100000: episode: 9741, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000207, mae: 0.003448, mean_q: 0.024266
 97366/100000: episode: 9742, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000052, mae: 0.002347, mean_q: 0.023715
 97376/100000: episode: 9743, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000261, mae: 0.003968, mean_q: 0.023878
 97386/100000: episode: 9744, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.200 [1.000, 10.000], loss: 0.000036, mae: 0.002882, mean_q: 0.024970
 97396/100000: episode: 9745, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000036, mae: 0.002878, mean_q: 0.022779
 97406/100000: episode: 9746, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000035, mae: 0.002274, mean_q: 0.023990
 97416/100000: episode: 9747, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000038, mae: 0.002926, mean_q: 0.023219
 97426/100000: episode: 9748, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000202, mae: 0.003528, mean_q: 0.022693
 97436/100000: episode: 9749, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.368, mean reward: 0.037 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [1.000, 10.000], loss: 0.000043, mae: 0.002852, mean_q: 0.023715
 97446/100000: episode: 9750, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000243, mae: 0.004081, mean_q: 0.023098
 97456/100000: episode: 9751, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [0.000, 10.000], loss: 0.000214, mae: 0.003053, mean_q: 0.024424
 97466/100000: episode: 9752, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000260, mae: 0.005066, mean_q: 0.025169
 97476/100000: episode: 9753, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000260, mae: 0.003967, mean_q: 0.023439
 97486/100000: episode: 9754, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000037, mae: 0.002690, mean_q: 0.024551
 97496/100000: episode: 9755, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000013, mae: 0.001726, mean_q: 0.023823
 97506/100000: episode: 9756, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [0.000, 10.000], loss: 0.000426, mae: 0.004971, mean_q: 0.023968
 97516/100000: episode: 9757, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000244, mae: 0.005774, mean_q: 0.026285
 97526/100000: episode: 9758, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000016, mae: 0.002470, mean_q: 0.023824
 97536/100000: episode: 9759, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000036, mae: 0.002971, mean_q: 0.022649
 97546/100000: episode: 9760, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000053, mae: 0.002312, mean_q: 0.024102
 97556/100000: episode: 9761, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000055, mae: 0.002987, mean_q: 0.023156
 97566/100000: episode: 9762, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000017, mae: 0.002726, mean_q: 0.022598
 97576/100000: episode: 9763, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000063, mae: 0.003512, mean_q: 0.023015
 97586/100000: episode: 9764, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000078, mae: 0.003488, mean_q: 0.022976
 97596/100000: episode: 9765, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000035, mae: 0.001912, mean_q: 0.023493
 97606/100000: episode: 9766, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000243, mae: 0.004032, mean_q: 0.022905
 97616/100000: episode: 9767, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000019, mae: 0.002188, mean_q: 0.023676
 97626/100000: episode: 9768, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000055, mae: 0.003611, mean_q: 0.021805
 97636/100000: episode: 9769, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000414, mae: 0.005605, mean_q: 0.024224
 97646/100000: episode: 9770, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000384, mae: 0.005441, mean_q: 0.025715
 97656/100000: episode: 9771, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000054, mae: 0.002763, mean_q: 0.023735
 97666/100000: episode: 9772, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000034, mae: 0.002913, mean_q: 0.022318
 97676/100000: episode: 9773, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000076, mae: 0.003447, mean_q: 0.022837
 97686/100000: episode: 9774, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [0.000, 10.000], loss: 0.000258, mae: 0.004610, mean_q: 0.024712
 97696/100000: episode: 9775, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [1.000, 10.000], loss: 0.000042, mae: 0.003458, mean_q: 0.024287
 97706/100000: episode: 9776, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000222, mae: 0.004396, mean_q: 0.021909
 97716/100000: episode: 9777, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000075, mae: 0.003493, mean_q: 0.024351
 97726/100000: episode: 9778, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000205, mae: 0.003434, mean_q: 0.023216
 97736/100000: episode: 9779, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000041, mae: 0.002595, mean_q: 0.023235
 97746/100000: episode: 9780, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000075, mae: 0.003157, mean_q: 0.022903
 97756/100000: episode: 9781, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.100 [1.000, 10.000], loss: 0.000036, mae: 0.002097, mean_q: 0.023345
 97766/100000: episode: 9782, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000317, mae: 0.005701, mean_q: 0.024749
 97776/100000: episode: 9783, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000036, mae: 0.002762, mean_q: 0.023957
 97786/100000: episode: 9784, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000221, mae: 0.004011, mean_q: 0.022046
 97796/100000: episode: 9785, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000058, mae: 0.003272, mean_q: 0.024446
 97806/100000: episode: 9786, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000098, mae: 0.003706, mean_q: 0.023516
 97816/100000: episode: 9787, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000057, mae: 0.002705, mean_q: 0.023645
 97826/100000: episode: 9788, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000099, mae: 0.003980, mean_q: 0.024256
 97836/100000: episode: 9789, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000221, mae: 0.003287, mean_q: 0.023602
 97846/100000: episode: 9790, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000268, mae: 0.005741, mean_q: 0.025247
 97856/100000: episode: 9791, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000218, mae: 0.003804, mean_q: 0.024925
[Info] 1-TH LEVEL FOUND: 0.021921729668974876, Considering 100/100 traces
 97866/100000: episode: 9792, duration: 0.726s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000017, mae: 0.002502, mean_q: 0.022925
[Info] 2-TH LEVEL FOUND: 0.022695565596222878, Considering 100/100 traces
 97876/100000: episode: 9793, duration: 0.712s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000035, mae: 0.003403, mean_q: 0.021817
[Info] 3-TH LEVEL FOUND: 0.024400608614087105, Considering 100/100 traces
 97886/100000: episode: 9794, duration: 0.735s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000082, mae: 0.003838, mean_q: 0.023948
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.024400608614087105
3
 97896/100000: episode: 9795, duration: 0.632s, episode steps: 10, steps per second: 16, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000012, mae: 0.001743, mean_q: 0.023524
 97906/100000: episode: 9796, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [0.000, 10.000], loss: 0.000224, mae: 0.004006, mean_q: 0.022585
 97916/100000: episode: 9797, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000378, mae: 0.004091, mean_q: 0.024723
 97926/100000: episode: 9798, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000038, mae: 0.003319, mean_q: 0.025122
 97936/100000: episode: 9799, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000060, mae: 0.003934, mean_q: 0.022295
 97946/100000: episode: 9800, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000038, mae: 0.002529, mean_q: 0.023081
 97956/100000: episode: 9801, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000068, mae: 0.002025, mean_q: 0.023465
 97966/100000: episode: 9802, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [0.000, 10.000], loss: 0.000059, mae: 0.002939, mean_q: 0.023822
 97976/100000: episode: 9803, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000018, mae: 0.002585, mean_q: 0.022720
 97986/100000: episode: 9804, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000038, mae: 0.003543, mean_q: 0.021785
 97996/100000: episode: 9805, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000015, mae: 0.002853, mean_q: 0.021766
Step 98000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.98000.hdf5
 98006/100000: episode: 9806, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000079, mae: 0.003541, mean_q: 0.022389
 98016/100000: episode: 9807, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000073, mae: 0.003064, mean_q: 0.023774
 98026/100000: episode: 9808, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000053, mae: 0.002210, mean_q: 0.022999
 98036/100000: episode: 9809, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000048, mae: 0.002132, mean_q: 0.023628
 98046/100000: episode: 9810, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000228, mae: 0.004180, mean_q: 0.022455
 98056/100000: episode: 9811, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000034, mae: 0.002059, mean_q: 0.023198
 98066/100000: episode: 9812, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000304, mae: 0.005992, mean_q: 0.024642
 98076/100000: episode: 9813, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000059, mae: 0.003576, mean_q: 0.023947
 98086/100000: episode: 9814, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000077, mae: 0.004057, mean_q: 0.021508
 98096/100000: episode: 9815, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000015, mae: 0.002040, mean_q: 0.023734
 98106/100000: episode: 9816, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000035, mae: 0.002563, mean_q: 0.022269
 98116/100000: episode: 9817, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000258, mae: 0.003884, mean_q: 0.022801
 98126/100000: episode: 9818, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000042, mae: 0.004051, mean_q: 0.025160
 98136/100000: episode: 9819, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000073, mae: 0.003040, mean_q: 0.022597
 98146/100000: episode: 9820, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000057, mae: 0.002656, mean_q: 0.023010
 98156/100000: episode: 9821, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000410, mae: 0.004595, mean_q: 0.023100
 98166/100000: episode: 9822, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000081, mae: 0.004941, mean_q: 0.025448
 98176/100000: episode: 9823, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000219, mae: 0.002969, mean_q: 0.023147
 98186/100000: episode: 9824, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.002565, mean_q: 0.022540
 98196/100000: episode: 9825, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000021, mae: 0.002690, mean_q: 0.022650
 98206/100000: episode: 9826, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000080, mae: 0.003969, mean_q: 0.022162
 98216/100000: episode: 9827, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000040, mae: 0.002537, mean_q: 0.022831
 98226/100000: episode: 9828, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000035, mae: 0.002136, mean_q: 0.022801
 98236/100000: episode: 9829, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000039, mae: 0.003047, mean_q: 0.022059
 98246/100000: episode: 9830, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000036, mae: 0.002700, mean_q: 0.021997
 98256/100000: episode: 9831, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000058, mae: 0.002693, mean_q: 0.022843
 98266/100000: episode: 9832, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [1.000, 10.000], loss: 0.000202, mae: 0.002878, mean_q: 0.022414
 98276/100000: episode: 9833, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000055, mae: 0.002393, mean_q: 0.022945
 98286/100000: episode: 9834, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000035, mae: 0.002435, mean_q: 0.022156
 98296/100000: episode: 9835, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000016, mae: 0.002102, mean_q: 0.022024
 98306/100000: episode: 9836, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000078, mae: 0.003069, mean_q: 0.022592
 98316/100000: episode: 9837, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000412, mae: 0.005477, mean_q: 0.024010
 98326/100000: episode: 9838, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [1.000, 10.000], loss: 0.000044, mae: 0.003455, mean_q: 0.023425
 98336/100000: episode: 9839, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.003251, mean_q: 0.021280
 98346/100000: episode: 9840, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000055, mae: 0.002594, mean_q: 0.022525
 98356/100000: episode: 9841, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000075, mae: 0.002932, mean_q: 0.023035
 98366/100000: episode: 9842, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000052, mae: 0.002021, mean_q: 0.022677
 98376/100000: episode: 9843, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000059, mae: 0.003608, mean_q: 0.021417
 98386/100000: episode: 9844, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000053, mae: 0.002800, mean_q: 0.021583
 98396/100000: episode: 9845, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000093, mae: 0.003892, mean_q: 0.023806
 98406/100000: episode: 9846, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000035, mae: 0.002611, mean_q: 0.022319
 98416/100000: episode: 9847, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000030, mae: 0.002482, mean_q: 0.021168
 98426/100000: episode: 9848, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000267, mae: 0.005013, mean_q: 0.023445
 98436/100000: episode: 9849, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000199, mae: 0.003078, mean_q: 0.023471
 98446/100000: episode: 9850, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000041, mae: 0.003138, mean_q: 0.021646
 98456/100000: episode: 9851, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [1.000, 10.000], loss: 0.000054, mae: 0.002760, mean_q: 0.021824
 98466/100000: episode: 9852, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000038, mae: 0.002680, mean_q: 0.021903
 98476/100000: episode: 9853, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000222, mae: 0.003599, mean_q: 0.021670
 98486/100000: episode: 9854, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000083, mae: 0.003955, mean_q: 0.022883
 98496/100000: episode: 9855, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [0.000, 10.000], loss: 0.000058, mae: 0.002705, mean_q: 0.022044
 98506/100000: episode: 9856, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000016, mae: 0.001962, mean_q: 0.021858
 98516/100000: episode: 9857, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000019, mae: 0.002535, mean_q: 0.021361
 98526/100000: episode: 9858, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000052, mae: 0.002802, mean_q: 0.021246
 98536/100000: episode: 9859, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000264, mae: 0.004200, mean_q: 0.021839
 98546/100000: episode: 9860, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000201, mae: 0.004441, mean_q: 0.024378
 98556/100000: episode: 9861, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000035, mae: 0.002751, mean_q: 0.021327
 98566/100000: episode: 9862, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000037, mae: 0.002968, mean_q: 0.020934
 98576/100000: episode: 9863, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000016, mae: 0.001769, mean_q: 0.022113
 98586/100000: episode: 9864, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [0.000, 10.000], loss: 0.000267, mae: 0.004621, mean_q: 0.021992
 98596/100000: episode: 9865, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000209, mae: 0.004389, mean_q: 0.023443
 98606/100000: episode: 9866, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000016, mae: 0.003190, mean_q: 0.020259
 98616/100000: episode: 9867, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000054, mae: 0.002825, mean_q: 0.021029
 98626/100000: episode: 9868, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000038, mae: 0.002400, mean_q: 0.021983
 98636/100000: episode: 9869, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000039, mae: 0.002977, mean_q: 0.020780
 98646/100000: episode: 9870, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000053, mae: 0.002315, mean_q: 0.022007
 98656/100000: episode: 9871, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000050, mae: 0.002017, mean_q: 0.021725
 98666/100000: episode: 9872, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000064, mae: 0.003416, mean_q: 0.021204
 98676/100000: episode: 9873, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000231, mae: 0.004502, mean_q: 0.020699
 98686/100000: episode: 9874, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000027, mae: 0.001531, mean_q: 0.021897
 98696/100000: episode: 9875, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000054, mae: 0.002356, mean_q: 0.021808
 98706/100000: episode: 9876, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000035, mae: 0.002436, mean_q: 0.020836
 98716/100000: episode: 9877, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000056, mae: 0.002637, mean_q: 0.021177
 98726/100000: episode: 9878, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [0.000, 10.000], loss: 0.000455, mae: 0.006167, mean_q: 0.022641
 98736/100000: episode: 9879, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [0.000, 10.000], loss: 0.000018, mae: 0.002493, mean_q: 0.022231
 98746/100000: episode: 9880, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000052, mae: 0.002514, mean_q: 0.020802
 98756/100000: episode: 9881, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [0.000, 10.000], loss: 0.000015, mae: 0.001797, mean_q: 0.022105
 98766/100000: episode: 9882, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000407, mae: 0.005103, mean_q: 0.023019
 98776/100000: episode: 9883, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000219, mae: 0.004351, mean_q: 0.023648
 98786/100000: episode: 9884, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [1.000, 10.000], loss: 0.000203, mae: 0.002948, mean_q: 0.021710
 98796/100000: episode: 9885, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [0.000, 10.000], loss: 0.000058, mae: 0.003658, mean_q: 0.020624
 98806/100000: episode: 9886, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000034, mae: 0.002224, mean_q: 0.021281
 98816/100000: episode: 9887, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000054, mae: 0.002197, mean_q: 0.021693
 98826/100000: episode: 9888, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000053, mae: 0.002326, mean_q: 0.021423
 98836/100000: episode: 9889, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000017, mae: 0.001808, mean_q: 0.021792
 98846/100000: episode: 9890, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000052, mae: 0.002734, mean_q: 0.020690
 98856/100000: episode: 9891, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000073, mae: 0.002655, mean_q: 0.021732
 98866/100000: episode: 9892, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [0.000, 10.000], loss: 0.000038, mae: 0.002598, mean_q: 0.022183
 98876/100000: episode: 9893, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000013, mae: 0.002417, mean_q: 0.020198
 98886/100000: episode: 9894, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [0.000, 10.000], loss: 0.000034, mae: 0.002151, mean_q: 0.021530
[Info] 1-TH LEVEL FOUND: 0.021794402971863747, Considering 100/100 traces
 98896/100000: episode: 9895, duration: 0.745s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [0.000, 10.000], loss: 0.000221, mae: 0.003167, mean_q: 0.020877
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.021794402971863747
1
 98906/100000: episode: 9896, duration: 0.517s, episode steps: 10, steps per second: 19, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000017, mae: 0.002029, mean_q: 0.021942
 98916/100000: episode: 9897, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000248, mae: 0.004162, mean_q: 0.020874
 98926/100000: episode: 9898, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000079, mae: 0.003342, mean_q: 0.022004
 98936/100000: episode: 9899, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000076, mae: 0.003569, mean_q: 0.022625
 98946/100000: episode: 9900, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000201, mae: 0.002900, mean_q: 0.021150
 98956/100000: episode: 9901, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000053, mae: 0.002460, mean_q: 0.021606
 98966/100000: episode: 9902, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [1.000, 10.000], loss: 0.000231, mae: 0.004051, mean_q: 0.021625
 98976/100000: episode: 9903, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000017, mae: 0.001988, mean_q: 0.021185
 98986/100000: episode: 9904, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [1.000, 10.000], loss: 0.000059, mae: 0.002863, mean_q: 0.021108
 98996/100000: episode: 9905, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000242, mae: 0.003967, mean_q: 0.022435
Step 99000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.99000.hdf5
 99006/100000: episode: 9906, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000031, mae: 0.001973, mean_q: 0.021780
 99016/100000: episode: 9907, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000244, mae: 0.003832, mean_q: 0.021150
 99026/100000: episode: 9908, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000054, mae: 0.003519, mean_q: 0.023221
 99036/100000: episode: 9909, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000029, mae: 0.001795, mean_q: 0.021488
 99046/100000: episode: 9910, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000054, mae: 0.002742, mean_q: 0.020910
 99056/100000: episode: 9911, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000017, mae: 0.002102, mean_q: 0.021022
 99066/100000: episode: 9912, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [1.000, 10.000], loss: 0.000062, mae: 0.003057, mean_q: 0.021252
 99076/100000: episode: 9913, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [0.000, 10.000], loss: 0.000215, mae: 0.002412, mean_q: 0.021167
 99086/100000: episode: 9914, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000031, mae: 0.002221, mean_q: 0.022319
 99096/100000: episode: 9915, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [1.000, 10.000], loss: 0.000037, mae: 0.002306, mean_q: 0.021423
 99106/100000: episode: 9916, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000043, mae: 0.003583, mean_q: 0.020007
 99116/100000: episode: 9917, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [0.000, 10.000], loss: 0.000037, mae: 0.002254, mean_q: 0.021422
 99126/100000: episode: 9918, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000014, mae: 0.001916, mean_q: 0.020822
 99136/100000: episode: 9919, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000056, mae: 0.002843, mean_q: 0.020487
 99146/100000: episode: 9920, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000245, mae: 0.004422, mean_q: 0.022317
 99156/100000: episode: 9921, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [1.000, 10.000], loss: 0.000071, mae: 0.003057, mean_q: 0.022374
 99166/100000: episode: 9922, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000453, mae: 0.005354, mean_q: 0.021897
 99176/100000: episode: 9923, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [1.000, 10.000], loss: 0.000395, mae: 0.006475, mean_q: 0.024205
 99186/100000: episode: 9924, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000064, mae: 0.003454, mean_q: 0.021480
 99196/100000: episode: 9925, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000225, mae: 0.003419, mean_q: 0.021387
 99206/100000: episode: 9926, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000018, mae: 0.002264, mean_q: 0.022092
 99216/100000: episode: 9927, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000014, mae: 0.002661, mean_q: 0.020196
 99226/100000: episode: 9928, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000013, mae: 0.001659, mean_q: 0.021123
 99236/100000: episode: 9929, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [0.000, 10.000], loss: 0.000015, mae: 0.002029, mean_q: 0.020974
 99246/100000: episode: 9930, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000071, mae: 0.002829, mean_q: 0.020709
 99256/100000: episode: 9931, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000060, mae: 0.002999, mean_q: 0.020983
 99266/100000: episode: 9932, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000202, mae: 0.002901, mean_q: 0.020823
 99276/100000: episode: 9933, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000011, mae: 0.001350, mean_q: 0.021604
 99286/100000: episode: 9934, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000233, mae: 0.004212, mean_q: 0.021871
 99296/100000: episode: 9935, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000017, mae: 0.002096, mean_q: 0.021709
 99306/100000: episode: 9936, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000037, mae: 0.002476, mean_q: 0.020721
 99316/100000: episode: 9937, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [1.000, 10.000], loss: 0.000082, mae: 0.003219, mean_q: 0.021423
 99326/100000: episode: 9938, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000037, mae: 0.002185, mean_q: 0.021526
 99336/100000: episode: 9939, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000034, mae: 0.002317, mean_q: 0.020541
 99346/100000: episode: 9940, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.150 [1.000, 10.000], loss: 0.000038, mae: 0.002341, mean_q: 0.021024
 99356/100000: episode: 9941, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [0.000, 10.000], loss: 0.000034, mae: 0.002218, mean_q: 0.020763
 99366/100000: episode: 9942, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000036, mae: 0.002406, mean_q: 0.020621
 99376/100000: episode: 9943, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000019, mae: 0.002462, mean_q: 0.021849
 99386/100000: episode: 9944, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000014, mae: 0.002234, mean_q: 0.020349
 99396/100000: episode: 9945, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000019, mae: 0.002801, mean_q: 0.019806
 99406/100000: episode: 9946, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000018, mae: 0.002180, mean_q: 0.021620
 99416/100000: episode: 9947, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000016, mae: 0.002256, mean_q: 0.020763
 99426/100000: episode: 9948, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.050 [1.000, 10.000], loss: 0.000038, mae: 0.003418, mean_q: 0.019308
 99436/100000: episode: 9949, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000059, mae: 0.002746, mean_q: 0.020834
 99446/100000: episode: 9950, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000254, mae: 0.004743, mean_q: 0.021501
 99456/100000: episode: 9951, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000012, mae: 0.001908, mean_q: 0.021374
 99466/100000: episode: 9952, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000040, mae: 0.002518, mean_q: 0.020490
 99476/100000: episode: 9953, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000209, mae: 0.004105, mean_q: 0.022141
 99486/100000: episode: 9954, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [0.000, 10.000], loss: 0.000027, mae: 0.001907, mean_q: 0.020104
 99496/100000: episode: 9955, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [0.000, 10.000], loss: 0.000029, mae: 0.001746, mean_q: 0.020235
 99506/100000: episode: 9956, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000031, mae: 0.001722, mean_q: 0.021165
 99516/100000: episode: 9957, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [1.000, 10.000], loss: 0.000059, mae: 0.003447, mean_q: 0.019902
 99526/100000: episode: 9958, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [1.000, 10.000], loss: 0.000034, mae: 0.002520, mean_q: 0.019800
 99536/100000: episode: 9959, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [0.000, 10.000], loss: 0.000201, mae: 0.002595, mean_q: 0.020291
 99546/100000: episode: 9960, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [0.000, 10.000], loss: 0.000014, mae: 0.001992, mean_q: 0.021310
 99556/100000: episode: 9961, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000056, mae: 0.003024, mean_q: 0.019711
 99566/100000: episode: 9962, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000037, mae: 0.002332, mean_q: 0.021077
 99576/100000: episode: 9963, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000014, mae: 0.002174, mean_q: 0.020023
 99586/100000: episode: 9964, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [0.000, 10.000], loss: 0.000036, mae: 0.002774, mean_q: 0.019451
 99596/100000: episode: 9965, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [0.000, 10.000], loss: 0.000012, mae: 0.001551, mean_q: 0.020714
 99606/100000: episode: 9966, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [0.000, 10.000], loss: 0.000037, mae: 0.002675, mean_q: 0.019388
 99616/100000: episode: 9967, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [0.000, 10.000], loss: 0.000038, mae: 0.002231, mean_q: 0.020537
 99626/100000: episode: 9968, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000263, mae: 0.003991, mean_q: 0.021022
 99636/100000: episode: 9969, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [1.000, 10.000], loss: 0.000011, mae: 0.002056, mean_q: 0.021088
 99646/100000: episode: 9970, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.135, mean reward: 0.014 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [1.000, 10.000], loss: 0.000037, mae: 0.003191, mean_q: 0.018858
 99656/100000: episode: 9971, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [0.000, 10.000], loss: 0.000225, mae: 0.004604, mean_q: 0.022259
 99666/100000: episode: 9972, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.950 [1.000, 10.000], loss: 0.000034, mae: 0.002763, mean_q: 0.020279
 99676/100000: episode: 9973, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [1.000, 10.000], loss: 0.000012, mae: 0.002824, mean_q: 0.018651
 99686/100000: episode: 9974, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000059, mae: 0.003003, mean_q: 0.020634
 99696/100000: episode: 9975, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [0.000, 10.000], loss: 0.000063, mae: 0.002903, mean_q: 0.020592
 99706/100000: episode: 9976, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000015, mae: 0.001862, mean_q: 0.019806
 99716/100000: episode: 9977, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [0.000, 10.000], loss: 0.000247, mae: 0.003987, mean_q: 0.020595
 99726/100000: episode: 9978, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [1.000, 10.000], loss: 0.000054, mae: 0.002799, mean_q: 0.020897
 99736/100000: episode: 9979, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000219, mae: 0.003133, mean_q: 0.019611
 99746/100000: episode: 9980, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [0.000, 10.000], loss: 0.000012, mae: 0.001888, mean_q: 0.020882
 99756/100000: episode: 9981, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [0.000, 10.000], loss: 0.000011, mae: 0.001632, mean_q: 0.019722
 99766/100000: episode: 9982, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [1.000, 10.000], loss: 0.000062, mae: 0.003042, mean_q: 0.020463
 99776/100000: episode: 9983, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000016, mae: 0.002145, mean_q: 0.020679
 99786/100000: episode: 9984, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [1.000, 10.000], loss: 0.000054, mae: 0.002351, mean_q: 0.019852
 99796/100000: episode: 9985, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [0.000, 10.000], loss: 0.000019, mae: 0.002292, mean_q: 0.020311
 99806/100000: episode: 9986, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000035, mae: 0.002191, mean_q: 0.019753
 99816/100000: episode: 9987, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [0.000, 10.000], loss: 0.000035, mae: 0.001876, mean_q: 0.020156
 99826/100000: episode: 9988, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [0.000, 10.000], loss: 0.000071, mae: 0.002327, mean_q: 0.019967
 99836/100000: episode: 9989, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000204, mae: 0.002748, mean_q: 0.020433
 99846/100000: episode: 9990, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [1.000, 10.000], loss: 0.000033, mae: 0.001913, mean_q: 0.020293
 99856/100000: episode: 9991, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000058, mae: 0.002792, mean_q: 0.020597
 99866/100000: episode: 9992, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [0.000, 10.000], loss: 0.000015, mae: 0.002040, mean_q: 0.019791
 99876/100000: episode: 9993, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [0.000, 10.000], loss: 0.000051, mae: 0.002257, mean_q: 0.019561
 99886/100000: episode: 9994, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000031, mae: 0.001886, mean_q: 0.019805
 99896/100000: episode: 9995, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000032, mae: 0.001917, mean_q: 0.019663
[Info] 1-TH LEVEL FOUND: 0.019152311608195305, Considering 100/100 traces
 99906/100000: episode: 9996, duration: 0.742s, episode steps: 10, steps per second: 13, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [1.000, 10.000], loss: 0.000036, mae: 0.002880, mean_q: 0.018888
[Info] 2-TH LEVEL FOUND: 0.01990358717739582, Considering 100/100 traces
 99916/100000: episode: 9997, duration: 0.682s, episode steps: 10, steps per second: 15, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [1.000, 10.000], loss: 0.000060, mae: 0.002948, mean_q: 0.019413
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.01990358717739582
2
 99926/100000: episode: 9998, duration: 0.497s, episode steps: 10, steps per second: 20, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [1.000, 10.000], loss: 0.000034, mae: 0.002111, mean_q: 0.019647
 99936/100000: episode: 9999, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [1.000, 10.000], loss: 0.000036, mae: 0.002532, mean_q: 0.019168
 99946/100000: episode: 10000, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [1.000, 10.000], loss: 0.000009, mae: 0.001153, mean_q: 0.019570
 99956/100000: episode: 10001, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [1.000, 10.000], loss: 0.000078, mae: 0.003048, mean_q: 0.020222
 99966/100000: episode: 10002, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [1.000, 10.000], loss: 0.000080, mae: 0.002993, mean_q: 0.020063
 99976/100000: episode: 10003, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [0.000, 10.000], loss: 0.000222, mae: 0.003566, mean_q: 0.020714
 99986/100000: episode: 10004, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [1.000, 10.000], loss: 0.000015, mae: 0.001934, mean_q: 0.019327
 99996/100000: episode: 10005, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [0.000, 10.000], loss: 0.000012, mae: 0.001614, mean_q: 0.019290
Step 100000: saving model to out/succruns/mer27nov2019_12_59_41_CET/models/weights.100000.hdf5
done, took 635.379 seconds
[Info] End Importance Splitting. Falsification occurred 11 times.
