Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                48        
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting on succruns-v1.
Training for 10000 steps ...
   10/10000: episode: 1, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   20/10000: episode: 2, duration: 0.006s, episode steps: 10, steps per second: 1813, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   30/10000: episode: 3, duration: 0.005s, episode steps: 10, steps per second: 1857, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   40/10000: episode: 4, duration: 0.006s, episode steps: 10, steps per second: 1728, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   50/10000: episode: 5, duration: 0.006s, episode steps: 10, steps per second: 1794, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   60/10000: episode: 6, duration: 0.005s, episode steps: 10, steps per second: 1915, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   70/10000: episode: 7, duration: 0.005s, episode steps: 10, steps per second: 1899, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   80/10000: episode: 8, duration: 0.005s, episode steps: 10, steps per second: 1828, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
   90/10000: episode: 9, duration: 0.006s, episode steps: 10, steps per second: 1787, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  100/10000: episode: 10, duration: 0.006s, episode steps: 10, steps per second: 1668, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  110/10000: episode: 11, duration: 0.006s, episode steps: 10, steps per second: 1709, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  120/10000: episode: 12, duration: 0.006s, episode steps: 10, steps per second: 1712, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  130/10000: episode: 13, duration: 0.006s, episode steps: 10, steps per second: 1795, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  140/10000: episode: 14, duration: 0.006s, episode steps: 10, steps per second: 1669, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  150/10000: episode: 15, duration: 0.006s, episode steps: 10, steps per second: 1567, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  160/10000: episode: 16, duration: 0.006s, episode steps: 10, steps per second: 1572, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  170/10000: episode: 17, duration: 0.009s, episode steps: 10, steps per second: 1058, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  180/10000: episode: 18, duration: 0.006s, episode steps: 10, steps per second: 1588, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  190/10000: episode: 19, duration: 0.011s, episode steps: 10, steps per second: 894, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  200/10000: episode: 20, duration: 0.010s, episode steps: 10, steps per second: 1042, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  210/10000: episode: 21, duration: 0.006s, episode steps: 10, steps per second: 1703, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  220/10000: episode: 22, duration: 0.006s, episode steps: 10, steps per second: 1780, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  230/10000: episode: 23, duration: 0.006s, episode steps: 10, steps per second: 1783, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  240/10000: episode: 24, duration: 0.013s, episode steps: 10, steps per second: 788, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  250/10000: episode: 25, duration: 0.007s, episode steps: 10, steps per second: 1436, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  260/10000: episode: 26, duration: 0.006s, episode steps: 10, steps per second: 1739, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  270/10000: episode: 27, duration: 0.012s, episode steps: 10, steps per second: 818, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  280/10000: episode: 28, duration: 0.012s, episode steps: 10, steps per second: 819, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  290/10000: episode: 29, duration: 0.009s, episode steps: 10, steps per second: 1080, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  300/10000: episode: 30, duration: 0.017s, episode steps: 10, steps per second: 583, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  310/10000: episode: 31, duration: 0.013s, episode steps: 10, steps per second: 791, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  320/10000: episode: 32, duration: 0.011s, episode steps: 10, steps per second: 911, episode reward: 0.029, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  330/10000: episode: 33, duration: 0.008s, episode steps: 10, steps per second: 1246, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  340/10000: episode: 34, duration: 0.007s, episode steps: 10, steps per second: 1394, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  350/10000: episode: 35, duration: 0.007s, episode steps: 10, steps per second: 1410, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  360/10000: episode: 36, duration: 0.007s, episode steps: 10, steps per second: 1373, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  370/10000: episode: 37, duration: 0.010s, episode steps: 10, steps per second: 970, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  380/10000: episode: 38, duration: 0.007s, episode steps: 10, steps per second: 1399, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  390/10000: episode: 39, duration: 0.007s, episode steps: 10, steps per second: 1375, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  400/10000: episode: 40, duration: 0.007s, episode steps: 10, steps per second: 1397, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  410/10000: episode: 41, duration: 0.009s, episode steps: 10, steps per second: 1146, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  420/10000: episode: 42, duration: 0.006s, episode steps: 10, steps per second: 1702, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  430/10000: episode: 43, duration: 0.007s, episode steps: 10, steps per second: 1347, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  440/10000: episode: 44, duration: 0.007s, episode steps: 10, steps per second: 1417, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  450/10000: episode: 45, duration: 0.005s, episode steps: 10, steps per second: 1982, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  460/10000: episode: 46, duration: 0.005s, episode steps: 10, steps per second: 2010, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  470/10000: episode: 47, duration: 0.005s, episode steps: 10, steps per second: 1981, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  480/10000: episode: 48, duration: 0.005s, episode steps: 10, steps per second: 2017, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  490/10000: episode: 49, duration: 0.005s, episode steps: 10, steps per second: 2095, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  500/10000: episode: 50, duration: 0.005s, episode steps: 10, steps per second: 2054, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
  510/10000: episode: 51, duration: 0.669s, episode steps: 10, steps per second: 15, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.403574, mae: 0.589041, mean_q: -1.223457
  520/10000: episode: 52, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.155270, mae: 0.315539, mean_q: -0.873703
  530/10000: episode: 53, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.080481, mae: 0.258673, mean_q: -0.548414
  540/10000: episode: 54, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.088347, mae: 0.303960, mean_q: -0.456272
  550/10000: episode: 55, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.070334, mae: 0.271101, mean_q: -0.480054
  560/10000: episode: 56, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.053092, mae: 0.237302, mean_q: -0.394963
  570/10000: episode: 57, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.042287, mae: 0.202956, mean_q: -0.377780
  580/10000: episode: 58, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.044542, mae: 0.200084, mean_q: -0.414336
  590/10000: episode: 59, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.038684, mae: 0.186363, mean_q: -0.375304
  600/10000: episode: 60, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.036410, mae: 0.171488, mean_q: -0.375224
  610/10000: episode: 61, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [-1.000, 10.000], loss: 0.036937, mae: 0.186389, mean_q: -0.326045
  620/10000: episode: 62, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.029142, mae: 0.169746, mean_q: -0.318853
  630/10000: episode: 63, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.025431, mae: 0.150912, mean_q: -0.284246
  640/10000: episode: 64, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.022342, mae: 0.147587, mean_q: -0.257703
  650/10000: episode: 65, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.018585, mae: 0.129371, mean_q: -0.256925
  660/10000: episode: 66, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.017113, mae: 0.125344, mean_q: -0.239249
  670/10000: episode: 67, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.013788, mae: 0.125772, mean_q: -0.218832
  680/10000: episode: 68, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.014177, mae: 0.114196, mean_q: -0.220654
  690/10000: episode: 69, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.010241, mae: 0.101534, mean_q: -0.198202
  700/10000: episode: 70, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.007357, mae: 0.088020, mean_q: -0.195332
  710/10000: episode: 71, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.007680, mae: 0.078251, mean_q: -0.213200
  720/10000: episode: 72, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.008332, mae: 0.103190, mean_q: -0.155978
  730/10000: episode: 73, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.005945, mae: 0.069960, mean_q: -0.210121
  740/10000: episode: 74, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.004899, mae: 0.071227, mean_q: -0.169873
  750/10000: episode: 75, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.004717, mae: 0.068176, mean_q: -0.174688
  760/10000: episode: 76, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.005202, mae: 0.082519, mean_q: -0.144925
  770/10000: episode: 77, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003878, mae: 0.057872, mean_q: -0.171642
  780/10000: episode: 78, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003459, mae: 0.066274, mean_q: -0.139557
  790/10000: episode: 79, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.002806, mae: 0.058255, mean_q: -0.152787
  800/10000: episode: 80, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002124, mae: 0.052278, mean_q: -0.132011
  810/10000: episode: 81, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001858, mae: 0.047407, mean_q: -0.132457
  820/10000: episode: 82, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001678, mae: 0.047412, mean_q: -0.111953
  830/10000: episode: 83, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001641, mae: 0.043377, mean_q: -0.124346
  840/10000: episode: 84, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001253, mae: 0.040957, mean_q: -0.110676
  850/10000: episode: 85, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001148, mae: 0.038669, mean_q: -0.103271
  860/10000: episode: 86, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001184, mae: 0.038977, mean_q: -0.094258
  870/10000: episode: 87, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000855, mae: 0.035002, mean_q: -0.091911
  880/10000: episode: 88, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.100, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000719, mae: 0.031406, mean_q: -0.086091
  890/10000: episode: 89, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000565, mae: 0.028240, mean_q: -0.084543
  900/10000: episode: 90, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000610, mae: 0.029119, mean_q: -0.074612
  910/10000: episode: 91, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000550, mae: 0.026947, mean_q: -0.074755
  920/10000: episode: 92, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.035, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000449, mae: 0.025355, mean_q: -0.066431
  930/10000: episode: 93, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000355, mae: 0.022685, mean_q: -0.064183
  940/10000: episode: 94, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000411, mae: 0.022866, mean_q: -0.062266
  950/10000: episode: 95, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000294, mae: 0.020215, mean_q: -0.054531
  960/10000: episode: 96, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000296, mae: 0.019123, mean_q: -0.054769
  970/10000: episode: 97, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000250, mae: 0.018322, mean_q: -0.051866
  980/10000: episode: 98, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000201, mae: 0.016223, mean_q: -0.050272
  990/10000: episode: 99, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000174, mae: 0.014860, mean_q: -0.046676
 1000/10000: episode: 100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000187, mae: 0.015609, mean_q: -0.041639
 1010/10000: episode: 101, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000146, mae: 0.013448, mean_q: -0.042334
 1020/10000: episode: 102, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000151, mae: 0.013061, mean_q: -0.037241
 1030/10000: episode: 103, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000132, mae: 0.012559, mean_q: -0.036136
 1040/10000: episode: 104, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000113, mae: 0.011888, mean_q: -0.033544
 1050/10000: episode: 105, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000111, mae: 0.010914, mean_q: -0.031937
 1060/10000: episode: 106, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000102, mae: 0.010379, mean_q: -0.027729
 1070/10000: episode: 107, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000099, mae: 0.010497, mean_q: -0.029260
 1080/10000: episode: 108, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000090, mae: 0.009983, mean_q: -0.026921
 1090/10000: episode: 109, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000116, mae: 0.009501, mean_q: -0.023826
 1100/10000: episode: 110, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000070, mae: 0.008554, mean_q: -0.022598
 1110/10000: episode: 111, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000074, mae: 0.008551, mean_q: -0.019481
 1120/10000: episode: 112, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000081, mae: 0.008211, mean_q: -0.020271
 1130/10000: episode: 113, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000055, mae: 0.007106, mean_q: -0.020249
 1140/10000: episode: 114, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000046, mae: 0.006700, mean_q: -0.016685
 1150/10000: episode: 115, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000057, mae: 0.006959, mean_q: -0.016687
 1160/10000: episode: 116, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000059, mae: 0.007364, mean_q: -0.016458
 1170/10000: episode: 117, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000069, mae: 0.006547, mean_q: -0.012466
 1180/10000: episode: 118, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000081, mae: 0.007433, mean_q: -0.012751
 1190/10000: episode: 119, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000065, mae: 0.006899, mean_q: -0.010922
 1200/10000: episode: 120, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000070, mae: 0.007362, mean_q: -0.012478
 1210/10000: episode: 121, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000035, mae: 0.006268, mean_q: -0.010256
 1220/10000: episode: 122, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000055, mae: 0.005931, mean_q: -0.007359
 1230/10000: episode: 123, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.016, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000061, mae: 0.006319, mean_q: -0.009043
 1240/10000: episode: 124, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000044, mae: 0.006264, mean_q: -0.007637
 1250/10000: episode: 125, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000041, mae: 0.005890, mean_q: -0.005949
 1260/10000: episode: 126, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000067, mae: 0.007007, mean_q: -0.006252
 1270/10000: episode: 127, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000038, mae: 0.005554, mean_q: -0.004977
 1280/10000: episode: 128, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000035, mae: 0.005396, mean_q: -0.005093
 1290/10000: episode: 129, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000035, mae: 0.005323, mean_q: -0.003728
 1300/10000: episode: 130, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000035, mae: 0.005234, mean_q: -0.005089
 1310/10000: episode: 131, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000034, mae: 0.005043, mean_q: -0.003067
 1320/10000: episode: 132, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000028, mae: 0.004793, mean_q: -0.003726
 1330/10000: episode: 133, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000033, mae: 0.004618, mean_q: -0.003919
 1340/10000: episode: 134, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000031, mae: 0.004864, mean_q: -0.001739
 1350/10000: episode: 135, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000031, mae: 0.004780, mean_q: -0.002477
 1360/10000: episode: 136, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000055, mae: 0.005171, mean_q: -0.001306
 1370/10000: episode: 137, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000034, mae: 0.004925, mean_q: -0.001904
 1380/10000: episode: 138, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000035, mae: 0.004789, mean_q: 0.000014
 1390/10000: episode: 139, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000050, mae: 0.004858, mean_q: -0.000287
 1400/10000: episode: 140, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000030, mae: 0.004368, mean_q: -0.000329
 1410/10000: episode: 141, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000033, mae: 0.004920, mean_q: -0.000448
 1420/10000: episode: 142, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000034, mae: 0.005199, mean_q: 0.001315
 1430/10000: episode: 143, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000022, mae: 0.003774, mean_q: -0.001069
 1440/10000: episode: 144, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000106, mae: 0.006716, mean_q: 0.002712
 1450/10000: episode: 145, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000073, mae: 0.006197, mean_q: 0.001923
 1460/10000: episode: 146, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000037, mae: 0.005815, mean_q: -0.001366
 1470/10000: episode: 147, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000039, mae: 0.005166, mean_q: 0.001114
 1480/10000: episode: 148, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000033, mae: 0.005214, mean_q: 0.000562
 1490/10000: episode: 149, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000097, mae: 0.007120, mean_q: 0.004607
[Info] 1-TH LEVEL FOUND: 0.1696232110261917, Considering 100/100 traces
 1500/10000: episode: 150, duration: 1.125s, episode steps: 10, steps per second: 9, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000064, mae: 0.005902, mean_q: 0.002538
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.1696232110261917
 1501/10000: episode: 151, duration: 1.027s, episode steps: 1, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000037, mae: 0.004728, mean_q: 0.002269
 1511/10000: episode: 152, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000060, mae: 0.005856, mean_q: 0.001904
 1521/10000: episode: 153, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000061, mae: 0.005565, mean_q: 0.000279
 1531/10000: episode: 154, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000062, mae: 0.005468, mean_q: 0.002602
 1541/10000: episode: 155, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000031, mae: 0.004235, mean_q: 0.003005
 1551/10000: episode: 156, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000083, mae: 0.005295, mean_q: 0.003385
 1561/10000: episode: 157, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000069, mae: 0.006263, mean_q: 0.004524
 1571/10000: episode: 158, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000035, mae: 0.005745, mean_q: 0.002355
 1581/10000: episode: 159, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000060, mae: 0.005451, mean_q: 0.002384
 1591/10000: episode: 160, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000071, mae: 0.005084, mean_q: 0.003198
 1601/10000: episode: 161, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000100, mae: 0.005076, mean_q: 0.003083
 1611/10000: episode: 162, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000050, mae: 0.006398, mean_q: 0.002340
 1621/10000: episode: 163, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000071, mae: 0.005211, mean_q: 0.005342
 1631/10000: episode: 164, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000035, mae: 0.004700, mean_q: 0.001748
 1641/10000: episode: 165, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000032, mae: 0.005006, mean_q: 0.002524
 1651/10000: episode: 166, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000066, mae: 0.005123, mean_q: 0.003387
 1661/10000: episode: 167, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000030, mae: 0.004321, mean_q: 0.002843
 1671/10000: episode: 168, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000098, mae: 0.005632, mean_q: 0.005163
 1681/10000: episode: 169, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000046, mae: 0.006075, mean_q: 0.000923
 1691/10000: episode: 170, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000058, mae: 0.006020, mean_q: 0.003371
 1701/10000: episode: 171, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000045, mae: 0.006018, mean_q: 0.002784
 1711/10000: episode: 172, duration: 0.084s, episode steps: 10, steps per second: 120, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000049, mae: 0.004579, mean_q: 0.003806
 1721/10000: episode: 173, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000026, mae: 0.003932, mean_q: 0.002447
 1731/10000: episode: 174, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000027, mae: 0.004475, mean_q: 0.002389
 1741/10000: episode: 175, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000077, mae: 0.005748, mean_q: 0.003998
 1751/10000: episode: 176, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000037, mae: 0.005688, mean_q: 0.004106
 1761/10000: episode: 177, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000030, mae: 0.004358, mean_q: 0.003844
 1771/10000: episode: 178, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000025, mae: 0.004338, mean_q: 0.003768
 1781/10000: episode: 179, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.221, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.000075, mae: 0.005749, mean_q: 0.005186
 1791/10000: episode: 180, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000068, mae: 0.006701, mean_q: 0.003602
 1801/10000: episode: 181, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000044, mae: 0.005445, mean_q: 0.004136
 1811/10000: episode: 182, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000032, mae: 0.004761, mean_q: 0.003982
 1821/10000: episode: 183, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000059, mae: 0.005177, mean_q: 0.004639
 1831/10000: episode: 184, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000067, mae: 0.006084, mean_q: 0.005445
 1841/10000: episode: 185, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000055, mae: 0.006257, mean_q: 0.004432
 1851/10000: episode: 186, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000051, mae: 0.004654, mean_q: 0.003584
 1861/10000: episode: 187, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000046, mae: 0.005951, mean_q: 0.004565
 1871/10000: episode: 188, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000053, mae: 0.004650, mean_q: 0.003007
 1881/10000: episode: 189, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000047, mae: 0.006086, mean_q: 0.002587
 1891/10000: episode: 190, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000044, mae: 0.005709, mean_q: 0.004911
 1901/10000: episode: 191, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000064, mae: 0.006332, mean_q: 0.006211
 1911/10000: episode: 192, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000025, mae: 0.004350, mean_q: 0.003825
 1921/10000: episode: 193, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000070, mae: 0.005798, mean_q: 0.003253
 1931/10000: episode: 194, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000034, mae: 0.005092, mean_q: 0.004548
 1941/10000: episode: 195, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000066, mae: 0.005271, mean_q: 0.004884
 1951/10000: episode: 196, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000058, mae: 0.004663, mean_q: 0.005719
 1961/10000: episode: 197, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000036, mae: 0.004909, mean_q: 0.003388
 1971/10000: episode: 198, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000101, mae: 0.006572, mean_q: 0.005305
 1981/10000: episode: 199, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000034, mae: 0.005427, mean_q: 0.004552
 1991/10000: episode: 200, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000047, mae: 0.005677, mean_q: 0.004137
 2001/10000: episode: 201, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000052, mae: 0.005294, mean_q: 0.005898
 2011/10000: episode: 202, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000086, mae: 0.006178, mean_q: 0.004415
 2021/10000: episode: 203, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000075, mae: 0.005598, mean_q: 0.004393
 2031/10000: episode: 204, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000042, mae: 0.005611, mean_q: 0.002855
 2041/10000: episode: 205, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000081, mae: 0.005377, mean_q: 0.004838
 2051/10000: episode: 206, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000059, mae: 0.005839, mean_q: 0.004930
 2061/10000: episode: 207, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000034, mae: 0.004722, mean_q: 0.004189
 2071/10000: episode: 208, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000071, mae: 0.005527, mean_q: 0.003981
 2081/10000: episode: 209, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000059, mae: 0.004850, mean_q: 0.005690
 2091/10000: episode: 210, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000063, mae: 0.005428, mean_q: 0.004357
 2101/10000: episode: 211, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000097, mae: 0.006382, mean_q: 0.004614
 2111/10000: episode: 212, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000073, mae: 0.006311, mean_q: 0.004289
 2121/10000: episode: 213, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000097, mae: 0.006681, mean_q: 0.005277
 2131/10000: episode: 214, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000067, mae: 0.006563, mean_q: 0.004540
 2141/10000: episode: 215, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000039, mae: 0.005040, mean_q: 0.005230
 2151/10000: episode: 216, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000033, mae: 0.004822, mean_q: 0.004891
 2161/10000: episode: 217, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000057, mae: 0.005218, mean_q: 0.004839
 2171/10000: episode: 218, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000033, mae: 0.004879, mean_q: 0.003629
 2181/10000: episode: 219, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000034, mae: 0.005757, mean_q: 0.002511
 2191/10000: episode: 220, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000046, mae: 0.005671, mean_q: 0.003537
 2201/10000: episode: 221, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000054, mae: 0.004726, mean_q: 0.005324
 2211/10000: episode: 222, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000062, mae: 0.005209, mean_q: 0.003598
 2221/10000: episode: 223, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000038, mae: 0.005198, mean_q: 0.004069
 2231/10000: episode: 224, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000039, mae: 0.005268, mean_q: 0.005173
 2241/10000: episode: 225, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000066, mae: 0.005438, mean_q: 0.004571
 2251/10000: episode: 226, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000089, mae: 0.005518, mean_q: 0.005043
 2261/10000: episode: 227, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000037, mae: 0.006006, mean_q: 0.002675
 2271/10000: episode: 228, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000030, mae: 0.004824, mean_q: 0.003224
 2281/10000: episode: 229, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000054, mae: 0.005262, mean_q: 0.002896
 2291/10000: episode: 230, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000037, mae: 0.005817, mean_q: 0.003513
 2301/10000: episode: 231, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000068, mae: 0.005938, mean_q: 0.005110
 2311/10000: episode: 232, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000076, mae: 0.006141, mean_q: 0.004723
 2321/10000: episode: 233, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000080, mae: 0.005624, mean_q: 0.005810
 2331/10000: episode: 234, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000067, mae: 0.006092, mean_q: 0.004984
 2341/10000: episode: 235, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000039, mae: 0.004745, mean_q: 0.004196
 2351/10000: episode: 236, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000109, mae: 0.007303, mean_q: 0.005394
 2361/10000: episode: 237, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000082, mae: 0.005431, mean_q: 0.006246
 2371/10000: episode: 238, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000055, mae: 0.005283, mean_q: 0.004388
 2381/10000: episode: 239, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000034, mae: 0.004848, mean_q: 0.003804
 2391/10000: episode: 240, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000065, mae: 0.005181, mean_q: 0.005037
 2401/10000: episode: 241, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000087, mae: 0.005972, mean_q: 0.006463
 2411/10000: episode: 242, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000052, mae: 0.005995, mean_q: 0.003502
 2421/10000: episode: 243, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000062, mae: 0.006019, mean_q: 0.004620
 2431/10000: episode: 244, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000045, mae: 0.004349, mean_q: 0.004989
 2441/10000: episode: 245, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000063, mae: 0.006005, mean_q: 0.003521
 2451/10000: episode: 246, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000054, mae: 0.006104, mean_q: 0.005791
 2461/10000: episode: 247, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000120, mae: 0.006912, mean_q: 0.006006
 2471/10000: episode: 248, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000052, mae: 0.005953, mean_q: 0.004108
 2481/10000: episode: 249, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000032, mae: 0.004735, mean_q: 0.004902
 2491/10000: episode: 250, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000032, mae: 0.004702, mean_q: 0.003713
[Info] 1-TH LEVEL FOUND: 0.06898478418588638, Considering 100/100 traces
 2501/10000: episode: 251, duration: 0.660s, episode steps: 10, steps per second: 15, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000049, mae: 0.005351, mean_q: 0.004913
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.06898478418588638
 2502/10000: episode: 252, duration: 0.494s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000017, mae: 0.004064, mean_q: 0.005813
 2512/10000: episode: 253, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000112, mae: 0.007143, mean_q: 0.006788
 2522/10000: episode: 254, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000091, mae: 0.006934, mean_q: 0.003886
 2532/10000: episode: 255, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000055, mae: 0.004991, mean_q: 0.005462
 2542/10000: episode: 256, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000050, mae: 0.006104, mean_q: 0.003802
 2552/10000: episode: 257, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000025, mae: 0.004064, mean_q: 0.004483
 2562/10000: episode: 258, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000057, mae: 0.004961, mean_q: 0.004927
 2572/10000: episode: 259, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000031, mae: 0.004558, mean_q: 0.003874
 2582/10000: episode: 260, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000055, mae: 0.005660, mean_q: 0.004101
 2592/10000: episode: 261, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000066, mae: 0.005666, mean_q: 0.003720
 2602/10000: episode: 262, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000031, mae: 0.005038, mean_q: 0.002827
 2612/10000: episode: 263, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000150, mae: 0.007729, mean_q: 0.007430
 2622/10000: episode: 264, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000074, mae: 0.007152, mean_q: 0.004487
 2632/10000: episode: 265, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000043, mae: 0.006262, mean_q: 0.003472
 2642/10000: episode: 266, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000096, mae: 0.006343, mean_q: 0.004680
 2652/10000: episode: 267, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000067, mae: 0.006694, mean_q: 0.003606
 2662/10000: episode: 268, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000115, mae: 0.007363, mean_q: 0.007327
 2672/10000: episode: 269, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000029, mae: 0.004650, mean_q: 0.003978
 2682/10000: episode: 270, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000076, mae: 0.005074, mean_q: 0.002650
 2692/10000: episode: 271, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000036, mae: 0.005336, mean_q: 0.004911
 2702/10000: episode: 272, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000057, mae: 0.005145, mean_q: 0.005093
 2712/10000: episode: 273, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000056, mae: 0.005663, mean_q: 0.003534
 2722/10000: episode: 274, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000061, mae: 0.005030, mean_q: 0.003947
 2732/10000: episode: 275, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000096, mae: 0.006779, mean_q: 0.005959
 2742/10000: episode: 276, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000068, mae: 0.007294, mean_q: 0.004552
 2752/10000: episode: 277, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000056, mae: 0.005373, mean_q: 0.004931
 2762/10000: episode: 278, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.055, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000086, mae: 0.006321, mean_q: 0.006293
 2772/10000: episode: 279, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000059, mae: 0.005443, mean_q: 0.005092
 2782/10000: episode: 280, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000027, mae: 0.004412, mean_q: 0.004440
 2792/10000: episode: 281, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000081, mae: 0.006865, mean_q: 0.004787
 2802/10000: episode: 282, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000075, mae: 0.007442, mean_q: 0.004445
 2812/10000: episode: 283, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000053, mae: 0.005181, mean_q: 0.005347
 2822/10000: episode: 284, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000038, mae: 0.005359, mean_q: 0.003017
 2832/10000: episode: 285, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000028, mae: 0.004812, mean_q: 0.003102
 2842/10000: episode: 286, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000032, mae: 0.004649, mean_q: 0.004862
 2852/10000: episode: 287, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000025, mae: 0.004469, mean_q: 0.003697
 2862/10000: episode: 288, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000050, mae: 0.004162, mean_q: 0.004059
 2872/10000: episode: 289, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000044, mae: 0.003795, mean_q: 0.004381
 2882/10000: episode: 290, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000064, mae: 0.004858, mean_q: 0.005158
 2892/10000: episode: 291, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000073, mae: 0.005668, mean_q: 0.005429
 2902/10000: episode: 292, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000061, mae: 0.004820, mean_q: 0.004197
 2912/10000: episode: 293, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000116, mae: 0.008116, mean_q: 0.005533
 2922/10000: episode: 294, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000052, mae: 0.007278, mean_q: 0.002849
 2932/10000: episode: 295, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000072, mae: 0.006105, mean_q: 0.005742
 2942/10000: episode: 296, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000085, mae: 0.005829, mean_q: 0.005459
 2952/10000: episode: 297, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000037, mae: 0.004829, mean_q: 0.004082
 2962/10000: episode: 298, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000021, mae: 0.003912, mean_q: 0.004211
 2972/10000: episode: 299, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000025, mae: 0.004288, mean_q: 0.003060
 2982/10000: episode: 300, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000045, mae: 0.005032, mean_q: 0.006194
 2992/10000: episode: 301, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000048, mae: 0.004370, mean_q: 0.003800
 3002/10000: episode: 302, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000026, mae: 0.004542, mean_q: 0.004317
 3012/10000: episode: 303, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000080, mae: 0.006396, mean_q: 0.004462
 3022/10000: episode: 304, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000061, mae: 0.005464, mean_q: 0.004023
 3032/10000: episode: 305, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000045, mae: 0.004288, mean_q: 0.004487
 3042/10000: episode: 306, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000078, mae: 0.005439, mean_q: 0.005900
 3052/10000: episode: 307, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000050, mae: 0.005753, mean_q: 0.003802
 3062/10000: episode: 308, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000029, mae: 0.004822, mean_q: 0.003310
 3072/10000: episode: 309, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000068, mae: 0.004658, mean_q: 0.005139
 3082/10000: episode: 310, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000062, mae: 0.005910, mean_q: 0.005317
 3092/10000: episode: 311, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000048, mae: 0.005318, mean_q: 0.003782
 3102/10000: episode: 312, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000026, mae: 0.004246, mean_q: 0.002860
 3112/10000: episode: 313, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000034, mae: 0.004516, mean_q: 0.005287
 3122/10000: episode: 314, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000023, mae: 0.004642, mean_q: 0.003555
 3132/10000: episode: 315, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000028, mae: 0.004363, mean_q: 0.003777
 3142/10000: episode: 316, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000073, mae: 0.005099, mean_q: 0.005291
 3152/10000: episode: 317, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000057, mae: 0.006880, mean_q: 0.005388
 3162/10000: episode: 318, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000031, mae: 0.005168, mean_q: 0.003267
 3172/10000: episode: 319, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000099, mae: 0.005662, mean_q: 0.006349
 3182/10000: episode: 320, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000052, mae: 0.004718, mean_q: 0.005580
 3192/10000: episode: 321, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000028, mae: 0.003860, mean_q: 0.004003
 3202/10000: episode: 322, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000043, mae: 0.003997, mean_q: 0.005142
 3212/10000: episode: 323, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000040, mae: 0.004378, mean_q: 0.004538
 3222/10000: episode: 324, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000054, mae: 0.006011, mean_q: 0.004242
 3232/10000: episode: 325, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000029, mae: 0.004237, mean_q: 0.005133
 3242/10000: episode: 326, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000047, mae: 0.004231, mean_q: 0.003701
 3252/10000: episode: 327, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000026, mae: 0.004543, mean_q: 0.004025
 3262/10000: episode: 328, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000019, mae: 0.003847, mean_q: 0.003873
 3272/10000: episode: 329, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000051, mae: 0.004811, mean_q: 0.004601
 3282/10000: episode: 330, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000049, mae: 0.006544, mean_q: 0.003666
 3292/10000: episode: 331, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000029, mae: 0.004971, mean_q: 0.003436
 3302/10000: episode: 332, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000042, mae: 0.004066, mean_q: 0.005351
 3312/10000: episode: 333, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000052, mae: 0.005609, mean_q: 0.004506
 3322/10000: episode: 334, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000034, mae: 0.004635, mean_q: 0.003184
 3332/10000: episode: 335, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000079, mae: 0.005662, mean_q: 0.006767
 3342/10000: episode: 336, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000025, mae: 0.003807, mean_q: 0.003392
 3352/10000: episode: 337, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000062, mae: 0.005227, mean_q: 0.004075
 3362/10000: episode: 338, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000031, mae: 0.004781, mean_q: 0.005172
 3372/10000: episode: 339, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000022, mae: 0.003871, mean_q: 0.003973
 3382/10000: episode: 340, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000124, mae: 0.007001, mean_q: 0.007356
 3392/10000: episode: 341, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000140, mae: 0.008897, mean_q: 0.004399
 3402/10000: episode: 342, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.033, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000089, mae: 0.006262, mean_q: 0.003511
 3412/10000: episode: 343, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000022, mae: 0.004811, mean_q: 0.004479
 3422/10000: episode: 344, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000029, mae: 0.004236, mean_q: 0.004846
 3432/10000: episode: 345, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000031, mae: 0.003956, mean_q: 0.004397
 3442/10000: episode: 346, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000043, mae: 0.003536, mean_q: 0.004098
 3452/10000: episode: 347, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000029, mae: 0.003719, mean_q: 0.003922
 3462/10000: episode: 348, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000077, mae: 0.005381, mean_q: 0.004326
 3472/10000: episode: 349, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000042, mae: 0.005488, mean_q: 0.004957
 3482/10000: episode: 350, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000033, mae: 0.004622, mean_q: 0.003990
 3492/10000: episode: 351, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000036, mae: 0.005892, mean_q: 0.004663
[Info] 1-TH LEVEL FOUND: 0.0422486811876297, Considering 100/100 traces
 3502/10000: episode: 352, duration: 0.660s, episode steps: 10, steps per second: 15, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000028, mae: 0.004338, mean_q: 0.002847
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.0422486811876297
 3503/10000: episode: 353, duration: 0.443s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000020, mae: 0.004444, mean_q: 0.007471
 3513/10000: episode: 354, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000032, mae: 0.004816, mean_q: 0.004963
 3523/10000: episode: 355, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000093, mae: 0.006993, mean_q: 0.004337
 3533/10000: episode: 356, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.131, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000042, mae: 0.004623, mean_q: 0.004253
 3543/10000: episode: 357, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000036, mae: 0.004677, mean_q: 0.004810
 3553/10000: episode: 358, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000031, mae: 0.004375, mean_q: 0.003275
 3563/10000: episode: 359, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000066, mae: 0.005391, mean_q: 0.005843
 3573/10000: episode: 360, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000053, mae: 0.005037, mean_q: 0.003974
 3583/10000: episode: 361, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000025, mae: 0.004640, mean_q: 0.004562
 3593/10000: episode: 362, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000041, mae: 0.004081, mean_q: 0.004416
 3603/10000: episode: 363, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000054, mae: 0.005741, mean_q: 0.004689
 3613/10000: episode: 364, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000047, mae: 0.004855, mean_q: 0.004837
 3623/10000: episode: 365, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000215, mae: 0.005421, mean_q: 0.004060
 3633/10000: episode: 366, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000081, mae: 0.009022, mean_q: 0.006009
 3643/10000: episode: 367, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000152, mae: 0.010865, mean_q: 0.002758
 3653/10000: episode: 368, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000105, mae: 0.010878, mean_q: 0.007330
 3663/10000: episode: 369, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000281, mae: 0.011812, mean_q: 0.005797
 3673/10000: episode: 370, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000217, mae: 0.010833, mean_q: 0.005377
 3683/10000: episode: 371, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.115, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000169, mae: 0.013257, mean_q: 0.004032
 3693/10000: episode: 372, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000074, mae: 0.008652, mean_q: 0.004538
 3703/10000: episode: 373, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000039, mae: 0.004199, mean_q: 0.005311
 3713/10000: episode: 374, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000081, mae: 0.006621, mean_q: 0.005543
 3723/10000: episode: 375, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000087, mae: 0.006175, mean_q: 0.005516
 3733/10000: episode: 376, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000061, mae: 0.004324, mean_q: 0.004819
 3743/10000: episode: 377, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000040, mae: 0.004296, mean_q: 0.006568
 3753/10000: episode: 378, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000059, mae: 0.005079, mean_q: 0.003692
 3763/10000: episode: 379, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000039, mae: 0.004682, mean_q: 0.004683
 3773/10000: episode: 380, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000082, mae: 0.006500, mean_q: 0.005318
 3783/10000: episode: 381, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000060, mae: 0.006411, mean_q: 0.005603
 3793/10000: episode: 382, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000041, mae: 0.005321, mean_q: 0.004562
 3803/10000: episode: 383, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000082, mae: 0.006498, mean_q: 0.005071
 3813/10000: episode: 384, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000034, mae: 0.005368, mean_q: 0.005107
 3823/10000: episode: 385, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000250, mae: 0.006862, mean_q: 0.006169
 3833/10000: episode: 386, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000093, mae: 0.008020, mean_q: 0.004960
 3843/10000: episode: 387, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000089, mae: 0.007332, mean_q: 0.005239
 3853/10000: episode: 388, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000071, mae: 0.005521, mean_q: 0.006841
 3863/10000: episode: 389, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000056, mae: 0.005696, mean_q: 0.004907
 3873/10000: episode: 390, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000089, mae: 0.007506, mean_q: 0.003744
 3883/10000: episode: 391, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000260, mae: 0.008705, mean_q: 0.004731
 3893/10000: episode: 392, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000371, mae: 0.015131, mean_q: 0.008032
 3903/10000: episode: 393, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000079, mae: 0.008493, mean_q: 0.003660
 3913/10000: episode: 394, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000133, mae: 0.007369, mean_q: 0.007966
 3923/10000: episode: 395, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000362, mae: 0.014045, mean_q: 0.006643
 3933/10000: episode: 396, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000137, mae: 0.010916, mean_q: 0.004721
 3943/10000: episode: 397, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000147, mae: 0.010716, mean_q: 0.008115
 3953/10000: episode: 398, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000142, mae: 0.011603, mean_q: 0.003211
 3963/10000: episode: 399, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000084, mae: 0.007619, mean_q: 0.005201
 3973/10000: episode: 400, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000069, mae: 0.007198, mean_q: 0.005163
 3983/10000: episode: 401, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000078, mae: 0.008071, mean_q: 0.004386
 3993/10000: episode: 402, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000053, mae: 0.006625, mean_q: 0.006209
 4003/10000: episode: 403, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000045, mae: 0.005651, mean_q: 0.003339
 4013/10000: episode: 404, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000042, mae: 0.004582, mean_q: 0.005512
 4023/10000: episode: 405, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000027, mae: 0.004168, mean_q: 0.003930
 4033/10000: episode: 406, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000037, mae: 0.003913, mean_q: 0.005172
 4043/10000: episode: 407, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000035, mae: 0.004805, mean_q: 0.003809
 4053/10000: episode: 408, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000049, mae: 0.004359, mean_q: 0.005126
 4063/10000: episode: 409, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000044, mae: 0.004322, mean_q: 0.005387
 4073/10000: episode: 410, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000050, mae: 0.004635, mean_q: 0.004616
 4083/10000: episode: 411, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000043, mae: 0.004291, mean_q: 0.005363
 4093/10000: episode: 412, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000117, mae: 0.007266, mean_q: 0.005439
 4103/10000: episode: 413, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000107, mae: 0.006287, mean_q: 0.006294
 4113/10000: episode: 414, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000075, mae: 0.006510, mean_q: 0.004661
 4123/10000: episode: 415, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000092, mae: 0.005485, mean_q: 0.004956
 4133/10000: episode: 416, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000046, mae: 0.004609, mean_q: 0.005224
 4143/10000: episode: 417, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000072, mae: 0.004687, mean_q: 0.005309
 4153/10000: episode: 418, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000053, mae: 0.006226, mean_q: 0.005034
 4163/10000: episode: 419, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000064, mae: 0.005580, mean_q: 0.004946
 4173/10000: episode: 420, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000054, mae: 0.004887, mean_q: 0.004238
 4183/10000: episode: 421, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000023, mae: 0.003699, mean_q: 0.004197
 4193/10000: episode: 422, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000072, mae: 0.006013, mean_q: 0.004853
 4203/10000: episode: 423, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000068, mae: 0.005321, mean_q: 0.005488
 4213/10000: episode: 424, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000063, mae: 0.005829, mean_q: 0.004414
 4223/10000: episode: 425, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000076, mae: 0.005101, mean_q: 0.005618
 4233/10000: episode: 426, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000068, mae: 0.004633, mean_q: 0.005324
 4243/10000: episode: 427, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000041, mae: 0.005395, mean_q: 0.004637
 4253/10000: episode: 428, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000050, mae: 0.004306, mean_q: 0.004450
 4263/10000: episode: 429, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000055, mae: 0.004521, mean_q: 0.005392
 4273/10000: episode: 430, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000098, mae: 0.007106, mean_q: 0.005984
 4283/10000: episode: 431, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000048, mae: 0.006590, mean_q: 0.003507
 4293/10000: episode: 432, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000089, mae: 0.007584, mean_q: 0.005813
 4303/10000: episode: 433, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000075, mae: 0.006247, mean_q: 0.005585
 4313/10000: episode: 434, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000077, mae: 0.006024, mean_q: 0.006277
 4323/10000: episode: 435, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.084, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000088, mae: 0.007951, mean_q: 0.004247
 4333/10000: episode: 436, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000055, mae: 0.006467, mean_q: 0.006708
 4343/10000: episode: 437, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000056, mae: 0.006656, mean_q: 0.004290
 4353/10000: episode: 438, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000060, mae: 0.006606, mean_q: 0.003092
 4363/10000: episode: 439, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000098, mae: 0.007534, mean_q: 0.005291
 4373/10000: episode: 440, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000152, mae: 0.006975, mean_q: 0.007302
 4383/10000: episode: 441, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000078, mae: 0.008455, mean_q: 0.003337
 4393/10000: episode: 442, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000039, mae: 0.005095, mean_q: 0.005316
 4403/10000: episode: 443, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000067, mae: 0.004950, mean_q: 0.005676
 4413/10000: episode: 444, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000072, mae: 0.006245, mean_q: 0.004838
 4423/10000: episode: 445, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000067, mae: 0.007112, mean_q: 0.006185
 4433/10000: episode: 446, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000038, mae: 0.004499, mean_q: 0.004699
 4443/10000: episode: 447, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000054, mae: 0.005674, mean_q: 0.003305
 4453/10000: episode: 448, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000096, mae: 0.006025, mean_q: 0.006643
 4463/10000: episode: 449, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000064, mae: 0.007815, mean_q: 0.004551
 4473/10000: episode: 450, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000079, mae: 0.006415, mean_q: 0.004151
 4483/10000: episode: 451, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000035, mae: 0.005905, mean_q: 0.004877
 4493/10000: episode: 452, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000081, mae: 0.006304, mean_q: 0.004463
[Info] 1-TH LEVEL FOUND: 0.02677052468061447, Considering 100/100 traces
 4503/10000: episode: 453, duration: 0.665s, episode steps: 10, steps per second: 15, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000084, mae: 0.007601, mean_q: 0.007373
[Info] 2-TH LEVEL FOUND: 0.03416549041867256, Considering 100/100 traces
 4504/10000: episode: 454, duration: 0.606s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000068, mae: 0.010063, mean_q: -0.004848
[Info] 3-TH LEVEL FOUND: 0.06112230196595192, Considering 100/100 traces
 4505/10000: episode: 455, duration: 0.635s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000022, mae: 0.005319, mean_q: -0.002876
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.06112230196595192
 4506/10000: episode: 456, duration: 0.429s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000105, mae: 0.009321, mean_q: 0.013396
 4516/10000: episode: 457, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000054, mae: 0.005896, mean_q: 0.004746
 4526/10000: episode: 458, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000268, mae: 0.011362, mean_q: 0.007192
 4536/10000: episode: 459, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000108, mae: 0.009358, mean_q: 0.007408
 4546/10000: episode: 460, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000071, mae: 0.006733, mean_q: 0.004195
 4556/10000: episode: 461, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000063, mae: 0.005799, mean_q: 0.006395
 4566/10000: episode: 462, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000050, mae: 0.005376, mean_q: 0.003636
 4576/10000: episode: 463, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000048, mae: 0.004648, mean_q: 0.005612
 4586/10000: episode: 464, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000034, mae: 0.003978, mean_q: 0.004537
 4596/10000: episode: 465, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000075, mae: 0.005517, mean_q: 0.005675
 4606/10000: episode: 466, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000058, mae: 0.005831, mean_q: 0.003699
 4616/10000: episode: 467, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000238, mae: 0.012482, mean_q: 0.009244
 4626/10000: episode: 468, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000091, mae: 0.007592, mean_q: 0.004036
 4636/10000: episode: 469, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000160, mae: 0.006254, mean_q: 0.008303
 4646/10000: episode: 470, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000065, mae: 0.007251, mean_q: 0.002134
 4656/10000: episode: 471, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000055, mae: 0.006931, mean_q: 0.004923
 4666/10000: episode: 472, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000106, mae: 0.006255, mean_q: 0.007106
 4676/10000: episode: 473, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000051, mae: 0.005951, mean_q: 0.004938
 4686/10000: episode: 474, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000235, mae: 0.006568, mean_q: 0.004366
 4696/10000: episode: 475, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000197, mae: 0.013364, mean_q: 0.006453
 4706/10000: episode: 476, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000090, mae: 0.006956, mean_q: 0.004702
 4716/10000: episode: 477, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.147, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.000191, mae: 0.010140, mean_q: 0.004603
 4726/10000: episode: 478, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000117, mae: 0.008688, mean_q: 0.007399
 4736/10000: episode: 479, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000074, mae: 0.006550, mean_q: 0.004129
 4746/10000: episode: 480, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000031, mae: 0.004604, mean_q: 0.004560
 4756/10000: episode: 481, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000030, mae: 0.004049, mean_q: 0.005105
 4766/10000: episode: 482, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000048, mae: 0.005250, mean_q: 0.004267
 4776/10000: episode: 483, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000161, mae: 0.006615, mean_q: 0.007063
 4786/10000: episode: 484, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000106, mae: 0.008500, mean_q: 0.002635
 4796/10000: episode: 485, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000055, mae: 0.006287, mean_q: 0.005083
 4806/10000: episode: 486, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000420, mae: 0.015365, mean_q: 0.007192
 4816/10000: episode: 487, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000155, mae: 0.011224, mean_q: 0.007366
 4826/10000: episode: 488, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000067, mae: 0.006894, mean_q: 0.002886
 4836/10000: episode: 489, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000080, mae: 0.006580, mean_q: 0.007953
 4846/10000: episode: 490, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000147, mae: 0.006461, mean_q: 0.003521
 4856/10000: episode: 491, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000091, mae: 0.009539, mean_q: 0.006564
 4866/10000: episode: 492, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000221, mae: 0.006908, mean_q: 0.005611
 4876/10000: episode: 493, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000080, mae: 0.006924, mean_q: 0.004449
 4886/10000: episode: 494, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000060, mae: 0.006562, mean_q: 0.008059
 4896/10000: episode: 495, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000267, mae: 0.008621, mean_q: 0.007196
 4906/10000: episode: 496, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000045, mae: 0.005270, mean_q: 0.004310
 4916/10000: episode: 497, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000229, mae: 0.006344, mean_q: 0.005997
 4926/10000: episode: 498, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000234, mae: 0.009837, mean_q: 0.005603
 4936/10000: episode: 499, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000297, mae: 0.014342, mean_q: 0.006353
 4946/10000: episode: 500, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000106, mae: 0.010572, mean_q: 0.003269
 4956/10000: episode: 501, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000071, mae: 0.007165, mean_q: 0.004798
 4966/10000: episode: 502, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000109, mae: 0.008840, mean_q: 0.008146
 4976/10000: episode: 503, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000063, mae: 0.008238, mean_q: 0.003873
 4986/10000: episode: 504, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000095, mae: 0.007781, mean_q: 0.008503
 4996/10000: episode: 505, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000042, mae: 0.005563, mean_q: 0.004605
 5006/10000: episode: 506, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000219, mae: 0.007652, mean_q: 0.007404
 5016/10000: episode: 507, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000116, mae: 0.010735, mean_q: 0.000933
 5026/10000: episode: 508, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000274, mae: 0.011134, mean_q: 0.010420
 5036/10000: episode: 509, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000132, mae: 0.011342, mean_q: 0.003301
 5046/10000: episode: 510, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.042, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000087, mae: 0.008257, mean_q: 0.006917
 5056/10000: episode: 511, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000203, mae: 0.008439, mean_q: 0.005819
 5066/10000: episode: 512, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000219, mae: 0.006367, mean_q: 0.007306
 5076/10000: episode: 513, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000093, mae: 0.008201, mean_q: 0.007679
 5086/10000: episode: 514, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000101, mae: 0.006426, mean_q: 0.006251
 5096/10000: episode: 515, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000028, mae: 0.003991, mean_q: 0.005532
 5106/10000: episode: 516, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000039, mae: 0.004954, mean_q: 0.003909
 5116/10000: episode: 517, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000052, mae: 0.005215, mean_q: 0.005353
 5126/10000: episode: 518, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000068, mae: 0.005392, mean_q: 0.003568
 5136/10000: episode: 519, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000125, mae: 0.008233, mean_q: 0.007601
 5146/10000: episode: 520, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000187, mae: 0.007754, mean_q: 0.007240
 5156/10000: episode: 521, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000135, mae: 0.007880, mean_q: 0.007959
 5166/10000: episode: 522, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000054, mae: 0.004889, mean_q: 0.005329
 5176/10000: episode: 523, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000073, mae: 0.006235, mean_q: 0.005257
 5186/10000: episode: 524, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000093, mae: 0.006693, mean_q: 0.005857
 5196/10000: episode: 525, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000217, mae: 0.006234, mean_q: 0.007595
 5206/10000: episode: 526, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000069, mae: 0.006246, mean_q: 0.004942
 5216/10000: episode: 527, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000054, mae: 0.007162, mean_q: 0.007069
 5226/10000: episode: 528, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000072, mae: 0.006884, mean_q: 0.003331
 5236/10000: episode: 529, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000063, mae: 0.006187, mean_q: 0.007540
 5246/10000: episode: 530, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000083, mae: 0.006010, mean_q: 0.004649
 5256/10000: episode: 531, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000078, mae: 0.005067, mean_q: 0.008245
 5266/10000: episode: 532, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000062, mae: 0.006409, mean_q: 0.004758
 5276/10000: episode: 533, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000087, mae: 0.007117, mean_q: 0.004031
 5286/10000: episode: 534, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000056, mae: 0.006222, mean_q: 0.004559
 5296/10000: episode: 535, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000036, mae: 0.004731, mean_q: 0.004463
 5306/10000: episode: 536, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000071, mae: 0.004927, mean_q: 0.006250
 5316/10000: episode: 537, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000083, mae: 0.008875, mean_q: 0.005652
 5326/10000: episode: 538, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000048, mae: 0.004889, mean_q: 0.005696
 5336/10000: episode: 539, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000054, mae: 0.005015, mean_q: 0.004997
 5346/10000: episode: 540, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000037, mae: 0.005214, mean_q: 0.004744
 5356/10000: episode: 541, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000245, mae: 0.008413, mean_q: 0.008062
 5366/10000: episode: 542, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000167, mae: 0.011670, mean_q: 0.008553
 5376/10000: episode: 543, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000064, mae: 0.008203, mean_q: 0.002868
 5386/10000: episode: 544, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000053, mae: 0.006055, mean_q: 0.006648
 5396/10000: episode: 545, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000097, mae: 0.009259, mean_q: 0.004630
 5406/10000: episode: 546, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000326, mae: 0.014778, mean_q: 0.010522
 5416/10000: episode: 547, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.016, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000282, mae: 0.011850, mean_q: 0.007989
 5426/10000: episode: 548, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000137, mae: 0.007641, mean_q: 0.007839
 5436/10000: episode: 549, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000081, mae: 0.006569, mean_q: 0.003691
 5446/10000: episode: 550, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000263, mae: 0.008578, mean_q: 0.007331
 5456/10000: episode: 551, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000223, mae: 0.007506, mean_q: 0.008378
 5466/10000: episode: 552, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000076, mae: 0.007012, mean_q: 0.004586
 5476/10000: episode: 553, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000063, mae: 0.007704, mean_q: 0.006316
 5486/10000: episode: 554, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000051, mae: 0.007170, mean_q: 0.003512
 5496/10000: episode: 555, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000077, mae: 0.005389, mean_q: 0.007568
[Info] 1-TH LEVEL FOUND: 0.041060976684093475, Considering 15/100 traces
 5506/10000: episode: 556, duration: 0.662s, episode steps: 10, steps per second: 15, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000070, mae: 0.005181, mean_q: 0.004588
 5508/10000: episode: 557, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000008, mae: 0.003318, mean_q: 0.004597
 5515/10000: episode: 558, duration: 0.034s, episode steps: 7, steps per second: 204, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000233, mae: 0.009961, mean_q: 0.011186
 5517/10000: episode: 559, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000220, mae: 0.015033, mean_q: -0.004441
 5519/10000: episode: 560, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000089, mae: 0.008993, mean_q: 0.014124
 5526/10000: episode: 561, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000060, mae: 0.007668, mean_q: 0.003897
 5528/10000: episode: 562, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000029, mae: 0.004382, mean_q: 0.006423
 5535/10000: episode: 563, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000041, mae: 0.006035, mean_q: 0.005316
 5537/10000: episode: 564, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000026, mae: 0.004858, mean_q: 0.001316
 5539/10000: episode: 565, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000020, mae: 0.004224, mean_q: 0.008694
 5541/10000: episode: 566, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000053, mae: 0.005011, mean_q: 0.004789
 5548/10000: episode: 567, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000032, mae: 0.004096, mean_q: 0.005795
 5555/10000: episode: 568, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000289, mae: 0.007207, mean_q: 0.008128
 5557/10000: episode: 569, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000065, mae: 0.008395, mean_q: -0.000887
 5564/10000: episode: 570, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000088, mae: 0.008078, mean_q: 0.005287
 5566/10000: episode: 571, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000129, mae: 0.012666, mean_q: 0.017236
 5573/10000: episode: 572, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000198, mae: 0.013622, mean_q: 0.005904
 5580/10000: episode: 573, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000167, mae: 0.010770, mean_q: 0.008084
 5587/10000: episode: 574, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000095, mae: 0.008017, mean_q: 0.006444
 5589/10000: episode: 575, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000074, mae: 0.006628, mean_q: 0.004025
 5596/10000: episode: 576, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000048, mae: 0.005642, mean_q: 0.003797
 5603/10000: episode: 577, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000128, mae: 0.007248, mean_q: 0.008729
 5610/10000: episode: 578, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000050, mae: 0.004765, mean_q: 0.004494
 5617/10000: episode: 579, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000041, mae: 0.005761, mean_q: 0.004546
 5619/10000: episode: 580, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000553, mae: 0.014724, mean_q: 0.015496
 5623/10000: episode: 581, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000310, mae: 0.010710, mean_q: 0.003828
 5630/10000: episode: 582, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000098, mae: 0.009449, mean_q: 0.004960
 5637/10000: episode: 583, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000364, mae: 0.011325, mean_q: 0.010340
 5639/10000: episode: 584, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.011762, mean_q: -0.001755
 5646/10000: episode: 585, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000112, mae: 0.010067, mean_q: 0.006289
 5648/10000: episode: 586, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000059, mae: 0.007921, mean_q: 0.012276
 5655/10000: episode: 587, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000077, mae: 0.008387, mean_q: 0.003610
 5657/10000: episode: 588, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000095, mae: 0.008125, mean_q: 0.003164
 5659/10000: episode: 589, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000075, mae: 0.006995, mean_q: 0.007435
 5666/10000: episode: 590, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000080, mae: 0.006867, mean_q: 0.006201
 5668/10000: episode: 591, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000135, mae: 0.011005, mean_q: -0.003173
 5675/10000: episode: 592, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000283, mae: 0.011976, mean_q: 0.011926
 5677/10000: episode: 593, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000067, mae: 0.006071, mean_q: 0.005172
 5684/10000: episode: 594, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000068, mae: 0.007188, mean_q: 0.005255
 5686/10000: episode: 595, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000069, mae: 0.006520, mean_q: 0.005295
 5693/10000: episode: 596, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000107, mae: 0.009739, mean_q: 0.006993
 5700/10000: episode: 597, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000045, mae: 0.006671, mean_q: 0.004787
 5707/10000: episode: 598, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000064, mae: 0.005925, mean_q: 0.005708
 5714/10000: episode: 599, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000041, mae: 0.004736, mean_q: 0.006692
 5721/10000: episode: 600, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000144, mae: 0.009651, mean_q: 0.005375
 5723/10000: episode: 601, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000059, mae: 0.008105, mean_q: 0.010337
 5725/10000: episode: 602, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000190, mae: 0.010039, mean_q: 0.004820
 5727/10000: episode: 603, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000125, mae: 0.009314, mean_q: 0.011956
 5734/10000: episode: 604, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000140, mae: 0.008190, mean_q: 0.006410
 5736/10000: episode: 605, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000136, mae: 0.008321, mean_q: 0.008084
 5743/10000: episode: 606, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000197, mae: 0.006322, mean_q: 0.003367
 5750/10000: episode: 607, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000403, mae: 0.011929, mean_q: 0.011659
 5757/10000: episode: 608, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000099, mae: 0.010048, mean_q: 0.002575
 5764/10000: episode: 609, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000106, mae: 0.008563, mean_q: 0.005246
 5766/10000: episode: 610, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000153, mae: 0.010499, mean_q: -0.001164
 5773/10000: episode: 611, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000089, mae: 0.007413, mean_q: 0.005730
 5780/10000: episode: 612, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000093, mae: 0.007421, mean_q: 0.007578
 5787/10000: episode: 613, duration: 0.035s, episode steps: 7, steps per second: 203, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000209, mae: 0.010802, mean_q: 0.010897
 5794/10000: episode: 614, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000111, mae: 0.008761, mean_q: 0.003278
 5801/10000: episode: 615, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000100, mae: 0.008321, mean_q: 0.005793
 5808/10000: episode: 616, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000107, mae: 0.006789, mean_q: 0.007290
 5810/10000: episode: 617, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000147, mae: 0.007860, mean_q: 0.005286
 5812/10000: episode: 618, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000083, mae: 0.008531, mean_q: 0.010056
 5819/10000: episode: 619, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000384, mae: 0.012978, mean_q: 0.006719
 5821/10000: episode: 620, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000198, mae: 0.011033, mean_q: 0.015428
 5828/10000: episode: 621, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000255, mae: 0.013231, mean_q: 0.009354
 5830/10000: episode: 622, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000247, mae: 0.017847, mean_q: -0.010034
 5837/10000: episode: 623, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.002089, mae: 0.022122, mean_q: 0.017061
 5841/10000: episode: 624, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000403, mae: 0.021136, mean_q: 0.010034
 5843/10000: episode: 625, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000326, mae: 0.020761, mean_q: -0.012293
 5850/10000: episode: 626, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000618, mae: 0.019527, mean_q: 0.012162
 5857/10000: episode: 627, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000242, mae: 0.015886, mean_q: 0.009316
 5864/10000: episode: 628, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000144, mae: 0.009710, mean_q: 0.007117
 5871/10000: episode: 629, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000305, mae: 0.010430, mean_q: 0.009792
 5873/10000: episode: 630, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000162, mae: 0.007694, mean_q: 0.002659
 5880/10000: episode: 631, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000080, mae: 0.005867, mean_q: 0.007464
 5887/10000: episode: 632, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000084, mae: 0.006103, mean_q: 0.006286
 5894/10000: episode: 633, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000130, mae: 0.008501, mean_q: 0.010169
 5896/10000: episode: 634, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000158, mae: 0.008646, mean_q: 0.006620
 5903/10000: episode: 635, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000193, mae: 0.008920, mean_q: 0.008226
 5905/10000: episode: 636, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001149, mae: 0.034854, mean_q: 0.040773
 5912/10000: episode: 637, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000468, mae: 0.023236, mean_q: 0.001791
 5914/10000: episode: 638, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000387, mae: 0.017925, mean_q: 0.009873
 5916/10000: episode: 639, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000213, mae: 0.017058, mean_q: -0.011302
 5923/10000: episode: 640, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000226, mae: 0.013473, mean_q: 0.005803
[Info] 2-TH LEVEL FOUND: 0.11852243542671204, Considering 10/100 traces
 5930/10000: episode: 641, duration: 0.662s, episode steps: 7, steps per second: 11, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000170, mae: 0.011036, mean_q: 0.005600
 5934/10000: episode: 642, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000246, mae: 0.011324, mean_q: 0.007213
[Info] FALSIFICATION!
 5937/10000: episode: 643, duration: 0.439s, episode steps: 3, steps per second: 7, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000139, mae: 0.011138, mean_q: 0.014718
 5941/10000: episode: 644, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000075, mae: 0.007159, mean_q: 0.005429
 5945/10000: episode: 645, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 1.240, mean reward: 0.310 [0.135, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.125 [8.000, 11.000], loss: 0.000110, mae: 0.006234, mean_q: 0.006039
 5949/10000: episode: 646, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.000173, mae: 0.009637, mean_q: 0.008461
 5953/10000: episode: 647, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000209, mae: 0.010699, mean_q: 0.013764
 5957/10000: episode: 648, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000873, mae: 0.014847, mean_q: 0.013932
 5961/10000: episode: 649, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.003219, mae: 0.017760, mean_q: 0.003326
 5965/10000: episode: 650, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001005, mae: 0.032977, mean_q: 0.012969
 5969/10000: episode: 651, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000597, mae: 0.021833, mean_q: 0.014851
 5973/10000: episode: 652, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000447, mae: 0.020433, mean_q: -0.006940
 5977/10000: episode: 653, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000472, mae: 0.023117, mean_q: 0.029568
 5981/10000: episode: 654, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000566, mae: 0.023076, mean_q: -0.000505
[Info] FALSIFICATION!
 5984/10000: episode: 655, duration: 0.239s, episode steps: 3, steps per second: 13, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000463, mae: 0.019412, mean_q: 0.011683
 5988/10000: episode: 656, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000306, mae: 0.016016, mean_q: 0.008903
 5992/10000: episode: 657, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000336, mae: 0.016283, mean_q: -0.001960
 5996/10000: episode: 658, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000347, mae: 0.019292, mean_q: 0.026258
 6000/10000: episode: 659, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000337, mae: 0.018071, mean_q: 0.000046
 6004/10000: episode: 660, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 1.240, mean reward: 0.310 [0.135, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.125 [8.000, 11.000], loss: 0.000168, mae: 0.012029, mean_q: 0.009227
 6008/10000: episode: 661, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.003834, mae: 0.039210, mean_q: 0.030843
 6012/10000: episode: 662, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.000774, mae: 0.029867, mean_q: -0.013474
[Info] FALSIFICATION!
 6015/10000: episode: 663, duration: 0.281s, episode steps: 3, steps per second: 11, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000970, mae: 0.028619, mean_q: 0.035354
 6019/10000: episode: 664, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000425, mae: 0.022388, mean_q: -0.007882
 6023/10000: episode: 665, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.003543, mae: 0.029140, mean_q: 0.026691
 6027/10000: episode: 666, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000653, mae: 0.021237, mean_q: 0.011988
 6031/10000: episode: 667, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000578, mae: 0.019096, mean_q: 0.020676
 6035/10000: episode: 668, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000412, mae: 0.020753, mean_q: 0.000956
 6039/10000: episode: 669, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000522, mae: 0.015473, mean_q: 0.024235
 6043/10000: episode: 670, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000081, mae: 0.008533, mean_q: 0.000973
[Info] FALSIFICATION!
 6046/10000: episode: 671, duration: 0.268s, episode steps: 3, steps per second: 11, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000156, mae: 0.009501, mean_q: 0.016958
 6050/10000: episode: 672, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000272, mae: 0.010390, mean_q: 0.006547
[Info] FALSIFICATION!
 6053/10000: episode: 673, duration: 0.274s, episode steps: 3, steps per second: 11, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000163, mae: 0.010675, mean_q: 0.013811
 6057/10000: episode: 674, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.003240, mae: 0.017488, mean_q: 0.008036
 6061/10000: episode: 675, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000615, mae: 0.024742, mean_q: 0.006564
 6065/10000: episode: 676, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000600, mae: 0.022790, mean_q: 0.028896
 6069/10000: episode: 677, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000760, mae: 0.022527, mean_q: -0.002660
 6073/10000: episode: 678, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.005033, mae: 0.030098, mean_q: 0.029164
 6077/10000: episode: 679, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000924, mae: 0.028683, mean_q: 0.012997
 6081/10000: episode: 680, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002093, mae: 0.024398, mean_q: 0.016167
 6085/10000: episode: 681, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001070, mae: 0.033701, mean_q: 0.012737
 6089/10000: episode: 682, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.003833, mae: 0.046266, mean_q: 0.040580
 6093/10000: episode: 683, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001125, mae: 0.035558, mean_q: -0.012810
[Info] FALSIFICATION!
 6096/10000: episode: 684, duration: 0.274s, episode steps: 3, steps per second: 11, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000519, mae: 0.018998, mean_q: 0.025728
 6100/10000: episode: 685, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000426, mae: 0.016773, mean_q: 0.000734
 6104/10000: episode: 686, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002525, mae: 0.021009, mean_q: 0.019761
 6108/10000: episode: 687, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000435, mae: 0.020414, mean_q: 0.006625
 6112/10000: episode: 688, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001214, mae: 0.028397, mean_q: 0.025270
 6116/10000: episode: 689, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.002892, mae: 0.027384, mean_q: 0.015747
 6120/10000: episode: 690, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000656, mae: 0.018730, mean_q: 0.024499
 6124/10000: episode: 691, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000343, mae: 0.021087, mean_q: -0.007456
 6128/10000: episode: 692, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.002799, mae: 0.028063, mean_q: 0.035819
 6132/10000: episode: 693, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000455, mae: 0.021130, mean_q: 0.000788
 6136/10000: episode: 694, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000180, mae: 0.010938, mean_q: 0.018640
 6140/10000: episode: 695, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000309, mae: 0.012856, mean_q: 0.008282
 6144/10000: episode: 696, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000417, mae: 0.017349, mean_q: 0.004456
 6148/10000: episode: 697, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.000379, mae: 0.017712, mean_q: 0.026280
[Info] FALSIFICATION!
 6151/10000: episode: 698, duration: 0.257s, episode steps: 3, steps per second: 12, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000123, mae: 0.007005, mean_q: 0.006175
 6155/10000: episode: 699, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000885, mae: 0.018883, mean_q: 0.024853
 6159/10000: episode: 700, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000733, mae: 0.013198, mean_q: 0.012752
 6163/10000: episode: 701, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000757, mae: 0.012912, mean_q: 0.016254
 6167/10000: episode: 702, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002547, mae: 0.020289, mean_q: 0.024138
 6171/10000: episode: 703, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000807, mae: 0.021118, mean_q: 0.010610
 6175/10000: episode: 704, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.006601, mae: 0.046539, mean_q: 0.053194
[Info] FALSIFICATION!
 6178/10000: episode: 705, duration: 0.202s, episode steps: 3, steps per second: 15, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.003537, mae: 0.037002, mean_q: 0.030091
 6182/10000: episode: 706, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001290, mae: 0.036947, mean_q: -0.000759
 6186/10000: episode: 707, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000641, mae: 0.015425, mean_q: 0.019859
 6190/10000: episode: 708, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 1.240, mean reward: 0.310 [0.135, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.125 [8.000, 11.000], loss: 0.000621, mae: 0.015506, mean_q: 0.010425
[Info] FALSIFICATION!
 6193/10000: episode: 709, duration: 0.171s, episode steps: 3, steps per second: 18, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.001405, mae: 0.036721, mean_q: 0.042060
 6197/10000: episode: 710, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000658, mae: 0.027493, mean_q: -0.011322
 6201/10000: episode: 711, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.003109, mae: 0.037649, mean_q: 0.050355
 6205/10000: episode: 712, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.007222, mae: 0.046456, mean_q: 0.011198
 6209/10000: episode: 713, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.005711, mae: 0.061796, mean_q: 0.046580
 6213/10000: episode: 714, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.007452, mae: 0.060349, mean_q: 0.050708
 6217/10000: episode: 715, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001510, mae: 0.042649, mean_q: -0.004184
[Info] FALSIFICATION!
 6220/10000: episode: 716, duration: 0.294s, episode steps: 3, steps per second: 10, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.001330, mae: 0.029513, mean_q: 0.005776
 6224/10000: episode: 717, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000775, mae: 0.028142, mean_q: 0.010587
[Info] FALSIFICATION!
 6227/10000: episode: 718, duration: 0.259s, episode steps: 3, steps per second: 12, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000519, mae: 0.023898, mean_q: 0.001916
 6231/10000: episode: 719, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000859, mae: 0.021162, mean_q: 0.024525
 6235/10000: episode: 720, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000538, mae: 0.021209, mean_q: 0.030155
 6239/10000: episode: 721, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.002757, mae: 0.029254, mean_q: 0.030495
 6243/10000: episode: 722, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.000344, mae: 0.012890, mean_q: 0.004476
 6247/10000: episode: 723, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001428, mae: 0.029733, mean_q: 0.022540
 6251/10000: episode: 724, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.003128, mae: 0.027508, mean_q: 0.031747
 6255/10000: episode: 725, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.003375, mae: 0.036163, mean_q: 0.023991
 6259/10000: episode: 726, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.004740, mae: 0.037748, mean_q: 0.026406
[Info] FALSIFICATION!
 6262/10000: episode: 727, duration: 0.258s, episode steps: 3, steps per second: 12, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.001359, mae: 0.028530, mean_q: 0.011494
 6266/10000: episode: 728, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001567, mae: 0.032025, mean_q: 0.032814
 6270/10000: episode: 729, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001665, mae: 0.026632, mean_q: 0.019362
 6274/10000: episode: 730, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000821, mae: 0.019516, mean_q: 0.032659
[Info] Complete ISplit Iteration
[Info] Levels: [0.041060977, 0.118522435, 0.371739]
[Info] Cond. Prob: [0.15, 0.1, 0.12]
[Info] Error Prob: 0.0018

 6278/10000: episode: 731, duration: 0.987s, episode steps: 4, steps per second: 4, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000745, mae: 0.028801, mean_q: 0.001993
 6288/10000: episode: 732, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000786, mae: 0.019175, mean_q: 0.020038
 6298/10000: episode: 733, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.002233, mae: 0.030103, mean_q: 0.019195
 6308/10000: episode: 734, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.003618, mae: 0.044007, mean_q: 0.026230
 6318/10000: episode: 735, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001375, mae: 0.035298, mean_q: 0.012808
 6328/10000: episode: 736, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000931, mae: 0.026880, mean_q: 0.016251
 6338/10000: episode: 737, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000825, mae: 0.017628, mean_q: 0.023677
 6348/10000: episode: 738, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.002743, mae: 0.025410, mean_q: 0.028975
 6358/10000: episode: 739, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.002520, mae: 0.033668, mean_q: 0.026974
 6368/10000: episode: 740, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000926, mae: 0.028125, mean_q: 0.015480
 6378/10000: episode: 741, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001898, mae: 0.028137, mean_q: 0.016070
 6388/10000: episode: 742, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001386, mae: 0.033835, mean_q: 0.021839
 6398/10000: episode: 743, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.003008, mae: 0.030946, mean_q: 0.031430
 6408/10000: episode: 744, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.115, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000991, mae: 0.028953, mean_q: 0.009952
 6418/10000: episode: 745, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.016, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001979, mae: 0.025478, mean_q: 0.021695
 6428/10000: episode: 746, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000803, mae: 0.016410, mean_q: 0.019820
 6438/10000: episode: 747, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001963, mae: 0.024390, mean_q: 0.022406
 6448/10000: episode: 748, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002406, mae: 0.035290, mean_q: 0.022138
 6458/10000: episode: 749, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002363, mae: 0.028446, mean_q: 0.025122
 6468/10000: episode: 750, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.004138, mae: 0.039369, mean_q: 0.029412
 6478/10000: episode: 751, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.003243, mae: 0.036512, mean_q: 0.018978
 6488/10000: episode: 752, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.003123, mae: 0.034519, mean_q: 0.023726
 6498/10000: episode: 753, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001921, mae: 0.033809, mean_q: 0.023646
 6508/10000: episode: 754, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.004514, mae: 0.037782, mean_q: 0.033004
 6518/10000: episode: 755, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002445, mae: 0.028156, mean_q: 0.028486
 6528/10000: episode: 756, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001788, mae: 0.029160, mean_q: 0.020091
 6538/10000: episode: 757, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000514, mae: 0.017484, mean_q: 0.008110
 6548/10000: episode: 758, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.002819, mae: 0.025141, mean_q: 0.018003
 6558/10000: episode: 759, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.001813, mae: 0.029813, mean_q: 0.025593
 6568/10000: episode: 760, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002860, mae: 0.037293, mean_q: 0.022163
 6578/10000: episode: 761, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.002328, mae: 0.025544, mean_q: 0.016948
 6588/10000: episode: 762, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002389, mae: 0.022870, mean_q: 0.026613
 6598/10000: episode: 763, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.004076, mae: 0.040136, mean_q: 0.026738
 6608/10000: episode: 764, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002005, mae: 0.023959, mean_q: 0.030606
 6618/10000: episode: 765, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001596, mae: 0.023394, mean_q: 0.024280
 6628/10000: episode: 766, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001925, mae: 0.034093, mean_q: 0.022420
 6638/10000: episode: 767, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002530, mae: 0.027264, mean_q: 0.024534
 6648/10000: episode: 768, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001871, mae: 0.020537, mean_q: 0.011574
 6658/10000: episode: 769, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.016, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.004703, mae: 0.036899, mean_q: 0.043780
 6668/10000: episode: 770, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001488, mae: 0.025158, mean_q: 0.018911
 6678/10000: episode: 771, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003020, mae: 0.033438, mean_q: 0.028892
 6688/10000: episode: 772, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003226, mae: 0.030308, mean_q: 0.037554
 6698/10000: episode: 773, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.002406, mae: 0.034456, mean_q: 0.015894
 6708/10000: episode: 774, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000734, mae: 0.022821, mean_q: 0.005397
 6718/10000: episode: 775, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002285, mae: 0.029031, mean_q: 0.025182
 6728/10000: episode: 776, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.005036, mae: 0.037651, mean_q: 0.037953
 6738/10000: episode: 777, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001145, mae: 0.031214, mean_q: 0.013883
 6748/10000: episode: 778, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000952, mae: 0.022091, mean_q: 0.025666
 6758/10000: episode: 779, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001988, mae: 0.019770, mean_q: 0.019970
 6768/10000: episode: 780, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002629, mae: 0.030794, mean_q: 0.039358
 6778/10000: episode: 781, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000998, mae: 0.022007, mean_q: 0.011607
 6788/10000: episode: 782, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.002677, mae: 0.029095, mean_q: 0.025973
 6798/10000: episode: 783, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001493, mae: 0.028750, mean_q: 0.009691
 6808/10000: episode: 784, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001660, mae: 0.035008, mean_q: 0.015298
 6818/10000: episode: 785, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001710, mae: 0.031555, mean_q: 0.020823
 6828/10000: episode: 786, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001862, mae: 0.023586, mean_q: 0.030244
 6838/10000: episode: 787, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002394, mae: 0.023159, mean_q: 0.026618
 6848/10000: episode: 788, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002642, mae: 0.031541, mean_q: 0.020174
[Info] FALSIFICATION!
 6858/10000: episode: 789, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.001598, mae: 0.025761, mean_q: 0.018160
 6868/10000: episode: 790, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001385, mae: 0.025788, mean_q: 0.030521
 6878/10000: episode: 791, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.002688, mae: 0.025138, mean_q: 0.027371
 6888/10000: episode: 792, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001127, mae: 0.027649, mean_q: 0.025810
 6898/10000: episode: 793, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.004525, mae: 0.038847, mean_q: 0.031626
 6908/10000: episode: 794, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002445, mae: 0.026165, mean_q: 0.026611
 6918/10000: episode: 795, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000974, mae: 0.019352, mean_q: 0.016695
 6928/10000: episode: 796, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001958, mae: 0.026796, mean_q: 0.026183
 6938/10000: episode: 797, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.003770, mae: 0.039966, mean_q: 0.041884
 6948/10000: episode: 798, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001170, mae: 0.034255, mean_q: 0.007039
 6958/10000: episode: 799, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000669, mae: 0.023157, mean_q: 0.005253
 6968/10000: episode: 800, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000879, mae: 0.020964, mean_q: 0.022097
 6978/10000: episode: 801, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002323, mae: 0.025391, mean_q: 0.035828
 6988/10000: episode: 802, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001153, mae: 0.017676, mean_q: 0.020596
 6998/10000: episode: 803, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000783, mae: 0.018394, mean_q: 0.011369
 7008/10000: episode: 804, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.004547, mae: 0.035017, mean_q: 0.037716
 7018/10000: episode: 805, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002482, mae: 0.028638, mean_q: 0.029837
 7028/10000: episode: 806, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.002096, mae: 0.022763, mean_q: 0.020569
 7038/10000: episode: 807, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002246, mae: 0.033112, mean_q: 0.021624
 7048/10000: episode: 808, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001501, mae: 0.021196, mean_q: 0.014859
 7058/10000: episode: 809, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.003187, mae: 0.029274, mean_q: 0.031344
 7068/10000: episode: 810, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.003230, mae: 0.033013, mean_q: 0.039834
 7078/10000: episode: 811, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.100, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.002087, mae: 0.025584, mean_q: 0.028584
 7088/10000: episode: 812, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.002381, mae: 0.025754, mean_q: 0.026730
 7098/10000: episode: 813, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002680, mae: 0.032389, mean_q: 0.017531
 7108/10000: episode: 814, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001987, mae: 0.037262, mean_q: 0.023883
 7118/10000: episode: 815, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002864, mae: 0.040214, mean_q: 0.027437
 7128/10000: episode: 816, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001566, mae: 0.030375, mean_q: 0.027806
 7138/10000: episode: 817, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001570, mae: 0.026925, mean_q: 0.024951
 7148/10000: episode: 818, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001056, mae: 0.021574, mean_q: 0.028656
 7158/10000: episode: 819, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001075, mae: 0.024337, mean_q: 0.020199
 7168/10000: episode: 820, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.003438, mae: 0.041150, mean_q: 0.030697
 7178/10000: episode: 821, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.700 [-1.000, 10.000], loss: 0.001465, mae: 0.030339, mean_q: 0.020971
 7188/10000: episode: 822, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000809, mae: 0.022386, mean_q: 0.022133
 7198/10000: episode: 823, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001821, mae: 0.028261, mean_q: 0.020602
 7208/10000: episode: 824, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000961, mae: 0.025702, mean_q: 0.010391
 7218/10000: episode: 825, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.002192, mae: 0.030990, mean_q: 0.017317
 7228/10000: episode: 826, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.002300, mae: 0.034383, mean_q: 0.033268
 7238/10000: episode: 827, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001425, mae: 0.021077, mean_q: 0.029766
 7248/10000: episode: 828, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.003114, mae: 0.031705, mean_q: 0.040348
 7258/10000: episode: 829, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002634, mae: 0.028761, mean_q: 0.026881
 7268/10000: episode: 830, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.002507, mae: 0.029180, mean_q: 0.021891
[Info] Complete ISplit Iteration
[Info] Levels: [0.5246364]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 7278/10000: episode: 831, duration: 0.891s, episode steps: 10, steps per second: 11, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.002075, mae: 0.029373, mean_q: 0.015226
 7288/10000: episode: 832, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001027, mae: 0.022610, mean_q: 0.016587
 7298/10000: episode: 833, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.002274, mae: 0.024842, mean_q: 0.032893
 7308/10000: episode: 834, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001472, mae: 0.027722, mean_q: 0.018136
 7318/10000: episode: 835, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.002597, mae: 0.032425, mean_q: 0.035077
 7328/10000: episode: 836, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001617, mae: 0.025396, mean_q: 0.032853
 7338/10000: episode: 837, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000889, mae: 0.018542, mean_q: 0.011910
 7348/10000: episode: 838, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001950, mae: 0.024247, mean_q: 0.026638
 7358/10000: episode: 839, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.003414, mae: 0.033878, mean_q: 0.041902
 7368/10000: episode: 840, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.003191, mae: 0.032834, mean_q: 0.026640
 7378/10000: episode: 841, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001452, mae: 0.026549, mean_q: 0.020877
 7388/10000: episode: 842, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.002321, mae: 0.024379, mean_q: 0.019567
 7398/10000: episode: 843, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001920, mae: 0.025600, mean_q: 0.024753
 7408/10000: episode: 844, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001884, mae: 0.027821, mean_q: 0.035644
 7418/10000: episode: 845, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.003139, mae: 0.027933, mean_q: 0.031425
 7428/10000: episode: 846, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.002309, mae: 0.037813, mean_q: 0.012569
 7438/10000: episode: 847, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.003340, mae: 0.035992, mean_q: 0.047949
 7448/10000: episode: 848, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.003392, mae: 0.039108, mean_q: 0.028284
 7458/10000: episode: 849, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001504, mae: 0.028671, mean_q: 0.008036
 7468/10000: episode: 850, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000698, mae: 0.015949, mean_q: 0.016506
 7478/10000: episode: 851, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002113, mae: 0.024117, mean_q: 0.025802
 7488/10000: episode: 852, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000993, mae: 0.021732, mean_q: 0.021710
 7498/10000: episode: 853, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002493, mae: 0.030414, mean_q: 0.025646
 7508/10000: episode: 854, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001803, mae: 0.024007, mean_q: 0.029950
 7518/10000: episode: 855, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001502, mae: 0.027117, mean_q: 0.021620
 7528/10000: episode: 856, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001809, mae: 0.022509, mean_q: 0.023162
 7538/10000: episode: 857, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000794, mae: 0.020862, mean_q: 0.022480
 7548/10000: episode: 858, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001016, mae: 0.019342, mean_q: 0.022052
 7558/10000: episode: 859, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001269, mae: 0.018039, mean_q: 0.018100
 7568/10000: episode: 860, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000922, mae: 0.019617, mean_q: 0.020395
 7578/10000: episode: 861, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.002286, mae: 0.027139, mean_q: 0.032138
 7588/10000: episode: 862, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002307, mae: 0.030399, mean_q: 0.022140
 7598/10000: episode: 863, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.003227, mae: 0.038702, mean_q: 0.037820
 7608/10000: episode: 864, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001748, mae: 0.025567, mean_q: 0.030004
 7618/10000: episode: 865, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001226, mae: 0.024880, mean_q: 0.020522
 7628/10000: episode: 866, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.221, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.002754, mae: 0.027374, mean_q: 0.028775
 7638/10000: episode: 867, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001687, mae: 0.022038, mean_q: 0.017024
 7648/10000: episode: 868, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002128, mae: 0.027692, mean_q: 0.029303
 7658/10000: episode: 869, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001560, mae: 0.024956, mean_q: 0.031641
 7668/10000: episode: 870, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002178, mae: 0.025905, mean_q: 0.027443
 7678/10000: episode: 871, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002046, mae: 0.026764, mean_q: 0.023029
 7688/10000: episode: 872, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001370, mae: 0.015825, mean_q: 0.015077
 7698/10000: episode: 873, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000860, mae: 0.018476, mean_q: 0.016927
 7708/10000: episode: 874, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001673, mae: 0.018590, mean_q: 0.023408
 7718/10000: episode: 875, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.002383, mae: 0.026184, mean_q: 0.029181
 7728/10000: episode: 876, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002758, mae: 0.031026, mean_q: 0.023550
 7738/10000: episode: 877, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000894, mae: 0.025328, mean_q: 0.010883
 7748/10000: episode: 878, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000736, mae: 0.020224, mean_q: 0.019642
 7758/10000: episode: 879, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.003356, mae: 0.028471, mean_q: 0.031323
 7768/10000: episode: 880, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.002685, mae: 0.032526, mean_q: 0.017565
 7778/10000: episode: 881, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001193, mae: 0.030174, mean_q: 0.019382
 7788/10000: episode: 882, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001714, mae: 0.026808, mean_q: 0.029827
 7798/10000: episode: 883, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001995, mae: 0.023815, mean_q: 0.014985
 7808/10000: episode: 884, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001847, mae: 0.021405, mean_q: 0.027145
 7818/10000: episode: 885, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001780, mae: 0.024378, mean_q: 0.014338
 7828/10000: episode: 886, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001542, mae: 0.023011, mean_q: 0.027009
 7838/10000: episode: 887, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001439, mae: 0.021778, mean_q: 0.018415
 7848/10000: episode: 888, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001820, mae: 0.023713, mean_q: 0.031401
 7858/10000: episode: 889, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001824, mae: 0.020371, mean_q: 0.027728
 7868/10000: episode: 890, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002949, mae: 0.033729, mean_q: 0.021110
 7878/10000: episode: 891, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001560, mae: 0.024112, mean_q: 0.013758
 7888/10000: episode: 892, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002718, mae: 0.036682, mean_q: 0.020051
 7898/10000: episode: 893, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000855, mae: 0.019007, mean_q: 0.016327
 7908/10000: episode: 894, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000616, mae: 0.013408, mean_q: 0.015791
 7918/10000: episode: 895, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001803, mae: 0.021607, mean_q: 0.029454
 7928/10000: episode: 896, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001645, mae: 0.031825, mean_q: 0.013656
 7938/10000: episode: 897, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000868, mae: 0.025339, mean_q: 0.009527
 7948/10000: episode: 898, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001214, mae: 0.026030, mean_q: 0.022008
 7958/10000: episode: 899, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001185, mae: 0.020383, mean_q: 0.018653
 7968/10000: episode: 900, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.002130, mae: 0.026525, mean_q: 0.026394
 7978/10000: episode: 901, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001869, mae: 0.021654, mean_q: 0.031582
 7988/10000: episode: 902, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.002059, mae: 0.020971, mean_q: 0.020937
 7998/10000: episode: 903, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001357, mae: 0.027006, mean_q: 0.015166
 8008/10000: episode: 904, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001861, mae: 0.022767, mean_q: 0.024793
 8018/10000: episode: 905, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.115, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.001863, mae: 0.023043, mean_q: 0.024261
 8028/10000: episode: 906, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001407, mae: 0.028266, mean_q: 0.016324
 8038/10000: episode: 907, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002313, mae: 0.026304, mean_q: 0.023269
 8048/10000: episode: 908, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.002831, mae: 0.027414, mean_q: 0.024647
 8058/10000: episode: 909, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.002295, mae: 0.029048, mean_q: 0.024115
 8068/10000: episode: 910, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.003791, mae: 0.035510, mean_q: 0.034954
 8078/10000: episode: 911, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001090, mae: 0.027614, mean_q: 0.023226
 8088/10000: episode: 912, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.002768, mae: 0.030029, mean_q: 0.030163
 8098/10000: episode: 913, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000772, mae: 0.021914, mean_q: 0.010053
 8108/10000: episode: 914, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001693, mae: 0.022551, mean_q: 0.017858
 8118/10000: episode: 915, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003681, mae: 0.029758, mean_q: 0.033014
 8128/10000: episode: 916, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.002199, mae: 0.037873, mean_q: 0.014461
 8138/10000: episode: 917, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002772, mae: 0.035769, mean_q: 0.023900
 8148/10000: episode: 918, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001261, mae: 0.021642, mean_q: 0.022669
 8158/10000: episode: 919, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001153, mae: 0.024791, mean_q: 0.021231
 8168/10000: episode: 920, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001101, mae: 0.021009, mean_q: 0.026702
 8178/10000: episode: 921, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000779, mae: 0.015478, mean_q: 0.016940
 8188/10000: episode: 922, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.115, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.001029, mae: 0.018345, mean_q: 0.021560
 8198/10000: episode: 923, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.002015, mae: 0.021831, mean_q: 0.030321
 8208/10000: episode: 924, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001761, mae: 0.026066, mean_q: 0.018103
 8218/10000: episode: 925, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001612, mae: 0.025242, mean_q: 0.028146
 8228/10000: episode: 926, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001042, mae: 0.013846, mean_q: 0.019700
 8238/10000: episode: 927, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001358, mae: 0.018365, mean_q: 0.019593
 8248/10000: episode: 928, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000890, mae: 0.016321, mean_q: 0.021604
 8258/10000: episode: 929, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000957, mae: 0.023265, mean_q: 0.021057
 8268/10000: episode: 930, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.004323, mae: 0.036524, mean_q: 0.035493
[Info] 1-TH LEVEL FOUND: 0.003465084359049797, Considering 11/100 traces
 8278/10000: episode: 931, duration: 0.699s, episode steps: 10, steps per second: 14, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001711, mae: 0.027686, mean_q: 0.024296
 8282/10000: episode: 932, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000870, mae: 0.029716, mean_q: -0.010036
 8286/10000: episode: 933, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001301, mae: 0.023030, mean_q: 0.029578
 8294/10000: episode: 934, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.062 [-1.000, 11.000], loss: 0.002615, mae: 0.024811, mean_q: 0.021905
 8302/10000: episode: 935, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.214, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.001937, mae: 0.019633, mean_q: 0.019606
 8310/10000: episode: 936, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.033, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [-1.000, 11.000], loss: 0.002012, mae: 0.023514, mean_q: 0.019008
 8318/10000: episode: 937, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [-1.000, 11.000], loss: 0.000899, mae: 0.024804, mean_q: 0.020460
 8326/10000: episode: 938, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.188 [-1.000, 11.000], loss: 0.002754, mae: 0.038068, mean_q: 0.033226
 8334/10000: episode: 939, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.001586, mae: 0.026650, mean_q: 0.011562
 8342/10000: episode: 940, duration: 0.036s, episode steps: 8, steps per second: 220, episode reward: 0.028, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001664, mae: 0.021201, mean_q: 0.019228
 8350/10000: episode: 941, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.030, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.001786, mae: 0.016906, mean_q: 0.012892
 8358/10000: episode: 942, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.001090, mae: 0.020203, mean_q: 0.024220
 8366/10000: episode: 943, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.938 [-1.000, 11.000], loss: 0.003652, mae: 0.032579, mean_q: 0.046454
 8374/10000: episode: 944, duration: 0.038s, episode steps: 8, steps per second: 209, episode reward: 0.071, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.000994, mae: 0.025421, mean_q: 0.005881
 8382/10000: episode: 945, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.081, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001355, mae: 0.027618, mean_q: 0.013388
 8390/10000: episode: 946, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.000752, mae: 0.020532, mean_q: 0.021352
 8398/10000: episode: 947, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.004026, mae: 0.028338, mean_q: 0.028089
 8406/10000: episode: 948, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 0.128, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.002708, mae: 0.029012, mean_q: 0.025954
 8414/10000: episode: 949, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.022, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.188 [-1.000, 11.000], loss: 0.000975, mae: 0.023903, mean_q: 0.012938
 8418/10000: episode: 950, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000596, mae: 0.020689, mean_q: 0.008916
 8426/10000: episode: 951, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.134, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.002441, mae: 0.029326, mean_q: 0.033639
 8434/10000: episode: 952, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.079, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.000610, mae: 0.020142, mean_q: 0.012204
 8438/10000: episode: 953, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004044, mae: 0.026432, mean_q: 0.029975
 8446/10000: episode: 954, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.015, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.001769, mae: 0.024392, mean_q: 0.035739
 8454/10000: episode: 955, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.130, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.001769, mae: 0.024306, mean_q: 0.019906
 8462/10000: episode: 956, duration: 0.039s, episode steps: 8, steps per second: 204, episode reward: 0.038, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.000522, mae: 0.015689, mean_q: 0.027498
 8470/10000: episode: 957, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.051, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001513, mae: 0.018009, mean_q: 0.023671
 8478/10000: episode: 958, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.220, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.812 [-1.000, 11.000], loss: 0.002063, mae: 0.022151, mean_q: 0.025039
 8486/10000: episode: 959, duration: 0.037s, episode steps: 8, steps per second: 218, episode reward: 0.085, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001753, mae: 0.022843, mean_q: 0.028934
 8494/10000: episode: 960, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.042, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.003576, mae: 0.029793, mean_q: 0.036921
 8502/10000: episode: 961, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.000782, mae: 0.019628, mean_q: 0.010449
 8510/10000: episode: 962, duration: 0.039s, episode steps: 8, steps per second: 203, episode reward: 0.215, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.001074, mae: 0.021927, mean_q: 0.016004
 8518/10000: episode: 963, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 0.079, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.001871, mae: 0.023864, mean_q: 0.029805
 8526/10000: episode: 964, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 0.036, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001187, mae: 0.023656, mean_q: 0.008274
 8534/10000: episode: 965, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.030, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.002418, mae: 0.023715, mean_q: 0.029247
 8542/10000: episode: 966, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001860, mae: 0.019620, mean_q: 0.020002
 8550/10000: episode: 967, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.177, mean reward: 0.022 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.001874, mae: 0.025027, mean_q: 0.025019
 8558/10000: episode: 968, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.013, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.062 [-1.000, 11.000], loss: 0.002185, mae: 0.027774, mean_q: 0.014388
 8566/10000: episode: 969, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.028, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001938, mae: 0.028226, mean_q: 0.027779
 8574/10000: episode: 970, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.054, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.003753, mae: 0.037096, mean_q: 0.052345
 8582/10000: episode: 971, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.231, mean reward: 0.029 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.003520, mae: 0.051843, mean_q: 0.013706
 8590/10000: episode: 972, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.030, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.002417, mae: 0.041540, mean_q: 0.027143
 8598/10000: episode: 973, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.015, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.001863, mae: 0.026850, mean_q: 0.039866
 8606/10000: episode: 974, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.042, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.001376, mae: 0.030875, mean_q: 0.028150
 8614/10000: episode: 975, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.002559, mae: 0.032531, mean_q: 0.014276
 8622/10000: episode: 976, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.220, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.812 [-1.000, 11.000], loss: 0.001819, mae: 0.023064, mean_q: 0.030334
 8630/10000: episode: 977, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.030, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001382, mae: 0.018840, mean_q: 0.029761
 8634/10000: episode: 978, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000933, mae: 0.024272, mean_q: 0.004022
 8642/10000: episode: 979, duration: 0.038s, episode steps: 8, steps per second: 208, episode reward: 0.348, mean reward: 0.044 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.006371, mae: 0.054394, mean_q: 0.058609
 8650/10000: episode: 980, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.049, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.001436, mae: 0.034119, mean_q: 0.004072
 8658/10000: episode: 981, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.130, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.003186, mae: 0.038055, mean_q: 0.039755
 8662/10000: episode: 982, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002353, mae: 0.027455, mean_q: 0.029460
 8670/10000: episode: 983, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.030, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001732, mae: 0.024948, mean_q: 0.030986
 8678/10000: episode: 984, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.028, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001081, mae: 0.019256, mean_q: 0.019847
 8686/10000: episode: 985, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.013, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.938 [-1.000, 11.000], loss: 0.003518, mae: 0.030553, mean_q: 0.035711
 8694/10000: episode: 986, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.049, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.002818, mae: 0.030329, mean_q: 0.039811
 8702/10000: episode: 987, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.081, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001294, mae: 0.021187, mean_q: 0.016244
 8710/10000: episode: 988, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.098, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.001618, mae: 0.020915, mean_q: 0.025020
 8714/10000: episode: 989, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002505, mae: 0.026614, mean_q: 0.040434
 8722/10000: episode: 990, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.130, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.002061, mae: 0.022342, mean_q: 0.017499
 8730/10000: episode: 991, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.042, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.003227, mae: 0.024563, mean_q: 0.030642
 8738/10000: episode: 992, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.812 [-1.000, 11.000], loss: 0.002775, mae: 0.029717, mean_q: 0.015035
 8746/10000: episode: 993, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.215, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.001677, mae: 0.026579, mean_q: 0.025504
 8750/10000: episode: 994, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000722, mae: 0.017796, mean_q: 0.029447
 8758/10000: episode: 995, duration: 0.038s, episode steps: 8, steps per second: 208, episode reward: 0.091, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.001124, mae: 0.026036, mean_q: 0.021492
 8766/10000: episode: 996, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.128, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.002076, mae: 0.027517, mean_q: 0.036583
 8774/10000: episode: 997, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.051, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001580, mae: 0.028614, mean_q: 0.007712
 8782/10000: episode: 998, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.003102, mae: 0.025932, mean_q: 0.030254
 8790/10000: episode: 999, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.079, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.001060, mae: 0.022950, mean_q: 0.020572
 8798/10000: episode: 1000, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.002739, mae: 0.028050, mean_q: 0.038693
 8806/10000: episode: 1001, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.042, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.002143, mae: 0.025643, mean_q: 0.025205
 8810/10000: episode: 1002, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001474, mae: 0.034211, mean_q: -0.006081
 8818/10000: episode: 1003, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.065, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001214, mae: 0.027375, mean_q: 0.043007
 8826/10000: episode: 1004, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001632, mae: 0.038471, mean_q: 0.014179
 8834/10000: episode: 1005, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001907, mae: 0.030393, mean_q: 0.022527
 8838/10000: episode: 1006, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001270, mae: 0.023786, mean_q: 0.033351
 8842/10000: episode: 1007, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001618, mae: 0.035272, mean_q: 0.015383
 8850/10000: episode: 1008, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.035, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001809, mae: 0.032270, mean_q: 0.040045
 8854/10000: episode: 1009, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.004020, mae: 0.050823, mean_q: 0.004465
 8858/10000: episode: 1010, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002341, mae: 0.043370, mean_q: 0.059052
 8866/10000: episode: 1011, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.020, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.001729, mae: 0.031746, mean_q: 0.008301
 8870/10000: episode: 1012, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001792, mae: 0.026823, mean_q: 0.046373
 8878/10000: episode: 1013, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.214, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.003024, mae: 0.032542, mean_q: 0.023485
 8882/10000: episode: 1014, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001511, mae: 0.033897, mean_q: 0.015339
 8890/10000: episode: 1015, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.085, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001237, mae: 0.029694, mean_q: 0.028074
 8894/10000: episode: 1016, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001911, mae: 0.026182, mean_q: 0.013962
 8902/10000: episode: 1017, duration: 0.036s, episode steps: 8, steps per second: 220, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.062 [-1.000, 11.000], loss: 0.001617, mae: 0.023302, mean_q: 0.026582
 8910/10000: episode: 1018, duration: 0.038s, episode steps: 8, steps per second: 208, episode reward: 0.032, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.004126, mae: 0.032300, mean_q: 0.029214
 8914/10000: episode: 1019, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.003099, mae: 0.027824, mean_q: 0.034853
[Info] 2-TH LEVEL FOUND: 0.16228902339935303, Considering 17/100 traces
 8922/10000: episode: 1020, duration: 0.675s, episode steps: 8, steps per second: 12, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.002057, mae: 0.029858, mean_q: 0.029676
 8924/10000: episode: 1021, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005567, mae: 0.035110, mean_q: 0.029881
 8929/10000: episode: 1022, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001883, mae: 0.027575, mean_q: 0.044689
 8931/10000: episode: 1023, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001284, mae: 0.022173, mean_q: 0.024227
 8933/10000: episode: 1024, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.004340, mae: 0.023753, mean_q: 0.029175
 8938/10000: episode: 1025, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001315, mae: 0.022662, mean_q: 0.034163
 8943/10000: episode: 1026, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.003169, mae: 0.030260, mean_q: 0.040112
 8948/10000: episode: 1027, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.149, mean reward: 0.030 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001121, mae: 0.020445, mean_q: 0.010081
 8950/10000: episode: 1028, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003393, mae: 0.032245, mean_q: 0.017365
 8952/10000: episode: 1029, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002195, mae: 0.039857, mean_q: 0.058093
 8954/10000: episode: 1030, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003328, mae: 0.030355, mean_q: 0.035161
 8956/10000: episode: 1031, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000896, mae: 0.018205, mean_q: 0.013922
 8958/10000: episode: 1032, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002465, mae: 0.021419, mean_q: 0.030821
 8963/10000: episode: 1033, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.003427, mae: 0.035981, mean_q: 0.046354
 8965/10000: episode: 1034, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001255, mae: 0.038413, mean_q: -0.010146
 8967/10000: episode: 1035, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001215, mae: 0.029809, mean_q: 0.006320
 8969/10000: episode: 1036, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000268, mae: 0.015982, mean_q: 0.024417
 8971/10000: episode: 1037, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002587, mae: 0.047749, mean_q: 0.061332
 8973/10000: episode: 1038, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001350, mae: 0.020722, mean_q: 0.021208
 8975/10000: episode: 1039, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001193, mae: 0.030777, mean_q: 0.002462
 8977/10000: episode: 1040, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002086, mae: 0.038565, mean_q: -0.004047
 8979/10000: episode: 1041, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005261, mae: 0.037082, mean_q: 0.042260
 8981/10000: episode: 1042, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001969, mae: 0.049477, mean_q: 0.055585
 8986/10000: episode: 1043, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002423, mae: 0.029386, mean_q: 0.019956
 8988/10000: episode: 1044, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003308, mae: 0.039209, mean_q: 0.029619
 8993/10000: episode: 1045, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.001698, mae: 0.029736, mean_q: 0.033711
 8995/10000: episode: 1046, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001643, mae: 0.036585, mean_q: -0.003175
 8997/10000: episode: 1047, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001841, mae: 0.034729, mean_q: 0.045948
 9002/10000: episode: 1048, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.149, mean reward: 0.030 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.003123, mae: 0.030631, mean_q: 0.024228
 9004/10000: episode: 1049, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001904, mae: 0.029024, mean_q: 0.034562
 9006/10000: episode: 1050, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003007, mae: 0.026245, mean_q: 0.031405
 9008/10000: episode: 1051, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002620, mae: 0.038485, mean_q: 0.026376
 9013/10000: episode: 1052, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001277, mae: 0.023368, mean_q: 0.030106
 9015/10000: episode: 1053, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002332, mae: 0.033334, mean_q: 0.045375
 9017/10000: episode: 1054, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000902, mae: 0.037303, mean_q: -0.029592
 9019/10000: episode: 1055, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006648, mae: 0.038469, mean_q: 0.013061
 9021/10000: episode: 1056, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001548, mae: 0.044620, mean_q: 0.060282
 9026/10000: episode: 1057, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001306, mae: 0.036039, mean_q: 0.007271
 9031/10000: episode: 1058, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.005900, mae: 0.052939, mean_q: 0.050115
 9033/10000: episode: 1059, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005707, mae: 0.048126, mean_q: 0.054266
 9035/10000: episode: 1060, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002078, mae: 0.047066, mean_q: -0.007966
 9040/10000: episode: 1061, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001106, mae: 0.027651, mean_q: 0.010483
 9042/10000: episode: 1062, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006108, mae: 0.043018, mean_q: 0.037814
 9044/10000: episode: 1063, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002347, mae: 0.035862, mean_q: 0.048790
 9046/10000: episode: 1064, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.007155, mae: 0.039807, mean_q: 0.028688
 9048/10000: episode: 1065, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000576, mae: 0.021081, mean_q: 0.031605
 9053/10000: episode: 1066, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.001478, mae: 0.030525, mean_q: 0.028419
 9055/10000: episode: 1067, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000482, mae: 0.017460, mean_q: 0.011345
[Info] FALSIFICATION!
 9059/10000: episode: 1068, duration: 0.174s, episode steps: 4, steps per second: 23, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.002524, mae: 0.047110, mean_q: 0.056480
 9061/10000: episode: 1069, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001188, mae: 0.040171, mean_q: -0.012129
 9063/10000: episode: 1070, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001888, mae: 0.045479, mean_q: -0.011453
 9068/10000: episode: 1071, duration: 0.026s, episode steps: 5, steps per second: 191, episode reward: 0.149, mean reward: 0.030 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.004911, mae: 0.052465, mean_q: 0.066405
 9070/10000: episode: 1072, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001722, mae: 0.027330, mean_q: 0.007355
 9072/10000: episode: 1073, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000621, mae: 0.026219, mean_q: 0.002313
 9074/10000: episode: 1074, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005342, mae: 0.036765, mean_q: 0.039508
 9076/10000: episode: 1075, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002526, mae: 0.051162, mean_q: 0.077112
 9081/10000: episode: 1076, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.003678, mae: 0.042583, mean_q: -0.000329
[Info] FALSIFICATION!
 9085/10000: episode: 1077, duration: 0.173s, episode steps: 4, steps per second: 23, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.001099, mae: 0.024285, mean_q: 0.028953
 9087/10000: episode: 1078, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001394, mae: 0.032509, mean_q: 0.048580
 9092/10000: episode: 1079, duration: 0.026s, episode steps: 5, steps per second: 192, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001680, mae: 0.032359, mean_q: 0.031065
 9094/10000: episode: 1080, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001155, mae: 0.024651, mean_q: 0.027150
 9099/10000: episode: 1081, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000696, mae: 0.018200, mean_q: 0.025342
 9101/10000: episode: 1082, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002700, mae: 0.021403, mean_q: 0.020447
 9103/10000: episode: 1083, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000834, mae: 0.019774, mean_q: 0.027691
 9105/10000: episode: 1084, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000764, mae: 0.016454, mean_q: 0.022324
[Info] FALSIFICATION!
 9109/10000: episode: 1085, duration: 0.263s, episode steps: 4, steps per second: 15, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.001327, mae: 0.029960, mean_q: 0.018934
 9111/10000: episode: 1086, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000955, mae: 0.020542, mean_q: 0.013221
 9113/10000: episode: 1087, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000218, mae: 0.015614, mean_q: 0.026731
 9118/10000: episode: 1088, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002742, mae: 0.023198, mean_q: 0.029636
 9123/10000: episode: 1089, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000546, mae: 0.017562, mean_q: 0.011182
 9128/10000: episode: 1090, duration: 0.024s, episode steps: 5, steps per second: 204, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.003040, mae: 0.027243, mean_q: 0.021847
 9130/10000: episode: 1091, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001172, mae: 0.030444, mean_q: 0.043451
 9132/10000: episode: 1092, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001045, mae: 0.023857, mean_q: 0.035935
 9134/10000: episode: 1093, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000742, mae: 0.025952, mean_q: 0.002283
 9136/10000: episode: 1094, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.012302, mae: 0.055601, mean_q: 0.041444
 9141/10000: episode: 1095, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.001754, mae: 0.036865, mean_q: 0.049337
 9146/10000: episode: 1096, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.004235, mae: 0.044125, mean_q: 0.003384
 9151/10000: episode: 1097, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.006236, mae: 0.052886, mean_q: 0.064354
 9153/10000: episode: 1098, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001405, mae: 0.028342, mean_q: 0.038322
 9155/10000: episode: 1099, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002056, mae: 0.055591, mean_q: -0.025592
 9157/10000: episode: 1100, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001076, mae: 0.030001, mean_q: 0.013547
 9159/10000: episode: 1101, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001852, mae: 0.038404, mean_q: 0.049735
 9164/10000: episode: 1102, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.003957, mae: 0.028754, mean_q: 0.024536
[Info] Complete ISplit Iteration
[Info] Levels: [0.0034650844, 0.16228902, 0.5819121]
[Info] Cond. Prob: [0.11, 0.17, 0.03]
[Info] Error Prob: 0.000561

 9166/10000: episode: 1103, duration: 0.831s, episode steps: 2, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000655, mae: 0.018179, mean_q: 0.022492
 9176/10000: episode: 1104, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001622, mae: 0.029328, mean_q: 0.027567
 9186/10000: episode: 1105, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.084, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001853, mae: 0.029215, mean_q: 0.030718
 9196/10000: episode: 1106, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002223, mae: 0.032257, mean_q: 0.033858
 9206/10000: episode: 1107, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001444, mae: 0.022868, mean_q: 0.025273
 9216/10000: episode: 1108, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002408, mae: 0.027942, mean_q: 0.030620
 9226/10000: episode: 1109, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001033, mae: 0.025124, mean_q: 0.023329
 9236/10000: episode: 1110, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.002249, mae: 0.023331, mean_q: 0.025514
 9246/10000: episode: 1111, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.002238, mae: 0.029767, mean_q: 0.025866
 9256/10000: episode: 1112, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001449, mae: 0.025206, mean_q: 0.019550
 9266/10000: episode: 1113, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001107, mae: 0.028325, mean_q: 0.027637
 9276/10000: episode: 1114, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001858, mae: 0.026027, mean_q: 0.031501
 9286/10000: episode: 1115, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001035, mae: 0.027435, mean_q: 0.019186
 9296/10000: episode: 1116, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002820, mae: 0.025740, mean_q: 0.024713
 9306/10000: episode: 1117, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001778, mae: 0.023337, mean_q: 0.024355
 9316/10000: episode: 1118, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001844, mae: 0.025020, mean_q: 0.026752
 9326/10000: episode: 1119, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.002369, mae: 0.028718, mean_q: 0.028789
 9336/10000: episode: 1120, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001977, mae: 0.025575, mean_q: 0.032889
 9346/10000: episode: 1121, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.003556, mae: 0.035096, mean_q: 0.028844
 9356/10000: episode: 1122, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001483, mae: 0.027147, mean_q: 0.028734
 9366/10000: episode: 1123, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.002245, mae: 0.025567, mean_q: 0.029585
 9376/10000: episode: 1124, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.003141, mae: 0.035697, mean_q: 0.025861
 9386/10000: episode: 1125, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002070, mae: 0.033268, mean_q: 0.027389
 9396/10000: episode: 1126, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000966, mae: 0.022722, mean_q: 0.021830
 9406/10000: episode: 1127, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000860, mae: 0.020624, mean_q: 0.019634
 9416/10000: episode: 1128, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.003172, mae: 0.026010, mean_q: 0.027349
 9426/10000: episode: 1129, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.002389, mae: 0.030433, mean_q: 0.031297
 9436/10000: episode: 1130, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.002607, mae: 0.025576, mean_q: 0.032445
 9446/10000: episode: 1131, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.003419, mae: 0.052026, mean_q: 0.029111
 9456/10000: episode: 1132, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.002215, mae: 0.033688, mean_q: 0.018041
 9466/10000: episode: 1133, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003894, mae: 0.030768, mean_q: 0.035738
 9476/10000: episode: 1134, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001976, mae: 0.024814, mean_q: 0.026272
 9486/10000: episode: 1135, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002390, mae: 0.027251, mean_q: 0.015437
 9496/10000: episode: 1136, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.003302, mae: 0.039146, mean_q: 0.032129
 9506/10000: episode: 1137, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002475, mae: 0.048385, mean_q: 0.018591
 9516/10000: episode: 1138, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001858, mae: 0.036089, mean_q: 0.025668
 9526/10000: episode: 1139, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.003590, mae: 0.040694, mean_q: 0.048175
 9536/10000: episode: 1140, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.003029, mae: 0.035696, mean_q: 0.030636
 9546/10000: episode: 1141, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.004205, mae: 0.036635, mean_q: 0.029737
 9556/10000: episode: 1142, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002942, mae: 0.032465, mean_q: 0.043340
 9566/10000: episode: 1143, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001127, mae: 0.026203, mean_q: 0.016673
 9576/10000: episode: 1144, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.002323, mae: 0.024367, mean_q: 0.031347
 9586/10000: episode: 1145, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.003245, mae: 0.038744, mean_q: 0.041511
 9596/10000: episode: 1146, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000893, mae: 0.023176, mean_q: 0.019227
 9606/10000: episode: 1147, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.002900, mae: 0.029153, mean_q: 0.031374
 9616/10000: episode: 1148, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001815, mae: 0.028166, mean_q: 0.023382
 9626/10000: episode: 1149, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.004965, mae: 0.041210, mean_q: 0.037012
 9636/10000: episode: 1150, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.002779, mae: 0.037480, mean_q: 0.017706
 9646/10000: episode: 1151, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001321, mae: 0.025852, mean_q: 0.012385
 9656/10000: episode: 1152, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001838, mae: 0.028183, mean_q: 0.024934
 9666/10000: episode: 1153, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001636, mae: 0.026103, mean_q: 0.030588
 9676/10000: episode: 1154, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001451, mae: 0.023191, mean_q: 0.023369
 9686/10000: episode: 1155, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.035, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001164, mae: 0.025893, mean_q: 0.020246
 9696/10000: episode: 1156, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001316, mae: 0.033230, mean_q: 0.020791
 9706/10000: episode: 1157, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000806, mae: 0.020822, mean_q: 0.031095
 9716/10000: episode: 1158, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.002564, mae: 0.029859, mean_q: 0.040559
 9726/10000: episode: 1159, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001924, mae: 0.031706, mean_q: 0.018414
 9736/10000: episode: 1160, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.002770, mae: 0.027460, mean_q: 0.027697
 9746/10000: episode: 1161, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001765, mae: 0.025640, mean_q: 0.027175
 9756/10000: episode: 1162, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001924, mae: 0.025816, mean_q: 0.026232
 9766/10000: episode: 1163, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001660, mae: 0.025009, mean_q: 0.031769
 9776/10000: episode: 1164, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.003602, mae: 0.040132, mean_q: 0.038505
 9786/10000: episode: 1165, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.002760, mae: 0.036678, mean_q: 0.033562
 9796/10000: episode: 1166, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001154, mae: 0.026022, mean_q: 0.015539
 9806/10000: episode: 1167, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.003969, mae: 0.030561, mean_q: 0.033256
 9816/10000: episode: 1168, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.002681, mae: 0.027442, mean_q: 0.035481
 9826/10000: episode: 1169, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001257, mae: 0.024215, mean_q: 0.023229
 9836/10000: episode: 1170, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002136, mae: 0.025865, mean_q: 0.032147
 9846/10000: episode: 1171, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001925, mae: 0.033157, mean_q: 0.031006
 9856/10000: episode: 1172, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001185, mae: 0.023560, mean_q: 0.020456
 9866/10000: episode: 1173, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001466, mae: 0.021042, mean_q: 0.026713
 9876/10000: episode: 1174, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.003569, mae: 0.031372, mean_q: 0.028484
 9886/10000: episode: 1175, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.001070, mae: 0.022659, mean_q: 0.028774
 9896/10000: episode: 1176, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000851, mae: 0.017477, mean_q: 0.027804
 9906/10000: episode: 1177, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001239, mae: 0.027733, mean_q: 0.023957
 9916/10000: episode: 1178, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000940, mae: 0.022167, mean_q: 0.026706
 9926/10000: episode: 1179, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000922, mae: 0.021194, mean_q: 0.022192
 9936/10000: episode: 1180, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.004692, mae: 0.032966, mean_q: 0.031714
 9946/10000: episode: 1181, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001975, mae: 0.032383, mean_q: 0.036789
 9956/10000: episode: 1182, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000989, mae: 0.024221, mean_q: 0.016912
 9966/10000: episode: 1183, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001215, mae: 0.022659, mean_q: 0.023699
 9976/10000: episode: 1184, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.002206, mae: 0.026971, mean_q: 0.034439
 9986/10000: episode: 1185, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001062, mae: 0.025215, mean_q: 0.014038
 9996/10000: episode: 1186, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001481, mae: 0.021835, mean_q: 0.033185
done, took 62.517 seconds
[Info] End Importance Splitting. Falsification occurred 16 times.
