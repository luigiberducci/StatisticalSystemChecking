Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                48        
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting on succruns-v1.
Training for 1000000 steps ...
     10/1000000: episode: 1, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.084, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     20/1000000: episode: 2, duration: 0.005s, episode steps: 10, steps per second: 2042, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     30/1000000: episode: 3, duration: 0.005s, episode steps: 10, steps per second: 1987, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     40/1000000: episode: 4, duration: 0.005s, episode steps: 10, steps per second: 2049, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     50/1000000: episode: 5, duration: 0.005s, episode steps: 10, steps per second: 2053, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     60/1000000: episode: 6, duration: 0.005s, episode steps: 10, steps per second: 2051, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     70/1000000: episode: 7, duration: 0.005s, episode steps: 10, steps per second: 2140, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     80/1000000: episode: 8, duration: 0.005s, episode steps: 10, steps per second: 2070, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     90/1000000: episode: 9, duration: 0.005s, episode steps: 10, steps per second: 2085, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    100/1000000: episode: 10, duration: 0.005s, episode steps: 10, steps per second: 2138, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    110/1000000: episode: 11, duration: 0.005s, episode steps: 10, steps per second: 2105, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    120/1000000: episode: 12, duration: 0.005s, episode steps: 10, steps per second: 2139, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    130/1000000: episode: 13, duration: 0.005s, episode steps: 10, steps per second: 2127, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    140/1000000: episode: 14, duration: 0.005s, episode steps: 10, steps per second: 2119, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    150/1000000: episode: 15, duration: 0.004s, episode steps: 10, steps per second: 2246, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    160/1000000: episode: 16, duration: 0.004s, episode steps: 10, steps per second: 2228, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    170/1000000: episode: 17, duration: 0.004s, episode steps: 10, steps per second: 2259, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    180/1000000: episode: 18, duration: 0.004s, episode steps: 10, steps per second: 2259, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    190/1000000: episode: 19, duration: 0.004s, episode steps: 10, steps per second: 2249, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    200/1000000: episode: 20, duration: 0.004s, episode steps: 10, steps per second: 2232, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    210/1000000: episode: 21, duration: 0.004s, episode steps: 10, steps per second: 2251, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    220/1000000: episode: 22, duration: 0.004s, episode steps: 10, steps per second: 2248, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    230/1000000: episode: 23, duration: 0.004s, episode steps: 10, steps per second: 2234, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    240/1000000: episode: 24, duration: 0.004s, episode steps: 10, steps per second: 2252, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    250/1000000: episode: 25, duration: 0.005s, episode steps: 10, steps per second: 2216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    260/1000000: episode: 26, duration: 0.004s, episode steps: 10, steps per second: 2241, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    270/1000000: episode: 27, duration: 0.004s, episode steps: 10, steps per second: 2245, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    280/1000000: episode: 28, duration: 0.004s, episode steps: 10, steps per second: 2257, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    290/1000000: episode: 29, duration: 0.004s, episode steps: 10, steps per second: 2259, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    300/1000000: episode: 30, duration: 0.004s, episode steps: 10, steps per second: 2225, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    310/1000000: episode: 31, duration: 0.004s, episode steps: 10, steps per second: 2262, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    320/1000000: episode: 32, duration: 0.004s, episode steps: 10, steps per second: 2256, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    330/1000000: episode: 33, duration: 0.004s, episode steps: 10, steps per second: 2245, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    340/1000000: episode: 34, duration: 0.005s, episode steps: 10, steps per second: 2082, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    350/1000000: episode: 35, duration: 0.005s, episode steps: 10, steps per second: 2144, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    360/1000000: episode: 36, duration: 0.005s, episode steps: 10, steps per second: 2123, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    370/1000000: episode: 37, duration: 0.007s, episode steps: 10, steps per second: 1441, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    380/1000000: episode: 38, duration: 0.004s, episode steps: 10, steps per second: 2243, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    390/1000000: episode: 39, duration: 0.005s, episode steps: 10, steps per second: 2169, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    400/1000000: episode: 40, duration: 0.005s, episode steps: 10, steps per second: 2211, episode reward: 0.221, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    410/1000000: episode: 41, duration: 0.005s, episode steps: 10, steps per second: 2098, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    420/1000000: episode: 42, duration: 0.005s, episode steps: 10, steps per second: 2135, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    430/1000000: episode: 43, duration: 0.005s, episode steps: 10, steps per second: 2119, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    440/1000000: episode: 44, duration: 0.005s, episode steps: 10, steps per second: 2156, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    450/1000000: episode: 45, duration: 0.005s, episode steps: 10, steps per second: 2168, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    460/1000000: episode: 46, duration: 0.005s, episode steps: 10, steps per second: 2188, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    470/1000000: episode: 47, duration: 0.005s, episode steps: 10, steps per second: 2128, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    480/1000000: episode: 48, duration: 0.005s, episode steps: 10, steps per second: 2152, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    490/1000000: episode: 49, duration: 0.005s, episode steps: 10, steps per second: 2172, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    500/1000000: episode: 50, duration: 0.005s, episode steps: 10, steps per second: 2180, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    510/1000000: episode: 51, duration: 0.593s, episode steps: 10, steps per second: 17, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.417796, mae: 0.587698, mean_q: -1.231166
    520/1000000: episode: 52, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.243932, mae: 0.405339, mean_q: -0.977799
    530/1000000: episode: 53, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.162349, mae: 0.323801, mean_q: -0.756261
    540/1000000: episode: 54, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.105229, mae: 0.297583, mean_q: -0.586506
    550/1000000: episode: 55, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.102760, mae: 0.297371, mean_q: -0.516810
    560/1000000: episode: 56, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.077138, mae: 0.252143, mean_q: -0.473161
    570/1000000: episode: 57, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.085573, mae: 0.265932, mean_q: -0.495707
    580/1000000: episode: 58, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.056022, mae: 0.209698, mean_q: -0.424824
    590/1000000: episode: 59, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.051941, mae: 0.195761, mean_q: -0.412934
    600/1000000: episode: 60, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.045223, mae: 0.186480, mean_q: -0.387798
    610/1000000: episode: 61, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.039925, mae: 0.174029, mean_q: -0.398163
    620/1000000: episode: 62, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.031668, mae: 0.149329, mean_q: -0.347681
    630/1000000: episode: 63, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.028914, mae: 0.144563, mean_q: -0.326586
    640/1000000: episode: 64, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.023348, mae: 0.135721, mean_q: -0.301011
    650/1000000: episode: 65, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.016079, mae: 0.102649, mean_q: -0.311047
    660/1000000: episode: 66, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.023951, mae: 0.121977, mean_q: -0.301981
    670/1000000: episode: 67, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.017459, mae: 0.109823, mean_q: -0.264777
    680/1000000: episode: 68, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.018582, mae: 0.112028, mean_q: -0.278207
    690/1000000: episode: 69, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.016034, mae: 0.114360, mean_q: -0.241901
    700/1000000: episode: 70, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.011438, mae: 0.092439, mean_q: -0.233059
    710/1000000: episode: 71, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.013591, mae: 0.097692, mean_q: -0.240759
    720/1000000: episode: 72, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.010418, mae: 0.096835, mean_q: -0.198367
    730/1000000: episode: 73, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.009151, mae: 0.090641, mean_q: -0.190087
    740/1000000: episode: 74, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.009758, mae: 0.091216, mean_q: -0.181560
    750/1000000: episode: 75, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.012871, mae: 0.099144, mean_q: -0.190871
    760/1000000: episode: 76, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.006552, mae: 0.076689, mean_q: -0.161173
    770/1000000: episode: 77, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.009843, mae: 0.088810, mean_q: -0.152873
    780/1000000: episode: 78, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.006719, mae: 0.073357, mean_q: -0.133179
    790/1000000: episode: 79, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.006163, mae: 0.070728, mean_q: -0.137327
    800/1000000: episode: 80, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.004254, mae: 0.061784, mean_q: -0.121216
    810/1000000: episode: 81, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.003145, mae: 0.054890, mean_q: -0.122695
    820/1000000: episode: 82, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.003212, mae: 0.050298, mean_q: -0.116005
    830/1000000: episode: 83, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.002693, mae: 0.048944, mean_q: -0.105556
    840/1000000: episode: 84, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001911, mae: 0.042047, mean_q: -0.097512
    850/1000000: episode: 85, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001627, mae: 0.040956, mean_q: -0.095949
    860/1000000: episode: 86, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001872, mae: 0.043368, mean_q: -0.086149
    870/1000000: episode: 87, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001650, mae: 0.037792, mean_q: -0.098910
    880/1000000: episode: 88, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001282, mae: 0.034679, mean_q: -0.078896
    890/1000000: episode: 89, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001143, mae: 0.033059, mean_q: -0.075126
    900/1000000: episode: 90, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000700, mae: 0.027482, mean_q: -0.070517
    910/1000000: episode: 91, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001071, mae: 0.030223, mean_q: -0.072103
    920/1000000: episode: 92, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000762, mae: 0.028025, mean_q: -0.064085
    930/1000000: episode: 93, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000635, mae: 0.025086, mean_q: -0.062065
    940/1000000: episode: 94, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000551, mae: 0.023007, mean_q: -0.058299
    950/1000000: episode: 95, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000408, mae: 0.021429, mean_q: -0.053719
    960/1000000: episode: 96, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000405, mae: 0.020123, mean_q: -0.053043
    970/1000000: episode: 97, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000349, mae: 0.018683, mean_q: -0.048391
    980/1000000: episode: 98, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000339, mae: 0.019214, mean_q: -0.048936
    990/1000000: episode: 99, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000227, mae: 0.016467, mean_q: -0.039655
   1000/1000000: episode: 100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000199, mae: 0.015267, mean_q: -0.041251
   1010/1000000: episode: 101, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000170, mae: 0.014553, mean_q: -0.037128
   1020/1000000: episode: 102, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000197, mae: 0.013720, mean_q: -0.035879
   1030/1000000: episode: 103, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000215, mae: 0.014322, mean_q: -0.032252
   1040/1000000: episode: 104, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000139, mae: 0.012049, mean_q: -0.030088
   1050/1000000: episode: 105, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000190, mae: 0.012676, mean_q: -0.029119
   1060/1000000: episode: 106, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000193, mae: 0.012637, mean_q: -0.025746
   1070/1000000: episode: 107, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000107, mae: 0.010682, mean_q: -0.022990
   1080/1000000: episode: 108, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000158, mae: 0.011267, mean_q: -0.021861
   1090/1000000: episode: 109, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000099, mae: 0.009889, mean_q: -0.020603
   1100/1000000: episode: 110, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000122, mae: 0.010277, mean_q: -0.019918
   1110/1000000: episode: 111, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000072, mae: 0.009220, mean_q: -0.018529
   1120/1000000: episode: 112, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000086, mae: 0.008265, mean_q: -0.017027
   1130/1000000: episode: 113, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000125, mae: 0.008704, mean_q: -0.015632
   1140/1000000: episode: 114, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000044, mae: 0.006699, mean_q: -0.014725
   1150/1000000: episode: 115, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000088, mae: 0.008173, mean_q: -0.013921
   1160/1000000: episode: 116, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000039, mae: 0.006460, mean_q: -0.012428
   1170/1000000: episode: 117, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000068, mae: 0.007469, mean_q: -0.010531
   1180/1000000: episode: 118, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000040, mae: 0.006446, mean_q: -0.011309
   1190/1000000: episode: 119, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000036, mae: 0.006337, mean_q: -0.009710
   1200/1000000: episode: 120, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000033, mae: 0.006244, mean_q: -0.009318
   1210/1000000: episode: 121, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000060, mae: 0.006241, mean_q: -0.008640
   1220/1000000: episode: 122, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000058, mae: 0.006404, mean_q: -0.007582
   1230/1000000: episode: 123, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000061, mae: 0.006760, mean_q: -0.005809
   1240/1000000: episode: 124, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000042, mae: 0.006281, mean_q: -0.005011
   1250/1000000: episode: 125, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000038, mae: 0.006050, mean_q: -0.005720
   1260/1000000: episode: 126, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000076, mae: 0.006389, mean_q: -0.004212
   1270/1000000: episode: 127, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000030, mae: 0.005328, mean_q: -0.004620
   1280/1000000: episode: 128, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000095, mae: 0.006732, mean_q: -0.003413
   1290/1000000: episode: 129, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000041, mae: 0.004777, mean_q: -0.003367
   1300/1000000: episode: 130, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [-1.000, 10.000], loss: 0.000027, mae: 0.005020, mean_q: -0.002633
   1310/1000000: episode: 131, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.029, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000043, mae: 0.005247, mean_q: -0.002312
   1320/1000000: episode: 132, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000048, mae: 0.005244, mean_q: -0.000888
   1330/1000000: episode: 133, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000019, mae: 0.004529, mean_q: -0.001502
   1340/1000000: episode: 134, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000021, mae: 0.004555, mean_q: -0.000596
   1350/1000000: episode: 135, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.088, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000045, mae: 0.005155, mean_q: 0.000253
   1360/1000000: episode: 136, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000045, mae: 0.005118, mean_q: -0.000120
   1370/1000000: episode: 137, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000041, mae: 0.004985, mean_q: 0.000147
   1380/1000000: episode: 138, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000043, mae: 0.005099, mean_q: -0.000310
   1390/1000000: episode: 139, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000024, mae: 0.004668, mean_q: 0.002057
   1400/1000000: episode: 140, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.088, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000043, mae: 0.005095, mean_q: 0.000878
   1410/1000000: episode: 141, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000029, mae: 0.004658, mean_q: 0.002311
   1420/1000000: episode: 142, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.215, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000042, mae: 0.004826, mean_q: 0.001611
   1430/1000000: episode: 143, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000100, mae: 0.005945, mean_q: 0.003165
   1440/1000000: episode: 144, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000045, mae: 0.005015, mean_q: 0.002744
   1450/1000000: episode: 145, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000045, mae: 0.004990, mean_q: 0.003473
   1460/1000000: episode: 146, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000072, mae: 0.005237, mean_q: 0.003287
   1470/1000000: episode: 147, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000040, mae: 0.004412, mean_q: 0.002719
   1480/1000000: episode: 148, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000036, mae: 0.004219, mean_q: 0.002636
   1490/1000000: episode: 149, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000036, mae: 0.004209, mean_q: 0.002520
[Info] 1-TH LEVEL FOUND: 0.13034968078136444, Considering 100/100 traces
   1500/1000000: episode: 150, duration: 1.063s, episode steps: 10, steps per second: 9, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000039, mae: 0.004659, mean_q: 0.003199
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.13034968078136444
   1501/1000000: episode: 151, duration: 0.464s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.000 [-1.000, 11.000], loss: 0.000025, mae: 0.004342, mean_q: 0.007495
   1511/1000000: episode: 152, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000078, mae: 0.005239, mean_q: 0.003602
   1521/1000000: episode: 153, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000040, mae: 0.004211, mean_q: 0.003729
   1531/1000000: episode: 154, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000033, mae: 0.004013, mean_q: 0.003705
   1541/1000000: episode: 155, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000043, mae: 0.004130, mean_q: 0.004616
   1551/1000000: episode: 156, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000095, mae: 0.006098, mean_q: 0.005376
   1561/1000000: episode: 157, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000032, mae: 0.004077, mean_q: 0.003295
   1571/1000000: episode: 158, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000101, mae: 0.005192, mean_q: 0.003871
   1581/1000000: episode: 159, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000062, mae: 0.004936, mean_q: 0.004588
   1591/1000000: episode: 160, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000069, mae: 0.004551, mean_q: 0.003585
   1601/1000000: episode: 161, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000019, mae: 0.003835, mean_q: 0.003734
   1611/1000000: episode: 162, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000015, mae: 0.003362, mean_q: 0.003605
   1621/1000000: episode: 163, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000018, mae: 0.003614, mean_q: 0.004534
   1631/1000000: episode: 164, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000041, mae: 0.004029, mean_q: 0.004723
   1641/1000000: episode: 165, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000019, mae: 0.003569, mean_q: 0.004166
   1651/1000000: episode: 166, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000054, mae: 0.004005, mean_q: 0.005271
   1661/1000000: episode: 167, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000020, mae: 0.003532, mean_q: 0.004129
   1671/1000000: episode: 168, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000020, mae: 0.003556, mean_q: 0.004076
   1681/1000000: episode: 169, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000041, mae: 0.003835, mean_q: 0.004768
   1691/1000000: episode: 170, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000020, mae: 0.003690, mean_q: 0.004123
   1701/1000000: episode: 171, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000096, mae: 0.004761, mean_q: 0.004983
   1711/1000000: episode: 172, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000023, mae: 0.003678, mean_q: 0.005398
   1721/1000000: episode: 173, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000039, mae: 0.004190, mean_q: 0.004927
   1731/1000000: episode: 174, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000080, mae: 0.004556, mean_q: 0.004912
   1741/1000000: episode: 175, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000080, mae: 0.004779, mean_q: 0.005593
   1751/1000000: episode: 176, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000059, mae: 0.003986, mean_q: 0.006019
   1761/1000000: episode: 177, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000039, mae: 0.004217, mean_q: 0.005410
   1771/1000000: episode: 178, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000070, mae: 0.004975, mean_q: 0.005484
   1781/1000000: episode: 179, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000084, mae: 0.004912, mean_q: 0.006235
   1791/1000000: episode: 180, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000048, mae: 0.004381, mean_q: 0.006208
   1801/1000000: episode: 181, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000040, mae: 0.004052, mean_q: 0.005812
   1811/1000000: episode: 182, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000041, mae: 0.003973, mean_q: 0.005437
   1821/1000000: episode: 183, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000033, mae: 0.003452, mean_q: 0.004625
   1831/1000000: episode: 184, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000022, mae: 0.004039, mean_q: 0.006037
   1841/1000000: episode: 185, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000016, mae: 0.003385, mean_q: 0.005099
   1851/1000000: episode: 186, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000025, mae: 0.003937, mean_q: 0.005553
   1861/1000000: episode: 187, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000057, mae: 0.004336, mean_q: 0.005811
   1871/1000000: episode: 188, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000019, mae: 0.003356, mean_q: 0.005386
   1881/1000000: episode: 189, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000041, mae: 0.003920, mean_q: 0.005595
   1891/1000000: episode: 190, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000019, mae: 0.003278, mean_q: 0.005013
   1901/1000000: episode: 191, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000092, mae: 0.004543, mean_q: 0.005693
   1911/1000000: episode: 192, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000089, mae: 0.005302, mean_q: 0.006557
   1921/1000000: episode: 193, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000056, mae: 0.004017, mean_q: 0.005024
   1931/1000000: episode: 194, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000037, mae: 0.003630, mean_q: 0.005739
   1941/1000000: episode: 195, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000036, mae: 0.003745, mean_q: 0.006075
   1951/1000000: episode: 196, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000023, mae: 0.003904, mean_q: 0.005909
   1961/1000000: episode: 197, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000042, mae: 0.004193, mean_q: 0.005417
   1971/1000000: episode: 198, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000036, mae: 0.003425, mean_q: 0.005358
   1981/1000000: episode: 199, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000040, mae: 0.003997, mean_q: 0.005859
   1991/1000000: episode: 200, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000063, mae: 0.004381, mean_q: 0.006145
   2001/1000000: episode: 201, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000019, mae: 0.003561, mean_q: 0.005720
   2011/1000000: episode: 202, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000057, mae: 0.004137, mean_q: 0.005606
   2021/1000000: episode: 203, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000040, mae: 0.003975, mean_q: 0.006005
   2031/1000000: episode: 204, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000043, mae: 0.004272, mean_q: 0.005837
   2041/1000000: episode: 205, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000087, mae: 0.004806, mean_q: 0.005629
   2051/1000000: episode: 206, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000043, mae: 0.004025, mean_q: 0.005517
   2061/1000000: episode: 207, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000060, mae: 0.004193, mean_q: 0.005888
   2071/1000000: episode: 208, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000040, mae: 0.003970, mean_q: 0.006705
   2081/1000000: episode: 209, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000063, mae: 0.004645, mean_q: 0.005809
   2091/1000000: episode: 210, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000075, mae: 0.004308, mean_q: 0.006170
   2101/1000000: episode: 211, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000044, mae: 0.003972, mean_q: 0.006301
   2111/1000000: episode: 212, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000020, mae: 0.003553, mean_q: 0.006046
   2121/1000000: episode: 213, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000020, mae: 0.003731, mean_q: 0.005043
   2131/1000000: episode: 214, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.215, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000014, mae: 0.002954, mean_q: 0.004753
   2141/1000000: episode: 215, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000063, mae: 0.004186, mean_q: 0.006080
   2151/1000000: episode: 216, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000047, mae: 0.004580, mean_q: 0.007343
   2161/1000000: episode: 217, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000093, mae: 0.004647, mean_q: 0.006043
   2171/1000000: episode: 218, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000045, mae: 0.003907, mean_q: 0.005260
   2181/1000000: episode: 219, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000060, mae: 0.004303, mean_q: 0.005690
   2191/1000000: episode: 220, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000047, mae: 0.004526, mean_q: 0.006051
   2201/1000000: episode: 221, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000063, mae: 0.004640, mean_q: 0.006318
   2211/1000000: episode: 222, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000020, mae: 0.003637, mean_q: 0.005364
   2221/1000000: episode: 223, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000018, mae: 0.003577, mean_q: 0.005428
   2231/1000000: episode: 224, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000016, mae: 0.003145, mean_q: 0.005085
   2241/1000000: episode: 225, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000018, mae: 0.003307, mean_q: 0.006016
   2251/1000000: episode: 226, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000056, mae: 0.004051, mean_q: 0.006259
   2261/1000000: episode: 227, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000028, mae: 0.003977, mean_q: 0.006801
   2271/1000000: episode: 228, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000056, mae: 0.003854, mean_q: 0.006129
   2281/1000000: episode: 229, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000083, mae: 0.004605, mean_q: 0.005746
   2291/1000000: episode: 230, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000095, mae: 0.004286, mean_q: 0.006092
   2301/1000000: episode: 231, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000035, mae: 0.003309, mean_q: 0.005679
   2311/1000000: episode: 232, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000012, mae: 0.002873, mean_q: 0.005847
   2321/1000000: episode: 233, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000064, mae: 0.004395, mean_q: 0.005426
   2331/1000000: episode: 234, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000023, mae: 0.003563, mean_q: 0.006151
   2341/1000000: episode: 235, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000043, mae: 0.004123, mean_q: 0.006241
   2351/1000000: episode: 236, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000054, mae: 0.003611, mean_q: 0.006137
   2361/1000000: episode: 237, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000041, mae: 0.003930, mean_q: 0.006339
   2371/1000000: episode: 238, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000019, mae: 0.003712, mean_q: 0.005919
   2381/1000000: episode: 239, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000044, mae: 0.003950, mean_q: 0.006438
   2391/1000000: episode: 240, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000063, mae: 0.004244, mean_q: 0.006408
   2401/1000000: episode: 241, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000039, mae: 0.003918, mean_q: 0.006355
   2411/1000000: episode: 242, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000020, mae: 0.003254, mean_q: 0.006200
   2421/1000000: episode: 243, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000041, mae: 0.004156, mean_q: 0.005737
   2431/1000000: episode: 244, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000064, mae: 0.004908, mean_q: 0.006613
   2441/1000000: episode: 245, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000015, mae: 0.003253, mean_q: 0.006081
   2451/1000000: episode: 246, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000040, mae: 0.003938, mean_q: 0.005517
   2461/1000000: episode: 247, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000018, mae: 0.003620, mean_q: 0.005844
   2471/1000000: episode: 248, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000031, mae: 0.003345, mean_q: 0.006083
   2481/1000000: episode: 249, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000059, mae: 0.003632, mean_q: 0.005951
   2491/1000000: episode: 250, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000016, mae: 0.002914, mean_q: 0.004885
[Info] 1-TH LEVEL FOUND: 0.03127110004425049, Considering 31/100 traces
   2501/1000000: episode: 251, duration: 0.661s, episode steps: 10, steps per second: 15, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000059, mae: 0.003712, mean_q: 0.005925
   2507/1000000: episode: 252, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000019, mae: 0.003545, mean_q: 0.006462
   2511/1000000: episode: 253, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000068, mae: 0.004289, mean_q: 0.008283
   2517/1000000: episode: 254, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000020, mae: 0.003540, mean_q: 0.005880
   2523/1000000: episode: 255, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000050, mae: 0.003867, mean_q: 0.005987
   2529/1000000: episode: 256, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000027, mae: 0.003935, mean_q: 0.006860
   2533/1000000: episode: 257, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000021, mae: 0.003220, mean_q: 0.005146
   2539/1000000: episode: 258, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000018, mae: 0.003360, mean_q: 0.005238
   2545/1000000: episode: 259, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000079, mae: 0.004044, mean_q: 0.006610
   2551/1000000: episode: 260, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000059, mae: 0.003991, mean_q: 0.005945
   2557/1000000: episode: 261, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000018, mae: 0.003211, mean_q: 0.007112
   2563/1000000: episode: 262, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000020, mae: 0.003438, mean_q: 0.005772
   2569/1000000: episode: 263, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000111, mae: 0.004643, mean_q: 0.005338
   2575/1000000: episode: 264, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000018, mae: 0.003358, mean_q: 0.005915
   2581/1000000: episode: 265, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000017, mae: 0.003346, mean_q: 0.005990
   2587/1000000: episode: 266, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000015, mae: 0.003067, mean_q: 0.005668
   2593/1000000: episode: 267, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000058, mae: 0.004194, mean_q: 0.006316
   2599/1000000: episode: 268, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000107, mae: 0.005503, mean_q: 0.006374
[Info] FALSIFICATION!
   2604/1000000: episode: 269, duration: 0.427s, episode steps: 5, steps per second: 12, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000023, mae: 0.003688, mean_q: 0.004678
   2610/1000000: episode: 270, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000023, mae: 0.003860, mean_q: 0.006182
   2614/1000000: episode: 271, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000067, mae: 0.003953, mean_q: 0.005898
   2620/1000000: episode: 272, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000022, mae: 0.003972, mean_q: 0.006773
   2626/1000000: episode: 273, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000026, mae: 0.004035, mean_q: 0.005381
   2632/1000000: episode: 274, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.002535, mae: 0.009627, mean_q: 0.005787
   2638/1000000: episode: 275, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000057, mae: 0.004063, mean_q: 0.006098
   2644/1000000: episode: 276, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000064, mae: 0.004760, mean_q: 0.007982
   2650/1000000: episode: 277, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000020, mae: 0.003550, mean_q: 0.005699
   2656/1000000: episode: 278, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000086, mae: 0.004506, mean_q: 0.006136
   2662/1000000: episode: 279, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000053, mae: 0.004305, mean_q: 0.005876
   2668/1000000: episode: 280, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000022, mae: 0.003586, mean_q: 0.006965
   2672/1000000: episode: 281, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.014, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000025, mae: 0.003695, mean_q: 0.006568
   2678/1000000: episode: 282, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000096, mae: 0.004706, mean_q: 0.006335
   2684/1000000: episode: 283, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000086, mae: 0.004820, mean_q: 0.007989
   2690/1000000: episode: 284, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.023, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000036, mae: 0.004773, mean_q: 0.007778
   2696/1000000: episode: 285, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000155, mae: 0.005574, mean_q: 0.006126
   2702/1000000: episode: 286, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000096, mae: 0.005130, mean_q: 0.007179
   2708/1000000: episode: 287, duration: 0.029s, episode steps: 6, steps per second: 203, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000086, mae: 0.004906, mean_q: 0.008013
   2714/1000000: episode: 288, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000085, mae: 0.004781, mean_q: 0.005597
   2720/1000000: episode: 289, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000084, mae: 0.005113, mean_q: 0.005281
   2726/1000000: episode: 290, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000160, mae: 0.006519, mean_q: 0.007593
   2732/1000000: episode: 291, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000071, mae: 0.005578, mean_q: 0.008649
   2738/1000000: episode: 292, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.168, mean reward: 0.028 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.002447, mae: 0.009297, mean_q: 0.006144
   2744/1000000: episode: 293, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000059, mae: 0.004969, mean_q: 0.006849
   2750/1000000: episode: 294, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000088, mae: 0.004768, mean_q: 0.008134
   2756/1000000: episode: 295, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000052, mae: 0.003740, mean_q: 0.006950
   2762/1000000: episode: 296, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000021, mae: 0.003617, mean_q: 0.006473
   2768/1000000: episode: 297, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000022, mae: 0.003715, mean_q: 0.007708
   2772/1000000: episode: 298, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000134, mae: 0.005902, mean_q: 0.006857
   2778/1000000: episode: 299, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000060, mae: 0.004656, mean_q: 0.006798
   2782/1000000: episode: 300, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.025, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000118, mae: 0.005306, mean_q: 0.007762
   2788/1000000: episode: 301, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000029, mae: 0.004232, mean_q: 0.008390
   2792/1000000: episode: 302, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000029, mae: 0.003931, mean_q: 0.007264
   2796/1000000: episode: 303, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.014, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000034, mae: 0.004207, mean_q: 0.007063
   2802/1000000: episode: 304, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000122, mae: 0.005684, mean_q: 0.006596
   2806/1000000: episode: 305, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000090, mae: 0.005523, mean_q: 0.007004
   2810/1000000: episode: 306, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.025, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000025, mae: 0.003874, mean_q: 0.006735
   2816/1000000: episode: 307, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.023, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000026, mae: 0.004050, mean_q: 0.007342
   2822/1000000: episode: 308, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000027, mae: 0.004213, mean_q: 0.006532
   2828/1000000: episode: 309, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000085, mae: 0.005381, mean_q: 0.007072
   2834/1000000: episode: 310, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.002490, mae: 0.011016, mean_q: 0.007530
   2840/1000000: episode: 311, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000143, mae: 0.008754, mean_q: 0.008267
   2844/1000000: episode: 312, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.025, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000083, mae: 0.008038, mean_q: 0.011289
   2848/1000000: episode: 313, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.014, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000038, mae: 0.005071, mean_q: 0.005219
   2854/1000000: episode: 314, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000102, mae: 0.007538, mean_q: 0.008293
   2860/1000000: episode: 315, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000158, mae: 0.008289, mean_q: 0.008568
   2866/1000000: episode: 316, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000083, mae: 0.006159, mean_q: 0.007771
   2872/1000000: episode: 317, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000069, mae: 0.006777, mean_q: 0.008280
   2878/1000000: episode: 318, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000120, mae: 0.006765, mean_q: 0.006798
   2882/1000000: episode: 319, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.014, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000127, mae: 0.005696, mean_q: 0.008922
[Info] Complete ISplit Iteration
[Info] Levels: [0.0312711, 0.03999848]
[Info] Cond. Prob: [0.31, 0.6]
[Info] Error Prob: 0.186

   2888/1000000: episode: 320, duration: 0.882s, episode steps: 6, steps per second: 7, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000154, mae: 0.007783, mean_q: 0.008517
   2898/1000000: episode: 321, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000251, mae: 0.006398, mean_q: 0.007501
   2908/1000000: episode: 322, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000100, mae: 0.005946, mean_q: 0.008692
   2918/1000000: episode: 323, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000064, mae: 0.005439, mean_q: 0.007845
   2928/1000000: episode: 324, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000092, mae: 0.005479, mean_q: 0.006884
   2938/1000000: episode: 325, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000147, mae: 0.007381, mean_q: 0.008442
   2948/1000000: episode: 326, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000107, mae: 0.006486, mean_q: 0.006929
   2958/1000000: episode: 327, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000075, mae: 0.005866, mean_q: 0.007085
   2968/1000000: episode: 328, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000110, mae: 0.006656, mean_q: 0.009032
   2978/1000000: episode: 329, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000281, mae: 0.006621, mean_q: 0.007339
   2988/1000000: episode: 330, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000060, mae: 0.005314, mean_q: 0.007505
   2998/1000000: episode: 331, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000072, mae: 0.005142, mean_q: 0.007372
   3008/1000000: episode: 332, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000057, mae: 0.004750, mean_q: 0.006820
   3018/1000000: episode: 333, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000069, mae: 0.005441, mean_q: 0.007722
   3028/1000000: episode: 334, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000066, mae: 0.005117, mean_q: 0.008725
   3038/1000000: episode: 335, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000101, mae: 0.005586, mean_q: 0.007365
   3048/1000000: episode: 336, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000137, mae: 0.006452, mean_q: 0.007150
   3058/1000000: episode: 337, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000108, mae: 0.007006, mean_q: 0.008915
   3068/1000000: episode: 338, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001565, mae: 0.010098, mean_q: 0.007501
   3078/1000000: episode: 339, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000060, mae: 0.005416, mean_q: 0.008045
   3088/1000000: episode: 340, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000081, mae: 0.005655, mean_q: 0.007799
   3098/1000000: episode: 341, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000057, mae: 0.004578, mean_q: 0.006721
   3108/1000000: episode: 342, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000081, mae: 0.006496, mean_q: 0.007832
   3118/1000000: episode: 343, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000106, mae: 0.006234, mean_q: 0.007948
   3128/1000000: episode: 344, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000272, mae: 0.006109, mean_q: 0.007311
   3138/1000000: episode: 345, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000084, mae: 0.005682, mean_q: 0.008685
   3148/1000000: episode: 346, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000064, mae: 0.005711, mean_q: 0.008089
   3158/1000000: episode: 347, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000312, mae: 0.007535, mean_q: 0.008814
   3168/1000000: episode: 348, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000141, mae: 0.006657, mean_q: 0.007643
   3178/1000000: episode: 349, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000077, mae: 0.005394, mean_q: 0.008252
   3188/1000000: episode: 350, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001461, mae: 0.008144, mean_q: 0.007705
   3198/1000000: episode: 351, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000084, mae: 0.005704, mean_q: 0.007746
   3208/1000000: episode: 352, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001513, mae: 0.009304, mean_q: 0.009077
   3218/1000000: episode: 353, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001486, mae: 0.009130, mean_q: 0.008318
   3228/1000000: episode: 354, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000097, mae: 0.006326, mean_q: 0.007886
   3238/1000000: episode: 355, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000118, mae: 0.007059, mean_q: 0.008419
   3248/1000000: episode: 356, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000073, mae: 0.005461, mean_q: 0.008123
   3258/1000000: episode: 357, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001455, mae: 0.007841, mean_q: 0.006467
   3268/1000000: episode: 358, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000113, mae: 0.006473, mean_q: 0.008543
   3278/1000000: episode: 359, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000094, mae: 0.005443, mean_q: 0.007498
   3288/1000000: episode: 360, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000106, mae: 0.005753, mean_q: 0.007365
   3298/1000000: episode: 361, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000080, mae: 0.006265, mean_q: 0.008650
   3308/1000000: episode: 362, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000067, mae: 0.006244, mean_q: 0.007593
   3318/1000000: episode: 363, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000108, mae: 0.006162, mean_q: 0.008517
   3328/1000000: episode: 364, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000032, mae: 0.004472, mean_q: 0.007449
   3338/1000000: episode: 365, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000112, mae: 0.006659, mean_q: 0.009129
   3348/1000000: episode: 366, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001474, mae: 0.008449, mean_q: 0.007715
   3358/1000000: episode: 367, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001519, mae: 0.009967, mean_q: 0.009615
   3368/1000000: episode: 368, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000144, mae: 0.007872, mean_q: 0.007754
   3378/1000000: episode: 369, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001458, mae: 0.008410, mean_q: 0.006987
   3388/1000000: episode: 370, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000054, mae: 0.005469, mean_q: 0.006905
   3398/1000000: episode: 371, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000055, mae: 0.005382, mean_q: 0.007231
   3408/1000000: episode: 372, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000095, mae: 0.006321, mean_q: 0.006009
   3418/1000000: episode: 373, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000082, mae: 0.005607, mean_q: 0.006947
   3428/1000000: episode: 374, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000130, mae: 0.006581, mean_q: 0.007516
   3438/1000000: episode: 375, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000099, mae: 0.006409, mean_q: 0.007271
   3448/1000000: episode: 376, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000107, mae: 0.005994, mean_q: 0.007611
   3458/1000000: episode: 377, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000084, mae: 0.006242, mean_q: 0.006836
   3468/1000000: episode: 378, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000071, mae: 0.005181, mean_q: 0.006690
   3478/1000000: episode: 379, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000102, mae: 0.006132, mean_q: 0.007815
   3488/1000000: episode: 380, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000056, mae: 0.005866, mean_q: 0.006121
   3498/1000000: episode: 381, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000059, mae: 0.005568, mean_q: 0.006452
   3508/1000000: episode: 382, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000073, mae: 0.006569, mean_q: 0.006247
   3518/1000000: episode: 383, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000061, mae: 0.005348, mean_q: 0.005792
   3528/1000000: episode: 384, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000090, mae: 0.005664, mean_q: 0.006716
   3538/1000000: episode: 385, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000090, mae: 0.005587, mean_q: 0.007182
   3548/1000000: episode: 386, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000047, mae: 0.004469, mean_q: 0.005189
   3558/1000000: episode: 387, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000037, mae: 0.005264, mean_q: 0.007456
   3568/1000000: episode: 388, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000045, mae: 0.004297, mean_q: 0.006335
   3578/1000000: episode: 389, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000208, mae: 0.005138, mean_q: 0.006120
   3588/1000000: episode: 390, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001586, mae: 0.008463, mean_q: 0.006914
   3598/1000000: episode: 391, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000081, mae: 0.005970, mean_q: 0.008514
   3608/1000000: episode: 392, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000054, mae: 0.005160, mean_q: 0.006861
   3618/1000000: episode: 393, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000098, mae: 0.007002, mean_q: 0.006895
   3628/1000000: episode: 394, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000085, mae: 0.006261, mean_q: 0.007211
   3638/1000000: episode: 395, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000226, mae: 0.005870, mean_q: 0.006750
   3648/1000000: episode: 396, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000043, mae: 0.003792, mean_q: 0.005511
   3658/1000000: episode: 397, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000098, mae: 0.005432, mean_q: 0.008418
   3668/1000000: episode: 398, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [-1.000, 10.000], loss: 0.000041, mae: 0.004258, mean_q: 0.005864
   3678/1000000: episode: 399, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000093, mae: 0.005652, mean_q: 0.007978
   3688/1000000: episode: 400, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000250, mae: 0.006851, mean_q: 0.007731
   3698/1000000: episode: 401, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001452, mae: 0.008523, mean_q: 0.008131
   3708/1000000: episode: 402, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000057, mae: 0.005320, mean_q: 0.007084
   3718/1000000: episode: 403, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000036, mae: 0.004022, mean_q: 0.005333
   3728/1000000: episode: 404, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000056, mae: 0.005463, mean_q: 0.007434
   3738/1000000: episode: 405, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000270, mae: 0.007046, mean_q: 0.007975
   3748/1000000: episode: 406, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000060, mae: 0.005148, mean_q: 0.007818
   3758/1000000: episode: 407, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000090, mae: 0.005492, mean_q: 0.006899
   3768/1000000: episode: 408, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000060, mae: 0.005287, mean_q: 0.006797
   3778/1000000: episode: 409, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001427, mae: 0.008781, mean_q: 0.008476
   3788/1000000: episode: 410, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000238, mae: 0.006541, mean_q: 0.007814
   3798/1000000: episode: 411, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001571, mae: 0.009541, mean_q: 0.007486
   3808/1000000: episode: 412, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000113, mae: 0.007270, mean_q: 0.009276
   3818/1000000: episode: 413, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.131, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000057, mae: 0.005623, mean_q: 0.006435
   3828/1000000: episode: 414, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000092, mae: 0.006173, mean_q: 0.007508
   3838/1000000: episode: 415, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000047, mae: 0.004613, mean_q: 0.006828
   3848/1000000: episode: 416, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001439, mae: 0.010147, mean_q: 0.008800
   3858/1000000: episode: 417, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000081, mae: 0.006732, mean_q: 0.007530
   3868/1000000: episode: 418, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000089, mae: 0.007154, mean_q: 0.009094
   3878/1000000: episode: 419, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000103, mae: 0.007573, mean_q: 0.007010
[Info] 1-TH LEVEL FOUND: 0.0331365130841732, Considering 11/100 traces
   3888/1000000: episode: 420, duration: 0.690s, episode steps: 10, steps per second: 14, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000048, mae: 0.005012, mean_q: 0.007149
   3892/1000000: episode: 421, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000079, mae: 0.006302, mean_q: 0.010626
   3894/1000000: episode: 422, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000033, mae: 0.005894, mean_q: 0.003545
   3900/1000000: episode: 423, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000092, mae: 0.006053, mean_q: 0.008891
   3902/1000000: episode: 424, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000046, mae: 0.004799, mean_q: 0.003964
   3906/1000000: episode: 425, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000049, mae: 0.005771, mean_q: 0.008478
   3908/1000000: episode: 426, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000269, mae: 0.008783, mean_q: 0.008026
   3914/1000000: episode: 427, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000121, mae: 0.005718, mean_q: 0.008605
   3916/1000000: episode: 428, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000014, mae: 0.002649, mean_q: 0.006040
   3922/1000000: episode: 429, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000163, mae: 0.008750, mean_q: 0.011671
   3928/1000000: episode: 430, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.168, mean reward: 0.028 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.002332, mae: 0.012318, mean_q: 0.010240
   3934/1000000: episode: 431, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000083, mae: 0.005193, mean_q: 0.006369
   3940/1000000: episode: 432, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000157, mae: 0.006986, mean_q: 0.009887
   3942/1000000: episode: 433, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000043, mae: 0.004890, mean_q: 0.007915
   3948/1000000: episode: 434, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000300, mae: 0.005831, mean_q: 0.007536
   3950/1000000: episode: 435, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000143, mae: 0.005685, mean_q: 0.005863
   3954/1000000: episode: 436, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000033, mae: 0.005016, mean_q: 0.009170
   3960/1000000: episode: 437, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000046, mae: 0.004815, mean_q: 0.007010
   3962/1000000: episode: 438, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000119, mae: 0.005228, mean_q: 0.005580
   3968/1000000: episode: 439, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000074, mae: 0.005410, mean_q: 0.009048
   3970/1000000: episode: 440, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000024, mae: 0.004050, mean_q: 0.007059
   3976/1000000: episode: 441, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000046, mae: 0.004740, mean_q: 0.007734
   3980/1000000: episode: 442, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000205, mae: 0.008519, mean_q: 0.010269
   3984/1000000: episode: 443, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000095, mae: 0.006517, mean_q: 0.006888
   3986/1000000: episode: 444, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000108, mae: 0.006207, mean_q: 0.011682
   3988/1000000: episode: 445, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000050, mae: 0.005310, mean_q: 0.009031
   3990/1000000: episode: 446, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000142, mae: 0.007365, mean_q: 0.004295
   3996/1000000: episode: 447, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000060, mae: 0.005215, mean_q: 0.006315
   4000/1000000: episode: 448, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000138, mae: 0.007216, mean_q: 0.010549
   4002/1000000: episode: 449, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000933, mae: 0.012355, mean_q: 0.013439
   4006/1000000: episode: 450, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000120, mae: 0.006640, mean_q: 0.007440
   4008/1000000: episode: 451, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000058, mae: 0.007192, mean_q: 0.005099
   4012/1000000: episode: 452, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000050, mae: 0.004546, mean_q: 0.007240
   4016/1000000: episode: 453, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000029, mae: 0.004763, mean_q: 0.006107
   4022/1000000: episode: 454, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000052, mae: 0.005480, mean_q: 0.008031
   4024/1000000: episode: 455, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000026, mae: 0.004550, mean_q: 0.005944
   4028/1000000: episode: 456, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000086, mae: 0.005947, mean_q: 0.008825
   4030/1000000: episode: 457, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000112, mae: 0.006570, mean_q: 0.010003
   4032/1000000: episode: 458, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000099, mae: 0.005124, mean_q: 0.006357
   4034/1000000: episode: 459, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000138, mae: 0.006345, mean_q: 0.012628
   4038/1000000: episode: 460, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000097, mae: 0.006928, mean_q: 0.007882
   4040/1000000: episode: 461, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000224, mae: 0.008452, mean_q: 0.012613
   4044/1000000: episode: 462, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000180, mae: 0.008521, mean_q: 0.008347
   4048/1000000: episode: 463, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000072, mae: 0.006089, mean_q: 0.010358
   4050/1000000: episode: 464, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000052, mae: 0.006331, mean_q: 0.001067
   4052/1000000: episode: 465, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000229, mae: 0.011610, mean_q: 0.015632
   4056/1000000: episode: 466, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000142, mae: 0.010049, mean_q: 0.007332
   4058/1000000: episode: 467, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000058, mae: 0.006472, mean_q: 0.006933
   4060/1000000: episode: 468, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000090, mae: 0.008563, mean_q: 0.012077
   4062/1000000: episode: 469, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000078, mae: 0.010377, mean_q: -0.002668
   4064/1000000: episode: 470, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000042, mae: 0.006081, mean_q: 0.009269
   4066/1000000: episode: 471, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000260, mae: 0.010527, mean_q: 0.011824
   4072/1000000: episode: 472, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000094, mae: 0.007270, mean_q: 0.008485
   4074/1000000: episode: 473, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000116, mae: 0.006524, mean_q: 0.010896
   4078/1000000: episode: 474, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000119, mae: 0.006906, mean_q: 0.010264
   4082/1000000: episode: 475, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000468, mae: 0.007580, mean_q: 0.008346
   4088/1000000: episode: 476, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000354, mae: 0.008800, mean_q: 0.009457
   4090/1000000: episode: 477, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000107, mae: 0.006434, mean_q: 0.010568
   4096/1000000: episode: 478, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000102, mae: 0.006456, mean_q: 0.007261
   4098/1000000: episode: 479, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000091, mae: 0.006781, mean_q: 0.013700
   4100/1000000: episode: 480, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000194, mae: 0.008241, mean_q: 0.007717
   4102/1000000: episode: 481, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000134, mae: 0.007819, mean_q: 0.013517
   4104/1000000: episode: 482, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000191, mae: 0.008856, mean_q: 0.011761
   4106/1000000: episode: 483, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000136, mae: 0.008310, mean_q: 0.003957
   4112/1000000: episode: 484, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000092, mae: 0.008090, mean_q: 0.008760
   4114/1000000: episode: 485, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000139, mae: 0.007562, mean_q: 0.011845
   4116/1000000: episode: 486, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000059, mae: 0.005639, mean_q: 0.008922
   4120/1000000: episode: 487, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000065, mae: 0.004547, mean_q: 0.004361
   4126/1000000: episode: 488, duration: 0.028s, episode steps: 6, steps per second: 213, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000126, mae: 0.007461, mean_q: 0.008064
   4130/1000000: episode: 489, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000090, mae: 0.006663, mean_q: 0.010189
   4132/1000000: episode: 490, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000099, mae: 0.005829, mean_q: 0.007954
   4138/1000000: episode: 491, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000102, mae: 0.007608, mean_q: 0.009660
   4144/1000000: episode: 492, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000088, mae: 0.005723, mean_q: 0.006401
   4146/1000000: episode: 493, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000049, mae: 0.005016, mean_q: 0.007778
   4152/1000000: episode: 494, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000062, mae: 0.004694, mean_q: 0.006655
   4154/1000000: episode: 495, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000109, mae: 0.005318, mean_q: 0.007835
   4156/1000000: episode: 496, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000179, mae: 0.008736, mean_q: 0.012711
   4158/1000000: episode: 497, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000067, mae: 0.006363, mean_q: 0.006845
   4164/1000000: episode: 498, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000079, mae: 0.005440, mean_q: 0.007520
   4166/1000000: episode: 499, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000120, mae: 0.006024, mean_q: 0.007420
   4168/1000000: episode: 500, duration: 0.012s, episode steps: 2, steps per second: 168, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000091, mae: 0.005826, mean_q: 0.008429
   4172/1000000: episode: 501, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000082, mae: 0.005098, mean_q: 0.008319
   4174/1000000: episode: 502, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000070, mae: 0.007621, mean_q: 0.003959
   4176/1000000: episode: 503, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000092, mae: 0.005923, mean_q: 0.008067
   4182/1000000: episode: 504, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000077, mae: 0.006351, mean_q: 0.005381
   4184/1000000: episode: 505, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000948, mae: 0.013019, mean_q: 0.013553
   4190/1000000: episode: 506, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000103, mae: 0.005892, mean_q: 0.008390
   4192/1000000: episode: 507, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000046, mae: 0.005558, mean_q: 0.007080
   4194/1000000: episode: 508, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000079, mae: 0.005473, mean_q: 0.004941
[Info] 2-TH LEVEL FOUND: 0.08144206553697586, Considering 14/100 traces
   4196/1000000: episode: 509, duration: 0.642s, episode steps: 2, steps per second: 3, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000085, mae: 0.007159, mean_q: 0.009948
   4198/1000000: episode: 510, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000092, mae: 0.006166, mean_q: 0.004866
   4200/1000000: episode: 511, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000081, mae: 0.005886, mean_q: 0.007556
   4203/1000000: episode: 512, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 1.737, mean reward: 0.579 [0.368, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.667 [9.000, 11.000], loss: 0.000080, mae: 0.005523, mean_q: 0.007567
   4205/1000000: episode: 513, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000244, mae: 0.007493, mean_q: 0.007227
   4208/1000000: episode: 514, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 1.737, mean reward: 0.579 [0.368, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.667 [9.000, 11.000], loss: 0.000180, mae: 0.008285, mean_q: 0.012882
   4210/1000000: episode: 515, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000133, mae: 0.007934, mean_q: 0.006194
   4212/1000000: episode: 516, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000122, mae: 0.008422, mean_q: 0.013752
   4214/1000000: episode: 517, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000159, mae: 0.008642, mean_q: 0.009104
   4216/1000000: episode: 518, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000120, mae: 0.005964, mean_q: 0.007598
   4218/1000000: episode: 519, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.006763, mean_q: 0.009982
   4220/1000000: episode: 520, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000150, mae: 0.007229, mean_q: 0.008283
   4222/1000000: episode: 521, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000067, mae: 0.007058, mean_q: 0.012339
   4224/1000000: episode: 522, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000022, mae: 0.004905, mean_q: 0.004676
   4226/1000000: episode: 523, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000155, mae: 0.009385, mean_q: 0.008419
   4229/1000000: episode: 524, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 1.737, mean reward: 0.579 [0.368, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.667 [9.000, 11.000], loss: 0.000174, mae: 0.009219, mean_q: 0.011212
   4231/1000000: episode: 525, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000245, mae: 0.010769, mean_q: 0.009586
   4233/1000000: episode: 526, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000198, mae: 0.010599, mean_q: 0.012442
   4235/1000000: episode: 527, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000045, mae: 0.005935, mean_q: 0.010437
   4238/1000000: episode: 528, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 1.104, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 11.000], loss: 0.000042, mae: 0.005488, mean_q: 0.005669
   4240/1000000: episode: 529, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000083, mae: 0.007507, mean_q: 0.009157
   4242/1000000: episode: 530, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000935, mae: 0.011226, mean_q: 0.008686
   4244/1000000: episode: 531, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000096, mae: 0.006589, mean_q: 0.006827
   4246/1000000: episode: 532, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.007686, mean_q: 0.011790
   4248/1000000: episode: 533, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000086, mae: 0.005102, mean_q: 0.009779
   4250/1000000: episode: 534, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000070, mae: 0.006710, mean_q: 0.005627
   4252/1000000: episode: 535, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000250, mae: 0.009534, mean_q: 0.009377
   4254/1000000: episode: 536, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000091, mae: 0.005927, mean_q: 0.011046
   4256/1000000: episode: 537, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000984, mae: 0.012087, mean_q: 0.010693
   4258/1000000: episode: 538, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000044, mae: 0.006101, mean_q: 0.005189
   4260/1000000: episode: 539, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000952, mae: 0.012602, mean_q: 0.009921
   4262/1000000: episode: 540, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000102, mae: 0.008412, mean_q: 0.011102
   4264/1000000: episode: 541, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001013, mae: 0.012968, mean_q: 0.005993
   4266/1000000: episode: 542, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000082, mae: 0.008141, mean_q: 0.015013
   4268/1000000: episode: 543, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000238, mae: 0.010553, mean_q: 0.009430
   4270/1000000: episode: 544, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000094, mae: 0.008105, mean_q: 0.013358
   4272/1000000: episode: 545, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000153, mae: 0.007068, mean_q: 0.011127
   4274/1000000: episode: 546, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000986, mae: 0.013698, mean_q: 0.006979
   4276/1000000: episode: 547, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000128, mae: 0.008305, mean_q: 0.014877
   4278/1000000: episode: 548, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006647, mae: 0.020860, mean_q: 0.009230
   4280/1000000: episode: 549, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001001, mae: 0.012737, mean_q: 0.007686
   4282/1000000: episode: 550, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000050, mae: 0.006364, mean_q: 0.009353
   4284/1000000: episode: 551, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000159, mae: 0.007989, mean_q: 0.010442
   4286/1000000: episode: 552, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000110, mae: 0.007687, mean_q: 0.010236
   4288/1000000: episode: 553, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000087, mae: 0.006082, mean_q: 0.008145
   4290/1000000: episode: 554, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000149, mae: 0.007336, mean_q: 0.007495
   4292/1000000: episode: 555, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000053, mae: 0.006745, mean_q: 0.006940
   4294/1000000: episode: 556, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000937, mae: 0.012634, mean_q: 0.011737
   4296/1000000: episode: 557, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000280, mae: 0.010257, mean_q: 0.012105
   4298/1000000: episode: 558, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000866, mae: 0.010664, mean_q: 0.008459
   4300/1000000: episode: 559, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000961, mae: 0.011531, mean_q: 0.010769
   4302/1000000: episode: 560, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000272, mae: 0.009457, mean_q: 0.011349
   4304/1000000: episode: 561, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001031, mae: 0.013441, mean_q: 0.011404
   4306/1000000: episode: 562, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000144, mae: 0.006766, mean_q: 0.007259
   4308/1000000: episode: 563, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000021, mae: 0.004039, mean_q: 0.006245
   4310/1000000: episode: 564, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000158, mae: 0.008309, mean_q: 0.011096
   4312/1000000: episode: 565, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000153, mae: 0.009058, mean_q: 0.006952
   4314/1000000: episode: 566, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.007859, mean_q: 0.012998
   4316/1000000: episode: 567, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000141, mae: 0.006648, mean_q: 0.008011
   4318/1000000: episode: 568, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000162, mae: 0.009589, mean_q: 0.007104
   4320/1000000: episode: 569, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000157, mae: 0.008189, mean_q: 0.012284
   4322/1000000: episode: 570, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000220, mae: 0.007746, mean_q: 0.007614
   4325/1000000: episode: 571, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000061, mae: 0.004836, mean_q: 0.006546
   4327/1000000: episode: 572, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000090, mae: 0.005567, mean_q: 0.005048
   4330/1000000: episode: 573, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000080, mae: 0.005860, mean_q: 0.007359
   4332/1000000: episode: 574, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000871, mae: 0.010188, mean_q: 0.009793
   4334/1000000: episode: 575, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000231, mae: 0.008490, mean_q: 0.009614
   4336/1000000: episode: 576, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000166, mae: 0.007206, mean_q: 0.010714
   4339/1000000: episode: 577, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000132, mae: 0.008304, mean_q: 0.007515
   4341/1000000: episode: 578, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000091, mae: 0.006059, mean_q: 0.008739
   4343/1000000: episode: 579, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000170, mae: 0.007951, mean_q: 0.011247
   4345/1000000: episode: 580, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000147, mae: 0.007680, mean_q: 0.004672
   4347/1000000: episode: 581, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000207, mae: 0.009871, mean_q: 0.014484
   4349/1000000: episode: 582, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000050, mae: 0.005444, mean_q: 0.007804
   4351/1000000: episode: 583, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000160, mae: 0.007411, mean_q: 0.006515
   4354/1000000: episode: 584, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001348, mae: 0.016153, mean_q: 0.010809
   4356/1000000: episode: 585, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000094, mae: 0.005800, mean_q: 0.008812
   4358/1000000: episode: 586, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000209, mae: 0.009200, mean_q: 0.013674
   4360/1000000: episode: 587, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000255, mae: 0.012386, mean_q: 0.009193
   4362/1000000: episode: 588, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000217, mae: 0.009285, mean_q: 0.008886
   4364/1000000: episode: 589, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000989, mae: 0.016269, mean_q: 0.016071
   4366/1000000: episode: 590, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000138, mae: 0.011588, mean_q: 0.004713
   4368/1000000: episode: 591, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000242, mae: 0.011088, mean_q: 0.015103
   4370/1000000: episode: 592, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000891, mae: 0.011839, mean_q: 0.007872
   4372/1000000: episode: 593, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000133, mae: 0.008377, mean_q: 0.005211
   4374/1000000: episode: 594, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000239, mae: 0.011970, mean_q: 0.017419
[Info] 3-TH LEVEL FOUND: 0.08211036771535873, Considering 100/100 traces
   4376/1000000: episode: 595, duration: 0.644s, episode steps: 2, steps per second: 3, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006792, mae: 0.024288, mean_q: 0.006359
[Info] 4-TH LEVEL FOUND: 0.08263310045003891, Considering 100/100 traces
   4379/1000000: episode: 596, duration: 0.706s, episode steps: 3, steps per second: 4, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000204, mae: 0.009277, mean_q: 0.011958
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.08263310045003891
   4382/1000000: episode: 597, duration: 0.483s, episode steps: 3, steps per second: 6, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000181, mae: 0.008362, mean_q: 0.007957
   4392/1000000: episode: 598, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000073, mae: 0.006095, mean_q: 0.007180
   4402/1000000: episode: 599, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000105, mae: 0.006570, mean_q: 0.007342
   4412/1000000: episode: 600, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000159, mae: 0.007620, mean_q: 0.008758
   4422/1000000: episode: 601, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000237, mae: 0.009724, mean_q: 0.010898
   4432/1000000: episode: 602, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000284, mae: 0.007980, mean_q: 0.009989
   4442/1000000: episode: 603, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000434, mae: 0.007924, mean_q: 0.007741
   4452/1000000: episode: 604, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001530, mae: 0.011997, mean_q: 0.009839
   4462/1000000: episode: 605, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000150, mae: 0.007603, mean_q: 0.009624
   4472/1000000: episode: 606, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000240, mae: 0.008089, mean_q: 0.008607
   4482/1000000: episode: 607, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000313, mae: 0.009013, mean_q: 0.007370
   4492/1000000: episode: 608, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000314, mae: 0.009202, mean_q: 0.010582
   4502/1000000: episode: 609, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.001530, mae: 0.011965, mean_q: 0.011905
   4512/1000000: episode: 610, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000278, mae: 0.007486, mean_q: 0.006869
   4522/1000000: episode: 611, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000171, mae: 0.008246, mean_q: 0.009547
   4532/1000000: episode: 612, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000369, mae: 0.010385, mean_q: 0.010669
   4542/1000000: episode: 613, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000369, mae: 0.009584, mean_q: 0.009783
   4552/1000000: episode: 614, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000465, mae: 0.009490, mean_q: 0.008659
   4562/1000000: episode: 615, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001667, mae: 0.011659, mean_q: 0.009925
   4572/1000000: episode: 616, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001497, mae: 0.011915, mean_q: 0.010495
   4582/1000000: episode: 617, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000328, mae: 0.010120, mean_q: 0.010583
   4592/1000000: episode: 618, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001663, mae: 0.011459, mean_q: 0.009830
   4602/1000000: episode: 619, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000319, mae: 0.007600, mean_q: 0.009765
   4612/1000000: episode: 620, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000262, mae: 0.006356, mean_q: 0.009736
   4622/1000000: episode: 621, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.131, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000302, mae: 0.007699, mean_q: 0.009112
   4632/1000000: episode: 622, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000084, mae: 0.005978, mean_q: 0.008908
   4642/1000000: episode: 623, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000177, mae: 0.007793, mean_q: 0.010320
   4652/1000000: episode: 624, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000390, mae: 0.010313, mean_q: 0.011170
   4662/1000000: episode: 625, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000436, mae: 0.009232, mean_q: 0.009113
   4672/1000000: episode: 626, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001706, mae: 0.013629, mean_q: 0.011226
   4682/1000000: episode: 627, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000162, mae: 0.008125, mean_q: 0.009766
   4692/1000000: episode: 628, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000350, mae: 0.009174, mean_q: 0.009591
   4702/1000000: episode: 629, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000087, mae: 0.006631, mean_q: 0.009000
   4712/1000000: episode: 630, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000303, mae: 0.009425, mean_q: 0.010367
   4722/1000000: episode: 631, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000566, mae: 0.012876, mean_q: 0.011799
   4732/1000000: episode: 632, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000313, mae: 0.008480, mean_q: 0.009501
   4742/1000000: episode: 633, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001821, mae: 0.013361, mean_q: 0.010939
   4752/1000000: episode: 634, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000350, mae: 0.009468, mean_q: 0.009152
   4762/1000000: episode: 635, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000525, mae: 0.010435, mean_q: 0.011108
   4772/1000000: episode: 636, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000526, mae: 0.011365, mean_q: 0.011948
   4782/1000000: episode: 637, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001469, mae: 0.009832, mean_q: 0.009539
   4792/1000000: episode: 638, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000163, mae: 0.008101, mean_q: 0.010808
   4802/1000000: episode: 639, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000086, mae: 0.005728, mean_q: 0.007697
   4812/1000000: episode: 640, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000230, mae: 0.008756, mean_q: 0.010387
   4822/1000000: episode: 641, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000117, mae: 0.006552, mean_q: 0.008480
   4832/1000000: episode: 642, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000350, mae: 0.008973, mean_q: 0.009509
   4842/1000000: episode: 643, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000200, mae: 0.009177, mean_q: 0.010571
   4852/1000000: episode: 644, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000271, mae: 0.006873, mean_q: 0.007473
   4862/1000000: episode: 645, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000535, mae: 0.011132, mean_q: 0.010192
   4872/1000000: episode: 646, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000334, mae: 0.009207, mean_q: 0.010195
   4882/1000000: episode: 647, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000187, mae: 0.008952, mean_q: 0.010972
   4892/1000000: episode: 648, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000167, mae: 0.008705, mean_q: 0.009357
   4902/1000000: episode: 649, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000564, mae: 0.011858, mean_q: 0.012209
   4912/1000000: episode: 650, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000316, mae: 0.008555, mean_q: 0.010982
   4922/1000000: episode: 651, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001556, mae: 0.009377, mean_q: 0.007349
   4932/1000000: episode: 652, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000468, mae: 0.009415, mean_q: 0.009041
   4942/1000000: episode: 653, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000443, mae: 0.010159, mean_q: 0.010353
   4952/1000000: episode: 654, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000178, mae: 0.008195, mean_q: 0.010576
   4962/1000000: episode: 655, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000402, mae: 0.011281, mean_q: 0.011644
   4972/1000000: episode: 656, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001454, mae: 0.010385, mean_q: 0.009171
   4982/1000000: episode: 657, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000317, mae: 0.008241, mean_q: 0.008538
   4992/1000000: episode: 658, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001494, mae: 0.011563, mean_q: 0.010845
   5002/1000000: episode: 659, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001476, mae: 0.011253, mean_q: 0.011743
   5012/1000000: episode: 660, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000301, mae: 0.008271, mean_q: 0.009131
   5022/1000000: episode: 661, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000545, mae: 0.011209, mean_q: 0.011975
   5032/1000000: episode: 662, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002989, mae: 0.015239, mean_q: 0.010797
   5042/1000000: episode: 663, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000361, mae: 0.009935, mean_q: 0.011833
   5052/1000000: episode: 664, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000223, mae: 0.009798, mean_q: 0.010454
   5062/1000000: episode: 665, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000395, mae: 0.010902, mean_q: 0.010837
   5072/1000000: episode: 666, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000510, mae: 0.011259, mean_q: 0.011597
   5082/1000000: episode: 667, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000171, mae: 0.008651, mean_q: 0.009969
   5092/1000000: episode: 668, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000180, mae: 0.007600, mean_q: 0.009813
   5102/1000000: episode: 669, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001704, mae: 0.012961, mean_q: 0.011810
   5112/1000000: episode: 670, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000179, mae: 0.009159, mean_q: 0.010216
   5122/1000000: episode: 671, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001452, mae: 0.010513, mean_q: 0.009822
   5132/1000000: episode: 672, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000336, mae: 0.009077, mean_q: 0.009777
   5142/1000000: episode: 673, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001484, mae: 0.010891, mean_q: 0.010998
   5152/1000000: episode: 674, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000546, mae: 0.013076, mean_q: 0.012844
   5162/1000000: episode: 675, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000133, mae: 0.007852, mean_q: 0.007516
   5172/1000000: episode: 676, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000165, mae: 0.007962, mean_q: 0.008806
   5182/1000000: episode: 677, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000172, mae: 0.008861, mean_q: 0.009787
   5192/1000000: episode: 678, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001652, mae: 0.013509, mean_q: 0.010546
   5202/1000000: episode: 679, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000141, mae: 0.007697, mean_q: 0.009574
   5212/1000000: episode: 680, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000168, mae: 0.007617, mean_q: 0.009301
   5222/1000000: episode: 681, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000183, mae: 0.008687, mean_q: 0.010029
   5232/1000000: episode: 682, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001703, mae: 0.012321, mean_q: 0.011374
   5242/1000000: episode: 683, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001513, mae: 0.012215, mean_q: 0.011039
   5252/1000000: episode: 684, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000127, mae: 0.009156, mean_q: 0.007742
   5262/1000000: episode: 685, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001993, mae: 0.014413, mean_q: 0.011356
   5272/1000000: episode: 686, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000468, mae: 0.010097, mean_q: 0.008391
   5282/1000000: episode: 687, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000505, mae: 0.012136, mean_q: 0.013733
   5292/1000000: episode: 688, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000452, mae: 0.009403, mean_q: 0.007720
   5302/1000000: episode: 689, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000182, mae: 0.008069, mean_q: 0.009464
   5312/1000000: episode: 690, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000232, mae: 0.009820, mean_q: 0.011698
   5322/1000000: episode: 691, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000163, mae: 0.007370, mean_q: 0.009146
   5332/1000000: episode: 692, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000339, mae: 0.010053, mean_q: 0.009103
   5342/1000000: episode: 693, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001645, mae: 0.012425, mean_q: 0.010642
   5352/1000000: episode: 694, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000812, mae: 0.012719, mean_q: 0.011845
   5362/1000000: episode: 695, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002804, mae: 0.016046, mean_q: 0.013540
   5372/1000000: episode: 696, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000179, mae: 0.009290, mean_q: 0.006683
[Info] 1-TH LEVEL FOUND: 0.020614102482795715, Considering 11/100 traces
   5382/1000000: episode: 697, duration: 0.655s, episode steps: 10, steps per second: 15, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000464, mae: 0.012884, mean_q: 0.010575
   5384/1000000: episode: 698, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.005920, mean_q: 0.007724
   5388/1000000: episode: 699, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000603, mae: 0.010381, mean_q: 0.008468
   5390/1000000: episode: 700, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.007199, mae: 0.028508, mean_q: 0.015575
   5397/1000000: episode: 701, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000229, mae: 0.010116, mean_q: 0.013239
   5399/1000000: episode: 702, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001063, mae: 0.016640, mean_q: 0.017362
   5406/1000000: episode: 703, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.002048, mae: 0.013548, mean_q: 0.012367
   5408/1000000: episode: 704, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000300, mae: 0.012276, mean_q: 0.013038
   5410/1000000: episode: 705, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000256, mae: 0.009492, mean_q: 0.015869
   5414/1000000: episode: 706, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000184, mae: 0.007761, mean_q: 0.009083
   5421/1000000: episode: 707, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000896, mae: 0.014374, mean_q: 0.014781
   5428/1000000: episode: 708, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000209, mae: 0.009417, mean_q: 0.012611
   5430/1000000: episode: 709, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000354, mae: 0.012887, mean_q: 0.015368
   5434/1000000: episode: 710, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000228, mae: 0.009764, mean_q: 0.013856
   5441/1000000: episode: 711, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000143, mae: 0.008367, mean_q: 0.008033
   5443/1000000: episode: 712, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000164, mae: 0.007404, mean_q: 0.005027
   5445/1000000: episode: 713, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006813, mae: 0.026941, mean_q: 0.017225
   5449/1000000: episode: 714, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000197, mae: 0.010111, mean_q: 0.010076
   5456/1000000: episode: 715, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000105, mae: 0.007032, mean_q: 0.008616
   5463/1000000: episode: 716, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000203, mae: 0.008333, mean_q: 0.011241
   5465/1000000: episode: 717, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000037, mae: 0.004283, mean_q: 0.006235
   5467/1000000: episode: 718, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001134, mae: 0.016112, mean_q: 0.014943
   5471/1000000: episode: 719, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000117, mae: 0.006578, mean_q: 0.009506
   5473/1000000: episode: 720, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.007437, mae: 0.028064, mean_q: 0.012065
   5475/1000000: episode: 721, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000136, mae: 0.007922, mean_q: 0.011090
   5482/1000000: episode: 722, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000267, mae: 0.011037, mean_q: 0.012024
   5489/1000000: episode: 723, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000100, mae: 0.008530, mean_q: 0.009179
   5493/1000000: episode: 724, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000179, mae: 0.009807, mean_q: 0.011500
   5495/1000000: episode: 725, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000120, mae: 0.009143, mean_q: 0.007053
   5497/1000000: episode: 726, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000091, mae: 0.008382, mean_q: 0.012126
   5499/1000000: episode: 727, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000172, mae: 0.010399, mean_q: 0.014950
   5506/1000000: episode: 728, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000174, mae: 0.009776, mean_q: 0.008677
   5510/1000000: episode: 729, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000716, mae: 0.014858, mean_q: 0.014813
   5512/1000000: episode: 730, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000306, mae: 0.010946, mean_q: 0.013987
   5519/1000000: episode: 731, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000672, mae: 0.011790, mean_q: 0.011137
   5526/1000000: episode: 732, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000398, mae: 0.010843, mean_q: 0.010608
   5530/1000000: episode: 733, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000172, mae: 0.008933, mean_q: 0.009357
   5537/1000000: episode: 734, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000183, mae: 0.008970, mean_q: 0.010204
   5541/1000000: episode: 735, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000280, mae: 0.012582, mean_q: 0.011296
   5545/1000000: episode: 736, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000615, mae: 0.011832, mean_q: 0.014237
   5547/1000000: episode: 737, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000189, mae: 0.009223, mean_q: 0.007550
   5549/1000000: episode: 738, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000247, mae: 0.011286, mean_q: 0.015099
   5556/1000000: episode: 739, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000125, mae: 0.007503, mean_q: 0.010002
   5563/1000000: episode: 740, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000203, mae: 0.009804, mean_q: 0.010478
   5567/1000000: episode: 741, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000249, mae: 0.010596, mean_q: 0.009910
   5574/1000000: episode: 742, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000126, mae: 0.007649, mean_q: 0.010783
   5578/1000000: episode: 743, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000169, mae: 0.008590, mean_q: 0.007918
   5585/1000000: episode: 744, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000236, mae: 0.010037, mean_q: 0.011317
   5589/1000000: episode: 745, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000207, mae: 0.009072, mean_q: 0.009043
   5596/1000000: episode: 746, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000361, mae: 0.013695, mean_q: 0.015581
   5598/1000000: episode: 747, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000170, mae: 0.007880, mean_q: 0.010780
   5605/1000000: episode: 748, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000219, mae: 0.010291, mean_q: 0.011158
   5609/1000000: episode: 749, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000246, mae: 0.009933, mean_q: 0.013212
   5613/1000000: episode: 750, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000271, mae: 0.010985, mean_q: 0.012244
   5615/1000000: episode: 751, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000257, mae: 0.012003, mean_q: 0.014852
   5622/1000000: episode: 752, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000425, mae: 0.011317, mean_q: 0.009242
   5624/1000000: episode: 753, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000037, mae: 0.006323, mean_q: 0.003193
   5631/1000000: episode: 754, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000669, mae: 0.011856, mean_q: 0.010189
   5633/1000000: episode: 755, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000127, mae: 0.007031, mean_q: 0.009390
   5635/1000000: episode: 756, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000213, mae: 0.009382, mean_q: 0.006512
   5637/1000000: episode: 757, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001097, mae: 0.015515, mean_q: 0.014847
   5639/1000000: episode: 758, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000164, mae: 0.006522, mean_q: 0.007240
   5641/1000000: episode: 759, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000357, mae: 0.011474, mean_q: 0.011201
   5648/1000000: episode: 760, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000300, mae: 0.008169, mean_q: 0.007998
   5650/1000000: episode: 761, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001576, mae: 0.020437, mean_q: 0.016570
   5654/1000000: episode: 762, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000575, mae: 0.012468, mean_q: 0.012554
   5656/1000000: episode: 763, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000278, mae: 0.010907, mean_q: 0.007462
   5658/1000000: episode: 764, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000073, mae: 0.005692, mean_q: 0.006959
   5660/1000000: episode: 765, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000103, mae: 0.007408, mean_q: 0.008517
   5664/1000000: episode: 766, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000161, mae: 0.007998, mean_q: 0.010623
   5666/1000000: episode: 767, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000191, mae: 0.008325, mean_q: 0.011608
   5673/1000000: episode: 768, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000381, mae: 0.009605, mean_q: 0.010059
   5680/1000000: episode: 769, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.002091, mae: 0.013415, mean_q: 0.011755
   5684/1000000: episode: 770, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000601, mae: 0.010982, mean_q: 0.007266
   5686/1000000: episode: 771, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000165, mae: 0.009936, mean_q: 0.013800
   5693/1000000: episode: 772, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.002113, mae: 0.015065, mean_q: 0.012029
   5697/1000000: episode: 773, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000636, mae: 0.012331, mean_q: 0.013382
   5701/1000000: episode: 774, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000655, mae: 0.013592, mean_q: 0.011104
   5703/1000000: episode: 775, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000090, mae: 0.006561, mean_q: 0.010897
   5707/1000000: episode: 776, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000298, mae: 0.015510, mean_q: 0.015580
   5714/1000000: episode: 777, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000533, mae: 0.014753, mean_q: 0.012335
   5718/1000000: episode: 778, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000552, mae: 0.013060, mean_q: 0.011644
   5722/1000000: episode: 779, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000195, mae: 0.010051, mean_q: 0.006808
   5724/1000000: episode: 780, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000700, mae: 0.012645, mean_q: 0.011334
   5731/1000000: episode: 781, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000133, mae: 0.008461, mean_q: 0.010959
   5738/1000000: episode: 782, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000202, mae: 0.010365, mean_q: 0.010024
   5740/1000000: episode: 783, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000069, mae: 0.008657, mean_q: 0.003561
   5742/1000000: episode: 784, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000116, mae: 0.008475, mean_q: 0.007393
   5744/1000000: episode: 785, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000280, mae: 0.010344, mean_q: 0.011913
[Info] 2-TH LEVEL FOUND: 0.08721098303794861, Considering 20/100 traces
   5746/1000000: episode: 786, duration: 0.670s, episode steps: 2, steps per second: 3, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000738, mae: 0.011229, mean_q: 0.006416
   5749/1000000: episode: 787, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001337, mae: 0.017272, mean_q: 0.016200
   5752/1000000: episode: 788, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000223, mae: 0.011121, mean_q: 0.012021
   5755/1000000: episode: 789, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000183, mae: 0.008883, mean_q: 0.011439
   5758/1000000: episode: 790, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000188, mae: 0.008579, mean_q: 0.009147
   5761/1000000: episode: 791, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004652, mae: 0.021177, mean_q: 0.016687
   5764/1000000: episode: 792, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000189, mae: 0.009743, mean_q: 0.009070
   5767/1000000: episode: 793, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000320, mae: 0.011864, mean_q: 0.014261
   5770/1000000: episode: 794, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000761, mae: 0.016359, mean_q: 0.017297
   5773/1000000: episode: 795, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000778, mae: 0.014160, mean_q: 0.016112
   5776/1000000: episode: 796, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000285, mae: 0.010215, mean_q: 0.009877
   5779/1000000: episode: 797, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000673, mae: 0.011036, mean_q: 0.011124
   5782/1000000: episode: 798, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000144, mae: 0.008445, mean_q: 0.010368
   5785/1000000: episode: 799, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000171, mae: 0.008205, mean_q: 0.011209
   5788/1000000: episode: 800, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000158, mae: 0.007645, mean_q: 0.008657
   5791/1000000: episode: 801, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000168, mae: 0.008057, mean_q: 0.010495
   5794/1000000: episode: 802, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000168, mae: 0.008345, mean_q: 0.012176
   5797/1000000: episode: 803, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005069, mae: 0.020621, mean_q: 0.010301
   5800/1000000: episode: 804, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000825, mae: 0.014892, mean_q: 0.018240
   5803/1000000: episode: 805, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000259, mae: 0.010746, mean_q: 0.014159
   5806/1000000: episode: 806, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000082, mae: 0.006127, mean_q: 0.008688
   5809/1000000: episode: 807, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000756, mae: 0.013740, mean_q: 0.012909
   5812/1000000: episode: 808, duration: 0.016s, episode steps: 3, steps per second: 185, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000202, mae: 0.010758, mean_q: 0.013866
   5815/1000000: episode: 809, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000263, mae: 0.011104, mean_q: 0.013463
   5818/1000000: episode: 810, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000578, mae: 0.010146, mean_q: 0.011351
   5821/1000000: episode: 811, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004490, mae: 0.016955, mean_q: 0.008754
   5824/1000000: episode: 812, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 1.104, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 11.000], loss: 0.004587, mae: 0.020158, mean_q: 0.013351
   5827/1000000: episode: 813, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000201, mae: 0.009269, mean_q: 0.010326
[Info] FALSIFICATION!
   5829/1000000: episode: 814, duration: 0.249s, episode steps: 2, steps per second: 8, episode reward: 1.368, mean reward: 0.684 [0.368, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 10.000], loss: 0.001190, mae: 0.018792, mean_q: 0.020493
   5832/1000000: episode: 815, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000257, mae: 0.010750, mean_q: 0.013917
   5835/1000000: episode: 816, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000240, mae: 0.009024, mean_q: 0.011052
   5838/1000000: episode: 817, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005023, mae: 0.020549, mean_q: 0.009835
   5841/1000000: episode: 818, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000166, mae: 0.008513, mean_q: 0.011449
   5844/1000000: episode: 819, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000144, mae: 0.007969, mean_q: 0.008657
   5847/1000000: episode: 820, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000802, mae: 0.014503, mean_q: 0.012273
   5850/1000000: episode: 821, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000835, mae: 0.014906, mean_q: 0.012525
   5853/1000000: episode: 822, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000220, mae: 0.012205, mean_q: 0.012184
   5856/1000000: episode: 823, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004529, mae: 0.020543, mean_q: 0.018189
   5859/1000000: episode: 824, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000381, mae: 0.015896, mean_q: 0.010693
   5862/1000000: episode: 825, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000819, mae: 0.017599, mean_q: 0.020166
   5865/1000000: episode: 826, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000259, mae: 0.013068, mean_q: 0.009450
   5868/1000000: episode: 827, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004601, mae: 0.021205, mean_q: 0.016375
   5871/1000000: episode: 828, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000297, mae: 0.011153, mean_q: 0.009720
   5874/1000000: episode: 829, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000754, mae: 0.012897, mean_q: 0.009846
   5877/1000000: episode: 830, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000262, mae: 0.009766, mean_q: 0.011694
   5880/1000000: episode: 831, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004637, mae: 0.021319, mean_q: 0.015116
   5883/1000000: episode: 832, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000253, mae: 0.010079, mean_q: 0.012967
   5886/1000000: episode: 833, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000165, mae: 0.009865, mean_q: 0.011485
   5889/1000000: episode: 834, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000138, mae: 0.007532, mean_q: 0.011767
   5892/1000000: episode: 835, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000354, mae: 0.012506, mean_q: 0.012330
   5895/1000000: episode: 836, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000247, mae: 0.009864, mean_q: 0.013811
   5898/1000000: episode: 837, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000257, mae: 0.010376, mean_q: 0.009667
   5901/1000000: episode: 838, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000288, mae: 0.011015, mean_q: 0.011861
   5904/1000000: episode: 839, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004783, mae: 0.025798, mean_q: 0.019389
   5907/1000000: episode: 840, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000266, mae: 0.010860, mean_q: 0.014048
   5910/1000000: episode: 841, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000602, mae: 0.011190, mean_q: 0.010316
   5913/1000000: episode: 842, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000930, mae: 0.015873, mean_q: 0.015360
   5916/1000000: episode: 843, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000765, mae: 0.012320, mean_q: 0.013428
   5919/1000000: episode: 844, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005151, mae: 0.022288, mean_q: 0.013207
   5922/1000000: episode: 845, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000204, mae: 0.008635, mean_q: 0.011326
   5925/1000000: episode: 846, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000867, mae: 0.015993, mean_q: 0.016041
   5928/1000000: episode: 847, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000272, mae: 0.011696, mean_q: 0.014572
   5931/1000000: episode: 848, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000233, mae: 0.010802, mean_q: 0.017220
   5934/1000000: episode: 849, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000306, mae: 0.012186, mean_q: 0.011703
[Info] FALSIFICATION!
   5936/1000000: episode: 850, duration: 0.250s, episode steps: 2, steps per second: 8, episode reward: 1.368, mean reward: 0.684 [0.368, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 10.000], loss: 0.001017, mae: 0.014629, mean_q: 0.013219
   5939/1000000: episode: 851, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000287, mae: 0.011429, mean_q: 0.012131
   5942/1000000: episode: 852, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000207, mae: 0.009647, mean_q: 0.013295
   5945/1000000: episode: 853, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004675, mae: 0.023642, mean_q: 0.019079
   5948/1000000: episode: 854, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000543, mae: 0.018384, mean_q: 0.016223
   5951/1000000: episode: 855, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000737, mae: 0.013109, mean_q: 0.012954
   5954/1000000: episode: 856, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000257, mae: 0.010914, mean_q: 0.013390
   5957/1000000: episode: 857, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000256, mae: 0.010797, mean_q: 0.012590
   5960/1000000: episode: 858, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000287, mae: 0.011641, mean_q: 0.011766
[Info] FALSIFICATION!
   5962/1000000: episode: 859, duration: 0.163s, episode steps: 2, steps per second: 12, episode reward: 1.368, mean reward: 0.684 [0.368, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 10.000], loss: 0.000714, mae: 0.010745, mean_q: 0.008162
   5965/1000000: episode: 860, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000891, mae: 0.016341, mean_q: 0.013653
   5968/1000000: episode: 861, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000352, mae: 0.013390, mean_q: 0.013654
   5971/1000000: episode: 862, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000255, mae: 0.011496, mean_q: 0.013891
   5974/1000000: episode: 863, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000918, mae: 0.016782, mean_q: 0.012210
   5977/1000000: episode: 864, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001375, mae: 0.019326, mean_q: 0.018842
[Info] FALSIFICATION!
   5979/1000000: episode: 865, duration: 0.251s, episode steps: 2, steps per second: 8, episode reward: 1.368, mean reward: 0.684 [0.368, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.500 [9.000, 10.000], loss: 0.000969, mae: 0.015198, mean_q: 0.008162
[Info] Complete ISplit Iteration
[Info] Levels: [0.020614102, 0.08721098, 0.08682642]
[Info] Cond. Prob: [0.11, 0.2, 1.0]
[Info] Error Prob: 0.022000000000000002

   5982/1000000: episode: 866, duration: 0.864s, episode steps: 3, steps per second: 3, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000241, mae: 0.011113, mean_q: 0.014983
   5992/1000000: episode: 867, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000464, mae: 0.012194, mean_q: 0.014236
   6002/1000000: episode: 868, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001796, mae: 0.015990, mean_q: 0.014586
   6012/1000000: episode: 869, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000478, mae: 0.012644, mean_q: 0.015369
   6022/1000000: episode: 870, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001794, mae: 0.013900, mean_q: 0.012092
   6032/1000000: episode: 871, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001923, mae: 0.016455, mean_q: 0.016170
   6042/1000000: episode: 872, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.003132, mae: 0.018737, mean_q: 0.013944
   6052/1000000: episode: 873, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000480, mae: 0.014189, mean_q: 0.015753
   6062/1000000: episode: 874, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001987, mae: 0.020749, mean_q: 0.020456
   6072/1000000: episode: 875, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001573, mae: 0.015225, mean_q: 0.013710
   6082/1000000: episode: 876, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000514, mae: 0.014625, mean_q: 0.013652
   6092/1000000: episode: 877, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000308, mae: 0.013735, mean_q: 0.015476
   6102/1000000: episode: 878, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001605, mae: 0.016787, mean_q: 0.012011
   6112/1000000: episode: 879, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000753, mae: 0.015915, mean_q: 0.015031
   6122/1000000: episode: 880, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000665, mae: 0.015184, mean_q: 0.014851
   6132/1000000: episode: 881, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000653, mae: 0.015626, mean_q: 0.016589
   6142/1000000: episode: 882, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001788, mae: 0.018240, mean_q: 0.011806
   6152/1000000: episode: 883, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003207, mae: 0.019825, mean_q: 0.016265
   6162/1000000: episode: 884, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001708, mae: 0.015688, mean_q: 0.014984
   6172/1000000: episode: 885, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001836, mae: 0.015483, mean_q: 0.015489
   6182/1000000: episode: 886, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000632, mae: 0.013036, mean_q: 0.014370
   6192/1000000: episode: 887, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000234, mae: 0.010693, mean_q: 0.013185
   6202/1000000: episode: 888, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000991, mae: 0.016810, mean_q: 0.014156
   6212/1000000: episode: 889, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001044, mae: 0.015452, mean_q: 0.014296
   6222/1000000: episode: 890, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002072, mae: 0.018279, mean_q: 0.015987
   6232/1000000: episode: 891, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000420, mae: 0.013078, mean_q: 0.014608
   6242/1000000: episode: 892, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000219, mae: 0.009681, mean_q: 0.012039
   6252/1000000: episode: 893, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000353, mae: 0.010392, mean_q: 0.012376
   6262/1000000: episode: 894, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000575, mae: 0.014054, mean_q: 0.017053
   6272/1000000: episode: 895, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000401, mae: 0.010955, mean_q: 0.011993
   6282/1000000: episode: 896, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000350, mae: 0.012042, mean_q: 0.014279
   6292/1000000: episode: 897, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000556, mae: 0.012267, mean_q: 0.013332
   6302/1000000: episode: 898, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000551, mae: 0.011576, mean_q: 0.010936
   6312/1000000: episode: 899, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000624, mae: 0.013516, mean_q: 0.014437
   6322/1000000: episode: 900, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000311, mae: 0.011662, mean_q: 0.012468
   6332/1000000: episode: 901, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002069, mae: 0.016607, mean_q: 0.013190
   6342/1000000: episode: 902, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.003367, mae: 0.020853, mean_q: 0.015115
   6352/1000000: episode: 903, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000596, mae: 0.015050, mean_q: 0.017076
   6362/1000000: episode: 904, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001670, mae: 0.016707, mean_q: 0.015278
   6372/1000000: episode: 905, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001623, mae: 0.015159, mean_q: 0.013510
   6382/1000000: episode: 906, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000236, mae: 0.010681, mean_q: 0.012216
   6392/1000000: episode: 907, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000749, mae: 0.014820, mean_q: 0.014669
   6402/1000000: episode: 908, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001873, mae: 0.017389, mean_q: 0.016477
   6412/1000000: episode: 909, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000327, mae: 0.012889, mean_q: 0.012686
   6422/1000000: episode: 910, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000363, mae: 0.011654, mean_q: 0.012724
   6432/1000000: episode: 911, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000588, mae: 0.012972, mean_q: 0.014222
   6442/1000000: episode: 912, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000751, mae: 0.014005, mean_q: 0.013860
   6452/1000000: episode: 913, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000484, mae: 0.012985, mean_q: 0.014830
   6462/1000000: episode: 914, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000616, mae: 0.013644, mean_q: 0.011644
   6472/1000000: episode: 915, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001702, mae: 0.016061, mean_q: 0.014864
   6482/1000000: episode: 916, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000323, mae: 0.013315, mean_q: 0.014267
   6492/1000000: episode: 917, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000598, mae: 0.013244, mean_q: 0.013598
   6502/1000000: episode: 918, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000519, mae: 0.015093, mean_q: 0.015593
   6512/1000000: episode: 919, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000565, mae: 0.013286, mean_q: 0.010256
   6522/1000000: episode: 920, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000159, mae: 0.007948, mean_q: 0.010714
   6532/1000000: episode: 921, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001694, mae: 0.013532, mean_q: 0.012996
   6542/1000000: episode: 922, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.004277, mae: 0.019712, mean_q: 0.015513
   6552/1000000: episode: 923, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000359, mae: 0.010721, mean_q: 0.012150
   6562/1000000: episode: 924, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001743, mae: 0.016837, mean_q: 0.016989
   6572/1000000: episode: 925, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.035, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001634, mae: 0.015669, mean_q: 0.014401
   6582/1000000: episode: 926, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000719, mae: 0.013827, mean_q: 0.015798
   6592/1000000: episode: 927, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000274, mae: 0.010853, mean_q: 0.012173
   6602/1000000: episode: 928, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001641, mae: 0.015463, mean_q: 0.015388
   6612/1000000: episode: 929, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000889, mae: 0.013803, mean_q: 0.013061
   6622/1000000: episode: 930, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000693, mae: 0.013764, mean_q: 0.015359
   6632/1000000: episode: 931, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000513, mae: 0.013704, mean_q: 0.014878
   6642/1000000: episode: 932, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001671, mae: 0.013176, mean_q: 0.011775
   6652/1000000: episode: 933, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000662, mae: 0.015436, mean_q: 0.017257
   6662/1000000: episode: 934, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001661, mae: 0.012976, mean_q: 0.012156
   6672/1000000: episode: 935, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000570, mae: 0.012336, mean_q: 0.011373
   6682/1000000: episode: 936, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000580, mae: 0.014053, mean_q: 0.012831
   6692/1000000: episode: 937, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000733, mae: 0.013348, mean_q: 0.010785
   6702/1000000: episode: 938, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000432, mae: 0.014005, mean_q: 0.017221
   6712/1000000: episode: 939, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000362, mae: 0.010335, mean_q: 0.011069
   6722/1000000: episode: 940, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001578, mae: 0.015428, mean_q: 0.012324
   6732/1000000: episode: 941, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000249, mae: 0.011013, mean_q: 0.011488
   6742/1000000: episode: 942, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001690, mae: 0.014424, mean_q: 0.013141
   6752/1000000: episode: 943, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001547, mae: 0.013863, mean_q: 0.013855
   6762/1000000: episode: 944, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000263, mae: 0.011875, mean_q: 0.014891
   6772/1000000: episode: 945, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000237, mae: 0.011070, mean_q: 0.011889
   6782/1000000: episode: 946, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001840, mae: 0.017771, mean_q: 0.016368
   6792/1000000: episode: 947, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000289, mae: 0.011308, mean_q: 0.012547
   6802/1000000: episode: 948, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000520, mae: 0.014102, mean_q: 0.015416
   6812/1000000: episode: 949, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000417, mae: 0.010962, mean_q: 0.012554
   6822/1000000: episode: 950, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000387, mae: 0.011405, mean_q: 0.012821
   6832/1000000: episode: 951, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.002042, mae: 0.016947, mean_q: 0.014245
   6842/1000000: episode: 952, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000353, mae: 0.012882, mean_q: 0.014251
   6852/1000000: episode: 953, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.003156, mae: 0.018622, mean_q: 0.015529
   6862/1000000: episode: 954, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001687, mae: 0.013177, mean_q: 0.009800
   6872/1000000: episode: 955, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001966, mae: 0.017639, mean_q: 0.016831
   6882/1000000: episode: 956, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000584, mae: 0.012519, mean_q: 0.015117
   6892/1000000: episode: 957, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001499, mae: 0.013106, mean_q: 0.012759
   6902/1000000: episode: 958, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000852, mae: 0.016764, mean_q: 0.013490
   6912/1000000: episode: 959, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000436, mae: 0.012585, mean_q: 0.013473
   6922/1000000: episode: 960, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000468, mae: 0.011919, mean_q: 0.013155
   6932/1000000: episode: 961, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.004619, mae: 0.024348, mean_q: 0.018422
   6942/1000000: episode: 962, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001664, mae: 0.014394, mean_q: 0.012949
   6952/1000000: episode: 963, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001795, mae: 0.017058, mean_q: 0.018288
   6962/1000000: episode: 964, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.001684, mae: 0.014093, mean_q: 0.014158
   6972/1000000: episode: 965, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002048, mae: 0.017307, mean_q: 0.014406
[Info] 1-TH LEVEL FOUND: 0.039437152445316315, Considering 11/100 traces
   6982/1000000: episode: 966, duration: 0.772s, episode steps: 10, steps per second: 13, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000288, mae: 0.012367, mean_q: 0.015780
   6987/1000000: episode: 967, duration: 0.026s, episode steps: 5, steps per second: 190, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.002899, mae: 0.020637, mean_q: 0.016778
   6993/1000000: episode: 968, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000586, mae: 0.015867, mean_q: 0.015794
   6998/1000000: episode: 969, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000381, mae: 0.010580, mean_q: 0.013101
   7003/1000000: episode: 970, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000299, mae: 0.011501, mean_q: 0.013827
   7008/1000000: episode: 971, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000694, mae: 0.015396, mean_q: 0.017459
   7013/1000000: episode: 972, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.002844, mae: 0.015877, mean_q: 0.013547
   7018/1000000: episode: 973, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000355, mae: 0.012403, mean_q: 0.013344
   7024/1000000: episode: 974, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000942, mae: 0.017232, mean_q: 0.017457
   7029/1000000: episode: 975, duration: 0.026s, episode steps: 5, steps per second: 191, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000362, mae: 0.013816, mean_q: 0.018408
   7034/1000000: episode: 976, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.002789, mae: 0.016534, mean_q: 0.011219
   7039/1000000: episode: 977, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000495, mae: 0.012881, mean_q: 0.014382
   7044/1000000: episode: 978, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.002844, mae: 0.016573, mean_q: 0.012899
   7049/1000000: episode: 979, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005991, mae: 0.025576, mean_q: 0.012788
   7054/1000000: episode: 980, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000374, mae: 0.012489, mean_q: 0.016964
   7059/1000000: episode: 981, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000649, mae: 0.014009, mean_q: 0.014104
   7064/1000000: episode: 982, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.003172, mae: 0.021054, mean_q: 0.018074
   7069/1000000: episode: 983, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000545, mae: 0.012859, mean_q: 0.012533
   7075/1000000: episode: 984, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000187, mae: 0.009430, mean_q: 0.010884
   7080/1000000: episode: 985, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000232, mae: 0.011053, mean_q: 0.011285
   7085/1000000: episode: 986, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000417, mae: 0.013955, mean_q: 0.013306
   7090/1000000: episode: 987, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000307, mae: 0.013734, mean_q: 0.014467
   7095/1000000: episode: 988, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000534, mae: 0.015138, mean_q: 0.011525
   7100/1000000: episode: 989, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000199, mae: 0.011115, mean_q: 0.014136
   7105/1000000: episode: 990, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000313, mae: 0.012046, mean_q: 0.012633
   7110/1000000: episode: 991, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000802, mae: 0.017922, mean_q: 0.017519
   7115/1000000: episode: 992, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000279, mae: 0.012471, mean_q: 0.012426
   7121/1000000: episode: 993, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000793, mae: 0.014111, mean_q: 0.011291
   7126/1000000: episode: 994, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.001074, mae: 0.017964, mean_q: 0.015445
   7131/1000000: episode: 995, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000379, mae: 0.013288, mean_q: 0.016219
   7136/1000000: episode: 996, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000379, mae: 0.013313, mean_q: 0.014617
   7141/1000000: episode: 997, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000309, mae: 0.011911, mean_q: 0.012453
   7146/1000000: episode: 998, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002872, mae: 0.017921, mean_q: 0.013658
   7151/1000000: episode: 999, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000576, mae: 0.012754, mean_q: 0.010620
   7156/1000000: episode: 1000, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000552, mae: 0.012479, mean_q: 0.012583
   7161/1000000: episode: 1001, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.002860, mae: 0.016961, mean_q: 0.013804
   7166/1000000: episode: 1002, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002951, mae: 0.018388, mean_q: 0.016442
   7171/1000000: episode: 1003, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000555, mae: 0.012044, mean_q: 0.012987
   7177/1000000: episode: 1004, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000232, mae: 0.009061, mean_q: 0.010758
   7182/1000000: episode: 1005, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000526, mae: 0.012853, mean_q: 0.012962
   7187/1000000: episode: 1006, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000155, mae: 0.008292, mean_q: 0.007579
   7192/1000000: episode: 1007, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.002898, mae: 0.017210, mean_q: 0.017613
   7197/1000000: episode: 1008, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002894, mae: 0.015847, mean_q: 0.012092
   7202/1000000: episode: 1009, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000242, mae: 0.010098, mean_q: 0.013451
   7207/1000000: episode: 1010, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.005737, mae: 0.023263, mean_q: 0.013605
   7212/1000000: episode: 1011, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001200, mae: 0.015982, mean_q: 0.015584
   7217/1000000: episode: 1012, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000313, mae: 0.012515, mean_q: 0.017496
   7222/1000000: episode: 1013, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000680, mae: 0.015917, mean_q: 0.016792
   7227/1000000: episode: 1014, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000351, mae: 0.014166, mean_q: 0.012757
   7232/1000000: episode: 1015, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000473, mae: 0.016487, mean_q: 0.018798
   7237/1000000: episode: 1016, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000640, mae: 0.016203, mean_q: 0.011420
   7243/1000000: episode: 1017, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000577, mae: 0.015897, mean_q: 0.014576
   7248/1000000: episode: 1018, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000281, mae: 0.012006, mean_q: 0.013017
   7253/1000000: episode: 1019, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.003095, mae: 0.016527, mean_q: 0.008545
   7258/1000000: episode: 1020, duration: 0.026s, episode steps: 5, steps per second: 192, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.003173, mae: 0.018801, mean_q: 0.014766
   7263/1000000: episode: 1021, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000256, mae: 0.010946, mean_q: 0.012370
   7268/1000000: episode: 1022, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.002978, mae: 0.019220, mean_q: 0.015650
   7273/1000000: episode: 1023, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000334, mae: 0.012983, mean_q: 0.017177
   7279/1000000: episode: 1024, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.002837, mae: 0.020549, mean_q: 0.015665
   7284/1000000: episode: 1025, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.003265, mae: 0.021742, mean_q: 0.017591
   7289/1000000: episode: 1026, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000370, mae: 0.012358, mean_q: 0.014285
   7294/1000000: episode: 1027, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000665, mae: 0.015486, mean_q: 0.016075
   7299/1000000: episode: 1028, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000684, mae: 0.015522, mean_q: 0.017800
   7304/1000000: episode: 1029, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000957, mae: 0.016368, mean_q: 0.015409
   7309/1000000: episode: 1030, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000449, mae: 0.016092, mean_q: 0.014802
   7314/1000000: episode: 1031, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000547, mae: 0.012119, mean_q: 0.009933
   7319/1000000: episode: 1032, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000331, mae: 0.012686, mean_q: 0.017401
   7324/1000000: episode: 1033, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000185, mae: 0.009034, mean_q: 0.011042
   7329/1000000: episode: 1034, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000633, mae: 0.013239, mean_q: 0.012991
   7334/1000000: episode: 1035, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.003516, mae: 0.021460, mean_q: 0.015870
   7339/1000000: episode: 1036, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005563, mae: 0.026024, mean_q: 0.018519
   7344/1000000: episode: 1037, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000214, mae: 0.010438, mean_q: 0.011882
   7349/1000000: episode: 1038, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000587, mae: 0.012908, mean_q: 0.015032
   7354/1000000: episode: 1039, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.003124, mae: 0.021310, mean_q: 0.017243
   7360/1000000: episode: 1040, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002506, mae: 0.020621, mean_q: 0.016628
   7365/1000000: episode: 1041, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000871, mae: 0.019151, mean_q: 0.019192
   7370/1000000: episode: 1042, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000677, mae: 0.015507, mean_q: 0.015003
   7376/1000000: episode: 1043, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000501, mae: 0.014320, mean_q: 0.017633
   7381/1000000: episode: 1044, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000710, mae: 0.015959, mean_q: 0.016512
   7386/1000000: episode: 1045, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.005620, mae: 0.024753, mean_q: 0.015793
   7391/1000000: episode: 1046, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.005397, mae: 0.024787, mean_q: 0.018339
   7396/1000000: episode: 1047, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000402, mae: 0.012823, mean_q: 0.016092
   7402/1000000: episode: 1048, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000777, mae: 0.014274, mean_q: 0.015194
   7407/1000000: episode: 1049, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000736, mae: 0.016711, mean_q: 0.018043
   7412/1000000: episode: 1050, duration: 0.026s, episode steps: 5, steps per second: 192, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000331, mae: 0.014541, mean_q: 0.014940
   7417/1000000: episode: 1051, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002775, mae: 0.017943, mean_q: 0.014598
   7422/1000000: episode: 1052, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000392, mae: 0.014724, mean_q: 0.021918
   7427/1000000: episode: 1053, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000558, mae: 0.010679, mean_q: 0.011647
   7432/1000000: episode: 1054, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.003344, mae: 0.023616, mean_q: 0.020380
[Info] 2-TH LEVEL FOUND: 0.10420779138803482, Considering 29/100 traces
   7438/1000000: episode: 1055, duration: 0.697s, episode steps: 6, steps per second: 9, episode reward: 1.308, mean reward: 0.218 [0.018, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 8.250 [6.000, 11.000], loss: 0.000370, mae: 0.013659, mean_q: 0.017522
   7441/1000000: episode: 1056, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000754, mae: 0.014928, mean_q: 0.013785
   7445/1000000: episode: 1057, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000330, mae: 0.012377, mean_q: 0.011582
   7448/1000000: episode: 1058, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000555, mae: 0.015190, mean_q: 0.016111
   7451/1000000: episode: 1059, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000356, mae: 0.013469, mean_q: 0.019377
   7454/1000000: episode: 1060, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000882, mae: 0.018384, mean_q: 0.013481
   7457/1000000: episode: 1061, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000426, mae: 0.013997, mean_q: 0.011692
   7460/1000000: episode: 1062, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000344, mae: 0.013833, mean_q: 0.018056
   7463/1000000: episode: 1063, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000833, mae: 0.015882, mean_q: 0.014289
   7466/1000000: episode: 1064, duration: 0.017s, episode steps: 3, steps per second: 171, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000598, mae: 0.013890, mean_q: 0.015668
   7469/1000000: episode: 1065, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000473, mae: 0.014993, mean_q: 0.015943
   7472/1000000: episode: 1066, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000295, mae: 0.012422, mean_q: 0.016574
   7475/1000000: episode: 1067, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000281, mae: 0.010962, mean_q: 0.013251
   7478/1000000: episode: 1068, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000360, mae: 0.013605, mean_q: 0.012163
   7481/1000000: episode: 1069, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000823, mae: 0.015321, mean_q: 0.014669
   7484/1000000: episode: 1070, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000205, mae: 0.011789, mean_q: 0.015159
   7487/1000000: episode: 1071, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005086, mae: 0.024862, mean_q: 0.017268
   7490/1000000: episode: 1072, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000424, mae: 0.015650, mean_q: 0.016131
   7493/1000000: episode: 1073, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000737, mae: 0.016066, mean_q: 0.016427
   7496/1000000: episode: 1074, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000862, mae: 0.017096, mean_q: 0.010171
   7499/1000000: episode: 1075, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004563, mae: 0.022576, mean_q: 0.017700
   7502/1000000: episode: 1076, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001019, mae: 0.019895, mean_q: 0.014993
   7505/1000000: episode: 1077, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000284, mae: 0.011920, mean_q: 0.016311
   7508/1000000: episode: 1078, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000180, mae: 0.009147, mean_q: 0.010165
   7511/1000000: episode: 1079, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004449, mae: 0.017368, mean_q: 0.012281
   7514/1000000: episode: 1080, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000369, mae: 0.012681, mean_q: 0.015490
   7517/1000000: episode: 1081, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000282, mae: 0.010575, mean_q: 0.012770
   7520/1000000: episode: 1082, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000539, mae: 0.017741, mean_q: 0.022714
   7523/1000000: episode: 1083, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000131, mae: 0.008974, mean_q: 0.009750
   7526/1000000: episode: 1084, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000233, mae: 0.009082, mean_q: 0.010177
   7529/1000000: episode: 1085, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000331, mae: 0.012416, mean_q: 0.014938
   7532/1000000: episode: 1086, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000507, mae: 0.016497, mean_q: 0.018962
[Info] FALSIFICATION!
   7535/1000000: episode: 1087, duration: 0.260s, episode steps: 3, steps per second: 12, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.004757, mae: 0.025569, mean_q: 0.020561
   7538/1000000: episode: 1088, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000210, mae: 0.013123, mean_q: 0.012402
   7542/1000000: episode: 1089, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000313, mae: 0.015088, mean_q: 0.011256
   7545/1000000: episode: 1090, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000261, mae: 0.013313, mean_q: 0.018038
   7548/1000000: episode: 1091, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000858, mae: 0.016565, mean_q: 0.011470
   7551/1000000: episode: 1092, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004866, mae: 0.027226, mean_q: 0.021234
   7554/1000000: episode: 1093, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001003, mae: 0.020057, mean_q: 0.018165
   7557/1000000: episode: 1094, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005776, mae: 0.031927, mean_q: 0.024622
   7560/1000000: episode: 1095, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000248, mae: 0.010760, mean_q: 0.010289
   7564/1000000: episode: 1096, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000274, mae: 0.010249, mean_q: 0.009818
   7567/1000000: episode: 1097, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.008725, mae: 0.030913, mean_q: 0.018255
   7570/1000000: episode: 1098, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000400, mae: 0.013768, mean_q: 0.016177
   7573/1000000: episode: 1099, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004471, mae: 0.019658, mean_q: 0.014582
   7576/1000000: episode: 1100, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002623, mae: 0.028960, mean_q: 0.022391
   7579/1000000: episode: 1101, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000435, mae: 0.014866, mean_q: 0.017316
   7582/1000000: episode: 1102, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004470, mae: 0.019758, mean_q: 0.015012
   7586/1000000: episode: 1103, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000475, mae: 0.015502, mean_q: 0.018922
   7590/1000000: episode: 1104, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000699, mae: 0.016340, mean_q: 0.018068
   7593/1000000: episode: 1105, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000628, mae: 0.018228, mean_q: 0.024512
   7596/1000000: episode: 1106, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001254, mae: 0.016708, mean_q: 0.013296
   7599/1000000: episode: 1107, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000545, mae: 0.016027, mean_q: 0.021600
   7602/1000000: episode: 1108, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000889, mae: 0.016470, mean_q: 0.017542
   7605/1000000: episode: 1109, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000463, mae: 0.014104, mean_q: 0.017132
   7608/1000000: episode: 1110, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000846, mae: 0.015090, mean_q: 0.014700
   7611/1000000: episode: 1111, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004524, mae: 0.021559, mean_q: 0.016780
   7614/1000000: episode: 1112, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001128, mae: 0.021721, mean_q: 0.021005
   7617/1000000: episode: 1113, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000537, mae: 0.018057, mean_q: 0.023511
   7620/1000000: episode: 1114, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005242, mae: 0.026964, mean_q: 0.018602
   7623/1000000: episode: 1115, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001515, mae: 0.024054, mean_q: 0.019889
   7626/1000000: episode: 1116, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000759, mae: 0.012928, mean_q: 0.013335
   7629/1000000: episode: 1117, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000876, mae: 0.017397, mean_q: 0.019961
   7632/1000000: episode: 1118, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005815, mae: 0.030703, mean_q: 0.018866
   7635/1000000: episode: 1119, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004701, mae: 0.026635, mean_q: 0.023023
   7638/1000000: episode: 1120, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000347, mae: 0.013568, mean_q: 0.017595
   7641/1000000: episode: 1121, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000408, mae: 0.014855, mean_q: 0.015940
   7644/1000000: episode: 1122, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000540, mae: 0.018807, mean_q: 0.026938
   7647/1000000: episode: 1123, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001009, mae: 0.019101, mean_q: 0.021609
   7650/1000000: episode: 1124, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000255, mae: 0.010628, mean_q: 0.014718
   7653/1000000: episode: 1125, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000751, mae: 0.016592, mean_q: 0.020866
[Info] Complete ISplit Iteration
[Info] Levels: [0.039437152, 0.10420779, 0.104443684]
[Info] Cond. Prob: [0.11, 0.29, 1.0]
[Info] Error Prob: 0.0319

   7656/1000000: episode: 1126, duration: 0.853s, episode steps: 3, steps per second: 4, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000355, mae: 0.013033, mean_q: 0.017619
   7666/1000000: episode: 1127, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.002399, mae: 0.021044, mean_q: 0.016201
   7676/1000000: episode: 1128, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001841, mae: 0.018981, mean_q: 0.021564
   7686/1000000: episode: 1129, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000801, mae: 0.017702, mean_q: 0.018595
   7696/1000000: episode: 1130, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000644, mae: 0.018659, mean_q: 0.021332
   7706/1000000: episode: 1131, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000816, mae: 0.019295, mean_q: 0.020721
   7716/1000000: episode: 1132, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000670, mae: 0.017456, mean_q: 0.017134
   7726/1000000: episode: 1133, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.003205, mae: 0.022976, mean_q: 0.018200
   7736/1000000: episode: 1134, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.002405, mae: 0.021543, mean_q: 0.018085
   7746/1000000: episode: 1135, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001010, mae: 0.020357, mean_q: 0.021157
   7756/1000000: episode: 1136, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000526, mae: 0.012648, mean_q: 0.015267
   7766/1000000: episode: 1137, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000827, mae: 0.018066, mean_q: 0.019098
   7776/1000000: episode: 1138, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000431, mae: 0.014825, mean_q: 0.015606
   7786/1000000: episode: 1139, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002109, mae: 0.020262, mean_q: 0.018072
   7796/1000000: episode: 1140, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000611, mae: 0.018358, mean_q: 0.019424
   7806/1000000: episode: 1141, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000677, mae: 0.016673, mean_q: 0.015373
   7816/1000000: episode: 1142, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.003339, mae: 0.021256, mean_q: 0.015635
   7826/1000000: episode: 1143, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000778, mae: 0.018271, mean_q: 0.019638
   7836/1000000: episode: 1144, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.002946, mae: 0.020224, mean_q: 0.019022
   7846/1000000: episode: 1145, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000415, mae: 0.013988, mean_q: 0.016622
   7856/1000000: episode: 1146, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000831, mae: 0.016244, mean_q: 0.015771
   7866/1000000: episode: 1147, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002320, mae: 0.021763, mean_q: 0.018513
   7876/1000000: episode: 1148, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000499, mae: 0.015723, mean_q: 0.018593
   7886/1000000: episode: 1149, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000603, mae: 0.016085, mean_q: 0.017698
   7896/1000000: episode: 1150, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000620, mae: 0.017353, mean_q: 0.019498
   7906/1000000: episode: 1151, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000522, mae: 0.017456, mean_q: 0.018518
   7916/1000000: episode: 1152, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000451, mae: 0.012834, mean_q: 0.016368
   7926/1000000: episode: 1153, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000518, mae: 0.013979, mean_q: 0.014960
   7936/1000000: episode: 1154, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000838, mae: 0.016574, mean_q: 0.016632
   7946/1000000: episode: 1155, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.003169, mae: 0.021643, mean_q: 0.017338
   7956/1000000: episode: 1156, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000496, mae: 0.015641, mean_q: 0.019501
   7966/1000000: episode: 1157, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.002561, mae: 0.023156, mean_q: 0.020144
   7976/1000000: episode: 1158, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001724, mae: 0.017003, mean_q: 0.017008
   7986/1000000: episode: 1159, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000613, mae: 0.016222, mean_q: 0.020084
   7996/1000000: episode: 1160, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000670, mae: 0.015978, mean_q: 0.017740
   8006/1000000: episode: 1161, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000575, mae: 0.015308, mean_q: 0.015902
   8016/1000000: episode: 1162, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000486, mae: 0.013821, mean_q: 0.015839
   8026/1000000: episode: 1163, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002005, mae: 0.019181, mean_q: 0.016685
   8036/1000000: episode: 1164, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.003102, mae: 0.019338, mean_q: 0.015243
   8046/1000000: episode: 1165, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000696, mae: 0.015782, mean_q: 0.017440
   8056/1000000: episode: 1166, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001808, mae: 0.017681, mean_q: 0.018125
   8066/1000000: episode: 1167, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000753, mae: 0.014646, mean_q: 0.015774
   8076/1000000: episode: 1168, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000347, mae: 0.014100, mean_q: 0.014476
   8086/1000000: episode: 1169, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000864, mae: 0.020654, mean_q: 0.021704
   8096/1000000: episode: 1170, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000961, mae: 0.017439, mean_q: 0.017273
   8106/1000000: episode: 1171, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.002194, mae: 0.020988, mean_q: 0.018218
   8116/1000000: episode: 1172, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.002126, mae: 0.021299, mean_q: 0.021718
   8126/1000000: episode: 1173, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000472, mae: 0.015500, mean_q: 0.016679
   8136/1000000: episode: 1174, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000744, mae: 0.016888, mean_q: 0.017661
   8146/1000000: episode: 1175, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000298, mae: 0.011972, mean_q: 0.014733
   8156/1000000: episode: 1176, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.215, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.004482, mae: 0.024659, mean_q: 0.021200
   8166/1000000: episode: 1177, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001953, mae: 0.020574, mean_q: 0.022582
   8176/1000000: episode: 1178, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000635, mae: 0.016890, mean_q: 0.020091
   8186/1000000: episode: 1179, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002891, mae: 0.018628, mean_q: 0.015436
   8196/1000000: episode: 1180, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000405, mae: 0.013854, mean_q: 0.017945
   8206/1000000: episode: 1181, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000586, mae: 0.015469, mean_q: 0.018468
   8216/1000000: episode: 1182, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.003397, mae: 0.022057, mean_q: 0.016774
   8226/1000000: episode: 1183, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000865, mae: 0.016631, mean_q: 0.016671
   8236/1000000: episode: 1184, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001976, mae: 0.021866, mean_q: 0.020141
   8246/1000000: episode: 1185, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000685, mae: 0.016437, mean_q: 0.018109
   8256/1000000: episode: 1186, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000516, mae: 0.013606, mean_q: 0.014603
   8266/1000000: episode: 1187, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001770, mae: 0.018829, mean_q: 0.019307
   8276/1000000: episode: 1188, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001865, mae: 0.018409, mean_q: 0.016712
   8286/1000000: episode: 1189, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.002003, mae: 0.020010, mean_q: 0.020161
   8296/1000000: episode: 1190, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000379, mae: 0.014051, mean_q: 0.016200
   8306/1000000: episode: 1191, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000526, mae: 0.012371, mean_q: 0.011909
   8316/1000000: episode: 1192, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000883, mae: 0.017754, mean_q: 0.018092
   8326/1000000: episode: 1193, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000580, mae: 0.015050, mean_q: 0.015537
   8336/1000000: episode: 1194, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000497, mae: 0.017380, mean_q: 0.019214
   8346/1000000: episode: 1195, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.005046, mae: 0.027939, mean_q: 0.017612
   8356/1000000: episode: 1196, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000756, mae: 0.017876, mean_q: 0.018818
   8366/1000000: episode: 1197, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000762, mae: 0.019122, mean_q: 0.020697
   8376/1000000: episode: 1198, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001615, mae: 0.017036, mean_q: 0.016437
   8386/1000000: episode: 1199, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000658, mae: 0.017293, mean_q: 0.020038
   8396/1000000: episode: 1200, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000548, mae: 0.016022, mean_q: 0.016582
   8406/1000000: episode: 1201, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000633, mae: 0.016796, mean_q: 0.015436
   8416/1000000: episode: 1202, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001810, mae: 0.017529, mean_q: 0.015634
   8426/1000000: episode: 1203, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001704, mae: 0.017467, mean_q: 0.017589
   8436/1000000: episode: 1204, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000819, mae: 0.014832, mean_q: 0.014291
   8446/1000000: episode: 1205, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000515, mae: 0.012598, mean_q: 0.012965
   8456/1000000: episode: 1206, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001815, mae: 0.017053, mean_q: 0.015710
   8466/1000000: episode: 1207, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000755, mae: 0.016297, mean_q: 0.017774
   8476/1000000: episode: 1208, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000304, mae: 0.013519, mean_q: 0.013596
   8486/1000000: episode: 1209, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000703, mae: 0.017234, mean_q: 0.016192
   8496/1000000: episode: 1210, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000898, mae: 0.017719, mean_q: 0.014793
   8506/1000000: episode: 1211, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000623, mae: 0.013446, mean_q: 0.014604
   8516/1000000: episode: 1212, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000607, mae: 0.013584, mean_q: 0.015026
   8526/1000000: episode: 1213, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000622, mae: 0.013998, mean_q: 0.015370
   8536/1000000: episode: 1214, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000463, mae: 0.012365, mean_q: 0.014742
   8546/1000000: episode: 1215, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000921, mae: 0.014360, mean_q: 0.012760
   8556/1000000: episode: 1216, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000719, mae: 0.014896, mean_q: 0.016761
   8566/1000000: episode: 1217, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000731, mae: 0.015243, mean_q: 0.014961
   8576/1000000: episode: 1218, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.002880, mae: 0.017109, mean_q: 0.014513
   8586/1000000: episode: 1219, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000554, mae: 0.014630, mean_q: 0.016872
   8596/1000000: episode: 1220, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001613, mae: 0.015417, mean_q: 0.013280
   8606/1000000: episode: 1221, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000463, mae: 0.013380, mean_q: 0.015323
   8616/1000000: episode: 1222, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000259, mae: 0.010817, mean_q: 0.010382
   8626/1000000: episode: 1223, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000478, mae: 0.013574, mean_q: 0.014829
   8636/1000000: episode: 1224, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001871, mae: 0.018792, mean_q: 0.017942
   8646/1000000: episode: 1225, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000632, mae: 0.012857, mean_q: 0.015120
[Info] 1-TH LEVEL FOUND: 0.006720848381519318, Considering 13/100 traces
   8656/1000000: episode: 1226, duration: 0.697s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000339, mae: 0.011389, mean_q: 0.014346
   8663/1000000: episode: 1227, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000569, mae: 0.013088, mean_q: 0.013930
   8670/1000000: episode: 1228, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000308, mae: 0.011162, mean_q: 0.013681
   8676/1000000: episode: 1229, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000383, mae: 0.013449, mean_q: 0.016059
   8682/1000000: episode: 1230, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000478, mae: 0.010868, mean_q: 0.013358
   8688/1000000: episode: 1231, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000318, mae: 0.011533, mean_q: 0.014396
   8694/1000000: episode: 1232, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000297, mae: 0.010401, mean_q: 0.012849
   8700/1000000: episode: 1233, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000293, mae: 0.011282, mean_q: 0.011427
   8707/1000000: episode: 1234, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.002563, mae: 0.019776, mean_q: 0.013073
   8713/1000000: episode: 1235, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000283, mae: 0.011763, mean_q: 0.011380
   8719/1000000: episode: 1236, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000393, mae: 0.011327, mean_q: 0.011212
   8725/1000000: episode: 1237, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.023, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000513, mae: 0.012438, mean_q: 0.012833
   8731/1000000: episode: 1238, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000463, mae: 0.010460, mean_q: 0.012201
   8738/1000000: episode: 1239, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000438, mae: 0.011959, mean_q: 0.014541
   8744/1000000: episode: 1240, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000421, mae: 0.010668, mean_q: 0.012173
   8750/1000000: episode: 1241, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000409, mae: 0.014046, mean_q: 0.017453
   8756/1000000: episode: 1242, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000324, mae: 0.012576, mean_q: 0.015449
   8762/1000000: episode: 1243, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.002487, mae: 0.017297, mean_q: 0.014987
   8768/1000000: episode: 1244, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000206, mae: 0.010461, mean_q: 0.012723
   8774/1000000: episode: 1245, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.002932, mae: 0.018034, mean_q: 0.014418
   8780/1000000: episode: 1246, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000279, mae: 0.011511, mean_q: 0.015758
   8786/1000000: episode: 1247, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000301, mae: 0.011884, mean_q: 0.015613
   8792/1000000: episode: 1248, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000476, mae: 0.013128, mean_q: 0.018577
   8798/1000000: episode: 1249, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000397, mae: 0.013889, mean_q: 0.017340
   8804/1000000: episode: 1250, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000530, mae: 0.012774, mean_q: 0.015297
   8810/1000000: episode: 1251, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.023, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000606, mae: 0.014418, mean_q: 0.015391
   8816/1000000: episode: 1252, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000533, mae: 0.013196, mean_q: 0.015606
   8822/1000000: episode: 1253, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000298, mae: 0.011750, mean_q: 0.010888
   8828/1000000: episode: 1254, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000222, mae: 0.010958, mean_q: 0.012001
   8834/1000000: episode: 1255, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000298, mae: 0.011860, mean_q: 0.014909
   8841/1000000: episode: 1256, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000370, mae: 0.013070, mean_q: 0.015832
   8847/1000000: episode: 1257, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000256, mae: 0.011315, mean_q: 0.012191
   8853/1000000: episode: 1258, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000510, mae: 0.013176, mean_q: 0.012870
   8859/1000000: episode: 1259, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000518, mae: 0.011545, mean_q: 0.013039
   8865/1000000: episode: 1260, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000156, mae: 0.008583, mean_q: 0.013032
   8872/1000000: episode: 1261, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000503, mae: 0.011210, mean_q: 0.011025
   8878/1000000: episode: 1262, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000217, mae: 0.010205, mean_q: 0.014923
   8884/1000000: episode: 1263, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000209, mae: 0.009697, mean_q: 0.011384
   8890/1000000: episode: 1264, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000434, mae: 0.009796, mean_q: 0.009801
   8896/1000000: episode: 1265, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.002419, mae: 0.015082, mean_q: 0.014139
   8902/1000000: episode: 1266, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000271, mae: 0.011331, mean_q: 0.013632
   8908/1000000: episode: 1267, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000249, mae: 0.011421, mean_q: 0.015685
   8914/1000000: episode: 1268, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000204, mae: 0.010717, mean_q: 0.010075
   8920/1000000: episode: 1269, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000221, mae: 0.011475, mean_q: 0.010803
   8926/1000000: episode: 1270, duration: 0.029s, episode steps: 6, steps per second: 203, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000226, mae: 0.011439, mean_q: 0.012461
   8932/1000000: episode: 1271, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000572, mae: 0.014663, mean_q: 0.014040
   8938/1000000: episode: 1272, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000257, mae: 0.011830, mean_q: 0.014205
   8944/1000000: episode: 1273, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000163, mae: 0.009738, mean_q: 0.009336
   8950/1000000: episode: 1274, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000531, mae: 0.013340, mean_q: 0.015638
   8956/1000000: episode: 1275, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001243, mae: 0.016934, mean_q: 0.015496
   8962/1000000: episode: 1276, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000276, mae: 0.010961, mean_q: 0.014896
   8968/1000000: episode: 1277, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.077, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000194, mae: 0.009691, mean_q: 0.011244
   8974/1000000: episode: 1278, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000617, mae: 0.014627, mean_q: 0.015518
   8980/1000000: episode: 1279, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000212, mae: 0.009253, mean_q: 0.010935
   8986/1000000: episode: 1280, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002358, mae: 0.015253, mean_q: 0.012654
   8992/1000000: episode: 1281, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000265, mae: 0.010495, mean_q: 0.014067
   8998/1000000: episode: 1282, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000260, mae: 0.010955, mean_q: 0.013643
   9005/1000000: episode: 1283, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000465, mae: 0.013426, mean_q: 0.015240
   9011/1000000: episode: 1284, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000377, mae: 0.013665, mean_q: 0.016155
   9017/1000000: episode: 1285, duration: 0.030s, episode steps: 6, steps per second: 199, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000717, mae: 0.013221, mean_q: 0.013886
   9023/1000000: episode: 1286, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000273, mae: 0.010854, mean_q: 0.011954
   9029/1000000: episode: 1287, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000108, mae: 0.007669, mean_q: 0.008975
   9035/1000000: episode: 1288, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.002342, mae: 0.014759, mean_q: 0.010298
   9041/1000000: episode: 1289, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000311, mae: 0.013495, mean_q: 0.015860
   9047/1000000: episode: 1290, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000689, mae: 0.013477, mean_q: 0.012269
   9053/1000000: episode: 1291, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000557, mae: 0.015243, mean_q: 0.016083
   9060/1000000: episode: 1292, duration: 0.035s, episode steps: 7, steps per second: 203, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000266, mae: 0.013081, mean_q: 0.011849
   9066/1000000: episode: 1293, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000288, mae: 0.012701, mean_q: 0.017291
   9073/1000000: episode: 1294, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000328, mae: 0.011993, mean_q: 0.014092
   9079/1000000: episode: 1295, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000124, mae: 0.007755, mean_q: 0.010684
   9085/1000000: episode: 1296, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000229, mae: 0.010443, mean_q: 0.014418
   9091/1000000: episode: 1297, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000481, mae: 0.011828, mean_q: 0.011464
   9097/1000000: episode: 1298, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.046, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000501, mae: 0.011341, mean_q: 0.013181
   9103/1000000: episode: 1299, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000452, mae: 0.011504, mean_q: 0.013339
   9110/1000000: episode: 1300, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.002363, mae: 0.017205, mean_q: 0.015223
   9116/1000000: episode: 1301, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.014, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000328, mae: 0.012907, mean_q: 0.015870
   9122/1000000: episode: 1302, duration: 0.029s, episode steps: 6, steps per second: 203, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000260, mae: 0.010693, mean_q: 0.015136
   9128/1000000: episode: 1303, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.018, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.583 [-1.000, 11.000], loss: 0.000204, mae: 0.010097, mean_q: 0.013906
   9135/1000000: episode: 1304, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.002440, mae: 0.017526, mean_q: 0.015211
   9141/1000000: episode: 1305, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002466, mae: 0.018197, mean_q: 0.016630
   9147/1000000: episode: 1306, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000559, mae: 0.014436, mean_q: 0.016059
   9153/1000000: episode: 1307, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.010, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.417 [-1.000, 11.000], loss: 0.000298, mae: 0.012093, mean_q: 0.015182
   9159/1000000: episode: 1308, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000164, mae: 0.008965, mean_q: 0.013648
   9165/1000000: episode: 1309, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000413, mae: 0.014730, mean_q: 0.020870
   9171/1000000: episode: 1310, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.030, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000277, mae: 0.013626, mean_q: 0.013391
   9177/1000000: episode: 1311, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.034, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000278, mae: 0.014520, mean_q: 0.014919
   9183/1000000: episode: 1312, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002849, mae: 0.020329, mean_q: 0.017805
[Info] 2-TH LEVEL FOUND: 0.10107281804084778, Considering 13/100 traces
   9189/1000000: episode: 1313, duration: 0.680s, episode steps: 6, steps per second: 9, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000290, mae: 0.012111, mean_q: 0.016675
   9192/1000000: episode: 1314, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004660, mae: 0.024938, mean_q: 0.019356
   9195/1000000: episode: 1315, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000343, mae: 0.013080, mean_q: 0.011563
   9198/1000000: episode: 1316, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.008683, mae: 0.031967, mean_q: 0.021353
   9201/1000000: episode: 1317, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000885, mae: 0.016661, mean_q: 0.013026
   9204/1000000: episode: 1318, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000386, mae: 0.013562, mean_q: 0.017124
   9207/1000000: episode: 1319, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000768, mae: 0.016258, mean_q: 0.014764
   9210/1000000: episode: 1320, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000494, mae: 0.015794, mean_q: 0.015724
   9213/1000000: episode: 1321, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000439, mae: 0.015171, mean_q: 0.020309
   9216/1000000: episode: 1322, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000765, mae: 0.015935, mean_q: 0.014975
   9219/1000000: episode: 1323, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000255, mae: 0.011789, mean_q: 0.012959
   9222/1000000: episode: 1324, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000247, mae: 0.012591, mean_q: 0.012655
   9225/1000000: episode: 1325, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000234, mae: 0.014164, mean_q: 0.009384
   9228/1000000: episode: 1326, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000357, mae: 0.014699, mean_q: 0.017677
   9231/1000000: episode: 1327, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000416, mae: 0.017029, mean_q: 0.017474
   9234/1000000: episode: 1328, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000319, mae: 0.016294, mean_q: 0.016219
   9237/1000000: episode: 1329, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000250, mae: 0.012628, mean_q: 0.016079
   9240/1000000: episode: 1330, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005090, mae: 0.026391, mean_q: 0.015388
   9243/1000000: episode: 1331, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000381, mae: 0.016365, mean_q: 0.020927
   9246/1000000: episode: 1332, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000285, mae: 0.011874, mean_q: 0.014042
   9249/1000000: episode: 1333, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004590, mae: 0.024335, mean_q: 0.015450
   9252/1000000: episode: 1334, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000332, mae: 0.013147, mean_q: 0.017457
   9255/1000000: episode: 1335, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004962, mae: 0.025588, mean_q: 0.020660
   9258/1000000: episode: 1336, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000410, mae: 0.013280, mean_q: 0.016674
   9261/1000000: episode: 1337, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000381, mae: 0.013620, mean_q: 0.017046
   9264/1000000: episode: 1338, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000380, mae: 0.015296, mean_q: 0.019799
   9267/1000000: episode: 1339, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000793, mae: 0.014783, mean_q: 0.016903
   9270/1000000: episode: 1340, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000356, mae: 0.014225, mean_q: 0.018216
   9273/1000000: episode: 1341, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000185, mae: 0.009019, mean_q: 0.011819
   9276/1000000: episode: 1342, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001139, mae: 0.019690, mean_q: 0.023613
   9279/1000000: episode: 1343, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000250, mae: 0.010706, mean_q: 0.015595
   9282/1000000: episode: 1344, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000461, mae: 0.015265, mean_q: 0.017623
   9285/1000000: episode: 1345, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000509, mae: 0.017009, mean_q: 0.021360
   9288/1000000: episode: 1346, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000706, mae: 0.013554, mean_q: 0.013835
   9291/1000000: episode: 1347, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004536, mae: 0.023163, mean_q: 0.020158
   9294/1000000: episode: 1348, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000375, mae: 0.015216, mean_q: 0.019575
   9297/1000000: episode: 1349, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000486, mae: 0.017798, mean_q: 0.019858
   9300/1000000: episode: 1350, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000543, mae: 0.016500, mean_q: 0.018766
   9303/1000000: episode: 1351, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000313, mae: 0.015049, mean_q: 0.014674
   9306/1000000: episode: 1352, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000731, mae: 0.024493, mean_q: 0.025654
   9309/1000000: episode: 1353, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000437, mae: 0.016538, mean_q: 0.021225
   9312/1000000: episode: 1354, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000693, mae: 0.023445, mean_q: 0.018699
   9315/1000000: episode: 1355, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000358, mae: 0.015163, mean_q: 0.021241
   9318/1000000: episode: 1356, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000377, mae: 0.015531, mean_q: 0.012916
   9321/1000000: episode: 1357, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004643, mae: 0.024948, mean_q: 0.016804
   9324/1000000: episode: 1358, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000439, mae: 0.018629, mean_q: 0.024982
   9327/1000000: episode: 1359, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004730, mae: 0.026665, mean_q: 0.020616
   9330/1000000: episode: 1360, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000400, mae: 0.015482, mean_q: 0.016149
   9333/1000000: episode: 1361, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.004473, mae: 0.021876, mean_q: 0.018645
   9336/1000000: episode: 1362, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001573, mae: 0.018181, mean_q: 0.012939
   9339/1000000: episode: 1363, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000349, mae: 0.012735, mean_q: 0.015787
   9342/1000000: episode: 1364, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000332, mae: 0.013280, mean_q: 0.021224
   9345/1000000: episode: 1365, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001602, mae: 0.022352, mean_q: 0.021692
   9348/1000000: episode: 1366, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000766, mae: 0.016837, mean_q: 0.021096
   9351/1000000: episode: 1367, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004538, mae: 0.022465, mean_q: 0.018109
   9354/1000000: episode: 1368, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000242, mae: 0.011045, mean_q: 0.015461
   9357/1000000: episode: 1369, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000528, mae: 0.016774, mean_q: 0.023691
   9360/1000000: episode: 1370, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000241, mae: 0.009627, mean_q: 0.013005
   9363/1000000: episode: 1371, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000451, mae: 0.014265, mean_q: 0.017591
   9366/1000000: episode: 1372, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000367, mae: 0.015093, mean_q: 0.021830
   9369/1000000: episode: 1373, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001560, mae: 0.024302, mean_q: 0.026956
   9372/1000000: episode: 1374, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000411, mae: 0.014538, mean_q: 0.019704
   9375/1000000: episode: 1375, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.005687, mae: 0.031842, mean_q: 0.027189
   9378/1000000: episode: 1376, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000321, mae: 0.012157, mean_q: 0.013276
   9381/1000000: episode: 1377, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000422, mae: 0.014792, mean_q: 0.017903
   9384/1000000: episode: 1378, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000428, mae: 0.013801, mean_q: 0.015721
   9387/1000000: episode: 1379, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004684, mae: 0.026087, mean_q: 0.020432
   9390/1000000: episode: 1380, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000249, mae: 0.011214, mean_q: 0.015645
   9393/1000000: episode: 1381, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001399, mae: 0.022665, mean_q: 0.020942
   9396/1000000: episode: 1382, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001122, mae: 0.019420, mean_q: 0.016843
   9399/1000000: episode: 1383, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000248, mae: 0.012091, mean_q: 0.018009
   9402/1000000: episode: 1384, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000691, mae: 0.019858, mean_q: 0.019260
   9405/1000000: episode: 1385, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000467, mae: 0.017114, mean_q: 0.023592
   9408/1000000: episode: 1386, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000300, mae: 0.013595, mean_q: 0.018074
   9411/1000000: episode: 1387, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000312, mae: 0.014454, mean_q: 0.019957
   9414/1000000: episode: 1388, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000903, mae: 0.017644, mean_q: 0.020622
   9417/1000000: episode: 1389, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000360, mae: 0.014821, mean_q: 0.017733
   9420/1000000: episode: 1390, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.004665, mae: 0.024616, mean_q: 0.019639
   9423/1000000: episode: 1391, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000299, mae: 0.012993, mean_q: 0.021755
   9426/1000000: episode: 1392, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.001140, mae: 0.022450, mean_q: 0.021957
   9429/1000000: episode: 1393, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000508, mae: 0.016778, mean_q: 0.020138
   9432/1000000: episode: 1394, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000402, mae: 0.015912, mean_q: 0.021100
   9435/1000000: episode: 1395, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000512, mae: 0.015577, mean_q: 0.018528
   9438/1000000: episode: 1396, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000581, mae: 0.018727, mean_q: 0.022269
   9441/1000000: episode: 1397, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000688, mae: 0.015113, mean_q: 0.015866
   9444/1000000: episode: 1398, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000241, mae: 0.013933, mean_q: 0.012833
   9447/1000000: episode: 1399, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000338, mae: 0.013812, mean_q: 0.019819
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.10107281804084778
   9450/1000000: episode: 1400, duration: 0.480s, episode steps: 3, steps per second: 6, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000507, mae: 0.017364, mean_q: 0.021713
   9460/1000000: episode: 1401, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001968, mae: 0.022769, mean_q: 0.024931
   9470/1000000: episode: 1402, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001947, mae: 0.019481, mean_q: 0.019276
   9480/1000000: episode: 1403, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000532, mae: 0.016192, mean_q: 0.020178
   9490/1000000: episode: 1404, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000669, mae: 0.016601, mean_q: 0.019937
   9500/1000000: episode: 1405, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001587, mae: 0.015579, mean_q: 0.016307
   9510/1000000: episode: 1406, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000439, mae: 0.015104, mean_q: 0.017560
   9520/1000000: episode: 1407, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000335, mae: 0.014316, mean_q: 0.017777
   9530/1000000: episode: 1408, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001748, mae: 0.019509, mean_q: 0.022206
   9540/1000000: episode: 1409, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001788, mae: 0.018235, mean_q: 0.020193
   9550/1000000: episode: 1410, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000406, mae: 0.014957, mean_q: 0.016781
   9560/1000000: episode: 1411, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000513, mae: 0.016830, mean_q: 0.019537
   9570/1000000: episode: 1412, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001973, mae: 0.020536, mean_q: 0.019178
   9580/1000000: episode: 1413, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000596, mae: 0.016663, mean_q: 0.021819
   9590/1000000: episode: 1414, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.005597, mae: 0.026223, mean_q: 0.020291
   9600/1000000: episode: 1415, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000317, mae: 0.011873, mean_q: 0.013625
   9610/1000000: episode: 1416, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001857, mae: 0.019853, mean_q: 0.017225
   9620/1000000: episode: 1417, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000719, mae: 0.017011, mean_q: 0.018403
   9630/1000000: episode: 1418, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000461, mae: 0.016447, mean_q: 0.016651
   9640/1000000: episode: 1419, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000604, mae: 0.019371, mean_q: 0.017421
   9650/1000000: episode: 1420, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001787, mae: 0.021867, mean_q: 0.018142
   9660/1000000: episode: 1421, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000303, mae: 0.012802, mean_q: 0.016212
   9670/1000000: episode: 1422, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001632, mae: 0.017978, mean_q: 0.017833
   9680/1000000: episode: 1423, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000616, mae: 0.015635, mean_q: 0.015874
   9690/1000000: episode: 1424, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000599, mae: 0.017188, mean_q: 0.019780
   9700/1000000: episode: 1425, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000439, mae: 0.014289, mean_q: 0.016292
   9710/1000000: episode: 1426, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000356, mae: 0.013994, mean_q: 0.014190
   9720/1000000: episode: 1427, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000192, mae: 0.010780, mean_q: 0.011592
   9730/1000000: episode: 1428, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000657, mae: 0.017045, mean_q: 0.019507
   9740/1000000: episode: 1429, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000646, mae: 0.014563, mean_q: 0.015363
   9750/1000000: episode: 1430, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000374, mae: 0.013556, mean_q: 0.016333
   9760/1000000: episode: 1431, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000558, mae: 0.015719, mean_q: 0.018597
   9770/1000000: episode: 1432, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000504, mae: 0.014491, mean_q: 0.014651
   9780/1000000: episode: 1433, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.003032, mae: 0.019070, mean_q: 0.016878
   9790/1000000: episode: 1434, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000513, mae: 0.015353, mean_q: 0.017886
   9800/1000000: episode: 1435, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000300, mae: 0.013622, mean_q: 0.016265
   9810/1000000: episode: 1436, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000467, mae: 0.014678, mean_q: 0.016952
   9820/1000000: episode: 1437, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001795, mae: 0.018385, mean_q: 0.015986
   9830/1000000: episode: 1438, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001833, mae: 0.019009, mean_q: 0.020158
   9840/1000000: episode: 1439, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000378, mae: 0.013743, mean_q: 0.018101
   9850/1000000: episode: 1440, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001612, mae: 0.015581, mean_q: 0.014855
   9860/1000000: episode: 1441, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000552, mae: 0.015842, mean_q: 0.016174
   9870/1000000: episode: 1442, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001608, mae: 0.016064, mean_q: 0.016832
   9880/1000000: episode: 1443, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003116, mae: 0.022658, mean_q: 0.021474
   9890/1000000: episode: 1444, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000687, mae: 0.016862, mean_q: 0.017315
   9900/1000000: episode: 1445, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.004081, mae: 0.023037, mean_q: 0.017709
   9910/1000000: episode: 1446, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002955, mae: 0.020578, mean_q: 0.020432
   9920/1000000: episode: 1447, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000653, mae: 0.015719, mean_q: 0.017819
   9930/1000000: episode: 1448, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000723, mae: 0.017863, mean_q: 0.020091
   9940/1000000: episode: 1449, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000411, mae: 0.015032, mean_q: 0.020026
   9950/1000000: episode: 1450, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000398, mae: 0.014314, mean_q: 0.016275
   9960/1000000: episode: 1451, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001698, mae: 0.017991, mean_q: 0.019050
   9970/1000000: episode: 1452, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000468, mae: 0.015575, mean_q: 0.020040
   9980/1000000: episode: 1453, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000281, mae: 0.010762, mean_q: 0.013549
   9990/1000000: episode: 1454, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001807, mae: 0.017301, mean_q: 0.015753
  10000/1000000: episode: 1455, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000656, mae: 0.014750, mean_q: 0.016971
  10010/1000000: episode: 1456, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000375, mae: 0.013171, mean_q: 0.017865
  10020/1000000: episode: 1457, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000431, mae: 0.013553, mean_q: 0.016146
  10030/1000000: episode: 1458, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001649, mae: 0.018520, mean_q: 0.016316
  10040/1000000: episode: 1459, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000529, mae: 0.015719, mean_q: 0.018610
  10050/1000000: episode: 1460, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001785, mae: 0.016147, mean_q: 0.015211
  10060/1000000: episode: 1461, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000421, mae: 0.014654, mean_q: 0.018168
  10070/1000000: episode: 1462, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000620, mae: 0.016904, mean_q: 0.017461
  10080/1000000: episode: 1463, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002985, mae: 0.019151, mean_q: 0.018654
  10090/1000000: episode: 1464, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000377, mae: 0.013560, mean_q: 0.017540
  10100/1000000: episode: 1465, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000278, mae: 0.010974, mean_q: 0.013399
  10110/1000000: episode: 1466, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000592, mae: 0.015885, mean_q: 0.018395
  10120/1000000: episode: 1467, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001764, mae: 0.016724, mean_q: 0.015360
  10130/1000000: episode: 1468, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000375, mae: 0.011825, mean_q: 0.012137
  10140/1000000: episode: 1469, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000393, mae: 0.014659, mean_q: 0.016719
  10150/1000000: episode: 1470, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001606, mae: 0.016238, mean_q: 0.015567
  10160/1000000: episode: 1471, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000257, mae: 0.011316, mean_q: 0.014346
  10170/1000000: episode: 1472, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003179, mae: 0.019213, mean_q: 0.016885
  10180/1000000: episode: 1473, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000442, mae: 0.012422, mean_q: 0.014196
  10190/1000000: episode: 1474, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001992, mae: 0.016980, mean_q: 0.014384
  10200/1000000: episode: 1475, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000238, mae: 0.010281, mean_q: 0.013545
  10210/1000000: episode: 1476, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000259, mae: 0.009818, mean_q: 0.013262
  10220/1000000: episode: 1477, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000443, mae: 0.012118, mean_q: 0.015010
  10230/1000000: episode: 1478, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000367, mae: 0.012632, mean_q: 0.014855
  10240/1000000: episode: 1479, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000273, mae: 0.011615, mean_q: 0.015305
  10250/1000000: episode: 1480, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000347, mae: 0.013148, mean_q: 0.015474
  10260/1000000: episode: 1481, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000607, mae: 0.013652, mean_q: 0.015607
  10270/1000000: episode: 1482, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000677, mae: 0.014487, mean_q: 0.015933
  10280/1000000: episode: 1483, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000494, mae: 0.010270, mean_q: 0.014272
  10290/1000000: episode: 1484, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.001773, mae: 0.016525, mean_q: 0.017845
  10300/1000000: episode: 1485, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000412, mae: 0.011886, mean_q: 0.014595
  10310/1000000: episode: 1486, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.147, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.001767, mae: 0.016242, mean_q: 0.017331
  10320/1000000: episode: 1487, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001702, mae: 0.015752, mean_q: 0.014791
  10330/1000000: episode: 1488, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001721, mae: 0.016405, mean_q: 0.018130
  10340/1000000: episode: 1489, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000300, mae: 0.011778, mean_q: 0.016653
  10350/1000000: episode: 1490, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000289, mae: 0.011294, mean_q: 0.013563
  10360/1000000: episode: 1491, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001542, mae: 0.014337, mean_q: 0.014263
  10370/1000000: episode: 1492, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000234, mae: 0.010176, mean_q: 0.012023
  10380/1000000: episode: 1493, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000243, mae: 0.010688, mean_q: 0.013090
  10390/1000000: episode: 1494, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001527, mae: 0.013106, mean_q: 0.013269
  10400/1000000: episode: 1495, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000376, mae: 0.014135, mean_q: 0.016754
  10410/1000000: episode: 1496, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000488, mae: 0.013691, mean_q: 0.017525
  10420/1000000: episode: 1497, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001581, mae: 0.014870, mean_q: 0.014149
  10430/1000000: episode: 1498, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000319, mae: 0.013273, mean_q: 0.015059
  10440/1000000: episode: 1499, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000247, mae: 0.011679, mean_q: 0.013183
[Info] 1-TH LEVEL FOUND: 0.01297333836555481, Considering 10/100 traces
  10450/1000000: episode: 1500, duration: 0.657s, episode steps: 10, steps per second: 15, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001665, mae: 0.014972, mean_q: 0.013983
  10452/1000000: episode: 1501, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000335, mae: 0.012458, mean_q: 0.013874
  10459/1000000: episode: 1502, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.002184, mae: 0.017974, mean_q: 0.014387
  10466/1000000: episode: 1503, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000274, mae: 0.011771, mean_q: 0.013953
  10471/1000000: episode: 1504, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000239, mae: 0.011182, mean_q: 0.014864
  10478/1000000: episode: 1505, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000446, mae: 0.011422, mean_q: 0.016532
  10485/1000000: episode: 1506, duration: 0.034s, episode steps: 7, steps per second: 204, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000280, mae: 0.010674, mean_q: 0.012892
  10492/1000000: episode: 1507, duration: 0.034s, episode steps: 7, steps per second: 204, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000278, mae: 0.010851, mean_q: 0.014682
  10497/1000000: episode: 1508, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000357, mae: 0.012137, mean_q: 0.014266
  10504/1000000: episode: 1509, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 1.315, mean reward: 0.188 [0.007, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 7.786 [5.000, 11.000], loss: 0.000214, mae: 0.009768, mean_q: 0.011866
  10509/1000000: episode: 1510, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000181, mae: 0.008577, mean_q: 0.012810
  10514/1000000: episode: 1511, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000593, mae: 0.013490, mean_q: 0.014025
  10516/1000000: episode: 1512, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000186, mae: 0.009881, mean_q: 0.009133
  10523/1000000: episode: 1513, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000258, mae: 0.012418, mean_q: 0.016186
  10530/1000000: episode: 1514, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000262, mae: 0.011374, mean_q: 0.012784
  10537/1000000: episode: 1515, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000329, mae: 0.012973, mean_q: 0.016250
  10542/1000000: episode: 1516, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000375, mae: 0.014529, mean_q: 0.017699
  10549/1000000: episode: 1517, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000533, mae: 0.014797, mean_q: 0.014853
  10556/1000000: episode: 1518, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000844, mae: 0.014055, mean_q: 0.012853
  10561/1000000: episode: 1519, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000252, mae: 0.012316, mean_q: 0.016327
  10566/1000000: episode: 1520, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000559, mae: 0.012729, mean_q: 0.015252
  10573/1000000: episode: 1521, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000560, mae: 0.014099, mean_q: 0.017540
  10580/1000000: episode: 1522, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000325, mae: 0.012430, mean_q: 0.016213
  10582/1000000: episode: 1523, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000684, mae: 0.011980, mean_q: 0.015345
  10589/1000000: episode: 1524, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000508, mae: 0.013459, mean_q: 0.015458
  10596/1000000: episode: 1525, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000370, mae: 0.010628, mean_q: 0.012367
  10603/1000000: episode: 1526, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000429, mae: 0.012219, mean_q: 0.014252
  10610/1000000: episode: 1527, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000323, mae: 0.012529, mean_q: 0.015724
  10615/1000000: episode: 1528, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000216, mae: 0.010659, mean_q: 0.014146
  10622/1000000: episode: 1529, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000736, mae: 0.013679, mean_q: 0.014700
  10627/1000000: episode: 1530, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.002717, mae: 0.015860, mean_q: 0.013875
  10634/1000000: episode: 1531, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000392, mae: 0.012298, mean_q: 0.015055
  10641/1000000: episode: 1532, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000197, mae: 0.010328, mean_q: 0.012026
  10646/1000000: episode: 1533, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000215, mae: 0.013374, mean_q: 0.014894
  10648/1000000: episode: 1534, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000190, mae: 0.011466, mean_q: 0.017859
  10655/1000000: episode: 1535, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000281, mae: 0.014512, mean_q: 0.010603
  10662/1000000: episode: 1536, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000494, mae: 0.015049, mean_q: 0.015991
  10667/1000000: episode: 1537, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000318, mae: 0.012788, mean_q: 0.016232
  10672/1000000: episode: 1538, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000324, mae: 0.012585, mean_q: 0.013661
  10679/1000000: episode: 1539, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 1.315, mean reward: 0.188 [0.007, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 7.786 [5.000, 11.000], loss: 0.000522, mae: 0.014834, mean_q: 0.017384
  10686/1000000: episode: 1540, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000276, mae: 0.011181, mean_q: 0.013198
  10693/1000000: episode: 1541, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000753, mae: 0.016411, mean_q: 0.018735
  10698/1000000: episode: 1542, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000529, mae: 0.013748, mean_q: 0.018311
  10705/1000000: episode: 1543, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.002045, mae: 0.014299, mean_q: 0.017252
  10712/1000000: episode: 1544, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000317, mae: 0.011837, mean_q: 0.015600
  10717/1000000: episode: 1545, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000324, mae: 0.013641, mean_q: 0.020467
  10722/1000000: episode: 1546, duration: 0.026s, episode steps: 5, steps per second: 193, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000273, mae: 0.012094, mean_q: 0.015862
  10729/1000000: episode: 1547, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000394, mae: 0.012854, mean_q: 0.015108
  10731/1000000: episode: 1548, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000255, mae: 0.011956, mean_q: 0.018128
  10738/1000000: episode: 1549, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000250, mae: 0.011417, mean_q: 0.012631
  10745/1000000: episode: 1550, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000266, mae: 0.010986, mean_q: 0.014508
  10752/1000000: episode: 1551, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000644, mae: 0.011785, mean_q: 0.016828
  10759/1000000: episode: 1552, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000327, mae: 0.012506, mean_q: 0.015151
  10764/1000000: episode: 1553, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000519, mae: 0.011652, mean_q: 0.012627
  10771/1000000: episode: 1554, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000616, mae: 0.012689, mean_q: 0.015007
  10778/1000000: episode: 1555, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000337, mae: 0.014256, mean_q: 0.017004
  10785/1000000: episode: 1556, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000512, mae: 0.014542, mean_q: 0.013077
  10792/1000000: episode: 1557, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000404, mae: 0.011868, mean_q: 0.011689
  10799/1000000: episode: 1558, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000522, mae: 0.014552, mean_q: 0.017251
  10806/1000000: episode: 1559, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000389, mae: 0.015205, mean_q: 0.017783
  10813/1000000: episode: 1560, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000289, mae: 0.012572, mean_q: 0.014547
  10820/1000000: episode: 1561, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000302, mae: 0.012388, mean_q: 0.015595
  10827/1000000: episode: 1562, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000950, mae: 0.017792, mean_q: 0.020393
  10832/1000000: episode: 1563, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000326, mae: 0.012161, mean_q: 0.013800
  10839/1000000: episode: 1564, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.002091, mae: 0.015619, mean_q: 0.014828
  10846/1000000: episode: 1565, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000681, mae: 0.014975, mean_q: 0.017009
  10853/1000000: episode: 1566, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000407, mae: 0.012829, mean_q: 0.014331
  10860/1000000: episode: 1567, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000375, mae: 0.013857, mean_q: 0.016829
  10862/1000000: episode: 1568, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001019, mae: 0.018220, mean_q: 0.025340
  10869/1000000: episode: 1569, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000277, mae: 0.011932, mean_q: 0.016767
  10874/1000000: episode: 1570, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.003048, mae: 0.019566, mean_q: 0.017351
  10879/1000000: episode: 1571, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000241, mae: 0.010499, mean_q: 0.016303
  10881/1000000: episode: 1572, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000271, mae: 0.012689, mean_q: 0.019326
  10886/1000000: episode: 1573, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000682, mae: 0.016290, mean_q: 0.017921
  10891/1000000: episode: 1574, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.001210, mae: 0.018897, mean_q: 0.019613
  10898/1000000: episode: 1575, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000564, mae: 0.016212, mean_q: 0.020607
  10905/1000000: episode: 1576, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000588, mae: 0.017922, mean_q: 0.020794
  10907/1000000: episode: 1577, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001022, mae: 0.019365, mean_q: 0.019025
  10909/1000000: episode: 1578, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000295, mae: 0.014099, mean_q: 0.018420
  10914/1000000: episode: 1579, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000666, mae: 0.014871, mean_q: 0.015523
  10921/1000000: episode: 1580, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000628, mae: 0.016580, mean_q: 0.020671
  10926/1000000: episode: 1581, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001113, mae: 0.016698, mean_q: 0.016583
  10933/1000000: episode: 1582, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000472, mae: 0.012707, mean_q: 0.015816
  10940/1000000: episode: 1583, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000322, mae: 0.013632, mean_q: 0.017657
  10942/1000000: episode: 1584, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000295, mae: 0.012472, mean_q: 0.020720
  10947/1000000: episode: 1585, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000621, mae: 0.013802, mean_q: 0.018332
  10954/1000000: episode: 1586, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000679, mae: 0.013917, mean_q: 0.015572
  10961/1000000: episode: 1587, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.002143, mae: 0.015174, mean_q: 0.014965
[Info] FALSIFICATION!
  10967/1000000: episode: 1588, duration: 0.174s, episode steps: 6, steps per second: 34, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000555, mae: 0.015112, mean_q: 0.020554
  10974/1000000: episode: 1589, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001151, mae: 0.021763, mean_q: 0.025448
[Info] Complete ISplit Iteration
[Info] Levels: [0.012973338, 0.1059576]
[Info] Cond. Prob: [0.1, 0.31]
[Info] Error Prob: 0.031

  10981/1000000: episode: 1590, duration: 0.826s, episode steps: 7, steps per second: 8, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000576, mae: 0.014778, mean_q: 0.019460
  10991/1000000: episode: 1591, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001970, mae: 0.018862, mean_q: 0.020650
  11001/1000000: episode: 1592, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.700 [-1.000, 10.000], loss: 0.000325, mae: 0.013543, mean_q: 0.018031
  11011/1000000: episode: 1593, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001830, mae: 0.017209, mean_q: 0.017091
  11021/1000000: episode: 1594, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000399, mae: 0.012445, mean_q: 0.015254
  11031/1000000: episode: 1595, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.003008, mae: 0.022252, mean_q: 0.022801
  11041/1000000: episode: 1596, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000598, mae: 0.015704, mean_q: 0.019921
  11051/1000000: episode: 1597, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.002775, mae: 0.019002, mean_q: 0.017985
  11061/1000000: episode: 1598, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000488, mae: 0.016507, mean_q: 0.023515
  11071/1000000: episode: 1599, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000610, mae: 0.012509, mean_q: 0.014027
  11081/1000000: episode: 1600, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001642, mae: 0.014852, mean_q: 0.014508
  11091/1000000: episode: 1601, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001690, mae: 0.017351, mean_q: 0.018341
  11101/1000000: episode: 1602, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000417, mae: 0.014887, mean_q: 0.020687
  11111/1000000: episode: 1603, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.001864, mae: 0.019096, mean_q: 0.021040
  11121/1000000: episode: 1604, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001627, mae: 0.018142, mean_q: 0.016678
  11131/1000000: episode: 1605, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002925, mae: 0.020744, mean_q: 0.018848
  11141/1000000: episode: 1606, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000529, mae: 0.015754, mean_q: 0.023191
  11151/1000000: episode: 1607, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000394, mae: 0.013827, mean_q: 0.016417
  11161/1000000: episode: 1608, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000588, mae: 0.016260, mean_q: 0.018032
  11171/1000000: episode: 1609, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000263, mae: 0.013789, mean_q: 0.015834
  11181/1000000: episode: 1610, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000491, mae: 0.014025, mean_q: 0.015900
  11191/1000000: episode: 1611, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000503, mae: 0.017099, mean_q: 0.022698
  11201/1000000: episode: 1612, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000685, mae: 0.016229, mean_q: 0.016346
  11211/1000000: episode: 1613, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001586, mae: 0.015963, mean_q: 0.018976
  11221/1000000: episode: 1614, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000500, mae: 0.013806, mean_q: 0.017185
  11231/1000000: episode: 1615, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000439, mae: 0.013230, mean_q: 0.013731
  11241/1000000: episode: 1616, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000306, mae: 0.013084, mean_q: 0.017384
  11251/1000000: episode: 1617, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001716, mae: 0.017599, mean_q: 0.017790
  11261/1000000: episode: 1618, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000740, mae: 0.017276, mean_q: 0.016538
  11271/1000000: episode: 1619, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000368, mae: 0.016795, mean_q: 0.019361
  11281/1000000: episode: 1620, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000329, mae: 0.014190, mean_q: 0.016539
  11291/1000000: episode: 1621, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000870, mae: 0.018133, mean_q: 0.021045
  11301/1000000: episode: 1622, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001889, mae: 0.019260, mean_q: 0.018462
  11311/1000000: episode: 1623, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001614, mae: 0.018199, mean_q: 0.022559
  11321/1000000: episode: 1624, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000536, mae: 0.015406, mean_q: 0.018756
  11331/1000000: episode: 1625, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000442, mae: 0.013386, mean_q: 0.018397
  11341/1000000: episode: 1626, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001611, mae: 0.013240, mean_q: 0.013898
  11351/1000000: episode: 1627, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000305, mae: 0.010582, mean_q: 0.015685
  11361/1000000: episode: 1628, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000498, mae: 0.015025, mean_q: 0.020953
  11371/1000000: episode: 1629, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001700, mae: 0.015525, mean_q: 0.018974
  11381/1000000: episode: 1630, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000652, mae: 0.015792, mean_q: 0.018771
  11391/1000000: episode: 1631, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000274, mae: 0.011456, mean_q: 0.014215
  11401/1000000: episode: 1632, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000645, mae: 0.017559, mean_q: 0.021091
  11411/1000000: episode: 1633, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000506, mae: 0.014672, mean_q: 0.017442
  11421/1000000: episode: 1634, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000341, mae: 0.014273, mean_q: 0.016703
  11431/1000000: episode: 1635, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001632, mae: 0.016296, mean_q: 0.015000
  11441/1000000: episode: 1636, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000259, mae: 0.011148, mean_q: 0.016058
  11451/1000000: episode: 1637, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000538, mae: 0.015552, mean_q: 0.021583
  11461/1000000: episode: 1638, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001806, mae: 0.017134, mean_q: 0.018138
  11471/1000000: episode: 1639, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000332, mae: 0.012619, mean_q: 0.015607
  11481/1000000: episode: 1640, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000313, mae: 0.012303, mean_q: 0.017402
  11491/1000000: episode: 1641, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000501, mae: 0.014169, mean_q: 0.017955
  11501/1000000: episode: 1642, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000646, mae: 0.014844, mean_q: 0.018361
  11511/1000000: episode: 1643, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000404, mae: 0.014398, mean_q: 0.017109
  11521/1000000: episode: 1644, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000523, mae: 0.015089, mean_q: 0.019684
  11531/1000000: episode: 1645, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001735, mae: 0.016201, mean_q: 0.013487
  11541/1000000: episode: 1646, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000483, mae: 0.015553, mean_q: 0.017251
  11551/1000000: episode: 1647, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000381, mae: 0.014600, mean_q: 0.016634
  11561/1000000: episode: 1648, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000269, mae: 0.011178, mean_q: 0.012921
  11571/1000000: episode: 1649, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000292, mae: 0.011575, mean_q: 0.015062
  11581/1000000: episode: 1650, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000409, mae: 0.014788, mean_q: 0.020179
  11591/1000000: episode: 1651, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000873, mae: 0.016127, mean_q: 0.014606
  11601/1000000: episode: 1652, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000255, mae: 0.011588, mean_q: 0.016041
  11611/1000000: episode: 1653, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000588, mae: 0.014479, mean_q: 0.017006
  11621/1000000: episode: 1654, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000635, mae: 0.014523, mean_q: 0.015502
  11631/1000000: episode: 1655, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000487, mae: 0.010372, mean_q: 0.012032
  11641/1000000: episode: 1656, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000261, mae: 0.011734, mean_q: 0.015562
  11651/1000000: episode: 1657, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000597, mae: 0.014489, mean_q: 0.016183
  11661/1000000: episode: 1658, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000345, mae: 0.013099, mean_q: 0.015096
  11671/1000000: episode: 1659, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000417, mae: 0.011685, mean_q: 0.014024
  11681/1000000: episode: 1660, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.033, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000434, mae: 0.013000, mean_q: 0.015500
  11691/1000000: episode: 1661, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000429, mae: 0.011828, mean_q: 0.014841
  11701/1000000: episode: 1662, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000577, mae: 0.013391, mean_q: 0.016097
  11711/1000000: episode: 1663, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000766, mae: 0.015207, mean_q: 0.018293
  11721/1000000: episode: 1664, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000415, mae: 0.011809, mean_q: 0.014746
  11731/1000000: episode: 1665, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001791, mae: 0.016919, mean_q: 0.016305
  11741/1000000: episode: 1666, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000428, mae: 0.015094, mean_q: 0.015779
  11751/1000000: episode: 1667, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000492, mae: 0.015654, mean_q: 0.014163
  11761/1000000: episode: 1668, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000566, mae: 0.016368, mean_q: 0.016654
  11771/1000000: episode: 1669, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000690, mae: 0.015341, mean_q: 0.016523
  11781/1000000: episode: 1670, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000150, mae: 0.007559, mean_q: 0.011329
  11791/1000000: episode: 1671, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001596, mae: 0.012858, mean_q: 0.014592
  11801/1000000: episode: 1672, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000648, mae: 0.012733, mean_q: 0.015251
  11811/1000000: episode: 1673, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001560, mae: 0.014736, mean_q: 0.015610
  11821/1000000: episode: 1674, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000496, mae: 0.014040, mean_q: 0.018129
  11831/1000000: episode: 1675, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000527, mae: 0.014069, mean_q: 0.015378
  11841/1000000: episode: 1676, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.088, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000456, mae: 0.012207, mean_q: 0.015599
  11851/1000000: episode: 1677, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000284, mae: 0.010541, mean_q: 0.016108
  11861/1000000: episode: 1678, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000652, mae: 0.013385, mean_q: 0.014995
  11871/1000000: episode: 1679, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000310, mae: 0.011367, mean_q: 0.012851
  11881/1000000: episode: 1680, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000569, mae: 0.012557, mean_q: 0.012835
  11891/1000000: episode: 1681, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000395, mae: 0.011580, mean_q: 0.013013
  11901/1000000: episode: 1682, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001570, mae: 0.012808, mean_q: 0.013132
  11911/1000000: episode: 1683, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000384, mae: 0.011060, mean_q: 0.013456
  11921/1000000: episode: 1684, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000308, mae: 0.011845, mean_q: 0.014484
  11931/1000000: episode: 1685, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000715, mae: 0.013892, mean_q: 0.016968
  11941/1000000: episode: 1686, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000339, mae: 0.010323, mean_q: 0.011995
  11951/1000000: episode: 1687, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000257, mae: 0.009801, mean_q: 0.012511
  11961/1000000: episode: 1688, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000360, mae: 0.010461, mean_q: 0.013046
  11971/1000000: episode: 1689, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000377, mae: 0.010206, mean_q: 0.013462
[Info] 1-TH LEVEL FOUND: 0.03411736339330673, Considering 17/100 traces
  11981/1000000: episode: 1690, duration: 0.691s, episode steps: 10, steps per second: 14, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001660, mae: 0.013403, mean_q: 0.013889
  11986/1000000: episode: 1691, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000486, mae: 0.013099, mean_q: 0.016060
  11991/1000000: episode: 1692, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000231, mae: 0.010163, mean_q: 0.013458
  11996/1000000: episode: 1693, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.002617, mae: 0.013583, mean_q: 0.011142
  12001/1000000: episode: 1694, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000167, mae: 0.009801, mean_q: 0.015415
  12006/1000000: episode: 1695, duration: 0.026s, episode steps: 5, steps per second: 193, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000155, mae: 0.009311, mean_q: 0.014932
  12012/1000000: episode: 1696, duration: 0.029s, episode steps: 6, steps per second: 203, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000171, mae: 0.010046, mean_q: 0.013297
  12017/1000000: episode: 1697, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000262, mae: 0.011525, mean_q: 0.016399
  12022/1000000: episode: 1698, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000430, mae: 0.012182, mean_q: 0.012630
  12027/1000000: episode: 1699, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000174, mae: 0.010335, mean_q: 0.013399
  12032/1000000: episode: 1700, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000382, mae: 0.011238, mean_q: 0.011603
  12037/1000000: episode: 1701, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000505, mae: 0.011295, mean_q: 0.010762
  12042/1000000: episode: 1702, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000486, mae: 0.015153, mean_q: 0.019524
  12047/1000000: episode: 1703, duration: 0.026s, episode steps: 5, steps per second: 192, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000255, mae: 0.011233, mean_q: 0.014178
  12052/1000000: episode: 1704, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000215, mae: 0.010858, mean_q: 0.014643
  12057/1000000: episode: 1705, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000300, mae: 0.011393, mean_q: 0.017123
  12062/1000000: episode: 1706, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000541, mae: 0.012288, mean_q: 0.010806
  12067/1000000: episode: 1707, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000170, mae: 0.008694, mean_q: 0.010184
  12072/1000000: episode: 1708, duration: 0.026s, episode steps: 5, steps per second: 192, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000089, mae: 0.007607, mean_q: 0.010456
  12077/1000000: episode: 1709, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000558, mae: 0.015188, mean_q: 0.017694
  12082/1000000: episode: 1710, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000411, mae: 0.011524, mean_q: 0.012352
[Info] FALSIFICATION!
  12087/1000000: episode: 1711, duration: 0.321s, episode steps: 5, steps per second: 16, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000217, mae: 0.011300, mean_q: 0.012638
  12092/1000000: episode: 1712, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000264, mae: 0.007955, mean_q: 0.009666
  12097/1000000: episode: 1713, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000173, mae: 0.008511, mean_q: 0.012302
  12102/1000000: episode: 1714, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000221, mae: 0.011384, mean_q: 0.015922
  12107/1000000: episode: 1715, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000221, mae: 0.009594, mean_q: 0.014867
  12112/1000000: episode: 1716, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000236, mae: 0.010192, mean_q: 0.013696
  12117/1000000: episode: 1717, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000528, mae: 0.013459, mean_q: 0.016567
  12122/1000000: episode: 1718, duration: 0.025s, episode steps: 5, steps per second: 196, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000595, mae: 0.014745, mean_q: 0.014850
  12127/1000000: episode: 1719, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000402, mae: 0.011805, mean_q: 0.013054
  12132/1000000: episode: 1720, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000210, mae: 0.010471, mean_q: 0.013378
  12137/1000000: episode: 1721, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000123, mae: 0.008187, mean_q: 0.012140
  12143/1000000: episode: 1722, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000192, mae: 0.009369, mean_q: 0.014668
  12148/1000000: episode: 1723, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.002976, mae: 0.016642, mean_q: 0.014141
  12154/1000000: episode: 1724, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000181, mae: 0.009454, mean_q: 0.015124
  12159/1000000: episode: 1725, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.002995, mae: 0.018805, mean_q: 0.015453
  12165/1000000: episode: 1726, duration: 0.030s, episode steps: 6, steps per second: 197, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000465, mae: 0.011987, mean_q: 0.013777
  12170/1000000: episode: 1727, duration: 0.026s, episode steps: 5, steps per second: 190, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000183, mae: 0.009475, mean_q: 0.010714
  12175/1000000: episode: 1728, duration: 0.024s, episode steps: 5, steps per second: 205, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000267, mae: 0.010609, mean_q: 0.012216
  12180/1000000: episode: 1729, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000136, mae: 0.008516, mean_q: 0.012029
  12185/1000000: episode: 1730, duration: 0.024s, episode steps: 5, steps per second: 204, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000321, mae: 0.011089, mean_q: 0.014511
  12190/1000000: episode: 1731, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000582, mae: 0.013269, mean_q: 0.017040
  12195/1000000: episode: 1732, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000144, mae: 0.008499, mean_q: 0.010560
  12201/1000000: episode: 1733, duration: 0.029s, episode steps: 6, steps per second: 203, episode reward: 1.941, mean reward: 0.323 [0.018, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 8.333 [6.000, 11.000], loss: 0.002295, mae: 0.015374, mean_q: 0.014982
  12206/1000000: episode: 1734, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000186, mae: 0.009031, mean_q: 0.014493
  12211/1000000: episode: 1735, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000444, mae: 0.010982, mean_q: 0.013309
  12216/1000000: episode: 1736, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.002929, mae: 0.016839, mean_q: 0.012098
  12221/1000000: episode: 1737, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.002547, mae: 0.013720, mean_q: 0.010938
  12226/1000000: episode: 1738, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000820, mae: 0.015139, mean_q: 0.014863
  12231/1000000: episode: 1739, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000266, mae: 0.011672, mean_q: 0.016971
  12236/1000000: episode: 1740, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000664, mae: 0.014805, mean_q: 0.020965
  12241/1000000: episode: 1741, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000180, mae: 0.009625, mean_q: 0.016124
  12246/1000000: episode: 1742, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000546, mae: 0.012844, mean_q: 0.014748
  12251/1000000: episode: 1743, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000255, mae: 0.011798, mean_q: 0.016094
  12256/1000000: episode: 1744, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000220, mae: 0.010727, mean_q: 0.016340
  12262/1000000: episode: 1745, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.002436, mae: 0.017616, mean_q: 0.019187
  12267/1000000: episode: 1746, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000215, mae: 0.009652, mean_q: 0.014291
  12272/1000000: episode: 1747, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000092, mae: 0.008052, mean_q: 0.009916
  12277/1000000: episode: 1748, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000468, mae: 0.011941, mean_q: 0.009069
  12282/1000000: episode: 1749, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000318, mae: 0.012781, mean_q: 0.017754
  12287/1000000: episode: 1750, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000345, mae: 0.012494, mean_q: 0.013748
  12292/1000000: episode: 1751, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000605, mae: 0.013414, mean_q: 0.010842
  12297/1000000: episode: 1752, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000585, mae: 0.016394, mean_q: 0.014740
  12303/1000000: episode: 1753, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000534, mae: 0.014291, mean_q: 0.014774
  12308/1000000: episode: 1754, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000415, mae: 0.016372, mean_q: 0.021724
  12314/1000000: episode: 1755, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000216, mae: 0.012989, mean_q: 0.012573
  12319/1000000: episode: 1756, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000390, mae: 0.015954, mean_q: 0.018921
  12324/1000000: episode: 1757, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000703, mae: 0.018379, mean_q: 0.021216
  12329/1000000: episode: 1758, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000351, mae: 0.014681, mean_q: 0.016758
  12335/1000000: episode: 1759, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.002217, mae: 0.013825, mean_q: 0.011854
  12341/1000000: episode: 1760, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.002309, mae: 0.017209, mean_q: 0.014161
  12346/1000000: episode: 1761, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000547, mae: 0.013477, mean_q: 0.018038
  12352/1000000: episode: 1762, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.002531, mae: 0.017913, mean_q: 0.017620
  12357/1000000: episode: 1763, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000636, mae: 0.015174, mean_q: 0.019166
  12362/1000000: episode: 1764, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000167, mae: 0.009181, mean_q: 0.011336
  12367/1000000: episode: 1765, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000333, mae: 0.012654, mean_q: 0.019179
  12372/1000000: episode: 1766, duration: 0.026s, episode steps: 5, steps per second: 196, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000314, mae: 0.011931, mean_q: 0.015834
  12377/1000000: episode: 1767, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000159, mae: 0.009339, mean_q: 0.013964
  12382/1000000: episode: 1768, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000297, mae: 0.011328, mean_q: 0.015214
  12388/1000000: episode: 1769, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000383, mae: 0.013442, mean_q: 0.018315
  12394/1000000: episode: 1770, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000214, mae: 0.009644, mean_q: 0.014156
  12399/1000000: episode: 1771, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000526, mae: 0.011142, mean_q: 0.011679
  12404/1000000: episode: 1772, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000243, mae: 0.010400, mean_q: 0.012350
[Info] Complete ISplit Iteration
[Info] Levels: [0.034117363, 0.11950634]
[Info] Cond. Prob: [0.17, 0.26]
[Info] Error Prob: 0.0442

  12410/1000000: episode: 1773, duration: 0.816s, episode steps: 6, steps per second: 7, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002834, mae: 0.019586, mean_q: 0.015419
  12420/1000000: episode: 1774, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001687, mae: 0.015839, mean_q: 0.015015
  12430/1000000: episode: 1775, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001781, mae: 0.016997, mean_q: 0.017409
  12440/1000000: episode: 1776, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.067, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000334, mae: 0.013288, mean_q: 0.017148
  12450/1000000: episode: 1777, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000533, mae: 0.012420, mean_q: 0.015465
  12460/1000000: episode: 1778, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000177, mae: 0.010066, mean_q: 0.013105
  12470/1000000: episode: 1779, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000272, mae: 0.011036, mean_q: 0.012562
  12480/1000000: episode: 1780, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.001890, mae: 0.016199, mean_q: 0.017095
  12490/1000000: episode: 1781, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000646, mae: 0.013723, mean_q: 0.017625
  12500/1000000: episode: 1782, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000356, mae: 0.012108, mean_q: 0.016343
  12510/1000000: episode: 1783, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001527, mae: 0.013572, mean_q: 0.013291
  12520/1000000: episode: 1784, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000223, mae: 0.010181, mean_q: 0.012580
  12530/1000000: episode: 1785, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001443, mae: 0.015202, mean_q: 0.017092
  12540/1000000: episode: 1786, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.004337, mae: 0.025342, mean_q: 0.019314
  12550/1000000: episode: 1787, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000224, mae: 0.011187, mean_q: 0.013329
  12560/1000000: episode: 1788, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000491, mae: 0.015154, mean_q: 0.015634
  12570/1000000: episode: 1789, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.001410, mae: 0.014147, mean_q: 0.019146
  12580/1000000: episode: 1790, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000435, mae: 0.013127, mean_q: 0.020271
  12590/1000000: episode: 1791, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000774, mae: 0.014050, mean_q: 0.014305
  12600/1000000: episode: 1792, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000372, mae: 0.012240, mean_q: 0.016282
  12610/1000000: episode: 1793, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000382, mae: 0.012350, mean_q: 0.016661
  12620/1000000: episode: 1794, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001749, mae: 0.016488, mean_q: 0.017280
  12630/1000000: episode: 1795, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000624, mae: 0.012329, mean_q: 0.014244
  12640/1000000: episode: 1796, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000638, mae: 0.019312, mean_q: 0.020501
  12650/1000000: episode: 1797, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001570, mae: 0.017541, mean_q: 0.016031
  12660/1000000: episode: 1798, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001864, mae: 0.018438, mean_q: 0.015646
  12670/1000000: episode: 1799, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003918, mae: 0.023481, mean_q: 0.015883
  12680/1000000: episode: 1800, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.002901, mae: 0.024559, mean_q: 0.018698
  12690/1000000: episode: 1801, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000465, mae: 0.015825, mean_q: 0.012788
  12700/1000000: episode: 1802, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000401, mae: 0.014256, mean_q: 0.013907
  12710/1000000: episode: 1803, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000430, mae: 0.014567, mean_q: 0.015421
  12720/1000000: episode: 1804, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000416, mae: 0.013061, mean_q: 0.014575
  12730/1000000: episode: 1805, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002654, mae: 0.016904, mean_q: 0.015568
  12740/1000000: episode: 1806, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001876, mae: 0.023381, mean_q: 0.020298
  12750/1000000: episode: 1807, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000726, mae: 0.018040, mean_q: 0.018615
  12760/1000000: episode: 1808, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.002756, mae: 0.021279, mean_q: 0.022173
  12770/1000000: episode: 1809, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001840, mae: 0.023325, mean_q: 0.016812
  12780/1000000: episode: 1810, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001863, mae: 0.021888, mean_q: 0.017830
  12790/1000000: episode: 1811, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000675, mae: 0.018278, mean_q: 0.025132
  12800/1000000: episode: 1812, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000485, mae: 0.014502, mean_q: 0.012402
  12810/1000000: episode: 1813, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000337, mae: 0.011877, mean_q: 0.016731
  12820/1000000: episode: 1814, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000204, mae: 0.011101, mean_q: 0.013530
  12830/1000000: episode: 1815, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.001579, mae: 0.017398, mean_q: 0.017170
  12840/1000000: episode: 1816, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000390, mae: 0.013797, mean_q: 0.018736
  12850/1000000: episode: 1817, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000594, mae: 0.013413, mean_q: 0.013339
  12860/1000000: episode: 1818, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000389, mae: 0.014069, mean_q: 0.019514
  12870/1000000: episode: 1819, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001550, mae: 0.019843, mean_q: 0.015292
  12880/1000000: episode: 1820, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000447, mae: 0.015078, mean_q: 0.016503
  12890/1000000: episode: 1821, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000570, mae: 0.015036, mean_q: 0.012805
  12900/1000000: episode: 1822, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000314, mae: 0.014258, mean_q: 0.017144
  12910/1000000: episode: 1823, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001641, mae: 0.022202, mean_q: 0.022008
  12920/1000000: episode: 1824, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000483, mae: 0.017189, mean_q: 0.013232
  12930/1000000: episode: 1825, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000526, mae: 0.016183, mean_q: 0.016540
  12940/1000000: episode: 1826, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000247, mae: 0.012706, mean_q: 0.011038
  12950/1000000: episode: 1827, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000206, mae: 0.010770, mean_q: 0.014772
  12960/1000000: episode: 1828, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000298, mae: 0.011106, mean_q: 0.012959
  12970/1000000: episode: 1829, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001574, mae: 0.015315, mean_q: 0.015645
  12980/1000000: episode: 1830, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000634, mae: 0.015540, mean_q: 0.019260
  12990/1000000: episode: 1831, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000439, mae: 0.015094, mean_q: 0.014042
  13000/1000000: episode: 1832, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001726, mae: 0.020983, mean_q: 0.020204
  13010/1000000: episode: 1833, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001772, mae: 0.022531, mean_q: 0.022264
  13020/1000000: episode: 1834, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000562, mae: 0.015071, mean_q: 0.018545
  13030/1000000: episode: 1835, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.215, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000515, mae: 0.013088, mean_q: 0.016857
  13040/1000000: episode: 1836, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001808, mae: 0.018296, mean_q: 0.017488
  13050/1000000: episode: 1837, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.003677, mae: 0.025529, mean_q: 0.022597
  13060/1000000: episode: 1838, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000680, mae: 0.017908, mean_q: 0.020255
  13070/1000000: episode: 1839, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000266, mae: 0.010382, mean_q: 0.014664
  13080/1000000: episode: 1840, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000325, mae: 0.012836, mean_q: 0.013040
  13090/1000000: episode: 1841, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000366, mae: 0.011584, mean_q: 0.013644
  13100/1000000: episode: 1842, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000935, mae: 0.017384, mean_q: 0.019208
  13110/1000000: episode: 1843, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.001375, mae: 0.013818, mean_q: 0.018314
  13120/1000000: episode: 1844, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000461, mae: 0.011231, mean_q: 0.013781
  13130/1000000: episode: 1845, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000462, mae: 0.013524, mean_q: 0.016954
  13140/1000000: episode: 1846, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000476, mae: 0.016926, mean_q: 0.020450
  13150/1000000: episode: 1847, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000646, mae: 0.018414, mean_q: 0.016177
  13160/1000000: episode: 1848, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000412, mae: 0.015183, mean_q: 0.017988
  13170/1000000: episode: 1849, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000525, mae: 0.017527, mean_q: 0.019683
  13180/1000000: episode: 1850, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000401, mae: 0.016319, mean_q: 0.012434
  13190/1000000: episode: 1851, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000469, mae: 0.013008, mean_q: 0.012755
  13200/1000000: episode: 1852, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001506, mae: 0.016562, mean_q: 0.017960
  13210/1000000: episode: 1853, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001586, mae: 0.021000, mean_q: 0.019705
  13220/1000000: episode: 1854, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000335, mae: 0.016231, mean_q: 0.008958
  13230/1000000: episode: 1855, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000356, mae: 0.013329, mean_q: 0.012389
  13240/1000000: episode: 1856, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000565, mae: 0.014739, mean_q: 0.013812
  13250/1000000: episode: 1857, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000339, mae: 0.016236, mean_q: 0.014478
  13260/1000000: episode: 1858, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000545, mae: 0.015620, mean_q: 0.018043
  13270/1000000: episode: 1859, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000362, mae: 0.011320, mean_q: 0.012506
  13280/1000000: episode: 1860, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000422, mae: 0.015382, mean_q: 0.020392
  13290/1000000: episode: 1861, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.067, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001526, mae: 0.018992, mean_q: 0.013481
  13300/1000000: episode: 1862, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001511, mae: 0.016800, mean_q: 0.017672
  13310/1000000: episode: 1863, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000462, mae: 0.013130, mean_q: 0.011929
  13320/1000000: episode: 1864, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000322, mae: 0.012137, mean_q: 0.014030
  13330/1000000: episode: 1865, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000230, mae: 0.010527, mean_q: 0.014510
  13340/1000000: episode: 1866, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001518, mae: 0.015937, mean_q: 0.014469
  13350/1000000: episode: 1867, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000702, mae: 0.018806, mean_q: 0.020207
  13360/1000000: episode: 1868, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000556, mae: 0.014999, mean_q: 0.015107
  13370/1000000: episode: 1869, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001518, mae: 0.018006, mean_q: 0.018226
  13380/1000000: episode: 1870, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001545, mae: 0.017673, mean_q: 0.014519
  13390/1000000: episode: 1871, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000530, mae: 0.017733, mean_q: 0.020457
  13400/1000000: episode: 1872, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000274, mae: 0.013522, mean_q: 0.011490
[Info] 1-TH LEVEL FOUND: 0.0297582745552063, Considering 11/100 traces
  13410/1000000: episode: 1873, duration: 0.697s, episode steps: 10, steps per second: 14, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000460, mae: 0.013526, mean_q: 0.014063
  13416/1000000: episode: 1874, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000305, mae: 0.012669, mean_q: 0.014578
  13422/1000000: episode: 1875, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000244, mae: 0.010273, mean_q: 0.013708
  13426/1000000: episode: 1876, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000531, mae: 0.011507, mean_q: 0.012356
  13432/1000000: episode: 1877, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000585, mae: 0.013354, mean_q: 0.015513
  13438/1000000: episode: 1878, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000387, mae: 0.014032, mean_q: 0.019562
  13442/1000000: episode: 1879, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000739, mae: 0.014244, mean_q: 0.014769
  13448/1000000: episode: 1880, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000291, mae: 0.011368, mean_q: 0.014078
  13454/1000000: episode: 1881, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000673, mae: 0.016459, mean_q: 0.018815
  13458/1000000: episode: 1882, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000496, mae: 0.017852, mean_q: 0.012832
  13464/1000000: episode: 1883, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002646, mae: 0.021334, mean_q: 0.020541
  13470/1000000: episode: 1884, duration: 0.030s, episode steps: 6, steps per second: 197, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000359, mae: 0.014561, mean_q: 0.016938
  13474/1000000: episode: 1885, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.002917, mae: 0.018141, mean_q: 0.015328
  13480/1000000: episode: 1886, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.002201, mae: 0.019115, mean_q: 0.011797
  13486/1000000: episode: 1887, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.168, mean reward: 0.028 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000598, mae: 0.021873, mean_q: 0.023002
  13490/1000000: episode: 1888, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001083, mae: 0.024842, mean_q: 0.022577
  13494/1000000: episode: 1889, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000424, mae: 0.019985, mean_q: 0.012836
  13498/1000000: episode: 1890, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000304, mae: 0.012298, mean_q: 0.006220
  13502/1000000: episode: 1891, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000133, mae: 0.008534, mean_q: 0.011784
  13506/1000000: episode: 1892, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000620, mae: 0.010763, mean_q: 0.012372
  13510/1000000: episode: 1893, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000218, mae: 0.008945, mean_q: 0.008716
  13516/1000000: episode: 1894, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000599, mae: 0.014366, mean_q: 0.015248
  13522/1000000: episode: 1895, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000502, mae: 0.010570, mean_q: 0.011128
  13526/1000000: episode: 1896, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002886, mae: 0.019513, mean_q: 0.023327
  13530/1000000: episode: 1897, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000410, mae: 0.016061, mean_q: 0.025895
  13534/1000000: episode: 1898, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000932, mae: 0.021217, mean_q: 0.024666
  13540/1000000: episode: 1899, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000515, mae: 0.016051, mean_q: 0.012848
  13544/1000000: episode: 1900, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000729, mae: 0.017857, mean_q: 0.014820
  13550/1000000: episode: 1901, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000583, mae: 0.013639, mean_q: 0.015186
  13556/1000000: episode: 1902, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.002316, mae: 0.019186, mean_q: 0.021499
  13562/1000000: episode: 1903, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000567, mae: 0.015702, mean_q: 0.019664
  13568/1000000: episode: 1904, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000544, mae: 0.016242, mean_q: 0.019400
  13574/1000000: episode: 1905, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000458, mae: 0.012626, mean_q: 0.012731
  13578/1000000: episode: 1906, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000569, mae: 0.014430, mean_q: 0.020301
  13584/1000000: episode: 1907, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000493, mae: 0.015801, mean_q: 0.018591
  13590/1000000: episode: 1908, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000552, mae: 0.014457, mean_q: 0.018496
  13594/1000000: episode: 1909, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.002944, mae: 0.017461, mean_q: 0.019071
  13598/1000000: episode: 1910, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000548, mae: 0.017980, mean_q: 0.025685
  13604/1000000: episode: 1911, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000306, mae: 0.014565, mean_q: 0.011012
[Info] FALSIFICATION!
  13609/1000000: episode: 1912, duration: 0.172s, episode steps: 5, steps per second: 29, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000961, mae: 0.022738, mean_q: 0.017944
  13615/1000000: episode: 1913, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000738, mae: 0.017416, mean_q: 0.022441
  13619/1000000: episode: 1914, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000541, mae: 0.012840, mean_q: 0.018253
  13623/1000000: episode: 1915, duration: 0.021s, episode steps: 4, steps per second: 186, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000267, mae: 0.009166, mean_q: 0.015212
  13629/1000000: episode: 1916, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000296, mae: 0.013820, mean_q: 0.014340
  13633/1000000: episode: 1917, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000441, mae: 0.014802, mean_q: 0.016449
  13639/1000000: episode: 1918, duration: 0.030s, episode steps: 6, steps per second: 198, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.002398, mae: 0.020212, mean_q: 0.022257
  13645/1000000: episode: 1919, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000304, mae: 0.010906, mean_q: 0.016026
  13651/1000000: episode: 1920, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002020, mae: 0.016576, mean_q: 0.018366
  13655/1000000: episode: 1921, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001035, mae: 0.018645, mean_q: 0.016974
  13661/1000000: episode: 1922, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000629, mae: 0.014304, mean_q: 0.014673
  13665/1000000: episode: 1923, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001198, mae: 0.021353, mean_q: 0.025967
  13669/1000000: episode: 1924, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000311, mae: 0.013669, mean_q: 0.015051
  13675/1000000: episode: 1925, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000482, mae: 0.013909, mean_q: 0.014368
  13679/1000000: episode: 1926, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.005935, mae: 0.023449, mean_q: 0.018172
  13683/1000000: episode: 1927, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000652, mae: 0.024313, mean_q: 0.031992
  13687/1000000: episode: 1928, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000918, mae: 0.023990, mean_q: 0.022488
  13693/1000000: episode: 1929, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.001284, mae: 0.025661, mean_q: 0.008929
  13697/1000000: episode: 1930, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002946, mae: 0.026305, mean_q: 0.030300
  13701/1000000: episode: 1931, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001396, mae: 0.027596, mean_q: 0.020415
  13707/1000000: episode: 1932, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000653, mae: 0.018154, mean_q: 0.011993
  13711/1000000: episode: 1933, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000433, mae: 0.016723, mean_q: 0.012293
  13715/1000000: episode: 1934, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002965, mae: 0.026234, mean_q: 0.029520
  13719/1000000: episode: 1935, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000443, mae: 0.017906, mean_q: 0.010757
  13723/1000000: episode: 1936, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000450, mae: 0.015826, mean_q: 0.019317
  13729/1000000: episode: 1937, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000245, mae: 0.012537, mean_q: 0.012760
  13733/1000000: episode: 1938, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000411, mae: 0.015056, mean_q: 0.016264
  13739/1000000: episode: 1939, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000334, mae: 0.013390, mean_q: 0.015037
  13743/1000000: episode: 1940, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000567, mae: 0.012504, mean_q: 0.014055
  13747/1000000: episode: 1941, duration: 0.021s, episode steps: 4, steps per second: 186, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000281, mae: 0.009691, mean_q: 0.012765
  13753/1000000: episode: 1942, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000520, mae: 0.013197, mean_q: 0.021778
  13759/1000000: episode: 1943, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001836, mae: 0.014109, mean_q: 0.014325
  13765/1000000: episode: 1944, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000467, mae: 0.012045, mean_q: 0.013874
  13769/1000000: episode: 1945, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000563, mae: 0.013379, mean_q: 0.015211
  13773/1000000: episode: 1946, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000314, mae: 0.011789, mean_q: 0.016496
  13777/1000000: episode: 1947, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000718, mae: 0.017531, mean_q: 0.025942
  13781/1000000: episode: 1948, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000239, mae: 0.011301, mean_q: 0.013679
  13785/1000000: episode: 1949, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000314, mae: 0.012071, mean_q: 0.014900
  13791/1000000: episode: 1950, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000447, mae: 0.014284, mean_q: 0.013526
  13797/1000000: episode: 1951, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000309, mae: 0.011416, mean_q: 0.016944
  13803/1000000: episode: 1952, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000540, mae: 0.014318, mean_q: 0.019941
  13809/1000000: episode: 1953, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000573, mae: 0.016764, mean_q: 0.023194
  13813/1000000: episode: 1954, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000497, mae: 0.013354, mean_q: 0.022131
  13817/1000000: episode: 1955, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.002629, mae: 0.016842, mean_q: 0.016314
  13821/1000000: episode: 1956, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000873, mae: 0.019225, mean_q: 0.024807
  13825/1000000: episode: 1957, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000683, mae: 0.016914, mean_q: 0.016496
  13829/1000000: episode: 1958, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000498, mae: 0.014916, mean_q: 0.016014
  13835/1000000: episode: 1959, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 1.308, mean reward: 0.218 [0.018, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 8.250 [6.000, 11.000], loss: 0.000266, mae: 0.011515, mean_q: 0.014044
[Info] FALSIFICATION!
  13840/1000000: episode: 1960, duration: 0.172s, episode steps: 5, steps per second: 29, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000209, mae: 0.009824, mean_q: 0.012200
  13844/1000000: episode: 1961, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000211, mae: 0.008871, mean_q: 0.010859
[Info] Complete ISplit Iteration
[Info] Levels: [0.029758275, 0.21240747]
[Info] Cond. Prob: [0.11, 0.02]
[Info] Error Prob: 0.0022

  13848/1000000: episode: 1962, duration: 0.717s, episode steps: 4, steps per second: 6, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000296, mae: 0.013005, mean_q: 0.015754
  13858/1000000: episode: 1963, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001891, mae: 0.020268, mean_q: 0.022634
  13868/1000000: episode: 1964, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000845, mae: 0.024163, mean_q: 0.017514
  13878/1000000: episode: 1965, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000707, mae: 0.019172, mean_q: 0.014900
  13888/1000000: episode: 1966, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000372, mae: 0.012308, mean_q: 0.012541
  13898/1000000: episode: 1967, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001681, mae: 0.016816, mean_q: 0.018517
  13908/1000000: episode: 1968, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000615, mae: 0.016871, mean_q: 0.015243
  13918/1000000: episode: 1969, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.001498, mae: 0.018187, mean_q: 0.022096
  13928/1000000: episode: 1970, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000304, mae: 0.013432, mean_q: 0.011566
  13938/1000000: episode: 1971, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001520, mae: 0.017812, mean_q: 0.021602
  13948/1000000: episode: 1972, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000504, mae: 0.014504, mean_q: 0.017422
  13958/1000000: episode: 1973, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000410, mae: 0.012613, mean_q: 0.016371
  13968/1000000: episode: 1974, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001904, mae: 0.018708, mean_q: 0.017261
  13978/1000000: episode: 1975, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.002595, mae: 0.021836, mean_q: 0.020261
  13988/1000000: episode: 1976, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000652, mae: 0.016697, mean_q: 0.021836
  13998/1000000: episode: 1977, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000453, mae: 0.013552, mean_q: 0.019730
  14008/1000000: episode: 1978, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002537, mae: 0.020016, mean_q: 0.020618
  14018/1000000: episode: 1979, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.088, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001605, mae: 0.017539, mean_q: 0.021123
  14028/1000000: episode: 1980, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000584, mae: 0.012984, mean_q: 0.018251
  14038/1000000: episode: 1981, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000629, mae: 0.016263, mean_q: 0.017627
  14048/1000000: episode: 1982, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000339, mae: 0.011458, mean_q: 0.015027
  14058/1000000: episode: 1983, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000541, mae: 0.013694, mean_q: 0.018382
  14068/1000000: episode: 1984, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001327, mae: 0.014130, mean_q: 0.020105
  14078/1000000: episode: 1985, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000900, mae: 0.021156, mean_q: 0.016193
  14088/1000000: episode: 1986, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001525, mae: 0.019302, mean_q: 0.021342
  14098/1000000: episode: 1987, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002340, mae: 0.017381, mean_q: 0.017537
  14108/1000000: episode: 1988, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000443, mae: 0.014599, mean_q: 0.022099
  14118/1000000: episode: 1989, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000480, mae: 0.013884, mean_q: 0.019936
  14128/1000000: episode: 1990, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000340, mae: 0.012027, mean_q: 0.014760
  14138/1000000: episode: 1991, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001502, mae: 0.015971, mean_q: 0.021377
  14148/1000000: episode: 1992, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000575, mae: 0.015401, mean_q: 0.021316
  14158/1000000: episode: 1993, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000728, mae: 0.019030, mean_q: 0.020274
  14168/1000000: episode: 1994, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.264, mean reward: 0.026 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [-1.000, 10.000], loss: 0.000296, mae: 0.012653, mean_q: 0.013935
  14178/1000000: episode: 1995, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000522, mae: 0.012440, mean_q: 0.015926
  14188/1000000: episode: 1996, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.700 [-1.000, 10.000], loss: 0.000347, mae: 0.011206, mean_q: 0.018351
  14198/1000000: episode: 1997, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.002394, mae: 0.021900, mean_q: 0.022218
  14208/1000000: episode: 1998, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002466, mae: 0.020774, mean_q: 0.020291
  14218/1000000: episode: 1999, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000503, mae: 0.016242, mean_q: 0.016086
  14228/1000000: episode: 2000, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000747, mae: 0.015421, mean_q: 0.014569
  14238/1000000: episode: 2001, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000374, mae: 0.013845, mean_q: 0.015582
  14248/1000000: episode: 2002, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000257, mae: 0.012572, mean_q: 0.015309
  14258/1000000: episode: 2003, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000324, mae: 0.010533, mean_q: 0.015087
  14268/1000000: episode: 2004, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.115, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.001636, mae: 0.017922, mean_q: 0.020967
  14278/1000000: episode: 2005, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000429, mae: 0.013426, mean_q: 0.018254
  14288/1000000: episode: 2006, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000414, mae: 0.013277, mean_q: 0.017888
[Info] FALSIFICATION!
  14298/1000000: episode: 2007, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.001638, mae: 0.017040, mean_q: 0.022737
  14308/1000000: episode: 2008, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001751, mae: 0.021647, mean_q: 0.022324
  14318/1000000: episode: 2009, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000504, mae: 0.017194, mean_q: 0.020178
  14328/1000000: episode: 2010, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [-1.000, 10.000], loss: 0.000584, mae: 0.019165, mean_q: 0.020173
  14338/1000000: episode: 2011, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000541, mae: 0.016393, mean_q: 0.016095
  14348/1000000: episode: 2012, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.002687, mae: 0.021356, mean_q: 0.021448
  14358/1000000: episode: 2013, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001640, mae: 0.024096, mean_q: 0.025277
  14368/1000000: episode: 2014, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001018, mae: 0.026331, mean_q: 0.014536
  14378/1000000: episode: 2015, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000898, mae: 0.022141, mean_q: 0.020181
  14388/1000000: episode: 2016, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000346, mae: 0.013349, mean_q: 0.010937
  14398/1000000: episode: 2017, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000260, mae: 0.011351, mean_q: 0.015211
  14408/1000000: episode: 2018, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001066, mae: 0.020915, mean_q: 0.024370
  14418/1000000: episode: 2019, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002572, mae: 0.019517, mean_q: 0.017646
  14428/1000000: episode: 2020, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000870, mae: 0.021433, mean_q: 0.021821
  14438/1000000: episode: 2021, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001501, mae: 0.018698, mean_q: 0.019277
  14448/1000000: episode: 2022, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000595, mae: 0.016735, mean_q: 0.018856
  14458/1000000: episode: 2023, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000657, mae: 0.014335, mean_q: 0.018938
  14468/1000000: episode: 2024, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001412, mae: 0.016034, mean_q: 0.021061
  14478/1000000: episode: 2025, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000422, mae: 0.014123, mean_q: 0.016430
  14488/1000000: episode: 2026, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000390, mae: 0.012200, mean_q: 0.015674
  14498/1000000: episode: 2027, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000448, mae: 0.012381, mean_q: 0.015375
  14508/1000000: episode: 2028, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000294, mae: 0.010134, mean_q: 0.014213
  14518/1000000: episode: 2029, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000396, mae: 0.013408, mean_q: 0.019207
  14528/1000000: episode: 2030, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000441, mae: 0.014736, mean_q: 0.014474
  14538/1000000: episode: 2031, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000317, mae: 0.013355, mean_q: 0.017827
  14548/1000000: episode: 2032, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.002364, mae: 0.017555, mean_q: 0.018500
  14558/1000000: episode: 2033, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000580, mae: 0.014352, mean_q: 0.014156
  14568/1000000: episode: 2034, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000404, mae: 0.013744, mean_q: 0.017279
  14578/1000000: episode: 2035, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000307, mae: 0.012853, mean_q: 0.014576
  14588/1000000: episode: 2036, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000782, mae: 0.016293, mean_q: 0.019442
  14598/1000000: episode: 2037, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001523, mae: 0.016446, mean_q: 0.025424
  14608/1000000: episode: 2038, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001388, mae: 0.017943, mean_q: 0.013192
  14618/1000000: episode: 2039, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000883, mae: 0.020777, mean_q: 0.024430
  14628/1000000: episode: 2040, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002675, mae: 0.025247, mean_q: 0.018559
  14638/1000000: episode: 2041, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000818, mae: 0.022403, mean_q: 0.020852
  14648/1000000: episode: 2042, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000416, mae: 0.014984, mean_q: 0.014617
  14658/1000000: episode: 2043, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001320, mae: 0.014246, mean_q: 0.018400
  14668/1000000: episode: 2044, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001553, mae: 0.019885, mean_q: 0.025892
  14678/1000000: episode: 2045, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000304, mae: 0.012534, mean_q: 0.016576
  14688/1000000: episode: 2046, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000585, mae: 0.013983, mean_q: 0.018654
  14698/1000000: episode: 2047, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000445, mae: 0.013772, mean_q: 0.016954
  14708/1000000: episode: 2048, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001399, mae: 0.014967, mean_q: 0.020033
  14718/1000000: episode: 2049, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000283, mae: 0.010404, mean_q: 0.017270
  14728/1000000: episode: 2050, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001352, mae: 0.016401, mean_q: 0.019670
  14738/1000000: episode: 2051, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000205, mae: 0.011434, mean_q: 0.015048
  14748/1000000: episode: 2052, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000364, mae: 0.013027, mean_q: 0.017262
  14758/1000000: episode: 2053, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000418, mae: 0.015474, mean_q: 0.015890
  14768/1000000: episode: 2054, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001601, mae: 0.017654, mean_q: 0.019231
  14778/1000000: episode: 2055, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.130, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.004173, mae: 0.027867, mean_q: 0.027641
  14788/1000000: episode: 2056, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002336, mae: 0.023667, mean_q: 0.018251
  14798/1000000: episode: 2057, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001400, mae: 0.019445, mean_q: 0.025014
  14808/1000000: episode: 2058, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001484, mae: 0.021023, mean_q: 0.020652
  14818/1000000: episode: 2059, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.002500, mae: 0.023299, mean_q: 0.020598
  14828/1000000: episode: 2060, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001775, mae: 0.023964, mean_q: 0.021959
  14838/1000000: episode: 2061, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002273, mae: 0.020513, mean_q: 0.020975
[Info] Complete ISplit Iteration
[Info] Levels: [0.26172295]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

  14848/1000000: episode: 2062, duration: 0.721s, episode steps: 10, steps per second: 14, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001529, mae: 0.020812, mean_q: 0.017616
  14858/1000000: episode: 2063, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000639, mae: 0.021221, mean_q: 0.023041
  14868/1000000: episode: 2064, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000604, mae: 0.019159, mean_q: 0.015133
  14878/1000000: episode: 2065, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.055, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001282, mae: 0.017597, mean_q: 0.024015
  14888/1000000: episode: 2066, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000347, mae: 0.012489, mean_q: 0.015411
  14898/1000000: episode: 2067, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000578, mae: 0.016747, mean_q: 0.021200
  14908/1000000: episode: 2068, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001471, mae: 0.016367, mean_q: 0.017653
  14918/1000000: episode: 2069, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000309, mae: 0.013742, mean_q: 0.015830
  14928/1000000: episode: 2070, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001743, mae: 0.022678, mean_q: 0.024745
  14938/1000000: episode: 2071, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000427, mae: 0.015671, mean_q: 0.013892
  14948/1000000: episode: 2072, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.264, mean reward: 0.026 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [-1.000, 10.000], loss: 0.000489, mae: 0.013135, mean_q: 0.017937
  14958/1000000: episode: 2073, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000830, mae: 0.018197, mean_q: 0.023640
  14968/1000000: episode: 2074, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000348, mae: 0.013572, mean_q: 0.010267
  14978/1000000: episode: 2075, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000219, mae: 0.011016, mean_q: 0.014637
  14988/1000000: episode: 2076, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000267, mae: 0.009925, mean_q: 0.011278
  14998/1000000: episode: 2077, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000359, mae: 0.012696, mean_q: 0.015195
  15008/1000000: episode: 2078, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001407, mae: 0.017226, mean_q: 0.021799
  15018/1000000: episode: 2079, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000501, mae: 0.015002, mean_q: 0.016903
  15028/1000000: episode: 2080, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001879, mae: 0.022150, mean_q: 0.022860
  15038/1000000: episode: 2081, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001229, mae: 0.017656, mean_q: 0.016907
  15048/1000000: episode: 2082, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000321, mae: 0.010531, mean_q: 0.013363
  15058/1000000: episode: 2083, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.002180, mae: 0.017677, mean_q: 0.016674
  15068/1000000: episode: 2084, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000411, mae: 0.012845, mean_q: 0.016316
  15078/1000000: episode: 2085, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001284, mae: 0.013957, mean_q: 0.016401
  15088/1000000: episode: 2086, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002007, mae: 0.016155, mean_q: 0.017457
  15098/1000000: episode: 2087, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001102, mae: 0.011634, mean_q: 0.014298
  15108/1000000: episode: 2088, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001153, mae: 0.013282, mean_q: 0.021046
  15118/1000000: episode: 2089, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001209, mae: 0.013091, mean_q: 0.018459
  15128/1000000: episode: 2090, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000572, mae: 0.014569, mean_q: 0.017829
  15138/1000000: episode: 2091, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000571, mae: 0.017214, mean_q: 0.011867
  15148/1000000: episode: 2092, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.001306, mae: 0.017191, mean_q: 0.012771
  15158/1000000: episode: 2093, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001780, mae: 0.022938, mean_q: 0.022468
  15168/1000000: episode: 2094, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000549, mae: 0.014896, mean_q: 0.019066
  15178/1000000: episode: 2095, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000509, mae: 0.013397, mean_q: 0.018130
  15188/1000000: episode: 2096, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000344, mae: 0.011921, mean_q: 0.015932
  15198/1000000: episode: 2097, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002411, mae: 0.020218, mean_q: 0.020721
  15208/1000000: episode: 2098, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001298, mae: 0.017214, mean_q: 0.015304
  15218/1000000: episode: 2099, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000458, mae: 0.011946, mean_q: 0.017357
  15228/1000000: episode: 2100, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.002105, mae: 0.017160, mean_q: 0.019887
  15238/1000000: episode: 2101, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000252, mae: 0.010286, mean_q: 0.013268
  15248/1000000: episode: 2102, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001155, mae: 0.014785, mean_q: 0.020376
  15258/1000000: episode: 2103, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.002065, mae: 0.016079, mean_q: 0.016588
  15268/1000000: episode: 2104, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000272, mae: 0.010709, mean_q: 0.015483
  15278/1000000: episode: 2105, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000586, mae: 0.014483, mean_q: 0.020006
  15288/1000000: episode: 2106, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001474, mae: 0.016643, mean_q: 0.016583
  15298/1000000: episode: 2107, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000450, mae: 0.013850, mean_q: 0.016104
  15308/1000000: episode: 2108, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001165, mae: 0.014045, mean_q: 0.012463
  15318/1000000: episode: 2109, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001401, mae: 0.016228, mean_q: 0.021778
  15328/1000000: episode: 2110, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000400, mae: 0.014176, mean_q: 0.015053
  15338/1000000: episode: 2111, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001339, mae: 0.018083, mean_q: 0.018403
  15348/1000000: episode: 2112, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001335, mae: 0.015465, mean_q: 0.016384
  15358/1000000: episode: 2113, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000219, mae: 0.009468, mean_q: 0.011469
  15368/1000000: episode: 2114, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000344, mae: 0.011937, mean_q: 0.012616
  15378/1000000: episode: 2115, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001285, mae: 0.012422, mean_q: 0.014009
  15388/1000000: episode: 2116, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.002172, mae: 0.015401, mean_q: 0.013579
  15398/1000000: episode: 2117, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001281, mae: 0.014814, mean_q: 0.018107
  15408/1000000: episode: 2118, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002204, mae: 0.018139, mean_q: 0.018667
  15418/1000000: episode: 2119, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000265, mae: 0.012160, mean_q: 0.012266
  15428/1000000: episode: 2120, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001391, mae: 0.016265, mean_q: 0.019300
  15438/1000000: episode: 2121, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000321, mae: 0.010339, mean_q: 0.011953
  15448/1000000: episode: 2122, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.001265, mae: 0.014741, mean_q: 0.015190
  15458/1000000: episode: 2123, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000403, mae: 0.012202, mean_q: 0.015932
  15468/1000000: episode: 2124, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000607, mae: 0.013143, mean_q: 0.016762
[Info] FALSIFICATION!
  15478/1000000: episode: 2125, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000220, mae: 0.010229, mean_q: 0.012502
  15488/1000000: episode: 2126, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000369, mae: 0.011331, mean_q: 0.016680
  15498/1000000: episode: 2127, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001181, mae: 0.014428, mean_q: 0.018233
  15508/1000000: episode: 2128, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000436, mae: 0.014951, mean_q: 0.014135
  15518/1000000: episode: 2129, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001256, mae: 0.015297, mean_q: 0.014054
  15528/1000000: episode: 2130, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000318, mae: 0.011618, mean_q: 0.016464
  15538/1000000: episode: 2131, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000647, mae: 0.013934, mean_q: 0.017843
  15548/1000000: episode: 2132, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000414, mae: 0.012412, mean_q: 0.016122
  15558/1000000: episode: 2133, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002039, mae: 0.016706, mean_q: 0.020353
  15568/1000000: episode: 2134, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000556, mae: 0.012847, mean_q: 0.017072
  15578/1000000: episode: 2135, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000515, mae: 0.012967, mean_q: 0.016180
  15588/1000000: episode: 2136, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000297, mae: 0.011382, mean_q: 0.014081
  15598/1000000: episode: 2137, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001139, mae: 0.013860, mean_q: 0.011544
  15608/1000000: episode: 2138, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000248, mae: 0.012312, mean_q: 0.014552
  15618/1000000: episode: 2139, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000409, mae: 0.012460, mean_q: 0.015141
[Info] FALSIFICATION!
  15628/1000000: episode: 2140, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000279, mae: 0.010753, mean_q: 0.013643
  15638/1000000: episode: 2141, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001091, mae: 0.013346, mean_q: 0.017301
  15648/1000000: episode: 2142, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000534, mae: 0.015371, mean_q: 0.011887
  15658/1000000: episode: 2143, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001559, mae: 0.019526, mean_q: 0.022777
  15668/1000000: episode: 2144, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001174, mae: 0.016727, mean_q: 0.016351
  15678/1000000: episode: 2145, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001179, mae: 0.015535, mean_q: 0.012183
  15688/1000000: episode: 2146, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001972, mae: 0.015706, mean_q: 0.020037
  15698/1000000: episode: 2147, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000518, mae: 0.014490, mean_q: 0.018483
  15708/1000000: episode: 2148, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000283, mae: 0.010460, mean_q: 0.010017
  15718/1000000: episode: 2149, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000210, mae: 0.009117, mean_q: 0.010588
  15728/1000000: episode: 2150, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001692, mae: 0.016445, mean_q: 0.017868
  15738/1000000: episode: 2151, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000334, mae: 0.011677, mean_q: 0.014912
  15748/1000000: episode: 2152, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.002072, mae: 0.017866, mean_q: 0.016035
  15758/1000000: episode: 2153, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000625, mae: 0.015875, mean_q: 0.017629
  15768/1000000: episode: 2154, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001199, mae: 0.013909, mean_q: 0.015381
  15778/1000000: episode: 2155, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000462, mae: 0.014769, mean_q: 0.013921
  15788/1000000: episode: 2156, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001333, mae: 0.021405, mean_q: 0.019974
  15798/1000000: episode: 2157, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001187, mae: 0.019106, mean_q: 0.016047
  15808/1000000: episode: 2158, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000461, mae: 0.015478, mean_q: 0.015972
  15818/1000000: episode: 2159, duration: 0.048s, episode steps: 10, steps per second: 211, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000234, mae: 0.011537, mean_q: 0.011432
  15828/1000000: episode: 2160, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001797, mae: 0.015582, mean_q: 0.018726
  15838/1000000: episode: 2161, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000379, mae: 0.013826, mean_q: 0.019301
[Info] Complete ISplit Iteration
[Info] Levels: [0.29644385]
[Info] Cond. Prob: [0.02]
[Info] Error Prob: 0.02

  15848/1000000: episode: 2162, duration: 0.710s, episode steps: 10, steps per second: 14, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000353, mae: 0.012445, mean_q: 0.010976
  15858/1000000: episode: 2163, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000320, mae: 0.012517, mean_q: 0.015890
  15868/1000000: episode: 2164, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001104, mae: 0.014667, mean_q: 0.013189
  15878/1000000: episode: 2165, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000535, mae: 0.015373, mean_q: 0.021565
  15888/1000000: episode: 2166, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000726, mae: 0.018023, mean_q: 0.009110
  15898/1000000: episode: 2167, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000463, mae: 0.015664, mean_q: 0.018599
  15908/1000000: episode: 2168, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000294, mae: 0.013230, mean_q: 0.008253
  15918/1000000: episode: 2169, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000579, mae: 0.015993, mean_q: 0.018561
  15928/1000000: episode: 2170, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000302, mae: 0.010257, mean_q: 0.008676
  15938/1000000: episode: 2171, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000708, mae: 0.014597, mean_q: 0.017780
  15948/1000000: episode: 2172, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000421, mae: 0.011806, mean_q: 0.009662
  15958/1000000: episode: 2173, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000334, mae: 0.009910, mean_q: 0.013462
  15968/1000000: episode: 2174, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000401, mae: 0.012540, mean_q: 0.014892
  15978/1000000: episode: 2175, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000414, mae: 0.011369, mean_q: 0.012151
  15988/1000000: episode: 2176, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001413, mae: 0.013667, mean_q: 0.014558
  15998/1000000: episode: 2177, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000308, mae: 0.012790, mean_q: 0.009557
  16008/1000000: episode: 2178, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001500, mae: 0.016662, mean_q: 0.017653
  16018/1000000: episode: 2179, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001938, mae: 0.016262, mean_q: 0.016348
  16028/1000000: episode: 2180, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000542, mae: 0.015393, mean_q: 0.019216
  16038/1000000: episode: 2181, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000418, mae: 0.013077, mean_q: 0.014375
  16048/1000000: episode: 2182, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000250, mae: 0.009048, mean_q: 0.012795
  16058/1000000: episode: 2183, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000690, mae: 0.014958, mean_q: 0.019795
  16068/1000000: episode: 2184, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000273, mae: 0.009916, mean_q: 0.012405
  16078/1000000: episode: 2185, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001524, mae: 0.018664, mean_q: 0.018222
  16088/1000000: episode: 2186, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001410, mae: 0.014835, mean_q: 0.013631
  16098/1000000: episode: 2187, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.002224, mae: 0.020922, mean_q: 0.019703
  16108/1000000: episode: 2188, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.067, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000398, mae: 0.013499, mean_q: 0.015811
  16118/1000000: episode: 2189, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000411, mae: 0.012976, mean_q: 0.013613
  16128/1000000: episode: 2190, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000368, mae: 0.011824, mean_q: 0.013090
  16138/1000000: episode: 2191, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.003357, mae: 0.025960, mean_q: 0.026656
  16148/1000000: episode: 2192, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000790, mae: 0.020627, mean_q: 0.015708
  16158/1000000: episode: 2193, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001203, mae: 0.018442, mean_q: 0.016415
  16168/1000000: episode: 2194, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000286, mae: 0.010440, mean_q: 0.011246
  16178/1000000: episode: 2195, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000409, mae: 0.013392, mean_q: 0.017215
  16188/1000000: episode: 2196, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002128, mae: 0.018841, mean_q: 0.019210
  16198/1000000: episode: 2197, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000436, mae: 0.013608, mean_q: 0.014239
  16208/1000000: episode: 2198, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000435, mae: 0.012793, mean_q: 0.016763
  16218/1000000: episode: 2199, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000556, mae: 0.014342, mean_q: 0.016709
  16228/1000000: episode: 2200, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000299, mae: 0.009583, mean_q: 0.014345
  16238/1000000: episode: 2201, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001881, mae: 0.015094, mean_q: 0.016091
  16248/1000000: episode: 2202, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001102, mae: 0.015625, mean_q: 0.014453
  16258/1000000: episode: 2203, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000967, mae: 0.011413, mean_q: 0.016660
  16268/1000000: episode: 2204, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.002124, mae: 0.015986, mean_q: 0.017116
  16278/1000000: episode: 2205, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000226, mae: 0.010397, mean_q: 0.013856
  16288/1000000: episode: 2206, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000514, mae: 0.013560, mean_q: 0.009938
  16298/1000000: episode: 2207, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000292, mae: 0.011331, mean_q: 0.015600
  16308/1000000: episode: 2208, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001887, mae: 0.016971, mean_q: 0.017888
  16318/1000000: episode: 2209, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001295, mae: 0.018579, mean_q: 0.017559
  16328/1000000: episode: 2210, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000411, mae: 0.013211, mean_q: 0.017444
  16338/1000000: episode: 2211, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001387, mae: 0.018155, mean_q: 0.017170
  16348/1000000: episode: 2212, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001407, mae: 0.017464, mean_q: 0.017648
  16358/1000000: episode: 2213, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001236, mae: 0.021657, mean_q: 0.018530
  16368/1000000: episode: 2214, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001383, mae: 0.021781, mean_q: 0.011583
  16378/1000000: episode: 2215, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000606, mae: 0.017223, mean_q: 0.017234
  16388/1000000: episode: 2216, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.002004, mae: 0.019612, mean_q: 0.021036
  16398/1000000: episode: 2217, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001247, mae: 0.017886, mean_q: 0.017914
  16408/1000000: episode: 2218, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000348, mae: 0.012599, mean_q: 0.013570
  16418/1000000: episode: 2219, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001317, mae: 0.016364, mean_q: 0.019174
  16428/1000000: episode: 2220, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000453, mae: 0.013082, mean_q: 0.016320
  16438/1000000: episode: 2221, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000612, mae: 0.014512, mean_q: 0.013490
  16448/1000000: episode: 2222, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.001137, mae: 0.015512, mean_q: 0.021193
  16458/1000000: episode: 2223, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000614, mae: 0.015391, mean_q: 0.019052
  16468/1000000: episode: 2224, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001241, mae: 0.015401, mean_q: 0.018016
  16478/1000000: episode: 2225, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000331, mae: 0.011552, mean_q: 0.014087
  16488/1000000: episode: 2226, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.800 [-1.000, 10.000], loss: 0.000381, mae: 0.012447, mean_q: 0.015218
  16498/1000000: episode: 2227, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001294, mae: 0.014743, mean_q: 0.018134
  16508/1000000: episode: 2228, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001253, mae: 0.017618, mean_q: 0.014574
  16518/1000000: episode: 2229, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000254, mae: 0.011113, mean_q: 0.014065
  16528/1000000: episode: 2230, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.900 [-1.000, 10.000], loss: 0.000294, mae: 0.010933, mean_q: 0.012142
  16538/1000000: episode: 2231, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000415, mae: 0.011889, mean_q: 0.012581
  16548/1000000: episode: 2232, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000695, mae: 0.013675, mean_q: 0.015698
  16558/1000000: episode: 2233, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000251, mae: 0.009361, mean_q: 0.009251
  16568/1000000: episode: 2234, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000312, mae: 0.009902, mean_q: 0.011215
  16578/1000000: episode: 2235, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000329, mae: 0.011870, mean_q: 0.012310
  16588/1000000: episode: 2236, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000445, mae: 0.012160, mean_q: 0.013688
  16598/1000000: episode: 2237, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000281, mae: 0.009449, mean_q: 0.010724
  16608/1000000: episode: 2238, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000209, mae: 0.009165, mean_q: 0.009641
  16618/1000000: episode: 2239, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000318, mae: 0.009328, mean_q: 0.012839
  16628/1000000: episode: 2240, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001252, mae: 0.012564, mean_q: 0.016143
  16638/1000000: episode: 2241, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000339, mae: 0.012190, mean_q: 0.012641
  16648/1000000: episode: 2242, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001523, mae: 0.018051, mean_q: 0.016615
  16658/1000000: episode: 2243, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.098, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.003216, mae: 0.023285, mean_q: 0.021614
  16668/1000000: episode: 2244, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000332, mae: 0.010318, mean_q: 0.011775
  16678/1000000: episode: 2245, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001110, mae: 0.012619, mean_q: 0.014101
  16688/1000000: episode: 2246, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000203, mae: 0.010334, mean_q: 0.015625
  16698/1000000: episode: 2247, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001917, mae: 0.017004, mean_q: 0.010612
  16708/1000000: episode: 2248, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001056, mae: 0.014772, mean_q: 0.016148
  16718/1000000: episode: 2249, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000190, mae: 0.009256, mean_q: 0.008041
  16728/1000000: episode: 2250, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000181, mae: 0.009144, mean_q: 0.009470
  16738/1000000: episode: 2251, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000468, mae: 0.010937, mean_q: 0.011082
  16748/1000000: episode: 2252, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000219, mae: 0.009882, mean_q: 0.010802
  16758/1000000: episode: 2253, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000323, mae: 0.012776, mean_q: 0.013869
  16768/1000000: episode: 2254, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001034, mae: 0.012544, mean_q: 0.010225
  16778/1000000: episode: 2255, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000166, mae: 0.009095, mean_q: 0.011534
  16788/1000000: episode: 2256, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000417, mae: 0.013715, mean_q: 0.012945
  16798/1000000: episode: 2257, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000298, mae: 0.010744, mean_q: 0.011986
  16808/1000000: episode: 2258, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.001104, mae: 0.011110, mean_q: 0.012611
  16818/1000000: episode: 2259, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000238, mae: 0.009558, mean_q: 0.010389
  16828/1000000: episode: 2260, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000213, mae: 0.008782, mean_q: 0.008357
  16838/1000000: episode: 2261, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000265, mae: 0.009659, mean_q: 0.012865
[Info] 1-TH LEVEL FOUND: 0.059669822454452515, Considering 11/100 traces
  16848/1000000: episode: 2262, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000341, mae: 0.008853, mean_q: 0.011463
  16854/1000000: episode: 2263, duration: 0.031s, episode steps: 6, steps per second: 197, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000416, mae: 0.010186, mean_q: 0.014068
  16857/1000000: episode: 2264, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000242, mae: 0.012202, mean_q: 0.013619
  16863/1000000: episode: 2265, duration: 0.029s, episode steps: 6, steps per second: 204, episode reward: 0.062, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000296, mae: 0.008829, mean_q: 0.010836
  16866/1000000: episode: 2266, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000427, mae: 0.012800, mean_q: 0.010654
  16872/1000000: episode: 2267, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000299, mae: 0.012653, mean_q: 0.009802
  16875/1000000: episode: 2268, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000166, mae: 0.008633, mean_q: 0.011546
  16881/1000000: episode: 2269, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001482, mae: 0.012885, mean_q: 0.012539
  16887/1000000: episode: 2270, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000153, mae: 0.009850, mean_q: 0.006098
  16893/1000000: episode: 2271, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000296, mae: 0.010394, mean_q: 0.013085
  16899/1000000: episode: 2272, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.001706, mae: 0.015248, mean_q: 0.014336
[Info] FALSIFICATION!
  16904/1000000: episode: 2273, duration: 0.172s, episode steps: 5, steps per second: 29, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000560, mae: 0.014657, mean_q: 0.010789
  16910/1000000: episode: 2274, duration: 0.031s, episode steps: 6, steps per second: 195, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000663, mae: 0.016191, mean_q: 0.016327
  16916/1000000: episode: 2275, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000232, mae: 0.014268, mean_q: 0.010038
  16922/1000000: episode: 2276, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000329, mae: 0.012232, mean_q: 0.009680
  16928/1000000: episode: 2277, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.001492, mae: 0.017662, mean_q: 0.023586
  16934/1000000: episode: 2278, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000217, mae: 0.011279, mean_q: 0.011110
  16940/1000000: episode: 2279, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000284, mae: 0.011677, mean_q: 0.008252
  16943/1000000: episode: 2280, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000194, mae: 0.010149, mean_q: 0.015193
  16946/1000000: episode: 2281, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000362, mae: 0.013852, mean_q: 0.008704
  16952/1000000: episode: 2282, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000344, mae: 0.009430, mean_q: 0.012590
  16958/1000000: episode: 2283, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000173, mae: 0.008206, mean_q: 0.009486
  16964/1000000: episode: 2284, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 1.941, mean reward: 0.323 [0.018, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 8.333 [6.000, 11.000], loss: 0.000427, mae: 0.013298, mean_q: 0.013306
  16970/1000000: episode: 2285, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000162, mae: 0.008277, mean_q: 0.009764
  16976/1000000: episode: 2286, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.001644, mae: 0.015365, mean_q: 0.018149
  16982/1000000: episode: 2287, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.003887, mae: 0.023472, mean_q: 0.018496
  16985/1000000: episode: 2288, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000105, mae: 0.007382, mean_q: 0.015005
  16991/1000000: episode: 2289, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000442, mae: 0.016418, mean_q: 0.009232
  16997/1000000: episode: 2290, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000251, mae: 0.010820, mean_q: 0.006734
  17003/1000000: episode: 2291, duration: 0.030s, episode steps: 6, steps per second: 201, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000459, mae: 0.013008, mean_q: 0.016215
  17009/1000000: episode: 2292, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000253, mae: 0.009537, mean_q: 0.008877
  17015/1000000: episode: 2293, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.001785, mae: 0.015032, mean_q: 0.013235
  17021/1000000: episode: 2294, duration: 0.030s, episode steps: 6, steps per second: 203, episode reward: 0.168, mean reward: 0.028 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000480, mae: 0.016250, mean_q: 0.014292
  17027/1000000: episode: 2295, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.001548, mae: 0.011679, mean_q: 0.012673
  17030/1000000: episode: 2296, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000270, mae: 0.010750, mean_q: 0.014350
  17036/1000000: episode: 2297, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 1.308, mean reward: 0.218 [0.018, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 8.250 [6.000, 11.000], loss: 0.000143, mae: 0.010191, mean_q: 0.009386
  17039/1000000: episode: 2298, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000373, mae: 0.012529, mean_q: 0.019219
  17042/1000000: episode: 2299, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000189, mae: 0.012442, mean_q: 0.001757
  17048/1000000: episode: 2300, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.001515, mae: 0.015112, mean_q: 0.011325
  17054/1000000: episode: 2301, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000433, mae: 0.012810, mean_q: 0.013854
  17060/1000000: episode: 2302, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.082, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000787, mae: 0.013688, mean_q: 0.013405
  17063/1000000: episode: 2303, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000347, mae: 0.014340, mean_q: 0.018319
  17069/1000000: episode: 2304, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000417, mae: 0.014747, mean_q: 0.012770
  17072/1000000: episode: 2305, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000607, mae: 0.014722, mean_q: 0.011010
  17075/1000000: episode: 2306, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000324, mae: 0.009641, mean_q: 0.014083
  17081/1000000: episode: 2307, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.001631, mae: 0.015147, mean_q: 0.018371
  17087/1000000: episode: 2308, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000588, mae: 0.016024, mean_q: 0.015870
  17090/1000000: episode: 2309, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000400, mae: 0.013970, mean_q: 0.017659
  17093/1000000: episode: 2310, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000230, mae: 0.013426, mean_q: 0.007305
  17096/1000000: episode: 2311, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000329, mae: 0.011058, mean_q: 0.010631
  17102/1000000: episode: 2312, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.001838, mae: 0.018222, mean_q: 0.023039
  17108/1000000: episode: 2313, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.001572, mae: 0.018032, mean_q: 0.018875
  17114/1000000: episode: 2314, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000366, mae: 0.012889, mean_q: 0.010284
  17120/1000000: episode: 2315, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000775, mae: 0.018569, mean_q: 0.012361
  17126/1000000: episode: 2316, duration: 0.028s, episode steps: 6, steps per second: 213, episode reward: 0.210, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000528, mae: 0.013496, mean_q: 0.018672
  17129/1000000: episode: 2317, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000321, mae: 0.011233, mean_q: 0.015184
  17135/1000000: episode: 2318, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000583, mae: 0.014698, mean_q: 0.016079
  17141/1000000: episode: 2319, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000335, mae: 0.009398, mean_q: 0.010657
  17144/1000000: episode: 2320, duration: 0.016s, episode steps: 3, steps per second: 185, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000978, mae: 0.017021, mean_q: 0.019751
  17150/1000000: episode: 2321, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000424, mae: 0.011214, mean_q: 0.016738
  17153/1000000: episode: 2322, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000388, mae: 0.013611, mean_q: 0.010992
  17159/1000000: episode: 2323, duration: 0.028s, episode steps: 6, steps per second: 212, episode reward: 0.125, mean reward: 0.021 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000290, mae: 0.013379, mean_q: 0.014396
  17162/1000000: episode: 2324, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000182, mae: 0.010930, mean_q: 0.005822
  17168/1000000: episode: 2325, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.105, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.167 [-1.000, 11.000], loss: 0.000347, mae: 0.014989, mean_q: 0.021185
  17174/1000000: episode: 2326, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.002426, mae: 0.023245, mean_q: 0.018156
  17180/1000000: episode: 2327, duration: 0.029s, episode steps: 6, steps per second: 209, episode reward: 1.941, mean reward: 0.323 [0.018, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 8.333 [6.000, 11.000], loss: 0.001604, mae: 0.017228, mean_q: 0.011276
  17186/1000000: episode: 2328, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000462, mae: 0.015942, mean_q: 0.015932
  17189/1000000: episode: 2329, duration: 0.016s, episode steps: 3, steps per second: 186, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000416, mae: 0.016189, mean_q: 0.012526
  17195/1000000: episode: 2330, duration: 0.028s, episode steps: 6, steps per second: 213, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000722, mae: 0.019490, mean_q: 0.011417
  17201/1000000: episode: 2331, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.136, mean reward: 0.023 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000759, mae: 0.015069, mean_q: 0.012674
  17207/1000000: episode: 2332, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.168, mean reward: 0.028 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000221, mae: 0.011462, mean_q: 0.015085
  17213/1000000: episode: 2333, duration: 0.030s, episode steps: 6, steps per second: 202, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000438, mae: 0.015582, mean_q: 0.020719
  17216/1000000: episode: 2334, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000673, mae: 0.019808, mean_q: 0.014374
  17222/1000000: episode: 2335, duration: 0.029s, episode steps: 6, steps per second: 208, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000742, mae: 0.016837, mean_q: 0.022315
  17228/1000000: episode: 2336, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.253, mean reward: 0.042 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.417 [-1.000, 11.000], loss: 0.000686, mae: 0.015282, mean_q: 0.011582
  17231/1000000: episode: 2337, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000600, mae: 0.016239, mean_q: 0.018019
  17237/1000000: episode: 2338, duration: 0.029s, episode steps: 6, steps per second: 210, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000450, mae: 0.012093, mean_q: 0.011952
  17243/1000000: episode: 2339, duration: 0.029s, episode steps: 6, steps per second: 205, episode reward: 0.093, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000282, mae: 0.010178, mean_q: 0.011754
  17249/1000000: episode: 2340, duration: 0.028s, episode steps: 6, steps per second: 212, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.000525, mae: 0.011910, mean_q: 0.015547
  17252/1000000: episode: 2341, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000151, mae: 0.009099, mean_q: 0.008515
  17258/1000000: episode: 2342, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.027, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000437, mae: 0.012857, mean_q: 0.018019
  17264/1000000: episode: 2343, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.073, mean reward: 0.012 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.083 [-1.000, 11.000], loss: 0.000578, mae: 0.012486, mean_q: 0.019041
  17270/1000000: episode: 2344, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 0.222, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.333 [-1.000, 11.000], loss: 0.001600, mae: 0.014929, mean_q: 0.016805
  17276/1000000: episode: 2345, duration: 0.029s, episode steps: 6, steps per second: 207, episode reward: 1.941, mean reward: 0.323 [0.018, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 8.333 [6.000, 11.000], loss: 0.000864, mae: 0.019327, mean_q: 0.020084
  17279/1000000: episode: 2346, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.136, mean reward: 0.045 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000643, mae: 0.016529, mean_q: 0.012004
  17285/1000000: episode: 2347, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.050, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.917 [-1.000, 11.000], loss: 0.000299, mae: 0.011965, mean_q: 0.016342
  17291/1000000: episode: 2348, duration: 0.028s, episode steps: 6, steps per second: 211, episode reward: 0.039, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.002000, mae: 0.023253, mean_q: 0.031142
  17294/1000000: episode: 2349, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000950, mae: 0.023216, mean_q: 0.016697
  17300/1000000: episode: 2350, duration: 0.029s, episode steps: 6, steps per second: 206, episode reward: 0.339, mean reward: 0.056 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000344, mae: 0.013381, mean_q: 0.018429
[Info] FALSIFICATION!
[Info] Complete ISplit Iteration
[Info] Levels: [0.059669822, 0.32583356]
[Info] Cond. Prob: [0.11, 0.02]
[Info] Error Prob: 0.0022

  17305/1000000: episode: 2351, duration: 0.938s, episode steps: 5, steps per second: 5, episode reward: 1.571, mean reward: 0.314 [0.018, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.000 [6.000, 10.000], loss: 0.000289, mae: 0.015022, mean_q: 0.001697
  17315/1000000: episode: 2352, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001346, mae: 0.020306, mean_q: 0.020661
  17325/1000000: episode: 2353, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000603, mae: 0.015835, mean_q: 0.016217
  17335/1000000: episode: 2354, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001226, mae: 0.016172, mean_q: 0.019720
  17345/1000000: episode: 2355, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000718, mae: 0.017531, mean_q: 0.016964
  17355/1000000: episode: 2356, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.001360, mae: 0.018645, mean_q: 0.016842
  17365/1000000: episode: 2357, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000673, mae: 0.015013, mean_q: 0.017356
  17375/1000000: episode: 2358, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001500, mae: 0.016731, mean_q: 0.019522
  17385/1000000: episode: 2359, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000686, mae: 0.016366, mean_q: 0.021821
  17395/1000000: episode: 2360, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000545, mae: 0.012321, mean_q: 0.020085
  17405/1000000: episode: 2361, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001328, mae: 0.016496, mean_q: 0.020694
  17415/1000000: episode: 2362, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001361, mae: 0.017416, mean_q: 0.020905
  17425/1000000: episode: 2363, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001278, mae: 0.015430, mean_q: 0.021910
  17435/1000000: episode: 2364, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001338, mae: 0.016267, mean_q: 0.018080
  17445/1000000: episode: 2365, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.003001, mae: 0.022900, mean_q: 0.027227
  17455/1000000: episode: 2366, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000469, mae: 0.015915, mean_q: 0.010392
  17465/1000000: episode: 2367, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002050, mae: 0.020772, mean_q: 0.022914
  17475/1000000: episode: 2368, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.002148, mae: 0.021514, mean_q: 0.026612
  17485/1000000: episode: 2369, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000620, mae: 0.016669, mean_q: 0.017818
  17495/1000000: episode: 2370, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001229, mae: 0.017512, mean_q: 0.014697
  17505/1000000: episode: 2371, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001404, mae: 0.016284, mean_q: 0.015330
  17515/1000000: episode: 2372, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000724, mae: 0.016661, mean_q: 0.021658
  17525/1000000: episode: 2373, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [-1.000, 10.000], loss: 0.000923, mae: 0.019000, mean_q: 0.020801
  17535/1000000: episode: 2374, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000512, mae: 0.014665, mean_q: 0.019212
  17545/1000000: episode: 2375, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000585, mae: 0.014921, mean_q: 0.018139
  17555/1000000: episode: 2376, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000555, mae: 0.017058, mean_q: 0.017764
  17565/1000000: episode: 2377, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000492, mae: 0.014903, mean_q: 0.015287
  17575/1000000: episode: 2378, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001538, mae: 0.018752, mean_q: 0.020153
  17585/1000000: episode: 2379, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000556, mae: 0.014643, mean_q: 0.022113
  17595/1000000: episode: 2380, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001613, mae: 0.016672, mean_q: 0.024095
  17605/1000000: episode: 2381, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002145, mae: 0.018634, mean_q: 0.022055
  17615/1000000: episode: 2382, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.001172, mae: 0.016824, mean_q: 0.013821
  17625/1000000: episode: 2383, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000393, mae: 0.011913, mean_q: 0.016142
  17635/1000000: episode: 2384, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001382, mae: 0.017838, mean_q: 0.020396
  17645/1000000: episode: 2385, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001370, mae: 0.016090, mean_q: 0.016932
  17655/1000000: episode: 2386, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000580, mae: 0.014313, mean_q: 0.021345
  17665/1000000: episode: 2387, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [-1.000, 10.000], loss: 0.001217, mae: 0.014501, mean_q: 0.014026
  17675/1000000: episode: 2388, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001373, mae: 0.018232, mean_q: 0.023506
  17685/1000000: episode: 2389, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000528, mae: 0.015038, mean_q: 0.016073
  17695/1000000: episode: 2390, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000378, mae: 0.012122, mean_q: 0.015073
  17705/1000000: episode: 2391, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000812, mae: 0.018926, mean_q: 0.022294
  17715/1000000: episode: 2392, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001553, mae: 0.016430, mean_q: 0.018631
  17725/1000000: episode: 2393, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001403, mae: 0.015733, mean_q: 0.018145
  17735/1000000: episode: 2394, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.001237, mae: 0.017148, mean_q: 0.022588
  17745/1000000: episode: 2395, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001474, mae: 0.017286, mean_q: 0.018943
  17755/1000000: episode: 2396, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002668, mae: 0.020414, mean_q: 0.021444
  17765/1000000: episode: 2397, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000488, mae: 0.014848, mean_q: 0.010991
  17775/1000000: episode: 2398, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001188, mae: 0.018939, mean_q: 0.022511
  17785/1000000: episode: 2399, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001270, mae: 0.017530, mean_q: 0.020714
  17795/1000000: episode: 2400, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001279, mae: 0.020641, mean_q: 0.020874
  17805/1000000: episode: 2401, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001583, mae: 0.021453, mean_q: 0.019237
  17815/1000000: episode: 2402, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001467, mae: 0.018530, mean_q: 0.021143
  17825/1000000: episode: 2403, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000501, mae: 0.014841, mean_q: 0.017036
  17835/1000000: episode: 2404, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001143, mae: 0.015593, mean_q: 0.019068
  17845/1000000: episode: 2405, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000606, mae: 0.015585, mean_q: 0.017025
  17855/1000000: episode: 2406, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000520, mae: 0.015562, mean_q: 0.013273
  17865/1000000: episode: 2407, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000386, mae: 0.012602, mean_q: 0.016073
  17875/1000000: episode: 2408, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000424, mae: 0.012895, mean_q: 0.013515
  17885/1000000: episode: 2409, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001576, mae: 0.017187, mean_q: 0.017621
  17895/1000000: episode: 2410, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001619, mae: 0.026875, mean_q: 0.026975
  17905/1000000: episode: 2411, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000706, mae: 0.020271, mean_q: 0.009982
  17915/1000000: episode: 2412, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [-1.000, 10.000], loss: 0.000684, mae: 0.016790, mean_q: 0.018768
  17925/1000000: episode: 2413, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001602, mae: 0.016574, mean_q: 0.014350
  17935/1000000: episode: 2414, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001593, mae: 0.018685, mean_q: 0.024604
  17945/1000000: episode: 2415, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001202, mae: 0.017705, mean_q: 0.010766
  17955/1000000: episode: 2416, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000456, mae: 0.015664, mean_q: 0.017047
  17965/1000000: episode: 2417, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000449, mae: 0.014988, mean_q: 0.014336
  17975/1000000: episode: 2418, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001922, mae: 0.016715, mean_q: 0.018330
  17985/1000000: episode: 2419, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000299, mae: 0.011606, mean_q: 0.014136
  17995/1000000: episode: 2420, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000442, mae: 0.013066, mean_q: 0.014135
  18005/1000000: episode: 2421, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000364, mae: 0.011096, mean_q: 0.012454
  18015/1000000: episode: 2422, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000674, mae: 0.014823, mean_q: 0.019223
  18025/1000000: episode: 2423, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002074, mae: 0.021407, mean_q: 0.022027
  18035/1000000: episode: 2424, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001434, mae: 0.018122, mean_q: 0.020622
  18045/1000000: episode: 2425, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.003292, mae: 0.022325, mean_q: 0.015808
  18055/1000000: episode: 2426, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001474, mae: 0.023320, mean_q: 0.022552
  18065/1000000: episode: 2427, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001633, mae: 0.020775, mean_q: 0.014900
  18075/1000000: episode: 2428, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001195, mae: 0.019227, mean_q: 0.020529
  18085/1000000: episode: 2429, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001439, mae: 0.018468, mean_q: 0.021334
  18095/1000000: episode: 2430, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000771, mae: 0.016906, mean_q: 0.013192
  18105/1000000: episode: 2431, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000487, mae: 0.013344, mean_q: 0.015165
  18115/1000000: episode: 2432, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001016, mae: 0.014745, mean_q: 0.016300
  18125/1000000: episode: 2433, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000366, mae: 0.012299, mean_q: 0.011165
  18135/1000000: episode: 2434, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000646, mae: 0.014746, mean_q: 0.017229
  18145/1000000: episode: 2435, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001051, mae: 0.013231, mean_q: 0.016332
  18155/1000000: episode: 2436, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001013, mae: 0.012938, mean_q: 0.011871
  18165/1000000: episode: 2437, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001175, mae: 0.015360, mean_q: 0.013636
  18175/1000000: episode: 2438, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.178, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.003204, mae: 0.025506, mean_q: 0.022713
  18185/1000000: episode: 2439, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001463, mae: 0.021818, mean_q: 0.020918
  18195/1000000: episode: 2440, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000463, mae: 0.018656, mean_q: 0.008952
  18205/1000000: episode: 2441, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000504, mae: 0.013348, mean_q: 0.014807
  18215/1000000: episode: 2442, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001237, mae: 0.014761, mean_q: 0.017047
  18225/1000000: episode: 2443, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001151, mae: 0.015704, mean_q: 0.019484
  18235/1000000: episode: 2444, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001268, mae: 0.014525, mean_q: 0.015631
  18245/1000000: episode: 2445, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000601, mae: 0.015807, mean_q: 0.014360
  18255/1000000: episode: 2446, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000481, mae: 0.014272, mean_q: 0.016773
  18265/1000000: episode: 2447, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001313, mae: 0.013152, mean_q: 0.011662
  18275/1000000: episode: 2448, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000233, mae: 0.009989, mean_q: 0.008448
  18285/1000000: episode: 2449, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000581, mae: 0.011448, mean_q: 0.009102
  18295/1000000: episode: 2450, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000995, mae: 0.014554, mean_q: 0.015309
[Info] 1-TH LEVEL FOUND: 0.02615654468536377, Considering 14/100 traces
  18305/1000000: episode: 2451, duration: 0.689s, episode steps: 10, steps per second: 15, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000300, mae: 0.010460, mean_q: 0.011315
  18309/1000000: episode: 2452, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000661, mae: 0.013061, mean_q: 0.014956
  18317/1000000: episode: 2453, duration: 0.036s, episode steps: 8, steps per second: 219, episode reward: 0.038, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.000347, mae: 0.010897, mean_q: 0.013468
  18325/1000000: episode: 2454, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.015, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.000513, mae: 0.011542, mean_q: 0.011676
  18329/1000000: episode: 2455, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000736, mae: 0.016513, mean_q: 0.018776
  18333/1000000: episode: 2456, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.005605, mae: 0.025508, mean_q: 0.024751
  18341/1000000: episode: 2457, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.036, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [-1.000, 11.000], loss: 0.000417, mae: 0.013201, mean_q: 0.017160
  18349/1000000: episode: 2458, duration: 0.037s, episode steps: 8, steps per second: 217, episode reward: 0.059, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.562 [-1.000, 11.000], loss: 0.000740, mae: 0.015972, mean_q: 0.019033
  18357/1000000: episode: 2459, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.081, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.002066, mae: 0.018711, mean_q: 0.014657
  18361/1000000: episode: 2460, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000219, mae: 0.011526, mean_q: 0.014613
  18369/1000000: episode: 2461, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.231, mean reward: 0.029 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000563, mae: 0.013983, mean_q: 0.023260
  18377/1000000: episode: 2462, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000527, mae: 0.013450, mean_q: 0.014943
  18385/1000000: episode: 2463, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.042, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.000450, mae: 0.012667, mean_q: 0.013619
  18389/1000000: episode: 2464, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000391, mae: 0.011914, mean_q: 0.018355
  18393/1000000: episode: 2465, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002705, mae: 0.016071, mean_q: 0.011335
  18397/1000000: episode: 2466, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000446, mae: 0.012559, mean_q: 0.013952
  18405/1000000: episode: 2467, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.013, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.062 [-1.000, 11.000], loss: 0.000669, mae: 0.015029, mean_q: 0.015205
  18413/1000000: episode: 2468, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.130, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.000424, mae: 0.011264, mean_q: 0.011387
  18421/1000000: episode: 2469, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.044, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.000363, mae: 0.010789, mean_q: 0.011589
  18425/1000000: episode: 2470, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000337, mae: 0.010431, mean_q: 0.011686
  18433/1000000: episode: 2471, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.188 [-1.000, 11.000], loss: 0.000320, mae: 0.010822, mean_q: 0.012839
  18437/1000000: episode: 2472, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000391, mae: 0.011112, mean_q: 0.012710
  18445/1000000: episode: 2473, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.134, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000476, mae: 0.011217, mean_q: 0.012286
  18453/1000000: episode: 2474, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.215, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.000480, mae: 0.010662, mean_q: 0.013179
  18457/1000000: episode: 2475, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.002678, mae: 0.015081, mean_q: 0.014517
  18465/1000000: episode: 2476, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.214, mean reward: 0.027 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.000311, mae: 0.010214, mean_q: 0.012736
  18469/1000000: episode: 2477, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000103, mae: 0.008581, mean_q: 0.013803
  18473/1000000: episode: 2478, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002393, mae: 0.020950, mean_q: 0.023747
  18481/1000000: episode: 2479, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.032, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.002706, mae: 0.021396, mean_q: 0.021583
  18489/1000000: episode: 2480, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.013, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.062 [-1.000, 11.000], loss: 0.000567, mae: 0.013042, mean_q: 0.016162
  18493/1000000: episode: 2481, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.002308, mae: 0.018462, mean_q: 0.018713
  18501/1000000: episode: 2482, duration: 0.037s, episode steps: 8, steps per second: 219, episode reward: 0.030, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.000678, mae: 0.016432, mean_q: 0.018759
  18509/1000000: episode: 2483, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.000480, mae: 0.014429, mean_q: 0.014334
  18513/1000000: episode: 2484, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000806, mae: 0.016899, mean_q: 0.022435
  18517/1000000: episode: 2485, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000278, mae: 0.011895, mean_q: 0.015589
  18525/1000000: episode: 2486, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.030, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.000284, mae: 0.011101, mean_q: 0.014023
  18529/1000000: episode: 2487, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000493, mae: 0.013671, mean_q: 0.021660
  18533/1000000: episode: 2488, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000412, mae: 0.011462, mean_q: 0.015058
  18537/1000000: episode: 2489, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000698, mae: 0.013027, mean_q: 0.014262
  18545/1000000: episode: 2490, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.263, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.938 [-1.000, 11.000], loss: 0.000195, mae: 0.009637, mean_q: 0.011579
  18553/1000000: episode: 2491, duration: 0.037s, episode steps: 8, steps per second: 213, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.938 [-1.000, 11.000], loss: 0.000472, mae: 0.012649, mean_q: 0.016520
  18561/1000000: episode: 2492, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.177, mean reward: 0.022 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000493, mae: 0.012339, mean_q: 0.016486
  18565/1000000: episode: 2493, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.002835, mae: 0.024179, mean_q: 0.024395
  18569/1000000: episode: 2494, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000298, mae: 0.010227, mean_q: 0.013179
  18577/1000000: episode: 2495, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.000296, mae: 0.010184, mean_q: 0.012758
  18585/1000000: episode: 2496, duration: 0.038s, episode steps: 8, steps per second: 211, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.000764, mae: 0.014437, mean_q: 0.012716
  18589/1000000: episode: 2497, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000361, mae: 0.011729, mean_q: 0.014834
  18597/1000000: episode: 2498, duration: 0.037s, episode steps: 8, steps per second: 218, episode reward: 0.231, mean reward: 0.029 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000219, mae: 0.008418, mean_q: 0.012912
  18601/1000000: episode: 2499, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000273, mae: 0.009877, mean_q: 0.015166
  18605/1000000: episode: 2500, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000183, mae: 0.009111, mean_q: 0.009615
  18613/1000000: episode: 2501, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.938 [-1.000, 11.000], loss: 0.001474, mae: 0.015463, mean_q: 0.015422
  18617/1000000: episode: 2502, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000844, mae: 0.016562, mean_q: 0.023224
  18621/1000000: episode: 2503, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002621, mae: 0.023895, mean_q: 0.028349
  18629/1000000: episode: 2504, duration: 0.038s, episode steps: 8, steps per second: 208, episode reward: 0.081, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.001397, mae: 0.014726, mean_q: 0.019752
  18637/1000000: episode: 2505, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.038, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001172, mae: 0.013840, mean_q: 0.012622
  18645/1000000: episode: 2506, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.048, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000649, mae: 0.015575, mean_q: 0.018045
  18653/1000000: episode: 2507, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.177, mean reward: 0.022 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.000457, mae: 0.014532, mean_q: 0.014558
  18661/1000000: episode: 2508, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.032, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.000236, mae: 0.009388, mean_q: 0.012170
  18669/1000000: episode: 2509, duration: 0.037s, episode steps: 8, steps per second: 217, episode reward: 0.083, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.688 [-1.000, 11.000], loss: 0.000388, mae: 0.011468, mean_q: 0.012638
  18677/1000000: episode: 2510, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.030, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.000407, mae: 0.012522, mean_q: 0.012781
  18685/1000000: episode: 2511, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.875 [-1.000, 11.000], loss: 0.000286, mae: 0.009479, mean_q: 0.015494
  18689/1000000: episode: 2512, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000264, mae: 0.009387, mean_q: 0.015700
  18697/1000000: episode: 2513, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.030, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.000579, mae: 0.011641, mean_q: 0.015390
  18705/1000000: episode: 2514, duration: 0.038s, episode steps: 8, steps per second: 212, episode reward: 0.054, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.438 [-1.000, 11.000], loss: 0.000415, mae: 0.010889, mean_q: 0.014275
  18713/1000000: episode: 2515, duration: 0.037s, episode steps: 8, steps per second: 219, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.001226, mae: 0.013746, mean_q: 0.016189
  18721/1000000: episode: 2516, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.036, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [-1.000, 11.000], loss: 0.000451, mae: 0.011939, mean_q: 0.016322
  18729/1000000: episode: 2517, duration: 0.037s, episode steps: 8, steps per second: 218, episode reward: 0.231, mean reward: 0.029 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 5.875 [-1.000, 11.000], loss: 0.002877, mae: 0.019076, mean_q: 0.022308
  18737/1000000: episode: 2518, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.049, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.000655, mae: 0.016736, mean_q: 0.020069
  18741/1000000: episode: 2519, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000484, mae: 0.013936, mean_q: 0.012972
  18749/1000000: episode: 2520, duration: 0.038s, episode steps: 8, steps per second: 210, episode reward: 0.028, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.000702, mae: 0.019110, mean_q: 0.013996
  18757/1000000: episode: 2521, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.033, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.250 [-1.000, 11.000], loss: 0.000308, mae: 0.012634, mean_q: 0.014946
  18765/1000000: episode: 2522, duration: 0.037s, episode steps: 8, steps per second: 217, episode reward: 0.032, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.188 [-1.000, 11.000], loss: 0.000331, mae: 0.011607, mean_q: 0.014044
  18769/1000000: episode: 2523, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000143, mae: 0.008597, mean_q: 0.007780
  18773/1000000: episode: 2524, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000303, mae: 0.008952, mean_q: 0.007624
  18777/1000000: episode: 2525, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000378, mae: 0.013136, mean_q: 0.017402
  18781/1000000: episode: 2526, duration: 0.022s, episode steps: 4, steps per second: 186, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000266, mae: 0.012596, mean_q: 0.012651
  18789/1000000: episode: 2527, duration: 0.038s, episode steps: 8, steps per second: 208, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.125 [-1.000, 11.000], loss: 0.000421, mae: 0.017362, mean_q: 0.016140
  18797/1000000: episode: 2528, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.038, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.312 [-1.000, 11.000], loss: 0.001448, mae: 0.021093, mean_q: 0.016111
  18805/1000000: episode: 2529, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.128, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.625 [-1.000, 11.000], loss: 0.000510, mae: 0.015173, mean_q: 0.013088
  18813/1000000: episode: 2530, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.032, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.375 [-1.000, 11.000], loss: 0.000319, mae: 0.012890, mean_q: 0.012304
  18817/1000000: episode: 2531, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000351, mae: 0.014536, mean_q: 0.025649
  18825/1000000: episode: 2532, duration: 0.037s, episode steps: 8, steps per second: 214, episode reward: 0.082, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.002250, mae: 0.021267, mean_q: 0.021648
  18833/1000000: episode: 2533, duration: 0.037s, episode steps: 8, steps per second: 216, episode reward: 0.082, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000710, mae: 0.018719, mean_q: 0.016787
  18841/1000000: episode: 2534, duration: 0.038s, episode steps: 8, steps per second: 213, episode reward: 0.055, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000448, mae: 0.017854, mean_q: 0.005395
  18849/1000000: episode: 2535, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 0.114, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.750 [-1.000, 11.000], loss: 0.002119, mae: 0.025132, mean_q: 0.016784
  18853/1000000: episode: 2536, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000381, mae: 0.018710, mean_q: 0.019229
[Info] 2-TH LEVEL FOUND: 0.1321193128824234, Considering 17/100 traces
  18861/1000000: episode: 2537, duration: 0.694s, episode steps: 8, steps per second: 12, episode reward: 0.013, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.938 [-1.000, 11.000], loss: 0.000297, mae: 0.013069, mean_q: 0.011725
  18863/1000000: episode: 2538, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000316, mae: 0.015245, mean_q: 0.003478
  18865/1000000: episode: 2539, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000524, mae: 0.015133, mean_q: 0.013910
  18869/1000000: episode: 2540, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000077, mae: 0.007809, mean_q: 0.000737
  18871/1000000: episode: 2541, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000094, mae: 0.008454, mean_q: 0.010476
  18873/1000000: episode: 2542, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000202, mae: 0.007563, mean_q: 0.009592
  18875/1000000: episode: 2543, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000585, mae: 0.018342, mean_q: 0.019275
  18877/1000000: episode: 2544, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000636, mae: 0.015762, mean_q: 0.021852
  18881/1000000: episode: 2545, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 1.872, mean reward: 0.468 [0.135, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 9.250 [8.000, 11.000], loss: 0.000164, mae: 0.009009, mean_q: 0.011426
  18883/1000000: episode: 2546, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000136, mae: 0.007317, mean_q: 0.015432
  18885/1000000: episode: 2547, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000360, mae: 0.013431, mean_q: 0.017428
  18887/1000000: episode: 2548, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001094, mae: 0.015427, mean_q: 0.018009
[Info] FALSIFICATION!
  18890/1000000: episode: 2549, duration: 0.262s, episode steps: 3, steps per second: 11, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.006904, mae: 0.030356, mean_q: 0.025313
  18892/1000000: episode: 2550, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.004435, mae: 0.021178, mean_q: 0.022005
  18894/1000000: episode: 2551, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000637, mae: 0.023319, mean_q: 0.017243
  18896/1000000: episode: 2552, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001653, mae: 0.030760, mean_q: 0.022410
  18898/1000000: episode: 2553, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003871, mae: 0.024960, mean_q: 0.022667
  18900/1000000: episode: 2554, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002097, mae: 0.031553, mean_q: 0.026492
  18902/1000000: episode: 2555, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000410, mae: 0.022866, mean_q: -0.002858
  18906/1000000: episode: 2556, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000505, mae: 0.022076, mean_q: 0.009714
  18908/1000000: episode: 2557, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000552, mae: 0.019539, mean_q: 0.019270
  18910/1000000: episode: 2558, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000794, mae: 0.024888, mean_q: 0.003949
  18912/1000000: episode: 2559, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000277, mae: 0.017509, mean_q: 0.016002
  18914/1000000: episode: 2560, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000336, mae: 0.017365, mean_q: 0.018730
  18916/1000000: episode: 2561, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000082, mae: 0.010648, mean_q: -0.000066
  18918/1000000: episode: 2562, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000225, mae: 0.012497, mean_q: 0.014621
  18920/1000000: episode: 2563, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000448, mae: 0.013681, mean_q: 0.016505
  18922/1000000: episode: 2564, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000462, mae: 0.015803, mean_q: 0.009404
  18926/1000000: episode: 2565, duration: 0.021s, episode steps: 4, steps per second: 186, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000375, mae: 0.011802, mean_q: 0.020095
  18930/1000000: episode: 2566, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000454, mae: 0.012877, mean_q: 0.016679
  18932/1000000: episode: 2567, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000831, mae: 0.013683, mean_q: 0.020341
  18934/1000000: episode: 2568, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000100, mae: 0.008321, mean_q: 0.009530
  18936/1000000: episode: 2569, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000482, mae: 0.013253, mean_q: 0.010096
  18938/1000000: episode: 2570, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000160, mae: 0.009406, mean_q: 0.006239
  18940/1000000: episode: 2571, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000406, mae: 0.013731, mean_q: 0.013947
  18942/1000000: episode: 2572, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000362, mae: 0.012365, mean_q: 0.008767
  18944/1000000: episode: 2573, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000308, mae: 0.013644, mean_q: 0.008765
  18946/1000000: episode: 2574, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005963, mae: 0.026808, mean_q: 0.014921
  18948/1000000: episode: 2575, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000264, mae: 0.016493, mean_q: 0.024492
  18950/1000000: episode: 2576, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000624, mae: 0.020520, mean_q: 0.012267
  18952/1000000: episode: 2577, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001444, mae: 0.021313, mean_q: 0.030351
  18954/1000000: episode: 2578, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000798, mae: 0.016900, mean_q: 0.020045
  18956/1000000: episode: 2579, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003954, mae: 0.032112, mean_q: 0.012482
  18958/1000000: episode: 2580, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005350, mae: 0.028054, mean_q: 0.019678
  18960/1000000: episode: 2581, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000386, mae: 0.014229, mean_q: 0.016113
  18962/1000000: episode: 2582, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000688, mae: 0.019734, mean_q: 0.011052
  18964/1000000: episode: 2583, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000565, mae: 0.015060, mean_q: 0.013051
  18966/1000000: episode: 2584, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000115, mae: 0.011717, mean_q: 0.014511
  18968/1000000: episode: 2585, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000072, mae: 0.008354, mean_q: 0.006189
  18970/1000000: episode: 2586, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001000, mae: 0.014532, mean_q: 0.010945
  18974/1000000: episode: 2587, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000442, mae: 0.016636, mean_q: 0.023431
  18978/1000000: episode: 2588, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000596, mae: 0.013116, mean_q: 0.012378
  18980/1000000: episode: 2589, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000245, mae: 0.010864, mean_q: 0.013335
  18982/1000000: episode: 2590, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000438, mae: 0.015173, mean_q: 0.010255
  18984/1000000: episode: 2591, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001212, mae: 0.018626, mean_q: 0.017572
  18986/1000000: episode: 2592, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006166, mae: 0.025380, mean_q: 0.011639
  18990/1000000: episode: 2593, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.271, mean reward: 0.068 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000632, mae: 0.017774, mean_q: 0.024208
  18994/1000000: episode: 2594, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000448, mae: 0.014921, mean_q: 0.018780
[Info] FALSIFICATION!
  18997/1000000: episode: 2595, duration: 0.259s, episode steps: 3, steps per second: 12, episode reward: 1.503, mean reward: 0.501 [0.135, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 9.000 [8.000, 10.000], loss: 0.000479, mae: 0.014230, mean_q: 0.015747
  18999/1000000: episode: 2596, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000850, mae: 0.016530, mean_q: 0.012816
  19001/1000000: episode: 2597, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000313, mae: 0.011456, mean_q: 0.014812
  19003/1000000: episode: 2598, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006946, mae: 0.034995, mean_q: 0.023623
  19005/1000000: episode: 2599, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000604, mae: 0.017139, mean_q: 0.026687
  19009/1000000: episode: 2600, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002974, mae: 0.023735, mean_q: 0.026225
  19011/1000000: episode: 2601, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002070, mae: 0.029192, mean_q: 0.030173
  19013/1000000: episode: 2602, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000462, mae: 0.014206, mean_q: 0.009665
  19015/1000000: episode: 2603, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000692, mae: 0.019887, mean_q: 0.014538
  19017/1000000: episode: 2604, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000259, mae: 0.013002, mean_q: 0.005740
  19019/1000000: episode: 2605, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001211, mae: 0.019411, mean_q: 0.011567
  19021/1000000: episode: 2606, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000337, mae: 0.014934, mean_q: 0.016245
  19023/1000000: episode: 2607, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000356, mae: 0.011551, mean_q: 0.014160
  19025/1000000: episode: 2608, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000504, mae: 0.018398, mean_q: 0.026389
  19027/1000000: episode: 2609, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000532, mae: 0.018048, mean_q: 0.025225
  19029/1000000: episode: 2610, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005652, mae: 0.028927, mean_q: 0.012711
  19031/1000000: episode: 2611, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.004147, mae: 0.031302, mean_q: 0.034048
  19035/1000000: episode: 2612, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 1.240, mean reward: 0.310 [0.135, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 9.125 [8.000, 11.000], loss: 0.005011, mae: 0.031692, mean_q: 0.030220
  19037/1000000: episode: 2613, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000768, mae: 0.018637, mean_q: 0.035611
  19039/1000000: episode: 2614, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000779, mae: 0.018502, mean_q: 0.019460
  19041/1000000: episode: 2615, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000990, mae: 0.019923, mean_q: 0.012970
  19043/1000000: episode: 2616, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000304, mae: 0.012824, mean_q: 0.008491
  19045/1000000: episode: 2617, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003888, mae: 0.025130, mean_q: 0.023773
  19047/1000000: episode: 2618, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000804, mae: 0.019845, mean_q: 0.021812
  19049/1000000: episode: 2619, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000774, mae: 0.023476, mean_q: 0.020329
[Info] Complete ISplit Iteration
[Info] Levels: [0.026156545, 0.13211931, 0.35933965]
[Info] Cond. Prob: [0.14, 0.17, 0.04]
[Info] Error Prob: 0.0009520000000000003

  19051/1000000: episode: 2620, duration: 0.827s, episode steps: 2, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000361, mae: 0.014960, mean_q: 0.020413
  19061/1000000: episode: 2621, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002434, mae: 0.022035, mean_q: 0.016407
  19071/1000000: episode: 2622, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000654, mae: 0.015809, mean_q: 0.022105
  19081/1000000: episode: 2623, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001511, mae: 0.017410, mean_q: 0.012750
  19091/1000000: episode: 2624, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000793, mae: 0.018490, mean_q: 0.016767
  19101/1000000: episode: 2625, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000808, mae: 0.022288, mean_q: 0.020257
  19111/1000000: episode: 2626, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000589, mae: 0.017683, mean_q: 0.016030
  19121/1000000: episode: 2627, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000421, mae: 0.015238, mean_q: 0.016841
  19131/1000000: episode: 2628, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000723, mae: 0.017376, mean_q: 0.016208
  19141/1000000: episode: 2629, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001091, mae: 0.015928, mean_q: 0.018318
  19151/1000000: episode: 2630, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001578, mae: 0.019371, mean_q: 0.020612
  19161/1000000: episode: 2631, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.003094, mae: 0.025387, mean_q: 0.021857
  19171/1000000: episode: 2632, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001270, mae: 0.016990, mean_q: 0.018560
  19181/1000000: episode: 2633, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.002789, mae: 0.023482, mean_q: 0.020712
  19191/1000000: episode: 2634, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000635, mae: 0.017344, mean_q: 0.020281
  19201/1000000: episode: 2635, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001263, mae: 0.018893, mean_q: 0.013441
  19211/1000000: episode: 2636, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000770, mae: 0.018751, mean_q: 0.018968
  19221/1000000: episode: 2637, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000746, mae: 0.014538, mean_q: 0.015848
  19231/1000000: episode: 2638, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001240, mae: 0.017566, mean_q: 0.018336
  19241/1000000: episode: 2639, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001801, mae: 0.018381, mean_q: 0.019032
  19251/1000000: episode: 2640, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002420, mae: 0.021266, mean_q: 0.022856
  19261/1000000: episode: 2641, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001304, mae: 0.022273, mean_q: 0.028964
  19271/1000000: episode: 2642, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000742, mae: 0.018851, mean_q: 0.015799
  19281/1000000: episode: 2643, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000709, mae: 0.018153, mean_q: 0.017733
  19291/1000000: episode: 2644, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001414, mae: 0.017964, mean_q: 0.018169
  19301/1000000: episode: 2645, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000650, mae: 0.015852, mean_q: 0.020938
  19311/1000000: episode: 2646, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000483, mae: 0.014543, mean_q: 0.011854
  19321/1000000: episode: 2647, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000407, mae: 0.013138, mean_q: 0.014861
  19331/1000000: episode: 2648, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000538, mae: 0.014082, mean_q: 0.014492
  19341/1000000: episode: 2649, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000547, mae: 0.012772, mean_q: 0.017576
  19351/1000000: episode: 2650, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000653, mae: 0.016992, mean_q: 0.021817
  19361/1000000: episode: 2651, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000713, mae: 0.014098, mean_q: 0.015795
  19371/1000000: episode: 2652, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001061, mae: 0.014211, mean_q: 0.014519
  19381/1000000: episode: 2653, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002531, mae: 0.022698, mean_q: 0.020242
  19391/1000000: episode: 2654, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.100, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001819, mae: 0.023592, mean_q: 0.023045
  19401/1000000: episode: 2655, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001509, mae: 0.020186, mean_q: 0.009993
  19411/1000000: episode: 2656, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000428, mae: 0.014821, mean_q: 0.016581
  19421/1000000: episode: 2657, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000537, mae: 0.016867, mean_q: 0.015689
  19431/1000000: episode: 2658, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002452, mae: 0.021571, mean_q: 0.023543
  19441/1000000: episode: 2659, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001554, mae: 0.019311, mean_q: 0.015964
  19451/1000000: episode: 2660, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001060, mae: 0.015011, mean_q: 0.014779
  19461/1000000: episode: 2661, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000465, mae: 0.012661, mean_q: 0.013134
  19471/1000000: episode: 2662, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000543, mae: 0.014968, mean_q: 0.014475
  19481/1000000: episode: 2663, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002358, mae: 0.022005, mean_q: 0.024604
  19491/1000000: episode: 2664, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000654, mae: 0.021004, mean_q: 0.015478
  19501/1000000: episode: 2665, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000589, mae: 0.016288, mean_q: 0.016796
  19511/1000000: episode: 2666, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000574, mae: 0.016449, mean_q: 0.018035
  19521/1000000: episode: 2667, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001019, mae: 0.016647, mean_q: 0.017854
  19531/1000000: episode: 2668, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.100, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000436, mae: 0.013702, mean_q: 0.015170
  19541/1000000: episode: 2669, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000318, mae: 0.012284, mean_q: 0.013125
  19551/1000000: episode: 2670, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002467, mae: 0.020917, mean_q: 0.018572
  19561/1000000: episode: 2671, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000921, mae: 0.021088, mean_q: 0.020866
  19571/1000000: episode: 2672, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000354, mae: 0.013024, mean_q: 0.016600
  19581/1000000: episode: 2673, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001571, mae: 0.015978, mean_q: 0.015263
  19591/1000000: episode: 2674, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000383, mae: 0.012818, mean_q: 0.017066
  19601/1000000: episode: 2675, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000276, mae: 0.012698, mean_q: 0.009291
  19611/1000000: episode: 2676, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002925, mae: 0.021324, mean_q: 0.022169
  19621/1000000: episode: 2677, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000489, mae: 0.013668, mean_q: 0.016264
  19631/1000000: episode: 2678, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002416, mae: 0.022282, mean_q: 0.021398
  19641/1000000: episode: 2679, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000631, mae: 0.016883, mean_q: 0.015749
  19651/1000000: episode: 2680, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.002349, mae: 0.022848, mean_q: 0.020106
  19661/1000000: episode: 2681, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000560, mae: 0.016300, mean_q: 0.018989
  19671/1000000: episode: 2682, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.002114, mae: 0.021204, mean_q: 0.022432
  19681/1000000: episode: 2683, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000513, mae: 0.016432, mean_q: 0.016600
  19691/1000000: episode: 2684, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000760, mae: 0.014862, mean_q: 0.013181
  19701/1000000: episode: 2685, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001209, mae: 0.017847, mean_q: 0.019110
  19711/1000000: episode: 2686, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000401, mae: 0.015837, mean_q: 0.014038
  19721/1000000: episode: 2687, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000863, mae: 0.019207, mean_q: 0.023723
  19731/1000000: episode: 2688, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000260, mae: 0.010330, mean_q: 0.013184
  19741/1000000: episode: 2689, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000314, mae: 0.011249, mean_q: 0.014828
  19751/1000000: episode: 2690, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001563, mae: 0.015256, mean_q: 0.013857
  19761/1000000: episode: 2691, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.221, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.000503, mae: 0.014464, mean_q: 0.015850
  19771/1000000: episode: 2692, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000481, mae: 0.012170, mean_q: 0.013407
  19781/1000000: episode: 2693, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000364, mae: 0.012553, mean_q: 0.014532
  19791/1000000: episode: 2694, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001164, mae: 0.017144, mean_q: 0.019534
  19801/1000000: episode: 2695, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000424, mae: 0.014350, mean_q: 0.013213
  19811/1000000: episode: 2696, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002066, mae: 0.018035, mean_q: 0.018749
  19821/1000000: episode: 2697, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000447, mae: 0.014828, mean_q: 0.016393
  19831/1000000: episode: 2698, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000371, mae: 0.013610, mean_q: 0.014512
  19841/1000000: episode: 2699, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000321, mae: 0.012753, mean_q: 0.012381
  19851/1000000: episode: 2700, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001002, mae: 0.013260, mean_q: 0.015675
  19861/1000000: episode: 2701, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000426, mae: 0.016318, mean_q: 0.022945
  19871/1000000: episode: 2702, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001503, mae: 0.015225, mean_q: 0.014906
  19881/1000000: episode: 2703, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000427, mae: 0.013268, mean_q: 0.015778
  19891/1000000: episode: 2704, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000771, mae: 0.013947, mean_q: 0.011931
  19901/1000000: episode: 2705, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000732, mae: 0.016280, mean_q: 0.019508
  19911/1000000: episode: 2706, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001628, mae: 0.016120, mean_q: 0.017116
  19921/1000000: episode: 2707, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002289, mae: 0.019724, mean_q: 0.019368
  19931/1000000: episode: 2708, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.003992, mae: 0.029681, mean_q: 0.027178
  19941/1000000: episode: 2709, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000490, mae: 0.016572, mean_q: 0.010163
  19951/1000000: episode: 2710, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001219, mae: 0.020615, mean_q: 0.017789
  19961/1000000: episode: 2711, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000669, mae: 0.018467, mean_q: 0.015015
  19971/1000000: episode: 2712, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000415, mae: 0.016436, mean_q: 0.012868
  19981/1000000: episode: 2713, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000291, mae: 0.013104, mean_q: 0.014484
  19991/1000000: episode: 2714, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000376, mae: 0.011493, mean_q: 0.011697
  20001/1000000: episode: 2715, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000317, mae: 0.012000, mean_q: 0.015781
  20011/1000000: episode: 2716, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000382, mae: 0.011058, mean_q: 0.010298
  20021/1000000: episode: 2717, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001475, mae: 0.015228, mean_q: 0.019811
  20031/1000000: episode: 2718, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.042, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001762, mae: 0.018925, mean_q: 0.019094
  20041/1000000: episode: 2719, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001145, mae: 0.016573, mean_q: 0.017226
[Info] 1-TH LEVEL FOUND: 0.02612370252609253, Considering 11/100 traces
  20051/1000000: episode: 2720, duration: 0.690s, episode steps: 10, steps per second: 14, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001145, mae: 0.014779, mean_q: 0.016099
  20056/1000000: episode: 2721, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000271, mae: 0.012339, mean_q: 0.008669
  20063/1000000: episode: 2722, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.001851, mae: 0.015031, mean_q: 0.015938
  20068/1000000: episode: 2723, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000423, mae: 0.013231, mean_q: 0.015372
  20075/1000000: episode: 2724, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000507, mae: 0.012785, mean_q: 0.015937
  20080/1000000: episode: 2725, duration: 0.024s, episode steps: 5, steps per second: 205, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.001899, mae: 0.020123, mean_q: 0.024500
  20087/1000000: episode: 2726, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001600, mae: 0.018591, mean_q: 0.020836
  20094/1000000: episode: 2727, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.002009, mae: 0.019461, mean_q: 0.017206
  20101/1000000: episode: 2728, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000462, mae: 0.016521, mean_q: 0.021014
  20108/1000000: episode: 2729, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000400, mae: 0.015601, mean_q: 0.009278
  20115/1000000: episode: 2730, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000367, mae: 0.014340, mean_q: 0.016135
  20120/1000000: episode: 2731, duration: 0.024s, episode steps: 5, steps per second: 207, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.001941, mae: 0.019069, mean_q: 0.017350
  20127/1000000: episode: 2732, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.002114, mae: 0.020020, mean_q: 0.017684
  20134/1000000: episode: 2733, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000423, mae: 0.014002, mean_q: 0.018598
  20139/1000000: episode: 2734, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000292, mae: 0.010777, mean_q: 0.011826
  20144/1000000: episode: 2735, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000470, mae: 0.011682, mean_q: 0.008946
  20149/1000000: episode: 2736, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000667, mae: 0.016549, mean_q: 0.020049
  20156/1000000: episode: 2737, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000234, mae: 0.010621, mean_q: 0.010703
  20163/1000000: episode: 2738, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000632, mae: 0.013943, mean_q: 0.013373
  20168/1000000: episode: 2739, duration: 0.024s, episode steps: 5, steps per second: 205, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000306, mae: 0.012238, mean_q: 0.016298
  20175/1000000: episode: 2740, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000586, mae: 0.015287, mean_q: 0.019622
  20180/1000000: episode: 2741, duration: 0.024s, episode steps: 5, steps per second: 209, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.001682, mae: 0.019260, mean_q: 0.020791
  20185/1000000: episode: 2742, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000337, mae: 0.014045, mean_q: 0.009078
  20190/1000000: episode: 2743, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.002642, mae: 0.023022, mean_q: 0.021429
  20197/1000000: episode: 2744, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001214, mae: 0.014474, mean_q: 0.017556
  20204/1000000: episode: 2745, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000454, mae: 0.015166, mean_q: 0.019826
  20211/1000000: episode: 2746, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001550, mae: 0.018383, mean_q: 0.019701
  20218/1000000: episode: 2747, duration: 0.032s, episode steps: 7, steps per second: 215, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000533, mae: 0.015904, mean_q: 0.013681
  20225/1000000: episode: 2748, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000735, mae: 0.016121, mean_q: 0.013437
  20232/1000000: episode: 2749, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000393, mae: 0.015671, mean_q: 0.016296
  20239/1000000: episode: 2750, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000408, mae: 0.015569, mean_q: 0.017092
  20246/1000000: episode: 2751, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000618, mae: 0.019970, mean_q: 0.024971
  20251/1000000: episode: 2752, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000406, mae: 0.015020, mean_q: 0.019360
  20256/1000000: episode: 2753, duration: 0.025s, episode steps: 5, steps per second: 204, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000934, mae: 0.020820, mean_q: 0.009454
  20261/1000000: episode: 2754, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001231, mae: 0.019825, mean_q: 0.018988
  20268/1000000: episode: 2755, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000370, mae: 0.014193, mean_q: 0.018049
  20275/1000000: episode: 2756, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.003565, mae: 0.021962, mean_q: 0.021220
[Info] FALSIFICATION!
  20281/1000000: episode: 2757, duration: 0.259s, episode steps: 6, steps per second: 23, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000473, mae: 0.015318, mean_q: 0.020333
  20286/1000000: episode: 2758, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000686, mae: 0.019849, mean_q: 0.012405
  20291/1000000: episode: 2759, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001818, mae: 0.020227, mean_q: 0.029550
  20298/1000000: episode: 2760, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000744, mae: 0.020110, mean_q: 0.021949
  20303/1000000: episode: 2761, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000310, mae: 0.013294, mean_q: 0.015937
  20308/1000000: episode: 2762, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.004166, mae: 0.021880, mean_q: 0.017038
  20313/1000000: episode: 2763, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001726, mae: 0.019423, mean_q: 0.018668
  20320/1000000: episode: 2764, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001095, mae: 0.021242, mean_q: 0.028616
  20327/1000000: episode: 2765, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000897, mae: 0.020293, mean_q: 0.019979
  20334/1000000: episode: 2766, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000348, mae: 0.015432, mean_q: 0.016414
  20341/1000000: episode: 2767, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.001506, mae: 0.017202, mean_q: 0.013787
  20346/1000000: episode: 2768, duration: 0.025s, episode steps: 5, steps per second: 204, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000788, mae: 0.013831, mean_q: 0.015310
  20353/1000000: episode: 2769, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000685, mae: 0.015431, mean_q: 0.021969
  20360/1000000: episode: 2770, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000374, mae: 0.013663, mean_q: 0.018499
  20367/1000000: episode: 2771, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000685, mae: 0.016365, mean_q: 0.014872
  20374/1000000: episode: 2772, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000292, mae: 0.012360, mean_q: 0.013156
  20381/1000000: episode: 2773, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000457, mae: 0.012025, mean_q: 0.013654
  20388/1000000: episode: 2774, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.001722, mae: 0.018202, mean_q: 0.020166
  20395/1000000: episode: 2775, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000718, mae: 0.015431, mean_q: 0.022215
  20400/1000000: episode: 2776, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001695, mae: 0.018455, mean_q: 0.016557
  20405/1000000: episode: 2777, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.000483, mae: 0.016364, mean_q: 0.024671
  20412/1000000: episode: 2778, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000499, mae: 0.013630, mean_q: 0.014935
  20417/1000000: episode: 2779, duration: 0.025s, episode steps: 5, steps per second: 202, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000766, mae: 0.020102, mean_q: 0.019096
  20424/1000000: episode: 2780, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.002243, mae: 0.022409, mean_q: 0.024175
  20429/1000000: episode: 2781, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000738, mae: 0.017553, mean_q: 0.025516
  20436/1000000: episode: 2782, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000299, mae: 0.013948, mean_q: 0.008671
  20443/1000000: episode: 2783, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.002192, mae: 0.018148, mean_q: 0.018523
  20448/1000000: episode: 2784, duration: 0.025s, episode steps: 5, steps per second: 198, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001960, mae: 0.021172, mean_q: 0.027829
  20455/1000000: episode: 2785, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000439, mae: 0.012654, mean_q: 0.017285
  20460/1000000: episode: 2786, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.020, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.800 [-1.000, 11.000], loss: 0.005305, mae: 0.030650, mean_q: 0.027642
  20467/1000000: episode: 2787, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.001044, mae: 0.020962, mean_q: 0.027058
  20474/1000000: episode: 2788, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.001368, mae: 0.019057, mean_q: 0.021228
  20481/1000000: episode: 2789, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000684, mae: 0.017013, mean_q: 0.022594
  20488/1000000: episode: 2790, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001302, mae: 0.013948, mean_q: 0.017057
  20493/1000000: episode: 2791, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000518, mae: 0.014318, mean_q: 0.015186
  20500/1000000: episode: 2792, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001538, mae: 0.017834, mean_q: 0.015272
  20507/1000000: episode: 2793, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000775, mae: 0.019451, mean_q: 0.023926
  20512/1000000: episode: 2794, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.003926, mae: 0.021181, mean_q: 0.020813
  20517/1000000: episode: 2795, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000643, mae: 0.017507, mean_q: 0.021552
  20524/1000000: episode: 2796, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.001462, mae: 0.019110, mean_q: 0.017256
  20531/1000000: episode: 2797, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000280, mae: 0.011783, mean_q: 0.012502
  20538/1000000: episode: 2798, duration: 0.032s, episode steps: 7, steps per second: 217, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000568, mae: 0.016989, mean_q: 0.015346
  20545/1000000: episode: 2799, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000315, mae: 0.014228, mean_q: 0.014554
  20552/1000000: episode: 2800, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000244, mae: 0.011506, mean_q: 0.012515
  20559/1000000: episode: 2801, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000661, mae: 0.014158, mean_q: 0.016298
  20566/1000000: episode: 2802, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000276, mae: 0.011463, mean_q: 0.010871
  20571/1000000: episode: 2803, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000207, mae: 0.010610, mean_q: 0.013228
  20576/1000000: episode: 2804, duration: 0.025s, episode steps: 5, steps per second: 204, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000421, mae: 0.015693, mean_q: 0.019678
  20581/1000000: episode: 2805, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 0.075, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000534, mae: 0.014678, mean_q: 0.015049
  20588/1000000: episode: 2806, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.002182, mae: 0.020146, mean_q: 0.022156
  20593/1000000: episode: 2807, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 0.043, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000814, mae: 0.021080, mean_q: 0.020075
  20600/1000000: episode: 2808, duration: 0.032s, episode steps: 7, steps per second: 219, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001449, mae: 0.019454, mean_q: 0.023048
[Info] Complete ISplit Iteration
[Info] Levels: [0.026123703, 0.37237206]
[Info] Cond. Prob: [0.11, 0.01]
[Info] Error Prob: 0.0011

  20605/1000000: episode: 2809, duration: 0.811s, episode steps: 5, steps per second: 6, episode reward: 0.032, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.900 [-1.000, 11.000], loss: 0.000328, mae: 0.013920, mean_q: 0.014834
  20615/1000000: episode: 2810, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002010, mae: 0.023207, mean_q: 0.023936
  20625/1000000: episode: 2811, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001652, mae: 0.020978, mean_q: 0.023143
  20635/1000000: episode: 2812, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000622, mae: 0.020823, mean_q: 0.020636
  20645/1000000: episode: 2813, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000507, mae: 0.016098, mean_q: 0.014814
  20655/1000000: episode: 2814, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001064, mae: 0.015521, mean_q: 0.016707
  20665/1000000: episode: 2815, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001493, mae: 0.016001, mean_q: 0.014756
  20675/1000000: episode: 2816, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000316, mae: 0.012602, mean_q: 0.016314
  20685/1000000: episode: 2817, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000287, mae: 0.012397, mean_q: 0.010872
  20695/1000000: episode: 2818, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000371, mae: 0.014109, mean_q: 0.014319
  20705/1000000: episode: 2819, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000302, mae: 0.012257, mean_q: 0.014768
  20715/1000000: episode: 2820, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000436, mae: 0.013923, mean_q: 0.018454
  20725/1000000: episode: 2821, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000315, mae: 0.013635, mean_q: 0.014712
  20735/1000000: episode: 2822, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001179, mae: 0.015262, mean_q: 0.016200
  20745/1000000: episode: 2823, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001157, mae: 0.017319, mean_q: 0.021200
  20755/1000000: episode: 2824, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.029, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000368, mae: 0.013808, mean_q: 0.015139
  20765/1000000: episode: 2825, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000468, mae: 0.014959, mean_q: 0.017320
  20775/1000000: episode: 2826, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001790, mae: 0.021189, mean_q: 0.018525
  20785/1000000: episode: 2827, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001140, mae: 0.017981, mean_q: 0.016215
  20795/1000000: episode: 2828, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000464, mae: 0.016785, mean_q: 0.021329
  20805/1000000: episode: 2829, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.002594, mae: 0.025693, mean_q: 0.027301
  20815/1000000: episode: 2830, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000614, mae: 0.016842, mean_q: 0.016787
  20825/1000000: episode: 2831, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002547, mae: 0.021181, mean_q: 0.018994
  20835/1000000: episode: 2832, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000745, mae: 0.020283, mean_q: 0.022741
  20845/1000000: episode: 2833, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001268, mae: 0.020898, mean_q: 0.022502
  20855/1000000: episode: 2834, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.004400, mae: 0.028799, mean_q: 0.026833
  20865/1000000: episode: 2835, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.002635, mae: 0.027787, mean_q: 0.024408
  20875/1000000: episode: 2836, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001212, mae: 0.028038, mean_q: 0.017468
  20885/1000000: episode: 2837, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000567, mae: 0.019572, mean_q: 0.016738
  20895/1000000: episode: 2838, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000495, mae: 0.015048, mean_q: 0.017491
  20905/1000000: episode: 2839, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000446, mae: 0.014882, mean_q: 0.015052
  20915/1000000: episode: 2840, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000560, mae: 0.017689, mean_q: 0.018344
  20925/1000000: episode: 2841, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000699, mae: 0.018257, mean_q: 0.020193
  20935/1000000: episode: 2842, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001614, mae: 0.021654, mean_q: 0.022267
  20945/1000000: episode: 2843, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000615, mae: 0.021833, mean_q: 0.020804
  20955/1000000: episode: 2844, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000560, mae: 0.022420, mean_q: 0.018901
  20965/1000000: episode: 2845, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000523, mae: 0.019983, mean_q: 0.016802
  20975/1000000: episode: 2846, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001330, mae: 0.019177, mean_q: 0.020596
  20985/1000000: episode: 2847, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001054, mae: 0.021435, mean_q: 0.022212
  20995/1000000: episode: 2848, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000978, mae: 0.015962, mean_q: 0.015610
  21005/1000000: episode: 2849, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000800, mae: 0.021203, mean_q: 0.018820
  21015/1000000: episode: 2850, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001657, mae: 0.018804, mean_q: 0.019179
  21025/1000000: episode: 2851, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001451, mae: 0.017597, mean_q: 0.025212
  21035/1000000: episode: 2852, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001260, mae: 0.018313, mean_q: 0.025181
  21045/1000000: episode: 2853, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000403, mae: 0.014672, mean_q: 0.017436
  21055/1000000: episode: 2854, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000292, mae: 0.011971, mean_q: 0.015917
  21065/1000000: episode: 2855, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000337, mae: 0.013355, mean_q: 0.014583
  21075/1000000: episode: 2856, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001068, mae: 0.016694, mean_q: 0.019707
  21085/1000000: episode: 2857, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000497, mae: 0.016327, mean_q: 0.019992
  21095/1000000: episode: 2858, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000510, mae: 0.014657, mean_q: 0.018728
  21105/1000000: episode: 2859, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000531, mae: 0.016531, mean_q: 0.015643
  21115/1000000: episode: 2860, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000669, mae: 0.018159, mean_q: 0.018345
  21125/1000000: episode: 2861, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000372, mae: 0.016354, mean_q: 0.016190
  21135/1000000: episode: 2862, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001554, mae: 0.017177, mean_q: 0.018935
  21145/1000000: episode: 2863, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001107, mae: 0.018074, mean_q: 0.019205
  21155/1000000: episode: 2864, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000634, mae: 0.017779, mean_q: 0.023310
  21165/1000000: episode: 2865, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001161, mae: 0.020032, mean_q: 0.019940
  21175/1000000: episode: 2866, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000472, mae: 0.017714, mean_q: 0.022842
  21185/1000000: episode: 2867, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000437, mae: 0.016143, mean_q: 0.012614
  21195/1000000: episode: 2868, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000534, mae: 0.017263, mean_q: 0.017027
  21205/1000000: episode: 2869, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001975, mae: 0.022317, mean_q: 0.020600
  21215/1000000: episode: 2870, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001191, mae: 0.021965, mean_q: 0.023378
  21225/1000000: episode: 2871, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001458, mae: 0.019435, mean_q: 0.014232
  21235/1000000: episode: 2872, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001715, mae: 0.020279, mean_q: 0.023501
  21245/1000000: episode: 2873, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000493, mae: 0.015649, mean_q: 0.018378
  21255/1000000: episode: 2874, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001109, mae: 0.016873, mean_q: 0.017698
  21265/1000000: episode: 2875, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001501, mae: 0.017224, mean_q: 0.020303
  21275/1000000: episode: 2876, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.002623, mae: 0.020354, mean_q: 0.021244
  21285/1000000: episode: 2877, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001143, mae: 0.018131, mean_q: 0.022667
  21295/1000000: episode: 2878, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000336, mae: 0.013565, mean_q: 0.012020
  21305/1000000: episode: 2879, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000618, mae: 0.015662, mean_q: 0.020571
  21315/1000000: episode: 2880, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000591, mae: 0.017398, mean_q: 0.017587
  21325/1000000: episode: 2881, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000520, mae: 0.016465, mean_q: 0.020341
  21335/1000000: episode: 2882, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000562, mae: 0.014041, mean_q: 0.014204
  21345/1000000: episode: 2883, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000411, mae: 0.013346, mean_q: 0.013776
  21355/1000000: episode: 2884, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001115, mae: 0.018331, mean_q: 0.022636
  21365/1000000: episode: 2885, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000306, mae: 0.012952, mean_q: 0.010438
  21375/1000000: episode: 2886, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000417, mae: 0.014000, mean_q: 0.013768
  21385/1000000: episode: 2887, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000593, mae: 0.015215, mean_q: 0.017157
  21395/1000000: episode: 2888, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.002661, mae: 0.019340, mean_q: 0.015804
  21405/1000000: episode: 2889, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000570, mae: 0.018233, mean_q: 0.020776
  21415/1000000: episode: 2890, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000787, mae: 0.018572, mean_q: 0.015758
  21425/1000000: episode: 2891, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000406, mae: 0.013688, mean_q: 0.015940
  21435/1000000: episode: 2892, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001096, mae: 0.015324, mean_q: 0.011364
  21445/1000000: episode: 2893, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.002001, mae: 0.018291, mean_q: 0.018298
  21455/1000000: episode: 2894, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000475, mae: 0.015525, mean_q: 0.015854
  21465/1000000: episode: 2895, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000426, mae: 0.012801, mean_q: 0.011712
  21475/1000000: episode: 2896, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000395, mae: 0.012359, mean_q: 0.012207
  21485/1000000: episode: 2897, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001639, mae: 0.015748, mean_q: 0.018053
  21495/1000000: episode: 2898, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000582, mae: 0.016335, mean_q: 0.017009
  21505/1000000: episode: 2899, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000805, mae: 0.011475, mean_q: 0.015349
  21515/1000000: episode: 2900, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000305, mae: 0.012154, mean_q: 0.015003
  21525/1000000: episode: 2901, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000877, mae: 0.012044, mean_q: 0.011456
  21535/1000000: episode: 2902, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001380, mae: 0.016928, mean_q: 0.017711
  21545/1000000: episode: 2903, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000506, mae: 0.015983, mean_q: 0.016828
  21555/1000000: episode: 2904, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001656, mae: 0.017552, mean_q: 0.016039
  21565/1000000: episode: 2905, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.002108, mae: 0.017623, mean_q: 0.015829
  21575/1000000: episode: 2906, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002131, mae: 0.020014, mean_q: 0.022500
  21585/1000000: episode: 2907, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000981, mae: 0.016259, mean_q: 0.018751
  21595/1000000: episode: 2908, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000367, mae: 0.014269, mean_q: 0.012292
[Info] 1-TH LEVEL FOUND: 0.029419898986816406, Considering 13/100 traces
  21605/1000000: episode: 2909, duration: 0.685s, episode steps: 10, steps per second: 15, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001694, mae: 0.018050, mean_q: 0.014732
  21607/1000000: episode: 2910, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000260, mae: 0.012334, mean_q: 0.015449
[Info] FALSIFICATION!
  21613/1000000: episode: 2911, duration: 0.262s, episode steps: 6, steps per second: 23, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000381, mae: 0.015415, mean_q: 0.012530
  21617/1000000: episode: 2912, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000350, mae: 0.013651, mean_q: 0.018407
  21619/1000000: episode: 2913, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000944, mae: 0.015315, mean_q: 0.015383
  21626/1000000: episode: 2914, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000154, mae: 0.008603, mean_q: 0.009904
  21628/1000000: episode: 2915, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000254, mae: 0.011250, mean_q: 0.010358
  21635/1000000: episode: 2916, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000323, mae: 0.010187, mean_q: 0.009129
  21637/1000000: episode: 2917, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000646, mae: 0.017001, mean_q: 0.010059
  21644/1000000: episode: 2918, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000580, mae: 0.012407, mean_q: 0.015256
  21651/1000000: episode: 2919, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.001175, mae: 0.016302, mean_q: 0.017876
  21653/1000000: episode: 2920, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000742, mae: 0.013835, mean_q: 0.011149
  21655/1000000: episode: 2921, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000240, mae: 0.009301, mean_q: 0.006366
  21659/1000000: episode: 2922, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000862, mae: 0.015011, mean_q: 0.016206
  21666/1000000: episode: 2923, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000101, mae: 0.006517, mean_q: 0.006119
  21673/1000000: episode: 2924, duration: 0.032s, episode steps: 7, steps per second: 217, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000175, mae: 0.008965, mean_q: 0.010174
  21677/1000000: episode: 2925, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000276, mae: 0.011721, mean_q: 0.013597
  21679/1000000: episode: 2926, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000166, mae: 0.009562, mean_q: 0.011454
  21686/1000000: episode: 2927, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000389, mae: 0.011043, mean_q: 0.012695
  21688/1000000: episode: 2928, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000331, mae: 0.010691, mean_q: 0.012800
  21690/1000000: episode: 2929, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000349, mae: 0.011497, mean_q: 0.015070
  21697/1000000: episode: 2930, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000253, mae: 0.011568, mean_q: 0.012281
  21699/1000000: episode: 2931, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000400, mae: 0.013511, mean_q: 0.019798
  21701/1000000: episode: 2932, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000148, mae: 0.009009, mean_q: 0.014722
  21705/1000000: episode: 2933, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000224, mae: 0.009976, mean_q: 0.004683
  21712/1000000: episode: 2934, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000706, mae: 0.014230, mean_q: 0.013895
  21714/1000000: episode: 2935, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.006042, mae: 0.028788, mean_q: 0.025607
  21716/1000000: episode: 2936, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000460, mae: 0.015699, mean_q: 0.016916
  21723/1000000: episode: 2937, duration: 0.032s, episode steps: 7, steps per second: 217, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.001460, mae: 0.018024, mean_q: 0.019719
  21730/1000000: episode: 2938, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000453, mae: 0.016262, mean_q: 0.011801
  21732/1000000: episode: 2939, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000458, mae: 0.014387, mean_q: 0.016905
  21734/1000000: episode: 2940, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000212, mae: 0.011097, mean_q: 0.015694
  21736/1000000: episode: 2941, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000226, mae: 0.011931, mean_q: 0.009957
  21738/1000000: episode: 2942, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000427, mae: 0.013055, mean_q: 0.015568
  21740/1000000: episode: 2943, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000198, mae: 0.010114, mean_q: 0.010072
  21747/1000000: episode: 2944, duration: 0.035s, episode steps: 7, steps per second: 203, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000406, mae: 0.013137, mean_q: 0.012001
  21749/1000000: episode: 2945, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000235, mae: 0.010094, mean_q: 0.007142
  21756/1000000: episode: 2946, duration: 0.035s, episode steps: 7, steps per second: 202, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000202, mae: 0.009905, mean_q: 0.012636
  21763/1000000: episode: 2947, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000293, mae: 0.010077, mean_q: 0.008133
  21765/1000000: episode: 2948, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000112, mae: 0.007960, mean_q: 0.010541
  21772/1000000: episode: 2949, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000213, mae: 0.009873, mean_q: 0.010628
  21774/1000000: episode: 2950, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000270, mae: 0.013781, mean_q: 0.022761
  21776/1000000: episode: 2951, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000285, mae: 0.012686, mean_q: 0.016536
  21780/1000000: episode: 2952, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000370, mae: 0.013527, mean_q: 0.011043
  21782/1000000: episode: 2953, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.003053, mae: 0.016026, mean_q: 0.016093
  21786/1000000: episode: 2954, duration: 0.020s, episode steps: 4, steps per second: 199, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000102, mae: 0.007148, mean_q: 0.006213
  21793/1000000: episode: 2955, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000183, mae: 0.007912, mean_q: 0.010119
  21795/1000000: episode: 2956, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000238, mae: 0.009954, mean_q: 0.011669
  21802/1000000: episode: 2957, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000281, mae: 0.009190, mean_q: 0.014830
  21806/1000000: episode: 2958, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000271, mae: 0.010588, mean_q: 0.014320
  21813/1000000: episode: 2959, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.001404, mae: 0.014842, mean_q: 0.013147
  21820/1000000: episode: 2960, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000289, mae: 0.012222, mean_q: 0.013864
  21827/1000000: episode: 2961, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.001390, mae: 0.017658, mean_q: 0.019252
  21829/1000000: episode: 2962, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000634, mae: 0.011623, mean_q: 0.013877
  21831/1000000: episode: 2963, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000791, mae: 0.017128, mean_q: 0.024216
  21838/1000000: episode: 2964, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.001228, mae: 0.016342, mean_q: 0.016127
  21845/1000000: episode: 2965, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.001252, mae: 0.015847, mean_q: 0.018402
  21849/1000000: episode: 2966, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000578, mae: 0.015930, mean_q: 0.012905
  21856/1000000: episode: 2967, duration: 0.035s, episode steps: 7, steps per second: 202, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000155, mae: 0.008696, mean_q: 0.007348
  21858/1000000: episode: 2968, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000190, mae: 0.009244, mean_q: 0.009733
  21862/1000000: episode: 2969, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000129, mae: 0.008924, mean_q: 0.010603
  21869/1000000: episode: 2970, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000143, mae: 0.008601, mean_q: 0.010281
  21876/1000000: episode: 2971, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001259, mae: 0.014393, mean_q: 0.014626
  21883/1000000: episode: 2972, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000284, mae: 0.011051, mean_q: 0.012700
  21887/1000000: episode: 2973, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000295, mae: 0.010592, mean_q: 0.010306
  21889/1000000: episode: 2974, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000321, mae: 0.010061, mean_q: 0.013667
  21896/1000000: episode: 2975, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000222, mae: 0.008799, mean_q: 0.012117
  21903/1000000: episode: 2976, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000609, mae: 0.012905, mean_q: 0.014658
  21905/1000000: episode: 2977, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000319, mae: 0.010760, mean_q: 0.016504
  21907/1000000: episode: 2978, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000119, mae: 0.007811, mean_q: 0.010810
  21911/1000000: episode: 2979, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000148, mae: 0.011185, mean_q: 0.006320
  21918/1000000: episode: 2980, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000223, mae: 0.010322, mean_q: 0.010888
  21925/1000000: episode: 2981, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.001358, mae: 0.014355, mean_q: 0.015252
  21927/1000000: episode: 2982, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000067, mae: 0.007904, mean_q: 0.011854
  21934/1000000: episode: 2983, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000303, mae: 0.012480, mean_q: 0.011529
  21938/1000000: episode: 2984, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000251, mae: 0.009855, mean_q: 0.011420
  21940/1000000: episode: 2985, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000332, mae: 0.011639, mean_q: 0.012248
  21947/1000000: episode: 2986, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.002880, mae: 0.019937, mean_q: 0.021726
  21954/1000000: episode: 2987, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.001074, mae: 0.014600, mean_q: 0.015126
  21956/1000000: episode: 2988, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000214, mae: 0.011452, mean_q: 0.014744
  21958/1000000: episode: 2989, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000344, mae: 0.012322, mean_q: 0.003013
  21965/1000000: episode: 2990, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000555, mae: 0.016100, mean_q: 0.019627
  21967/1000000: episode: 2991, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000199, mae: 0.011668, mean_q: 0.016955
  21969/1000000: episode: 2992, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000381, mae: 0.012868, mean_q: 0.014348
  21971/1000000: episode: 2993, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000323, mae: 0.013260, mean_q: 0.012357
  21978/1000000: episode: 2994, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000179, mae: 0.010476, mean_q: 0.007695
  21985/1000000: episode: 2995, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.002107, mae: 0.015674, mean_q: 0.012365
[Info] Complete ISplit Iteration
[Info] Levels: [0.029419899, 0.39080927]
[Info] Cond. Prob: [0.13, 0.01]
[Info] Error Prob: 0.0013000000000000002

  21989/1000000: episode: 2996, duration: 0.735s, episode steps: 4, steps per second: 5, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000289, mae: 0.012878, mean_q: 0.018827
  21999/1000000: episode: 2997, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001929, mae: 0.016791, mean_q: 0.019195
  22009/1000000: episode: 2998, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001504, mae: 0.018724, mean_q: 0.014105
  22019/1000000: episode: 2999, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000349, mae: 0.011076, mean_q: 0.017928
  22029/1000000: episode: 3000, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000460, mae: 0.013699, mean_q: 0.013693
  22039/1000000: episode: 3001, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000192, mae: 0.010210, mean_q: 0.011421
  22049/1000000: episode: 3002, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000372, mae: 0.014402, mean_q: 0.011343
  22059/1000000: episode: 3003, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001814, mae: 0.017041, mean_q: 0.016711
  22069/1000000: episode: 3004, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001134, mae: 0.023270, mean_q: 0.017989
  22079/1000000: episode: 3005, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000378, mae: 0.016396, mean_q: 0.012096
  22089/1000000: episode: 3006, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.002737, mae: 0.022511, mean_q: 0.019140
  22099/1000000: episode: 3007, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000337, mae: 0.012880, mean_q: 0.014849
  22109/1000000: episode: 3008, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000929, mae: 0.013567, mean_q: 0.014045
  22119/1000000: episode: 3009, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000361, mae: 0.012992, mean_q: 0.015649
  22129/1000000: episode: 3010, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001520, mae: 0.016637, mean_q: 0.020046
  22139/1000000: episode: 3011, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002671, mae: 0.023453, mean_q: 0.022869
  22149/1000000: episode: 3012, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000429, mae: 0.013839, mean_q: 0.016872
  22159/1000000: episode: 3013, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000384, mae: 0.012684, mean_q: 0.014459
  22169/1000000: episode: 3014, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001405, mae: 0.016740, mean_q: 0.017797
  22179/1000000: episode: 3015, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000482, mae: 0.016257, mean_q: 0.014509
  22189/1000000: episode: 3016, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000546, mae: 0.015927, mean_q: 0.018634
  22199/1000000: episode: 3017, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000507, mae: 0.015476, mean_q: 0.013733
  22209/1000000: episode: 3018, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000910, mae: 0.014566, mean_q: 0.015868
  22219/1000000: episode: 3019, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000420, mae: 0.017259, mean_q: 0.017978
  22229/1000000: episode: 3020, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000251, mae: 0.012503, mean_q: 0.013407
  22239/1000000: episode: 3021, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.002077, mae: 0.019660, mean_q: 0.019765
  22249/1000000: episode: 3022, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000293, mae: 0.012199, mean_q: 0.012434
  22259/1000000: episode: 3023, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.057, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001130, mae: 0.014557, mean_q: 0.013991
  22269/1000000: episode: 3024, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001325, mae: 0.014936, mean_q: 0.015818
  22279/1000000: episode: 3025, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000462, mae: 0.013813, mean_q: 0.012370
  22289/1000000: episode: 3026, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.088, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001033, mae: 0.016955, mean_q: 0.012476
  22299/1000000: episode: 3027, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000433, mae: 0.013658, mean_q: 0.011627
  22309/1000000: episode: 3028, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000266, mae: 0.012805, mean_q: 0.014277
  22319/1000000: episode: 3029, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000367, mae: 0.014346, mean_q: 0.012228
  22329/1000000: episode: 3030, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.003266, mae: 0.023028, mean_q: 0.025626
  22339/1000000: episode: 3031, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000991, mae: 0.016378, mean_q: 0.014017
  22349/1000000: episode: 3032, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000535, mae: 0.015746, mean_q: 0.014136
  22359/1000000: episode: 3033, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001369, mae: 0.013100, mean_q: 0.011524
  22369/1000000: episode: 3034, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000635, mae: 0.016920, mean_q: 0.019714
  22379/1000000: episode: 3035, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000499, mae: 0.015979, mean_q: 0.013231
  22389/1000000: episode: 3036, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000350, mae: 0.016068, mean_q: 0.015683
  22399/1000000: episode: 3037, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000410, mae: 0.013816, mean_q: 0.016685
  22409/1000000: episode: 3038, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001547, mae: 0.015695, mean_q: 0.021360
  22419/1000000: episode: 3039, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000721, mae: 0.017182, mean_q: 0.013424
  22429/1000000: episode: 3040, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000459, mae: 0.017513, mean_q: 0.018391
  22439/1000000: episode: 3041, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000401, mae: 0.014106, mean_q: 0.014495
  22449/1000000: episode: 3042, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001062, mae: 0.016414, mean_q: 0.018406
  22459/1000000: episode: 3043, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001617, mae: 0.018429, mean_q: 0.018084
  22469/1000000: episode: 3044, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000421, mae: 0.015704, mean_q: 0.013369
  22479/1000000: episode: 3045, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001004, mae: 0.016748, mean_q: 0.018969
  22489/1000000: episode: 3046, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001548, mae: 0.016505, mean_q: 0.019504
  22499/1000000: episode: 3047, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000371, mae: 0.010792, mean_q: 0.011451
  22509/1000000: episode: 3048, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000310, mae: 0.012898, mean_q: 0.015796
  22519/1000000: episode: 3049, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001494, mae: 0.015942, mean_q: 0.016849
  22529/1000000: episode: 3050, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001052, mae: 0.015740, mean_q: 0.014182
  22539/1000000: episode: 3051, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000513, mae: 0.013795, mean_q: 0.015260
  22549/1000000: episode: 3052, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000437, mae: 0.014094, mean_q: 0.013707
  22559/1000000: episode: 3053, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000692, mae: 0.015282, mean_q: 0.017082
  22569/1000000: episode: 3054, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001534, mae: 0.016499, mean_q: 0.019069
  22579/1000000: episode: 3055, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000400, mae: 0.014952, mean_q: 0.017897
  22589/1000000: episode: 3056, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000233, mae: 0.011483, mean_q: 0.012803
  22599/1000000: episode: 3057, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000346, mae: 0.013542, mean_q: 0.014628
  22609/1000000: episode: 3058, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000347, mae: 0.014536, mean_q: 0.012935
  22619/1000000: episode: 3059, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001071, mae: 0.013561, mean_q: 0.011875
  22629/1000000: episode: 3060, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001450, mae: 0.017264, mean_q: 0.016855
  22639/1000000: episode: 3061, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000454, mae: 0.013805, mean_q: 0.014260
  22649/1000000: episode: 3062, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001684, mae: 0.017070, mean_q: 0.017313
  22659/1000000: episode: 3063, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000301, mae: 0.012603, mean_q: 0.013343
  22669/1000000: episode: 3064, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000905, mae: 0.013159, mean_q: 0.012142
  22679/1000000: episode: 3065, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000293, mae: 0.014220, mean_q: 0.018960
  22689/1000000: episode: 3066, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001239, mae: 0.012796, mean_q: 0.012651
  22699/1000000: episode: 3067, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000520, mae: 0.013236, mean_q: 0.016299
  22709/1000000: episode: 3068, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000242, mae: 0.012158, mean_q: 0.011741
  22719/1000000: episode: 3069, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.950, mean reward: 0.095 [0.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 5.450 [1.000, 10.000], loss: 0.000230, mae: 0.012998, mean_q: 0.009038
  22729/1000000: episode: 3070, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000732, mae: 0.017299, mean_q: 0.013568
  22739/1000000: episode: 3071, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001759, mae: 0.019488, mean_q: 0.017456
  22749/1000000: episode: 3072, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000549, mae: 0.016776, mean_q: 0.011855
  22759/1000000: episode: 3073, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000293, mae: 0.012745, mean_q: 0.009664
  22769/1000000: episode: 3074, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000624, mae: 0.014201, mean_q: 0.013959
  22779/1000000: episode: 3075, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000254, mae: 0.009089, mean_q: 0.012082
  22789/1000000: episode: 3076, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001086, mae: 0.014434, mean_q: 0.011813
  22799/1000000: episode: 3077, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001508, mae: 0.018325, mean_q: 0.018476
  22809/1000000: episode: 3078, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000176, mae: 0.009582, mean_q: 0.007278
  22819/1000000: episode: 3079, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.002145, mae: 0.018236, mean_q: 0.021167
  22829/1000000: episode: 3080, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.001585, mae: 0.019139, mean_q: 0.014239
  22839/1000000: episode: 3081, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000433, mae: 0.015540, mean_q: 0.015704
  22849/1000000: episode: 3082, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000423, mae: 0.016748, mean_q: 0.012491
  22859/1000000: episode: 3083, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000450, mae: 0.013528, mean_q: 0.010413
  22869/1000000: episode: 3084, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000330, mae: 0.012437, mean_q: 0.012331
  22879/1000000: episode: 3085, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000244, mae: 0.012270, mean_q: 0.010199
  22889/1000000: episode: 3086, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001859, mae: 0.019955, mean_q: 0.015861
  22899/1000000: episode: 3087, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001078, mae: 0.018374, mean_q: 0.014669
  22909/1000000: episode: 3088, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000235, mae: 0.013939, mean_q: 0.008513
  22919/1000000: episode: 3089, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000335, mae: 0.011418, mean_q: 0.009679
  22929/1000000: episode: 3090, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000311, mae: 0.012498, mean_q: 0.012224
  22939/1000000: episode: 3091, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000273, mae: 0.012367, mean_q: 0.008891
  22949/1000000: episode: 3092, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000381, mae: 0.013370, mean_q: 0.014293
  22959/1000000: episode: 3093, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001588, mae: 0.016432, mean_q: 0.019218
  22969/1000000: episode: 3094, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001142, mae: 0.020505, mean_q: 0.015766
  22979/1000000: episode: 3095, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001436, mae: 0.017127, mean_q: 0.013314
[Info] 1-TH LEVEL FOUND: 0.02238839864730835, Considering 10/100 traces
  22989/1000000: episode: 3096, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 0.264, mean reward: 0.026 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [-1.000, 10.000], loss: 0.000498, mae: 0.018183, mean_q: 0.016449
  22996/1000000: episode: 3097, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000280, mae: 0.012787, mean_q: 0.013900
  23000/1000000: episode: 3098, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000375, mae: 0.015870, mean_q: 0.013010
  23007/1000000: episode: 3099, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000312, mae: 0.015942, mean_q: 0.010969
  23014/1000000: episode: 3100, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000559, mae: 0.013223, mean_q: 0.014078
  23021/1000000: episode: 3101, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000359, mae: 0.012181, mean_q: 0.011284
  23028/1000000: episode: 3102, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.003549, mae: 0.025180, mean_q: 0.026941
  23035/1000000: episode: 3103, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000915, mae: 0.021504, mean_q: 0.013557
  23039/1000000: episode: 3104, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000799, mae: 0.022808, mean_q: 0.006767
  23043/1000000: episode: 3105, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000598, mae: 0.019122, mean_q: 0.012502
  23050/1000000: episode: 3106, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000248, mae: 0.012802, mean_q: 0.013780
  23057/1000000: episode: 3107, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000559, mae: 0.015798, mean_q: 0.021064
  23064/1000000: episode: 3108, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000413, mae: 0.011537, mean_q: 0.018677
  23071/1000000: episode: 3109, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.002735, mae: 0.021212, mean_q: 0.022948
  23078/1000000: episode: 3110, duration: 0.032s, episode steps: 7, steps per second: 217, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000463, mae: 0.014954, mean_q: 0.016489
  23085/1000000: episode: 3111, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000261, mae: 0.012866, mean_q: 0.011504
  23092/1000000: episode: 3112, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000278, mae: 0.010910, mean_q: 0.010126
  23099/1000000: episode: 3113, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000209, mae: 0.010039, mean_q: 0.013199
  23106/1000000: episode: 3114, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000230, mae: 0.010936, mean_q: 0.011566
  23113/1000000: episode: 3115, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000376, mae: 0.013112, mean_q: 0.011189
  23117/1000000: episode: 3116, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000535, mae: 0.016846, mean_q: 0.018986
  23124/1000000: episode: 3117, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000635, mae: 0.014280, mean_q: 0.016074
  23131/1000000: episode: 3118, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.001790, mae: 0.012026, mean_q: 0.011540
  23135/1000000: episode: 3119, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000389, mae: 0.013279, mean_q: 0.016091
  23142/1000000: episode: 3120, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000505, mae: 0.015428, mean_q: 0.013262
  23149/1000000: episode: 3121, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000331, mae: 0.013644, mean_q: 0.011301
  23156/1000000: episode: 3122, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000519, mae: 0.017406, mean_q: 0.016610
  23163/1000000: episode: 3123, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001992, mae: 0.022700, mean_q: 0.020381
  23170/1000000: episode: 3124, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000638, mae: 0.017443, mean_q: 0.016854
  23177/1000000: episode: 3125, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000506, mae: 0.014365, mean_q: 0.014397
  23184/1000000: episode: 3126, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000235, mae: 0.011369, mean_q: 0.007432
  23191/1000000: episode: 3127, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000229, mae: 0.010436, mean_q: 0.009482
  23198/1000000: episode: 3128, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000211, mae: 0.011962, mean_q: 0.011593
  23202/1000000: episode: 3129, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000746, mae: 0.020773, mean_q: 0.025567
  23209/1000000: episode: 3130, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.002960, mae: 0.024969, mean_q: 0.016168
  23216/1000000: episode: 3131, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000513, mae: 0.020381, mean_q: 0.009970
  23223/1000000: episode: 3132, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001745, mae: 0.018977, mean_q: 0.021431
  23227/1000000: episode: 3133, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001367, mae: 0.022194, mean_q: 0.029834
  23234/1000000: episode: 3134, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000587, mae: 0.016566, mean_q: 0.008485
  23241/1000000: episode: 3135, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000343, mae: 0.012947, mean_q: 0.013567
  23248/1000000: episode: 3136, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000483, mae: 0.014589, mean_q: 0.019475
  23252/1000000: episode: 3137, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000204, mae: 0.011693, mean_q: 0.006256
  23259/1000000: episode: 3138, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000152, mae: 0.009308, mean_q: 0.009695
  23266/1000000: episode: 3139, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000220, mae: 0.010501, mean_q: 0.014286
  23273/1000000: episode: 3140, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000214, mae: 0.009660, mean_q: 0.010523
  23280/1000000: episode: 3141, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000179, mae: 0.009606, mean_q: 0.010747
  23287/1000000: episode: 3142, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000297, mae: 0.012761, mean_q: 0.016491
[Info] FALSIFICATION!
  23293/1000000: episode: 3143, duration: 0.342s, episode steps: 6, steps per second: 18, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.001654, mae: 0.021447, mean_q: 0.024095
  23300/1000000: episode: 3144, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000312, mae: 0.011796, mean_q: 0.011508
  23307/1000000: episode: 3145, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000187, mae: 0.009702, mean_q: 0.007455
  23314/1000000: episode: 3146, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000456, mae: 0.013955, mean_q: 0.015928
  23321/1000000: episode: 3147, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.001526, mae: 0.019036, mean_q: 0.022026
  23328/1000000: episode: 3148, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000360, mae: 0.011781, mean_q: 0.012417
  23332/1000000: episode: 3149, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000165, mae: 0.008544, mean_q: 0.012117
  23336/1000000: episode: 3150, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000218, mae: 0.009386, mean_q: 0.009602
  23343/1000000: episode: 3151, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000195, mae: 0.011914, mean_q: 0.005739
  23350/1000000: episode: 3152, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000528, mae: 0.012490, mean_q: 0.011981
  23357/1000000: episode: 3153, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000315, mae: 0.011354, mean_q: 0.010754
  23361/1000000: episode: 3154, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000221, mae: 0.013109, mean_q: 0.019077
  23368/1000000: episode: 3155, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000391, mae: 0.012475, mean_q: 0.015291
  23375/1000000: episode: 3156, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000154, mae: 0.009364, mean_q: 0.008105
  23382/1000000: episode: 3157, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000222, mae: 0.010509, mean_q: 0.011122
  23389/1000000: episode: 3158, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000352, mae: 0.014417, mean_q: 0.014114
  23396/1000000: episode: 3159, duration: 0.032s, episode steps: 7, steps per second: 216, episode reward: 0.064, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000350, mae: 0.013066, mean_q: 0.013704
  23403/1000000: episode: 3160, duration: 0.032s, episode steps: 7, steps per second: 219, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000217, mae: 0.010509, mean_q: 0.013448
  23410/1000000: episode: 3161, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000391, mae: 0.011269, mean_q: 0.013371
  23417/1000000: episode: 3162, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000377, mae: 0.013877, mean_q: 0.018296
  23424/1000000: episode: 3163, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000271, mae: 0.013475, mean_q: 0.011185
  23431/1000000: episode: 3164, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000531, mae: 0.012778, mean_q: 0.012958
[Info] FALSIFICATION!
  23437/1000000: episode: 3165, duration: 0.260s, episode steps: 6, steps per second: 23, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000415, mae: 0.016293, mean_q: 0.017501
  23441/1000000: episode: 3166, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000191, mae: 0.012327, mean_q: 0.013221
  23448/1000000: episode: 3167, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 1.947, mean reward: 0.278 [0.007, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 7.857 [5.000, 11.000], loss: 0.000391, mae: 0.013895, mean_q: 0.010763
  23455/1000000: episode: 3168, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.001177, mae: 0.015650, mean_q: 0.018604
  23462/1000000: episode: 3169, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000414, mae: 0.014882, mean_q: 0.019293
  23466/1000000: episode: 3170, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000288, mae: 0.010964, mean_q: 0.017645
  23473/1000000: episode: 3171, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000932, mae: 0.012092, mean_q: 0.015043
  23480/1000000: episode: 3172, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000291, mae: 0.009352, mean_q: 0.012441
  23487/1000000: episode: 3173, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000294, mae: 0.009860, mean_q: 0.012529
  23494/1000000: episode: 3174, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000193, mae: 0.010191, mean_q: 0.015344
  23498/1000000: episode: 3175, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001777, mae: 0.020134, mean_q: 0.021860
  23502/1000000: episode: 3176, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000779, mae: 0.014710, mean_q: 0.022738
  23506/1000000: episode: 3177, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000240, mae: 0.009259, mean_q: 0.016517
  23513/1000000: episode: 3178, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.001132, mae: 0.016253, mean_q: 0.016568
  23520/1000000: episode: 3179, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000334, mae: 0.016408, mean_q: 0.009578
  23527/1000000: episode: 3180, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000237, mae: 0.013324, mean_q: 0.011516
  23534/1000000: episode: 3181, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000495, mae: 0.016363, mean_q: 0.023470
  23541/1000000: episode: 3182, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000389, mae: 0.016019, mean_q: 0.015534
  23548/1000000: episode: 3183, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000430, mae: 0.014664, mean_q: 0.011335
  23555/1000000: episode: 3184, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.001092, mae: 0.015795, mean_q: 0.009049
  23559/1000000: episode: 3185, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000132, mae: 0.012319, mean_q: 0.017438
[Info] Complete ISplit Iteration
[Info] Levels: [0.022388399, 0.5143147]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

  23563/1000000: episode: 3186, duration: 0.811s, episode steps: 4, steps per second: 5, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000532, mae: 0.016369, mean_q: 0.011716
  23573/1000000: episode: 3187, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000276, mae: 0.012455, mean_q: 0.016188
  23583/1000000: episode: 3188, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000258, mae: 0.012555, mean_q: 0.014523
  23593/1000000: episode: 3189, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001449, mae: 0.015332, mean_q: 0.020021
  23603/1000000: episode: 3190, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001448, mae: 0.019213, mean_q: 0.019781
  23613/1000000: episode: 3191, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002864, mae: 0.022940, mean_q: 0.017099
  23623/1000000: episode: 3192, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000950, mae: 0.023491, mean_q: 0.019081
  23633/1000000: episode: 3193, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000638, mae: 0.020261, mean_q: 0.009515
  23643/1000000: episode: 3194, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.001656, mae: 0.017759, mean_q: 0.015993
  23653/1000000: episode: 3195, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000425, mae: 0.017828, mean_q: 0.021381
[Info] FALSIFICATION!
  23663/1000000: episode: 3196, duration: 0.355s, episode steps: 10, steps per second: 28, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000254, mae: 0.012101, mean_q: 0.014190
  23673/1000000: episode: 3197, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000905, mae: 0.016301, mean_q: 0.019781
  23683/1000000: episode: 3198, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000750, mae: 0.013542, mean_q: 0.016269
  23693/1000000: episode: 3199, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001511, mae: 0.018478, mean_q: 0.024796
  23703/1000000: episode: 3200, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001145, mae: 0.021042, mean_q: 0.022883
  23713/1000000: episode: 3201, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000281, mae: 0.011266, mean_q: 0.011104
  23723/1000000: episode: 3202, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001843, mae: 0.020300, mean_q: 0.022080
  23733/1000000: episode: 3203, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000783, mae: 0.016893, mean_q: 0.017024
  23743/1000000: episode: 3204, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000343, mae: 0.012051, mean_q: 0.019113
  23753/1000000: episode: 3205, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000246, mae: 0.011334, mean_q: 0.013753
  23763/1000000: episode: 3206, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.131, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000911, mae: 0.014786, mean_q: 0.019299
  23773/1000000: episode: 3207, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000307, mae: 0.012273, mean_q: 0.016974
  23783/1000000: episode: 3208, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000464, mae: 0.012022, mean_q: 0.015842
  23793/1000000: episode: 3209, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001325, mae: 0.015761, mean_q: 0.022315
  23803/1000000: episode: 3210, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000800, mae: 0.015614, mean_q: 0.018078
  23813/1000000: episode: 3211, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001476, mae: 0.021241, mean_q: 0.021114
  23823/1000000: episode: 3212, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001327, mae: 0.022593, mean_q: 0.018941
  23833/1000000: episode: 3213, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001133, mae: 0.019535, mean_q: 0.021733
  23843/1000000: episode: 3214, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.001305, mae: 0.018973, mean_q: 0.017218
  23853/1000000: episode: 3215, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001613, mae: 0.017363, mean_q: 0.018917
  23863/1000000: episode: 3216, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000322, mae: 0.014372, mean_q: 0.013764
  23873/1000000: episode: 3217, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001070, mae: 0.020431, mean_q: 0.017280
  23883/1000000: episode: 3218, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000914, mae: 0.018352, mean_q: 0.018054
[Info] FALSIFICATION!
  23893/1000000: episode: 3219, duration: 0.295s, episode steps: 10, steps per second: 34, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000911, mae: 0.017937, mean_q: 0.022515
  23903/1000000: episode: 3220, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001159, mae: 0.015548, mean_q: 0.018286
  23913/1000000: episode: 3221, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000935, mae: 0.021301, mean_q: 0.021196
  23923/1000000: episode: 3222, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001784, mae: 0.020548, mean_q: 0.024399
  23933/1000000: episode: 3223, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001265, mae: 0.016976, mean_q: 0.022273
  23943/1000000: episode: 3224, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001176, mae: 0.027242, mean_q: 0.014798
  23953/1000000: episode: 3225, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000919, mae: 0.020112, mean_q: 0.015908
  23963/1000000: episode: 3226, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001101, mae: 0.019880, mean_q: 0.022671
  23973/1000000: episode: 3227, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.016, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000921, mae: 0.020434, mean_q: 0.021724
  23983/1000000: episode: 3228, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000485, mae: 0.017701, mean_q: 0.014459
  23993/1000000: episode: 3229, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000527, mae: 0.015090, mean_q: 0.015032
  24003/1000000: episode: 3230, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.002487, mae: 0.021312, mean_q: 0.022702
  24013/1000000: episode: 3231, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001217, mae: 0.021170, mean_q: 0.022285
  24023/1000000: episode: 3232, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000518, mae: 0.017731, mean_q: 0.013500
  24033/1000000: episode: 3233, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000353, mae: 0.012110, mean_q: 0.018368
  24043/1000000: episode: 3234, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000611, mae: 0.013800, mean_q: 0.017534
  24053/1000000: episode: 3235, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000819, mae: 0.017917, mean_q: 0.016940
  24063/1000000: episode: 3236, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000710, mae: 0.020850, mean_q: 0.018516
  24073/1000000: episode: 3237, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002322, mae: 0.020604, mean_q: 0.018180
  24083/1000000: episode: 3238, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000575, mae: 0.018423, mean_q: 0.014522
  24093/1000000: episode: 3239, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001647, mae: 0.022342, mean_q: 0.014341
  24103/1000000: episode: 3240, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000427, mae: 0.016641, mean_q: 0.021681
  24113/1000000: episode: 3241, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000637, mae: 0.020438, mean_q: 0.014265
  24123/1000000: episode: 3242, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000476, mae: 0.016121, mean_q: 0.013794
  24133/1000000: episode: 3243, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001237, mae: 0.018183, mean_q: 0.021520
  24143/1000000: episode: 3244, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.001893, mae: 0.018021, mean_q: 0.022444
  24153/1000000: episode: 3245, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001839, mae: 0.020562, mean_q: 0.025005
  24163/1000000: episode: 3246, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000487, mae: 0.015258, mean_q: 0.019859
  24173/1000000: episode: 3247, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000767, mae: 0.018501, mean_q: 0.017473
  24183/1000000: episode: 3248, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000453, mae: 0.014469, mean_q: 0.013136
  24193/1000000: episode: 3249, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000660, mae: 0.017820, mean_q: 0.020965
  24203/1000000: episode: 3250, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000448, mae: 0.016757, mean_q: 0.021787
  24213/1000000: episode: 3251, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000482, mae: 0.013729, mean_q: 0.016091
  24223/1000000: episode: 3252, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000717, mae: 0.014334, mean_q: 0.022609
  24233/1000000: episode: 3253, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001560, mae: 0.020319, mean_q: 0.025303
  24243/1000000: episode: 3254, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001795, mae: 0.019587, mean_q: 0.020337
  24253/1000000: episode: 3255, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.042, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.001003, mae: 0.020751, mean_q: 0.022718
  24263/1000000: episode: 3256, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001569, mae: 0.019585, mean_q: 0.024536
  24273/1000000: episode: 3257, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000833, mae: 0.020508, mean_q: 0.021011
  24283/1000000: episode: 3258, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000750, mae: 0.019409, mean_q: 0.014135
  24293/1000000: episode: 3259, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001651, mae: 0.021676, mean_q: 0.021648
  24303/1000000: episode: 3260, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002330, mae: 0.024026, mean_q: 0.024668
  24313/1000000: episode: 3261, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.750 [-1.000, 10.000], loss: 0.000748, mae: 0.012872, mean_q: 0.017970
  24323/1000000: episode: 3262, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000470, mae: 0.012662, mean_q: 0.017917
  24333/1000000: episode: 3263, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000313, mae: 0.010496, mean_q: 0.011808
  24343/1000000: episode: 3264, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000273, mae: 0.011049, mean_q: 0.014290
  24353/1000000: episode: 3265, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.001000, mae: 0.014995, mean_q: 0.017709
  24363/1000000: episode: 3266, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001030, mae: 0.018055, mean_q: 0.022661
  24373/1000000: episode: 3267, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000274, mae: 0.012452, mean_q: 0.014142
  24383/1000000: episode: 3268, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001141, mae: 0.021126, mean_q: 0.020308
  24393/1000000: episode: 3269, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000932, mae: 0.017200, mean_q: 0.022546
  24403/1000000: episode: 3270, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000788, mae: 0.014538, mean_q: 0.018933
  24413/1000000: episode: 3271, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000882, mae: 0.015883, mean_q: 0.016029
  24423/1000000: episode: 3272, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001282, mae: 0.019064, mean_q: 0.016678
  24433/1000000: episode: 3273, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000696, mae: 0.014844, mean_q: 0.016764
  24443/1000000: episode: 3274, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000305, mae: 0.011320, mean_q: 0.014244
  24453/1000000: episode: 3275, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000631, mae: 0.011627, mean_q: 0.015678
  24463/1000000: episode: 3276, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000374, mae: 0.013046, mean_q: 0.016760
  24473/1000000: episode: 3277, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.001666, mae: 0.016350, mean_q: 0.022384
  24483/1000000: episode: 3278, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000432, mae: 0.015858, mean_q: 0.015931
  24493/1000000: episode: 3279, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001404, mae: 0.019624, mean_q: 0.022019
  24503/1000000: episode: 3280, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000884, mae: 0.018211, mean_q: 0.019846
  24513/1000000: episode: 3281, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000480, mae: 0.016362, mean_q: 0.015033
  24523/1000000: episode: 3282, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000578, mae: 0.020422, mean_q: 0.020893
  24533/1000000: episode: 3283, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001296, mae: 0.020220, mean_q: 0.024237
  24543/1000000: episode: 3284, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000262, mae: 0.011980, mean_q: 0.013793
  24553/1000000: episode: 3285, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000325, mae: 0.011389, mean_q: 0.013056
[Info] Complete ISplit Iteration
[Info] Levels: [0.566134]
[Info] Cond. Prob: [0.02]
[Info] Error Prob: 0.02

  24563/1000000: episode: 3286, duration: 0.759s, episode steps: 10, steps per second: 13, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000462, mae: 0.013812, mean_q: 0.014525
  24573/1000000: episode: 3287, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.700 [-1.000, 10.000], loss: 0.000375, mae: 0.014181, mean_q: 0.017674
  24583/1000000: episode: 3288, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000977, mae: 0.016235, mean_q: 0.020041
  24593/1000000: episode: 3289, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001135, mae: 0.015956, mean_q: 0.024810
  24603/1000000: episode: 3290, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000869, mae: 0.017083, mean_q: 0.014996
  24613/1000000: episode: 3291, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000898, mae: 0.015998, mean_q: 0.014029
  24623/1000000: episode: 3292, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.001149, mae: 0.018495, mean_q: 0.019891
  24633/1000000: episode: 3293, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.001741, mae: 0.023434, mean_q: 0.022455
  24643/1000000: episode: 3294, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000835, mae: 0.015904, mean_q: 0.019759
  24653/1000000: episode: 3295, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000493, mae: 0.013966, mean_q: 0.018792
  24663/1000000: episode: 3296, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001226, mae: 0.014747, mean_q: 0.020527
  24673/1000000: episode: 3297, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.029, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000834, mae: 0.017407, mean_q: 0.021893
  24683/1000000: episode: 3298, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000696, mae: 0.018579, mean_q: 0.016959
  24693/1000000: episode: 3299, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000839, mae: 0.018430, mean_q: 0.017732
  24703/1000000: episode: 3300, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000369, mae: 0.015260, mean_q: 0.017291
  24713/1000000: episode: 3301, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000804, mae: 0.016601, mean_q: 0.013674
  24723/1000000: episode: 3302, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000395, mae: 0.015062, mean_q: 0.018705
  24733/1000000: episode: 3303, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000331, mae: 0.013522, mean_q: 0.008622
  24743/1000000: episode: 3304, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.001345, mae: 0.017557, mean_q: 0.018404
  24753/1000000: episode: 3305, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000682, mae: 0.012945, mean_q: 0.018072
  24763/1000000: episode: 3306, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000392, mae: 0.011536, mean_q: 0.015913
  24773/1000000: episode: 3307, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000763, mae: 0.013333, mean_q: 0.019772
  24783/1000000: episode: 3308, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000573, mae: 0.012118, mean_q: 0.015619
  24793/1000000: episode: 3309, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000457, mae: 0.013150, mean_q: 0.017825
  24803/1000000: episode: 3310, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000575, mae: 0.012017, mean_q: 0.013437
  24813/1000000: episode: 3311, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001609, mae: 0.018621, mean_q: 0.018943
  24823/1000000: episode: 3312, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000459, mae: 0.013252, mean_q: 0.015505
  24833/1000000: episode: 3313, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000435, mae: 0.012222, mean_q: 0.012652
  24843/1000000: episode: 3314, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000638, mae: 0.012943, mean_q: 0.015606
  24853/1000000: episode: 3315, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000183, mae: 0.011279, mean_q: 0.010711
  24863/1000000: episode: 3316, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000198, mae: 0.010128, mean_q: 0.011444
  24873/1000000: episode: 3317, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000361, mae: 0.011186, mean_q: 0.014627
  24883/1000000: episode: 3318, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.221, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.000422, mae: 0.013943, mean_q: 0.012783
  24893/1000000: episode: 3319, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.001406, mae: 0.017119, mean_q: 0.012948
  24903/1000000: episode: 3320, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000669, mae: 0.014351, mean_q: 0.011888
  24913/1000000: episode: 3321, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000274, mae: 0.010394, mean_q: 0.011111
  24923/1000000: episode: 3322, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000699, mae: 0.013625, mean_q: 0.018773
  24933/1000000: episode: 3323, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000186, mae: 0.008563, mean_q: 0.009188
  24943/1000000: episode: 3324, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.001736, mae: 0.017495, mean_q: 0.022340
  24953/1000000: episode: 3325, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001043, mae: 0.016077, mean_q: 0.018706
  24963/1000000: episode: 3326, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000275, mae: 0.011291, mean_q: 0.011402
  24973/1000000: episode: 3327, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000558, mae: 0.012968, mean_q: 0.017244
  24983/1000000: episode: 3328, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000420, mae: 0.011661, mean_q: 0.014339
  24993/1000000: episode: 3329, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001154, mae: 0.011424, mean_q: 0.014078
  25003/1000000: episode: 3330, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000442, mae: 0.009559, mean_q: 0.013029
  25013/1000000: episode: 3331, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001031, mae: 0.012360, mean_q: 0.013225
  25023/1000000: episode: 3332, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.001066, mae: 0.014467, mean_q: 0.014876
  25033/1000000: episode: 3333, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000867, mae: 0.017772, mean_q: 0.021045
  25043/1000000: episode: 3334, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.051, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000291, mae: 0.011331, mean_q: 0.010002
  25053/1000000: episode: 3335, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000891, mae: 0.011137, mean_q: 0.011008
  25063/1000000: episode: 3336, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000553, mae: 0.012285, mean_q: 0.011380
  25073/1000000: episode: 3337, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000343, mae: 0.011236, mean_q: 0.014939
  25083/1000000: episode: 3338, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000241, mae: 0.010636, mean_q: 0.014640
  25093/1000000: episode: 3339, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000129, mae: 0.007563, mean_q: 0.010786
  25103/1000000: episode: 3340, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000370, mae: 0.010593, mean_q: 0.011543
  25113/1000000: episode: 3341, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000470, mae: 0.013066, mean_q: 0.015904
  25123/1000000: episode: 3342, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000375, mae: 0.013439, mean_q: 0.014877
  25133/1000000: episode: 3343, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000417, mae: 0.013590, mean_q: 0.014553
  25143/1000000: episode: 3344, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001125, mae: 0.012506, mean_q: 0.017196
  25153/1000000: episode: 3345, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000269, mae: 0.012258, mean_q: 0.011267
  25163/1000000: episode: 3346, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000197, mae: 0.010550, mean_q: 0.010524
  25173/1000000: episode: 3347, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001193, mae: 0.014357, mean_q: 0.017405
  25183/1000000: episode: 3348, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000271, mae: 0.009657, mean_q: 0.012967
  25193/1000000: episode: 3349, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000405, mae: 0.012304, mean_q: 0.014264
  25203/1000000: episode: 3350, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000814, mae: 0.015764, mean_q: 0.011845
  25213/1000000: episode: 3351, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001318, mae: 0.016495, mean_q: 0.017972
  25223/1000000: episode: 3352, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000526, mae: 0.016155, mean_q: 0.017438
  25233/1000000: episode: 3353, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000310, mae: 0.012507, mean_q: 0.011212
  25243/1000000: episode: 3354, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.104, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000307, mae: 0.011505, mean_q: 0.007352
  25253/1000000: episode: 3355, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000722, mae: 0.015668, mean_q: 0.020908
  25263/1000000: episode: 3356, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000738, mae: 0.013899, mean_q: 0.013369
  25273/1000000: episode: 3357, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001933, mae: 0.018032, mean_q: 0.023098
  25283/1000000: episode: 3358, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000632, mae: 0.016439, mean_q: 0.022475
  25293/1000000: episode: 3359, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001258, mae: 0.016087, mean_q: 0.015504
  25303/1000000: episode: 3360, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000412, mae: 0.011642, mean_q: 0.009859
  25313/1000000: episode: 3361, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000569, mae: 0.015929, mean_q: 0.015275
  25323/1000000: episode: 3362, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000359, mae: 0.011008, mean_q: 0.011820
  25333/1000000: episode: 3363, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001440, mae: 0.018898, mean_q: 0.025082
  25343/1000000: episode: 3364, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000348, mae: 0.014087, mean_q: 0.009371
  25353/1000000: episode: 3365, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000528, mae: 0.014062, mean_q: 0.016053
  25363/1000000: episode: 3366, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000344, mae: 0.010670, mean_q: 0.013556
  25373/1000000: episode: 3367, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000343, mae: 0.011166, mean_q: 0.013239
  25383/1000000: episode: 3368, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000179, mae: 0.009432, mean_q: 0.010443
  25393/1000000: episode: 3369, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000286, mae: 0.010476, mean_q: 0.010309
  25403/1000000: episode: 3370, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.068, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000344, mae: 0.011537, mean_q: 0.010173
  25413/1000000: episode: 3371, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.002117, mae: 0.016620, mean_q: 0.018418
  25423/1000000: episode: 3372, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001782, mae: 0.028329, mean_q: 0.023172
  25433/1000000: episode: 3373, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.042, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000768, mae: 0.021787, mean_q: 0.012537
  25443/1000000: episode: 3374, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000645, mae: 0.013951, mean_q: 0.013473
  25453/1000000: episode: 3375, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001940, mae: 0.014228, mean_q: 0.015855
  25463/1000000: episode: 3376, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000341, mae: 0.013877, mean_q: 0.012297
  25473/1000000: episode: 3377, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000634, mae: 0.012893, mean_q: 0.018017
  25483/1000000: episode: 3378, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001087, mae: 0.016229, mean_q: 0.012658
  25493/1000000: episode: 3379, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000371, mae: 0.011813, mean_q: 0.014140
  25503/1000000: episode: 3380, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000399, mae: 0.011134, mean_q: 0.014482
  25513/1000000: episode: 3381, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000573, mae: 0.010497, mean_q: 0.014475
  25523/1000000: episode: 3382, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000191, mae: 0.010224, mean_q: 0.009306
  25533/1000000: episode: 3383, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000164, mae: 0.007823, mean_q: 0.009963
  25543/1000000: episode: 3384, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000605, mae: 0.010477, mean_q: 0.011145
  25553/1000000: episode: 3385, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000432, mae: 0.013708, mean_q: 0.016147
[Info] 1-TH LEVEL FOUND: 0.028403759002685547, Considering 10/100 traces
  25563/1000000: episode: 3386, duration: 0.656s, episode steps: 10, steps per second: 15, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000169, mae: 0.008430, mean_q: 0.012027
  25570/1000000: episode: 3387, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000237, mae: 0.010439, mean_q: 0.013999
  25577/1000000: episode: 3388, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.001554, mae: 0.015466, mean_q: 0.015349
  25581/1000000: episode: 3389, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000252, mae: 0.013700, mean_q: 0.001277
  25588/1000000: episode: 3390, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.001379, mae: 0.017494, mean_q: 0.022945
  25592/1000000: episode: 3391, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000277, mae: 0.012707, mean_q: 0.004624
  25599/1000000: episode: 3392, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.001412, mae: 0.015235, mean_q: 0.017122
  25603/1000000: episode: 3393, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000195, mae: 0.011581, mean_q: 0.011204
  25610/1000000: episode: 3394, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000545, mae: 0.011638, mean_q: 0.012207
  25617/1000000: episode: 3395, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.045, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.001599, mae: 0.014742, mean_q: 0.022410
  25621/1000000: episode: 3396, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001296, mae: 0.016673, mean_q: 0.020733
  25628/1000000: episode: 3397, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000889, mae: 0.015810, mean_q: 0.015345
  25632/1000000: episode: 3398, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000394, mae: 0.010925, mean_q: 0.005777
  25636/1000000: episode: 3399, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000180, mae: 0.012202, mean_q: 0.013474
  25643/1000000: episode: 3400, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.001098, mae: 0.016201, mean_q: 0.021136
  25647/1000000: episode: 3401, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000172, mae: 0.010987, mean_q: 0.017764
  25654/1000000: episode: 3402, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000523, mae: 0.014310, mean_q: 0.013424
  25661/1000000: episode: 3403, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000227, mae: 0.012904, mean_q: 0.007930
  25668/1000000: episode: 3404, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000461, mae: 0.011291, mean_q: 0.016484
  25672/1000000: episode: 3405, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000192, mae: 0.009718, mean_q: 0.010616
  25679/1000000: episode: 3406, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000315, mae: 0.010053, mean_q: 0.012662
  25686/1000000: episode: 3407, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000291, mae: 0.011951, mean_q: 0.010082
  25693/1000000: episode: 3408, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.001035, mae: 0.015278, mean_q: 0.022826
  25697/1000000: episode: 3409, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001512, mae: 0.018833, mean_q: 0.020345
  25701/1000000: episode: 3410, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000513, mae: 0.013973, mean_q: 0.016977
  25708/1000000: episode: 3411, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000240, mae: 0.012408, mean_q: 0.017290
  25712/1000000: episode: 3412, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000085, mae: 0.008000, mean_q: 0.005314
  25719/1000000: episode: 3413, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001404, mae: 0.013100, mean_q: 0.014089
  25726/1000000: episode: 3414, duration: 0.034s, episode steps: 7, steps per second: 206, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000281, mae: 0.011233, mean_q: 0.015275
  25733/1000000: episode: 3415, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000586, mae: 0.015365, mean_q: 0.019039
  25740/1000000: episode: 3416, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000722, mae: 0.016279, mean_q: 0.020910
  25747/1000000: episode: 3417, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000327, mae: 0.013240, mean_q: 0.011925
[Info] FALSIFICATION!
  25753/1000000: episode: 3418, duration: 0.266s, episode steps: 6, steps per second: 23, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000290, mae: 0.010775, mean_q: 0.013638
  25760/1000000: episode: 3419, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000213, mae: 0.009880, mean_q: 0.013189
  25764/1000000: episode: 3420, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000219, mae: 0.007872, mean_q: 0.007793
  25768/1000000: episode: 3421, duration: 0.021s, episode steps: 4, steps per second: 191, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000788, mae: 0.017380, mean_q: 0.024621
  25775/1000000: episode: 3422, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.080, mean reward: 0.011 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000614, mae: 0.016520, mean_q: 0.019868
  25779/1000000: episode: 3423, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000128, mae: 0.009473, mean_q: 0.016029
  25783/1000000: episode: 3424, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001697, mae: 0.017331, mean_q: 0.019768
  25787/1000000: episode: 3425, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.003655, mae: 0.022772, mean_q: 0.026981
[Info] FALSIFICATION!
  25793/1000000: episode: 3426, duration: 0.184s, episode steps: 6, steps per second: 33, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000442, mae: 0.014389, mean_q: 0.013640
  25800/1000000: episode: 3427, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000470, mae: 0.012101, mean_q: 0.012973
  25804/1000000: episode: 3428, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000614, mae: 0.015098, mean_q: 0.017394
  25808/1000000: episode: 3429, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000418, mae: 0.013130, mean_q: 0.012550
  25815/1000000: episode: 3430, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 0.037, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000206, mae: 0.010720, mean_q: 0.013641
  25819/1000000: episode: 3431, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001018, mae: 0.017293, mean_q: 0.019952
  25826/1000000: episode: 3432, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000291, mae: 0.011603, mean_q: 0.016936
  25833/1000000: episode: 3433, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000465, mae: 0.010189, mean_q: 0.017533
  25840/1000000: episode: 3434, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000329, mae: 0.011577, mean_q: 0.007537
  25847/1000000: episode: 3435, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000936, mae: 0.014587, mean_q: 0.016330
  25851/1000000: episode: 3436, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000980, mae: 0.014228, mean_q: 0.023475
  25855/1000000: episode: 3437, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000842, mae: 0.013705, mean_q: 0.019971
  25859/1000000: episode: 3438, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000402, mae: 0.015301, mean_q: 0.027354
  25866/1000000: episode: 3439, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000583, mae: 0.018298, mean_q: 0.013846
  25870/1000000: episode: 3440, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000468, mae: 0.015283, mean_q: 0.012390
  25874/1000000: episode: 3441, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001202, mae: 0.022743, mean_q: 0.035518
  25881/1000000: episode: 3442, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000619, mae: 0.015746, mean_q: 0.015494
  25885/1000000: episode: 3443, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000324, mae: 0.011548, mean_q: 0.015059
  25892/1000000: episode: 3444, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000830, mae: 0.016326, mean_q: 0.018577
  25899/1000000: episode: 3445, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000304, mae: 0.012871, mean_q: 0.010487
  25903/1000000: episode: 3446, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000367, mae: 0.011799, mean_q: 0.011443
  25910/1000000: episode: 3447, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000511, mae: 0.014125, mean_q: 0.019085
  25917/1000000: episode: 3448, duration: 0.035s, episode steps: 7, steps per second: 202, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000317, mae: 0.012130, mean_q: 0.014265
  25921/1000000: episode: 3449, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000375, mae: 0.013727, mean_q: 0.019090
  25925/1000000: episode: 3450, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000453, mae: 0.011326, mean_q: 0.007817
  25929/1000000: episode: 3451, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000270, mae: 0.012709, mean_q: 0.019770
  25936/1000000: episode: 3452, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.001707, mae: 0.019726, mean_q: 0.017531
  25943/1000000: episode: 3453, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000283, mae: 0.012744, mean_q: 0.012658
  25950/1000000: episode: 3454, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000526, mae: 0.015816, mean_q: 0.016898
  25957/1000000: episode: 3455, duration: 0.035s, episode steps: 7, steps per second: 202, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000391, mae: 0.012892, mean_q: 0.012365
  25964/1000000: episode: 3456, duration: 0.034s, episode steps: 7, steps per second: 204, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000288, mae: 0.013781, mean_q: 0.010660
  25968/1000000: episode: 3457, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.002202, mae: 0.020517, mean_q: 0.023746
  25972/1000000: episode: 3458, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000378, mae: 0.013017, mean_q: 0.013730
  25976/1000000: episode: 3459, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000475, mae: 0.014433, mean_q: 0.023944
  25983/1000000: episode: 3460, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000727, mae: 0.013628, mean_q: 0.018899
  25987/1000000: episode: 3461, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000282, mae: 0.011434, mean_q: 0.018350
  25991/1000000: episode: 3462, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000498, mae: 0.015086, mean_q: 0.018546
  25995/1000000: episode: 3463, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000568, mae: 0.012054, mean_q: 0.013419
  25999/1000000: episode: 3464, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000562, mae: 0.017023, mean_q: 0.011366
  26003/1000000: episode: 3465, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000582, mae: 0.017865, mean_q: 0.017531
  26010/1000000: episode: 3466, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000935, mae: 0.014731, mean_q: 0.011856
  26014/1000000: episode: 3467, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000459, mae: 0.016842, mean_q: 0.019032
  26021/1000000: episode: 3468, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000363, mae: 0.014330, mean_q: 0.015544
  26025/1000000: episode: 3469, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000258, mae: 0.010963, mean_q: 0.011794
  26029/1000000: episode: 3470, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000948, mae: 0.015905, mean_q: 0.020099
  26033/1000000: episode: 3471, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000236, mae: 0.010232, mean_q: 0.014379
  26037/1000000: episode: 3472, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000472, mae: 0.012942, mean_q: 0.013748
  26044/1000000: episode: 3473, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000324, mae: 0.013799, mean_q: 0.008792
  26051/1000000: episode: 3474, duration: 0.032s, episode steps: 7, steps per second: 215, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000424, mae: 0.015453, mean_q: 0.019253
  26055/1000000: episode: 3475, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001000, mae: 0.017585, mean_q: 0.018740
[Info] Complete ISplit Iteration
[Info] Levels: [0.028403759, 0.6554803]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

  26059/1000000: episode: 3476, duration: 0.849s, episode steps: 4, steps per second: 5, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000373, mae: 0.012855, mean_q: 0.020097
  26069/1000000: episode: 3477, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001386, mae: 0.015536, mean_q: 0.013490
  26079/1000000: episode: 3478, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.045, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000468, mae: 0.015898, mean_q: 0.016912
  26089/1000000: episode: 3479, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000639, mae: 0.014546, mean_q: 0.016956
  26099/1000000: episode: 3480, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000671, mae: 0.015686, mean_q: 0.016719
  26109/1000000: episode: 3481, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000620, mae: 0.017143, mean_q: 0.019093
  26119/1000000: episode: 3482, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000252, mae: 0.012206, mean_q: 0.010547
  26129/1000000: episode: 3483, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000343, mae: 0.012059, mean_q: 0.012527
  26139/1000000: episode: 3484, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000707, mae: 0.013152, mean_q: 0.014618
  26149/1000000: episode: 3485, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001433, mae: 0.018203, mean_q: 0.021905
  26159/1000000: episode: 3486, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000772, mae: 0.019285, mean_q: 0.018527
  26169/1000000: episode: 3487, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001166, mae: 0.020351, mean_q: 0.015744
  26179/1000000: episode: 3488, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000897, mae: 0.021990, mean_q: 0.020248
  26189/1000000: episode: 3489, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000709, mae: 0.020646, mean_q: 0.019979
  26199/1000000: episode: 3490, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000762, mae: 0.015938, mean_q: 0.021216
  26209/1000000: episode: 3491, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000946, mae: 0.017884, mean_q: 0.020198
  26219/1000000: episode: 3492, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000338, mae: 0.012426, mean_q: 0.017003
  26229/1000000: episode: 3493, duration: 0.072s, episode steps: 10, steps per second: 140, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000401, mae: 0.011771, mean_q: 0.014748
  26239/1000000: episode: 3494, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000492, mae: 0.014019, mean_q: 0.019310
  26249/1000000: episode: 3495, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000564, mae: 0.015868, mean_q: 0.014604
  26259/1000000: episode: 3496, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000330, mae: 0.011353, mean_q: 0.013971
  26269/1000000: episode: 3497, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000911, mae: 0.013710, mean_q: 0.016181
  26279/1000000: episode: 3498, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001028, mae: 0.014466, mean_q: 0.019483
  26289/1000000: episode: 3499, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000281, mae: 0.011984, mean_q: 0.011378
  26299/1000000: episode: 3500, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.067, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000616, mae: 0.013536, mean_q: 0.012384
  26309/1000000: episode: 3501, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.215, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000700, mae: 0.014825, mean_q: 0.020532
  26319/1000000: episode: 3502, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.001181, mae: 0.015559, mean_q: 0.015792
  26329/1000000: episode: 3503, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000657, mae: 0.016514, mean_q: 0.016831
  26339/1000000: episode: 3504, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.015, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000424, mae: 0.013850, mean_q: 0.015302
  26349/1000000: episode: 3505, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.029, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000288, mae: 0.012028, mean_q: 0.012410
  26359/1000000: episode: 3506, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000451, mae: 0.013078, mean_q: 0.014654
  26369/1000000: episode: 3507, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000296, mae: 0.010401, mean_q: 0.014519
  26379/1000000: episode: 3508, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000403, mae: 0.011545, mean_q: 0.015610
  26389/1000000: episode: 3509, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000907, mae: 0.015412, mean_q: 0.014078
  26399/1000000: episode: 3510, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000349, mae: 0.011846, mean_q: 0.015368
  26409/1000000: episode: 3511, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000830, mae: 0.014663, mean_q: 0.020261
  26419/1000000: episode: 3512, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000307, mae: 0.011188, mean_q: 0.013413
  26429/1000000: episode: 3513, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000203, mae: 0.009945, mean_q: 0.012890
  26439/1000000: episode: 3514, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000314, mae: 0.011573, mean_q: 0.011544
  26449/1000000: episode: 3515, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000723, mae: 0.017800, mean_q: 0.017310
  26459/1000000: episode: 3516, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000356, mae: 0.015064, mean_q: 0.012694
  26469/1000000: episode: 3517, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000386, mae: 0.013440, mean_q: 0.013537
  26479/1000000: episode: 3518, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000804, mae: 0.015640, mean_q: 0.018083
  26489/1000000: episode: 3519, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000453, mae: 0.016544, mean_q: 0.009241
  26499/1000000: episode: 3520, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000511, mae: 0.015547, mean_q: 0.013426
  26509/1000000: episode: 3521, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000271, mae: 0.009934, mean_q: 0.013141
  26519/1000000: episode: 3522, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000512, mae: 0.013945, mean_q: 0.016371
  26529/1000000: episode: 3523, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000179, mae: 0.009469, mean_q: 0.010798
  26539/1000000: episode: 3524, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000226, mae: 0.008923, mean_q: 0.010576
  26549/1000000: episode: 3525, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000420, mae: 0.014435, mean_q: 0.014048
  26559/1000000: episode: 3526, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000349, mae: 0.012703, mean_q: 0.013120
  26569/1000000: episode: 3527, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000725, mae: 0.017204, mean_q: 0.016777
  26579/1000000: episode: 3528, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000368, mae: 0.013328, mean_q: 0.015472
  26589/1000000: episode: 3529, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000463, mae: 0.015865, mean_q: 0.012147
  26599/1000000: episode: 3530, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000336, mae: 0.011436, mean_q: 0.015891
  26609/1000000: episode: 3531, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000338, mae: 0.010924, mean_q: 0.010333
  26619/1000000: episode: 3532, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000259, mae: 0.011470, mean_q: 0.011144
  26629/1000000: episode: 3533, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000557, mae: 0.014722, mean_q: 0.018242
  26639/1000000: episode: 3534, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000315, mae: 0.010739, mean_q: 0.012417
  26649/1000000: episode: 3535, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001261, mae: 0.016437, mean_q: 0.018852
  26659/1000000: episode: 3536, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000456, mae: 0.012643, mean_q: 0.015418
  26669/1000000: episode: 3537, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000447, mae: 0.012146, mean_q: 0.011433
  26679/1000000: episode: 3538, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000727, mae: 0.013070, mean_q: 0.017444
  26689/1000000: episode: 3539, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000244, mae: 0.013730, mean_q: 0.016240
  26699/1000000: episode: 3540, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000402, mae: 0.013181, mean_q: 0.011736
  26709/1000000: episode: 3541, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000800, mae: 0.014607, mean_q: 0.017696
  26719/1000000: episode: 3542, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000608, mae: 0.014168, mean_q: 0.016128
  26729/1000000: episode: 3543, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000664, mae: 0.012418, mean_q: 0.015911
  26739/1000000: episode: 3544, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000158, mae: 0.011093, mean_q: 0.011509
  26749/1000000: episode: 3545, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000305, mae: 0.011501, mean_q: 0.011822
  26759/1000000: episode: 3546, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000553, mae: 0.013701, mean_q: 0.013495
  26769/1000000: episode: 3547, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000698, mae: 0.013716, mean_q: 0.018273
  26779/1000000: episode: 3548, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000280, mae: 0.012286, mean_q: 0.012579
  26789/1000000: episode: 3549, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001235, mae: 0.012872, mean_q: 0.013256
  26799/1000000: episode: 3550, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000938, mae: 0.015141, mean_q: 0.017457
  26809/1000000: episode: 3551, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000308, mae: 0.010960, mean_q: 0.013573
  26819/1000000: episode: 3552, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000350, mae: 0.010722, mean_q: 0.010527
  26829/1000000: episode: 3553, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000735, mae: 0.014849, mean_q: 0.016792
  26839/1000000: episode: 3554, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000346, mae: 0.014015, mean_q: 0.014895
  26849/1000000: episode: 3555, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000698, mae: 0.014311, mean_q: 0.014847
  26859/1000000: episode: 3556, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000310, mae: 0.011073, mean_q: 0.009828
  26869/1000000: episode: 3557, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000670, mae: 0.015923, mean_q: 0.018582
  26879/1000000: episode: 3558, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000682, mae: 0.013272, mean_q: 0.018948
  26889/1000000: episode: 3559, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000398, mae: 0.010297, mean_q: 0.013525
  26899/1000000: episode: 3560, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000810, mae: 0.016326, mean_q: 0.014167
  26909/1000000: episode: 3561, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000644, mae: 0.014470, mean_q: 0.017702
  26919/1000000: episode: 3562, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000165, mae: 0.011147, mean_q: 0.011468
  26929/1000000: episode: 3563, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000501, mae: 0.010617, mean_q: 0.010444
  26939/1000000: episode: 3564, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000229, mae: 0.008909, mean_q: 0.012350
  26949/1000000: episode: 3565, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000329, mae: 0.010192, mean_q: 0.015289
  26959/1000000: episode: 3566, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000446, mae: 0.009655, mean_q: 0.011523
  26969/1000000: episode: 3567, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000371, mae: 0.012214, mean_q: 0.013253
  26979/1000000: episode: 3568, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000577, mae: 0.012208, mean_q: 0.014740
  26989/1000000: episode: 3569, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000210, mae: 0.011304, mean_q: 0.011081
  26999/1000000: episode: 3570, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000348, mae: 0.010122, mean_q: 0.013365
  27009/1000000: episode: 3571, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000721, mae: 0.013117, mean_q: 0.015298
  27019/1000000: episode: 3572, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000254, mae: 0.011585, mean_q: 0.007592
  27029/1000000: episode: 3573, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001251, mae: 0.015583, mean_q: 0.011944
  27039/1000000: episode: 3574, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000626, mae: 0.019792, mean_q: 0.014695
  27049/1000000: episode: 3575, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000602, mae: 0.012619, mean_q: 0.019164
[Info] 1-TH LEVEL FOUND: 0.021668970584869385, Considering 14/100 traces
  27059/1000000: episode: 3576, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000436, mae: 0.013369, mean_q: 0.017247
  27063/1000000: episode: 3577, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001713, mae: 0.025371, mean_q: 0.009265
  27065/1000000: episode: 3578, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005013, mae: 0.032902, mean_q: 0.032192
  27072/1000000: episode: 3579, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000781, mae: 0.020898, mean_q: 0.013544
  27079/1000000: episode: 3580, duration: 0.035s, episode steps: 7, steps per second: 201, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000495, mae: 0.013540, mean_q: 0.015561
  27086/1000000: episode: 3581, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 0.025, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000849, mae: 0.015965, mean_q: 0.019922
  27090/1000000: episode: 3582, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000576, mae: 0.016592, mean_q: 0.008011
  27097/1000000: episode: 3583, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.001178, mae: 0.020100, mean_q: 0.019945
  27101/1000000: episode: 3584, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000208, mae: 0.012276, mean_q: 0.017610
  27105/1000000: episode: 3585, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000397, mae: 0.015802, mean_q: 0.004074
  27107/1000000: episode: 3586, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000140, mae: 0.008940, mean_q: 0.012170
  27114/1000000: episode: 3587, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000729, mae: 0.012958, mean_q: 0.010465
  27116/1000000: episode: 3588, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001896, mae: 0.024644, mean_q: 0.033374
  27123/1000000: episode: 3589, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000731, mae: 0.015695, mean_q: 0.018559
  27125/1000000: episode: 3590, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000317, mae: 0.011035, mean_q: 0.016891
  27129/1000000: episode: 3591, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000101, mae: 0.008860, mean_q: 0.007090
  27136/1000000: episode: 3592, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000773, mae: 0.013398, mean_q: 0.012541
  27140/1000000: episode: 3593, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000515, mae: 0.015115, mean_q: 0.022641
  27142/1000000: episode: 3594, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000342, mae: 0.010526, mean_q: 0.013204
  27149/1000000: episode: 3595, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000789, mae: 0.012248, mean_q: 0.016116
  27151/1000000: episode: 3596, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000358, mae: 0.012007, mean_q: 0.013501
  27155/1000000: episode: 3597, duration: 0.030s, episode steps: 4, steps per second: 131, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000675, mae: 0.016788, mean_q: 0.024272
  27157/1000000: episode: 3598, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000619, mae: 0.015196, mean_q: 0.015814
  27161/1000000: episode: 3599, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000401, mae: 0.010973, mean_q: 0.012720
  27163/1000000: episode: 3600, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001647, mae: 0.016605, mean_q: 0.021590
  27167/1000000: episode: 3601, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000235, mae: 0.010370, mean_q: 0.013257
  27171/1000000: episode: 3602, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.001058, mae: 0.016258, mean_q: 0.022000
  27178/1000000: episode: 3603, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000226, mae: 0.009296, mean_q: 0.013227
  27182/1000000: episode: 3604, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000358, mae: 0.008698, mean_q: 0.012315
  27186/1000000: episode: 3605, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000256, mae: 0.010051, mean_q: 0.008047
  27193/1000000: episode: 3606, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.096, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000415, mae: 0.012804, mean_q: 0.011561
  27200/1000000: episode: 3607, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.088, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000943, mae: 0.016894, mean_q: 0.022518
  27207/1000000: episode: 3608, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000966, mae: 0.018143, mean_q: 0.018742
  27209/1000000: episode: 3609, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000191, mae: 0.009962, mean_q: 0.014286
  27216/1000000: episode: 3610, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000534, mae: 0.015402, mean_q: 0.018574
  27218/1000000: episode: 3611, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000596, mae: 0.019223, mean_q: 0.010131
  27225/1000000: episode: 3612, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.001014, mae: 0.018880, mean_q: 0.023018
  27229/1000000: episode: 3613, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001317, mae: 0.016631, mean_q: 0.016239
  27231/1000000: episode: 3614, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000150, mae: 0.010741, mean_q: 0.013942
  27235/1000000: episode: 3615, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000366, mae: 0.012991, mean_q: 0.011084
  27239/1000000: episode: 3616, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000432, mae: 0.013795, mean_q: 0.019236
  27243/1000000: episode: 3617, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000268, mae: 0.011963, mean_q: 0.008521
  27247/1000000: episode: 3618, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000232, mae: 0.013492, mean_q: 0.018492
  27251/1000000: episode: 3619, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000185, mae: 0.009704, mean_q: 0.009793
  27255/1000000: episode: 3620, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000704, mae: 0.016316, mean_q: 0.023681
  27257/1000000: episode: 3621, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000315, mae: 0.016760, mean_q: -0.000986
  27261/1000000: episode: 3622, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000529, mae: 0.016039, mean_q: 0.023571
  27263/1000000: episode: 3623, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000126, mae: 0.010723, mean_q: 0.004882
  27267/1000000: episode: 3624, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000422, mae: 0.014605, mean_q: 0.015962
  27274/1000000: episode: 3625, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000624, mae: 0.015001, mean_q: 0.017405
  27276/1000000: episode: 3626, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000305, mae: 0.012862, mean_q: 0.019568
  27278/1000000: episode: 3627, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000297, mae: 0.017981, mean_q: -0.001742
  27282/1000000: episode: 3628, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000204, mae: 0.011341, mean_q: 0.014028
  27284/1000000: episode: 3629, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000279, mae: 0.010903, mean_q: 0.008375
  27286/1000000: episode: 3630, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000193, mae: 0.010458, mean_q: 0.011439
  27288/1000000: episode: 3631, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001120, mae: 0.021079, mean_q: 0.026568
  27295/1000000: episode: 3632, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000499, mae: 0.016272, mean_q: 0.014564
  27302/1000000: episode: 3633, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.001177, mae: 0.015948, mean_q: 0.016506
  27306/1000000: episode: 3634, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000346, mae: 0.014841, mean_q: 0.018701
  27310/1000000: episode: 3635, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000317, mae: 0.013670, mean_q: 0.017025
  27314/1000000: episode: 3636, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000241, mae: 0.014582, mean_q: 0.008275
  27316/1000000: episode: 3637, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000244, mae: 0.011541, mean_q: 0.011254
  27318/1000000: episode: 3638, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000636, mae: 0.017244, mean_q: 0.020978
  27320/1000000: episode: 3639, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000401, mae: 0.015945, mean_q: 0.000071
  27322/1000000: episode: 3640, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000108, mae: 0.009735, mean_q: 0.002203
  27326/1000000: episode: 3641, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.001455, mae: 0.022534, mean_q: 0.032765
  27330/1000000: episode: 3642, duration: 0.021s, episode steps: 4, steps per second: 186, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000128, mae: 0.009256, mean_q: 0.005550
  27337/1000000: episode: 3643, duration: 0.034s, episode steps: 7, steps per second: 209, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000247, mae: 0.011195, mean_q: 0.013133
  27341/1000000: episode: 3644, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000362, mae: 0.013284, mean_q: 0.015750
  27345/1000000: episode: 3645, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001311, mae: 0.017210, mean_q: 0.014671
  27347/1000000: episode: 3646, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000645, mae: 0.017846, mean_q: 0.029154
  27354/1000000: episode: 3647, duration: 0.033s, episode steps: 7, steps per second: 210, episode reward: 0.112, mean reward: 0.016 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000478, mae: 0.014014, mean_q: 0.016802
  27358/1000000: episode: 3648, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000451, mae: 0.012792, mean_q: 0.013171
  27365/1000000: episode: 3649, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000555, mae: 0.015936, mean_q: 0.016826
  27369/1000000: episode: 3650, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000300, mae: 0.009731, mean_q: 0.012296
  27371/1000000: episode: 3651, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000640, mae: 0.016981, mean_q: 0.029876
  27375/1000000: episode: 3652, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: 0.068, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.125 [-1.000, 11.000], loss: 0.000097, mae: 0.009003, mean_q: 0.011191
  27379/1000000: episode: 3653, duration: 0.021s, episode steps: 4, steps per second: 189, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.001350, mae: 0.017329, mean_q: 0.025047
  27381/1000000: episode: 3654, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000180, mae: 0.010045, mean_q: 0.014094
  27383/1000000: episode: 3655, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001010, mae: 0.019106, mean_q: 0.016166
  27390/1000000: episode: 3656, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.053, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.714 [-1.000, 11.000], loss: 0.000722, mae: 0.015478, mean_q: 0.020089
  27397/1000000: episode: 3657, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000420, mae: 0.012232, mean_q: 0.014319
  27404/1000000: episode: 3658, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.132, mean reward: 0.019 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000370, mae: 0.010237, mean_q: 0.013536
  27408/1000000: episode: 3659, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000089, mae: 0.007658, mean_q: 0.011322
  27415/1000000: episode: 3660, duration: 0.033s, episode steps: 7, steps per second: 211, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000440, mae: 0.011780, mean_q: 0.016827
  27422/1000000: episode: 3661, duration: 0.033s, episode steps: 7, steps per second: 215, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000289, mae: 0.013028, mean_q: 0.014934
[Info] 2-TH LEVEL FOUND: 0.09312352538108826, Considering 23/100 traces
  27429/1000000: episode: 3662, duration: 0.652s, episode steps: 7, steps per second: 11, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000368, mae: 0.012669, mean_q: 0.017640
  27431/1000000: episode: 3663, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001760, mae: 0.017441, mean_q: 0.029204
  27433/1000000: episode: 3664, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000436, mae: 0.013281, mean_q: 0.021126
  27438/1000000: episode: 3665, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.001041, mae: 0.017414, mean_q: 0.022175
  27440/1000000: episode: 3666, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001834, mae: 0.026251, mean_q: 0.015986
  27442/1000000: episode: 3667, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000340, mae: 0.013882, mean_q: 0.016923
  27444/1000000: episode: 3668, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000479, mae: 0.014214, mean_q: 0.018160
  27446/1000000: episode: 3669, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000600, mae: 0.022690, mean_q: 0.000823
  27448/1000000: episode: 3670, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000300, mae: 0.014058, mean_q: 0.019190
  27450/1000000: episode: 3671, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000652, mae: 0.017987, mean_q: 0.024711
  27455/1000000: episode: 3672, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000308, mae: 0.016780, mean_q: 0.001115
  27457/1000000: episode: 3673, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000959, mae: 0.021558, mean_q: 0.026445
  27459/1000000: episode: 3674, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001205, mae: 0.025954, mean_q: -0.002085
  27461/1000000: episode: 3675, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000195, mae: 0.013019, mean_q: 0.016753
  27463/1000000: episode: 3676, duration: 0.012s, episode steps: 2, steps per second: 169, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000473, mae: 0.023496, mean_q: 0.032473
[Info] FALSIFICATION!
  27467/1000000: episode: 3677, duration: 0.171s, episode steps: 4, steps per second: 23, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.001391, mae: 0.017728, mean_q: 0.021577
  27469/1000000: episode: 3678, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000243, mae: 0.014722, mean_q: 0.021047
  27471/1000000: episode: 3679, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000120, mae: 0.008750, mean_q: 0.010935
  27476/1000000: episode: 3680, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.001372, mae: 0.017251, mean_q: 0.027340
  27478/1000000: episode: 3681, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000615, mae: 0.016823, mean_q: 0.020147
  27483/1000000: episode: 3682, duration: 0.026s, episode steps: 5, steps per second: 195, episode reward: 0.087, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.200 [-1.000, 11.000], loss: 0.000295, mae: 0.011959, mean_q: 0.013965
  27485/1000000: episode: 3683, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000627, mae: 0.014146, mean_q: 0.007148
  27490/1000000: episode: 3684, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.001368, mae: 0.018775, mean_q: 0.025116
  27492/1000000: episode: 3685, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000713, mae: 0.014163, mean_q: 0.015759
  27494/1000000: episode: 3686, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000386, mae: 0.013038, mean_q: 0.019139
  27496/1000000: episode: 3687, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000496, mae: 0.012407, mean_q: 0.018417
  27498/1000000: episode: 3688, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000313, mae: 0.012476, mean_q: 0.018604
  27503/1000000: episode: 3689, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 1.922, mean reward: 0.384 [0.050, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 8.800 [7.000, 11.000], loss: 0.000234, mae: 0.008731, mean_q: 0.009724
  27505/1000000: episode: 3690, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000362, mae: 0.016581, mean_q: 0.021845
  27510/1000000: episode: 3691, duration: 0.024s, episode steps: 5, steps per second: 207, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000439, mae: 0.012492, mean_q: 0.013006
  27512/1000000: episode: 3692, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000202, mae: 0.010899, mean_q: 0.015310
  27517/1000000: episode: 3693, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001476, mae: 0.020237, mean_q: 0.031286
  27519/1000000: episode: 3694, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001632, mae: 0.017796, mean_q: 0.026833
  27521/1000000: episode: 3695, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002088, mae: 0.027615, mean_q: 0.035371
  27526/1000000: episode: 3696, duration: 0.024s, episode steps: 5, steps per second: 204, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.000912, mae: 0.015991, mean_q: 0.018372
  27528/1000000: episode: 3697, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000879, mae: 0.021212, mean_q: 0.017402
  27530/1000000: episode: 3698, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000242, mae: 0.012503, mean_q: 0.014213
  27532/1000000: episode: 3699, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000318, mae: 0.012413, mean_q: 0.013137
  27534/1000000: episode: 3700, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000727, mae: 0.013835, mean_q: 0.015002
  27536/1000000: episode: 3701, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000308, mae: 0.011905, mean_q: 0.016974
  27538/1000000: episode: 3702, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000446, mae: 0.017007, mean_q: 0.021894
  27540/1000000: episode: 3703, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000564, mae: 0.013035, mean_q: 0.015708
  27542/1000000: episode: 3704, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000162, mae: 0.008142, mean_q: 0.006436
  27544/1000000: episode: 3705, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000482, mae: 0.014398, mean_q: 0.012279
  27546/1000000: episode: 3706, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000355, mae: 0.016957, mean_q: 0.017243
  27551/1000000: episode: 3707, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000721, mae: 0.017205, mean_q: 0.013200
  27553/1000000: episode: 3708, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000205, mae: 0.010719, mean_q: 0.016094
  27555/1000000: episode: 3709, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000122, mae: 0.008715, mean_q: 0.014290
  27557/1000000: episode: 3710, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000126, mae: 0.009586, mean_q: 0.001430
  27562/1000000: episode: 3711, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.001275, mae: 0.018610, mean_q: 0.024758
  27564/1000000: episode: 3712, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000127, mae: 0.009273, mean_q: 0.005893
  27566/1000000: episode: 3713, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000197, mae: 0.009469, mean_q: 0.017336
[Info] FALSIFICATION!
  27570/1000000: episode: 3714, duration: 0.172s, episode steps: 4, steps per second: 23, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.000418, mae: 0.014164, mean_q: 0.015902
  27575/1000000: episode: 3715, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 0.321, mean reward: 0.064 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.600 [-1.000, 11.000], loss: 0.000492, mae: 0.015708, mean_q: 0.015602
  27580/1000000: episode: 3716, duration: 0.026s, episode steps: 5, steps per second: 193, episode reward: 0.149, mean reward: 0.030 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000982, mae: 0.014474, mean_q: 0.016875
  27585/1000000: episode: 3717, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 0.055, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.100 [-1.000, 11.000], loss: 0.000370, mae: 0.014577, mean_q: 0.014839
  27587/1000000: episode: 3718, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000763, mae: 0.012607, mean_q: 0.020875
  27589/1000000: episode: 3719, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000183, mae: 0.010504, mean_q: 0.013469
  27594/1000000: episode: 3720, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000483, mae: 0.014452, mean_q: 0.014357
  27599/1000000: episode: 3721, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.204, mean reward: 0.041 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000410, mae: 0.012121, mean_q: 0.010544
  27601/1000000: episode: 3722, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000154, mae: 0.010352, mean_q: 0.013767
  27603/1000000: episode: 3723, duration: 0.012s, episode steps: 2, steps per second: 170, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000215, mae: 0.012034, mean_q: 0.015785
  27605/1000000: episode: 3724, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000762, mae: 0.019702, mean_q: 0.010579
  27610/1000000: episode: 3725, duration: 0.024s, episode steps: 5, steps per second: 205, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001450, mae: 0.021485, mean_q: 0.026476
  27612/1000000: episode: 3726, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000774, mae: 0.019382, mean_q: 0.023126
[Info] FALSIFICATION!
  27616/1000000: episode: 3727, duration: 0.258s, episode steps: 4, steps per second: 15, episode reward: 1.553, mean reward: 0.388 [0.050, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 8.500 [7.000, 10.000], loss: 0.000466, mae: 0.014052, mean_q: 0.028027
  27618/1000000: episode: 3728, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000166, mae: 0.008284, mean_q: 0.004716
  27623/1000000: episode: 3729, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000360, mae: 0.013556, mean_q: 0.011970
  27625/1000000: episode: 3730, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000386, mae: 0.014580, mean_q: 0.014058
  27630/1000000: episode: 3731, duration: 0.025s, episode steps: 5, steps per second: 197, episode reward: 0.118, mean reward: 0.024 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.300 [-1.000, 11.000], loss: 0.000431, mae: 0.015756, mean_q: 0.015255
  27632/1000000: episode: 3732, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.002461, mae: 0.024517, mean_q: 0.018365
  27637/1000000: episode: 3733, duration: 0.025s, episode steps: 5, steps per second: 204, episode reward: 0.235, mean reward: 0.047 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.500 [-1.000, 11.000], loss: 0.001339, mae: 0.021744, mean_q: 0.029779
  27639/1000000: episode: 3734, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000544, mae: 0.017615, mean_q: 0.016786
  27644/1000000: episode: 3735, duration: 0.025s, episode steps: 5, steps per second: 199, episode reward: 0.149, mean reward: 0.030 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.400 [-1.000, 11.000], loss: 0.000824, mae: 0.018216, mean_q: 0.024406
  27646/1000000: episode: 3736, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000692, mae: 0.017861, mean_q: 0.010560
  27648/1000000: episode: 3737, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000383, mae: 0.018003, mean_q: 0.021366
  27650/1000000: episode: 3738, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000360, mae: 0.013990, mean_q: 0.011934
[Info] Complete ISplit Iteration
[Info] Levels: [0.02166897, 0.093123525, 0.6821426]
[Info] Cond. Prob: [0.14, 0.23, 0.03]
[Info] Error Prob: 0.0009660000000000002

  27652/1000000: episode: 3739, duration: 0.873s, episode steps: 2, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001088, mae: 0.020785, mean_q: 0.020066
  27662/1000000: episode: 3740, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000973, mae: 0.018868, mean_q: 0.022113
  27672/1000000: episode: 3741, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000554, mae: 0.016597, mean_q: 0.015997
  27682/1000000: episode: 3742, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000837, mae: 0.019134, mean_q: 0.024473
  27692/1000000: episode: 3743, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001916, mae: 0.025144, mean_q: 0.027177
  27702/1000000: episode: 3744, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001384, mae: 0.016752, mean_q: 0.014292
  27712/1000000: episode: 3745, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000531, mae: 0.017001, mean_q: 0.015414
  27722/1000000: episode: 3746, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000797, mae: 0.019061, mean_q: 0.014570
  27732/1000000: episode: 3747, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000752, mae: 0.020118, mean_q: 0.021257
  27742/1000000: episode: 3748, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000803, mae: 0.018673, mean_q: 0.016347
  27752/1000000: episode: 3749, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000810, mae: 0.015361, mean_q: 0.017142
  27762/1000000: episode: 3750, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.001262, mae: 0.022726, mean_q: 0.020864
  27772/1000000: episode: 3751, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000972, mae: 0.019196, mean_q: 0.019300
  27782/1000000: episode: 3752, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000803, mae: 0.018273, mean_q: 0.014199
  27792/1000000: episode: 3753, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000330, mae: 0.013128, mean_q: 0.010015
  27802/1000000: episode: 3754, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001030, mae: 0.019798, mean_q: 0.024731
  27812/1000000: episode: 3755, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000680, mae: 0.016426, mean_q: 0.012265
  27822/1000000: episode: 3756, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.001076, mae: 0.019938, mean_q: 0.021633
  27832/1000000: episode: 3757, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000806, mae: 0.020350, mean_q: 0.020086
  27842/1000000: episode: 3758, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000771, mae: 0.017024, mean_q: 0.016975
  27852/1000000: episode: 3759, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000382, mae: 0.013864, mean_q: 0.014910
  27862/1000000: episode: 3760, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000888, mae: 0.018721, mean_q: 0.018308
  27872/1000000: episode: 3761, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000823, mae: 0.015320, mean_q: 0.014387
  27882/1000000: episode: 3762, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000427, mae: 0.015179, mean_q: 0.014631
  27892/1000000: episode: 3763, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000771, mae: 0.017502, mean_q: 0.019188
  27902/1000000: episode: 3764, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.042, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000413, mae: 0.013739, mean_q: 0.009825
  27912/1000000: episode: 3765, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000346, mae: 0.011597, mean_q: 0.011743
  27922/1000000: episode: 3766, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000370, mae: 0.012719, mean_q: 0.016598
  27932/1000000: episode: 3767, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000417, mae: 0.014417, mean_q: 0.016752
  27942/1000000: episode: 3768, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000974, mae: 0.019802, mean_q: 0.016818
  27952/1000000: episode: 3769, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000293, mae: 0.016140, mean_q: 0.011389
  27962/1000000: episode: 3770, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000448, mae: 0.013976, mean_q: 0.009045
  27972/1000000: episode: 3771, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000842, mae: 0.015396, mean_q: 0.018234
  27982/1000000: episode: 3772, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000310, mae: 0.012225, mean_q: 0.014654
  27992/1000000: episode: 3773, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.055, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000679, mae: 0.014199, mean_q: 0.011725
  28002/1000000: episode: 3774, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000576, mae: 0.015509, mean_q: 0.016063
[Info] FALSIFICATION!
  28012/1000000: episode: 3775, duration: 0.412s, episode steps: 10, steps per second: 24, episode reward: 1.582, mean reward: 0.158 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [1.000, 10.000], loss: 0.000270, mae: 0.010228, mean_q: 0.008767
  28022/1000000: episode: 3776, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000348, mae: 0.013835, mean_q: 0.013492
  28032/1000000: episode: 3777, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000315, mae: 0.010489, mean_q: 0.012534
  28042/1000000: episode: 3778, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000506, mae: 0.013489, mean_q: 0.013344
  28052/1000000: episode: 3779, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000237, mae: 0.010063, mean_q: 0.009990
  28062/1000000: episode: 3780, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000168, mae: 0.009455, mean_q: 0.009899
  28072/1000000: episode: 3781, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000959, mae: 0.014363, mean_q: 0.018413
  28082/1000000: episode: 3782, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000671, mae: 0.013895, mean_q: 0.020270
  28092/1000000: episode: 3783, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000831, mae: 0.018401, mean_q: 0.014439
  28102/1000000: episode: 3784, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.215, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000365, mae: 0.012022, mean_q: 0.012136
  28112/1000000: episode: 3785, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000247, mae: 0.010671, mean_q: 0.012576
  28122/1000000: episode: 3786, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000588, mae: 0.014456, mean_q: 0.017027
  28132/1000000: episode: 3787, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001298, mae: 0.016526, mean_q: 0.022434
  28142/1000000: episode: 3788, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000571, mae: 0.013224, mean_q: 0.014917
  28152/1000000: episode: 3789, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000679, mae: 0.015340, mean_q: 0.015466
  28162/1000000: episode: 3790, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000549, mae: 0.013744, mean_q: 0.014898
  28172/1000000: episode: 3791, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000816, mae: 0.019813, mean_q: 0.020200
  28182/1000000: episode: 3792, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000477, mae: 0.016096, mean_q: 0.010618
  28192/1000000: episode: 3793, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000309, mae: 0.010698, mean_q: 0.010612
  28202/1000000: episode: 3794, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000642, mae: 0.011901, mean_q: 0.013391
  28212/1000000: episode: 3795, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.001041, mae: 0.014577, mean_q: 0.020336
  28222/1000000: episode: 3796, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000829, mae: 0.014557, mean_q: 0.012469
  28232/1000000: episode: 3797, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.052, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000290, mae: 0.011666, mean_q: 0.012202
  28242/1000000: episode: 3798, duration: 0.044s, episode steps: 10, steps per second: 225, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000313, mae: 0.011387, mean_q: 0.010417
  28252/1000000: episode: 3799, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000428, mae: 0.012028, mean_q: 0.014253
  28262/1000000: episode: 3800, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000318, mae: 0.010599, mean_q: 0.010403
  28272/1000000: episode: 3801, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000242, mae: 0.009751, mean_q: 0.009645
  28282/1000000: episode: 3802, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000905, mae: 0.017137, mean_q: 0.018153
  28292/1000000: episode: 3803, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001526, mae: 0.018710, mean_q: 0.015603
  28302/1000000: episode: 3804, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000540, mae: 0.014663, mean_q: 0.010308
  28312/1000000: episode: 3805, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000676, mae: 0.015342, mean_q: 0.010524
  28322/1000000: episode: 3806, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.035, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000353, mae: 0.014943, mean_q: 0.014294
  28332/1000000: episode: 3807, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.147, mean reward: 0.015 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 10.000], loss: 0.000491, mae: 0.015278, mean_q: 0.014749
  28342/1000000: episode: 3808, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000403, mae: 0.012237, mean_q: 0.011363
  28352/1000000: episode: 3809, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000351, mae: 0.010880, mean_q: 0.011678
  28362/1000000: episode: 3810, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000161, mae: 0.009665, mean_q: 0.009135
  28372/1000000: episode: 3811, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000992, mae: 0.015763, mean_q: 0.017693
  28382/1000000: episode: 3812, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000633, mae: 0.013991, mean_q: 0.013593
  28392/1000000: episode: 3813, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000392, mae: 0.012900, mean_q: 0.015324
  28402/1000000: episode: 3814, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000188, mae: 0.011232, mean_q: 0.010237
  28412/1000000: episode: 3815, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000405, mae: 0.012177, mean_q: 0.010263
  28422/1000000: episode: 3816, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000796, mae: 0.015263, mean_q: 0.016904
  28432/1000000: episode: 3817, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.034, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000545, mae: 0.014296, mean_q: 0.015453
  28442/1000000: episode: 3818, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000391, mae: 0.011832, mean_q: 0.011136
  28452/1000000: episode: 3819, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000494, mae: 0.013911, mean_q: 0.016574
  28462/1000000: episode: 3820, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001523, mae: 0.015438, mean_q: 0.017429
  28472/1000000: episode: 3821, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.001672, mae: 0.019947, mean_q: 0.021150
  28482/1000000: episode: 3822, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000399, mae: 0.014373, mean_q: 0.013454
  28492/1000000: episode: 3823, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000212, mae: 0.012093, mean_q: 0.014360
  28502/1000000: episode: 3824, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000656, mae: 0.011782, mean_q: 0.016094
  28512/1000000: episode: 3825, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000507, mae: 0.011757, mean_q: 0.012895
  28522/1000000: episode: 3826, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000535, mae: 0.012787, mean_q: 0.015549
  28532/1000000: episode: 3827, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.072, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: 0.000534, mae: 0.011160, mean_q: 0.013194
  28542/1000000: episode: 3828, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000397, mae: 0.013949, mean_q: 0.012324
  28552/1000000: episode: 3829, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000231, mae: 0.012272, mean_q: 0.009545
  28562/1000000: episode: 3830, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000731, mae: 0.015975, mean_q: 0.016475
  28572/1000000: episode: 3831, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000721, mae: 0.014540, mean_q: 0.020021
  28582/1000000: episode: 3832, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000406, mae: 0.012207, mean_q: 0.011981
  28592/1000000: episode: 3833, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000453, mae: 0.011328, mean_q: 0.015558
  28602/1000000: episode: 3834, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000244, mae: 0.010647, mean_q: 0.012781
  28612/1000000: episode: 3835, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000585, mae: 0.016948, mean_q: 0.017868
  28622/1000000: episode: 3836, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000670, mae: 0.017562, mean_q: 0.016818
  28632/1000000: episode: 3837, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000296, mae: 0.013647, mean_q: 0.012840
  28642/1000000: episode: 3838, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000624, mae: 0.013602, mean_q: 0.019978
[Info] Complete ISplit Iteration
[Info] Levels: [0.7046777]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

  28652/1000000: episode: 3839, duration: 0.723s, episode steps: 10, steps per second: 14, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000237, mae: 0.011139, mean_q: 0.010614
  28662/1000000: episode: 3840, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000688, mae: 0.014966, mean_q: 0.021735
  28672/1000000: episode: 3841, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000623, mae: 0.016810, mean_q: 0.014522
  28682/1000000: episode: 3842, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000792, mae: 0.014555, mean_q: 0.012693
  28692/1000000: episode: 3843, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000619, mae: 0.013041, mean_q: 0.015521
  28702/1000000: episode: 3844, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000592, mae: 0.015075, mean_q: 0.017186
  28712/1000000: episode: 3845, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000429, mae: 0.012911, mean_q: 0.011281
  28722/1000000: episode: 3846, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000478, mae: 0.014535, mean_q: 0.013768
  28732/1000000: episode: 3847, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000490, mae: 0.013867, mean_q: 0.015845
  28742/1000000: episode: 3848, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001730, mae: 0.016414, mean_q: 0.018691
  28752/1000000: episode: 3849, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000483, mae: 0.012529, mean_q: 0.016898
  28762/1000000: episode: 3850, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000898, mae: 0.014762, mean_q: 0.016619
  28772/1000000: episode: 3851, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000377, mae: 0.012361, mean_q: 0.012908
  28782/1000000: episode: 3852, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000420, mae: 0.012940, mean_q: 0.012326
  28792/1000000: episode: 3853, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000330, mae: 0.011155, mean_q: 0.010584
  28802/1000000: episode: 3854, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000617, mae: 0.012610, mean_q: 0.011375
  28812/1000000: episode: 3855, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000397, mae: 0.012885, mean_q: 0.012371
  28822/1000000: episode: 3856, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000211, mae: 0.009962, mean_q: 0.011173
  28832/1000000: episode: 3857, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000801, mae: 0.015694, mean_q: 0.018351
  28842/1000000: episode: 3858, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000305, mae: 0.012705, mean_q: 0.009737
  28852/1000000: episode: 3859, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000344, mae: 0.015235, mean_q: 0.014973
  28862/1000000: episode: 3860, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000570, mae: 0.014012, mean_q: 0.015190
  28872/1000000: episode: 3861, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000223, mae: 0.010655, mean_q: 0.009928
  28882/1000000: episode: 3862, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000373, mae: 0.011049, mean_q: 0.013675
  28892/1000000: episode: 3863, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000284, mae: 0.009964, mean_q: 0.009954
  28902/1000000: episode: 3864, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.047, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001050, mae: 0.017266, mean_q: 0.019439
  28912/1000000: episode: 3865, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 0.054, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000702, mae: 0.014290, mean_q: 0.014496
  28922/1000000: episode: 3866, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000652, mae: 0.015047, mean_q: 0.016584
  28932/1000000: episode: 3867, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.084, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000099, mae: 0.007686, mean_q: 0.007607
  28942/1000000: episode: 3868, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000247, mae: 0.009494, mean_q: 0.011988
  28952/1000000: episode: 3869, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000542, mae: 0.015079, mean_q: 0.016095
  28962/1000000: episode: 3870, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000675, mae: 0.015499, mean_q: 0.017099
  28972/1000000: episode: 3871, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000604, mae: 0.014232, mean_q: 0.014244
  28982/1000000: episode: 3872, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000559, mae: 0.013404, mean_q: 0.010590
  28992/1000000: episode: 3873, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000194, mae: 0.011172, mean_q: 0.009000
  29002/1000000: episode: 3874, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001013, mae: 0.015987, mean_q: 0.018444
  29012/1000000: episode: 3875, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.000402, mae: 0.013263, mean_q: 0.014893
  29022/1000000: episode: 3876, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000814, mae: 0.015688, mean_q: 0.013028
  29032/1000000: episode: 3877, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001304, mae: 0.016928, mean_q: 0.020977
  29042/1000000: episode: 3878, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000501, mae: 0.015414, mean_q: 0.015239
  29052/1000000: episode: 3879, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000739, mae: 0.018043, mean_q: 0.014209
  29062/1000000: episode: 3880, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000385, mae: 0.014166, mean_q: 0.012370
  29072/1000000: episode: 3881, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000425, mae: 0.012268, mean_q: 0.012744
  29082/1000000: episode: 3882, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000757, mae: 0.013138, mean_q: 0.016646
  29092/1000000: episode: 3883, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000291, mae: 0.009764, mean_q: 0.012019
  29102/1000000: episode: 3884, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000384, mae: 0.012538, mean_q: 0.014324
  29112/1000000: episode: 3885, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000362, mae: 0.010612, mean_q: 0.012104
  29122/1000000: episode: 3886, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000578, mae: 0.013107, mean_q: 0.013137
  29132/1000000: episode: 3887, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000539, mae: 0.013012, mean_q: 0.014055
  29142/1000000: episode: 3888, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000344, mae: 0.011238, mean_q: 0.013853
  29152/1000000: episode: 3889, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000690, mae: 0.014571, mean_q: 0.017139
  29162/1000000: episode: 3890, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.015, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000769, mae: 0.012294, mean_q: 0.017286
  29172/1000000: episode: 3891, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000622, mae: 0.013035, mean_q: 0.015493
  29182/1000000: episode: 3892, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000378, mae: 0.013193, mean_q: 0.009523
  29192/1000000: episode: 3893, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000255, mae: 0.009151, mean_q: 0.008223
  29202/1000000: episode: 3894, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000190, mae: 0.009092, mean_q: 0.009791
  29212/1000000: episode: 3895, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000394, mae: 0.012936, mean_q: 0.012873
  29222/1000000: episode: 3896, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.002101, mae: 0.016206, mean_q: 0.015728
  29232/1000000: episode: 3897, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000925, mae: 0.022153, mean_q: 0.015753
  29242/1000000: episode: 3898, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000355, mae: 0.016850, mean_q: 0.009926
  29252/1000000: episode: 3899, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000455, mae: 0.013879, mean_q: 0.008308
  29262/1000000: episode: 3900, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000399, mae: 0.012297, mean_q: 0.012710
  29272/1000000: episode: 3901, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.217, mean reward: 0.022 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000615, mae: 0.013823, mean_q: 0.015292
  29282/1000000: episode: 3902, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000324, mae: 0.011857, mean_q: 0.010718
  29292/1000000: episode: 3903, duration: 0.045s, episode steps: 10, steps per second: 225, episode reward: 0.026, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000238, mae: 0.009703, mean_q: 0.011651
  29302/1000000: episode: 3904, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000492, mae: 0.011539, mean_q: 0.014181
  29312/1000000: episode: 3905, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000130, mae: 0.007627, mean_q: 0.008841
  29322/1000000: episode: 3906, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000229, mae: 0.009766, mean_q: 0.012325
  29332/1000000: episode: 3907, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001290, mae: 0.014155, mean_q: 0.014958
  29342/1000000: episode: 3908, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000410, mae: 0.011981, mean_q: 0.012900
  29352/1000000: episode: 3909, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000844, mae: 0.016164, mean_q: 0.018781
  29362/1000000: episode: 3910, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.001284, mae: 0.024824, mean_q: 0.017708
  29372/1000000: episode: 3911, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000556, mae: 0.017913, mean_q: 0.012929
  29382/1000000: episode: 3912, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000740, mae: 0.017043, mean_q: 0.013866
  29392/1000000: episode: 3913, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.001005, mae: 0.019989, mean_q: 0.016209
  29402/1000000: episode: 3914, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.002112, mae: 0.022621, mean_q: 0.014790
  29412/1000000: episode: 3915, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000536, mae: 0.021723, mean_q: 0.017317
  29422/1000000: episode: 3916, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000722, mae: 0.018605, mean_q: 0.015182
  29432/1000000: episode: 3917, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000545, mae: 0.015964, mean_q: 0.014123
  29442/1000000: episode: 3918, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000255, mae: 0.011969, mean_q: 0.011186
  29452/1000000: episode: 3919, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000397, mae: 0.012378, mean_q: 0.010535
  29462/1000000: episode: 3920, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: 0.000355, mae: 0.012105, mean_q: 0.012096
  29472/1000000: episode: 3921, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000310, mae: 0.011706, mean_q: 0.009392
  29482/1000000: episode: 3922, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000632, mae: 0.016440, mean_q: 0.013834
  29492/1000000: episode: 3923, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000285, mae: 0.012649, mean_q: 0.011856
  29502/1000000: episode: 3924, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000641, mae: 0.015129, mean_q: 0.015922
  29512/1000000: episode: 3925, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000332, mae: 0.011462, mean_q: 0.011383
  29522/1000000: episode: 3926, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000645, mae: 0.013193, mean_q: 0.013965
  29532/1000000: episode: 3927, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000376, mae: 0.012449, mean_q: 0.010870
  29542/1000000: episode: 3928, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000481, mae: 0.012702, mean_q: 0.012879
  29552/1000000: episode: 3929, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000691, mae: 0.013008, mean_q: 0.014072
  29562/1000000: episode: 3930, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000356, mae: 0.014316, mean_q: 0.014506
  29572/1000000: episode: 3931, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000368, mae: 0.014396, mean_q: 0.010603
  29582/1000000: episode: 3932, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.084, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.650 [-1.000, 10.000], loss: 0.000610, mae: 0.013259, mean_q: 0.013426
  29592/1000000: episode: 3933, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.001305, mae: 0.017621, mean_q: 0.016148
  29602/1000000: episode: 3934, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000527, mae: 0.019851, mean_q: 0.008811
  29612/1000000: episode: 3935, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000357, mae: 0.013893, mean_q: 0.009813
  29622/1000000: episode: 3936, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000217, mae: 0.011508, mean_q: 0.011366
  29632/1000000: episode: 3937, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.080, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000851, mae: 0.016302, mean_q: 0.017818
  29642/1000000: episode: 3938, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000717, mae: 0.011449, mean_q: 0.011779
[Info] 1-TH LEVEL FOUND: 0.027214229106903076, Considering 10/100 traces
  29652/1000000: episode: 3939, duration: 0.792s, episode steps: 10, steps per second: 13, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000607, mae: 0.013545, mean_q: 0.013888
  29654/1000000: episode: 3940, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000166, mae: 0.007596, mean_q: 0.007032
  29661/1000000: episode: 3941, duration: 0.034s, episode steps: 7, steps per second: 205, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000491, mae: 0.012893, mean_q: 0.010103
  29663/1000000: episode: 3942, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001375, mae: 0.021049, mean_q: 0.028688
  29665/1000000: episode: 3943, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001604, mae: 0.022082, mean_q: 0.032051
  29667/1000000: episode: 3944, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000191, mae: 0.012186, mean_q: 0.001319
  29674/1000000: episode: 3945, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 0.048, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000924, mae: 0.018394, mean_q: 0.021365
  29681/1000000: episode: 3946, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000743, mae: 0.016909, mean_q: 0.020992
  29683/1000000: episode: 3947, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000490, mae: 0.012589, mean_q: 0.008859
  29690/1000000: episode: 3948, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.357 [-1.000, 11.000], loss: 0.000425, mae: 0.012115, mean_q: 0.013906
  29697/1000000: episode: 3949, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000209, mae: 0.009413, mean_q: 0.011188
  29699/1000000: episode: 3950, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000103, mae: 0.008354, mean_q: 0.007633
  29701/1000000: episode: 3951, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000340, mae: 0.010373, mean_q: 0.009531
  29703/1000000: episode: 3952, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000143, mae: 0.007697, mean_q: 0.007908
  29705/1000000: episode: 3953, duration: 0.030s, episode steps: 2, steps per second: 66, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000721, mae: 0.016554, mean_q: 0.014775
  29712/1000000: episode: 3954, duration: 0.077s, episode steps: 7, steps per second: 91, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000517, mae: 0.012566, mean_q: 0.015162
  29719/1000000: episode: 3955, duration: 0.083s, episode steps: 7, steps per second: 84, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000489, mae: 0.013741, mean_q: 0.016309
  29721/1000000: episode: 3956, duration: 0.026s, episode steps: 2, steps per second: 75, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000289, mae: 0.008735, mean_q: 0.007209
  29723/1000000: episode: 3957, duration: 0.037s, episode steps: 2, steps per second: 54, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000788, mae: 0.016203, mean_q: 0.017499
  29725/1000000: episode: 3958, duration: 0.032s, episode steps: 2, steps per second: 62, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000329, mae: 0.010990, mean_q: 0.010018
  29727/1000000: episode: 3959, duration: 0.028s, episode steps: 2, steps per second: 72, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000432, mae: 0.014427, mean_q: 0.013311
  29729/1000000: episode: 3960, duration: 0.030s, episode steps: 2, steps per second: 67, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000434, mae: 0.014040, mean_q: 0.012243
  29731/1000000: episode: 3961, duration: 0.028s, episode steps: 2, steps per second: 71, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000401, mae: 0.013704, mean_q: 0.008023
  29733/1000000: episode: 3962, duration: 0.036s, episode steps: 2, steps per second: 56, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000172, mae: 0.010033, mean_q: 0.013998
  29735/1000000: episode: 3963, duration: 0.030s, episode steps: 2, steps per second: 67, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000909, mae: 0.011436, mean_q: 0.015954
  29737/1000000: episode: 3964, duration: 0.027s, episode steps: 2, steps per second: 74, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000437, mae: 0.015007, mean_q: 0.014959
  29744/1000000: episode: 3965, duration: 0.077s, episode steps: 7, steps per second: 91, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000776, mae: 0.012728, mean_q: 0.015313
  29746/1000000: episode: 3966, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000296, mae: 0.011357, mean_q: 0.006616
  29748/1000000: episode: 3967, duration: 0.026s, episode steps: 2, steps per second: 78, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000386, mae: 0.011466, mean_q: 0.013488
  29755/1000000: episode: 3968, duration: 0.056s, episode steps: 7, steps per second: 125, episode reward: 0.143, mean reward: 0.020 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000672, mae: 0.012693, mean_q: 0.016734
  29757/1000000: episode: 3969, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.005433, mae: 0.025814, mean_q: 0.012320
  29759/1000000: episode: 3970, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000077, mae: 0.008148, mean_q: 0.021000
  29761/1000000: episode: 3971, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001389, mae: 0.023345, mean_q: 0.032358
  29763/1000000: episode: 3972, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000116, mae: 0.011615, mean_q: 0.005856
  29765/1000000: episode: 3973, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000892, mae: 0.018071, mean_q: 0.009942
  29772/1000000: episode: 3974, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000303, mae: 0.011669, mean_q: 0.014032
  29779/1000000: episode: 3975, duration: 0.034s, episode steps: 7, steps per second: 207, episode reward: 0.041, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.000234, mae: 0.008872, mean_q: 0.010893
  29786/1000000: episode: 3976, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.034, mean reward: 0.005 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.643 [-1.000, 11.000], loss: 0.001255, mae: 0.013659, mean_q: 0.017303
  29788/1000000: episode: 3977, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000155, mae: 0.010341, mean_q: 0.005602
  29790/1000000: episode: 3978, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000456, mae: 0.010675, mean_q: 0.010343
  29797/1000000: episode: 3979, duration: 0.033s, episode steps: 7, steps per second: 214, episode reward: 0.217, mean reward: 0.031 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.071 [-1.000, 11.000], loss: 0.000665, mae: 0.013343, mean_q: 0.015861
  29799/1000000: episode: 3980, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000268, mae: 0.010526, mean_q: 0.008312
  29806/1000000: episode: 3981, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.346, mean reward: 0.049 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.286 [-1.000, 11.000], loss: 0.000356, mae: 0.012648, mean_q: 0.016967
  29813/1000000: episode: 3982, duration: 0.033s, episode steps: 7, steps per second: 209, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000211, mae: 0.012374, mean_q: 0.014600
  29820/1000000: episode: 3983, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.229, mean reward: 0.033 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000609, mae: 0.012657, mean_q: 0.015920
  29822/1000000: episode: 3984, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000172, mae: 0.011224, mean_q: 0.006942
  29824/1000000: episode: 3985, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000273, mae: 0.009690, mean_q: 0.005721
  29826/1000000: episode: 3986, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000185, mae: 0.009588, mean_q: 0.012315
  29828/1000000: episode: 3987, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000255, mae: 0.012042, mean_q: 0.015429
  29830/1000000: episode: 3988, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000655, mae: 0.015948, mean_q: 0.022439
  29832/1000000: episode: 3989, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000179, mae: 0.011865, mean_q: 0.018964
  29839/1000000: episode: 3990, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 0.080, mean reward: 0.011 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000435, mae: 0.013440, mean_q: 0.013052
  29846/1000000: episode: 3991, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000659, mae: 0.013552, mean_q: 0.012812
  29848/1000000: episode: 3992, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000228, mae: 0.010631, mean_q: 0.010449
  29855/1000000: episode: 3993, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 0.127, mean reward: 0.018 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000438, mae: 0.013748, mean_q: 0.013160
  29857/1000000: episode: 3994, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000400, mae: 0.014248, mean_q: 0.015863
  29859/1000000: episode: 3995, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000296, mae: 0.010266, mean_q: 0.005406
  29861/1000000: episode: 3996, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000179, mae: 0.008865, mean_q: 0.009115
  29868/1000000: episode: 3997, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 0.260, mean reward: 0.037 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.214 [-1.000, 11.000], loss: 0.000797, mae: 0.018338, mean_q: 0.022970
  29875/1000000: episode: 3998, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000593, mae: 0.015071, mean_q: 0.014890
  29877/1000000: episode: 3999, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000780, mae: 0.017842, mean_q: 0.031672
  29879/1000000: episode: 4000, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000102, mae: 0.009890, mean_q: 0.002149
  29886/1000000: episode: 4001, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 0.100, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.929 [-1.000, 11.000], loss: 0.000622, mae: 0.014706, mean_q: 0.017773
  29888/1000000: episode: 4002, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000361, mae: 0.017851, mean_q: 0.013802
  29890/1000000: episode: 4003, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000653, mae: 0.014185, mean_q: 0.020232
  29892/1000000: episode: 4004, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000428, mae: 0.012437, mean_q: 0.024835
  29894/1000000: episode: 4005, duration: 0.013s, episode steps: 2, steps per second: 156, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000221, mae: 0.011732, mean_q: 0.008485
  29896/1000000: episode: 4006, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000288, mae: 0.013822, mean_q: 0.009106
  29903/1000000: episode: 4007, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000469, mae: 0.013758, mean_q: 0.012537
  29910/1000000: episode: 4008, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000269, mae: 0.011612, mean_q: 0.014551
  29912/1000000: episode: 4009, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000135, mae: 0.011531, mean_q: 0.005124
  29914/1000000: episode: 4010, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000373, mae: 0.014202, mean_q: 0.015078
  29916/1000000: episode: 4011, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000237, mae: 0.013891, mean_q: 0.018816
  29923/1000000: episode: 4012, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.213, mean reward: 0.030 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.001055, mae: 0.012875, mean_q: 0.013052
[Info] FALSIFICATION!
  29929/1000000: episode: 4013, duration: 0.195s, episode steps: 6, steps per second: 31, episode reward: 1.578, mean reward: 0.263 [0.007, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 7.500 [5.000, 10.000], loss: 0.000372, mae: 0.014386, mean_q: 0.014571
  29936/1000000: episode: 4014, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 0.021, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.429 [-1.000, 11.000], loss: 0.000313, mae: 0.012748, mean_q: 0.016445
  29943/1000000: episode: 4015, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 0.033, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.500 [-1.000, 11.000], loss: 0.000419, mae: 0.012468, mean_q: 0.014066
  29950/1000000: episode: 4016, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.175, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.143 [-1.000, 11.000], loss: 0.000785, mae: 0.013409, mean_q: 0.016473
  29952/1000000: episode: 4017, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000176, mae: 0.012133, mean_q: 0.019695
  29959/1000000: episode: 4018, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.012, mean reward: 0.002 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 5.286 [-1.000, 11.000], loss: 0.000720, mae: 0.014140, mean_q: 0.017545
  29961/1000000: episode: 4019, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000537, mae: 0.013711, mean_q: 0.008185
  29968/1000000: episode: 4020, duration: 0.032s, episode steps: 7, steps per second: 217, episode reward: 0.084, mean reward: 0.012 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.003379, mae: 0.030567, mean_q: 0.031354
  29970/1000000: episode: 4021, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.001017, mae: 0.029460, mean_q: 0.011107
  29972/1000000: episode: 4022, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000100, mae: 0.010078, mean_q: 0.014980
  29979/1000000: episode: 4023, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000573, mae: 0.018126, mean_q: 0.010825
  29981/1000000: episode: 4024, duration: 0.012s, episode steps: 2, steps per second: 166, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000985, mae: 0.021851, mean_q: 0.029246
  29988/1000000: episode: 4025, duration: 0.033s, episode steps: 7, steps per second: 212, episode reward: 0.069, mean reward: 0.010 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.857 [-1.000, 11.000], loss: 0.000680, mae: 0.015643, mean_q: 0.013108
  29990/1000000: episode: 4026, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000350, mae: 0.018530, mean_q: 0.022279
  29997/1000000: episode: 4027, duration: 0.033s, episode steps: 7, steps per second: 213, episode reward: 0.057, mean reward: 0.008 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.786 [-1.000, 11.000], loss: 0.000651, mae: 0.015091, mean_q: 0.011755
  29999/1000000: episode: 4028, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000182, mae: 0.009673, mean_q: 0.010579
[Info] Complete ISplit Iteration
[Info] Levels: [0.02721423, 0.7267506]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

  30006/1000000: episode: 4029, duration: 0.842s, episode steps: 7, steps per second: 8, episode reward: 0.029, mean reward: 0.004 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 5.571 [-1.000, 11.000], loss: 0.000719, mae: 0.013762, mean_q: 0.017226
  30016/1000000: episode: 4030, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000513, mae: 0.014369, mean_q: 0.010912
  30026/1000000: episode: 4031, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000773, mae: 0.020472, mean_q: 0.014625
  30036/1000000: episode: 4032, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000477, mae: 0.017614, mean_q: 0.013628
  30046/1000000: episode: 4033, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.097, mean reward: 0.010 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.001629, mae: 0.021394, mean_q: 0.020788
  30056/1000000: episode: 4034, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000532, mae: 0.018267, mean_q: 0.016816
  30066/1000000: episode: 4035, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000598, mae: 0.019484, mean_q: 0.015224
  30076/1000000: episode: 4036, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000537, mae: 0.016704, mean_q: 0.014717
  30086/1000000: episode: 4037, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.023, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000305, mae: 0.010598, mean_q: 0.010823
  30096/1000000: episode: 4038, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.016, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000306, mae: 0.011750, mean_q: 0.012015
  30106/1000000: episode: 4039, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001301, mae: 0.020446, mean_q: 0.019974
  30116/1000000: episode: 4040, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000742, mae: 0.018069, mean_q: 0.017308
  30126/1000000: episode: 4041, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.006, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000491, mae: 0.014716, mean_q: 0.015714
  30136/1000000: episode: 4042, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000647, mae: 0.015999, mean_q: 0.015797
  30146/1000000: episode: 4043, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000637, mae: 0.018236, mean_q: 0.015634
  30156/1000000: episode: 4044, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000946, mae: 0.015815, mean_q: 0.015625
  30166/1000000: episode: 4045, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000397, mae: 0.012933, mean_q: 0.015295
  30176/1000000: episode: 4046, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000673, mae: 0.016617, mean_q: 0.009024
  30186/1000000: episode: 4047, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000490, mae: 0.013855, mean_q: 0.011336
  30196/1000000: episode: 4048, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000396, mae: 0.011663, mean_q: 0.010786
  30206/1000000: episode: 4049, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.032, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000368, mae: 0.012221, mean_q: 0.013061
  30216/1000000: episode: 4050, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000242, mae: 0.009250, mean_q: 0.010383
  30226/1000000: episode: 4051, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000655, mae: 0.010906, mean_q: 0.010485
  30236/1000000: episode: 4052, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001795, mae: 0.021169, mean_q: 0.017938
  30246/1000000: episode: 4053, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000728, mae: 0.016493, mean_q: 0.014449
  30256/1000000: episode: 4054, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000693, mae: 0.014823, mean_q: 0.012771
  30266/1000000: episode: 4055, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000740, mae: 0.012854, mean_q: 0.014282
  30276/1000000: episode: 4056, duration: 0.072s, episode steps: 10, steps per second: 140, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000454, mae: 0.015541, mean_q: 0.016077
  30286/1000000: episode: 4057, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000494, mae: 0.011914, mean_q: 0.014061
  30296/1000000: episode: 4058, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 0.233, mean reward: 0.023 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.800 [-1.000, 10.000], loss: 0.000296, mae: 0.011020, mean_q: 0.011968
  30306/1000000: episode: 4059, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000456, mae: 0.012697, mean_q: 0.014721
  30316/1000000: episode: 4060, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001055, mae: 0.016925, mean_q: 0.021525
  30326/1000000: episode: 4061, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000378, mae: 0.010601, mean_q: 0.010853
  30336/1000000: episode: 4062, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000328, mae: 0.012336, mean_q: 0.009058
  30346/1000000: episode: 4063, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001024, mae: 0.017110, mean_q: 0.017725
  30356/1000000: episode: 4064, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001312, mae: 0.011964, mean_q: 0.010195
  30366/1000000: episode: 4065, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.049, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.350 [-1.000, 10.000], loss: 0.000291, mae: 0.011950, mean_q: 0.011681
  30376/1000000: episode: 4066, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000319, mae: 0.009362, mean_q: 0.009208
  30386/1000000: episode: 4067, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000217, mae: 0.007847, mean_q: 0.011253
  30396/1000000: episode: 4068, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000306, mae: 0.011524, mean_q: 0.012304
  30406/1000000: episode: 4069, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000306, mae: 0.009558, mean_q: 0.011785
  30416/1000000: episode: 4070, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000286, mae: 0.009999, mean_q: 0.010527
  30426/1000000: episode: 4071, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.081, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.001078, mae: 0.013412, mean_q: 0.017922
  30436/1000000: episode: 4072, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000490, mae: 0.011845, mean_q: 0.012851
  30446/1000000: episode: 4073, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001180, mae: 0.010862, mean_q: 0.011201
  30456/1000000: episode: 4074, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.264, mean reward: 0.026 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.850 [-1.000, 10.000], loss: 0.000739, mae: 0.014670, mean_q: 0.017862
  30466/1000000: episode: 4075, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000438, mae: 0.011956, mean_q: 0.012395
  30476/1000000: episode: 4076, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000415, mae: 0.011334, mean_q: 0.009756
  30486/1000000: episode: 4077, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000518, mae: 0.011964, mean_q: 0.011631
  30496/1000000: episode: 4078, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000314, mae: 0.010315, mean_q: 0.011368
  30506/1000000: episode: 4079, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: 0.000263, mae: 0.008827, mean_q: 0.007941
  30516/1000000: episode: 4080, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000350, mae: 0.009664, mean_q: 0.010946
  30526/1000000: episode: 4081, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000228, mae: 0.010213, mean_q: 0.009888
  30536/1000000: episode: 4082, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.055, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001120, mae: 0.015761, mean_q: 0.018173
  30546/1000000: episode: 4083, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.001276, mae: 0.014993, mean_q: 0.016112
  30556/1000000: episode: 4084, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000510, mae: 0.015397, mean_q: 0.009595
  30566/1000000: episode: 4085, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.001505, mae: 0.013698, mean_q: 0.013028
  30576/1000000: episode: 4086, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000974, mae: 0.018545, mean_q: 0.020934
  30586/1000000: episode: 4087, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000513, mae: 0.010130, mean_q: 0.017049
  30596/1000000: episode: 4088, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000784, mae: 0.014888, mean_q: 0.015451
  30606/1000000: episode: 4089, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000341, mae: 0.009910, mean_q: 0.010155
  30616/1000000: episode: 4090, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000392, mae: 0.011083, mean_q: 0.011360
  30626/1000000: episode: 4091, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000162, mae: 0.008318, mean_q: 0.007460
  30636/1000000: episode: 4092, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000383, mae: 0.009303, mean_q: 0.008506
  30646/1000000: episode: 4093, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.015, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000485, mae: 0.009617, mean_q: 0.011178
  30656/1000000: episode: 4094, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.001101, mae: 0.016353, mean_q: 0.019036
  30666/1000000: episode: 4095, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000314, mae: 0.012505, mean_q: 0.007323
  30676/1000000: episode: 4096, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000694, mae: 0.014458, mean_q: 0.015611
  30686/1000000: episode: 4097, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000762, mae: 0.014928, mean_q: 0.014517
  30696/1000000: episode: 4098, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000588, mae: 0.015181, mean_q: 0.015395
  30706/1000000: episode: 4099, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000168, mae: 0.010473, mean_q: 0.009018
  30716/1000000: episode: 4100, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000506, mae: 0.013949, mean_q: 0.011832
  30726/1000000: episode: 4101, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000608, mae: 0.010835, mean_q: 0.010891
  30736/1000000: episode: 4102, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000356, mae: 0.008866, mean_q: 0.011305
  30746/1000000: episode: 4103, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000491, mae: 0.010025, mean_q: 0.012124
  30756/1000000: episode: 4104, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000458, mae: 0.013390, mean_q: 0.015422
  30766/1000000: episode: 4105, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000603, mae: 0.011444, mean_q: 0.010541
  30776/1000000: episode: 4106, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000196, mae: 0.009490, mean_q: 0.009789
  30786/1000000: episode: 4107, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000216, mae: 0.008138, mean_q: 0.010829
  30796/1000000: episode: 4108, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000619, mae: 0.013066, mean_q: 0.015671
  30806/1000000: episode: 4109, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000378, mae: 0.010919, mean_q: 0.011142
  30816/1000000: episode: 4110, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000359, mae: 0.011521, mean_q: 0.011785
  30826/1000000: episode: 4111, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000132, mae: 0.007533, mean_q: 0.007940
  30836/1000000: episode: 4112, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000311, mae: 0.010765, mean_q: 0.012289
  30846/1000000: episode: 4113, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000354, mae: 0.010800, mean_q: 0.009386
  30856/1000000: episode: 4114, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.084, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.000470, mae: 0.011489, mean_q: 0.012215
  30866/1000000: episode: 4115, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000193, mae: 0.009602, mean_q: 0.008647
  30876/1000000: episode: 4116, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000372, mae: 0.010660, mean_q: 0.012453
  30886/1000000: episode: 4117, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000366, mae: 0.010329, mean_q: 0.011743
  30896/1000000: episode: 4118, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000163, mae: 0.008261, mean_q: 0.009645
  30906/1000000: episode: 4119, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000368, mae: 0.011435, mean_q: 0.014419
  30916/1000000: episode: 4120, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000236, mae: 0.008167, mean_q: 0.009369
  30926/1000000: episode: 4121, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000387, mae: 0.009669, mean_q: 0.011599
  30936/1000000: episode: 4122, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000201, mae: 0.008921, mean_q: 0.010183
  30946/1000000: episode: 4123, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.031, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000826, mae: 0.014643, mean_q: 0.019357
  30956/1000000: episode: 4124, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000370, mae: 0.012246, mean_q: 0.012688
  30966/1000000: episode: 4125, duration: 0.048s, episode steps: 10, steps per second: 208, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000389, mae: 0.010906, mean_q: 0.010694
  30976/1000000: episode: 4126, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001492, mae: 0.016591, mean_q: 0.013436
  30986/1000000: episode: 4127, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000293, mae: 0.009846, mean_q: 0.010780
  30996/1000000: episode: 4128, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000548, mae: 0.013121, mean_q: 0.012540
[Info] 1-TH LEVEL FOUND: 0.025555670261383057, Considering 14/100 traces
done, took 187.750 seconds
[Info] End Importance Splitting. Falsification occurred 30 times.
