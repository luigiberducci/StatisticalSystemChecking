> [0;32m/home/luigi/Development/StatisticalSystemChecking/RL/gym-success_runs/gym_success_runs/envs/SuccessRunsEnv.py[0m(13)[0;36m__init__[0;34m()[0m
[0;32m     12 [0;31m[0;34m[0m[0m
[0m[0;32m---> 13 [0;31m        [0mself[0m[0;34m.[0m[0msys[0m [0;34m=[0m [0mFSA[0m[0;34m([0m[0mP[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     14 [0;31m        [0mDUMMY_ACTION[0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> ipdb> Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                48        
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting on succruns-v1.
Training for 1000000 steps ...
     10/1000000: episode: 1, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     20/1000000: episode: 2, duration: 0.005s, episode steps: 10, steps per second: 1910, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     30/1000000: episode: 3, duration: 0.005s, episode steps: 10, steps per second: 1944, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     40/1000000: episode: 4, duration: 0.005s, episode steps: 10, steps per second: 2040, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     50/1000000: episode: 5, duration: 0.005s, episode steps: 10, steps per second: 2103, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     60/1000000: episode: 6, duration: 0.005s, episode steps: 10, steps per second: 2100, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     70/1000000: episode: 7, duration: 0.005s, episode steps: 10, steps per second: 2009, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.350 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     80/1000000: episode: 8, duration: 0.005s, episode steps: 10, steps per second: 2058, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
     90/1000000: episode: 9, duration: 0.005s, episode steps: 10, steps per second: 2082, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    100/1000000: episode: 10, duration: 0.005s, episode steps: 10, steps per second: 2035, episode reward: 0.066, mean reward: 0.007 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    110/1000000: episode: 11, duration: 0.005s, episode steps: 10, steps per second: 2096, episode reward: 0.024, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    120/1000000: episode: 12, duration: 0.005s, episode steps: 10, steps per second: 2205, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    130/1000000: episode: 13, duration: 0.004s, episode steps: 10, steps per second: 2231, episode reward: 0.050, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    140/1000000: episode: 14, duration: 0.005s, episode steps: 10, steps per second: 2218, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    150/1000000: episode: 15, duration: 0.005s, episode steps: 10, steps per second: 2204, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    160/1000000: episode: 16, duration: 0.005s, episode steps: 10, steps per second: 2218, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    170/1000000: episode: 17, duration: 0.004s, episode steps: 10, steps per second: 2232, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    180/1000000: episode: 18, duration: 0.005s, episode steps: 10, steps per second: 2220, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    190/1000000: episode: 19, duration: 0.005s, episode steps: 10, steps per second: 2108, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    200/1000000: episode: 20, duration: 0.005s, episode steps: 10, steps per second: 2101, episode reward: 0.214, mean reward: 0.021 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    210/1000000: episode: 21, duration: 0.005s, episode steps: 10, steps per second: 2060, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    220/1000000: episode: 22, duration: 0.005s, episode steps: 10, steps per second: 2026, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    230/1000000: episode: 23, duration: 0.005s, episode steps: 10, steps per second: 2180, episode reward: 0.092, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    240/1000000: episode: 24, duration: 0.005s, episode steps: 10, steps per second: 2156, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    250/1000000: episode: 25, duration: 0.005s, episode steps: 10, steps per second: 2117, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    260/1000000: episode: 26, duration: 0.005s, episode steps: 10, steps per second: 2194, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    270/1000000: episode: 27, duration: 0.005s, episode steps: 10, steps per second: 2197, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    280/1000000: episode: 28, duration: 0.005s, episode steps: 10, steps per second: 2214, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    290/1000000: episode: 29, duration: 0.005s, episode steps: 10, steps per second: 2218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    300/1000000: episode: 30, duration: 0.005s, episode steps: 10, steps per second: 2178, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    310/1000000: episode: 31, duration: 0.006s, episode steps: 10, steps per second: 1765, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    320/1000000: episode: 32, duration: 0.005s, episode steps: 10, steps per second: 1843, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    330/1000000: episode: 33, duration: 0.005s, episode steps: 10, steps per second: 2166, episode reward: 0.021, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    340/1000000: episode: 34, duration: 0.005s, episode steps: 10, steps per second: 2008, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    350/1000000: episode: 35, duration: 0.005s, episode steps: 10, steps per second: 2122, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    360/1000000: episode: 36, duration: 0.005s, episode steps: 10, steps per second: 2180, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    370/1000000: episode: 37, duration: 0.005s, episode steps: 10, steps per second: 2105, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    380/1000000: episode: 38, duration: 0.005s, episode steps: 10, steps per second: 2167, episode reward: 0.010, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    390/1000000: episode: 39, duration: 0.005s, episode steps: 10, steps per second: 2146, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    400/1000000: episode: 40, duration: 0.005s, episode steps: 10, steps per second: 2106, episode reward: 0.022, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    410/1000000: episode: 41, duration: 0.005s, episode steps: 10, steps per second: 2141, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    420/1000000: episode: 42, duration: 0.005s, episode steps: 10, steps per second: 2137, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    430/1000000: episode: 43, duration: 0.005s, episode steps: 10, steps per second: 2113, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    440/1000000: episode: 44, duration: 0.005s, episode steps: 10, steps per second: 2112, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    450/1000000: episode: 45, duration: 0.005s, episode steps: 10, steps per second: 2134, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    460/1000000: episode: 46, duration: 0.005s, episode steps: 10, steps per second: 2126, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    470/1000000: episode: 47, duration: 0.005s, episode steps: 10, steps per second: 2166, episode reward: 0.004, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.300 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    480/1000000: episode: 48, duration: 0.005s, episode steps: 10, steps per second: 2222, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    490/1000000: episode: 49, duration: 0.004s, episode steps: 10, steps per second: 2226, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    500/1000000: episode: 50, duration: 0.004s, episode steps: 10, steps per second: 2234, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: --, mae: --, mean_q: --
    510/1000000: episode: 51, duration: 0.597s, episode steps: 10, steps per second: 17, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.393556, mae: 0.677310, mean_q: -0.856246
    520/1000000: episode: 52, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.139661, mae: 0.376368, mean_q: -0.540783
    530/1000000: episode: 53, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.052456, mae: 0.191827, mean_q: -0.301856
    540/1000000: episode: 54, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.019529, mae: 0.149099, mean_q: -0.134704
    550/1000000: episode: 55, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.500 [-1.000, 10.000], loss: 0.017715, mae: 0.150656, mean_q: -0.061759
    560/1000000: episode: 56, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.012096, mae: 0.131336, mean_q: -0.046768
    570/1000000: episode: 57, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.009664, mae: 0.113589, mean_q: -0.070939
    580/1000000: episode: 58, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.009691, mae: 0.101217, mean_q: -0.088742
    590/1000000: episode: 59, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.010150, mae: 0.097000, mean_q: -0.083657
    600/1000000: episode: 60, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.005448, mae: 0.083956, mean_q: -0.060218
    610/1000000: episode: 61, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.008823, mae: 0.088833, mean_q: -0.078069
    620/1000000: episode: 62, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.004212, mae: 0.071329, mean_q: -0.039807
    630/1000000: episode: 63, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.005150, mae: 0.069431, mean_q: -0.050781
    640/1000000: episode: 64, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.005499, mae: 0.069769, mean_q: -0.050135

Program interrupted. (Use 'cont' to resume).
> [0;32m/home/luigi/.local/lib/python3.6/site-packages/rl/agents/dqn.py[0m(315)[0;36mbackward[0;34m()[0m
[0;32m    314 [0;31m            [0;32mfor[0m [0midx[0m[0;34m,[0m [0;34m([0m[0mtarget[0m[0;34m,[0m [0mmask[0m[0;34m,[0m [0mR[0m[0;34m,[0m [0maction[0m[0;34m)[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mzip[0m[0;34m([0m[0mtargets[0m[0;34m,[0m [0mmasks[0m[0;34m,[0m [0mRs[0m[0;34m,[0m [0maction_batch[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 315 [0;31m                [0mtarget[0m[0;34m[[0m[0maction[0m[0;34m][0m [0;34m=[0m [0mR[0m  [0;31m# update action with estimated accumulated reward[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    316 [0;31m                [0mdummy_targets[0m[0;34m[[0m[0midx[0m[0;34m][0m [0;34m=[0m [0mR[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb>     650/1000000: episode: 65, duration: 0.305s, episode steps: 10, steps per second: 33, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.003579, mae: 0.057901, mean_q: -0.034787
    660/1000000: episode: 66, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.004047, mae: 0.059576, mean_q: -0.042228
    670/1000000: episode: 67, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.002641, mae: 0.050285, mean_q: -0.031859
    680/1000000: episode: 68, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.038, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.002860, mae: 0.047813, mean_q: -0.031970
    690/1000000: episode: 69, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.002530, mae: 0.048856, mean_q: -0.033018
    700/1000000: episode: 70, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.001661, mae: 0.042420, mean_q: -0.024729
    710/1000000: episode: 71, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.001725, mae: 0.040094, mean_q: -0.023824
    720/1000000: episode: 72, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.082, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.001447, mae: 0.038892, mean_q: -0.025745
    730/1000000: episode: 73, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.000 [-1.000, 10.000], loss: 0.001588, mae: 0.038627, mean_q: -0.022047
    740/1000000: episode: 74, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.002015, mae: 0.039999, mean_q: -0.014710
    750/1000000: episode: 75, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.011, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.002025, mae: 0.041788, mean_q: -0.016926
    760/1000000: episode: 76, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001530, mae: 0.035158, mean_q: -0.011534
    770/1000000: episode: 77, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.025, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.001237, mae: 0.033135, mean_q: -0.010361
    780/1000000: episode: 78, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.001443, mae: 0.034623, mean_q: -0.013259
    790/1000000: episode: 79, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.001194, mae: 0.033642, mean_q: -0.003298
    800/1000000: episode: 80, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000776, mae: 0.026958, mean_q: -0.006758
    810/1000000: episode: 81, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000719, mae: 0.024981, mean_q: -0.012679
    820/1000000: episode: 82, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.700 [-1.000, 10.000], loss: 0.000938, mae: 0.027705, mean_q: -0.006015
    830/1000000: episode: 83, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000858, mae: 0.027823, mean_q: -0.000616
    840/1000000: episode: 84, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001114, mae: 0.028664, mean_q: -0.005273
    850/1000000: episode: 85, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000641, mae: 0.025769, mean_q: -0.006413
    860/1000000: episode: 86, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.086, mean reward: 0.009 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000769, mae: 0.025968, mean_q: -0.004939
    870/1000000: episode: 87, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000756, mae: 0.027406, mean_q: -0.006220
    880/1000000: episode: 88, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000621, mae: 0.023377, mean_q: -0.001840
    890/1000000: episode: 89, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000751, mae: 0.023094, mean_q: -0.004712
    900/1000000: episode: 90, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.025, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000661, mae: 0.024078, mean_q: -0.002090
    910/1000000: episode: 91, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000511, mae: 0.020614, mean_q: 0.003605
    920/1000000: episode: 92, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 0.033, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000473, mae: 0.020468, mean_q: -0.003009
    930/1000000: episode: 93, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.135, mean reward: 0.014 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.700 [-1.000, 10.000], loss: 0.000645, mae: 0.019883, mean_q: -0.003006
    940/1000000: episode: 94, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000493, mae: 0.019632, mean_q: 0.003493
    950/1000000: episode: 95, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.350, mean reward: 0.035 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 4.900 [-1.000, 10.000], loss: 0.000493, mae: 0.019382, mean_q: -0.000627
    960/1000000: episode: 96, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000871, mae: 0.021935, mean_q: -0.004665
    970/1000000: episode: 97, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.001373, mae: 0.025383, mean_q: 0.002889
    980/1000000: episode: 98, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000924, mae: 0.025512, mean_q: 0.003630
    990/1000000: episode: 99, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.850 [-1.000, 10.000], loss: 0.000762, mae: 0.019219, mean_q: 0.000233
Step 1000: saving model to out/succruns/gio21nov2019_13_26_08_CET/models/weights.1000.hdf5
   1000/1000000: episode: 100, duration: 0.077s, episode steps: 10, steps per second: 129, episode reward: 0.029, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000498, mae: 0.019049, mean_q: 0.000298
   1010/1000000: episode: 101, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.012, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000275, mae: 0.015907, mean_q: 0.002870
   1020/1000000: episode: 102, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000340, mae: 0.014970, mean_q: -0.002242
   1030/1000000: episode: 103, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.450 [-1.000, 10.000], loss: 0.000755, mae: 0.018190, mean_q: -0.001447
   1040/1000000: episode: 104, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000422, mae: 0.021654, mean_q: 0.006445
   1050/1000000: episode: 105, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000352, mae: 0.014156, mean_q: 0.001777
   1060/1000000: episode: 106, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000482, mae: 0.011393, mean_q: -0.000248
   1070/1000000: episode: 107, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000622, mae: 0.017468, mean_q: 0.001755
   1080/1000000: episode: 108, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.050 [-1.000, 10.000], loss: 0.000995, mae: 0.023096, mean_q: 0.001677
   1090/1000000: episode: 109, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000468, mae: 0.018004, mean_q: 0.006597
   1100/1000000: episode: 110, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000336, mae: 0.012901, mean_q: 0.001419
   1110/1000000: episode: 111, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000531, mae: 0.015595, mean_q: -0.000109
   1120/1000000: episode: 112, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.013, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.650 [-1.000, 10.000], loss: 0.000174, mae: 0.012335, mean_q: 0.003192
   1130/1000000: episode: 113, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.150 [-1.000, 10.000], loss: 0.000144, mae: 0.009908, mean_q: 0.000513
   1140/1000000: episode: 114, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000157, mae: 0.010594, mean_q: 0.002294
   1150/1000000: episode: 115, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.011, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000215, mae: 0.011131, mean_q: 0.001060
   1160/1000000: episode: 116, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000139, mae: 0.010587, mean_q: 0.004046
   1170/1000000: episode: 117, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000351, mae: 0.010121, mean_q: 0.000982
   1180/1000000: episode: 118, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000161, mae: 0.011487, mean_q: 0.005463
   1190/1000000: episode: 119, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.043, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000293, mae: 0.011627, mean_q: 0.000353
   1200/1000000: episode: 120, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.600 [-1.000, 10.000], loss: 0.000436, mae: 0.012143, mean_q: 0.004033

Program interrupted. (Use 'cont' to resume).
--Return--
None
> [0;32m/home/luigi/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py[0m(2960)[0;36mbatch_set_value[0;34m()[0m
[0;32m   2959 [0;31m    """
[0m[0;32m-> 2960 [0;31m    [0mtf_keras_backend[0m[0;34m.[0m[0mbatch_set_value[0m[0;34m([0m[0mtuples[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m   2961 [0;31m[0;34m[0m[0m
[0m
ipdb>    1210/1000000: episode: 121, duration: 25.022s, episode steps: 10, steps per second: 0, episode reward: 0.005, mean reward: 0.000 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000109, mae: 0.009310, mean_q: 0.003894
   1220/1000000: episode: 122, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.250 [-1.000, 10.000], loss: 0.000123, mae: 0.009436, mean_q: 0.002227
   1230/1000000: episode: 123, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000181, mae: 0.009922, mean_q: 0.001524
   1240/1000000: episode: 124, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000386, mae: 0.010330, mean_q: 0.002050
   1250/1000000: episode: 125, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.037, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000168, mae: 0.009536, mean_q: 0.005873
   1260/1000000: episode: 126, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000083, mae: 0.006902, mean_q: 0.000739
   1270/1000000: episode: 127, duration: 0.046s, episode steps: 10, steps per second: 215, episode reward: 0.004, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000402, mae: 0.010420, mean_q: 0.001854
   1280/1000000: episode: 128, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000123, mae: 0.009731, mean_q: 0.005227
   1290/1000000: episode: 129, duration: 0.045s, episode steps: 10, steps per second: 220, episode reward: 0.005, mean reward: 0.001 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000121, mae: 0.007615, mean_q: 0.000397
   1300/1000000: episode: 130, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 0.027, mean reward: 0.003 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000074, mae: 0.006320, mean_q: 0.003088
   1310/1000000: episode: 131, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.013, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000215, mae: 0.007259, mean_q: 0.002567
   1320/1000000: episode: 132, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000133, mae: 0.007228, mean_q: 0.002638
   1330/1000000: episode: 133, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.020, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000231, mae: 0.008041, mean_q: 0.002863
   1340/1000000: episode: 134, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.129, mean reward: 0.013 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.450 [-1.000, 10.000], loss: 0.000057, mae: 0.006747, mean_q: 0.004569
   1350/1000000: episode: 135, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000233, mae: 0.008157, mean_q: 0.001909
   1360/1000000: episode: 136, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000202, mae: 0.006975, mean_q: 0.003503
   1370/1000000: episode: 137, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000037, mae: 0.005305, mean_q: 0.005015
   1380/1000000: episode: 138, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000134, mae: 0.006310, mean_q: 0.001792
   1390/1000000: episode: 139, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.018, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000219, mae: 0.008592, mean_q: 0.004673
   1400/1000000: episode: 140, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.400 [-1.000, 10.000], loss: 0.000182, mae: 0.006897, mean_q: 0.002650
   1410/1000000: episode: 141, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.005, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.550 [-1.000, 10.000], loss: 0.000129, mae: 0.006312, mean_q: 0.001980
   1420/1000000: episode: 142, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000043, mae: 0.005574, mean_q: 0.003102
   1430/1000000: episode: 143, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000071, mae: 0.005330, mean_q: 0.002008
   1440/1000000: episode: 144, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.003, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000132, mae: 0.005882, mean_q: 0.002433
   1450/1000000: episode: 145, duration: 0.047s, episode steps: 10, steps per second: 215, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.950 [-1.000, 10.000], loss: 0.000043, mae: 0.005521, mean_q: 0.001889
   1460/1000000: episode: 146, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.030, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.900 [-1.000, 10.000], loss: 0.000036, mae: 0.004517, mean_q: 0.000821
   1470/1000000: episode: 147, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 0.009, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.850 [-1.000, 10.000], loss: 0.000107, mae: 0.005204, mean_q: 0.002907
   1480/1000000: episode: 148, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.002, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.200 [-1.000, 10.000], loss: 0.000034, mae: 0.005064, mean_q: 0.003845
   1490/1000000: episode: 149, duration: 0.045s, episode steps: 10, steps per second: 222, episode reward: 0.061, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.550 [-1.000, 10.000], loss: 0.000040, mae: 0.004439, mean_q: 0.001057
[Info] 1-TH LEVEL FOUND: 0.025474179536104202, Considering 20/100 traces
   1500/1000000: episode: 150, duration: 1.126s, episode steps: 10, steps per second: 9, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 3.950 [-1.000, 10.000], loss: 0.000181, mae: 0.007247, mean_q: 0.003133
   1503/1000000: episode: 151, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000075, mae: 0.008022, mean_q: 0.007697
   1505/1000000: episode: 152, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000070, mae: 0.008247, mean_q: 0.008282
   1507/1000000: episode: 153, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000064, mae: 0.005597, mean_q: 0.003410
   1510/1000000: episode: 154, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000053, mae: 0.005179, mean_q: 0.001142
   1512/1000000: episode: 155, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000629, mae: 0.009964, mean_q: -0.002140
   1514/1000000: episode: 156, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000033, mae: 0.005157, mean_q: 0.005644
   1518/1000000: episode: 157, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000032, mae: 0.005876, mean_q: 0.006321
   1521/1000000: episode: 158, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000278, mae: 0.007803, mean_q: 0.001404
   1524/1000000: episode: 159, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000060, mae: 0.006159, mean_q: 0.003280
   1526/1000000: episode: 160, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000023, mae: 0.004179, mean_q: 0.002432
   1528/1000000: episode: 161, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000045, mae: 0.005368, mean_q: 0.000618
   1530/1000000: episode: 162, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000022, mae: 0.004273, mean_q: 0.001587
   1534/1000000: episode: 163, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000099, mae: 0.007215, mean_q: 0.000841
   1537/1000000: episode: 164, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000412, mae: 0.009029, mean_q: 0.003557
   1540/1000000: episode: 165, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000048, mae: 0.006227, mean_q: 0.005514
   1543/1000000: episode: 166, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000057, mae: 0.006737, mean_q: 0.006822
   1547/1000000: episode: 167, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000076, mae: 0.005672, mean_q: 0.000261
   1549/1000000: episode: 168, duration: 0.013s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000042, mae: 0.005463, mean_q: 0.001871
   1551/1000000: episode: 169, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000070, mae: 0.006682, mean_q: -0.000200
   1553/1000000: episode: 170, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000910, mae: 0.011879, mean_q: -0.000666
   1555/1000000: episode: 171, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000046, mae: 0.006540, mean_q: 0.006021
   1557/1000000: episode: 172, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000045, mae: 0.007403, mean_q: 0.008962
   1559/1000000: episode: 173, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000062, mae: 0.007469, mean_q: 0.006134
   1561/1000000: episode: 174, duration: 0.012s, episode steps: 2, steps per second: 164, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000019, mae: 0.004234, mean_q: 0.004092
   1563/1000000: episode: 175, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000032, mae: 0.004891, mean_q: -0.000553
   1566/1000000: episode: 176, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000058, mae: 0.006243, mean_q: -0.002726
   1568/1000000: episode: 177, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000027, mae: 0.005165, mean_q: 0.000854
   1571/1000000: episode: 178, duration: 0.017s, episode steps: 3, steps per second: 181, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000026, mae: 0.004538, mean_q: 0.001946
   1573/1000000: episode: 179, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000030, mae: 0.004854, mean_q: 0.002634
   1575/1000000: episode: 180, duration: 0.012s, episode steps: 2, steps per second: 160, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000029, mae: 0.004909, mean_q: 0.002558
   1577/1000000: episode: 181, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000036, mae: 0.004865, mean_q: 0.003478
   1581/1000000: episode: 182, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.185, mean reward: 0.046 [0.000, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 6.375 [-1.000, 11.000], loss: 0.000046, mae: 0.005883, mean_q: 0.002479
   1584/1000000: episode: 183, duration: 0.016s, episode steps: 3, steps per second: 185, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000016, mae: 0.003701, mean_q: 0.002307
   1587/1000000: episode: 184, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000015, mae: 0.003635, mean_q: 0.001477
   1589/1000000: episode: 185, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000138, mae: 0.005873, mean_q: 0.001355
   1592/1000000: episode: 186, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000061, mae: 0.004801, mean_q: 0.000559
   1594/1000000: episode: 187, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000026, mae: 0.004073, mean_q: 0.004095
   1596/1000000: episode: 188, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000029, mae: 0.004961, mean_q: 0.005675
   1600/1000000: episode: 189, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 0.037, mean reward: 0.009 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 6.000 [-1.000, 11.000], loss: 0.000209, mae: 0.006358, mean_q: 0.002573
   1603/1000000: episode: 190, duration: 0.017s, episode steps: 3, steps per second: 176, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000021, mae: 0.003877, mean_q: 0.002760
   1607/1000000: episode: 191, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000035, mae: 0.004610, mean_q: 0.002040
   1609/1000000: episode: 192, duration: 0.012s, episode steps: 2, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000121, mae: 0.006818, mean_q: -0.001118
   1612/1000000: episode: 193, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000039, mae: 0.004858, mean_q: 0.003863
   1614/1000000: episode: 194, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000013, mae: 0.003983, mean_q: 0.006033
   1616/1000000: episode: 195, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000049, mae: 0.006372, mean_q: 0.006171
   1618/1000000: episode: 196, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000039, mae: 0.004947, mean_q: 0.003559
   1620/1000000: episode: 197, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000021, mae: 0.003688, mean_q: 0.003603
   1622/1000000: episode: 198, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000020, mae: 0.004073, mean_q: 0.001523
   1624/1000000: episode: 199, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000134, mae: 0.005702, mean_q: 0.002245
   1627/1000000: episode: 200, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000109, mae: 0.007921, mean_q: 0.001625
   1629/1000000: episode: 201, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000012, mae: 0.003363, mean_q: 0.002523
   1632/1000000: episode: 202, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000052, mae: 0.005602, mean_q: 0.005152
   1634/1000000: episode: 203, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000199, mae: 0.009045, mean_q: 0.004847
   1636/1000000: episode: 204, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000028, mae: 0.004855, mean_q: 0.005466
   1638/1000000: episode: 205, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000029, mae: 0.004614, mean_q: 0.004289
   1640/1000000: episode: 206, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000114, mae: 0.006504, mean_q: 0.004312
   1643/1000000: episode: 207, duration: 0.016s, episode steps: 3, steps per second: 182, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000029, mae: 0.004648, mean_q: 0.003911
   1645/1000000: episode: 208, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000045, mae: 0.005252, mean_q: 0.003283
   1647/1000000: episode: 209, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000046, mae: 0.005530, mean_q: 0.004874
   1650/1000000: episode: 210, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000104, mae: 0.006912, mean_q: 0.007656
   1652/1000000: episode: 211, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000054, mae: 0.006427, mean_q: 0.008096
   1654/1000000: episode: 212, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000060, mae: 0.007047, mean_q: 0.005923
   1657/1000000: episode: 213, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000025, mae: 0.004547, mean_q: 0.000828
   1660/1000000: episode: 214, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000032, mae: 0.005231, mean_q: -0.001657
   1662/1000000: episode: 215, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000054, mae: 0.005637, mean_q: -0.003071
   1665/1000000: episode: 216, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000018, mae: 0.004206, mean_q: 0.002075
   1668/1000000: episode: 217, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000051, mae: 0.005713, mean_q: 0.003800
   1671/1000000: episode: 218, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000306, mae: 0.007556, mean_q: 0.003249
   1675/1000000: episode: 219, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000089, mae: 0.006136, mean_q: 0.003772
   1677/1000000: episode: 220, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000029, mae: 0.004141, mean_q: 0.003471
   1679/1000000: episode: 221, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000111, mae: 0.006739, mean_q: 0.003519
   1681/1000000: episode: 222, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000047, mae: 0.005512, mean_q: 0.004672
   1684/1000000: episode: 223, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.018, mean reward: 0.006 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 5.667 [-1.000, 11.000], loss: 0.000015, mae: 0.003697, mean_q: 0.003068
   1686/1000000: episode: 224, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000024, mae: 0.004434, mean_q: 0.000224
   1688/1000000: episode: 225, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000080, mae: 0.005994, mean_q: 0.000837
   1690/1000000: episode: 226, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000045, mae: 0.004983, mean_q: 0.003523
   1693/1000000: episode: 227, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000034, mae: 0.005418, mean_q: 0.005246
   1696/1000000: episode: 228, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.050, mean reward: 0.017 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 5.833 [-1.000, 11.000], loss: 0.000113, mae: 0.007698, mean_q: 0.007870
   1698/1000000: episode: 229, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 4.750 [-1.000, 11.000], loss: 0.000049, mae: 0.005486, mean_q: 0.004588
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.025474179536104202
   1702/1000000: episode: 230, duration: 0.572s, episode steps: 4, steps per second: 7, episode reward: 0.100, mean reward: 0.025 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 6.250 [-1.000, 11.000], loss: 0.000014, mae: 0.003689, mean_q: 0.001028
   1712/1000000: episode: 231, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 0.012, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000065, mae: 0.005553, mean_q: 0.002987
   1722/1000000: episode: 232, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000093, mae: 0.006279, mean_q: 0.004038
   1732/1000000: episode: 233, duration: 0.045s, episode steps: 10, steps per second: 221, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000142, mae: 0.005667, mean_q: 0.003427
   1742/1000000: episode: 234, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.039, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.300 [-1.000, 10.000], loss: 0.000042, mae: 0.005299, mean_q: 0.003455
   1752/1000000: episode: 235, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.014, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000233, mae: 0.007305, mean_q: 0.005120
   1762/1000000: episode: 236, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.031, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.150 [-1.000, 10.000], loss: 0.000076, mae: 0.005894, mean_q: 0.003915
   1772/1000000: episode: 237, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.036, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.100 [-1.000, 10.000], loss: 0.000039, mae: 0.005464, mean_q: 0.003853
   1782/1000000: episode: 238, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 0.048, mean reward: 0.005 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000023, mae: 0.004686, mean_q: 0.001340
   1792/1000000: episode: 239, duration: 0.046s, episode steps: 10, steps per second: 216, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000052, mae: 0.005209, mean_q: 0.003522
   1802/1000000: episode: 240, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 0.019, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000037, mae: 0.004696, mean_q: 0.002599
   1812/1000000: episode: 241, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 0.017, mean reward: 0.002 [0.000, 0.007], mean action: 0.000 [0.000, 0.000], mean observation: 4.000 [-1.000, 10.000], loss: 0.000119, mae: 0.006538, mean_q: 0.005278
   1822/1000000: episode: 242, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.750 [-1.000, 10.000], loss: 0.000059, mae: 0.005054, mean_q: 0.003095
   1832/1000000: episode: 243, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.006, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.700 [-1.000, 10.000], loss: 0.000027, mae: 0.004335, mean_q: 0.003738
   1842/1000000: episode: 244, duration: 0.047s, episode steps: 10, steps per second: 211, episode reward: 0.079, mean reward: 0.008 [0.000, 0.050], mean action: 0.000 [0.000, 0.000], mean observation: 4.200 [-1.000, 10.000], loss: 0.000068, mae: 0.005680, mean_q: 0.003232
   1852/1000000: episode: 245, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 0.041, mean reward: 0.004 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.400 [-1.000, 10.000], loss: 0.000031, mae: 0.005093, mean_q: 0.004603
   1862/1000000: episode: 246, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.007, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.500 [-1.000, 10.000], loss: 0.000127, mae: 0.005676, mean_q: 0.001805
   1872/1000000: episode: 247, duration: 0.046s, episode steps: 10, steps per second: 217, episode reward: 0.002, mean reward: 0.000 [0.000, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 3.250 [-1.000, 10.000], loss: 0.000035, mae: 0.005594, mean_q: 0.005965
   1882/1000000: episode: 248, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 0.001, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.100 [-1.000, 10.000], loss: 0.000249, mae: 0.007402, mean_q: 0.001054
   1892/1000000: episode: 249, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 0.008, mean reward: 0.001 [0.000, 0.002], mean action: 0.000 [0.000, 0.000], mean observation: 3.800 [-1.000, 10.000], loss: 0.000054, mae: 0.006953, mean_q: 0.005397
   1902/1000000: episode: 250, duration: 0.046s, episode steps: 10, steps per second: 219, episode reward: 0.032, mean reward: 0.003 [0.000, 0.018], mean action: 0.000 [0.000, 0.000], mean observation: 4.050 [-1.000, 10.000], loss: 0.000066, mae: 0.005991, mean_q: 0.003861

Program interrupted. (Use 'cont' to resume).
--Call--
> [0;32m/home/luigi/.local/lib/python3.6/site-packages/rl/memory.py[0m(55)[0;36m__getitem__[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m    [0;32mdef[0m [0m__getitem__[0m[0;34m([0m[0mself[0m[0;34m,[0m [0midx[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m        """Return element of buffer at specific index
[0m
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> 