Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.158s, episode steps: 100, steps per second: 635, episode reward: 17.406, mean reward: 0.174 [0.013, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.939, 10.424], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 18.586, mean reward: 0.186 [0.023, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.328, 10.174], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1596, episode reward: 16.684, mean reward: 0.167 [0.009, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.880, 10.131], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 18.145, mean reward: 0.181 [0.022, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.817, 10.168], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1618, episode reward: 15.367, mean reward: 0.154 [0.004, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.221], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 19.607, mean reward: 0.196 [0.041, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.088, 10.098], loss: 0.005848, mae: 0.083159, mean_q: 0.309649
   700/100000: episode: 7, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 17.545, mean reward: 0.175 [0.023, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.290, 10.169], loss: 0.004188, mae: 0.071977, mean_q: 0.339240
   800/100000: episode: 8, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.333, mean reward: 0.153 [0.030, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.294, 10.114], loss: 0.003694, mae: 0.067961, mean_q: 0.339789
   900/100000: episode: 9, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 15.766, mean reward: 0.158 [0.016, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.287, 10.122], loss: 0.003696, mae: 0.068891, mean_q: 0.341815
  1000/100000: episode: 10, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 21.164, mean reward: 0.212 [0.037, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.566, 10.098], loss: 0.003765, mae: 0.069475, mean_q: 0.337061
  1100/100000: episode: 11, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 13.691, mean reward: 0.137 [0.003, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.514, 10.098], loss: 0.003982, mae: 0.071196, mean_q: 0.341283
  1200/100000: episode: 12, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 19.558, mean reward: 0.196 [0.018, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.276], loss: 0.004022, mae: 0.071900, mean_q: 0.344466
  1300/100000: episode: 13, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 21.777, mean reward: 0.218 [0.027, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.647, 10.274], loss: 0.004163, mae: 0.073083, mean_q: 0.345953
  1400/100000: episode: 14, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.388, mean reward: 0.174 [0.019, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.478, 10.098], loss: 0.004290, mae: 0.074183, mean_q: 0.351380
  1500/100000: episode: 15, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 17.000, mean reward: 0.170 [0.006, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.823, 10.098], loss: 0.004102, mae: 0.072083, mean_q: 0.345415
  1600/100000: episode: 16, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.681, mean reward: 0.177 [0.020, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.463, 10.241], loss: 0.004637, mae: 0.076320, mean_q: 0.349709
  1700/100000: episode: 17, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 20.793, mean reward: 0.208 [0.017, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.485, 10.098], loss: 0.004250, mae: 0.072973, mean_q: 0.348808
  1800/100000: episode: 18, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 17.640, mean reward: 0.176 [0.055, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.778, 10.098], loss: 0.004341, mae: 0.075268, mean_q: 0.356646
  1900/100000: episode: 19, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 22.755, mean reward: 0.228 [0.025, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.098], loss: 0.003973, mae: 0.071138, mean_q: 0.351844
  2000/100000: episode: 20, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 12.783, mean reward: 0.128 [0.008, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.703, 10.119], loss: 0.004492, mae: 0.075238, mean_q: 0.350839
  2100/100000: episode: 21, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 17.182, mean reward: 0.172 [0.008, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.181, 10.210], loss: 0.004414, mae: 0.074748, mean_q: 0.353806
  2200/100000: episode: 22, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 13.350, mean reward: 0.133 [0.006, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.769, 10.121], loss: 0.004432, mae: 0.075977, mean_q: 0.353990
  2300/100000: episode: 23, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 21.478, mean reward: 0.215 [0.070, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.342], loss: 0.004606, mae: 0.076388, mean_q: 0.351815
  2400/100000: episode: 24, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 21.691, mean reward: 0.217 [0.042, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.207, 10.475], loss: 0.004245, mae: 0.073998, mean_q: 0.355463
  2500/100000: episode: 25, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 14.833, mean reward: 0.148 [0.015, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.421, 10.109], loss: 0.004506, mae: 0.075357, mean_q: 0.353507
  2600/100000: episode: 26, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 14.630, mean reward: 0.146 [0.017, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.440, 10.155], loss: 0.004238, mae: 0.072979, mean_q: 0.350008
  2700/100000: episode: 27, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 19.412, mean reward: 0.194 [0.025, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.264, 10.306], loss: 0.004335, mae: 0.074084, mean_q: 0.349169
  2800/100000: episode: 28, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 15.152, mean reward: 0.152 [0.016, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.119, 10.098], loss: 0.004392, mae: 0.075027, mean_q: 0.348957
  2900/100000: episode: 29, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 15.737, mean reward: 0.157 [0.010, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.924, 10.103], loss: 0.004022, mae: 0.071502, mean_q: 0.345877
  3000/100000: episode: 30, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.487, mean reward: 0.175 [0.008, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.652, 10.182], loss: 0.004403, mae: 0.074830, mean_q: 0.351851
  3100/100000: episode: 31, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 12.454, mean reward: 0.125 [0.008, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.821, 10.164], loss: 0.004392, mae: 0.074563, mean_q: 0.346594
  3200/100000: episode: 32, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 23.948, mean reward: 0.239 [0.010, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.160, 10.098], loss: 0.004182, mae: 0.073091, mean_q: 0.344841
  3300/100000: episode: 33, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 20.163, mean reward: 0.202 [0.008, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.236, 10.349], loss: 0.004752, mae: 0.077296, mean_q: 0.346990
  3400/100000: episode: 34, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.850, mean reward: 0.169 [0.021, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.442, 10.173], loss: 0.004566, mae: 0.076265, mean_q: 0.346791
  3500/100000: episode: 35, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 20.390, mean reward: 0.204 [0.023, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.614, 10.098], loss: 0.004651, mae: 0.076623, mean_q: 0.348993
  3600/100000: episode: 36, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.283, mean reward: 0.153 [0.013, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.889, 10.098], loss: 0.004536, mae: 0.075593, mean_q: 0.346936
  3700/100000: episode: 37, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.323, mean reward: 0.153 [0.012, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.463, 10.400], loss: 0.004467, mae: 0.075551, mean_q: 0.349248
  3800/100000: episode: 38, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.059, mean reward: 0.161 [0.024, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.486, 10.098], loss: 0.004418, mae: 0.075220, mean_q: 0.346542
  3900/100000: episode: 39, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.115, mean reward: 0.161 [0.015, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.821, 10.107], loss: 0.004530, mae: 0.075365, mean_q: 0.344464
  4000/100000: episode: 40, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 21.493, mean reward: 0.215 [0.038, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.839, 10.234], loss: 0.004294, mae: 0.073933, mean_q: 0.345463
  4100/100000: episode: 41, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.665, mean reward: 0.167 [0.015, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.093, 10.293], loss: 0.004354, mae: 0.074442, mean_q: 0.344553
  4200/100000: episode: 42, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 13.975, mean reward: 0.140 [0.004, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.754, 10.190], loss: 0.004606, mae: 0.076147, mean_q: 0.345301
  4300/100000: episode: 43, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 17.202, mean reward: 0.172 [0.009, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.313, 10.098], loss: 0.004655, mae: 0.076329, mean_q: 0.342518
  4400/100000: episode: 44, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 16.312, mean reward: 0.163 [0.041, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.484, 10.116], loss: 0.004448, mae: 0.075130, mean_q: 0.343318
  4500/100000: episode: 45, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.649, mean reward: 0.166 [0.018, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.676, 10.098], loss: 0.004599, mae: 0.076013, mean_q: 0.341042
  4600/100000: episode: 46, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.561, mean reward: 0.166 [0.006, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.237, 10.098], loss: 0.004181, mae: 0.073234, mean_q: 0.343761
  4700/100000: episode: 47, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.621, mean reward: 0.166 [0.027, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.572, 10.240], loss: 0.004454, mae: 0.075414, mean_q: 0.341767
  4800/100000: episode: 48, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 15.999, mean reward: 0.160 [0.011, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.787, 10.176], loss: 0.004404, mae: 0.075246, mean_q: 0.342768
  4900/100000: episode: 49, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.080, mean reward: 0.171 [0.018, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.903, 10.098], loss: 0.004555, mae: 0.076395, mean_q: 0.345512
  5000/100000: episode: 50, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 17.335, mean reward: 0.173 [0.032, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.353, 10.333], loss: 0.004535, mae: 0.076072, mean_q: 0.341731
  5100/100000: episode: 51, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 12.596, mean reward: 0.126 [0.012, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.236, 10.143], loss: 0.004561, mae: 0.076874, mean_q: 0.344736
  5200/100000: episode: 52, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.475, mean reward: 0.175 [0.030, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.529, 10.260], loss: 0.004216, mae: 0.072298, mean_q: 0.345586
  5300/100000: episode: 53, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 19.656, mean reward: 0.197 [0.018, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.380, 10.321], loss: 0.004214, mae: 0.072900, mean_q: 0.341418
  5400/100000: episode: 54, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.640, mean reward: 0.166 [0.026, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.347, 10.174], loss: 0.004321, mae: 0.074205, mean_q: 0.342640
  5500/100000: episode: 55, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 17.778, mean reward: 0.178 [0.020, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.770, 10.227], loss: 0.004690, mae: 0.076117, mean_q: 0.339969
  5600/100000: episode: 56, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.483, mean reward: 0.165 [0.009, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.990, 10.098], loss: 0.004331, mae: 0.074477, mean_q: 0.342870
  5700/100000: episode: 57, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 14.181, mean reward: 0.142 [0.005, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.987, 10.098], loss: 0.004272, mae: 0.074336, mean_q: 0.344217
  5800/100000: episode: 58, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 23.928, mean reward: 0.239 [0.004, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.235, 10.463], loss: 0.004505, mae: 0.076091, mean_q: 0.341179
  5900/100000: episode: 59, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.968, mean reward: 0.170 [0.025, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.142, 10.098], loss: 0.004237, mae: 0.073352, mean_q: 0.338614
  6000/100000: episode: 60, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.330, mean reward: 0.163 [0.024, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.922, 10.153], loss: 0.004193, mae: 0.073245, mean_q: 0.343092
  6100/100000: episode: 61, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 19.409, mean reward: 0.194 [0.018, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.375, 10.098], loss: 0.004371, mae: 0.074603, mean_q: 0.344607
  6200/100000: episode: 62, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 26.092, mean reward: 0.261 [0.029, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.795, 10.418], loss: 0.004585, mae: 0.075625, mean_q: 0.344251
  6300/100000: episode: 63, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.011, mean reward: 0.160 [0.016, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.529, 10.098], loss: 0.004398, mae: 0.075177, mean_q: 0.344702
  6400/100000: episode: 64, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 13.409, mean reward: 0.134 [0.006, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.074, 10.195], loss: 0.004328, mae: 0.073816, mean_q: 0.343656
  6500/100000: episode: 65, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 13.875, mean reward: 0.139 [0.008, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.318, 10.098], loss: 0.004659, mae: 0.077051, mean_q: 0.343422
  6600/100000: episode: 66, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.454, mean reward: 0.155 [0.015, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.997, 10.098], loss: 0.004322, mae: 0.074709, mean_q: 0.340410
  6700/100000: episode: 67, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 17.368, mean reward: 0.174 [0.019, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.475, 10.273], loss: 0.004546, mae: 0.075506, mean_q: 0.345304
  6800/100000: episode: 68, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 17.215, mean reward: 0.172 [0.028, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.972, 10.098], loss: 0.004263, mae: 0.073926, mean_q: 0.345555
  6900/100000: episode: 69, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 15.517, mean reward: 0.155 [0.015, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.398, 10.098], loss: 0.004192, mae: 0.072726, mean_q: 0.339240
  7000/100000: episode: 70, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.467, mean reward: 0.155 [0.019, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.147, 10.098], loss: 0.004110, mae: 0.072258, mean_q: 0.335747
  7100/100000: episode: 71, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.629, mean reward: 0.166 [0.014, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.044, 10.098], loss: 0.004164, mae: 0.072260, mean_q: 0.336469
  7200/100000: episode: 72, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 12.385, mean reward: 0.124 [0.018, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.190], loss: 0.004268, mae: 0.074048, mean_q: 0.337074
  7300/100000: episode: 73, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.546, mean reward: 0.155 [0.025, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.938, 10.300], loss: 0.004270, mae: 0.073859, mean_q: 0.338996
  7400/100000: episode: 74, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 17.577, mean reward: 0.176 [0.016, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.270, 10.251], loss: 0.004089, mae: 0.072006, mean_q: 0.333393
  7500/100000: episode: 75, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 14.820, mean reward: 0.148 [0.005, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.768, 10.129], loss: 0.004001, mae: 0.071322, mean_q: 0.333556
  7600/100000: episode: 76, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 19.843, mean reward: 0.198 [0.029, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.751, 10.098], loss: 0.004284, mae: 0.073796, mean_q: 0.335453
  7700/100000: episode: 77, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.226, mean reward: 0.152 [0.007, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.098], loss: 0.004222, mae: 0.072625, mean_q: 0.333956
  7800/100000: episode: 78, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 16.331, mean reward: 0.163 [0.011, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.316, 10.420], loss: 0.004257, mae: 0.073224, mean_q: 0.329456
  7900/100000: episode: 79, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.008, mean reward: 0.140 [0.008, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.666, 10.098], loss: 0.004228, mae: 0.073218, mean_q: 0.332866
  8000/100000: episode: 80, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 19.356, mean reward: 0.194 [0.038, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.192], loss: 0.004367, mae: 0.073286, mean_q: 0.336091
  8100/100000: episode: 81, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 18.586, mean reward: 0.186 [0.011, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.580, 10.098], loss: 0.004151, mae: 0.072234, mean_q: 0.336404
  8200/100000: episode: 82, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.190, mean reward: 0.142 [0.023, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.098], loss: 0.004082, mae: 0.071548, mean_q: 0.333925
  8300/100000: episode: 83, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.253, mean reward: 0.163 [0.024, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.159], loss: 0.004205, mae: 0.073386, mean_q: 0.334017
  8400/100000: episode: 84, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 17.630, mean reward: 0.176 [0.015, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.182, 10.098], loss: 0.003872, mae: 0.069982, mean_q: 0.331787
  8500/100000: episode: 85, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 13.677, mean reward: 0.137 [0.010, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.358, 10.098], loss: 0.003961, mae: 0.070574, mean_q: 0.331529
  8600/100000: episode: 86, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 17.121, mean reward: 0.171 [0.015, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.508, 10.228], loss: 0.004059, mae: 0.072036, mean_q: 0.328871
  8700/100000: episode: 87, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.542, mean reward: 0.155 [0.023, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.770, 10.255], loss: 0.004033, mae: 0.071957, mean_q: 0.332635
  8800/100000: episode: 88, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.628, mean reward: 0.176 [0.048, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.424], loss: 0.003904, mae: 0.070146, mean_q: 0.327714
  8900/100000: episode: 89, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.510, mean reward: 0.165 [0.022, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.241], loss: 0.004155, mae: 0.073381, mean_q: 0.330374
  9000/100000: episode: 90, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.444, mean reward: 0.154 [0.015, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.295, 10.098], loss: 0.003853, mae: 0.069795, mean_q: 0.328014
  9100/100000: episode: 91, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.737, mean reward: 0.167 [0.029, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.464, 10.284], loss: 0.003843, mae: 0.070592, mean_q: 0.325956
  9200/100000: episode: 92, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.404, mean reward: 0.154 [0.008, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.838, 10.133], loss: 0.003759, mae: 0.068666, mean_q: 0.327619
  9300/100000: episode: 93, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 20.026, mean reward: 0.200 [0.026, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.829, 10.423], loss: 0.003845, mae: 0.070115, mean_q: 0.328007
  9400/100000: episode: 94, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 16.120, mean reward: 0.161 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.098], loss: 0.003846, mae: 0.069300, mean_q: 0.327302
  9500/100000: episode: 95, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.792, mean reward: 0.168 [0.027, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.081, 10.448], loss: 0.003749, mae: 0.069000, mean_q: 0.329346
  9600/100000: episode: 96, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.893, mean reward: 0.159 [0.034, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.099, 10.222], loss: 0.003825, mae: 0.069294, mean_q: 0.329677
  9700/100000: episode: 97, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.790, mean reward: 0.158 [0.026, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.302, 10.098], loss: 0.004029, mae: 0.071418, mean_q: 0.328480
  9800/100000: episode: 98, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 20.530, mean reward: 0.205 [0.039, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.350, 10.310], loss: 0.003783, mae: 0.069241, mean_q: 0.324392
  9900/100000: episode: 99, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.516, mean reward: 0.165 [0.021, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.727, 10.237], loss: 0.003740, mae: 0.069835, mean_q: 0.327481
 10000/100000: episode: 100, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 17.958, mean reward: 0.180 [0.019, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.188, 10.169], loss: 0.003772, mae: 0.069882, mean_q: 0.328713
 10100/100000: episode: 101, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 19.513, mean reward: 0.195 [0.023, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.584, 10.449], loss: 0.003870, mae: 0.069809, mean_q: 0.328143
 10200/100000: episode: 102, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.910, mean reward: 0.209 [0.036, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.675, 10.311], loss: 0.003782, mae: 0.068924, mean_q: 0.331778
 10300/100000: episode: 103, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.259, mean reward: 0.163 [0.010, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.373, 10.098], loss: 0.003790, mae: 0.070171, mean_q: 0.330570
 10400/100000: episode: 104, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 22.134, mean reward: 0.221 [0.006, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.505, 10.098], loss: 0.003955, mae: 0.071569, mean_q: 0.337551
 10500/100000: episode: 105, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 23.368, mean reward: 0.234 [0.039, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.466, 10.436], loss: 0.003776, mae: 0.069391, mean_q: 0.336619
 10600/100000: episode: 106, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 14.246, mean reward: 0.142 [0.023, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.556, 10.120], loss: 0.003851, mae: 0.069616, mean_q: 0.334390
 10700/100000: episode: 107, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.436, mean reward: 0.164 [0.021, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.649, 10.098], loss: 0.003783, mae: 0.069714, mean_q: 0.333676
 10800/100000: episode: 108, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.784, mean reward: 0.148 [0.020, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.105], loss: 0.003721, mae: 0.068773, mean_q: 0.334067
 10900/100000: episode: 109, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.776, mean reward: 0.158 [0.004, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.726, 10.188], loss: 0.003869, mae: 0.069626, mean_q: 0.331586
 11000/100000: episode: 110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.009, mean reward: 0.150 [0.024, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.822, 10.148], loss: 0.003478, mae: 0.066445, mean_q: 0.329849
 11100/100000: episode: 111, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.095, mean reward: 0.171 [0.019, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.098], loss: 0.003789, mae: 0.069084, mean_q: 0.334062
 11200/100000: episode: 112, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.959, mean reward: 0.200 [0.012, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.864, 10.098], loss: 0.003522, mae: 0.067695, mean_q: 0.326185
 11300/100000: episode: 113, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.321, mean reward: 0.153 [0.005, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.098], loss: 0.003665, mae: 0.068073, mean_q: 0.330548
 11400/100000: episode: 114, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.509, mean reward: 0.155 [0.023, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.347, 10.098], loss: 0.003743, mae: 0.068881, mean_q: 0.326164
 11500/100000: episode: 115, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 20.166, mean reward: 0.202 [0.019, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.892, 10.098], loss: 0.003632, mae: 0.067113, mean_q: 0.330797
 11600/100000: episode: 116, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 14.541, mean reward: 0.145 [0.004, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: 0.003773, mae: 0.069188, mean_q: 0.329804
 11700/100000: episode: 117, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 12.981, mean reward: 0.130 [0.014, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.013, 10.098], loss: 0.003634, mae: 0.068478, mean_q: 0.330659
 11800/100000: episode: 118, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.726, mean reward: 0.177 [0.017, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.911, 10.098], loss: 0.004061, mae: 0.071634, mean_q: 0.332343
 11900/100000: episode: 119, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.274, mean reward: 0.143 [0.007, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.307, 10.098], loss: 0.003892, mae: 0.069597, mean_q: 0.329733
 12000/100000: episode: 120, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 12.686, mean reward: 0.127 [0.021, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.339, 10.098], loss: 0.003661, mae: 0.068042, mean_q: 0.328712
 12100/100000: episode: 121, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 18.351, mean reward: 0.184 [0.042, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.684, 10.098], loss: 0.003775, mae: 0.069134, mean_q: 0.328473
 12200/100000: episode: 122, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.397, mean reward: 0.164 [0.021, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.203], loss: 0.003800, mae: 0.068342, mean_q: 0.328511
 12300/100000: episode: 123, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 17.029, mean reward: 0.170 [0.031, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.140], loss: 0.003803, mae: 0.069215, mean_q: 0.332093
 12400/100000: episode: 124, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.738, mean reward: 0.167 [0.009, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.245], loss: 0.003685, mae: 0.067632, mean_q: 0.332987
 12500/100000: episode: 125, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.694, mean reward: 0.187 [0.042, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.892, 10.098], loss: 0.003663, mae: 0.067846, mean_q: 0.332845
 12600/100000: episode: 126, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 14.359, mean reward: 0.144 [0.017, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.520, 10.215], loss: 0.003737, mae: 0.067412, mean_q: 0.331938
 12700/100000: episode: 127, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.543, mean reward: 0.165 [0.019, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.564, 10.250], loss: 0.003867, mae: 0.070153, mean_q: 0.331007
 12800/100000: episode: 128, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.625, mean reward: 0.146 [0.020, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.749, 10.233], loss: 0.003682, mae: 0.067647, mean_q: 0.331512
 12900/100000: episode: 129, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 12.688, mean reward: 0.127 [0.006, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.907, 10.098], loss: 0.003980, mae: 0.069537, mean_q: 0.330484
 13000/100000: episode: 130, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 20.109, mean reward: 0.201 [0.016, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.790, 10.098], loss: 0.003628, mae: 0.067313, mean_q: 0.332182
 13100/100000: episode: 131, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 17.326, mean reward: 0.173 [0.028, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.967, 10.174], loss: 0.003870, mae: 0.068988, mean_q: 0.326287
 13200/100000: episode: 132, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.847, mean reward: 0.188 [0.022, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.363, 10.134], loss: 0.003924, mae: 0.069360, mean_q: 0.328770
 13300/100000: episode: 133, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.927, mean reward: 0.159 [0.014, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.670, 10.321], loss: 0.004029, mae: 0.070154, mean_q: 0.329080
 13400/100000: episode: 134, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 12.357, mean reward: 0.124 [0.004, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.632, 10.316], loss: 0.003833, mae: 0.068779, mean_q: 0.329471
 13500/100000: episode: 135, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 13.142, mean reward: 0.131 [0.005, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.499, 10.112], loss: 0.003775, mae: 0.068528, mean_q: 0.325982
 13600/100000: episode: 136, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 15.493, mean reward: 0.155 [0.001, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.020, 10.098], loss: 0.003740, mae: 0.067786, mean_q: 0.325047
 13700/100000: episode: 137, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.550, mean reward: 0.155 [0.012, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.317, 10.119], loss: 0.003932, mae: 0.070012, mean_q: 0.328673
 13800/100000: episode: 138, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.516, mean reward: 0.165 [0.014, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.119, 10.098], loss: 0.003773, mae: 0.068021, mean_q: 0.328326
 13900/100000: episode: 139, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.660, mean reward: 0.157 [0.021, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.213], loss: 0.003769, mae: 0.068033, mean_q: 0.326659
 14000/100000: episode: 140, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.587, mean reward: 0.146 [0.019, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.332, 10.163], loss: 0.003647, mae: 0.066813, mean_q: 0.323989
 14100/100000: episode: 141, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 22.640, mean reward: 0.226 [0.013, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.098], loss: 0.003594, mae: 0.066677, mean_q: 0.326485
 14200/100000: episode: 142, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.231, mean reward: 0.172 [0.007, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.670, 10.395], loss: 0.003826, mae: 0.069213, mean_q: 0.325394
 14300/100000: episode: 143, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 17.277, mean reward: 0.173 [0.015, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.598, 10.098], loss: 0.004230, mae: 0.071901, mean_q: 0.325535
 14400/100000: episode: 144, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.066, mean reward: 0.151 [0.032, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.098], loss: 0.003780, mae: 0.068484, mean_q: 0.326354
 14500/100000: episode: 145, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.404, mean reward: 0.194 [0.018, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.438, 10.230], loss: 0.003772, mae: 0.068478, mean_q: 0.321628
 14600/100000: episode: 146, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.480, mean reward: 0.175 [0.009, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.008, 10.098], loss: 0.003844, mae: 0.069036, mean_q: 0.324566
 14700/100000: episode: 147, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.981, mean reward: 0.150 [0.018, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.882, 10.098], loss: 0.003756, mae: 0.068405, mean_q: 0.323584
 14800/100000: episode: 148, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 15.081, mean reward: 0.151 [0.009, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.804, 10.098], loss: 0.004007, mae: 0.070646, mean_q: 0.324064
 14900/100000: episode: 149, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 20.162, mean reward: 0.202 [0.010, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.190, 10.098], loss: 0.004039, mae: 0.069744, mean_q: 0.323754
 15000/100000: episode: 150, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 17.819, mean reward: 0.178 [0.037, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.246], loss: 0.003829, mae: 0.068414, mean_q: 0.324848
 15100/100000: episode: 151, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 13.537, mean reward: 0.135 [0.021, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.132], loss: 0.004008, mae: 0.070493, mean_q: 0.326502
 15200/100000: episode: 152, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.973, mean reward: 0.150 [0.021, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.591, 10.098], loss: 0.004007, mae: 0.069632, mean_q: 0.323027
 15300/100000: episode: 153, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 21.666, mean reward: 0.217 [0.009, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.074, 10.098], loss: 0.003846, mae: 0.068984, mean_q: 0.320496
 15400/100000: episode: 154, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 18.041, mean reward: 0.180 [0.014, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.190, 10.336], loss: 0.004120, mae: 0.071376, mean_q: 0.322960
 15500/100000: episode: 155, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 20.008, mean reward: 0.200 [0.022, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.604, 10.106], loss: 0.004045, mae: 0.070692, mean_q: 0.323773
 15600/100000: episode: 156, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.082, mean reward: 0.151 [0.017, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.883, 10.149], loss: 0.003967, mae: 0.070184, mean_q: 0.325314
 15700/100000: episode: 157, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.678, mean reward: 0.177 [0.011, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.358, 10.098], loss: 0.004066, mae: 0.070971, mean_q: 0.325900
 15800/100000: episode: 158, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.799, mean reward: 0.158 [0.013, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.000, 10.098], loss: 0.003857, mae: 0.069360, mean_q: 0.322346
 15900/100000: episode: 159, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 19.358, mean reward: 0.194 [0.003, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.098], loss: 0.003959, mae: 0.069043, mean_q: 0.325462
 16000/100000: episode: 160, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 13.801, mean reward: 0.138 [0.018, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.187, 10.098], loss: 0.003910, mae: 0.069755, mean_q: 0.325751
 16100/100000: episode: 161, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.415, mean reward: 0.154 [0.025, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.363, 10.172], loss: 0.003705, mae: 0.067107, mean_q: 0.321098
 16200/100000: episode: 162, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 18.356, mean reward: 0.184 [0.011, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.508, 10.155], loss: 0.004009, mae: 0.070442, mean_q: 0.323553
 16300/100000: episode: 163, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 14.165, mean reward: 0.142 [0.014, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.098], loss: 0.003730, mae: 0.067780, mean_q: 0.323315
 16400/100000: episode: 164, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 14.857, mean reward: 0.149 [0.039, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.172, 10.098], loss: 0.004022, mae: 0.070078, mean_q: 0.321450
 16500/100000: episode: 165, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 14.739, mean reward: 0.147 [0.018, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.803, 10.098], loss: 0.003819, mae: 0.069573, mean_q: 0.321213
 16600/100000: episode: 166, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.871, mean reward: 0.199 [0.015, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.003594, mae: 0.067120, mean_q: 0.320359
 16700/100000: episode: 167, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.896, mean reward: 0.189 [0.035, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.277], loss: 0.003943, mae: 0.069411, mean_q: 0.325165
 16800/100000: episode: 168, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 21.749, mean reward: 0.217 [0.018, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.719, 10.412], loss: 0.003614, mae: 0.067016, mean_q: 0.321349
 16900/100000: episode: 169, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 19.552, mean reward: 0.196 [0.014, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.778, 10.334], loss: 0.003702, mae: 0.067664, mean_q: 0.325259
 17000/100000: episode: 170, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 18.995, mean reward: 0.190 [0.077, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.327, 10.518], loss: 0.003747, mae: 0.068639, mean_q: 0.323766
 17100/100000: episode: 171, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 14.427, mean reward: 0.144 [0.008, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.336, 10.098], loss: 0.003938, mae: 0.069495, mean_q: 0.330555
 17200/100000: episode: 172, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.924, mean reward: 0.159 [0.011, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.549, 10.098], loss: 0.003481, mae: 0.065321, mean_q: 0.324259
 17300/100000: episode: 173, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 20.228, mean reward: 0.202 [0.009, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.003576, mae: 0.066548, mean_q: 0.329571
 17400/100000: episode: 174, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 19.563, mean reward: 0.196 [0.024, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.745, 10.124], loss: 0.003738, mae: 0.067790, mean_q: 0.334597
 17500/100000: episode: 175, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.390, mean reward: 0.164 [0.004, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.824, 10.098], loss: 0.003799, mae: 0.069106, mean_q: 0.331901
 17600/100000: episode: 176, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 20.517, mean reward: 0.205 [0.027, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.119, 10.098], loss: 0.003824, mae: 0.069080, mean_q: 0.335198
 17700/100000: episode: 177, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 17.585, mean reward: 0.176 [0.018, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.673, 10.098], loss: 0.003746, mae: 0.068621, mean_q: 0.333543
 17800/100000: episode: 178, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 13.753, mean reward: 0.138 [0.007, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.048, 10.098], loss: 0.003760, mae: 0.068335, mean_q: 0.335074
 17900/100000: episode: 179, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 15.046, mean reward: 0.150 [0.026, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.907, 10.108], loss: 0.003991, mae: 0.070374, mean_q: 0.338640
 18000/100000: episode: 180, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 19.121, mean reward: 0.191 [0.017, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.112, 10.340], loss: 0.003769, mae: 0.068351, mean_q: 0.336089
 18100/100000: episode: 181, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 18.404, mean reward: 0.184 [0.017, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.240, 10.100], loss: 0.003778, mae: 0.068447, mean_q: 0.337247
 18200/100000: episode: 182, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 17.108, mean reward: 0.171 [0.027, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.472, 10.334], loss: 0.003612, mae: 0.066960, mean_q: 0.332438
 18300/100000: episode: 183, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 14.846, mean reward: 0.148 [0.047, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.078, 10.098], loss: 0.003680, mae: 0.068076, mean_q: 0.335654
 18400/100000: episode: 184, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 12.632, mean reward: 0.126 [0.014, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.687, 10.139], loss: 0.003913, mae: 0.068953, mean_q: 0.336051
 18500/100000: episode: 185, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 14.688, mean reward: 0.147 [0.002, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.169, 10.164], loss: 0.003693, mae: 0.067228, mean_q: 0.334762
 18600/100000: episode: 186, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 14.532, mean reward: 0.145 [0.013, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.629, 10.098], loss: 0.003553, mae: 0.065909, mean_q: 0.334215
 18700/100000: episode: 187, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.066, mean reward: 0.151 [0.017, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.431, 10.098], loss: 0.003860, mae: 0.068905, mean_q: 0.338541
 18800/100000: episode: 188, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.285, mean reward: 0.143 [0.014, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.140], loss: 0.003714, mae: 0.067949, mean_q: 0.334627
 18900/100000: episode: 189, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 27.281, mean reward: 0.273 [0.021, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.709, 10.308], loss: 0.003750, mae: 0.068269, mean_q: 0.329989
 19000/100000: episode: 190, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.310, mean reward: 0.153 [0.028, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.025, 10.345], loss: 0.003746, mae: 0.068712, mean_q: 0.337574
 19100/100000: episode: 191, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.757, mean reward: 0.148 [0.015, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.228, 10.098], loss: 0.003705, mae: 0.067867, mean_q: 0.335348
 19200/100000: episode: 192, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 15.934, mean reward: 0.159 [0.022, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.889, 10.106], loss: 0.003911, mae: 0.069939, mean_q: 0.335756
 19300/100000: episode: 193, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 13.307, mean reward: 0.133 [0.011, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.237, 10.098], loss: 0.003784, mae: 0.068634, mean_q: 0.334558
 19400/100000: episode: 194, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.703, mean reward: 0.187 [0.027, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.944, 10.098], loss: 0.003669, mae: 0.067581, mean_q: 0.331513
 19500/100000: episode: 195, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 15.738, mean reward: 0.157 [0.024, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.933, 10.279], loss: 0.003683, mae: 0.068200, mean_q: 0.329616
 19600/100000: episode: 196, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 21.018, mean reward: 0.210 [0.040, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.765, 10.176], loss: 0.003907, mae: 0.070046, mean_q: 0.333198
 19700/100000: episode: 197, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 19.534, mean reward: 0.195 [0.014, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.156, 10.178], loss: 0.003914, mae: 0.069793, mean_q: 0.331995
 19800/100000: episode: 198, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.907, mean reward: 0.159 [0.017, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.192, 10.098], loss: 0.003516, mae: 0.066513, mean_q: 0.334870
 19900/100000: episode: 199, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.308, mean reward: 0.173 [0.007, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.492, 10.182], loss: 0.003774, mae: 0.068474, mean_q: 0.334996
 20000/100000: episode: 200, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 13.732, mean reward: 0.137 [0.045, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.460, 10.163], loss: 0.003854, mae: 0.069876, mean_q: 0.334600
 20100/100000: episode: 201, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.452, mean reward: 0.185 [0.044, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.820, 10.326], loss: 0.003751, mae: 0.068248, mean_q: 0.331041
 20200/100000: episode: 202, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.315, mean reward: 0.143 [0.004, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.765, 10.131], loss: 0.003520, mae: 0.066554, mean_q: 0.335547
 20300/100000: episode: 203, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 12.521, mean reward: 0.125 [0.012, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.380, 10.142], loss: 0.003628, mae: 0.066945, mean_q: 0.332500
 20400/100000: episode: 204, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 17.567, mean reward: 0.176 [0.029, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.267], loss: 0.003658, mae: 0.066929, mean_q: 0.329720
 20500/100000: episode: 205, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 15.957, mean reward: 0.160 [0.018, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.056, 10.098], loss: 0.003774, mae: 0.068707, mean_q: 0.332562
 20600/100000: episode: 206, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.562, mean reward: 0.146 [0.011, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.098], loss: 0.003554, mae: 0.067459, mean_q: 0.330013
 20700/100000: episode: 207, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.774, mean reward: 0.188 [0.007, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.012, 10.298], loss: 0.003787, mae: 0.069051, mean_q: 0.332287
 20800/100000: episode: 208, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 19.748, mean reward: 0.197 [0.009, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.075, 10.098], loss: 0.003467, mae: 0.065793, mean_q: 0.329927
 20900/100000: episode: 209, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.379, mean reward: 0.164 [0.015, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.394, 10.098], loss: 0.003664, mae: 0.068459, mean_q: 0.333755
 21000/100000: episode: 210, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.514, mean reward: 0.145 [0.007, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.740, 10.102], loss: 0.003587, mae: 0.066436, mean_q: 0.329449
 21100/100000: episode: 211, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 14.188, mean reward: 0.142 [0.025, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.098], loss: 0.003446, mae: 0.066094, mean_q: 0.330816
 21200/100000: episode: 212, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.610, mean reward: 0.146 [0.026, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.585, 10.098], loss: 0.003527, mae: 0.066360, mean_q: 0.329119
 21300/100000: episode: 213, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.409, mean reward: 0.144 [0.008, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.276, 10.098], loss: 0.003411, mae: 0.065239, mean_q: 0.330783
 21400/100000: episode: 214, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 13.403, mean reward: 0.134 [0.020, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.003606, mae: 0.067416, mean_q: 0.327445
 21500/100000: episode: 215, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 19.799, mean reward: 0.198 [0.031, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.501, 10.410], loss: 0.003465, mae: 0.065696, mean_q: 0.327017
 21600/100000: episode: 216, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.634, mean reward: 0.176 [0.023, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.098], loss: 0.003446, mae: 0.066032, mean_q: 0.329829
 21700/100000: episode: 217, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.009, mean reward: 0.160 [0.009, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.289, 10.152], loss: 0.003426, mae: 0.065461, mean_q: 0.328814
 21800/100000: episode: 218, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 14.683, mean reward: 0.147 [0.037, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.859, 10.188], loss: 0.003786, mae: 0.069899, mean_q: 0.330231
 21900/100000: episode: 219, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 14.508, mean reward: 0.145 [0.006, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.125, 10.098], loss: 0.003483, mae: 0.066228, mean_q: 0.327598
 22000/100000: episode: 220, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 13.631, mean reward: 0.136 [0.012, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.900, 10.098], loss: 0.003591, mae: 0.066915, mean_q: 0.323880
 22100/100000: episode: 221, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 13.968, mean reward: 0.140 [0.020, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.197], loss: 0.003569, mae: 0.066894, mean_q: 0.321450
 22200/100000: episode: 222, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.434, mean reward: 0.174 [0.006, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.188], loss: 0.003549, mae: 0.066864, mean_q: 0.320422
 22300/100000: episode: 223, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 19.421, mean reward: 0.194 [0.018, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.545, 10.115], loss: 0.003613, mae: 0.067975, mean_q: 0.321548
 22400/100000: episode: 224, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.591, mean reward: 0.166 [0.008, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.875, 10.098], loss: 0.003534, mae: 0.066215, mean_q: 0.322175
 22500/100000: episode: 225, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 15.703, mean reward: 0.157 [0.020, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.750, 10.098], loss: 0.003655, mae: 0.067711, mean_q: 0.321185
 22600/100000: episode: 226, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 18.624, mean reward: 0.186 [0.022, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.260, 10.335], loss: 0.003618, mae: 0.067453, mean_q: 0.320237
 22700/100000: episode: 227, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.791, mean reward: 0.148 [0.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.098], loss: 0.003544, mae: 0.066612, mean_q: 0.317559
 22800/100000: episode: 228, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.541, mean reward: 0.145 [0.022, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.098], loss: 0.003349, mae: 0.065314, mean_q: 0.319568
 22900/100000: episode: 229, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 18.269, mean reward: 0.183 [0.016, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.447, 10.516], loss: 0.003473, mae: 0.065865, mean_q: 0.317057
 23000/100000: episode: 230, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.714, mean reward: 0.147 [0.011, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.370, 10.098], loss: 0.003474, mae: 0.065871, mean_q: 0.319321
 23100/100000: episode: 231, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.705, mean reward: 0.167 [0.006, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.210, 10.204], loss: 0.003359, mae: 0.065231, mean_q: 0.315226
 23200/100000: episode: 232, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 22.584, mean reward: 0.226 [0.037, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.072, 10.258], loss: 0.003320, mae: 0.064691, mean_q: 0.314779
 23300/100000: episode: 233, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.359, mean reward: 0.164 [0.011, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.186, 10.131], loss: 0.003516, mae: 0.066936, mean_q: 0.316327
 23400/100000: episode: 234, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.092, mean reward: 0.151 [0.010, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.223, 10.098], loss: 0.003730, mae: 0.067950, mean_q: 0.320172
 23500/100000: episode: 235, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.816, mean reward: 0.178 [0.007, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.896, 10.212], loss: 0.003560, mae: 0.067436, mean_q: 0.321261
 23600/100000: episode: 236, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.233, mean reward: 0.172 [0.017, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.387, 10.184], loss: 0.003475, mae: 0.066481, mean_q: 0.315887
 23700/100000: episode: 237, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.480, mean reward: 0.145 [0.003, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.805, 10.246], loss: 0.003729, mae: 0.068937, mean_q: 0.322251
 23800/100000: episode: 238, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 13.494, mean reward: 0.135 [0.011, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.578, 10.098], loss: 0.003519, mae: 0.066215, mean_q: 0.322409
 23900/100000: episode: 239, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.266, mean reward: 0.153 [0.008, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.225, 10.146], loss: 0.003626, mae: 0.067537, mean_q: 0.321676
 24000/100000: episode: 240, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.618, mean reward: 0.176 [0.018, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.356, 10.098], loss: 0.003624, mae: 0.067798, mean_q: 0.319667
 24100/100000: episode: 241, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 22.018, mean reward: 0.220 [0.005, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-2.056, 10.198], loss: 0.003736, mae: 0.069159, mean_q: 0.320205
 24200/100000: episode: 242, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.300, mean reward: 0.163 [0.007, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.468, 10.313], loss: 0.003530, mae: 0.067096, mean_q: 0.321693
 24300/100000: episode: 243, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.288, mean reward: 0.193 [0.020, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.615, 10.262], loss: 0.003626, mae: 0.067800, mean_q: 0.325619
 24400/100000: episode: 244, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 17.837, mean reward: 0.178 [0.030, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.200, 10.392], loss: 0.003662, mae: 0.067125, mean_q: 0.322472
 24500/100000: episode: 245, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 16.332, mean reward: 0.163 [0.008, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.137], loss: 0.003732, mae: 0.068298, mean_q: 0.321403
 24600/100000: episode: 246, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 23.642, mean reward: 0.236 [0.035, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.620, 10.098], loss: 0.003764, mae: 0.069452, mean_q: 0.321758
 24700/100000: episode: 247, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.761, mean reward: 0.178 [0.016, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.633, 10.243], loss: 0.003731, mae: 0.068323, mean_q: 0.323299
 24800/100000: episode: 248, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.093, mean reward: 0.171 [0.013, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.098], loss: 0.003634, mae: 0.067295, mean_q: 0.323442
 24900/100000: episode: 249, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 17.114, mean reward: 0.171 [0.020, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.793, 10.314], loss: 0.003546, mae: 0.066559, mean_q: 0.323796
 25000/100000: episode: 250, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 14.047, mean reward: 0.140 [0.006, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.500, 10.098], loss: 0.003457, mae: 0.066615, mean_q: 0.323130
 25100/100000: episode: 251, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 16.760, mean reward: 0.168 [0.023, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.184, 10.098], loss: 0.003793, mae: 0.068552, mean_q: 0.327700
 25200/100000: episode: 252, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 20.696, mean reward: 0.207 [0.015, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.655, 10.389], loss: 0.003616, mae: 0.067923, mean_q: 0.325114
 25300/100000: episode: 253, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 16.257, mean reward: 0.163 [0.015, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.557, 10.098], loss: 0.003917, mae: 0.069893, mean_q: 0.328115
 25400/100000: episode: 254, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 14.561, mean reward: 0.146 [0.038, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.237], loss: 0.003652, mae: 0.067949, mean_q: 0.323938
 25500/100000: episode: 255, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.720, mean reward: 0.157 [0.011, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.714, 10.322], loss: 0.003714, mae: 0.067961, mean_q: 0.327249
 25600/100000: episode: 256, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 19.567, mean reward: 0.196 [0.012, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.421, 10.112], loss: 0.003762, mae: 0.068715, mean_q: 0.325645
 25700/100000: episode: 257, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 14.183, mean reward: 0.142 [0.020, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.581, 10.098], loss: 0.003833, mae: 0.070166, mean_q: 0.330465
 25800/100000: episode: 258, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 20.118, mean reward: 0.201 [0.053, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.603, 10.098], loss: 0.003671, mae: 0.068850, mean_q: 0.323687
 25900/100000: episode: 259, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.900, mean reward: 0.169 [0.004, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.690, 10.136], loss: 0.003675, mae: 0.067743, mean_q: 0.325403
 26000/100000: episode: 260, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.668, mean reward: 0.137 [0.012, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.760, 10.098], loss: 0.003596, mae: 0.066669, mean_q: 0.325267
 26100/100000: episode: 261, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.495, mean reward: 0.185 [0.012, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.157, 10.098], loss: 0.003912, mae: 0.070504, mean_q: 0.328668
 26200/100000: episode: 262, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.273, mean reward: 0.163 [0.007, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.840, 10.291], loss: 0.003637, mae: 0.067456, mean_q: 0.328553
 26300/100000: episode: 263, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 17.684, mean reward: 0.177 [0.005, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.272], loss: 0.003949, mae: 0.070270, mean_q: 0.330720
 26400/100000: episode: 264, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.024, mean reward: 0.160 [0.021, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.499, 10.118], loss: 0.003551, mae: 0.067073, mean_q: 0.326981
 26500/100000: episode: 265, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 12.470, mean reward: 0.125 [0.010, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.439, 10.098], loss: 0.003663, mae: 0.067593, mean_q: 0.328768
 26600/100000: episode: 266, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 15.561, mean reward: 0.156 [0.016, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.914, 10.098], loss: 0.003594, mae: 0.066825, mean_q: 0.328114
 26700/100000: episode: 267, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 18.793, mean reward: 0.188 [0.013, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.549, 10.098], loss: 0.003739, mae: 0.068315, mean_q: 0.330155
 26800/100000: episode: 268, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 15.033, mean reward: 0.150 [0.014, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.360, 10.331], loss: 0.003638, mae: 0.067650, mean_q: 0.333010
 26900/100000: episode: 269, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.964, mean reward: 0.170 [0.006, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.031, 10.204], loss: 0.003864, mae: 0.069480, mean_q: 0.326244
 27000/100000: episode: 270, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 18.897, mean reward: 0.189 [0.012, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.395, 10.266], loss: 0.003873, mae: 0.069669, mean_q: 0.328043
 27100/100000: episode: 271, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.984, mean reward: 0.160 [0.023, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.933, 10.098], loss: 0.003544, mae: 0.066683, mean_q: 0.330460
 27200/100000: episode: 272, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.760, mean reward: 0.158 [0.011, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.098], loss: 0.003685, mae: 0.067598, mean_q: 0.328535
 27300/100000: episode: 273, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.138, mean reward: 0.151 [0.012, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.003579, mae: 0.067469, mean_q: 0.328063
 27400/100000: episode: 274, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.420, mean reward: 0.204 [0.022, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.014, 10.230], loss: 0.003842, mae: 0.069168, mean_q: 0.327590
 27500/100000: episode: 275, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 20.822, mean reward: 0.208 [0.037, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.069, 10.228], loss: 0.003761, mae: 0.068709, mean_q: 0.331011
 27600/100000: episode: 276, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 13.671, mean reward: 0.137 [0.006, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.113, 10.406], loss: 0.003867, mae: 0.068570, mean_q: 0.332213
 27700/100000: episode: 277, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.926, mean reward: 0.159 [0.008, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.098], loss: 0.003884, mae: 0.069488, mean_q: 0.330448
 27800/100000: episode: 278, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.734, mean reward: 0.167 [0.020, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.615, 10.230], loss: 0.003517, mae: 0.066106, mean_q: 0.332748
 27900/100000: episode: 279, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 19.422, mean reward: 0.194 [0.023, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.813, 10.111], loss: 0.004008, mae: 0.070977, mean_q: 0.333259
 28000/100000: episode: 280, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 13.936, mean reward: 0.139 [0.004, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.783, 10.113], loss: 0.003690, mae: 0.068135, mean_q: 0.331311
 28100/100000: episode: 281, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 23.292, mean reward: 0.233 [0.019, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.098], loss: 0.003958, mae: 0.070465, mean_q: 0.337404
 28200/100000: episode: 282, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.682, mean reward: 0.187 [0.015, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.003599, mae: 0.066216, mean_q: 0.331923
 28300/100000: episode: 283, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 18.556, mean reward: 0.186 [0.006, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.647, 10.409], loss: 0.003892, mae: 0.069950, mean_q: 0.334507
 28400/100000: episode: 284, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.272, mean reward: 0.163 [0.015, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.097, 10.342], loss: 0.003819, mae: 0.068595, mean_q: 0.334635
 28500/100000: episode: 285, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.420, mean reward: 0.154 [0.014, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.098], loss: 0.003851, mae: 0.069401, mean_q: 0.334486
 28600/100000: episode: 286, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.138, mean reward: 0.151 [0.007, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.051, 10.140], loss: 0.003846, mae: 0.068770, mean_q: 0.332592
 28700/100000: episode: 287, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.785, mean reward: 0.148 [0.019, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.940, 10.098], loss: 0.003868, mae: 0.069559, mean_q: 0.335849
 28800/100000: episode: 288, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 17.688, mean reward: 0.177 [0.014, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.210, 10.179], loss: 0.003581, mae: 0.066513, mean_q: 0.333007
 28900/100000: episode: 289, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.028, mean reward: 0.160 [0.016, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.208], loss: 0.003874, mae: 0.069442, mean_q: 0.337114
 29000/100000: episode: 290, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 12.834, mean reward: 0.128 [0.014, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.836, 10.201], loss: 0.003761, mae: 0.068000, mean_q: 0.337282
 29100/100000: episode: 291, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.018, mean reward: 0.160 [0.013, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.057, 10.098], loss: 0.003833, mae: 0.068668, mean_q: 0.334621
 29200/100000: episode: 292, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 20.293, mean reward: 0.203 [0.014, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.003878, mae: 0.068772, mean_q: 0.330172
 29300/100000: episode: 293, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.224, mean reward: 0.182 [0.013, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.580, 10.098], loss: 0.003973, mae: 0.070119, mean_q: 0.334648
 29400/100000: episode: 294, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 15.149, mean reward: 0.151 [0.006, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.557, 10.098], loss: 0.004008, mae: 0.070272, mean_q: 0.339620
 29500/100000: episode: 295, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 12.430, mean reward: 0.124 [0.032, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.112], loss: 0.003781, mae: 0.067908, mean_q: 0.334057
 29600/100000: episode: 296, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 13.338, mean reward: 0.133 [0.008, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.909, 10.204], loss: 0.003582, mae: 0.066164, mean_q: 0.328003
 29700/100000: episode: 297, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.173, mean reward: 0.172 [0.022, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.730, 10.098], loss: 0.003736, mae: 0.067782, mean_q: 0.330768
 29800/100000: episode: 298, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.119, mean reward: 0.161 [0.003, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.743, 10.098], loss: 0.003766, mae: 0.067489, mean_q: 0.331664
 29900/100000: episode: 299, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.844, mean reward: 0.158 [0.006, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.433, 10.098], loss: 0.003869, mae: 0.068974, mean_q: 0.326188
 30000/100000: episode: 300, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 12.913, mean reward: 0.129 [0.014, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.235], loss: 0.003613, mae: 0.066049, mean_q: 0.325056
 30100/100000: episode: 301, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.501, mean reward: 0.145 [0.025, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.275], loss: 0.003787, mae: 0.067781, mean_q: 0.327568
 30200/100000: episode: 302, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 13.984, mean reward: 0.140 [0.017, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.527, 10.098], loss: 0.003759, mae: 0.067280, mean_q: 0.322740
 30300/100000: episode: 303, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.094, mean reward: 0.151 [0.013, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.590, 10.098], loss: 0.003756, mae: 0.067798, mean_q: 0.321687
 30400/100000: episode: 304, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.294, mean reward: 0.153 [0.015, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.098], loss: 0.003611, mae: 0.066175, mean_q: 0.317653
 30500/100000: episode: 305, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 14.414, mean reward: 0.144 [0.008, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.479, 10.282], loss: 0.003715, mae: 0.067726, mean_q: 0.322747
 30600/100000: episode: 306, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 17.473, mean reward: 0.175 [0.003, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.560, 10.160], loss: 0.003722, mae: 0.067318, mean_q: 0.321969
 30700/100000: episode: 307, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.000, mean reward: 0.150 [0.011, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.827, 10.269], loss: 0.004007, mae: 0.068614, mean_q: 0.317154
 30800/100000: episode: 308, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 19.928, mean reward: 0.199 [0.020, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.899, 10.434], loss: 0.003677, mae: 0.067219, mean_q: 0.314092
 30900/100000: episode: 309, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 18.354, mean reward: 0.184 [0.027, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.274, 10.232], loss: 0.003592, mae: 0.066001, mean_q: 0.315281
 31000/100000: episode: 310, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 24.387, mean reward: 0.244 [0.039, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.542, 10.098], loss: 0.003908, mae: 0.068779, mean_q: 0.320756
 31100/100000: episode: 311, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 19.618, mean reward: 0.196 [0.022, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.541, 10.254], loss: 0.004009, mae: 0.070215, mean_q: 0.327745
 31200/100000: episode: 312, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 18.232, mean reward: 0.182 [0.009, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.884, 10.098], loss: 0.003853, mae: 0.067985, mean_q: 0.327708
 31300/100000: episode: 313, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.619, mean reward: 0.146 [0.018, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.129], loss: 0.003758, mae: 0.067747, mean_q: 0.319957
 31400/100000: episode: 314, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 21.542, mean reward: 0.215 [0.017, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.315, 10.098], loss: 0.003801, mae: 0.067892, mean_q: 0.326079
 31500/100000: episode: 315, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 24.087, mean reward: 0.241 [0.016, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.412], loss: 0.003931, mae: 0.069074, mean_q: 0.329719
 31600/100000: episode: 316, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.583, mean reward: 0.166 [0.013, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.208], loss: 0.003730, mae: 0.067385, mean_q: 0.331822
 31700/100000: episode: 317, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 13.721, mean reward: 0.137 [0.004, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.739, 10.183], loss: 0.003979, mae: 0.070065, mean_q: 0.331389
 31800/100000: episode: 318, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 17.129, mean reward: 0.171 [0.003, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.379, 10.098], loss: 0.003491, mae: 0.065448, mean_q: 0.325215
 31900/100000: episode: 319, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.133, mean reward: 0.171 [0.019, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.157, 10.385], loss: 0.003868, mae: 0.069205, mean_q: 0.332128
 32000/100000: episode: 320, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 18.757, mean reward: 0.188 [0.027, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.937, 10.098], loss: 0.003921, mae: 0.069195, mean_q: 0.331177
 32100/100000: episode: 321, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.383, mean reward: 0.164 [0.015, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.445], loss: 0.003676, mae: 0.067039, mean_q: 0.326923
 32200/100000: episode: 322, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 14.997, mean reward: 0.150 [0.014, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.263, 10.136], loss: 0.003784, mae: 0.068091, mean_q: 0.329630
 32300/100000: episode: 323, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.883, mean reward: 0.159 [0.014, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.822, 10.295], loss: 0.003710, mae: 0.067661, mean_q: 0.333256
 32400/100000: episode: 324, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.716, mean reward: 0.167 [0.020, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.692, 10.291], loss: 0.003951, mae: 0.069226, mean_q: 0.331596
 32500/100000: episode: 325, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 16.579, mean reward: 0.166 [0.013, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.368, 10.098], loss: 0.003839, mae: 0.068823, mean_q: 0.329937
 32600/100000: episode: 326, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.082, mean reward: 0.151 [0.014, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.585, 10.098], loss: 0.003744, mae: 0.068252, mean_q: 0.331458
 32700/100000: episode: 327, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.310, mean reward: 0.153 [0.010, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.926, 10.100], loss: 0.003820, mae: 0.068584, mean_q: 0.330447
 32800/100000: episode: 328, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 18.405, mean reward: 0.184 [0.011, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.098], loss: 0.003674, mae: 0.066749, mean_q: 0.325573
 32900/100000: episode: 329, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.885, mean reward: 0.169 [0.009, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.235], loss: 0.003738, mae: 0.067717, mean_q: 0.328156
 33000/100000: episode: 330, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.894, mean reward: 0.169 [0.012, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.180, 10.144], loss: 0.003497, mae: 0.065800, mean_q: 0.328937
 33100/100000: episode: 331, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.908, mean reward: 0.159 [0.024, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.111, 10.098], loss: 0.003737, mae: 0.068148, mean_q: 0.330684
 33200/100000: episode: 332, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 19.558, mean reward: 0.196 [0.036, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.934, 10.319], loss: 0.003534, mae: 0.065896, mean_q: 0.323169
 33300/100000: episode: 333, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.013, mean reward: 0.160 [0.027, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.988, 10.098], loss: 0.003760, mae: 0.067700, mean_q: 0.327969
 33400/100000: episode: 334, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.669, mean reward: 0.177 [0.022, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.361, 10.098], loss: 0.003656, mae: 0.067478, mean_q: 0.326615
 33500/100000: episode: 335, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 13.935, mean reward: 0.139 [0.026, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.423, 10.098], loss: 0.003554, mae: 0.066241, mean_q: 0.326288
 33600/100000: episode: 336, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.188, mean reward: 0.162 [0.012, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.735, 10.193], loss: 0.003523, mae: 0.065972, mean_q: 0.326099
 33700/100000: episode: 337, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.778, mean reward: 0.168 [0.009, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.028, 10.323], loss: 0.003663, mae: 0.067089, mean_q: 0.325703
 33800/100000: episode: 338, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 16.538, mean reward: 0.165 [0.033, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.933, 10.290], loss: 0.003618, mae: 0.066385, mean_q: 0.323275
 33900/100000: episode: 339, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.643, mean reward: 0.166 [0.005, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.388, 10.098], loss: 0.003724, mae: 0.068067, mean_q: 0.326197
 34000/100000: episode: 340, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.881, mean reward: 0.179 [0.011, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.240], loss: 0.003695, mae: 0.067258, mean_q: 0.327951
 34100/100000: episode: 341, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.087, mean reward: 0.171 [0.018, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.256], loss: 0.003531, mae: 0.065710, mean_q: 0.328245
 34200/100000: episode: 342, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 19.732, mean reward: 0.197 [0.014, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.181, 10.098], loss: 0.003623, mae: 0.066825, mean_q: 0.326817
 34300/100000: episode: 343, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 18.319, mean reward: 0.183 [0.020, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.478, 10.244], loss: 0.003471, mae: 0.065668, mean_q: 0.326101
 34400/100000: episode: 344, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.746, mean reward: 0.207 [0.025, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.832, 10.098], loss: 0.003646, mae: 0.066932, mean_q: 0.331921
 34500/100000: episode: 345, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 19.723, mean reward: 0.197 [0.018, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.031, 10.411], loss: 0.003591, mae: 0.066510, mean_q: 0.331576
 34600/100000: episode: 346, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.547, mean reward: 0.185 [0.004, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.227, 10.098], loss: 0.003576, mae: 0.066971, mean_q: 0.333718
 34700/100000: episode: 347, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 26.896, mean reward: 0.269 [0.022, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.717, 10.143], loss: 0.003562, mae: 0.066133, mean_q: 0.335360
 34800/100000: episode: 348, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 17.824, mean reward: 0.178 [0.017, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.128, 10.457], loss: 0.003890, mae: 0.069045, mean_q: 0.341841
 34900/100000: episode: 349, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.136, mean reward: 0.161 [0.013, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.321], loss: 0.003592, mae: 0.066185, mean_q: 0.340120
 35000/100000: episode: 350, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.580, mean reward: 0.186 [0.013, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.342, 10.342], loss: 0.003536, mae: 0.066590, mean_q: 0.342290
 35100/100000: episode: 351, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 18.046, mean reward: 0.180 [0.010, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.382], loss: 0.003697, mae: 0.067188, mean_q: 0.341319
 35200/100000: episode: 352, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 13.820, mean reward: 0.138 [0.009, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.693, 10.138], loss: 0.003631, mae: 0.067360, mean_q: 0.344219
 35300/100000: episode: 353, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.083, mean reward: 0.171 [0.027, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.732, 10.098], loss: 0.003557, mae: 0.066555, mean_q: 0.342839
 35400/100000: episode: 354, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.515, mean reward: 0.175 [0.013, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.679, 10.225], loss: 0.003675, mae: 0.067121, mean_q: 0.343495
 35500/100000: episode: 355, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 22.647, mean reward: 0.226 [0.032, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.507, 10.287], loss: 0.003723, mae: 0.068189, mean_q: 0.344771
 35600/100000: episode: 356, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.988, mean reward: 0.160 [0.034, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.933, 10.172], loss: 0.003630, mae: 0.067009, mean_q: 0.350242
 35700/100000: episode: 357, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.397, mean reward: 0.174 [0.045, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.147, 10.098], loss: 0.003646, mae: 0.067896, mean_q: 0.346899
 35800/100000: episode: 358, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.060, mean reward: 0.151 [0.042, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.479, 10.224], loss: 0.003434, mae: 0.065577, mean_q: 0.345269
 35900/100000: episode: 359, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 14.197, mean reward: 0.142 [0.013, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.883, 10.130], loss: 0.003380, mae: 0.064665, mean_q: 0.344106
 36000/100000: episode: 360, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 15.826, mean reward: 0.158 [0.013, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.003873, mae: 0.068905, mean_q: 0.338611
 36100/100000: episode: 361, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 14.557, mean reward: 0.146 [0.008, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.924, 10.261], loss: 0.003608, mae: 0.066890, mean_q: 0.339909
 36200/100000: episode: 362, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 15.887, mean reward: 0.159 [0.026, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.206, 10.220], loss: 0.003841, mae: 0.069506, mean_q: 0.341154
 36300/100000: episode: 363, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: 15.691, mean reward: 0.157 [0.014, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.867, 10.256], loss: 0.003837, mae: 0.069004, mean_q: 0.338640
 36400/100000: episode: 364, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 15.463, mean reward: 0.155 [0.010, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.361, 10.098], loss: 0.003533, mae: 0.066847, mean_q: 0.337840
 36500/100000: episode: 365, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 17.336, mean reward: 0.173 [0.030, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.067, 10.128], loss: 0.003859, mae: 0.068893, mean_q: 0.338855
 36600/100000: episode: 366, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 16.381, mean reward: 0.164 [0.017, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.581, 10.112], loss: 0.003617, mae: 0.067261, mean_q: 0.335402
 36700/100000: episode: 367, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 18.016, mean reward: 0.180 [0.009, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.031, 10.098], loss: 0.003463, mae: 0.065611, mean_q: 0.335664
 36800/100000: episode: 368, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.236, mean reward: 0.152 [0.017, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.303, 10.098], loss: 0.003684, mae: 0.066872, mean_q: 0.333016
 36900/100000: episode: 369, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.609, mean reward: 0.166 [0.009, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.654, 10.195], loss: 0.003593, mae: 0.067163, mean_q: 0.333442
 37000/100000: episode: 370, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.422, mean reward: 0.164 [0.036, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.686, 10.163], loss: 0.003867, mae: 0.069188, mean_q: 0.336640
 37100/100000: episode: 371, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 14.187, mean reward: 0.142 [0.028, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.508, 10.098], loss: 0.003655, mae: 0.066894, mean_q: 0.335270
 37200/100000: episode: 372, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.099, mean reward: 0.151 [0.003, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.752, 10.098], loss: 0.003536, mae: 0.066348, mean_q: 0.334899
 37300/100000: episode: 373, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 23.789, mean reward: 0.238 [0.015, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.794, 10.400], loss: 0.003755, mae: 0.067591, mean_q: 0.335388
 37400/100000: episode: 374, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 18.780, mean reward: 0.188 [0.027, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.053, 10.258], loss: 0.003763, mae: 0.067337, mean_q: 0.335666
 37500/100000: episode: 375, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 14.127, mean reward: 0.141 [0.011, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.727, 10.162], loss: 0.003631, mae: 0.067593, mean_q: 0.333779
 37600/100000: episode: 376, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 14.954, mean reward: 0.150 [0.011, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.589, 10.436], loss: 0.003698, mae: 0.068180, mean_q: 0.334403
 37700/100000: episode: 377, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.019, mean reward: 0.160 [0.022, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.484, 10.098], loss: 0.003461, mae: 0.065529, mean_q: 0.338004
 37800/100000: episode: 378, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.047, mean reward: 0.170 [0.007, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.489, 10.196], loss: 0.003392, mae: 0.064735, mean_q: 0.334360
 37900/100000: episode: 379, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 17.924, mean reward: 0.179 [0.035, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.810, 10.332], loss: 0.003547, mae: 0.065501, mean_q: 0.337997
 38000/100000: episode: 380, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 11.760, mean reward: 0.118 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.151, 10.098], loss: 0.003623, mae: 0.067481, mean_q: 0.338515
 38100/100000: episode: 381, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.065, mean reward: 0.141 [0.032, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.762, 10.260], loss: 0.003689, mae: 0.068105, mean_q: 0.337156
 38200/100000: episode: 382, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 14.383, mean reward: 0.144 [0.005, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.959, 10.098], loss: 0.003425, mae: 0.065728, mean_q: 0.330715
 38300/100000: episode: 383, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.621, mean reward: 0.186 [0.023, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.559, 10.098], loss: 0.003559, mae: 0.066343, mean_q: 0.334036
 38400/100000: episode: 384, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 22.268, mean reward: 0.223 [0.010, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.001, 10.098], loss: 0.003815, mae: 0.069233, mean_q: 0.333989
 38500/100000: episode: 385, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.256, mean reward: 0.173 [0.015, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.373], loss: 0.003971, mae: 0.069988, mean_q: 0.338009
 38600/100000: episode: 386, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.264, mean reward: 0.173 [0.016, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.797, 10.098], loss: 0.003683, mae: 0.068308, mean_q: 0.333719
 38700/100000: episode: 387, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.849, mean reward: 0.148 [0.009, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.098], loss: 0.003709, mae: 0.067754, mean_q: 0.333722
 38800/100000: episode: 388, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 19.810, mean reward: 0.198 [0.039, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.571, 10.098], loss: 0.003562, mae: 0.066871, mean_q: 0.337097
 38900/100000: episode: 389, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.567, mean reward: 0.166 [0.023, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.098], loss: 0.003469, mae: 0.065767, mean_q: 0.337137
 39000/100000: episode: 390, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 19.389, mean reward: 0.194 [0.013, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.747, 10.331], loss: 0.003455, mae: 0.066147, mean_q: 0.339176
 39100/100000: episode: 391, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.938, mean reward: 0.139 [0.008, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.732, 10.156], loss: 0.003688, mae: 0.067141, mean_q: 0.335640
 39200/100000: episode: 392, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.610, mean reward: 0.146 [0.022, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.285, 10.162], loss: 0.003479, mae: 0.065523, mean_q: 0.336549
 39300/100000: episode: 393, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 14.898, mean reward: 0.149 [0.013, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.455, 10.098], loss: 0.003374, mae: 0.065263, mean_q: 0.337306
 39400/100000: episode: 394, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 13.444, mean reward: 0.134 [0.004, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.473, 10.268], loss: 0.003618, mae: 0.066994, mean_q: 0.336972
 39500/100000: episode: 395, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 17.841, mean reward: 0.178 [0.015, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.070, 10.098], loss: 0.003717, mae: 0.068660, mean_q: 0.334224
 39600/100000: episode: 396, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.532, mean reward: 0.155 [0.009, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.898, 10.098], loss: 0.003385, mae: 0.065281, mean_q: 0.331948
 39700/100000: episode: 397, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 21.696, mean reward: 0.217 [0.008, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.004, 10.304], loss: 0.003531, mae: 0.066182, mean_q: 0.327890
 39800/100000: episode: 398, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.108, mean reward: 0.151 [0.042, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.554, 10.138], loss: 0.003513, mae: 0.066173, mean_q: 0.329847
 39900/100000: episode: 399, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.861, mean reward: 0.159 [0.009, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.042, 10.098], loss: 0.003481, mae: 0.066231, mean_q: 0.327469
 40000/100000: episode: 400, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.280, mean reward: 0.173 [0.025, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.818, 10.098], loss: 0.003460, mae: 0.065218, mean_q: 0.325583
 40100/100000: episode: 401, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 19.618, mean reward: 0.196 [0.051, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.933, 10.098], loss: 0.003449, mae: 0.065419, mean_q: 0.324594
 40200/100000: episode: 402, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 13.953, mean reward: 0.140 [0.013, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.743, 10.098], loss: 0.003462, mae: 0.065341, mean_q: 0.325232
 40300/100000: episode: 403, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 17.441, mean reward: 0.174 [0.012, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.472, 10.098], loss: 0.003393, mae: 0.065366, mean_q: 0.322970
 40400/100000: episode: 404, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.403, mean reward: 0.194 [0.010, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.720, 10.248], loss: 0.003370, mae: 0.064935, mean_q: 0.326384
 40500/100000: episode: 405, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 18.451, mean reward: 0.185 [0.011, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.854, 10.274], loss: 0.003459, mae: 0.066381, mean_q: 0.325726
 40600/100000: episode: 406, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.215, mean reward: 0.162 [0.021, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.010, 10.198], loss: 0.003216, mae: 0.063197, mean_q: 0.322856
 40700/100000: episode: 407, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.650, mean reward: 0.156 [0.016, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.233], loss: 0.003391, mae: 0.065443, mean_q: 0.328307
 40800/100000: episode: 408, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.376, mean reward: 0.174 [0.009, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.697, 10.238], loss: 0.003435, mae: 0.065309, mean_q: 0.326183
 40900/100000: episode: 409, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.407, mean reward: 0.144 [0.010, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.436, 10.098], loss: 0.003363, mae: 0.064736, mean_q: 0.322122
 41000/100000: episode: 410, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 16.763, mean reward: 0.168 [0.012, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.336, 10.098], loss: 0.003257, mae: 0.063995, mean_q: 0.326071
 41100/100000: episode: 411, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.872, mean reward: 0.159 [0.004, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.304, 10.098], loss: 0.003279, mae: 0.064115, mean_q: 0.327486
 41200/100000: episode: 412, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.889, mean reward: 0.139 [0.023, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.279, 10.098], loss: 0.003308, mae: 0.064409, mean_q: 0.325884
 41300/100000: episode: 413, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 14.901, mean reward: 0.149 [0.022, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.107, 10.098], loss: 0.003303, mae: 0.064226, mean_q: 0.324069
 41400/100000: episode: 414, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 27.753, mean reward: 0.278 [0.048, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.160, 10.098], loss: 0.003200, mae: 0.063613, mean_q: 0.327417
 41500/100000: episode: 415, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 23.408, mean reward: 0.234 [0.014, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.113, 10.374], loss: 0.003271, mae: 0.064020, mean_q: 0.325912
 41600/100000: episode: 416, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 20.183, mean reward: 0.202 [0.021, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.820, 10.098], loss: 0.003400, mae: 0.065926, mean_q: 0.330385
 41700/100000: episode: 417, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.233, mean reward: 0.182 [0.001, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.928, 10.303], loss: 0.003533, mae: 0.066721, mean_q: 0.334113
 41800/100000: episode: 418, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.208, mean reward: 0.192 [0.016, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.665, 10.340], loss: 0.003447, mae: 0.065809, mean_q: 0.333480
 41900/100000: episode: 419, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.129, mean reward: 0.141 [0.032, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.199], loss: 0.003358, mae: 0.065154, mean_q: 0.332927
 42000/100000: episode: 420, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 19.019, mean reward: 0.190 [0.008, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.830, 10.112], loss: 0.003403, mae: 0.065258, mean_q: 0.334289
 42100/100000: episode: 421, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.926, mean reward: 0.189 [0.032, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.611, 10.118], loss: 0.003370, mae: 0.064915, mean_q: 0.339593
 42200/100000: episode: 422, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 15.140, mean reward: 0.151 [0.027, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.949, 10.098], loss: 0.003295, mae: 0.065043, mean_q: 0.336222
 42300/100000: episode: 423, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.638, mean reward: 0.186 [0.031, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.367, 10.146], loss: 0.003361, mae: 0.064122, mean_q: 0.335179
 42400/100000: episode: 424, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.207, mean reward: 0.162 [0.026, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.520, 10.281], loss: 0.003393, mae: 0.065520, mean_q: 0.338273
 42500/100000: episode: 425, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 18.748, mean reward: 0.187 [0.016, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.225, 10.098], loss: 0.003268, mae: 0.063821, mean_q: 0.335632
 42600/100000: episode: 426, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.774, mean reward: 0.168 [0.021, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.835, 10.201], loss: 0.003405, mae: 0.065347, mean_q: 0.336001
 42700/100000: episode: 427, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 19.288, mean reward: 0.193 [0.022, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.304, 10.163], loss: 0.003348, mae: 0.064857, mean_q: 0.336732
 42800/100000: episode: 428, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 17.975, mean reward: 0.180 [0.005, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.357], loss: 0.003408, mae: 0.065309, mean_q: 0.336400
 42900/100000: episode: 429, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 14.666, mean reward: 0.147 [0.008, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.098], loss: 0.003449, mae: 0.064778, mean_q: 0.337268
 43000/100000: episode: 430, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 19.238, mean reward: 0.192 [0.005, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.412, 10.463], loss: 0.003365, mae: 0.064974, mean_q: 0.340411
 43100/100000: episode: 431, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 21.850, mean reward: 0.219 [0.042, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.162, 10.098], loss: 0.003293, mae: 0.064296, mean_q: 0.340338
 43200/100000: episode: 432, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 15.192, mean reward: 0.152 [0.009, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.681, 10.152], loss: 0.003624, mae: 0.066556, mean_q: 0.345089
 43300/100000: episode: 433, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 17.801, mean reward: 0.178 [0.014, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.170, 10.098], loss: 0.003601, mae: 0.066531, mean_q: 0.344605
 43400/100000: episode: 434, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 14.863, mean reward: 0.149 [0.017, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.062, 10.098], loss: 0.003564, mae: 0.067010, mean_q: 0.338675
 43500/100000: episode: 435, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.253, mean reward: 0.203 [0.010, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.605, 10.213], loss: 0.003708, mae: 0.067598, mean_q: 0.342721
 43600/100000: episode: 436, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 13.371, mean reward: 0.134 [0.022, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.554, 10.098], loss: 0.003372, mae: 0.065325, mean_q: 0.339775
 43700/100000: episode: 437, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.003, mean reward: 0.180 [0.033, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.098], loss: 0.003489, mae: 0.065159, mean_q: 0.338172
 43800/100000: episode: 438, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.699, mean reward: 0.167 [0.024, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.186], loss: 0.003877, mae: 0.069108, mean_q: 0.339993
 43900/100000: episode: 439, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.879, mean reward: 0.159 [0.016, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.852, 10.098], loss: 0.003284, mae: 0.063997, mean_q: 0.339087
 44000/100000: episode: 440, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.553, mean reward: 0.166 [0.011, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.098], loss: 0.003437, mae: 0.064876, mean_q: 0.336490
 44100/100000: episode: 441, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.407, mean reward: 0.144 [0.005, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.732, 10.098], loss: 0.003428, mae: 0.065382, mean_q: 0.337509
 44200/100000: episode: 442, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 13.516, mean reward: 0.135 [0.009, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.187, 10.098], loss: 0.003195, mae: 0.062877, mean_q: 0.334684
 44300/100000: episode: 443, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 21.262, mean reward: 0.213 [0.014, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.782, 10.098], loss: 0.003406, mae: 0.064811, mean_q: 0.334954
 44400/100000: episode: 444, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 14.009, mean reward: 0.140 [0.014, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.462, 10.126], loss: 0.003264, mae: 0.063872, mean_q: 0.338109
 44500/100000: episode: 445, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.368, mean reward: 0.154 [0.026, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.101], loss: 0.003491, mae: 0.066085, mean_q: 0.338769
 44600/100000: episode: 446, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 16.557, mean reward: 0.166 [0.016, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.175, 10.098], loss: 0.003231, mae: 0.063557, mean_q: 0.341275
 44700/100000: episode: 447, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 17.775, mean reward: 0.178 [0.017, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.852, 10.098], loss: 0.003803, mae: 0.068604, mean_q: 0.337945
 44800/100000: episode: 448, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 22.811, mean reward: 0.228 [0.017, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.441, 10.098], loss: 0.003637, mae: 0.067291, mean_q: 0.339115
 44900/100000: episode: 449, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 21.301, mean reward: 0.213 [0.014, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.607, 10.098], loss: 0.003444, mae: 0.065256, mean_q: 0.342802
 45000/100000: episode: 450, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 19.112, mean reward: 0.191 [0.016, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.203, 10.272], loss: 0.003726, mae: 0.067576, mean_q: 0.345852
 45100/100000: episode: 451, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 19.404, mean reward: 0.194 [0.033, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.807, 10.416], loss: 0.003578, mae: 0.067305, mean_q: 0.342529
 45200/100000: episode: 452, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 17.084, mean reward: 0.171 [0.044, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.332, 10.098], loss: 0.003644, mae: 0.067248, mean_q: 0.346874
 45300/100000: episode: 453, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 16.704, mean reward: 0.167 [0.025, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.669, 10.374], loss: 0.003678, mae: 0.067249, mean_q: 0.345148
 45400/100000: episode: 454, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.439, mean reward: 0.154 [0.018, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.142], loss: 0.003393, mae: 0.065383, mean_q: 0.343693
 45500/100000: episode: 455, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 15.486, mean reward: 0.155 [0.016, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.233, 10.190], loss: 0.003654, mae: 0.066595, mean_q: 0.342998
 45600/100000: episode: 456, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.436, mean reward: 0.154 [0.008, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.952, 10.264], loss: 0.003484, mae: 0.065080, mean_q: 0.343736
 45700/100000: episode: 457, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.499, mean reward: 0.155 [0.010, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.135], loss: 0.003500, mae: 0.065731, mean_q: 0.343724
 45800/100000: episode: 458, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.130, mean reward: 0.161 [0.033, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.549, 10.098], loss: 0.003674, mae: 0.067007, mean_q: 0.338013
 45900/100000: episode: 459, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 17.803, mean reward: 0.178 [0.024, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.429, 10.172], loss: 0.003584, mae: 0.066612, mean_q: 0.342695
 46000/100000: episode: 460, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.551, mean reward: 0.136 [0.011, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.421, 10.119], loss: 0.003681, mae: 0.068045, mean_q: 0.342522
 46100/100000: episode: 461, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.124, mean reward: 0.151 [0.008, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.114, 10.098], loss: 0.003727, mae: 0.067531, mean_q: 0.343873
 46200/100000: episode: 462, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.926, mean reward: 0.149 [0.013, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.044, 10.128], loss: 0.003737, mae: 0.068028, mean_q: 0.340629
 46300/100000: episode: 463, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 19.752, mean reward: 0.198 [0.025, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.344, 10.098], loss: 0.003842, mae: 0.068607, mean_q: 0.342778
 46400/100000: episode: 464, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.673, mean reward: 0.157 [0.005, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.044, 10.191], loss: 0.003619, mae: 0.067375, mean_q: 0.340588
 46500/100000: episode: 465, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.826, mean reward: 0.158 [0.017, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.962, 10.098], loss: 0.003737, mae: 0.068145, mean_q: 0.335320
 46600/100000: episode: 466, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 14.768, mean reward: 0.148 [0.034, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.762, 10.246], loss: 0.003786, mae: 0.068153, mean_q: 0.332534
 46700/100000: episode: 467, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.484, mean reward: 0.155 [0.012, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.447, 10.098], loss: 0.003776, mae: 0.067931, mean_q: 0.328190
 46800/100000: episode: 468, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 18.159, mean reward: 0.182 [0.040, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.839, 10.275], loss: 0.003747, mae: 0.068032, mean_q: 0.334670
 46900/100000: episode: 469, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 27.258, mean reward: 0.273 [0.007, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.552, 10.451], loss: 0.003699, mae: 0.067611, mean_q: 0.332319
 47000/100000: episode: 470, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.538, mean reward: 0.165 [0.030, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.374, 10.176], loss: 0.003721, mae: 0.067598, mean_q: 0.337575
 47100/100000: episode: 471, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.626, mean reward: 0.186 [0.012, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.124, 10.098], loss: 0.003531, mae: 0.066435, mean_q: 0.334768
 47200/100000: episode: 472, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 18.925, mean reward: 0.189 [0.014, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.215, 10.380], loss: 0.003831, mae: 0.068383, mean_q: 0.334597
 47300/100000: episode: 473, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 20.293, mean reward: 0.203 [0.037, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.859, 10.237], loss: 0.003966, mae: 0.069153, mean_q: 0.337165
 47400/100000: episode: 474, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.367, mean reward: 0.164 [0.019, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.129, 10.155], loss: 0.003776, mae: 0.067814, mean_q: 0.339077
 47500/100000: episode: 475, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 19.987, mean reward: 0.200 [0.011, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.442], loss: 0.003656, mae: 0.066606, mean_q: 0.337057
 47600/100000: episode: 476, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 19.862, mean reward: 0.199 [0.007, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.910, 10.098], loss: 0.003660, mae: 0.067025, mean_q: 0.337014
 47700/100000: episode: 477, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.609, mean reward: 0.186 [0.020, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.623, 10.098], loss: 0.003454, mae: 0.065007, mean_q: 0.334261
 47800/100000: episode: 478, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.814, mean reward: 0.178 [0.068, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.763, 10.098], loss: 0.003767, mae: 0.067223, mean_q: 0.341369
 47900/100000: episode: 479, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 17.552, mean reward: 0.176 [0.029, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.757, 10.134], loss: 0.003579, mae: 0.065872, mean_q: 0.341917
 48000/100000: episode: 480, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.810, mean reward: 0.158 [0.030, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.783, 10.125], loss: 0.003623, mae: 0.066635, mean_q: 0.337834
 48100/100000: episode: 481, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 14.236, mean reward: 0.142 [0.015, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.229, 10.098], loss: 0.003731, mae: 0.066654, mean_q: 0.334622
 48200/100000: episode: 482, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 14.207, mean reward: 0.142 [0.017, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.599, 10.341], loss: 0.003787, mae: 0.067953, mean_q: 0.337785
 48300/100000: episode: 483, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 13.759, mean reward: 0.138 [0.006, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.518, 10.185], loss: 0.003599, mae: 0.066415, mean_q: 0.334779
 48400/100000: episode: 484, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.251, mean reward: 0.163 [0.012, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.098], loss: 0.003553, mae: 0.065991, mean_q: 0.337288
 48500/100000: episode: 485, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.075, mean reward: 0.161 [0.024, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.114], loss: 0.003568, mae: 0.066773, mean_q: 0.336549
 48600/100000: episode: 486, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 21.480, mean reward: 0.215 [0.066, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.300, 10.416], loss: 0.003645, mae: 0.066496, mean_q: 0.335869
 48700/100000: episode: 487, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.112, mean reward: 0.161 [0.026, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.116, 10.098], loss: 0.003549, mae: 0.065588, mean_q: 0.329362
 48800/100000: episode: 488, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 22.395, mean reward: 0.224 [0.035, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.542, 10.299], loss: 0.003873, mae: 0.069105, mean_q: 0.342689
 48900/100000: episode: 489, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.736, mean reward: 0.177 [0.023, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.211, 10.422], loss: 0.003571, mae: 0.066041, mean_q: 0.336611
 49000/100000: episode: 490, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.539, mean reward: 0.155 [0.009, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.837, 10.215], loss: 0.003905, mae: 0.068342, mean_q: 0.337077
 49100/100000: episode: 491, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.305, mean reward: 0.143 [0.023, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.098], loss: 0.003645, mae: 0.066502, mean_q: 0.342687
 49200/100000: episode: 492, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 19.988, mean reward: 0.200 [0.020, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.275, 10.383], loss: 0.003818, mae: 0.068698, mean_q: 0.342242
 49300/100000: episode: 493, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.986, mean reward: 0.210 [0.008, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.609, 10.098], loss: 0.003678, mae: 0.067126, mean_q: 0.339298
 49400/100000: episode: 494, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 16.768, mean reward: 0.168 [0.021, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.703, 10.238], loss: 0.003671, mae: 0.067075, mean_q: 0.341999
 49500/100000: episode: 495, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 23.217, mean reward: 0.232 [0.026, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.626, 10.098], loss: 0.003744, mae: 0.068198, mean_q: 0.343791
 49600/100000: episode: 496, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.822, mean reward: 0.168 [0.028, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.736, 10.188], loss: 0.004040, mae: 0.070413, mean_q: 0.345148
 49700/100000: episode: 497, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 13.140, mean reward: 0.131 [0.005, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.272, 10.098], loss: 0.003710, mae: 0.067924, mean_q: 0.346450
 49800/100000: episode: 498, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 17.626, mean reward: 0.176 [0.013, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.668, 10.299], loss: 0.003778, mae: 0.068463, mean_q: 0.344506
 49900/100000: episode: 499, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.772, mean reward: 0.178 [0.024, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.209, 10.098], loss: 0.003657, mae: 0.067338, mean_q: 0.342953
 50000/100000: episode: 500, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 15.578, mean reward: 0.156 [0.014, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.268, 10.241], loss: 0.003726, mae: 0.068183, mean_q: 0.337704
 50100/100000: episode: 501, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 17.071, mean reward: 0.171 [0.004, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.893, 10.197], loss: 0.003955, mae: 0.069632, mean_q: 0.339891
 50200/100000: episode: 502, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 15.406, mean reward: 0.154 [0.018, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.283, 10.098], loss: 0.003777, mae: 0.068202, mean_q: 0.335540
 50300/100000: episode: 503, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.990, mean reward: 0.160 [0.008, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.225, 10.098], loss: 0.003607, mae: 0.067335, mean_q: 0.335862
 50400/100000: episode: 504, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 18.738, mean reward: 0.187 [0.035, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.275, 10.321], loss: 0.003991, mae: 0.069639, mean_q: 0.337958
 50500/100000: episode: 505, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 13.925, mean reward: 0.139 [0.016, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.173], loss: 0.003883, mae: 0.069293, mean_q: 0.339714
 50600/100000: episode: 506, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 18.635, mean reward: 0.186 [0.029, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.098], loss: 0.003641, mae: 0.066236, mean_q: 0.341668
 50700/100000: episode: 507, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.043, mean reward: 0.150 [0.005, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.129], loss: 0.003602, mae: 0.066633, mean_q: 0.339891
 50800/100000: episode: 508, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 17.094, mean reward: 0.171 [0.018, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.124, 10.230], loss: 0.003853, mae: 0.068364, mean_q: 0.335437
 50900/100000: episode: 509, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 13.015, mean reward: 0.130 [0.012, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.095, 10.146], loss: 0.003577, mae: 0.066782, mean_q: 0.337728
 51000/100000: episode: 510, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 16.130, mean reward: 0.161 [0.015, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.685, 10.154], loss: 0.003777, mae: 0.068319, mean_q: 0.332112
 51100/100000: episode: 511, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.997, mean reward: 0.180 [0.020, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.480, 10.197], loss: 0.003873, mae: 0.069701, mean_q: 0.340886
 51200/100000: episode: 512, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 11.867, mean reward: 0.119 [0.012, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.662, 10.098], loss: 0.003618, mae: 0.066623, mean_q: 0.336131
 51300/100000: episode: 513, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 18.511, mean reward: 0.185 [0.011, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.650, 10.375], loss: 0.003552, mae: 0.066202, mean_q: 0.340269
 51400/100000: episode: 514, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 16.300, mean reward: 0.163 [0.017, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.151, 10.098], loss: 0.003748, mae: 0.067639, mean_q: 0.342417
 51500/100000: episode: 515, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.652, mean reward: 0.147 [0.022, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.131], loss: 0.003650, mae: 0.067573, mean_q: 0.337283
 51600/100000: episode: 516, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.509, mean reward: 0.165 [0.012, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.974, 10.098], loss: 0.003941, mae: 0.069792, mean_q: 0.342667
 51700/100000: episode: 517, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 19.725, mean reward: 0.197 [0.028, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.680, 10.266], loss: 0.003595, mae: 0.066695, mean_q: 0.340398
 51800/100000: episode: 518, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 17.178, mean reward: 0.172 [0.021, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.422, 10.495], loss: 0.003688, mae: 0.067369, mean_q: 0.339264
 51900/100000: episode: 519, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 12.641, mean reward: 0.126 [0.013, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.290], loss: 0.003839, mae: 0.069138, mean_q: 0.337860
 52000/100000: episode: 520, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 16.890, mean reward: 0.169 [0.033, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.641, 10.255], loss: 0.003865, mae: 0.068913, mean_q: 0.337026
 52100/100000: episode: 521, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 18.927, mean reward: 0.189 [0.005, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.098], loss: 0.003574, mae: 0.066968, mean_q: 0.334144
 52200/100000: episode: 522, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.231, mean reward: 0.152 [0.012, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.484, 10.195], loss: 0.003737, mae: 0.067767, mean_q: 0.336552
 52300/100000: episode: 523, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 15.598, mean reward: 0.156 [0.009, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.101], loss: 0.003726, mae: 0.068177, mean_q: 0.332672
 52400/100000: episode: 524, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 17.134, mean reward: 0.171 [0.027, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.167], loss: 0.003516, mae: 0.065762, mean_q: 0.328918
 52500/100000: episode: 525, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.885, mean reward: 0.169 [0.032, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.250], loss: 0.003651, mae: 0.067648, mean_q: 0.332192
 52600/100000: episode: 526, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.097, mean reward: 0.171 [0.029, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.630, 10.203], loss: 0.003400, mae: 0.065233, mean_q: 0.328632
 52700/100000: episode: 527, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.944, mean reward: 0.159 [0.002, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.272, 10.179], loss: 0.003498, mae: 0.065593, mean_q: 0.327023
 52800/100000: episode: 528, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 17.339, mean reward: 0.173 [0.017, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.961, 10.331], loss: 0.003670, mae: 0.067861, mean_q: 0.324287
 52900/100000: episode: 529, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 15.476, mean reward: 0.155 [0.016, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.229], loss: 0.003561, mae: 0.066760, mean_q: 0.327428
 53000/100000: episode: 530, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 13.758, mean reward: 0.138 [0.018, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.619, 10.098], loss: 0.003636, mae: 0.066758, mean_q: 0.327656
 53100/100000: episode: 531, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 15.662, mean reward: 0.157 [0.014, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.666, 10.122], loss: 0.003273, mae: 0.063797, mean_q: 0.327710
 53200/100000: episode: 532, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 15.737, mean reward: 0.157 [0.017, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.641, 10.098], loss: 0.003654, mae: 0.067566, mean_q: 0.327579
 53300/100000: episode: 533, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.151, mean reward: 0.162 [0.014, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.076, 10.248], loss: 0.003473, mae: 0.065687, mean_q: 0.330073
 53400/100000: episode: 534, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.925, mean reward: 0.179 [0.020, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.670, 10.098], loss: 0.003944, mae: 0.070522, mean_q: 0.329855
 53500/100000: episode: 535, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.545, mean reward: 0.185 [0.022, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.822, 10.098], loss: 0.003533, mae: 0.066114, mean_q: 0.330046
 53600/100000: episode: 536, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 17.728, mean reward: 0.177 [0.033, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.552, 10.208], loss: 0.003328, mae: 0.065041, mean_q: 0.328005
 53700/100000: episode: 537, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 19.562, mean reward: 0.196 [0.009, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.154, 10.098], loss: 0.003726, mae: 0.068134, mean_q: 0.328435
 53800/100000: episode: 538, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 18.246, mean reward: 0.182 [0.020, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.255], loss: 0.003952, mae: 0.070275, mean_q: 0.324351
 53900/100000: episode: 539, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 20.802, mean reward: 0.208 [0.020, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.413, 10.098], loss: 0.003804, mae: 0.069383, mean_q: 0.329700
 54000/100000: episode: 540, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 18.451, mean reward: 0.185 [0.029, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.576, 10.247], loss: 0.003769, mae: 0.068409, mean_q: 0.332121
 54100/100000: episode: 541, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 15.286, mean reward: 0.153 [0.008, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.198, 10.371], loss: 0.003791, mae: 0.068616, mean_q: 0.329391
 54200/100000: episode: 542, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 12.973, mean reward: 0.130 [0.011, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.855, 10.106], loss: 0.003897, mae: 0.069347, mean_q: 0.326558
 54300/100000: episode: 543, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 19.505, mean reward: 0.195 [0.045, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.525, 10.098], loss: 0.003623, mae: 0.066656, mean_q: 0.320765
 54400/100000: episode: 544, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.618, mean reward: 0.166 [0.043, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.159, 10.192], loss: 0.003631, mae: 0.066615, mean_q: 0.329342
 54500/100000: episode: 545, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.054, mean reward: 0.161 [0.036, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.469, 10.098], loss: 0.003908, mae: 0.069345, mean_q: 0.324461
 54600/100000: episode: 546, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 13.700, mean reward: 0.137 [0.003, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.892, 10.162], loss: 0.003481, mae: 0.065708, mean_q: 0.325373
 54700/100000: episode: 547, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.979, mean reward: 0.180 [0.013, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.522, 10.098], loss: 0.003742, mae: 0.067215, mean_q: 0.324404
 54800/100000: episode: 548, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 14.688, mean reward: 0.147 [0.011, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.758, 10.236], loss: 0.003581, mae: 0.066390, mean_q: 0.319795
 54900/100000: episode: 549, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 18.765, mean reward: 0.188 [0.015, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.287, 10.259], loss: 0.003389, mae: 0.064959, mean_q: 0.323947
 55000/100000: episode: 550, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 13.545, mean reward: 0.135 [0.008, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.573, 10.205], loss: 0.003588, mae: 0.066812, mean_q: 0.323876
 55100/100000: episode: 551, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 15.149, mean reward: 0.151 [0.015, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.817, 10.191], loss: 0.003447, mae: 0.065395, mean_q: 0.320590
 55200/100000: episode: 552, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.722, mean reward: 0.167 [0.037, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.098], loss: 0.003525, mae: 0.066083, mean_q: 0.323885
 55300/100000: episode: 553, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 11.928, mean reward: 0.119 [0.002, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.440, 10.288], loss: 0.003589, mae: 0.066679, mean_q: 0.324148
 55400/100000: episode: 554, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 11.934, mean reward: 0.119 [0.008, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.110, 10.228], loss: 0.003377, mae: 0.064585, mean_q: 0.323178
 55500/100000: episode: 555, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 17.780, mean reward: 0.178 [0.023, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.098], loss: 0.003445, mae: 0.065513, mean_q: 0.323651
 55600/100000: episode: 556, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.580, mean reward: 0.186 [0.004, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.486, 10.176], loss: 0.003594, mae: 0.066844, mean_q: 0.322967
 55700/100000: episode: 557, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 18.454, mean reward: 0.185 [0.032, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.118], loss: 0.003677, mae: 0.068015, mean_q: 0.325795
 55800/100000: episode: 558, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.103, mean reward: 0.171 [0.012, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.147, 10.098], loss: 0.003574, mae: 0.066938, mean_q: 0.322700
 55900/100000: episode: 559, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.485, mean reward: 0.175 [0.045, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.989, 10.176], loss: 0.003659, mae: 0.068006, mean_q: 0.323687
 56000/100000: episode: 560, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 27.294, mean reward: 0.273 [0.016, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.948, 10.098], loss: 0.003543, mae: 0.066232, mean_q: 0.324926
 56100/100000: episode: 561, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.423, mean reward: 0.144 [0.023, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.383, 10.098], loss: 0.003639, mae: 0.066630, mean_q: 0.321798
 56200/100000: episode: 562, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 14.839, mean reward: 0.148 [0.009, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.493, 10.221], loss: 0.003930, mae: 0.069066, mean_q: 0.327763
 56300/100000: episode: 563, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.094, mean reward: 0.171 [0.024, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.270, 10.307], loss: 0.003747, mae: 0.067765, mean_q: 0.329243
 56400/100000: episode: 564, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 17.297, mean reward: 0.173 [0.010, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.741, 10.098], loss: 0.003859, mae: 0.068846, mean_q: 0.325761
 56500/100000: episode: 565, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 19.388, mean reward: 0.194 [0.028, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.441, 10.264], loss: 0.003736, mae: 0.067632, mean_q: 0.327573
 56600/100000: episode: 566, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.853, mean reward: 0.159 [0.004, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.021, 10.318], loss: 0.003597, mae: 0.066179, mean_q: 0.330326
 56700/100000: episode: 567, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 18.803, mean reward: 0.188 [0.020, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.364, 10.147], loss: 0.003986, mae: 0.069160, mean_q: 0.330286
 56800/100000: episode: 568, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 22.150, mean reward: 0.221 [0.026, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.418], loss: 0.003688, mae: 0.067017, mean_q: 0.333527
 56900/100000: episode: 569, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 16.340, mean reward: 0.163 [0.018, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.368, 10.191], loss: 0.003480, mae: 0.065167, mean_q: 0.333517
 57000/100000: episode: 570, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.290, mean reward: 0.183 [0.026, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.833, 10.098], loss: 0.003792, mae: 0.067832, mean_q: 0.337411
 57100/100000: episode: 571, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 17.224, mean reward: 0.172 [0.006, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.038, 10.315], loss: 0.003907, mae: 0.069532, mean_q: 0.338378
 57200/100000: episode: 572, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.696, mean reward: 0.157 [0.008, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.968, 10.260], loss: 0.003595, mae: 0.066081, mean_q: 0.333224
 57300/100000: episode: 573, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.417, mean reward: 0.144 [0.002, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.003764, mae: 0.067707, mean_q: 0.331603
 57400/100000: episode: 574, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.862, mean reward: 0.169 [0.006, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.602, 10.098], loss: 0.003670, mae: 0.067059, mean_q: 0.331872
 57500/100000: episode: 575, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 18.851, mean reward: 0.189 [0.031, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.408, 10.098], loss: 0.003626, mae: 0.067178, mean_q: 0.334309
 57600/100000: episode: 576, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.959, mean reward: 0.160 [0.026, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.454, 10.167], loss: 0.003631, mae: 0.066502, mean_q: 0.331346
 57700/100000: episode: 577, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.943, mean reward: 0.169 [0.010, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.618, 10.460], loss: 0.003464, mae: 0.065715, mean_q: 0.334042
 57800/100000: episode: 578, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 22.371, mean reward: 0.224 [0.010, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.539, 10.098], loss: 0.003639, mae: 0.067130, mean_q: 0.332879
 57900/100000: episode: 579, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 14.529, mean reward: 0.145 [0.014, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.696, 10.098], loss: 0.003569, mae: 0.066317, mean_q: 0.334479
 58000/100000: episode: 580, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 17.623, mean reward: 0.176 [0.003, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.142, 10.223], loss: 0.003712, mae: 0.067492, mean_q: 0.337341
 58100/100000: episode: 581, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.362, mean reward: 0.194 [0.016, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.313, 10.098], loss: 0.003756, mae: 0.068345, mean_q: 0.336787
 58200/100000: episode: 582, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 14.101, mean reward: 0.141 [0.016, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.809, 10.098], loss: 0.004052, mae: 0.070443, mean_q: 0.338044
 58300/100000: episode: 583, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.956, mean reward: 0.160 [0.033, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.191, 10.116], loss: 0.003890, mae: 0.069071, mean_q: 0.336369
 58400/100000: episode: 584, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 21.544, mean reward: 0.215 [0.009, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.077, 10.098], loss: 0.003430, mae: 0.065015, mean_q: 0.337124
 58500/100000: episode: 585, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.671, mean reward: 0.157 [0.019, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.339, 10.098], loss: 0.003907, mae: 0.069461, mean_q: 0.340677
 58600/100000: episode: 586, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 20.619, mean reward: 0.206 [0.051, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.494, 10.383], loss: 0.003780, mae: 0.068365, mean_q: 0.338091
 58700/100000: episode: 587, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 17.500, mean reward: 0.175 [0.017, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.039, 10.323], loss: 0.003842, mae: 0.069152, mean_q: 0.335210
 58800/100000: episode: 588, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.532, mean reward: 0.155 [0.022, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.402, 10.199], loss: 0.003652, mae: 0.067491, mean_q: 0.336986
 58900/100000: episode: 589, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.220, mean reward: 0.182 [0.040, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.683, 10.098], loss: 0.003720, mae: 0.067317, mean_q: 0.336162
 59000/100000: episode: 590, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 16.160, mean reward: 0.162 [0.022, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.756, 10.098], loss: 0.003673, mae: 0.067602, mean_q: 0.335939
 59100/100000: episode: 591, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 13.576, mean reward: 0.136 [0.017, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.753, 10.098], loss: 0.003728, mae: 0.067972, mean_q: 0.337309
 59200/100000: episode: 592, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.222, mean reward: 0.182 [0.022, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.791, 10.121], loss: 0.003779, mae: 0.068011, mean_q: 0.336082
 59300/100000: episode: 593, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.604, mean reward: 0.176 [0.020, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.187, 10.098], loss: 0.003791, mae: 0.068046, mean_q: 0.340178
 59400/100000: episode: 594, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 14.047, mean reward: 0.140 [0.015, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.655, 10.240], loss: 0.003618, mae: 0.066917, mean_q: 0.332463
 59500/100000: episode: 595, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.559, mean reward: 0.156 [0.010, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.865, 10.135], loss: 0.003832, mae: 0.068695, mean_q: 0.333677
 59600/100000: episode: 596, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.709, mean reward: 0.167 [0.021, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.186, 10.098], loss: 0.003659, mae: 0.066882, mean_q: 0.335159
 59700/100000: episode: 597, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.752, mean reward: 0.188 [0.008, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.777, 10.098], loss: 0.003734, mae: 0.067403, mean_q: 0.338080
 59800/100000: episode: 598, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.049, mean reward: 0.150 [0.029, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.093, 10.098], loss: 0.004026, mae: 0.070428, mean_q: 0.340025
 59900/100000: episode: 599, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 12.027, mean reward: 0.120 [0.013, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.051, 10.179], loss: 0.003581, mae: 0.065519, mean_q: 0.337070
 60000/100000: episode: 600, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 17.337, mean reward: 0.173 [0.011, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.239, 10.112], loss: 0.003823, mae: 0.067903, mean_q: 0.333607
 60100/100000: episode: 601, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.592, mean reward: 0.156 [0.021, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.886, 10.098], loss: 0.003841, mae: 0.068204, mean_q: 0.333207
 60200/100000: episode: 602, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 13.072, mean reward: 0.131 [0.025, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.863, 10.098], loss: 0.003722, mae: 0.067370, mean_q: 0.324977
 60300/100000: episode: 603, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 12.692, mean reward: 0.127 [0.005, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.156, 10.160], loss: 0.003809, mae: 0.068013, mean_q: 0.332783
 60400/100000: episode: 604, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.939, mean reward: 0.159 [0.014, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.631, 10.176], loss: 0.003783, mae: 0.068361, mean_q: 0.335726
 60500/100000: episode: 605, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 19.235, mean reward: 0.192 [0.051, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.845, 10.202], loss: 0.003815, mae: 0.068120, mean_q: 0.334105
 60600/100000: episode: 606, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.759, mean reward: 0.138 [0.019, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.609, 10.121], loss: 0.003704, mae: 0.067432, mean_q: 0.332207
 60700/100000: episode: 607, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.889, mean reward: 0.159 [0.011, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.045, 10.157], loss: 0.003786, mae: 0.067964, mean_q: 0.326708
 60800/100000: episode: 608, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.879, mean reward: 0.169 [0.022, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.538, 10.098], loss: 0.003813, mae: 0.067971, mean_q: 0.333433
 60900/100000: episode: 609, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.613, mean reward: 0.186 [0.054, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.519, 10.380], loss: 0.003801, mae: 0.068410, mean_q: 0.332997
 61000/100000: episode: 610, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.239, mean reward: 0.172 [0.008, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.937, 10.377], loss: 0.003831, mae: 0.068120, mean_q: 0.333848
 61100/100000: episode: 611, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 20.986, mean reward: 0.210 [0.012, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.248, 10.098], loss: 0.003654, mae: 0.066219, mean_q: 0.334461
 61200/100000: episode: 612, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.184, mean reward: 0.172 [0.012, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.064, 10.098], loss: 0.003723, mae: 0.067476, mean_q: 0.337985
 61300/100000: episode: 613, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 19.197, mean reward: 0.192 [0.014, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.282], loss: 0.003739, mae: 0.067798, mean_q: 0.330934
 61400/100000: episode: 614, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.983, mean reward: 0.170 [0.020, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.290, 10.239], loss: 0.003716, mae: 0.067645, mean_q: 0.331831
 61500/100000: episode: 615, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.571, mean reward: 0.146 [0.009, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.476, 10.217], loss: 0.003632, mae: 0.066643, mean_q: 0.330126
 61600/100000: episode: 616, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.411, mean reward: 0.154 [0.018, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.236, 10.098], loss: 0.003521, mae: 0.065814, mean_q: 0.332055
 61700/100000: episode: 617, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.793, mean reward: 0.178 [0.012, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.844, 10.098], loss: 0.004020, mae: 0.070332, mean_q: 0.329653
 61800/100000: episode: 618, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 21.950, mean reward: 0.219 [0.021, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.098], loss: 0.003935, mae: 0.069806, mean_q: 0.327662
 61900/100000: episode: 619, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.350, mean reward: 0.164 [0.015, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.517, 10.148], loss: 0.003750, mae: 0.067303, mean_q: 0.334450
 62000/100000: episode: 620, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 15.850, mean reward: 0.158 [0.015, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.457, 10.098], loss: 0.003826, mae: 0.069010, mean_q: 0.329826
 62100/100000: episode: 621, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 13.703, mean reward: 0.137 [0.001, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.122], loss: 0.003688, mae: 0.067936, mean_q: 0.331398
 62200/100000: episode: 622, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 13.640, mean reward: 0.136 [0.030, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.524, 10.138], loss: 0.003733, mae: 0.067979, mean_q: 0.331967
 62300/100000: episode: 623, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.664, mean reward: 0.157 [0.024, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.010, 10.156], loss: 0.003957, mae: 0.070413, mean_q: 0.329789
 62400/100000: episode: 624, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 15.461, mean reward: 0.155 [0.032, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.123], loss: 0.003583, mae: 0.066458, mean_q: 0.327606
 62500/100000: episode: 625, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.946, mean reward: 0.209 [0.021, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.157, 10.098], loss: 0.003780, mae: 0.068768, mean_q: 0.330950
 62600/100000: episode: 626, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 14.033, mean reward: 0.140 [0.038, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.191], loss: 0.003806, mae: 0.068666, mean_q: 0.329377
 62700/100000: episode: 627, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 21.767, mean reward: 0.218 [0.033, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.283, 10.226], loss: 0.003580, mae: 0.066071, mean_q: 0.324151
 62800/100000: episode: 628, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.628, mean reward: 0.156 [0.034, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.550, 10.098], loss: 0.003619, mae: 0.067500, mean_q: 0.328113
 62900/100000: episode: 629, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 17.046, mean reward: 0.170 [0.029, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.986, 10.291], loss: 0.003868, mae: 0.069443, mean_q: 0.328262
 63000/100000: episode: 630, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.121, mean reward: 0.151 [0.022, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.041, 10.098], loss: 0.003705, mae: 0.066914, mean_q: 0.329064
 63100/100000: episode: 631, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 14.591, mean reward: 0.146 [0.011, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.627, 10.148], loss: 0.003553, mae: 0.066803, mean_q: 0.325758
 63200/100000: episode: 632, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.739, mean reward: 0.147 [0.005, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.415, 10.098], loss: 0.003654, mae: 0.067335, mean_q: 0.322431
 63300/100000: episode: 633, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 13.793, mean reward: 0.138 [0.003, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.198, 10.124], loss: 0.003758, mae: 0.068365, mean_q: 0.323670
 63400/100000: episode: 634, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.885, mean reward: 0.159 [0.011, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.098], loss: 0.003780, mae: 0.068072, mean_q: 0.322071
 63500/100000: episode: 635, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 22.314, mean reward: 0.223 [0.014, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.683, 10.098], loss: 0.003788, mae: 0.068296, mean_q: 0.323397
 63600/100000: episode: 636, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 18.698, mean reward: 0.187 [0.019, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.437, 10.098], loss: 0.003727, mae: 0.068018, mean_q: 0.323974
 63700/100000: episode: 637, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 20.541, mean reward: 0.205 [0.030, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.440, 10.429], loss: 0.003634, mae: 0.066489, mean_q: 0.325680
 63800/100000: episode: 638, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 14.154, mean reward: 0.142 [0.011, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.035, 10.098], loss: 0.003667, mae: 0.068378, mean_q: 0.324933
 63900/100000: episode: 639, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.322, mean reward: 0.143 [0.028, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.535, 10.098], loss: 0.003898, mae: 0.069180, mean_q: 0.328295
 64000/100000: episode: 640, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 14.903, mean reward: 0.149 [0.016, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.118], loss: 0.003892, mae: 0.069076, mean_q: 0.322051
 64100/100000: episode: 641, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.028, mean reward: 0.160 [0.005, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.529, 10.098], loss: 0.003697, mae: 0.067703, mean_q: 0.319006
 64200/100000: episode: 642, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.630, mean reward: 0.176 [0.017, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.834, 10.098], loss: 0.003655, mae: 0.067970, mean_q: 0.321250
 64300/100000: episode: 643, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 14.302, mean reward: 0.143 [0.019, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.837, 10.098], loss: 0.003580, mae: 0.066283, mean_q: 0.322497
 64400/100000: episode: 644, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.950, mean reward: 0.180 [0.008, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.098], loss: 0.003873, mae: 0.069088, mean_q: 0.323783
 64500/100000: episode: 645, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 14.684, mean reward: 0.147 [0.026, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.892, 10.318], loss: 0.004002, mae: 0.070230, mean_q: 0.322268
 64600/100000: episode: 646, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.381, mean reward: 0.194 [0.018, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.454, 10.162], loss: 0.003967, mae: 0.070205, mean_q: 0.325959
 64700/100000: episode: 647, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.390, mean reward: 0.164 [0.028, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.512, 10.098], loss: 0.003936, mae: 0.069676, mean_q: 0.322328
 64800/100000: episode: 648, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.376, mean reward: 0.154 [0.028, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.259, 10.098], loss: 0.003836, mae: 0.068698, mean_q: 0.322612
 64900/100000: episode: 649, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.029, mean reward: 0.140 [0.031, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.463, 10.098], loss: 0.003697, mae: 0.068113, mean_q: 0.322519
 65000/100000: episode: 650, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 14.592, mean reward: 0.146 [0.020, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.192, 10.207], loss: 0.003836, mae: 0.069013, mean_q: 0.321527
 65100/100000: episode: 651, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.082, mean reward: 0.151 [0.008, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.995, 10.113], loss: 0.003880, mae: 0.069190, mean_q: 0.321102
 65200/100000: episode: 652, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 13.809, mean reward: 0.138 [0.010, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.795, 10.241], loss: 0.003818, mae: 0.068488, mean_q: 0.320870
 65300/100000: episode: 653, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.128, mean reward: 0.181 [0.018, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.235], loss: 0.003740, mae: 0.068680, mean_q: 0.323825
 65400/100000: episode: 654, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.825, mean reward: 0.168 [0.004, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.779, 10.098], loss: 0.003918, mae: 0.069332, mean_q: 0.324740
 65500/100000: episode: 655, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.514, mean reward: 0.185 [0.025, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.206, 10.142], loss: 0.003943, mae: 0.069558, mean_q: 0.327497
 65600/100000: episode: 656, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 16.440, mean reward: 0.164 [0.018, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.103, 10.255], loss: 0.003733, mae: 0.068234, mean_q: 0.327970
 65700/100000: episode: 657, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.155, mean reward: 0.152 [0.011, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.744, 10.121], loss: 0.003897, mae: 0.069679, mean_q: 0.326507
 65800/100000: episode: 658, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 13.871, mean reward: 0.139 [0.036, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.734, 10.098], loss: 0.003640, mae: 0.066866, mean_q: 0.322223
 65900/100000: episode: 659, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 13.958, mean reward: 0.140 [0.016, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.968, 10.221], loss: 0.003885, mae: 0.069500, mean_q: 0.318254
 66000/100000: episode: 660, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 21.055, mean reward: 0.211 [0.013, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.362, 10.098], loss: 0.003816, mae: 0.068969, mean_q: 0.324570
 66100/100000: episode: 661, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 11.804, mean reward: 0.118 [0.004, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.154], loss: 0.003683, mae: 0.067648, mean_q: 0.322071
 66200/100000: episode: 662, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.078, mean reward: 0.141 [0.029, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.256, 10.201], loss: 0.003881, mae: 0.069272, mean_q: 0.320896
 66300/100000: episode: 663, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.102, mean reward: 0.161 [0.020, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.461, 10.144], loss: 0.003878, mae: 0.069690, mean_q: 0.321188
 66400/100000: episode: 664, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.895, mean reward: 0.169 [0.014, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.098], loss: 0.003530, mae: 0.066128, mean_q: 0.317733
 66500/100000: episode: 665, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 12.921, mean reward: 0.129 [0.006, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.003823, mae: 0.068547, mean_q: 0.320007
 66600/100000: episode: 666, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 14.743, mean reward: 0.147 [0.010, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.495, 10.098], loss: 0.003746, mae: 0.067739, mean_q: 0.318284
 66700/100000: episode: 667, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 15.931, mean reward: 0.159 [0.032, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.655, 10.218], loss: 0.003896, mae: 0.069983, mean_q: 0.321402
 66800/100000: episode: 668, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.319, mean reward: 0.163 [0.018, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.600, 10.098], loss: 0.003929, mae: 0.069556, mean_q: 0.312294
 66900/100000: episode: 669, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 18.797, mean reward: 0.188 [0.030, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.027, 10.098], loss: 0.003714, mae: 0.068461, mean_q: 0.319120
 67000/100000: episode: 670, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 23.189, mean reward: 0.232 [0.013, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.705, 10.314], loss: 0.003783, mae: 0.068014, mean_q: 0.316685
 67100/100000: episode: 671, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 17.834, mean reward: 0.178 [0.031, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.761, 10.349], loss: 0.003804, mae: 0.068273, mean_q: 0.320406
 67200/100000: episode: 672, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.459, mean reward: 0.185 [0.049, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.559, 10.098], loss: 0.003676, mae: 0.067188, mean_q: 0.319707
 67300/100000: episode: 673, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 12.712, mean reward: 0.127 [0.010, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.302, 10.264], loss: 0.003767, mae: 0.068403, mean_q: 0.322995
 67400/100000: episode: 674, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.304, mean reward: 0.163 [0.048, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.098], loss: 0.003884, mae: 0.069830, mean_q: 0.323831
 67500/100000: episode: 675, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.326, mean reward: 0.153 [0.022, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.010, 10.206], loss: 0.003906, mae: 0.069726, mean_q: 0.320587
 67600/100000: episode: 676, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 13.594, mean reward: 0.136 [0.023, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.211, 10.105], loss: 0.004010, mae: 0.070354, mean_q: 0.320138
 67700/100000: episode: 677, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 20.676, mean reward: 0.207 [0.025, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.826, 10.098], loss: 0.003802, mae: 0.067485, mean_q: 0.318981
 67800/100000: episode: 678, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.114, mean reward: 0.191 [0.048, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.709, 10.355], loss: 0.004001, mae: 0.069683, mean_q: 0.321227
 67900/100000: episode: 679, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 21.326, mean reward: 0.213 [0.020, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.140, 10.150], loss: 0.003840, mae: 0.068983, mean_q: 0.320148
 68000/100000: episode: 680, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 18.148, mean reward: 0.181 [0.009, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.040, 10.287], loss: 0.003888, mae: 0.068995, mean_q: 0.321567
 68100/100000: episode: 681, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.974, mean reward: 0.160 [0.013, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.951, 10.206], loss: 0.003668, mae: 0.067083, mean_q: 0.322466
 68200/100000: episode: 682, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.201, mean reward: 0.152 [0.013, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.548, 10.287], loss: 0.003929, mae: 0.069530, mean_q: 0.327767
 68300/100000: episode: 683, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.105, mean reward: 0.151 [0.018, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.190, 10.177], loss: 0.003909, mae: 0.069665, mean_q: 0.328375
 68400/100000: episode: 684, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.558, mean reward: 0.166 [0.010, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.959, 10.391], loss: 0.003715, mae: 0.067784, mean_q: 0.322487
 68500/100000: episode: 685, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 15.231, mean reward: 0.152 [0.012, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.815, 10.098], loss: 0.003915, mae: 0.068941, mean_q: 0.322835
 68600/100000: episode: 686, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.651, mean reward: 0.157 [0.036, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.476, 10.186], loss: 0.003632, mae: 0.067004, mean_q: 0.318153
 68700/100000: episode: 687, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 13.789, mean reward: 0.138 [0.024, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.487, 10.098], loss: 0.003710, mae: 0.067712, mean_q: 0.318563
 68800/100000: episode: 688, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.911, mean reward: 0.189 [0.015, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.107, 10.098], loss: 0.004061, mae: 0.071090, mean_q: 0.319030
 68900/100000: episode: 689, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.595, mean reward: 0.146 [0.016, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.196, 10.098], loss: 0.003757, mae: 0.067342, mean_q: 0.315156
 69000/100000: episode: 690, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 17.816, mean reward: 0.178 [0.038, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.465, 10.098], loss: 0.004013, mae: 0.070076, mean_q: 0.319542
 69100/100000: episode: 691, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.738, mean reward: 0.187 [0.013, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.222], loss: 0.003796, mae: 0.068168, mean_q: 0.321769
 69200/100000: episode: 692, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 18.908, mean reward: 0.189 [0.021, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.485, 10.098], loss: 0.003789, mae: 0.067732, mean_q: 0.321028
 69300/100000: episode: 693, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 13.656, mean reward: 0.137 [0.013, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.098], loss: 0.003590, mae: 0.066490, mean_q: 0.321134
 69400/100000: episode: 694, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 14.963, mean reward: 0.150 [0.007, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.938, 10.194], loss: 0.003691, mae: 0.068052, mean_q: 0.321598
 69500/100000: episode: 695, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.425, mean reward: 0.164 [0.006, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.779, 10.219], loss: 0.003600, mae: 0.066766, mean_q: 0.315752
 69600/100000: episode: 696, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 27.335, mean reward: 0.273 [0.010, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.366, 10.494], loss: 0.003637, mae: 0.066630, mean_q: 0.320349
 69700/100000: episode: 697, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 20.883, mean reward: 0.209 [0.033, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.802, 10.435], loss: 0.003827, mae: 0.068256, mean_q: 0.325794
 69800/100000: episode: 698, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 16.403, mean reward: 0.164 [0.007, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.909, 10.098], loss: 0.003622, mae: 0.066608, mean_q: 0.327025
 69900/100000: episode: 699, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 17.588, mean reward: 0.176 [0.017, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.737, 10.098], loss: 0.003805, mae: 0.067604, mean_q: 0.325829
 70000/100000: episode: 700, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.864, mean reward: 0.159 [0.006, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.224, 10.098], loss: 0.003596, mae: 0.067498, mean_q: 0.326804
 70100/100000: episode: 701, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 20.638, mean reward: 0.206 [0.017, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.300, 10.375], loss: 0.003804, mae: 0.068566, mean_q: 0.329241
 70200/100000: episode: 702, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.801, mean reward: 0.158 [0.020, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.191], loss: 0.003705, mae: 0.068082, mean_q: 0.330535
 70300/100000: episode: 703, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.130, mean reward: 0.181 [0.049, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.325], loss: 0.003568, mae: 0.066183, mean_q: 0.332751
 70400/100000: episode: 704, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.512, mean reward: 0.155 [0.017, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.820, 10.098], loss: 0.003576, mae: 0.066001, mean_q: 0.331743
 70500/100000: episode: 705, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.142, mean reward: 0.181 [0.002, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.707, 10.098], loss: 0.003809, mae: 0.068374, mean_q: 0.329948
 70600/100000: episode: 706, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.419, mean reward: 0.154 [0.011, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.359, 10.098], loss: 0.003691, mae: 0.068012, mean_q: 0.329691
 70700/100000: episode: 707, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.891, mean reward: 0.169 [0.015, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.509, 10.098], loss: 0.003576, mae: 0.066248, mean_q: 0.329365
 70800/100000: episode: 708, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 17.133, mean reward: 0.171 [0.020, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.824, 10.247], loss: 0.003750, mae: 0.068081, mean_q: 0.330292
 70900/100000: episode: 709, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 20.875, mean reward: 0.209 [0.047, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.375, 10.098], loss: 0.003753, mae: 0.068232, mean_q: 0.334727
 71000/100000: episode: 710, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.907, mean reward: 0.159 [0.020, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.046, 10.098], loss: 0.003540, mae: 0.066071, mean_q: 0.331715
 71100/100000: episode: 711, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.512, mean reward: 0.145 [0.010, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.924, 10.191], loss: 0.003724, mae: 0.067860, mean_q: 0.334651
 71200/100000: episode: 712, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.888, mean reward: 0.159 [0.032, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.769, 10.114], loss: 0.003731, mae: 0.067204, mean_q: 0.336767
 71300/100000: episode: 713, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: 14.914, mean reward: 0.149 [0.014, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.306, 10.213], loss: 0.003747, mae: 0.068294, mean_q: 0.339049
 71400/100000: episode: 714, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 19.891, mean reward: 0.199 [0.045, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.308, 10.098], loss: 0.003632, mae: 0.066287, mean_q: 0.336935
 71500/100000: episode: 715, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 18.317, mean reward: 0.183 [0.010, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.845, 10.098], loss: 0.003726, mae: 0.067119, mean_q: 0.332073
 71600/100000: episode: 716, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 16.435, mean reward: 0.164 [0.010, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.082, 10.120], loss: 0.003665, mae: 0.066882, mean_q: 0.338711
 71700/100000: episode: 717, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.220, mean reward: 0.152 [0.007, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.786, 10.274], loss: 0.003600, mae: 0.066455, mean_q: 0.337336
 71800/100000: episode: 718, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 16.209, mean reward: 0.162 [0.006, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.481, 10.123], loss: 0.003443, mae: 0.065019, mean_q: 0.338169
 71900/100000: episode: 719, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 19.696, mean reward: 0.197 [0.028, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.184], loss: 0.003630, mae: 0.066690, mean_q: 0.336819
 72000/100000: episode: 720, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.428, mean reward: 0.184 [0.016, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.315], loss: 0.003806, mae: 0.067937, mean_q: 0.337847
 72100/100000: episode: 721, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 17.253, mean reward: 0.173 [0.006, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.337, 10.210], loss: 0.003840, mae: 0.068795, mean_q: 0.335954
 72200/100000: episode: 722, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 14.976, mean reward: 0.150 [0.007, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.710, 10.212], loss: 0.003969, mae: 0.069913, mean_q: 0.338379
 72300/100000: episode: 723, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 14.068, mean reward: 0.141 [0.021, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.565, 10.098], loss: 0.003709, mae: 0.067505, mean_q: 0.337587
 72400/100000: episode: 724, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 16.822, mean reward: 0.168 [0.012, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.098], loss: 0.003847, mae: 0.068675, mean_q: 0.334789
 72500/100000: episode: 725, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 17.879, mean reward: 0.179 [0.030, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.967, 10.314], loss: 0.003603, mae: 0.066256, mean_q: 0.333310
 72600/100000: episode: 726, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.372, mean reward: 0.164 [0.021, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.345, 10.098], loss: 0.003997, mae: 0.069619, mean_q: 0.339188
 72700/100000: episode: 727, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.921, mean reward: 0.149 [0.012, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.459, 10.098], loss: 0.003586, mae: 0.066661, mean_q: 0.337957
 72800/100000: episode: 728, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 23.537, mean reward: 0.235 [0.017, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.012, 10.480], loss: 0.003951, mae: 0.069928, mean_q: 0.342202
 72900/100000: episode: 729, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.074, mean reward: 0.141 [0.010, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.994, 10.237], loss: 0.003763, mae: 0.068401, mean_q: 0.333351
 73000/100000: episode: 730, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.056, mean reward: 0.151 [0.006, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.530, 10.139], loss: 0.004016, mae: 0.070766, mean_q: 0.335611
 73100/100000: episode: 731, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 13.141, mean reward: 0.131 [0.011, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.827, 10.163], loss: 0.003558, mae: 0.066420, mean_q: 0.332877
 73200/100000: episode: 732, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.987, mean reward: 0.180 [0.027, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.372, 10.258], loss: 0.003882, mae: 0.069612, mean_q: 0.332925
 73300/100000: episode: 733, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.913, mean reward: 0.149 [0.015, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.733, 10.205], loss: 0.003910, mae: 0.069702, mean_q: 0.332948
 73400/100000: episode: 734, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 21.176, mean reward: 0.212 [0.034, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.149, 10.265], loss: 0.003821, mae: 0.068502, mean_q: 0.339113
 73500/100000: episode: 735, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 18.322, mean reward: 0.183 [0.014, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.422, 10.098], loss: 0.003761, mae: 0.068434, mean_q: 0.339632
 73600/100000: episode: 736, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 17.306, mean reward: 0.173 [0.016, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.246, 10.169], loss: 0.003985, mae: 0.069739, mean_q: 0.337508
 73700/100000: episode: 737, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 22.446, mean reward: 0.224 [0.026, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.089, 10.122], loss: 0.003862, mae: 0.068459, mean_q: 0.339671
 73800/100000: episode: 738, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 18.150, mean reward: 0.181 [0.023, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.032, 10.098], loss: 0.003810, mae: 0.069342, mean_q: 0.336619
 73900/100000: episode: 739, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 24.288, mean reward: 0.243 [0.027, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.715, 10.098], loss: 0.003826, mae: 0.069240, mean_q: 0.334042
 74000/100000: episode: 740, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 17.134, mean reward: 0.171 [0.020, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.553, 10.132], loss: 0.003834, mae: 0.068961, mean_q: 0.339306
 74100/100000: episode: 741, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 18.609, mean reward: 0.186 [0.013, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.366, 10.098], loss: 0.004053, mae: 0.070837, mean_q: 0.343740
 74200/100000: episode: 742, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.831, mean reward: 0.178 [0.024, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.014, 10.453], loss: 0.003741, mae: 0.068655, mean_q: 0.339243
 74300/100000: episode: 743, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 13.852, mean reward: 0.139 [0.009, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.081, 10.098], loss: 0.003763, mae: 0.068741, mean_q: 0.343892
 74400/100000: episode: 744, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 16.246, mean reward: 0.162 [0.020, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.677, 10.098], loss: 0.004011, mae: 0.070591, mean_q: 0.343330
 74500/100000: episode: 745, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 22.193, mean reward: 0.222 [0.024, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.927, 10.354], loss: 0.003700, mae: 0.068196, mean_q: 0.345099
 74600/100000: episode: 746, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.895, mean reward: 0.149 [0.018, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.195], loss: 0.003830, mae: 0.068598, mean_q: 0.345037
 74700/100000: episode: 747, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 11.955, mean reward: 0.120 [0.010, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.745, 10.136], loss: 0.004063, mae: 0.070989, mean_q: 0.340660
 74800/100000: episode: 748, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 13.380, mean reward: 0.134 [0.013, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.667, 10.098], loss: 0.003816, mae: 0.068595, mean_q: 0.334429
 74900/100000: episode: 749, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.420, mean reward: 0.154 [0.021, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.331], loss: 0.003817, mae: 0.068793, mean_q: 0.333652
 75000/100000: episode: 750, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.391, mean reward: 0.174 [0.005, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.091, 10.098], loss: 0.003769, mae: 0.068009, mean_q: 0.333209
 75100/100000: episode: 751, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 17.067, mean reward: 0.171 [0.017, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-2.092, 10.172], loss: 0.003678, mae: 0.067736, mean_q: 0.335356
 75200/100000: episode: 752, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.642, mean reward: 0.136 [0.005, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.206, 10.160], loss: 0.003794, mae: 0.068386, mean_q: 0.332141
 75300/100000: episode: 753, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.666, mean reward: 0.187 [0.030, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.456, 10.098], loss: 0.004117, mae: 0.071351, mean_q: 0.337402
 75400/100000: episode: 754, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 20.932, mean reward: 0.209 [0.060, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.099], loss: 0.004211, mae: 0.071743, mean_q: 0.336366
 75500/100000: episode: 755, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.341, mean reward: 0.163 [0.033, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.574, 10.309], loss: 0.004164, mae: 0.072022, mean_q: 0.337721
 75600/100000: episode: 756, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.894, mean reward: 0.159 [0.026, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.992, 10.216], loss: 0.004102, mae: 0.071219, mean_q: 0.333685
 75700/100000: episode: 757, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.199, mean reward: 0.152 [0.010, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.411, 10.210], loss: 0.004029, mae: 0.070597, mean_q: 0.336396
 75800/100000: episode: 758, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.629, mean reward: 0.176 [0.034, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.158, 10.098], loss: 0.003884, mae: 0.069712, mean_q: 0.336588
 75900/100000: episode: 759, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 21.316, mean reward: 0.213 [0.045, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.580, 10.213], loss: 0.003780, mae: 0.068062, mean_q: 0.328397
 76000/100000: episode: 760, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.527, mean reward: 0.165 [0.019, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.003873, mae: 0.069600, mean_q: 0.335796
 76100/100000: episode: 761, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.556, mean reward: 0.166 [0.005, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.237, 10.124], loss: 0.003855, mae: 0.069742, mean_q: 0.337116
 76200/100000: episode: 762, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 14.419, mean reward: 0.144 [0.010, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.869, 10.098], loss: 0.004119, mae: 0.071773, mean_q: 0.334745
 76300/100000: episode: 763, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.470, mean reward: 0.145 [0.025, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.963, 10.203], loss: 0.004030, mae: 0.071266, mean_q: 0.334107
 76400/100000: episode: 764, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.612, mean reward: 0.156 [0.005, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.186], loss: 0.003995, mae: 0.070604, mean_q: 0.331185
 76500/100000: episode: 765, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 20.180, mean reward: 0.202 [0.011, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.026, 10.098], loss: 0.004081, mae: 0.071318, mean_q: 0.332937
 76600/100000: episode: 766, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 20.901, mean reward: 0.209 [0.025, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.098], loss: 0.004287, mae: 0.072787, mean_q: 0.335976
 76700/100000: episode: 767, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 20.242, mean reward: 0.202 [0.009, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.198, 10.430], loss: 0.003885, mae: 0.070191, mean_q: 0.332949
 76800/100000: episode: 768, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 18.813, mean reward: 0.188 [0.026, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.967, 10.098], loss: 0.004178, mae: 0.072536, mean_q: 0.336785
 76900/100000: episode: 769, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 26.310, mean reward: 0.263 [0.022, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.456, 10.344], loss: 0.004033, mae: 0.070663, mean_q: 0.340088
 77000/100000: episode: 770, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.891, mean reward: 0.159 [0.018, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.709, 10.202], loss: 0.004024, mae: 0.070196, mean_q: 0.339668
 77100/100000: episode: 771, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 19.728, mean reward: 0.197 [0.017, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.207, 10.098], loss: 0.004043, mae: 0.070451, mean_q: 0.343016
 77200/100000: episode: 772, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.356, mean reward: 0.194 [0.010, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.600, 10.098], loss: 0.003711, mae: 0.068335, mean_q: 0.341364
 77300/100000: episode: 773, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.236, mean reward: 0.162 [0.017, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.906, 10.141], loss: 0.004092, mae: 0.071156, mean_q: 0.347784
 77400/100000: episode: 774, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.868, mean reward: 0.179 [0.010, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.562, 10.257], loss: 0.004002, mae: 0.071329, mean_q: 0.344546
 77500/100000: episode: 775, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.558, mean reward: 0.176 [0.033, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.727, 10.122], loss: 0.003813, mae: 0.068825, mean_q: 0.344990
 77600/100000: episode: 776, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.474, mean reward: 0.155 [0.024, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.118, 10.186], loss: 0.003836, mae: 0.068353, mean_q: 0.342461
 77700/100000: episode: 777, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.832, mean reward: 0.168 [0.012, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.456, 10.203], loss: 0.003879, mae: 0.069826, mean_q: 0.343068
 77800/100000: episode: 778, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 18.380, mean reward: 0.184 [0.012, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.099], loss: 0.003744, mae: 0.068487, mean_q: 0.342317
 77900/100000: episode: 779, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 19.611, mean reward: 0.196 [0.005, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.105, 10.284], loss: 0.003850, mae: 0.068676, mean_q: 0.340521
 78000/100000: episode: 780, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 13.748, mean reward: 0.137 [0.022, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.952, 10.098], loss: 0.004010, mae: 0.069680, mean_q: 0.342394
 78100/100000: episode: 781, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 18.970, mean reward: 0.190 [0.029, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.678, 10.233], loss: 0.003993, mae: 0.069001, mean_q: 0.346731
 78200/100000: episode: 782, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 18.009, mean reward: 0.180 [0.008, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.836, 10.098], loss: 0.004256, mae: 0.072606, mean_q: 0.349209
 78300/100000: episode: 783, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.743, mean reward: 0.167 [0.021, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.173, 10.268], loss: 0.004319, mae: 0.073047, mean_q: 0.350933
 78400/100000: episode: 784, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 17.466, mean reward: 0.175 [0.016, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.703, 10.107], loss: 0.004161, mae: 0.071542, mean_q: 0.347359
 78500/100000: episode: 785, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.541, mean reward: 0.165 [0.014, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.374, 10.230], loss: 0.004106, mae: 0.070731, mean_q: 0.345956
 78600/100000: episode: 786, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 15.471, mean reward: 0.155 [0.018, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.780, 10.231], loss: 0.004031, mae: 0.070100, mean_q: 0.343873
 78700/100000: episode: 787, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.640, mean reward: 0.156 [0.017, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.257, 10.098], loss: 0.004008, mae: 0.070559, mean_q: 0.341636
 78800/100000: episode: 788, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.682, mean reward: 0.157 [0.023, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.543, 10.098], loss: 0.003814, mae: 0.068536, mean_q: 0.342094
 78900/100000: episode: 789, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 13.225, mean reward: 0.132 [0.027, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.847, 10.098], loss: 0.003927, mae: 0.069578, mean_q: 0.337032
 79000/100000: episode: 790, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.956, mean reward: 0.180 [0.027, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.263, 10.098], loss: 0.003858, mae: 0.068849, mean_q: 0.337439
 79100/100000: episode: 791, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 20.260, mean reward: 0.203 [0.006, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-2.032, 10.098], loss: 0.004126, mae: 0.070564, mean_q: 0.336293
 79200/100000: episode: 792, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 17.616, mean reward: 0.176 [0.020, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.481, 10.245], loss: 0.003941, mae: 0.069558, mean_q: 0.333045
 79300/100000: episode: 793, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 20.060, mean reward: 0.201 [0.010, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.899, 10.312], loss: 0.003745, mae: 0.067604, mean_q: 0.339083
 79400/100000: episode: 794, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 18.882, mean reward: 0.189 [0.008, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.156], loss: 0.004073, mae: 0.070930, mean_q: 0.342146
 79500/100000: episode: 795, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.802, mean reward: 0.148 [0.007, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.282], loss: 0.003985, mae: 0.069831, mean_q: 0.334875
 79600/100000: episode: 796, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 13.273, mean reward: 0.133 [0.009, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.098], loss: 0.004039, mae: 0.069881, mean_q: 0.339329
 79700/100000: episode: 797, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.053, mean reward: 0.151 [0.015, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.447, 10.098], loss: 0.003900, mae: 0.069049, mean_q: 0.339270
 79800/100000: episode: 798, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 16.175, mean reward: 0.162 [0.021, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.144], loss: 0.004132, mae: 0.070653, mean_q: 0.339166
 79900/100000: episode: 799, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 13.654, mean reward: 0.137 [0.012, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.940, 10.098], loss: 0.004017, mae: 0.070445, mean_q: 0.337012
 80000/100000: episode: 800, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.978, mean reward: 0.160 [0.023, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.223], loss: 0.004220, mae: 0.071012, mean_q: 0.339621
 80100/100000: episode: 801, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 19.121, mean reward: 0.191 [0.057, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 0.004045, mae: 0.069756, mean_q: 0.336855
 80200/100000: episode: 802, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.977, mean reward: 0.160 [0.009, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.653, 10.098], loss: 0.004110, mae: 0.071204, mean_q: 0.341754
 80300/100000: episode: 803, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.822, mean reward: 0.168 [0.037, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.098], loss: 0.004107, mae: 0.071167, mean_q: 0.337906
 80400/100000: episode: 804, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 15.945, mean reward: 0.159 [0.014, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.351, 10.128], loss: 0.003874, mae: 0.069174, mean_q: 0.344282
 80500/100000: episode: 805, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 12.591, mean reward: 0.126 [0.018, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.278, 10.098], loss: 0.004037, mae: 0.069946, mean_q: 0.335844
 80600/100000: episode: 806, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.583, mean reward: 0.176 [0.018, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.417, 10.098], loss: 0.003984, mae: 0.068731, mean_q: 0.335738
 80700/100000: episode: 807, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 12.369, mean reward: 0.124 [0.015, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.278, 10.098], loss: 0.003993, mae: 0.070390, mean_q: 0.338835
 80800/100000: episode: 808, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.237, mean reward: 0.172 [0.007, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.530, 10.098], loss: 0.003995, mae: 0.070409, mean_q: 0.342298
 80900/100000: episode: 809, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 13.057, mean reward: 0.131 [0.006, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.289, 10.098], loss: 0.004072, mae: 0.070715, mean_q: 0.334705
 81000/100000: episode: 810, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 16.942, mean reward: 0.169 [0.021, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.023, 10.098], loss: 0.003985, mae: 0.069154, mean_q: 0.334397
 81100/100000: episode: 811, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 18.624, mean reward: 0.186 [0.003, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.658, 10.098], loss: 0.004109, mae: 0.071248, mean_q: 0.335405
 81200/100000: episode: 812, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 20.283, mean reward: 0.203 [0.018, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.393], loss: 0.004055, mae: 0.070627, mean_q: 0.332704
 81300/100000: episode: 813, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 16.268, mean reward: 0.163 [0.007, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.512, 10.229], loss: 0.004082, mae: 0.070407, mean_q: 0.339341
 81400/100000: episode: 814, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.123, mean reward: 0.161 [0.018, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.074, 10.270], loss: 0.003836, mae: 0.068667, mean_q: 0.332429
 81500/100000: episode: 815, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 13.907, mean reward: 0.139 [0.022, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.413], loss: 0.004239, mae: 0.072009, mean_q: 0.335705
 81600/100000: episode: 816, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 16.563, mean reward: 0.166 [0.011, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.303, 10.273], loss: 0.003789, mae: 0.068020, mean_q: 0.329046
 81700/100000: episode: 817, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.342, mean reward: 0.153 [0.006, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.108], loss: 0.004104, mae: 0.070498, mean_q: 0.326724
 81800/100000: episode: 818, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 14.716, mean reward: 0.147 [0.024, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.587, 10.184], loss: 0.004085, mae: 0.070311, mean_q: 0.331237
 81900/100000: episode: 819, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 12.872, mean reward: 0.129 [0.024, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.048, 10.145], loss: 0.003959, mae: 0.069591, mean_q: 0.324763
 82000/100000: episode: 820, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.187, mean reward: 0.162 [0.021, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.537, 10.098], loss: 0.004031, mae: 0.069192, mean_q: 0.322831
 82100/100000: episode: 821, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.378, mean reward: 0.154 [0.010, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.973, 10.139], loss: 0.003843, mae: 0.068846, mean_q: 0.316569
 82200/100000: episode: 822, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 14.390, mean reward: 0.144 [0.011, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.385, 10.098], loss: 0.003859, mae: 0.068054, mean_q: 0.317791
 82300/100000: episode: 823, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.391, mean reward: 0.174 [0.014, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.451, 10.204], loss: 0.003983, mae: 0.069887, mean_q: 0.317498
 82400/100000: episode: 824, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.418, mean reward: 0.174 [0.022, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.331], loss: 0.004244, mae: 0.072670, mean_q: 0.319687
 82500/100000: episode: 825, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.119, mean reward: 0.151 [0.014, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.304, 10.147], loss: 0.003955, mae: 0.070098, mean_q: 0.318038
 82600/100000: episode: 826, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 13.792, mean reward: 0.138 [0.017, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.156, 10.098], loss: 0.004312, mae: 0.071653, mean_q: 0.320108
 82700/100000: episode: 827, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 16.730, mean reward: 0.167 [0.024, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.252, 10.214], loss: 0.004316, mae: 0.072521, mean_q: 0.317077
 82800/100000: episode: 828, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.021, mean reward: 0.170 [0.021, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.911, 10.098], loss: 0.004322, mae: 0.072859, mean_q: 0.320648
 82900/100000: episode: 829, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.002, mean reward: 0.150 [0.023, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.312, 10.131], loss: 0.003835, mae: 0.068835, mean_q: 0.314142
 83000/100000: episode: 830, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.550, mean reward: 0.175 [0.017, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.669, 10.098], loss: 0.003864, mae: 0.068705, mean_q: 0.315755
 83100/100000: episode: 831, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 14.567, mean reward: 0.146 [0.017, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.981, 10.098], loss: 0.004038, mae: 0.069757, mean_q: 0.312564
 83200/100000: episode: 832, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 14.299, mean reward: 0.143 [0.021, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.239], loss: 0.003933, mae: 0.069264, mean_q: 0.314839
 83300/100000: episode: 833, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 20.399, mean reward: 0.204 [0.012, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.902, 10.098], loss: 0.003827, mae: 0.068627, mean_q: 0.312458
 83400/100000: episode: 834, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 22.493, mean reward: 0.225 [0.013, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.789, 10.098], loss: 0.003959, mae: 0.069800, mean_q: 0.318050
 83500/100000: episode: 835, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 16.684, mean reward: 0.167 [0.036, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.270, 10.098], loss: 0.003927, mae: 0.069334, mean_q: 0.316832
 83600/100000: episode: 836, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.861, mean reward: 0.179 [0.024, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.365, 10.098], loss: 0.004148, mae: 0.071116, mean_q: 0.319208
 83700/100000: episode: 837, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 15.337, mean reward: 0.153 [0.009, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.105, 10.118], loss: 0.003633, mae: 0.067170, mean_q: 0.318748
 83800/100000: episode: 838, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.540, mean reward: 0.165 [0.035, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.252, 10.098], loss: 0.003968, mae: 0.069960, mean_q: 0.319334
 83900/100000: episode: 839, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 19.741, mean reward: 0.197 [0.025, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.890, 10.401], loss: 0.003962, mae: 0.069983, mean_q: 0.322313
 84000/100000: episode: 840, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 18.368, mean reward: 0.184 [0.031, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.308, 10.132], loss: 0.003893, mae: 0.068820, mean_q: 0.325529
 84100/100000: episode: 841, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 14.133, mean reward: 0.141 [0.009, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.150], loss: 0.003768, mae: 0.067541, mean_q: 0.321175
 84200/100000: episode: 842, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 13.520, mean reward: 0.135 [0.013, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.936, 10.098], loss: 0.003945, mae: 0.069537, mean_q: 0.315653
 84300/100000: episode: 843, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 15.727, mean reward: 0.157 [0.013, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.719, 10.098], loss: 0.003892, mae: 0.068815, mean_q: 0.316031
 84400/100000: episode: 844, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 19.862, mean reward: 0.199 [0.027, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.751, 10.098], loss: 0.003886, mae: 0.069663, mean_q: 0.315265
 84500/100000: episode: 845, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.675, mean reward: 0.157 [0.018, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.116, 10.291], loss: 0.004061, mae: 0.071028, mean_q: 0.318785
 84600/100000: episode: 846, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 24.514, mean reward: 0.245 [0.041, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.270, 10.098], loss: 0.003880, mae: 0.069495, mean_q: 0.314485
 84700/100000: episode: 847, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 18.080, mean reward: 0.181 [0.040, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.653, 10.098], loss: 0.003889, mae: 0.069291, mean_q: 0.320979
 84800/100000: episode: 848, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.561, mean reward: 0.166 [0.003, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.347, 10.190], loss: 0.004183, mae: 0.072813, mean_q: 0.323347
 84900/100000: episode: 849, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 19.714, mean reward: 0.197 [0.025, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.090, 10.098], loss: 0.003920, mae: 0.069830, mean_q: 0.326976
 85000/100000: episode: 850, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.420, mean reward: 0.194 [0.042, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.077, 10.450], loss: 0.004031, mae: 0.070428, mean_q: 0.327711
 85100/100000: episode: 851, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.618, mean reward: 0.166 [0.009, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.352], loss: 0.003931, mae: 0.068714, mean_q: 0.327327
 85200/100000: episode: 852, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 14.478, mean reward: 0.145 [0.004, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.679, 10.148], loss: 0.003776, mae: 0.068321, mean_q: 0.327040
 85300/100000: episode: 853, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 14.922, mean reward: 0.149 [0.025, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.734, 10.098], loss: 0.004019, mae: 0.071107, mean_q: 0.322548
 85400/100000: episode: 854, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 13.109, mean reward: 0.131 [0.029, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.155], loss: 0.003846, mae: 0.069287, mean_q: 0.325485
 85500/100000: episode: 855, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.023, mean reward: 0.160 [0.012, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.594, 10.126], loss: 0.003821, mae: 0.068825, mean_q: 0.324836
 85600/100000: episode: 856, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.773, mean reward: 0.158 [0.013, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.158], loss: 0.003784, mae: 0.068750, mean_q: 0.324646
 85700/100000: episode: 857, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.952, mean reward: 0.160 [0.017, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.119, 10.098], loss: 0.003888, mae: 0.069728, mean_q: 0.329179
 85800/100000: episode: 858, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.939, mean reward: 0.169 [0.017, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.403], loss: 0.004154, mae: 0.071092, mean_q: 0.323842
 85900/100000: episode: 859, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 26.776, mean reward: 0.268 [0.023, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.155, 10.462], loss: 0.003645, mae: 0.067763, mean_q: 0.324643
 86000/100000: episode: 860, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 15.461, mean reward: 0.155 [0.016, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.466, 10.148], loss: 0.003594, mae: 0.066723, mean_q: 0.325797
 86100/100000: episode: 861, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 16.320, mean reward: 0.163 [0.006, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.856, 10.098], loss: 0.004056, mae: 0.070500, mean_q: 0.325745
 86200/100000: episode: 862, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 18.043, mean reward: 0.180 [0.028, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.389, 10.098], loss: 0.003816, mae: 0.068809, mean_q: 0.328543
 86300/100000: episode: 863, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 20.436, mean reward: 0.204 [0.030, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.845, 10.554], loss: 0.003791, mae: 0.068835, mean_q: 0.328013
 86400/100000: episode: 864, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.193, mean reward: 0.182 [0.017, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.851, 10.160], loss: 0.004083, mae: 0.071455, mean_q: 0.328806
 86500/100000: episode: 865, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 18.622, mean reward: 0.186 [0.014, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.126, 10.098], loss: 0.003915, mae: 0.069996, mean_q: 0.330774
 86600/100000: episode: 866, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.053, mean reward: 0.171 [0.031, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.083, 10.143], loss: 0.003752, mae: 0.068101, mean_q: 0.333649
 86700/100000: episode: 867, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.854, mean reward: 0.169 [0.009, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.145], loss: 0.003729, mae: 0.068332, mean_q: 0.336863
 86800/100000: episode: 868, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.311, mean reward: 0.173 [0.013, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.079, 10.411], loss: 0.003663, mae: 0.067705, mean_q: 0.329149
 86900/100000: episode: 869, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 15.594, mean reward: 0.156 [0.011, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.505, 10.147], loss: 0.003782, mae: 0.068803, mean_q: 0.335283
 87000/100000: episode: 870, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 14.500, mean reward: 0.145 [0.010, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.795, 10.098], loss: 0.003793, mae: 0.069173, mean_q: 0.335558
 87100/100000: episode: 871, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 14.141, mean reward: 0.141 [0.006, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.964, 10.205], loss: 0.003900, mae: 0.069420, mean_q: 0.335664
 87200/100000: episode: 872, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.721, mean reward: 0.157 [0.009, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.358, 10.145], loss: 0.003973, mae: 0.070052, mean_q: 0.336044
 87300/100000: episode: 873, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.129, mean reward: 0.181 [0.021, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.613, 10.128], loss: 0.003989, mae: 0.070450, mean_q: 0.338747
 87400/100000: episode: 874, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 18.155, mean reward: 0.182 [0.024, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.466, 10.098], loss: 0.003542, mae: 0.066677, mean_q: 0.335547
 87500/100000: episode: 875, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.497, mean reward: 0.165 [0.028, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.530, 10.255], loss: 0.003942, mae: 0.069913, mean_q: 0.331592
 87600/100000: episode: 876, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.393, mean reward: 0.144 [0.025, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.213, 10.169], loss: 0.003838, mae: 0.069103, mean_q: 0.333431
 87700/100000: episode: 877, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.683, mean reward: 0.157 [0.011, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.555, 10.326], loss: 0.003923, mae: 0.069962, mean_q: 0.336469
 87800/100000: episode: 878, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 16.922, mean reward: 0.169 [0.010, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.407, 10.098], loss: 0.003862, mae: 0.069412, mean_q: 0.338389
 87900/100000: episode: 879, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.678, mean reward: 0.167 [0.031, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.445, 10.098], loss: 0.003705, mae: 0.067861, mean_q: 0.337645
 88000/100000: episode: 880, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.418, mean reward: 0.154 [0.007, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.900, 10.098], loss: 0.003966, mae: 0.070150, mean_q: 0.337979
 88100/100000: episode: 881, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 14.500, mean reward: 0.145 [0.017, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.120, 10.180], loss: 0.003715, mae: 0.067522, mean_q: 0.335889
 88200/100000: episode: 882, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 19.946, mean reward: 0.199 [0.035, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.822, 10.460], loss: 0.003655, mae: 0.066726, mean_q: 0.336946
 88300/100000: episode: 883, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 16.015, mean reward: 0.160 [0.003, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.854, 10.098], loss: 0.003732, mae: 0.067311, mean_q: 0.332733
 88400/100000: episode: 884, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 17.571, mean reward: 0.176 [0.017, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.414], loss: 0.003682, mae: 0.067466, mean_q: 0.332363
 88500/100000: episode: 885, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.966, mean reward: 0.170 [0.015, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.810, 10.371], loss: 0.003669, mae: 0.066779, mean_q: 0.328588
 88600/100000: episode: 886, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 16.269, mean reward: 0.163 [0.003, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.477, 10.098], loss: 0.003952, mae: 0.069327, mean_q: 0.332658
 88700/100000: episode: 887, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 12.567, mean reward: 0.126 [0.013, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.098], loss: 0.003764, mae: 0.067841, mean_q: 0.334890
 88800/100000: episode: 888, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 17.237, mean reward: 0.172 [0.017, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.674, 10.098], loss: 0.004071, mae: 0.071380, mean_q: 0.333553
 88900/100000: episode: 889, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 11.542, mean reward: 0.115 [0.022, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.158], loss: 0.003839, mae: 0.068693, mean_q: 0.327173
 89000/100000: episode: 890, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.392, mean reward: 0.174 [0.022, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.217, 10.098], loss: 0.003812, mae: 0.068656, mean_q: 0.325339
 89100/100000: episode: 891, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 19.139, mean reward: 0.191 [0.016, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.688, 10.220], loss: 0.003796, mae: 0.069181, mean_q: 0.328461
 89200/100000: episode: 892, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.905, mean reward: 0.189 [0.022, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.341, 10.359], loss: 0.003928, mae: 0.070084, mean_q: 0.332647
 89300/100000: episode: 893, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 19.328, mean reward: 0.193 [0.002, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.402, 10.317], loss: 0.003716, mae: 0.067573, mean_q: 0.331107
 89400/100000: episode: 894, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.438, mean reward: 0.154 [0.034, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.639, 10.168], loss: 0.003699, mae: 0.067459, mean_q: 0.330649
 89500/100000: episode: 895, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 13.844, mean reward: 0.138 [0.014, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.098], loss: 0.003862, mae: 0.068666, mean_q: 0.334195
 89600/100000: episode: 896, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.779, mean reward: 0.158 [0.010, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.105, 10.098], loss: 0.003914, mae: 0.069419, mean_q: 0.328375
 89700/100000: episode: 897, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.087, mean reward: 0.161 [0.021, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.784, 10.162], loss: 0.003758, mae: 0.068713, mean_q: 0.329217
 89800/100000: episode: 898, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 15.280, mean reward: 0.153 [0.001, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.400, 10.404], loss: 0.003765, mae: 0.068116, mean_q: 0.330172
 89900/100000: episode: 899, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.705, mean reward: 0.157 [0.005, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.132, 10.098], loss: 0.003907, mae: 0.068510, mean_q: 0.331586
 90000/100000: episode: 900, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.388, mean reward: 0.204 [0.029, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.257], loss: 0.003940, mae: 0.069148, mean_q: 0.328181
 90100/100000: episode: 901, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.791, mean reward: 0.158 [0.044, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.411, 10.211], loss: 0.003739, mae: 0.067264, mean_q: 0.324642
 90200/100000: episode: 902, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 18.828, mean reward: 0.188 [0.028, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.223, 10.100], loss: 0.003761, mae: 0.067159, mean_q: 0.326464
 90300/100000: episode: 903, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.380, mean reward: 0.154 [0.009, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.313], loss: 0.003892, mae: 0.069198, mean_q: 0.328779
 90400/100000: episode: 904, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.150, mean reward: 0.152 [0.011, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.127, 10.288], loss: 0.003817, mae: 0.068134, mean_q: 0.324577
 90500/100000: episode: 905, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 19.860, mean reward: 0.199 [0.012, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.126], loss: 0.003697, mae: 0.067700, mean_q: 0.331379
 90600/100000: episode: 906, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 14.449, mean reward: 0.144 [0.007, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.266, 10.098], loss: 0.003612, mae: 0.066991, mean_q: 0.330731
 90700/100000: episode: 907, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.494, mean reward: 0.155 [0.009, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.520, 10.098], loss: 0.003698, mae: 0.067761, mean_q: 0.332230
 90800/100000: episode: 908, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.668, mean reward: 0.157 [0.009, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.389, 10.098], loss: 0.003786, mae: 0.068496, mean_q: 0.323216
 90900/100000: episode: 909, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.081, mean reward: 0.171 [0.020, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.329, 10.098], loss: 0.003783, mae: 0.067708, mean_q: 0.326269
 91000/100000: episode: 910, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 14.354, mean reward: 0.144 [0.010, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.575, 10.098], loss: 0.003646, mae: 0.067096, mean_q: 0.320832
 91100/100000: episode: 911, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 20.075, mean reward: 0.201 [0.007, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.566, 10.222], loss: 0.003991, mae: 0.069857, mean_q: 0.323456
 91200/100000: episode: 912, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 13.248, mean reward: 0.132 [0.032, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.575, 10.098], loss: 0.003812, mae: 0.068735, mean_q: 0.328746
 91300/100000: episode: 913, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.485, mean reward: 0.145 [0.031, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.154, 10.137], loss: 0.003801, mae: 0.068054, mean_q: 0.322845
 91400/100000: episode: 914, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 16.952, mean reward: 0.170 [0.005, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.113, 10.098], loss: 0.003742, mae: 0.067659, mean_q: 0.322540
 91500/100000: episode: 915, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.222, mean reward: 0.162 [0.011, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.067, 10.098], loss: 0.003973, mae: 0.070468, mean_q: 0.318124
 91600/100000: episode: 916, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 12.121, mean reward: 0.121 [0.011, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.260, 10.098], loss: 0.003717, mae: 0.068033, mean_q: 0.314265
 91700/100000: episode: 917, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.610, mean reward: 0.176 [0.012, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.130], loss: 0.003751, mae: 0.067909, mean_q: 0.314036
 91800/100000: episode: 918, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 12.803, mean reward: 0.128 [0.011, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.044, 10.098], loss: 0.003802, mae: 0.068329, mean_q: 0.317971
 91900/100000: episode: 919, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.017, mean reward: 0.180 [0.005, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.497, 10.345], loss: 0.004104, mae: 0.070410, mean_q: 0.317335
 92000/100000: episode: 920, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 16.102, mean reward: 0.161 [0.027, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.543, 10.098], loss: 0.003846, mae: 0.068467, mean_q: 0.311629
 92100/100000: episode: 921, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 18.687, mean reward: 0.187 [0.047, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.366, 10.287], loss: 0.003613, mae: 0.066857, mean_q: 0.317663
 92200/100000: episode: 922, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 13.984, mean reward: 0.140 [0.005, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.783, 10.098], loss: 0.003877, mae: 0.068984, mean_q: 0.315344
 92300/100000: episode: 923, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.425, mean reward: 0.144 [0.015, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.276, 10.098], loss: 0.003799, mae: 0.068020, mean_q: 0.313878
 92400/100000: episode: 924, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.799, mean reward: 0.148 [0.020, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.199], loss: 0.003866, mae: 0.069002, mean_q: 0.318522
 92500/100000: episode: 925, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 14.744, mean reward: 0.147 [0.014, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.098], loss: 0.003788, mae: 0.068996, mean_q: 0.316356
 92600/100000: episode: 926, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 17.200, mean reward: 0.172 [0.021, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.789, 10.098], loss: 0.003642, mae: 0.066630, mean_q: 0.312772
 92700/100000: episode: 927, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 22.256, mean reward: 0.223 [0.015, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.476, 10.098], loss: 0.003780, mae: 0.067920, mean_q: 0.315711
 92800/100000: episode: 928, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.854, mean reward: 0.209 [0.019, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.848, 10.098], loss: 0.003620, mae: 0.066905, mean_q: 0.319987
 92900/100000: episode: 929, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 14.673, mean reward: 0.147 [0.013, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.098], loss: 0.003763, mae: 0.067435, mean_q: 0.321675
 93000/100000: episode: 930, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.537, mean reward: 0.145 [0.009, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.042, 10.154], loss: 0.003784, mae: 0.068189, mean_q: 0.319051
 93100/100000: episode: 931, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 13.976, mean reward: 0.140 [0.009, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.734, 10.098], loss: 0.004027, mae: 0.070455, mean_q: 0.316687
 93200/100000: episode: 932, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 15.638, mean reward: 0.156 [0.009, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.991, 10.150], loss: 0.003656, mae: 0.066548, mean_q: 0.315091
 93300/100000: episode: 933, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.444, mean reward: 0.164 [0.034, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.310, 10.098], loss: 0.003967, mae: 0.070381, mean_q: 0.317290
 93400/100000: episode: 934, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.249, mean reward: 0.152 [0.021, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.848, 10.098], loss: 0.003963, mae: 0.069005, mean_q: 0.316573
 93500/100000: episode: 935, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 17.231, mean reward: 0.172 [0.008, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.195, 10.098], loss: 0.003661, mae: 0.067304, mean_q: 0.318309
 93600/100000: episode: 936, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 17.162, mean reward: 0.172 [0.030, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.339, 10.098], loss: 0.003676, mae: 0.067545, mean_q: 0.316665
 93700/100000: episode: 937, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 13.937, mean reward: 0.139 [0.017, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.230, 10.098], loss: 0.003865, mae: 0.068212, mean_q: 0.316328
 93800/100000: episode: 938, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 13.599, mean reward: 0.136 [0.013, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.329, 10.098], loss: 0.003884, mae: 0.069164, mean_q: 0.322007
 93900/100000: episode: 939, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 13.942, mean reward: 0.139 [0.012, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.098], loss: 0.003747, mae: 0.067540, mean_q: 0.321318
 94000/100000: episode: 940, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 16.663, mean reward: 0.167 [0.021, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.578, 10.098], loss: 0.003911, mae: 0.069309, mean_q: 0.314727
 94100/100000: episode: 941, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.392, mean reward: 0.164 [0.008, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.374, 10.098], loss: 0.003753, mae: 0.067135, mean_q: 0.314844
 94200/100000: episode: 942, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.122, mean reward: 0.141 [0.007, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.291], loss: 0.003850, mae: 0.068918, mean_q: 0.317895
 94300/100000: episode: 943, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 13.012, mean reward: 0.130 [0.015, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.098], loss: 0.003977, mae: 0.069847, mean_q: 0.315005
 94400/100000: episode: 944, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 19.544, mean reward: 0.195 [0.021, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.377], loss: 0.003861, mae: 0.069143, mean_q: 0.313194
 94500/100000: episode: 945, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 17.465, mean reward: 0.175 [0.006, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.236], loss: 0.003798, mae: 0.068087, mean_q: 0.316989
 94600/100000: episode: 946, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 21.944, mean reward: 0.219 [0.006, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.692, 10.504], loss: 0.003909, mae: 0.068791, mean_q: 0.317832
 94700/100000: episode: 947, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 20.418, mean reward: 0.204 [0.021, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.429, 10.122], loss: 0.004054, mae: 0.070801, mean_q: 0.321597
 94800/100000: episode: 948, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.304, mean reward: 0.153 [0.026, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.256, 10.161], loss: 0.003819, mae: 0.068875, mean_q: 0.320577
 94900/100000: episode: 949, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.063, mean reward: 0.161 [0.017, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.269, 10.133], loss: 0.003979, mae: 0.069279, mean_q: 0.320178
 95000/100000: episode: 950, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 13.843, mean reward: 0.138 [0.008, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.104], loss: 0.003825, mae: 0.069209, mean_q: 0.320233
 95100/100000: episode: 951, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.590, mean reward: 0.156 [0.002, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.155, 10.098], loss: 0.003991, mae: 0.069812, mean_q: 0.312400
 95200/100000: episode: 952, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.378, mean reward: 0.154 [0.022, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.284, 10.098], loss: 0.003785, mae: 0.068320, mean_q: 0.318189
 95300/100000: episode: 953, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.598, mean reward: 0.176 [0.029, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.180, 10.099], loss: 0.003870, mae: 0.068419, mean_q: 0.314479
 95400/100000: episode: 954, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 20.055, mean reward: 0.201 [0.036, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.809, 10.098], loss: 0.003854, mae: 0.069457, mean_q: 0.321638
 95500/100000: episode: 955, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.647, mean reward: 0.176 [0.012, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.628, 10.098], loss: 0.003933, mae: 0.070051, mean_q: 0.315025
 95600/100000: episode: 956, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.977, mean reward: 0.160 [0.028, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.098], loss: 0.003715, mae: 0.067703, mean_q: 0.314983
 95700/100000: episode: 957, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 22.175, mean reward: 0.222 [0.011, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.829, 10.098], loss: 0.003797, mae: 0.068390, mean_q: 0.316369
 95800/100000: episode: 958, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 16.220, mean reward: 0.162 [0.027, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.793, 10.286], loss: 0.003737, mae: 0.067967, mean_q: 0.319280
 95900/100000: episode: 959, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.581, mean reward: 0.146 [0.015, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.659, 10.098], loss: 0.003767, mae: 0.067855, mean_q: 0.317882
 96000/100000: episode: 960, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 26.111, mean reward: 0.261 [0.023, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.420, 10.499], loss: 0.003632, mae: 0.066479, mean_q: 0.322330
 96100/100000: episode: 961, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.402, mean reward: 0.174 [0.006, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.211, 10.098], loss: 0.003616, mae: 0.066827, mean_q: 0.323390
 96200/100000: episode: 962, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 21.274, mean reward: 0.213 [0.017, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.582], loss: 0.003702, mae: 0.066943, mean_q: 0.325576
 96300/100000: episode: 963, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.571, mean reward: 0.156 [0.013, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.097, 10.098], loss: 0.003839, mae: 0.069036, mean_q: 0.328714
 96400/100000: episode: 964, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.090, mean reward: 0.171 [0.020, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.884, 10.098], loss: 0.003726, mae: 0.068101, mean_q: 0.326000
 96500/100000: episode: 965, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.759, mean reward: 0.158 [0.006, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.893, 10.260], loss: 0.003555, mae: 0.066195, mean_q: 0.327003
 96600/100000: episode: 966, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.693, mean reward: 0.157 [0.019, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.203], loss: 0.003793, mae: 0.068195, mean_q: 0.327079
 96700/100000: episode: 967, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 18.517, mean reward: 0.185 [0.029, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.881, 10.188], loss: 0.003922, mae: 0.069124, mean_q: 0.330809
 96800/100000: episode: 968, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.062, mean reward: 0.141 [0.016, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.608, 10.239], loss: 0.003860, mae: 0.068640, mean_q: 0.328861
 96900/100000: episode: 969, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.123, mean reward: 0.141 [0.011, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.303, 10.098], loss: 0.003792, mae: 0.068494, mean_q: 0.329902
 97000/100000: episode: 970, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 13.672, mean reward: 0.137 [0.022, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.121, 10.098], loss: 0.003529, mae: 0.065461, mean_q: 0.325863
 97100/100000: episode: 971, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.062, mean reward: 0.151 [0.026, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.206], loss: 0.003759, mae: 0.068326, mean_q: 0.329859
 97200/100000: episode: 972, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.699, mean reward: 0.167 [0.037, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.657, 10.098], loss: 0.003526, mae: 0.066026, mean_q: 0.327023
 97300/100000: episode: 973, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 20.011, mean reward: 0.200 [0.009, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.206, 10.106], loss: 0.003467, mae: 0.065796, mean_q: 0.326126
 97400/100000: episode: 974, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 23.168, mean reward: 0.232 [0.022, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.193, 10.314], loss: 0.003986, mae: 0.070261, mean_q: 0.334739
 97500/100000: episode: 975, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.348, mean reward: 0.153 [0.021, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.319, 10.379], loss: 0.003694, mae: 0.067877, mean_q: 0.332045
 97600/100000: episode: 976, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 20.690, mean reward: 0.207 [0.034, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.880, 10.098], loss: 0.003754, mae: 0.068722, mean_q: 0.334269
 97700/100000: episode: 977, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 14.460, mean reward: 0.145 [0.015, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.147], loss: 0.003700, mae: 0.066710, mean_q: 0.333182
 97800/100000: episode: 978, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 14.714, mean reward: 0.147 [0.023, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.118, 10.098], loss: 0.003545, mae: 0.066620, mean_q: 0.329915
 97900/100000: episode: 979, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.008, mean reward: 0.150 [0.004, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.254, 10.200], loss: 0.003577, mae: 0.066414, mean_q: 0.326829
 98000/100000: episode: 980, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 15.171, mean reward: 0.152 [0.009, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.159, 10.098], loss: 0.003764, mae: 0.068681, mean_q: 0.332391
 98100/100000: episode: 981, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.044, mean reward: 0.150 [0.017, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.643, 10.098], loss: 0.004088, mae: 0.071080, mean_q: 0.333847
 98200/100000: episode: 982, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 17.113, mean reward: 0.171 [0.024, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.510, 10.293], loss: 0.003766, mae: 0.068495, mean_q: 0.327691
 98300/100000: episode: 983, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.122, mean reward: 0.201 [0.047, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.635, 10.250], loss: 0.003982, mae: 0.071164, mean_q: 0.331582
 98400/100000: episode: 984, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.560, mean reward: 0.156 [0.012, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.098], loss: 0.003653, mae: 0.067520, mean_q: 0.328648
 98500/100000: episode: 985, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.229, mean reward: 0.172 [0.015, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.493, 10.205], loss: 0.003569, mae: 0.066717, mean_q: 0.326927
 98600/100000: episode: 986, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 14.703, mean reward: 0.147 [0.008, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.574, 10.151], loss: 0.003861, mae: 0.068867, mean_q: 0.333930
 98700/100000: episode: 987, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 19.295, mean reward: 0.193 [0.022, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.527, 10.098], loss: 0.003723, mae: 0.067285, mean_q: 0.332174
 98800/100000: episode: 988, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 27.312, mean reward: 0.273 [0.035, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.841, 10.287], loss: 0.003695, mae: 0.067973, mean_q: 0.334221
 98900/100000: episode: 989, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 16.777, mean reward: 0.168 [0.025, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.098], loss: 0.003762, mae: 0.068048, mean_q: 0.338357
 99000/100000: episode: 990, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.170, mean reward: 0.172 [0.007, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.098], loss: 0.003578, mae: 0.066798, mean_q: 0.339339
 99100/100000: episode: 991, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 15.711, mean reward: 0.157 [0.005, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.283], loss: 0.003677, mae: 0.066880, mean_q: 0.336386
 99200/100000: episode: 992, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 14.673, mean reward: 0.147 [0.013, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.063, 10.131], loss: 0.003603, mae: 0.065939, mean_q: 0.341171
 99300/100000: episode: 993, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.289, mean reward: 0.153 [0.022, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.071, 10.098], loss: 0.003785, mae: 0.068273, mean_q: 0.340483
 99400/100000: episode: 994, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.333, mean reward: 0.163 [0.018, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.902, 10.098], loss: 0.003534, mae: 0.066002, mean_q: 0.333315
 99500/100000: episode: 995, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 16.272, mean reward: 0.163 [0.012, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.193, 10.098], loss: 0.003373, mae: 0.064200, mean_q: 0.335965
 99600/100000: episode: 996, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 14.026, mean reward: 0.140 [0.008, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.787, 10.098], loss: 0.003374, mae: 0.064822, mean_q: 0.335043
 99700/100000: episode: 997, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.014, mean reward: 0.150 [0.020, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.647, 10.279], loss: 0.003789, mae: 0.068258, mean_q: 0.333575
 99800/100000: episode: 998, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.033, mean reward: 0.170 [0.006, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 0.003587, mae: 0.066871, mean_q: 0.335262
 99900/100000: episode: 999, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 19.890, mean reward: 0.199 [0.029, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.866, 10.098], loss: 0.003708, mae: 0.066925, mean_q: 0.331375
 100000/100000: episode: 1000, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.450, mean reward: 0.194 [0.025, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.367, 10.098], loss: 0.003623, mae: 0.066476, mean_q: 0.335084
done, took 535.003 seconds
[Info] End Uniform Random Simulation.
