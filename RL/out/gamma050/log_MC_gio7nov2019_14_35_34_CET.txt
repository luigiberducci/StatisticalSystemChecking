Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.178s, episode steps: 100, steps per second: 561, episode reward: 17.026, mean reward: 0.170 [0.030, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.645, 10.323], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.072s, episode steps: 100, steps per second: 1395, episode reward: 14.491, mean reward: 0.145 [0.013, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.135], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1533, episode reward: 15.421, mean reward: 0.154 [0.020, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.730, 10.336], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 21.101, mean reward: 0.211 [0.024, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.741, 10.384], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 19.810, mean reward: 0.198 [0.030, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.000, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 16.104, mean reward: 0.161 [0.041, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.698, 10.259], loss: 0.008208, mae: 0.095281, mean_q: 0.448018
   700/100000: episode: 7, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.070, mean reward: 0.161 [0.003, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.096, 10.098], loss: 0.004684, mae: 0.076834, mean_q: 0.399153
   800/100000: episode: 8, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 21.935, mean reward: 0.219 [0.030, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.487, 10.388], loss: 0.004644, mae: 0.075653, mean_q: 0.374479
   900/100000: episode: 9, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 20.118, mean reward: 0.201 [0.036, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.199, 10.098], loss: 0.004711, mae: 0.076819, mean_q: 0.370212
  1000/100000: episode: 10, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.553, mean reward: 0.156 [0.010, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.086, 10.189], loss: 0.004568, mae: 0.075715, mean_q: 0.362116
  1100/100000: episode: 11, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.104, mean reward: 0.161 [0.040, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.644, 10.098], loss: 0.004467, mae: 0.075425, mean_q: 0.355180
  1200/100000: episode: 12, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.209, mean reward: 0.162 [0.028, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.862, 10.383], loss: 0.004257, mae: 0.072723, mean_q: 0.349799
  1300/100000: episode: 13, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.386, mean reward: 0.174 [0.013, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.915, 10.098], loss: 0.004069, mae: 0.071532, mean_q: 0.345168
  1400/100000: episode: 14, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 18.798, mean reward: 0.188 [0.049, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.312, 10.367], loss: 0.004521, mae: 0.075829, mean_q: 0.349086
  1500/100000: episode: 15, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.260, mean reward: 0.153 [0.013, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.174], loss: 0.004383, mae: 0.074546, mean_q: 0.342340
  1600/100000: episode: 16, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.368, mean reward: 0.154 [0.012, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.174, 10.215], loss: 0.004038, mae: 0.071284, mean_q: 0.341756
  1700/100000: episode: 17, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 12.424, mean reward: 0.124 [0.011, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.449, 10.162], loss: 0.004080, mae: 0.071535, mean_q: 0.340564
  1800/100000: episode: 18, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 14.629, mean reward: 0.146 [0.020, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.168], loss: 0.004150, mae: 0.071840, mean_q: 0.335012
  1900/100000: episode: 19, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 14.574, mean reward: 0.146 [0.033, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.589, 10.098], loss: 0.004093, mae: 0.071691, mean_q: 0.332253
  2000/100000: episode: 20, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 20.023, mean reward: 0.200 [0.009, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.352, 10.357], loss: 0.003927, mae: 0.070133, mean_q: 0.329505
  2100/100000: episode: 21, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 14.957, mean reward: 0.150 [0.018, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.274, 10.156], loss: 0.004299, mae: 0.073435, mean_q: 0.333613
  2200/100000: episode: 22, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.832, mean reward: 0.158 [0.018, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.202, 10.098], loss: 0.004180, mae: 0.072527, mean_q: 0.333476
  2300/100000: episode: 23, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.509, mean reward: 0.185 [0.021, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.590, 10.098], loss: 0.004152, mae: 0.071657, mean_q: 0.332000
  2400/100000: episode: 24, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.876, mean reward: 0.169 [0.006, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.704, 10.138], loss: 0.004084, mae: 0.071931, mean_q: 0.335750
  2500/100000: episode: 25, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.022, mean reward: 0.140 [0.008, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.160], loss: 0.003945, mae: 0.070520, mean_q: 0.333894
  2600/100000: episode: 26, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 14.217, mean reward: 0.142 [0.016, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.098], loss: 0.003857, mae: 0.069649, mean_q: 0.329658
  2700/100000: episode: 27, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.974, mean reward: 0.160 [0.015, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.679, 10.116], loss: 0.004011, mae: 0.071333, mean_q: 0.332547
  2800/100000: episode: 28, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 14.431, mean reward: 0.144 [0.006, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.098], loss: 0.004025, mae: 0.070951, mean_q: 0.329187
  2900/100000: episode: 29, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.326, mean reward: 0.193 [0.018, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.717, 10.098], loss: 0.004159, mae: 0.071860, mean_q: 0.325401
  3000/100000: episode: 30, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 15.296, mean reward: 0.153 [0.022, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.368, 10.172], loss: 0.004064, mae: 0.071708, mean_q: 0.326737
  3100/100000: episode: 31, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 19.410, mean reward: 0.194 [0.011, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.402, 10.098], loss: 0.003928, mae: 0.070737, mean_q: 0.326221
  3200/100000: episode: 32, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.563, mean reward: 0.166 [0.024, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.589, 10.098], loss: 0.003985, mae: 0.070947, mean_q: 0.325963
  3300/100000: episode: 33, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 17.809, mean reward: 0.178 [0.032, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.793, 10.289], loss: 0.004014, mae: 0.071219, mean_q: 0.327454
  3400/100000: episode: 34, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.252, mean reward: 0.143 [0.009, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.828, 10.209], loss: 0.004035, mae: 0.070999, mean_q: 0.326074
  3500/100000: episode: 35, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.032, mean reward: 0.160 [0.020, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.391, 10.098], loss: 0.003938, mae: 0.070277, mean_q: 0.329649
  3600/100000: episode: 36, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.406, mean reward: 0.154 [0.018, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.478, 10.098], loss: 0.003674, mae: 0.067799, mean_q: 0.322277
  3700/100000: episode: 37, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 17.691, mean reward: 0.177 [0.025, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.174, 10.098], loss: 0.003789, mae: 0.068082, mean_q: 0.325111
  3800/100000: episode: 38, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 14.499, mean reward: 0.145 [0.017, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.679, 10.210], loss: 0.004033, mae: 0.071231, mean_q: 0.326438
  3900/100000: episode: 39, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 18.005, mean reward: 0.180 [0.046, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.054, 10.128], loss: 0.003972, mae: 0.070101, mean_q: 0.330337
  4000/100000: episode: 40, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.375, mean reward: 0.174 [0.016, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.779, 10.098], loss: 0.003852, mae: 0.069090, mean_q: 0.324297
  4100/100000: episode: 41, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.594, mean reward: 0.186 [0.023, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.486, 10.233], loss: 0.003824, mae: 0.069977, mean_q: 0.326979
  4200/100000: episode: 42, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 14.899, mean reward: 0.149 [0.010, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.304, 10.098], loss: 0.004040, mae: 0.070645, mean_q: 0.328422
  4300/100000: episode: 43, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.165, mean reward: 0.172 [0.013, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.178], loss: 0.004119, mae: 0.071249, mean_q: 0.327121
  4400/100000: episode: 44, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 20.403, mean reward: 0.204 [0.038, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.718, 10.098], loss: 0.003819, mae: 0.068675, mean_q: 0.329458
  4500/100000: episode: 45, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.880, mean reward: 0.189 [0.010, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.409, 10.098], loss: 0.004037, mae: 0.070940, mean_q: 0.331109
  4600/100000: episode: 46, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.078, mean reward: 0.141 [0.005, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.099], loss: 0.003682, mae: 0.067508, mean_q: 0.327012
  4700/100000: episode: 47, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 14.111, mean reward: 0.141 [0.012, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.499, 10.098], loss: 0.003839, mae: 0.069448, mean_q: 0.328291
  4800/100000: episode: 48, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 22.078, mean reward: 0.221 [0.013, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.864, 10.159], loss: 0.003804, mae: 0.068713, mean_q: 0.329471
  4900/100000: episode: 49, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 15.884, mean reward: 0.159 [0.011, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.098], loss: 0.004043, mae: 0.070974, mean_q: 0.327511
  5000/100000: episode: 50, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 15.440, mean reward: 0.154 [0.008, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.591, 10.098], loss: 0.003764, mae: 0.068938, mean_q: 0.330098
  5100/100000: episode: 51, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 20.330, mean reward: 0.203 [0.053, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.946, 10.254], loss: 0.003844, mae: 0.069020, mean_q: 0.326733
  5200/100000: episode: 52, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 16.010, mean reward: 0.160 [0.018, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.608, 10.098], loss: 0.003901, mae: 0.069404, mean_q: 0.325137
  5300/100000: episode: 53, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 22.599, mean reward: 0.226 [0.022, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.847, 10.098], loss: 0.003719, mae: 0.068866, mean_q: 0.331760
  5400/100000: episode: 54, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 21.815, mean reward: 0.218 [0.030, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.098], loss: 0.003824, mae: 0.069083, mean_q: 0.333336
  5500/100000: episode: 55, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 23.852, mean reward: 0.239 [0.053, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.098], loss: 0.004038, mae: 0.071635, mean_q: 0.337741
  5600/100000: episode: 56, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 21.013, mean reward: 0.210 [0.026, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.404, 10.098], loss: 0.004104, mae: 0.072159, mean_q: 0.332927
  5700/100000: episode: 57, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.466, mean reward: 0.155 [0.013, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.537, 10.199], loss: 0.004006, mae: 0.071730, mean_q: 0.335198
  5800/100000: episode: 58, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 16.583, mean reward: 0.166 [0.020, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.440, 10.143], loss: 0.003881, mae: 0.069936, mean_q: 0.335440
  5900/100000: episode: 59, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.889, mean reward: 0.169 [0.013, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.331, 10.098], loss: 0.004106, mae: 0.071886, mean_q: 0.335663
  6000/100000: episode: 60, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 15.955, mean reward: 0.160 [0.006, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.098], loss: 0.004026, mae: 0.071321, mean_q: 0.334048
  6100/100000: episode: 61, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.843, mean reward: 0.158 [0.025, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.965, 10.098], loss: 0.003974, mae: 0.071208, mean_q: 0.332679
  6200/100000: episode: 62, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.114, mean reward: 0.181 [0.007, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.904, 10.318], loss: 0.004101, mae: 0.072515, mean_q: 0.338317
  6300/100000: episode: 63, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.329, mean reward: 0.143 [0.005, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.813, 10.339], loss: 0.003885, mae: 0.070376, mean_q: 0.334064
  6400/100000: episode: 64, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 21.293, mean reward: 0.213 [0.018, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.232], loss: 0.004081, mae: 0.071985, mean_q: 0.332630
  6500/100000: episode: 65, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 12.476, mean reward: 0.125 [0.014, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.098], loss: 0.004017, mae: 0.071557, mean_q: 0.335673
  6600/100000: episode: 66, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.416, mean reward: 0.184 [0.020, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.257, 10.098], loss: 0.004101, mae: 0.071752, mean_q: 0.333032
  6700/100000: episode: 67, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 17.110, mean reward: 0.171 [0.004, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.699, 10.178], loss: 0.003943, mae: 0.071533, mean_q: 0.338642
  6800/100000: episode: 68, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 18.824, mean reward: 0.188 [0.039, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.280, 10.264], loss: 0.004136, mae: 0.072126, mean_q: 0.334681
  6900/100000: episode: 69, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.732, mean reward: 0.167 [0.022, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.415, 10.245], loss: 0.004168, mae: 0.072603, mean_q: 0.335978
  7000/100000: episode: 70, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.404, mean reward: 0.184 [0.029, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.686, 10.098], loss: 0.004077, mae: 0.072144, mean_q: 0.340487
  7100/100000: episode: 71, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.056, mean reward: 0.141 [0.004, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.776, 10.123], loss: 0.004142, mae: 0.072051, mean_q: 0.337889
  7200/100000: episode: 72, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 18.037, mean reward: 0.180 [0.028, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.098], loss: 0.004104, mae: 0.072171, mean_q: 0.339913
  7300/100000: episode: 73, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 19.695, mean reward: 0.197 [0.011, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.667, 10.098], loss: 0.004081, mae: 0.071797, mean_q: 0.338218
  7400/100000: episode: 74, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.806, mean reward: 0.168 [0.021, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.782, 10.232], loss: 0.004103, mae: 0.072884, mean_q: 0.340235
  7500/100000: episode: 75, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.414, mean reward: 0.164 [0.015, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.216, 10.098], loss: 0.004137, mae: 0.072002, mean_q: 0.336588
  7600/100000: episode: 76, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.693, mean reward: 0.147 [0.015, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.098], loss: 0.003956, mae: 0.070983, mean_q: 0.342096
  7700/100000: episode: 77, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 17.236, mean reward: 0.172 [0.035, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.915, 10.190], loss: 0.004080, mae: 0.072597, mean_q: 0.341909
  7800/100000: episode: 78, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 25.326, mean reward: 0.253 [0.029, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.311, 10.098], loss: 0.004085, mae: 0.072481, mean_q: 0.340943
  7900/100000: episode: 79, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 17.870, mean reward: 0.179 [0.025, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.184, 10.411], loss: 0.004140, mae: 0.072601, mean_q: 0.342772
  8000/100000: episode: 80, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 13.163, mean reward: 0.132 [0.014, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.098], loss: 0.004405, mae: 0.075113, mean_q: 0.345492
  8100/100000: episode: 81, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 20.757, mean reward: 0.208 [0.025, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.715, 10.098], loss: 0.004030, mae: 0.071615, mean_q: 0.343222
  8200/100000: episode: 82, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.323, mean reward: 0.193 [0.015, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.620, 10.098], loss: 0.004168, mae: 0.073793, mean_q: 0.346230
  8300/100000: episode: 83, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.872, mean reward: 0.179 [0.016, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.803, 10.098], loss: 0.004045, mae: 0.072571, mean_q: 0.347971
  8400/100000: episode: 84, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 19.443, mean reward: 0.194 [0.007, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.098], loss: 0.003972, mae: 0.071928, mean_q: 0.345630
  8500/100000: episode: 85, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.570, mean reward: 0.166 [0.008, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.380, 10.172], loss: 0.004302, mae: 0.073993, mean_q: 0.346541
  8600/100000: episode: 86, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.031, mean reward: 0.170 [0.025, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.785, 10.098], loss: 0.004028, mae: 0.072332, mean_q: 0.346759
  8700/100000: episode: 87, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 20.697, mean reward: 0.207 [0.039, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.289, 10.098], loss: 0.004218, mae: 0.073082, mean_q: 0.346623
  8800/100000: episode: 88, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 17.395, mean reward: 0.174 [0.014, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.548, 10.169], loss: 0.004168, mae: 0.073970, mean_q: 0.349250
  8900/100000: episode: 89, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 19.937, mean reward: 0.199 [0.035, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.577, 10.282], loss: 0.004065, mae: 0.072122, mean_q: 0.351853
  9000/100000: episode: 90, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.438, mean reward: 0.174 [0.017, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.905, 10.325], loss: 0.004145, mae: 0.073906, mean_q: 0.351841
  9100/100000: episode: 91, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 19.375, mean reward: 0.194 [0.007, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.280, 10.104], loss: 0.004278, mae: 0.073985, mean_q: 0.355765
  9200/100000: episode: 92, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 20.943, mean reward: 0.209 [0.018, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.984, 10.192], loss: 0.004233, mae: 0.073926, mean_q: 0.350735
  9300/100000: episode: 93, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 14.310, mean reward: 0.143 [0.020, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.226, 10.139], loss: 0.003956, mae: 0.071892, mean_q: 0.356478
  9400/100000: episode: 94, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.898, mean reward: 0.189 [0.017, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.841, 10.265], loss: 0.004224, mae: 0.073331, mean_q: 0.354387
  9500/100000: episode: 95, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 19.854, mean reward: 0.199 [0.028, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.289], loss: 0.004181, mae: 0.073479, mean_q: 0.353420
  9600/100000: episode: 96, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 17.300, mean reward: 0.173 [0.014, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.396, 10.098], loss: 0.004253, mae: 0.074221, mean_q: 0.351580
  9700/100000: episode: 97, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 19.822, mean reward: 0.198 [0.011, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.318, 10.098], loss: 0.004143, mae: 0.073728, mean_q: 0.355558
  9800/100000: episode: 98, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 18.096, mean reward: 0.181 [0.035, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.484, 10.400], loss: 0.004141, mae: 0.073275, mean_q: 0.356898
  9900/100000: episode: 99, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.373, mean reward: 0.174 [0.011, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.098], loss: 0.004098, mae: 0.072615, mean_q: 0.356908
 10000/100000: episode: 100, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 25.073, mean reward: 0.251 [0.024, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.010, 10.541], loss: 0.004400, mae: 0.075683, mean_q: 0.357359
 10100/100000: episode: 101, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.637, mean reward: 0.186 [0.011, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.746, 10.215], loss: 0.004224, mae: 0.073519, mean_q: 0.356653
 10200/100000: episode: 102, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 18.983, mean reward: 0.190 [0.033, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.716, 10.098], loss: 0.004292, mae: 0.074388, mean_q: 0.360924
 10300/100000: episode: 103, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 14.759, mean reward: 0.148 [0.006, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.630, 10.351], loss: 0.004214, mae: 0.073982, mean_q: 0.353817
 10400/100000: episode: 104, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 14.750, mean reward: 0.147 [0.019, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.794, 10.098], loss: 0.004330, mae: 0.075119, mean_q: 0.354795
 10500/100000: episode: 105, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 13.326, mean reward: 0.133 [0.006, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.887, 10.098], loss: 0.004169, mae: 0.073107, mean_q: 0.350864
 10600/100000: episode: 106, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 15.386, mean reward: 0.154 [0.016, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.195, 10.098], loss: 0.004168, mae: 0.073334, mean_q: 0.349624
 10700/100000: episode: 107, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.177, mean reward: 0.162 [0.025, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.263], loss: 0.004177, mae: 0.073748, mean_q: 0.349371
 10800/100000: episode: 108, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.361, mean reward: 0.154 [0.019, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.103, 10.314], loss: 0.003926, mae: 0.071721, mean_q: 0.344030
 10900/100000: episode: 109, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.270, mean reward: 0.163 [0.010, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.610, 10.298], loss: 0.004066, mae: 0.072242, mean_q: 0.341437
 11000/100000: episode: 110, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 14.615, mean reward: 0.146 [0.008, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.354, 10.098], loss: 0.004019, mae: 0.071977, mean_q: 0.341767
 11100/100000: episode: 111, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 18.723, mean reward: 0.187 [0.008, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.817, 10.098], loss: 0.004000, mae: 0.071952, mean_q: 0.346028
 11200/100000: episode: 112, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.405, mean reward: 0.174 [0.007, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.558, 10.130], loss: 0.004010, mae: 0.071618, mean_q: 0.346716
 11300/100000: episode: 113, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.607, mean reward: 0.166 [0.014, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.199, 10.098], loss: 0.004092, mae: 0.072232, mean_q: 0.345223
 11400/100000: episode: 114, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 24.138, mean reward: 0.241 [0.016, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.950, 10.098], loss: 0.003922, mae: 0.071060, mean_q: 0.348643
 11500/100000: episode: 115, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 14.410, mean reward: 0.144 [0.007, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.098], loss: 0.003945, mae: 0.071068, mean_q: 0.349036
 11600/100000: episode: 116, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 20.715, mean reward: 0.207 [0.030, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.462, 10.308], loss: 0.003975, mae: 0.071307, mean_q: 0.351496
 11700/100000: episode: 117, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.150, mean reward: 0.161 [0.023, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.098], loss: 0.003907, mae: 0.071211, mean_q: 0.349839
 11800/100000: episode: 118, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.579, mean reward: 0.156 [0.013, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.098], loss: 0.003942, mae: 0.071487, mean_q: 0.350971
 11900/100000: episode: 119, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.622, mean reward: 0.146 [0.007, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.117, 10.098], loss: 0.003816, mae: 0.070103, mean_q: 0.350758
 12000/100000: episode: 120, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.926, mean reward: 0.179 [0.056, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.525, 10.098], loss: 0.003827, mae: 0.070107, mean_q: 0.347951
 12100/100000: episode: 121, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 15.782, mean reward: 0.158 [0.020, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.503, 10.102], loss: 0.004002, mae: 0.071118, mean_q: 0.349974
 12200/100000: episode: 122, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.635, mean reward: 0.166 [0.027, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.870, 10.098], loss: 0.003626, mae: 0.067725, mean_q: 0.351313
 12300/100000: episode: 123, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 19.671, mean reward: 0.197 [0.018, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.623, 10.098], loss: 0.003615, mae: 0.067897, mean_q: 0.346020
 12400/100000: episode: 124, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.701, mean reward: 0.157 [0.012, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.988, 10.112], loss: 0.003756, mae: 0.069258, mean_q: 0.347157
 12500/100000: episode: 125, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 21.149, mean reward: 0.211 [0.024, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.223, 10.098], loss: 0.003787, mae: 0.069624, mean_q: 0.349104
 12600/100000: episode: 126, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 17.119, mean reward: 0.171 [0.024, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.642, 10.235], loss: 0.003724, mae: 0.069358, mean_q: 0.349556
 12700/100000: episode: 127, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.475, mean reward: 0.145 [0.009, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.098], loss: 0.003801, mae: 0.069343, mean_q: 0.346063
 12800/100000: episode: 128, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.049, mean reward: 0.140 [0.002, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.074, 10.145], loss: 0.003747, mae: 0.068726, mean_q: 0.346642
 12900/100000: episode: 129, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 17.283, mean reward: 0.173 [0.038, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.379, 10.296], loss: 0.003796, mae: 0.068481, mean_q: 0.346705
 13000/100000: episode: 130, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.726, mean reward: 0.167 [0.005, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.781, 10.098], loss: 0.003766, mae: 0.068916, mean_q: 0.347216
 13100/100000: episode: 131, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.971, mean reward: 0.160 [0.020, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.705, 10.203], loss: 0.003847, mae: 0.070008, mean_q: 0.342466
 13200/100000: episode: 132, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 20.168, mean reward: 0.202 [0.022, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.408], loss: 0.003819, mae: 0.069825, mean_q: 0.347290
 13300/100000: episode: 133, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.743, mean reward: 0.147 [0.023, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.249, 10.154], loss: 0.003997, mae: 0.070599, mean_q: 0.347079
 13400/100000: episode: 134, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 17.298, mean reward: 0.173 [0.007, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.270, 10.231], loss: 0.003656, mae: 0.067866, mean_q: 0.342647
 13500/100000: episode: 135, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 20.641, mean reward: 0.206 [0.028, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.448, 10.098], loss: 0.003657, mae: 0.068417, mean_q: 0.339837
 13600/100000: episode: 136, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 13.877, mean reward: 0.139 [0.012, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.226, 10.145], loss: 0.003772, mae: 0.069130, mean_q: 0.343311
 13700/100000: episode: 137, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 14.051, mean reward: 0.141 [0.016, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.208, 10.289], loss: 0.003773, mae: 0.068721, mean_q: 0.344565
 13800/100000: episode: 138, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.757, mean reward: 0.188 [0.033, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.396], loss: 0.003850, mae: 0.069190, mean_q: 0.342110
 13900/100000: episode: 139, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 15.487, mean reward: 0.155 [0.013, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.986, 10.350], loss: 0.003676, mae: 0.068060, mean_q: 0.340580
 14000/100000: episode: 140, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 15.008, mean reward: 0.150 [0.022, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.137], loss: 0.003752, mae: 0.068645, mean_q: 0.341044
 14100/100000: episode: 141, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.328, mean reward: 0.143 [0.026, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.688, 10.277], loss: 0.003890, mae: 0.069703, mean_q: 0.336029
 14200/100000: episode: 142, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 22.241, mean reward: 0.222 [0.055, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.640, 10.098], loss: 0.004174, mae: 0.071944, mean_q: 0.335881
 14300/100000: episode: 143, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 19.842, mean reward: 0.198 [0.017, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.266, 10.274], loss: 0.003707, mae: 0.068341, mean_q: 0.339991
 14400/100000: episode: 144, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.842, mean reward: 0.178 [0.027, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.573, 10.098], loss: 0.003974, mae: 0.070612, mean_q: 0.334607
 14500/100000: episode: 145, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.140, mean reward: 0.161 [0.015, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.245, 10.098], loss: 0.003808, mae: 0.069062, mean_q: 0.338746
 14600/100000: episode: 146, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 14.959, mean reward: 0.150 [0.010, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.098], loss: 0.003493, mae: 0.066448, mean_q: 0.341566
 14700/100000: episode: 147, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.445, mean reward: 0.164 [0.017, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.098], loss: 0.003605, mae: 0.066898, mean_q: 0.338027
 14800/100000: episode: 148, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 18.802, mean reward: 0.188 [0.021, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.761, 10.098], loss: 0.003795, mae: 0.068585, mean_q: 0.337468
 14900/100000: episode: 149, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.656, mean reward: 0.197 [0.039, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.600, 10.105], loss: 0.003398, mae: 0.064888, mean_q: 0.336861
 15000/100000: episode: 150, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 14.076, mean reward: 0.141 [0.025, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.990, 10.098], loss: 0.003518, mae: 0.065753, mean_q: 0.332675
 15100/100000: episode: 151, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 15.317, mean reward: 0.153 [0.013, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.069, 10.098], loss: 0.003436, mae: 0.065138, mean_q: 0.332142
 15200/100000: episode: 152, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.233, mean reward: 0.172 [0.013, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.607, 10.098], loss: 0.003646, mae: 0.067059, mean_q: 0.331182
 15300/100000: episode: 153, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 23.217, mean reward: 0.232 [0.010, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.805, 10.098], loss: 0.003584, mae: 0.067050, mean_q: 0.329416
 15400/100000: episode: 154, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 19.232, mean reward: 0.192 [0.018, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.042, 10.329], loss: 0.003613, mae: 0.067056, mean_q: 0.336258
 15500/100000: episode: 155, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.204, mean reward: 0.152 [0.003, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.183], loss: 0.003720, mae: 0.067783, mean_q: 0.338054
 15600/100000: episode: 156, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 12.857, mean reward: 0.129 [0.007, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.590, 10.264], loss: 0.003582, mae: 0.066829, mean_q: 0.338580
 15700/100000: episode: 157, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.360, mean reward: 0.174 [0.009, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.888, 10.153], loss: 0.003729, mae: 0.067625, mean_q: 0.337091
 15800/100000: episode: 158, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 15.537, mean reward: 0.155 [0.005, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.115, 10.202], loss: 0.003755, mae: 0.068048, mean_q: 0.338664
 15900/100000: episode: 159, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.708, mean reward: 0.157 [0.023, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.313, 10.098], loss: 0.003688, mae: 0.066994, mean_q: 0.336208
 16000/100000: episode: 160, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 14.119, mean reward: 0.141 [0.007, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.844, 10.293], loss: 0.003558, mae: 0.066707, mean_q: 0.337532
 16100/100000: episode: 161, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.331, mean reward: 0.153 [0.024, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.098], loss: 0.003462, mae: 0.065227, mean_q: 0.335937
 16200/100000: episode: 162, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.573, mean reward: 0.166 [0.026, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.021, 10.131], loss: 0.003723, mae: 0.067796, mean_q: 0.337060
 16300/100000: episode: 163, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 17.658, mean reward: 0.177 [0.030, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.187], loss: 0.003701, mae: 0.067862, mean_q: 0.337957
 16400/100000: episode: 164, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.991, mean reward: 0.150 [0.020, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.540, 10.098], loss: 0.003726, mae: 0.067889, mean_q: 0.331942
 16500/100000: episode: 165, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 16.672, mean reward: 0.167 [0.009, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.767, 10.098], loss: 0.003563, mae: 0.066400, mean_q: 0.327781
 16600/100000: episode: 166, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.540, mean reward: 0.155 [0.016, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.840, 10.426], loss: 0.003512, mae: 0.065801, mean_q: 0.325583
 16700/100000: episode: 167, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 15.149, mean reward: 0.151 [0.009, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.533, 10.098], loss: 0.003629, mae: 0.066768, mean_q: 0.329406
 16800/100000: episode: 168, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 17.390, mean reward: 0.174 [0.006, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.044, 10.254], loss: 0.003560, mae: 0.066760, mean_q: 0.328885
 16900/100000: episode: 169, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.591, mean reward: 0.136 [0.005, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.005, 10.166], loss: 0.003614, mae: 0.066504, mean_q: 0.330447
 17000/100000: episode: 170, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.017, mean reward: 0.170 [0.037, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.292], loss: 0.003730, mae: 0.068238, mean_q: 0.326971
 17100/100000: episode: 171, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 19.349, mean reward: 0.193 [0.009, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.499, 10.098], loss: 0.003402, mae: 0.065136, mean_q: 0.325586
 17200/100000: episode: 172, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 13.090, mean reward: 0.131 [0.009, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.449, 10.143], loss: 0.003596, mae: 0.066911, mean_q: 0.330498
 17300/100000: episode: 173, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.798, mean reward: 0.158 [0.029, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.171, 10.098], loss: 0.003467, mae: 0.065677, mean_q: 0.326516
 17400/100000: episode: 174, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 15.230, mean reward: 0.152 [0.007, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.116], loss: 0.003322, mae: 0.064216, mean_q: 0.326818
 17500/100000: episode: 175, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.243, mean reward: 0.152 [0.024, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.312], loss: 0.003319, mae: 0.064720, mean_q: 0.329462
 17600/100000: episode: 176, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 16.683, mean reward: 0.167 [0.005, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.005, 10.110], loss: 0.003971, mae: 0.070535, mean_q: 0.329523
 17700/100000: episode: 177, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 13.925, mean reward: 0.139 [0.009, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.218, 10.098], loss: 0.003501, mae: 0.066728, mean_q: 0.323207
 17800/100000: episode: 178, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 14.404, mean reward: 0.144 [0.009, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.892, 10.098], loss: 0.003532, mae: 0.066078, mean_q: 0.325026
 17900/100000: episode: 179, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.479, mean reward: 0.165 [0.017, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.150, 10.127], loss: 0.003573, mae: 0.066954, mean_q: 0.323218
 18000/100000: episode: 180, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.793, mean reward: 0.168 [0.009, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.858, 10.098], loss: 0.003276, mae: 0.064515, mean_q: 0.321009
 18100/100000: episode: 181, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.775, mean reward: 0.178 [0.016, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.701, 10.098], loss: 0.003566, mae: 0.066807, mean_q: 0.327504
 18200/100000: episode: 182, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.308, mean reward: 0.173 [0.009, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.263, 10.380], loss: 0.003750, mae: 0.067890, mean_q: 0.324747
 18300/100000: episode: 183, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.694, mean reward: 0.157 [0.021, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.211], loss: 0.003731, mae: 0.067888, mean_q: 0.322187
 18400/100000: episode: 184, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 17.795, mean reward: 0.178 [0.004, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.169, 10.098], loss: 0.003433, mae: 0.065363, mean_q: 0.320598
 18500/100000: episode: 185, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.374, mean reward: 0.144 [0.015, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.931, 10.098], loss: 0.003589, mae: 0.067420, mean_q: 0.322465
 18600/100000: episode: 186, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 13.343, mean reward: 0.133 [0.012, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.129, 10.128], loss: 0.003523, mae: 0.066531, mean_q: 0.322205
 18700/100000: episode: 187, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 17.385, mean reward: 0.174 [0.016, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.631, 10.098], loss: 0.003541, mae: 0.066635, mean_q: 0.320891
 18800/100000: episode: 188, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 19.685, mean reward: 0.197 [0.040, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.386, 10.189], loss: 0.003226, mae: 0.063484, mean_q: 0.324576
 18900/100000: episode: 189, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 13.095, mean reward: 0.131 [0.008, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.603, 10.210], loss: 0.003438, mae: 0.065093, mean_q: 0.323533
 19000/100000: episode: 190, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 16.586, mean reward: 0.166 [0.019, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.271, 10.128], loss: 0.003638, mae: 0.067058, mean_q: 0.325855
 19100/100000: episode: 191, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 16.212, mean reward: 0.162 [0.006, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.415, 10.098], loss: 0.003554, mae: 0.066774, mean_q: 0.323887
 19200/100000: episode: 192, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.755, mean reward: 0.168 [0.015, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.963, 10.125], loss: 0.003725, mae: 0.068213, mean_q: 0.322319
 19300/100000: episode: 193, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 18.705, mean reward: 0.187 [0.015, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.324, 10.129], loss: 0.003605, mae: 0.067188, mean_q: 0.324234
 19400/100000: episode: 194, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.948, mean reward: 0.189 [0.016, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.635, 10.350], loss: 0.003413, mae: 0.064156, mean_q: 0.317774
 19500/100000: episode: 195, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.619, mean reward: 0.166 [0.012, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.228, 10.098], loss: 0.003332, mae: 0.064707, mean_q: 0.318616
 19600/100000: episode: 196, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 16.922, mean reward: 0.169 [0.036, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.873, 10.098], loss: 0.003366, mae: 0.065104, mean_q: 0.319299
 19700/100000: episode: 197, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 17.611, mean reward: 0.176 [0.019, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.953, 10.218], loss: 0.003390, mae: 0.064953, mean_q: 0.320265
 19800/100000: episode: 198, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 17.582, mean reward: 0.176 [0.031, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.511, 10.215], loss: 0.003643, mae: 0.067069, mean_q: 0.320369
 19900/100000: episode: 199, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.513, mean reward: 0.165 [0.010, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.219, 10.098], loss: 0.003635, mae: 0.066987, mean_q: 0.321688
 20000/100000: episode: 200, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 17.124, mean reward: 0.171 [0.027, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.098], loss: 0.003620, mae: 0.067243, mean_q: 0.323063
 20100/100000: episode: 201, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 15.473, mean reward: 0.155 [0.042, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.720, 10.245], loss: 0.003469, mae: 0.066180, mean_q: 0.326261
 20200/100000: episode: 202, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 20.963, mean reward: 0.210 [0.019, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.485, 10.487], loss: 0.003552, mae: 0.066303, mean_q: 0.325977
 20300/100000: episode: 203, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 13.201, mean reward: 0.132 [0.007, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.522, 10.098], loss: 0.003392, mae: 0.065042, mean_q: 0.322055
 20400/100000: episode: 204, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 17.624, mean reward: 0.176 [0.016, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.366, 10.210], loss: 0.003543, mae: 0.067051, mean_q: 0.320486
 20500/100000: episode: 205, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 16.423, mean reward: 0.164 [0.016, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.158], loss: 0.003508, mae: 0.066279, mean_q: 0.322922
 20600/100000: episode: 206, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 23.109, mean reward: 0.231 [0.032, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.584, 10.288], loss: 0.003393, mae: 0.065349, mean_q: 0.321306
 20700/100000: episode: 207, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.339, mean reward: 0.163 [0.030, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.656, 10.322], loss: 0.003670, mae: 0.068187, mean_q: 0.325469
 20800/100000: episode: 208, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 18.103, mean reward: 0.181 [0.032, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.838, 10.183], loss: 0.003668, mae: 0.067420, mean_q: 0.322595
 20900/100000: episode: 209, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.222, mean reward: 0.152 [0.008, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.352, 10.098], loss: 0.003517, mae: 0.066765, mean_q: 0.326561
 21000/100000: episode: 210, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 16.302, mean reward: 0.163 [0.023, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.887, 10.165], loss: 0.003428, mae: 0.065930, mean_q: 0.327110
 21100/100000: episode: 211, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 20.287, mean reward: 0.203 [0.037, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.571, 10.202], loss: 0.003752, mae: 0.067859, mean_q: 0.327682
 21200/100000: episode: 212, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 19.737, mean reward: 0.197 [0.013, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.439, 10.098], loss: 0.003531, mae: 0.066100, mean_q: 0.325593
 21300/100000: episode: 213, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 18.023, mean reward: 0.180 [0.018, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.103, 10.352], loss: 0.003588, mae: 0.066513, mean_q: 0.325938
 21400/100000: episode: 214, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.150, mean reward: 0.171 [0.023, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.434, 10.365], loss: 0.003570, mae: 0.066786, mean_q: 0.325822
 21500/100000: episode: 215, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 22.792, mean reward: 0.228 [0.026, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.792, 10.098], loss: 0.003645, mae: 0.066847, mean_q: 0.326977
 21600/100000: episode: 216, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 19.365, mean reward: 0.194 [0.027, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.568, 10.312], loss: 0.003622, mae: 0.067915, mean_q: 0.331642
 21700/100000: episode: 217, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 22.691, mean reward: 0.227 [0.019, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.365, 10.206], loss: 0.003426, mae: 0.065889, mean_q: 0.334166
 21800/100000: episode: 218, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 20.330, mean reward: 0.203 [0.032, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.241], loss: 0.003660, mae: 0.068404, mean_q: 0.338317
 21900/100000: episode: 219, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 14.822, mean reward: 0.148 [0.012, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.487, 10.111], loss: 0.003521, mae: 0.066028, mean_q: 0.339351
 22000/100000: episode: 220, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.293, mean reward: 0.173 [0.034, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.295, 10.098], loss: 0.003832, mae: 0.069375, mean_q: 0.336784
 22100/100000: episode: 221, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 13.651, mean reward: 0.137 [0.016, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.812, 10.168], loss: 0.003700, mae: 0.067419, mean_q: 0.337363
 22200/100000: episode: 222, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 21.535, mean reward: 0.215 [0.017, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.512, 10.197], loss: 0.003753, mae: 0.068394, mean_q: 0.335334
 22300/100000: episode: 223, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.602, mean reward: 0.156 [0.018, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.774, 10.137], loss: 0.003625, mae: 0.067540, mean_q: 0.338181
 22400/100000: episode: 224, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 20.355, mean reward: 0.204 [0.029, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.084, 10.340], loss: 0.003679, mae: 0.068174, mean_q: 0.341798
 22500/100000: episode: 225, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 14.003, mean reward: 0.140 [0.011, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.047, 10.286], loss: 0.003723, mae: 0.068099, mean_q: 0.339852
 22600/100000: episode: 226, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 17.003, mean reward: 0.170 [0.034, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.120], loss: 0.003623, mae: 0.066443, mean_q: 0.335787
 22700/100000: episode: 227, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 19.537, mean reward: 0.195 [0.013, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.124, 10.442], loss: 0.003389, mae: 0.065640, mean_q: 0.338980
 22800/100000: episode: 228, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 18.696, mean reward: 0.187 [0.068, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.466, 10.098], loss: 0.003637, mae: 0.067913, mean_q: 0.341970
 22900/100000: episode: 229, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 14.364, mean reward: 0.144 [0.025, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.914, 10.104], loss: 0.003598, mae: 0.067969, mean_q: 0.346706
 23000/100000: episode: 230, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.702, mean reward: 0.157 [0.011, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.215, 10.227], loss: 0.003804, mae: 0.069812, mean_q: 0.344471
 23100/100000: episode: 231, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.334, mean reward: 0.163 [0.009, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.127, 10.128], loss: 0.003591, mae: 0.066661, mean_q: 0.342356
 23200/100000: episode: 232, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 18.084, mean reward: 0.181 [0.011, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.355, 10.321], loss: 0.003517, mae: 0.065997, mean_q: 0.341002
 23300/100000: episode: 233, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 15.089, mean reward: 0.151 [0.014, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.588, 10.098], loss: 0.003470, mae: 0.065939, mean_q: 0.344030
 23400/100000: episode: 234, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 22.365, mean reward: 0.224 [0.030, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.289, 10.318], loss: 0.003391, mae: 0.064814, mean_q: 0.347711
 23500/100000: episode: 235, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 12.602, mean reward: 0.126 [0.016, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.957, 10.190], loss: 0.003502, mae: 0.066073, mean_q: 0.346679
 23600/100000: episode: 236, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 18.423, mean reward: 0.184 [0.019, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.879, 10.111], loss: 0.003496, mae: 0.065840, mean_q: 0.348072
 23700/100000: episode: 237, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 20.599, mean reward: 0.206 [0.035, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.604, 10.134], loss: 0.003868, mae: 0.069456, mean_q: 0.347230
 23800/100000: episode: 238, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 13.751, mean reward: 0.138 [0.018, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.410, 10.098], loss: 0.003505, mae: 0.066232, mean_q: 0.342400
 23900/100000: episode: 239, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 13.641, mean reward: 0.136 [0.012, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.078, 10.098], loss: 0.003664, mae: 0.067445, mean_q: 0.347052
 24000/100000: episode: 240, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 18.864, mean reward: 0.189 [0.020, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.098], loss: 0.003687, mae: 0.067919, mean_q: 0.347697
 24100/100000: episode: 241, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 19.398, mean reward: 0.194 [0.023, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.808, 10.499], loss: 0.003577, mae: 0.066863, mean_q: 0.343967
 24200/100000: episode: 242, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 18.560, mean reward: 0.186 [0.017, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.745, 10.253], loss: 0.003611, mae: 0.067197, mean_q: 0.350650
 24300/100000: episode: 243, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.872, mean reward: 0.149 [0.014, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.572, 10.098], loss: 0.003434, mae: 0.065633, mean_q: 0.346340
 24400/100000: episode: 244, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 16.819, mean reward: 0.168 [0.012, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.321, 10.098], loss: 0.003428, mae: 0.065813, mean_q: 0.346889
 24500/100000: episode: 245, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 15.898, mean reward: 0.159 [0.011, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.140], loss: 0.003769, mae: 0.068070, mean_q: 0.349233
 24600/100000: episode: 246, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 18.159, mean reward: 0.182 [0.004, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.412, 10.326], loss: 0.003940, mae: 0.069938, mean_q: 0.349008
 24700/100000: episode: 247, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 15.195, mean reward: 0.152 [0.005, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.003423, mae: 0.065077, mean_q: 0.345840
 24800/100000: episode: 248, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.843, mean reward: 0.168 [0.030, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.683, 10.098], loss: 0.003449, mae: 0.065593, mean_q: 0.340273
 24900/100000: episode: 249, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 20.803, mean reward: 0.208 [0.021, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.279, 10.098], loss: 0.003603, mae: 0.066826, mean_q: 0.345946
 25000/100000: episode: 250, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.082, mean reward: 0.181 [0.013, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.175, 10.098], loss: 0.003609, mae: 0.067525, mean_q: 0.339984
 25100/100000: episode: 251, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.968, mean reward: 0.170 [0.016, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.142, 10.113], loss: 0.003397, mae: 0.064072, mean_q: 0.345416
 25200/100000: episode: 252, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 17.577, mean reward: 0.176 [0.022, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.695, 10.215], loss: 0.003507, mae: 0.066004, mean_q: 0.348229
 25300/100000: episode: 253, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 17.071, mean reward: 0.171 [0.023, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.098], loss: 0.003529, mae: 0.066101, mean_q: 0.345101
 25400/100000: episode: 254, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 16.729, mean reward: 0.167 [0.021, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.098], loss: 0.003750, mae: 0.067902, mean_q: 0.350229
 25500/100000: episode: 255, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 15.799, mean reward: 0.158 [0.015, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.893, 10.250], loss: 0.003689, mae: 0.067374, mean_q: 0.347122
 25600/100000: episode: 256, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 16.997, mean reward: 0.170 [0.024, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.098], loss: 0.003590, mae: 0.066566, mean_q: 0.345900
 25700/100000: episode: 257, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 13.703, mean reward: 0.137 [0.014, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.555, 10.098], loss: 0.003864, mae: 0.069901, mean_q: 0.346034
 25800/100000: episode: 258, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 15.110, mean reward: 0.151 [0.023, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.229], loss: 0.003429, mae: 0.065479, mean_q: 0.345889
 25900/100000: episode: 259, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 12.436, mean reward: 0.124 [0.008, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.457, 10.112], loss: 0.003687, mae: 0.067955, mean_q: 0.343659
 26000/100000: episode: 260, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 18.328, mean reward: 0.183 [0.009, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.456, 10.098], loss: 0.003598, mae: 0.066789, mean_q: 0.342172
 26100/100000: episode: 261, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 13.553, mean reward: 0.136 [0.027, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.184, 10.098], loss: 0.003360, mae: 0.064811, mean_q: 0.341356
 26200/100000: episode: 262, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 15.550, mean reward: 0.155 [0.020, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.480, 10.257], loss: 0.003634, mae: 0.067181, mean_q: 0.344282
 26300/100000: episode: 263, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 18.941, mean reward: 0.189 [0.017, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.313, 10.295], loss: 0.003388, mae: 0.064921, mean_q: 0.334849
 26400/100000: episode: 264, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 15.216, mean reward: 0.152 [0.009, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.086, 10.107], loss: 0.003477, mae: 0.066382, mean_q: 0.337550
 26500/100000: episode: 265, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 23.169, mean reward: 0.232 [0.003, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.082, 10.368], loss: 0.003598, mae: 0.067504, mean_q: 0.336427
 26600/100000: episode: 266, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 17.954, mean reward: 0.180 [0.026, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.493, 10.098], loss: 0.003505, mae: 0.066794, mean_q: 0.333583
 26700/100000: episode: 267, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.648, mean reward: 0.146 [0.032, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.098], loss: 0.003317, mae: 0.065051, mean_q: 0.334477
 26800/100000: episode: 268, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 14.316, mean reward: 0.143 [0.015, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.989, 10.226], loss: 0.003456, mae: 0.065236, mean_q: 0.332429
 26900/100000: episode: 269, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.438, mean reward: 0.164 [0.008, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.443, 10.214], loss: 0.003366, mae: 0.065524, mean_q: 0.334964
 27000/100000: episode: 270, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 17.433, mean reward: 0.174 [0.011, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.176, 10.241], loss: 0.003577, mae: 0.066871, mean_q: 0.327310
 27100/100000: episode: 271, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 14.534, mean reward: 0.145 [0.016, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.003364, mae: 0.065223, mean_q: 0.332101
 27200/100000: episode: 272, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.245, mean reward: 0.162 [0.004, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.101, 10.098], loss: 0.003362, mae: 0.064441, mean_q: 0.330338
 27300/100000: episode: 273, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.423, mean reward: 0.164 [0.013, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.167], loss: 0.003478, mae: 0.065777, mean_q: 0.331877
 27400/100000: episode: 274, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 20.585, mean reward: 0.206 [0.025, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.442], loss: 0.003372, mae: 0.064196, mean_q: 0.331476
 27500/100000: episode: 275, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 17.446, mean reward: 0.174 [0.029, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.944, 10.232], loss: 0.003510, mae: 0.065387, mean_q: 0.331364
 27600/100000: episode: 276, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 20.138, mean reward: 0.201 [0.020, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.636, 10.098], loss: 0.003308, mae: 0.064143, mean_q: 0.332828
 27700/100000: episode: 277, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.677, mean reward: 0.147 [0.028, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.562, 10.195], loss: 0.003397, mae: 0.065050, mean_q: 0.333741
 27800/100000: episode: 278, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.853, mean reward: 0.149 [0.019, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.402, 10.139], loss: 0.003406, mae: 0.064641, mean_q: 0.328203
 27900/100000: episode: 279, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 23.721, mean reward: 0.237 [0.045, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.276], loss: 0.003247, mae: 0.064018, mean_q: 0.329444
 28000/100000: episode: 280, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 13.227, mean reward: 0.132 [0.011, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.165], loss: 0.003401, mae: 0.065004, mean_q: 0.331264
 28100/100000: episode: 281, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 22.385, mean reward: 0.224 [0.039, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.695, 10.317], loss: 0.003763, mae: 0.068804, mean_q: 0.334997
 28200/100000: episode: 282, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.346, mean reward: 0.163 [0.018, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.210, 10.098], loss: 0.003378, mae: 0.064765, mean_q: 0.335224
 28300/100000: episode: 283, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 17.431, mean reward: 0.174 [0.014, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.139, 10.216], loss: 0.003317, mae: 0.063529, mean_q: 0.335545
 28400/100000: episode: 284, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 16.265, mean reward: 0.163 [0.006, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.683, 10.098], loss: 0.003239, mae: 0.064180, mean_q: 0.331622
 28500/100000: episode: 285, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 21.419, mean reward: 0.214 [0.023, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.611, 10.098], loss: 0.003512, mae: 0.066766, mean_q: 0.332611
 28600/100000: episode: 286, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.199, mean reward: 0.162 [0.023, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.738, 10.098], loss: 0.003515, mae: 0.066402, mean_q: 0.332865
 28700/100000: episode: 287, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 15.659, mean reward: 0.157 [0.006, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.108], loss: 0.003216, mae: 0.063399, mean_q: 0.329711
 28800/100000: episode: 288, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 17.539, mean reward: 0.175 [0.020, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.762, 10.135], loss: 0.003544, mae: 0.066555, mean_q: 0.332562
 28900/100000: episode: 289, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 15.817, mean reward: 0.158 [0.015, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.918, 10.098], loss: 0.003249, mae: 0.063709, mean_q: 0.330537
 29000/100000: episode: 290, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 20.646, mean reward: 0.206 [0.024, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.109, 10.126], loss: 0.003415, mae: 0.065476, mean_q: 0.331438
 29100/100000: episode: 291, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 17.990, mean reward: 0.180 [0.016, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.394, 10.098], loss: 0.003575, mae: 0.066817, mean_q: 0.336667
 29200/100000: episode: 292, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.660, mean reward: 0.157 [0.026, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.370, 10.343], loss: 0.003269, mae: 0.063932, mean_q: 0.334355
 29300/100000: episode: 293, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 15.660, mean reward: 0.157 [0.021, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.138], loss: 0.003375, mae: 0.064946, mean_q: 0.337700
 29400/100000: episode: 294, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 17.431, mean reward: 0.174 [0.006, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.687, 10.188], loss: 0.003362, mae: 0.064580, mean_q: 0.338720
 29500/100000: episode: 295, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.447, mean reward: 0.164 [0.015, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.085, 10.098], loss: 0.003275, mae: 0.064193, mean_q: 0.331571
 29600/100000: episode: 296, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 15.097, mean reward: 0.151 [0.009, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.073, 10.537], loss: 0.003283, mae: 0.064095, mean_q: 0.335257
 29700/100000: episode: 297, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.297, mean reward: 0.153 [0.039, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.790, 10.098], loss: 0.003317, mae: 0.064702, mean_q: 0.331942
 29800/100000: episode: 298, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 19.364, mean reward: 0.194 [0.039, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.505, 10.332], loss: 0.003624, mae: 0.066643, mean_q: 0.334707
 29900/100000: episode: 299, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 18.113, mean reward: 0.181 [0.022, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.418, 10.362], loss: 0.003048, mae: 0.062089, mean_q: 0.333052
 30000/100000: episode: 300, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 17.052, mean reward: 0.171 [0.027, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.073, 10.098], loss: 0.003242, mae: 0.064289, mean_q: 0.332169
 30100/100000: episode: 301, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 16.354, mean reward: 0.164 [0.020, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.985, 10.098], loss: 0.003141, mae: 0.062901, mean_q: 0.333571
 30200/100000: episode: 302, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 21.674, mean reward: 0.217 [0.031, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.624, 10.098], loss: 0.003543, mae: 0.066786, mean_q: 0.331485
 30300/100000: episode: 303, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.901, mean reward: 0.159 [0.014, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.852, 10.098], loss: 0.003350, mae: 0.064543, mean_q: 0.334943
 30400/100000: episode: 304, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 18.388, mean reward: 0.184 [0.012, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.203, 10.300], loss: 0.003251, mae: 0.063478, mean_q: 0.334594
 30500/100000: episode: 305, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.905, mean reward: 0.169 [0.010, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.051, 10.234], loss: 0.003605, mae: 0.067301, mean_q: 0.337941
 30600/100000: episode: 306, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 17.378, mean reward: 0.174 [0.039, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.640, 10.098], loss: 0.003344, mae: 0.064842, mean_q: 0.335355
 30700/100000: episode: 307, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 20.779, mean reward: 0.208 [0.034, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.793, 10.305], loss: 0.003301, mae: 0.064435, mean_q: 0.331175
 30800/100000: episode: 308, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.258, mean reward: 0.153 [0.012, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.215], loss: 0.003310, mae: 0.064514, mean_q: 0.341890
 30900/100000: episode: 309, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 20.445, mean reward: 0.204 [0.016, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.710, 10.270], loss: 0.003361, mae: 0.064855, mean_q: 0.337117
 31000/100000: episode: 310, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.196, mean reward: 0.162 [0.018, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.099], loss: 0.003128, mae: 0.062421, mean_q: 0.343635
 31100/100000: episode: 311, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 16.445, mean reward: 0.164 [0.025, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.477, 10.195], loss: 0.003368, mae: 0.065210, mean_q: 0.342512
 31200/100000: episode: 312, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 13.814, mean reward: 0.138 [0.016, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.098], loss: 0.003370, mae: 0.064270, mean_q: 0.337916
 31300/100000: episode: 313, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 14.835, mean reward: 0.148 [0.005, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.098], loss: 0.003547, mae: 0.066617, mean_q: 0.338668
 31400/100000: episode: 314, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 18.449, mean reward: 0.184 [0.031, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.191, 10.098], loss: 0.003499, mae: 0.065863, mean_q: 0.340232
 31500/100000: episode: 315, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 19.228, mean reward: 0.192 [0.013, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.055, 10.098], loss: 0.003218, mae: 0.063227, mean_q: 0.337928
 31600/100000: episode: 316, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 18.071, mean reward: 0.181 [0.018, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.098], loss: 0.003529, mae: 0.065479, mean_q: 0.340066
 31700/100000: episode: 317, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 14.052, mean reward: 0.141 [0.019, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.011, 10.098], loss: 0.003352, mae: 0.064388, mean_q: 0.340434
 31800/100000: episode: 318, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 11.372, mean reward: 0.114 [0.011, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.003468, mae: 0.065947, mean_q: 0.339208
 31900/100000: episode: 319, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 15.324, mean reward: 0.153 [0.024, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.114], loss: 0.003410, mae: 0.064940, mean_q: 0.335278
 32000/100000: episode: 320, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.162, mean reward: 0.152 [0.016, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.683, 10.098], loss: 0.003331, mae: 0.064315, mean_q: 0.341271
 32100/100000: episode: 321, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.718, mean reward: 0.157 [0.014, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.807, 10.098], loss: 0.003456, mae: 0.065622, mean_q: 0.337338
 32200/100000: episode: 322, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 21.090, mean reward: 0.211 [0.017, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.989, 10.312], loss: 0.003365, mae: 0.064983, mean_q: 0.338890
 32300/100000: episode: 323, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 14.924, mean reward: 0.149 [0.014, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.433, 10.230], loss: 0.003504, mae: 0.066090, mean_q: 0.342574
 32400/100000: episode: 324, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 17.668, mean reward: 0.177 [0.009, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.755, 10.098], loss: 0.003399, mae: 0.064887, mean_q: 0.338823
 32500/100000: episode: 325, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 15.484, mean reward: 0.155 [0.014, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.003276, mae: 0.063411, mean_q: 0.334957
 32600/100000: episode: 326, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 14.522, mean reward: 0.145 [0.010, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.153, 10.114], loss: 0.003416, mae: 0.064835, mean_q: 0.332697
 32700/100000: episode: 327, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 20.141, mean reward: 0.201 [0.023, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.145], loss: 0.003474, mae: 0.065933, mean_q: 0.333678
 32800/100000: episode: 328, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 19.101, mean reward: 0.191 [0.019, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.282, 10.235], loss: 0.003323, mae: 0.064075, mean_q: 0.338079
 32900/100000: episode: 329, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 20.387, mean reward: 0.204 [0.026, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.200, 10.216], loss: 0.003266, mae: 0.063231, mean_q: 0.338049
 33000/100000: episode: 330, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 17.063, mean reward: 0.171 [0.044, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.396, 10.098], loss: 0.003366, mae: 0.064137, mean_q: 0.334926
 33100/100000: episode: 331, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.984, mean reward: 0.150 [0.014, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.098], loss: 0.003483, mae: 0.065238, mean_q: 0.334085
 33200/100000: episode: 332, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 15.476, mean reward: 0.155 [0.016, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.098], loss: 0.003496, mae: 0.065725, mean_q: 0.332687
 33300/100000: episode: 333, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.686, mean reward: 0.137 [0.010, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.167], loss: 0.003443, mae: 0.065603, mean_q: 0.333753
 33400/100000: episode: 334, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 21.872, mean reward: 0.219 [0.047, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.252], loss: 0.003413, mae: 0.065231, mean_q: 0.334898
 33500/100000: episode: 335, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 17.110, mean reward: 0.171 [0.032, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.802, 10.098], loss: 0.003490, mae: 0.065783, mean_q: 0.337753
 33600/100000: episode: 336, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 14.180, mean reward: 0.142 [0.011, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.582, 10.098], loss: 0.003532, mae: 0.065942, mean_q: 0.337832
 33700/100000: episode: 337, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 18.144, mean reward: 0.181 [0.019, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.755, 10.291], loss: 0.003674, mae: 0.067624, mean_q: 0.340101
 33800/100000: episode: 338, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 23.207, mean reward: 0.232 [0.014, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.561, 10.428], loss: 0.003574, mae: 0.066470, mean_q: 0.339676
 33900/100000: episode: 339, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 19.684, mean reward: 0.197 [0.022, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.035, 10.316], loss: 0.003567, mae: 0.066241, mean_q: 0.339677
 34000/100000: episode: 340, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 16.359, mean reward: 0.164 [0.015, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.571, 10.304], loss: 0.003401, mae: 0.064695, mean_q: 0.341660
 34100/100000: episode: 341, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 16.570, mean reward: 0.166 [0.010, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.164], loss: 0.003438, mae: 0.064768, mean_q: 0.337473
 34200/100000: episode: 342, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.930, mean reward: 0.169 [0.028, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.868, 10.098], loss: 0.003401, mae: 0.063835, mean_q: 0.340722
 34300/100000: episode: 343, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 18.699, mean reward: 0.187 [0.027, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.049, 10.297], loss: 0.003241, mae: 0.063104, mean_q: 0.337933
 34400/100000: episode: 344, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 16.481, mean reward: 0.165 [0.018, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.382, 10.098], loss: 0.003493, mae: 0.065505, mean_q: 0.338684
 34500/100000: episode: 345, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 21.555, mean reward: 0.216 [0.028, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.153, 10.098], loss: 0.003470, mae: 0.065507, mean_q: 0.337814
 34600/100000: episode: 346, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 17.444, mean reward: 0.174 [0.015, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.390, 10.098], loss: 0.003322, mae: 0.064320, mean_q: 0.337755
 34700/100000: episode: 347, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.732, mean reward: 0.147 [0.014, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.212, 10.275], loss: 0.003440, mae: 0.064452, mean_q: 0.343762
 34800/100000: episode: 348, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 15.835, mean reward: 0.158 [0.029, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.197, 10.098], loss: 0.003433, mae: 0.064549, mean_q: 0.339916
 34900/100000: episode: 349, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 15.740, mean reward: 0.157 [0.012, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.132], loss: 0.003582, mae: 0.066095, mean_q: 0.338481
 35000/100000: episode: 350, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 16.672, mean reward: 0.167 [0.018, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.282], loss: 0.003268, mae: 0.063629, mean_q: 0.336659
 35100/100000: episode: 351, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 24.739, mean reward: 0.247 [0.004, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.466], loss: 0.003329, mae: 0.064085, mean_q: 0.338128
 35200/100000: episode: 352, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.427, mean reward: 0.154 [0.012, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.196], loss: 0.003453, mae: 0.065661, mean_q: 0.336699
 35300/100000: episode: 353, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 19.058, mean reward: 0.191 [0.050, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.200, 10.098], loss: 0.003544, mae: 0.066227, mean_q: 0.338515
 35400/100000: episode: 354, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 17.280, mean reward: 0.173 [0.028, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.474, 10.270], loss: 0.003418, mae: 0.064960, mean_q: 0.340303
 35500/100000: episode: 355, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 18.296, mean reward: 0.183 [0.003, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.572, 10.322], loss: 0.003414, mae: 0.064296, mean_q: 0.338680
 35600/100000: episode: 356, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 17.418, mean reward: 0.174 [0.005, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.746, 10.140], loss: 0.003669, mae: 0.067007, mean_q: 0.340496
 35700/100000: episode: 357, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 18.666, mean reward: 0.187 [0.011, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.729, 10.098], loss: 0.003648, mae: 0.066754, mean_q: 0.340368
 35800/100000: episode: 358, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 18.054, mean reward: 0.181 [0.034, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.137, 10.262], loss: 0.003445, mae: 0.064985, mean_q: 0.336619
 35900/100000: episode: 359, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 19.429, mean reward: 0.194 [0.036, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.034, 10.339], loss: 0.003620, mae: 0.067229, mean_q: 0.343114
 36000/100000: episode: 360, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 19.319, mean reward: 0.193 [0.022, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.354], loss: 0.003556, mae: 0.066597, mean_q: 0.338662
 36100/100000: episode: 361, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 14.376, mean reward: 0.144 [0.011, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.134, 10.196], loss: 0.003345, mae: 0.064290, mean_q: 0.340642
 36200/100000: episode: 362, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 17.226, mean reward: 0.172 [0.040, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.299], loss: 0.003392, mae: 0.065170, mean_q: 0.339984
 36300/100000: episode: 363, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 25.076, mean reward: 0.251 [0.043, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.528, 10.098], loss: 0.003416, mae: 0.065159, mean_q: 0.343271
 36400/100000: episode: 364, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.885, mean reward: 0.169 [0.006, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.305, 10.110], loss: 0.003468, mae: 0.065074, mean_q: 0.343966
 36500/100000: episode: 365, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.999, mean reward: 0.170 [0.003, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.201], loss: 0.003469, mae: 0.065717, mean_q: 0.341775
 36600/100000: episode: 366, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.725, mean reward: 0.157 [0.030, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.917, 10.098], loss: 0.003582, mae: 0.066050, mean_q: 0.345866
 36700/100000: episode: 367, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 23.216, mean reward: 0.232 [0.001, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.199, 10.098], loss: 0.003296, mae: 0.063845, mean_q: 0.342296
 36800/100000: episode: 368, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.236, mean reward: 0.132 [0.006, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.944, 10.143], loss: 0.003286, mae: 0.063820, mean_q: 0.345092
 36900/100000: episode: 369, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 13.743, mean reward: 0.137 [0.010, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.852, 10.159], loss: 0.003471, mae: 0.065975, mean_q: 0.349491
 37000/100000: episode: 370, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 16.091, mean reward: 0.161 [0.019, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.854, 10.463], loss: 0.003464, mae: 0.065008, mean_q: 0.348501
 37100/100000: episode: 371, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.524, mean reward: 0.155 [0.018, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.098], loss: 0.003458, mae: 0.065092, mean_q: 0.345510
 37200/100000: episode: 372, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.458, mean reward: 0.135 [0.016, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.434, 10.112], loss: 0.003328, mae: 0.063850, mean_q: 0.345912
 37300/100000: episode: 373, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 18.675, mean reward: 0.187 [0.008, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.098], loss: 0.003400, mae: 0.064837, mean_q: 0.340497
 37400/100000: episode: 374, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 18.430, mean reward: 0.184 [0.013, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.922, 10.257], loss: 0.003432, mae: 0.064876, mean_q: 0.344410
 37500/100000: episode: 375, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 17.902, mean reward: 0.179 [0.018, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.079, 10.184], loss: 0.003528, mae: 0.066329, mean_q: 0.349182
 37600/100000: episode: 376, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.867, mean reward: 0.139 [0.008, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.692, 10.197], loss: 0.003426, mae: 0.064755, mean_q: 0.344115
 37700/100000: episode: 377, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 16.002, mean reward: 0.160 [0.013, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.224, 10.126], loss: 0.003398, mae: 0.064331, mean_q: 0.345823
 37800/100000: episode: 378, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 16.794, mean reward: 0.168 [0.027, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.075, 10.098], loss: 0.003402, mae: 0.065397, mean_q: 0.343703
 37900/100000: episode: 379, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.611, mean reward: 0.146 [0.008, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.530, 10.219], loss: 0.003368, mae: 0.064265, mean_q: 0.341461
 38000/100000: episode: 380, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.556, mean reward: 0.166 [0.011, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.112], loss: 0.003568, mae: 0.065880, mean_q: 0.346063
 38100/100000: episode: 381, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 19.834, mean reward: 0.198 [0.041, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.233], loss: 0.003436, mae: 0.065063, mean_q: 0.345185
 38200/100000: episode: 382, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.219, mean reward: 0.172 [0.016, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.439, 10.195], loss: 0.003553, mae: 0.066456, mean_q: 0.347964
 38300/100000: episode: 383, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 12.713, mean reward: 0.127 [0.004, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.645, 10.171], loss: 0.003593, mae: 0.066367, mean_q: 0.343266
 38400/100000: episode: 384, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 15.559, mean reward: 0.156 [0.024, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.808, 10.141], loss: 0.003654, mae: 0.066948, mean_q: 0.343655
 38500/100000: episode: 385, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 22.394, mean reward: 0.224 [0.030, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.567, 10.098], loss: 0.003568, mae: 0.065713, mean_q: 0.340098
 38600/100000: episode: 386, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 14.313, mean reward: 0.143 [0.014, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.002, 10.339], loss: 0.003481, mae: 0.065638, mean_q: 0.345609
 38700/100000: episode: 387, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 18.307, mean reward: 0.183 [0.005, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.193, 10.098], loss: 0.003603, mae: 0.066741, mean_q: 0.344169
 38800/100000: episode: 388, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 25.361, mean reward: 0.254 [0.031, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.493, 10.098], loss: 0.003636, mae: 0.066854, mean_q: 0.339395
 38900/100000: episode: 389, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 21.945, mean reward: 0.219 [0.037, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.719, 10.278], loss: 0.003628, mae: 0.066819, mean_q: 0.342218
 39000/100000: episode: 390, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 16.083, mean reward: 0.161 [0.004, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.151, 10.098], loss: 0.003658, mae: 0.066783, mean_q: 0.344667
 39100/100000: episode: 391, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.201, mean reward: 0.162 [0.009, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.174, 10.173], loss: 0.003587, mae: 0.067072, mean_q: 0.346123
 39200/100000: episode: 392, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 13.939, mean reward: 0.139 [0.008, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.985, 10.198], loss: 0.003422, mae: 0.065095, mean_q: 0.343772
 39300/100000: episode: 393, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.702, mean reward: 0.157 [0.019, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.981, 10.098], loss: 0.003656, mae: 0.066815, mean_q: 0.344697
 39400/100000: episode: 394, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 18.808, mean reward: 0.188 [0.018, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.009, 10.098], loss: 0.003705, mae: 0.067458, mean_q: 0.343528
 39500/100000: episode: 395, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 16.680, mean reward: 0.167 [0.012, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.461, 10.448], loss: 0.003546, mae: 0.065574, mean_q: 0.341513
 39600/100000: episode: 396, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.573, mean reward: 0.146 [0.012, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.047, 10.098], loss: 0.003643, mae: 0.067258, mean_q: 0.339174
 39700/100000: episode: 397, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 15.377, mean reward: 0.154 [0.019, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.241, 10.098], loss: 0.003356, mae: 0.064678, mean_q: 0.338532
 39800/100000: episode: 398, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 18.282, mean reward: 0.183 [0.015, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.212], loss: 0.003738, mae: 0.068688, mean_q: 0.342892
 39900/100000: episode: 399, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 16.714, mean reward: 0.167 [0.025, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.128], loss: 0.003663, mae: 0.067106, mean_q: 0.340418
 40000/100000: episode: 400, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 13.545, mean reward: 0.135 [0.019, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.188, 10.098], loss: 0.003670, mae: 0.067755, mean_q: 0.339359
 40100/100000: episode: 401, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 12.557, mean reward: 0.126 [0.004, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.437, 10.129], loss: 0.003811, mae: 0.069115, mean_q: 0.338147
 40200/100000: episode: 402, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 15.484, mean reward: 0.155 [0.013, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.783, 10.098], loss: 0.003526, mae: 0.066167, mean_q: 0.334765
 40300/100000: episode: 403, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.778, mean reward: 0.158 [0.010, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.093, 10.098], loss: 0.003550, mae: 0.066211, mean_q: 0.332272
 40400/100000: episode: 404, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.737, mean reward: 0.167 [0.019, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.711, 10.098], loss: 0.003419, mae: 0.065131, mean_q: 0.337393
 40500/100000: episode: 405, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 17.016, mean reward: 0.170 [0.018, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.875, 10.098], loss: 0.003629, mae: 0.066795, mean_q: 0.336936
 40600/100000: episode: 406, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.823, mean reward: 0.168 [0.027, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.705, 10.098], loss: 0.003804, mae: 0.068763, mean_q: 0.333642
 40700/100000: episode: 407, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.946, mean reward: 0.169 [0.021, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.597, 10.180], loss: 0.003810, mae: 0.069087, mean_q: 0.336264
 40800/100000: episode: 408, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 20.179, mean reward: 0.202 [0.010, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.292, 10.194], loss: 0.003774, mae: 0.069419, mean_q: 0.335073
 40900/100000: episode: 409, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.336, mean reward: 0.163 [0.029, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.931, 10.098], loss: 0.003818, mae: 0.069352, mean_q: 0.330952
 41000/100000: episode: 410, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 13.304, mean reward: 0.133 [0.016, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.032, 10.098], loss: 0.003721, mae: 0.068500, mean_q: 0.329711
 41100/100000: episode: 411, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 17.689, mean reward: 0.177 [0.028, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.098], loss: 0.003874, mae: 0.069413, mean_q: 0.330328
 41200/100000: episode: 412, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 15.209, mean reward: 0.152 [0.006, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.098], loss: 0.003621, mae: 0.067194, mean_q: 0.330772
 41300/100000: episode: 413, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 31.473, mean reward: 0.315 [0.028, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.533, 10.511], loss: 0.003882, mae: 0.069558, mean_q: 0.330446
 41400/100000: episode: 414, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 19.232, mean reward: 0.192 [0.039, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.268, 10.098], loss: 0.003548, mae: 0.067531, mean_q: 0.331281
 41500/100000: episode: 415, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 13.316, mean reward: 0.133 [0.017, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.276, 10.177], loss: 0.003709, mae: 0.068141, mean_q: 0.334649
 41600/100000: episode: 416, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 13.367, mean reward: 0.134 [0.014, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.683, 10.109], loss: 0.003804, mae: 0.069533, mean_q: 0.334448
 41700/100000: episode: 417, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 21.722, mean reward: 0.217 [0.029, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.288, 10.215], loss: 0.003747, mae: 0.069075, mean_q: 0.329264
 41800/100000: episode: 418, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 15.991, mean reward: 0.160 [0.007, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.665, 10.128], loss: 0.003792, mae: 0.068809, mean_q: 0.329523
 41900/100000: episode: 419, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 13.220, mean reward: 0.132 [0.009, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.377, 10.098], loss: 0.003615, mae: 0.066917, mean_q: 0.333346
 42000/100000: episode: 420, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 19.641, mean reward: 0.196 [0.026, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.647, 10.098], loss: 0.003843, mae: 0.069059, mean_q: 0.331931
 42100/100000: episode: 421, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 15.852, mean reward: 0.159 [0.018, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.758, 10.262], loss: 0.003590, mae: 0.067066, mean_q: 0.331908
 42200/100000: episode: 422, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.059, mean reward: 0.161 [0.012, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.618, 10.098], loss: 0.003645, mae: 0.068260, mean_q: 0.333524
 42300/100000: episode: 423, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 20.149, mean reward: 0.201 [0.053, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.411], loss: 0.003517, mae: 0.066705, mean_q: 0.330320
 42400/100000: episode: 424, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 14.792, mean reward: 0.148 [0.024, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.784, 10.226], loss: 0.003612, mae: 0.067271, mean_q: 0.337338
 42500/100000: episode: 425, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 14.659, mean reward: 0.147 [0.010, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.028, 10.098], loss: 0.003530, mae: 0.065617, mean_q: 0.333032
 42600/100000: episode: 426, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.096, mean reward: 0.151 [0.006, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.812, 10.197], loss: 0.003621, mae: 0.067451, mean_q: 0.329914
 42700/100000: episode: 427, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 17.981, mean reward: 0.180 [0.013, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.308], loss: 0.003676, mae: 0.068280, mean_q: 0.335152
 42800/100000: episode: 428, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 18.016, mean reward: 0.180 [0.024, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.487, 10.098], loss: 0.003549, mae: 0.066528, mean_q: 0.332008
 42900/100000: episode: 429, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 14.025, mean reward: 0.140 [0.010, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.146, 10.098], loss: 0.003552, mae: 0.067302, mean_q: 0.334083
 43000/100000: episode: 430, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 15.708, mean reward: 0.157 [0.006, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.098], loss: 0.003631, mae: 0.066682, mean_q: 0.335473
 43100/100000: episode: 431, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 16.151, mean reward: 0.162 [0.008, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.249], loss: 0.003586, mae: 0.066793, mean_q: 0.333323
 43200/100000: episode: 432, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 21.879, mean reward: 0.219 [0.035, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.094, 10.250], loss: 0.003682, mae: 0.067786, mean_q: 0.335986
 43300/100000: episode: 433, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.926, mean reward: 0.179 [0.030, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.337, 10.098], loss: 0.003777, mae: 0.068921, mean_q: 0.336662
 43400/100000: episode: 434, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 18.827, mean reward: 0.188 [0.036, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.505, 10.098], loss: 0.003570, mae: 0.066766, mean_q: 0.335891
 43500/100000: episode: 435, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 16.126, mean reward: 0.161 [0.019, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.043, 10.098], loss: 0.003804, mae: 0.068921, mean_q: 0.337380
 43600/100000: episode: 436, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 12.679, mean reward: 0.127 [0.013, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.939, 10.166], loss: 0.003751, mae: 0.068734, mean_q: 0.332121
 43700/100000: episode: 437, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 18.085, mean reward: 0.181 [0.023, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.545, 10.131], loss: 0.003615, mae: 0.067046, mean_q: 0.327850
 43800/100000: episode: 438, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 14.132, mean reward: 0.141 [0.010, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.608, 10.098], loss: 0.003638, mae: 0.067420, mean_q: 0.327917
 43900/100000: episode: 439, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 13.926, mean reward: 0.139 [0.019, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.304, 10.170], loss: 0.003635, mae: 0.066940, mean_q: 0.327056
 44000/100000: episode: 440, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.528, mean reward: 0.185 [0.043, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.557, 10.308], loss: 0.003501, mae: 0.066501, mean_q: 0.324631
 44100/100000: episode: 441, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.909, mean reward: 0.169 [0.019, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.832, 10.098], loss: 0.003481, mae: 0.066070, mean_q: 0.324055
 44200/100000: episode: 442, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 14.085, mean reward: 0.141 [0.019, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.323, 10.098], loss: 0.003516, mae: 0.065534, mean_q: 0.327698
 44300/100000: episode: 443, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 19.511, mean reward: 0.195 [0.019, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.785, 10.484], loss: 0.003620, mae: 0.067352, mean_q: 0.327273
 44400/100000: episode: 444, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 17.419, mean reward: 0.174 [0.028, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.826, 10.098], loss: 0.003586, mae: 0.066800, mean_q: 0.328842
 44500/100000: episode: 445, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 26.709, mean reward: 0.267 [0.030, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.662, 10.098], loss: 0.003557, mae: 0.067128, mean_q: 0.327798
 44600/100000: episode: 446, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 18.485, mean reward: 0.185 [0.025, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.263, 10.098], loss: 0.003612, mae: 0.067454, mean_q: 0.332493
 44700/100000: episode: 447, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 18.576, mean reward: 0.186 [0.030, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.226, 10.098], loss: 0.003824, mae: 0.069792, mean_q: 0.335959
 44800/100000: episode: 448, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 15.579, mean reward: 0.156 [0.024, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.384, 10.098], loss: 0.003659, mae: 0.067594, mean_q: 0.335710
 44900/100000: episode: 449, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 14.143, mean reward: 0.141 [0.009, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.969, 10.332], loss: 0.003537, mae: 0.065986, mean_q: 0.334812
 45000/100000: episode: 450, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.693, mean reward: 0.157 [0.018, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.248, 10.098], loss: 0.003420, mae: 0.065680, mean_q: 0.332840
 45100/100000: episode: 451, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.272, mean reward: 0.153 [0.009, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.453, 10.277], loss: 0.003584, mae: 0.067047, mean_q: 0.338785
 45200/100000: episode: 452, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 13.496, mean reward: 0.135 [0.011, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.397, 10.098], loss: 0.003568, mae: 0.066768, mean_q: 0.339710
 45300/100000: episode: 453, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 21.545, mean reward: 0.215 [0.040, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.664, 10.360], loss: 0.003584, mae: 0.067022, mean_q: 0.339749
 45400/100000: episode: 454, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 19.298, mean reward: 0.193 [0.012, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.367, 10.449], loss: 0.003796, mae: 0.068238, mean_q: 0.340937
 45500/100000: episode: 455, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 14.592, mean reward: 0.146 [0.030, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.778, 10.098], loss: 0.003732, mae: 0.067789, mean_q: 0.339163
 45600/100000: episode: 456, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 16.358, mean reward: 0.164 [0.011, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.353, 10.131], loss: 0.003557, mae: 0.066602, mean_q: 0.338036
 45700/100000: episode: 457, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 19.717, mean reward: 0.197 [0.021, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.088, 10.298], loss: 0.003606, mae: 0.066458, mean_q: 0.333138
 45800/100000: episode: 458, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 14.089, mean reward: 0.141 [0.007, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.953, 10.098], loss: 0.003750, mae: 0.067473, mean_q: 0.338113
 45900/100000: episode: 459, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.444, mean reward: 0.164 [0.024, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.575, 10.098], loss: 0.003667, mae: 0.067452, mean_q: 0.336144
 46000/100000: episode: 460, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 15.809, mean reward: 0.158 [0.004, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.064, 10.231], loss: 0.003517, mae: 0.066584, mean_q: 0.338331
 46100/100000: episode: 461, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 14.567, mean reward: 0.146 [0.025, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.972, 10.098], loss: 0.003551, mae: 0.066244, mean_q: 0.335644
 46200/100000: episode: 462, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.021, mean reward: 0.130 [0.012, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.918, 10.098], loss: 0.003717, mae: 0.067491, mean_q: 0.331182
 46300/100000: episode: 463, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 13.937, mean reward: 0.139 [0.010, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.115, 10.119], loss: 0.003477, mae: 0.066300, mean_q: 0.329592
 46400/100000: episode: 464, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.932, mean reward: 0.149 [0.024, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.172, 10.237], loss: 0.003453, mae: 0.065603, mean_q: 0.324956
 46500/100000: episode: 465, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 20.519, mean reward: 0.205 [0.029, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.946, 10.098], loss: 0.003769, mae: 0.067881, mean_q: 0.326543
 46600/100000: episode: 466, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 16.247, mean reward: 0.162 [0.015, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.214], loss: 0.003653, mae: 0.067328, mean_q: 0.326486
 46700/100000: episode: 467, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 15.309, mean reward: 0.153 [0.010, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.355, 10.154], loss: 0.003786, mae: 0.068248, mean_q: 0.331780
 46800/100000: episode: 468, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 13.174, mean reward: 0.132 [0.018, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.098], loss: 0.003597, mae: 0.066992, mean_q: 0.325474
 46900/100000: episode: 469, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 11.509, mean reward: 0.115 [0.008, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.697, 10.115], loss: 0.003491, mae: 0.065559, mean_q: 0.327677
 47000/100000: episode: 470, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 28.561, mean reward: 0.286 [0.044, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.143, 10.098], loss: 0.003889, mae: 0.069871, mean_q: 0.328141
 47100/100000: episode: 471, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 17.314, mean reward: 0.173 [0.009, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.085, 10.160], loss: 0.003440, mae: 0.065323, mean_q: 0.332782
 47200/100000: episode: 472, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 14.335, mean reward: 0.143 [0.014, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.589, 10.098], loss: 0.003525, mae: 0.066245, mean_q: 0.331397
 47300/100000: episode: 473, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 15.166, mean reward: 0.152 [0.035, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.628, 10.129], loss: 0.003698, mae: 0.067860, mean_q: 0.335470
 47400/100000: episode: 474, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 14.743, mean reward: 0.147 [0.012, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.250], loss: 0.003436, mae: 0.065320, mean_q: 0.329610
 47500/100000: episode: 475, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 14.891, mean reward: 0.149 [0.015, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.160, 10.098], loss: 0.003642, mae: 0.067517, mean_q: 0.329072
 47600/100000: episode: 476, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.531, mean reward: 0.185 [0.010, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.827, 10.208], loss: 0.003707, mae: 0.067391, mean_q: 0.331983
 47700/100000: episode: 477, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 23.984, mean reward: 0.240 [0.040, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.251], loss: 0.003613, mae: 0.066575, mean_q: 0.326791
 47800/100000: episode: 478, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 14.801, mean reward: 0.148 [0.012, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.576, 10.098], loss: 0.003727, mae: 0.068283, mean_q: 0.329494
 47900/100000: episode: 479, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 17.876, mean reward: 0.179 [0.011, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.494, 10.098], loss: 0.003674, mae: 0.067730, mean_q: 0.330844
 48000/100000: episode: 480, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 19.049, mean reward: 0.190 [0.011, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.920, 10.098], loss: 0.003698, mae: 0.067777, mean_q: 0.325884
 48100/100000: episode: 481, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 16.889, mean reward: 0.169 [0.020, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.098], loss: 0.003620, mae: 0.067689, mean_q: 0.331875
 48200/100000: episode: 482, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 15.593, mean reward: 0.156 [0.020, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.449, 10.345], loss: 0.003481, mae: 0.066057, mean_q: 0.328779
 48300/100000: episode: 483, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 18.123, mean reward: 0.181 [0.036, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.412, 10.098], loss: 0.003590, mae: 0.067305, mean_q: 0.332486
 48400/100000: episode: 484, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 15.270, mean reward: 0.153 [0.027, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.990, 10.256], loss: 0.003893, mae: 0.069623, mean_q: 0.330575
 48500/100000: episode: 485, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 17.343, mean reward: 0.173 [0.014, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.437, 10.138], loss: 0.003615, mae: 0.067852, mean_q: 0.329600
 48600/100000: episode: 486, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 17.424, mean reward: 0.174 [0.023, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.379, 10.098], loss: 0.003900, mae: 0.069851, mean_q: 0.326869
 48700/100000: episode: 487, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 14.473, mean reward: 0.145 [0.006, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.098], loss: 0.003298, mae: 0.063549, mean_q: 0.328045
 48800/100000: episode: 488, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 16.833, mean reward: 0.168 [0.009, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.223, 10.098], loss: 0.003467, mae: 0.065536, mean_q: 0.327686
 48900/100000: episode: 489, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 20.280, mean reward: 0.203 [0.014, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.098], loss: 0.003639, mae: 0.067491, mean_q: 0.330441
 49000/100000: episode: 490, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.716, mean reward: 0.157 [0.005, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.156, 10.098], loss: 0.003583, mae: 0.067238, mean_q: 0.332846
 49100/100000: episode: 491, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 18.453, mean reward: 0.185 [0.017, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.279, 10.098], loss: 0.003710, mae: 0.066958, mean_q: 0.334453
 49200/100000: episode: 492, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 17.587, mean reward: 0.176 [0.007, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.208, 10.121], loss: 0.003781, mae: 0.068413, mean_q: 0.335999
 49300/100000: episode: 493, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 14.144, mean reward: 0.141 [0.003, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.902, 10.140], loss: 0.003631, mae: 0.066805, mean_q: 0.331701
 49400/100000: episode: 494, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 13.516, mean reward: 0.135 [0.007, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.639, 10.105], loss: 0.003582, mae: 0.066555, mean_q: 0.331020
 49500/100000: episode: 495, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 13.919, mean reward: 0.139 [0.007, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.051, 10.098], loss: 0.003806, mae: 0.068012, mean_q: 0.327734
 49600/100000: episode: 496, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.911, mean reward: 0.169 [0.010, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.610, 10.098], loss: 0.003545, mae: 0.065530, mean_q: 0.323763
 49700/100000: episode: 497, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 19.449, mean reward: 0.194 [0.016, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.394], loss: 0.003864, mae: 0.069889, mean_q: 0.321682
 49800/100000: episode: 498, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 14.008, mean reward: 0.140 [0.009, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.891, 10.129], loss: 0.003593, mae: 0.067204, mean_q: 0.319855
 49900/100000: episode: 499, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 12.828, mean reward: 0.128 [0.001, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.634, 10.256], loss: 0.003633, mae: 0.066412, mean_q: 0.324866
 50000/100000: episode: 500, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.561, mean reward: 0.146 [0.006, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.905, 10.183], loss: 0.003595, mae: 0.066483, mean_q: 0.325767
 50100/100000: episode: 501, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 14.399, mean reward: 0.144 [0.005, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.599, 10.098], loss: 0.003576, mae: 0.066533, mean_q: 0.321300
 50200/100000: episode: 502, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 14.578, mean reward: 0.146 [0.010, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.965, 10.216], loss: 0.003668, mae: 0.067157, mean_q: 0.326315
 50300/100000: episode: 503, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.120, mean reward: 0.161 [0.016, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.133, 10.098], loss: 0.003385, mae: 0.065144, mean_q: 0.319720
 50400/100000: episode: 504, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 19.476, mean reward: 0.195 [0.022, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.659, 10.098], loss: 0.003552, mae: 0.066627, mean_q: 0.319069
 50500/100000: episode: 505, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 22.246, mean reward: 0.222 [0.032, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.035, 10.098], loss: 0.003769, mae: 0.068196, mean_q: 0.320382
 50600/100000: episode: 506, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 16.546, mean reward: 0.165 [0.005, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.121, 10.158], loss: 0.003444, mae: 0.064597, mean_q: 0.321954
 50700/100000: episode: 507, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 12.048, mean reward: 0.120 [0.009, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.661, 10.098], loss: 0.003915, mae: 0.069322, mean_q: 0.321020
 50800/100000: episode: 508, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 18.620, mean reward: 0.186 [0.041, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.326, 10.216], loss: 0.003718, mae: 0.067861, mean_q: 0.321257
 50900/100000: episode: 509, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 17.753, mean reward: 0.178 [0.037, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.813, 10.098], loss: 0.003660, mae: 0.067303, mean_q: 0.321632
 51000/100000: episode: 510, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 17.915, mean reward: 0.179 [0.015, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.213, 10.098], loss: 0.004070, mae: 0.069700, mean_q: 0.323030
 51100/100000: episode: 511, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 18.965, mean reward: 0.190 [0.025, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.622, 10.098], loss: 0.003569, mae: 0.066274, mean_q: 0.327879
 51200/100000: episode: 512, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 14.233, mean reward: 0.142 [0.018, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.630, 10.102], loss: 0.003628, mae: 0.067272, mean_q: 0.326484
 51300/100000: episode: 513, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 11.629, mean reward: 0.116 [0.009, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.098], loss: 0.003599, mae: 0.066447, mean_q: 0.326089
 51400/100000: episode: 514, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 16.873, mean reward: 0.169 [0.025, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.631, 10.098], loss: 0.003795, mae: 0.068480, mean_q: 0.324470
 51500/100000: episode: 515, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 19.490, mean reward: 0.195 [0.012, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.039, 10.137], loss: 0.003649, mae: 0.066916, mean_q: 0.323729
 51600/100000: episode: 516, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 23.762, mean reward: 0.238 [0.023, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.144, 10.508], loss: 0.003709, mae: 0.067698, mean_q: 0.327718
 51700/100000: episode: 517, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 16.352, mean reward: 0.164 [0.018, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.358, 10.132], loss: 0.003640, mae: 0.066531, mean_q: 0.324537
 51800/100000: episode: 518, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 19.313, mean reward: 0.193 [0.034, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.236], loss: 0.003975, mae: 0.069605, mean_q: 0.329096
 51900/100000: episode: 519, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 21.136, mean reward: 0.211 [0.039, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.047, 10.098], loss: 0.003926, mae: 0.069255, mean_q: 0.332493
 52000/100000: episode: 520, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 18.145, mean reward: 0.181 [0.015, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.335, 10.098], loss: 0.003761, mae: 0.067990, mean_q: 0.330029
 52100/100000: episode: 521, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 17.768, mean reward: 0.178 [0.020, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.528, 10.098], loss: 0.003897, mae: 0.068578, mean_q: 0.326313
 52200/100000: episode: 522, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 17.872, mean reward: 0.179 [0.016, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.812, 10.098], loss: 0.003654, mae: 0.067295, mean_q: 0.330203
 52300/100000: episode: 523, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 14.003, mean reward: 0.140 [0.021, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.609, 10.098], loss: 0.003935, mae: 0.070243, mean_q: 0.335308
 52400/100000: episode: 524, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 19.608, mean reward: 0.196 [0.013, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.132, 10.344], loss: 0.003915, mae: 0.069072, mean_q: 0.334964
 52500/100000: episode: 525, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 16.873, mean reward: 0.169 [0.023, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.098], loss: 0.004014, mae: 0.069806, mean_q: 0.335983
 52600/100000: episode: 526, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 15.529, mean reward: 0.155 [0.018, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.902, 10.116], loss: 0.003442, mae: 0.064851, mean_q: 0.332090
 52700/100000: episode: 527, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 15.555, mean reward: 0.156 [0.025, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.077, 10.098], loss: 0.003869, mae: 0.068716, mean_q: 0.330151
 52800/100000: episode: 528, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 17.148, mean reward: 0.171 [0.021, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.213], loss: 0.004259, mae: 0.072314, mean_q: 0.333682
 52900/100000: episode: 529, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 14.152, mean reward: 0.142 [0.016, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.825, 10.202], loss: 0.003772, mae: 0.067789, mean_q: 0.331990
 53000/100000: episode: 530, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 15.594, mean reward: 0.156 [0.013, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.352, 10.154], loss: 0.003741, mae: 0.068367, mean_q: 0.323555
 53100/100000: episode: 531, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 16.547, mean reward: 0.165 [0.032, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.497, 10.405], loss: 0.003695, mae: 0.067371, mean_q: 0.327411
 53200/100000: episode: 532, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 16.384, mean reward: 0.164 [0.014, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.865, 10.266], loss: 0.003679, mae: 0.066710, mean_q: 0.327486
 53300/100000: episode: 533, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 19.375, mean reward: 0.194 [0.013, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.402, 10.371], loss: 0.003789, mae: 0.067680, mean_q: 0.325691
 53400/100000: episode: 534, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 14.581, mean reward: 0.146 [0.033, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.098], loss: 0.003881, mae: 0.068820, mean_q: 0.331229
 53500/100000: episode: 535, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 15.292, mean reward: 0.153 [0.017, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.890, 10.098], loss: 0.004023, mae: 0.070467, mean_q: 0.331430
 53600/100000: episode: 536, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 15.124, mean reward: 0.151 [0.006, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.145, 10.098], loss: 0.003701, mae: 0.067201, mean_q: 0.329002
 53700/100000: episode: 537, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 18.033, mean reward: 0.180 [0.019, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.653, 10.125], loss: 0.003881, mae: 0.068304, mean_q: 0.329643
 53800/100000: episode: 538, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 14.396, mean reward: 0.144 [0.023, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.264, 10.098], loss: 0.003750, mae: 0.068430, mean_q: 0.329755
 53900/100000: episode: 539, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 15.512, mean reward: 0.155 [0.012, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.119, 10.098], loss: 0.003975, mae: 0.069736, mean_q: 0.325351
 54000/100000: episode: 540, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 13.219, mean reward: 0.132 [0.011, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.794, 10.113], loss: 0.003692, mae: 0.067283, mean_q: 0.324983
 54100/100000: episode: 541, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 15.938, mean reward: 0.159 [0.022, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.340, 10.098], loss: 0.003820, mae: 0.068405, mean_q: 0.322756
 54200/100000: episode: 542, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 15.909, mean reward: 0.159 [0.008, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.627, 10.098], loss: 0.004041, mae: 0.069554, mean_q: 0.325408
 54300/100000: episode: 543, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 13.904, mean reward: 0.139 [0.008, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.996, 10.098], loss: 0.003858, mae: 0.067650, mean_q: 0.320286
 54400/100000: episode: 544, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 14.263, mean reward: 0.143 [0.020, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.559, 10.098], loss: 0.003776, mae: 0.068830, mean_q: 0.318828
 54500/100000: episode: 545, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 15.335, mean reward: 0.153 [0.018, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.619, 10.172], loss: 0.003754, mae: 0.068314, mean_q: 0.324584
 54600/100000: episode: 546, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 15.467, mean reward: 0.155 [0.017, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.085, 10.305], loss: 0.003601, mae: 0.066329, mean_q: 0.325256
 54700/100000: episode: 547, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 16.949, mean reward: 0.169 [0.005, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.812, 10.103], loss: 0.003765, mae: 0.068757, mean_q: 0.323128
 54800/100000: episode: 548, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 14.008, mean reward: 0.140 [0.014, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.710, 10.098], loss: 0.003694, mae: 0.067448, mean_q: 0.319103
 54900/100000: episode: 549, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 13.114, mean reward: 0.131 [0.022, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.155, 10.098], loss: 0.003927, mae: 0.069403, mean_q: 0.322263
 55000/100000: episode: 550, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 15.738, mean reward: 0.157 [0.033, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.523, 10.208], loss: 0.003794, mae: 0.068676, mean_q: 0.322350
 55100/100000: episode: 551, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 17.285, mean reward: 0.173 [0.010, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.139], loss: 0.004001, mae: 0.070158, mean_q: 0.324866
 55200/100000: episode: 552, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 24.706, mean reward: 0.247 [0.046, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.878, 10.376], loss: 0.003838, mae: 0.068737, mean_q: 0.326272
 55300/100000: episode: 553, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 15.839, mean reward: 0.158 [0.026, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.098], loss: 0.003988, mae: 0.069812, mean_q: 0.332493
 55400/100000: episode: 554, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 14.061, mean reward: 0.141 [0.007, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.898, 10.098], loss: 0.003972, mae: 0.069750, mean_q: 0.328081
 55500/100000: episode: 555, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 13.318, mean reward: 0.133 [0.009, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.458, 10.098], loss: 0.003951, mae: 0.070486, mean_q: 0.328030
 55600/100000: episode: 556, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 14.324, mean reward: 0.143 [0.016, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.621, 10.098], loss: 0.003610, mae: 0.067416, mean_q: 0.323221
 55700/100000: episode: 557, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 17.724, mean reward: 0.177 [0.009, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.166], loss: 0.003642, mae: 0.067214, mean_q: 0.326723
 55800/100000: episode: 558, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: 14.896, mean reward: 0.149 [0.003, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.531, 10.098], loss: 0.003943, mae: 0.070050, mean_q: 0.319765
 55900/100000: episode: 559, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 22.242, mean reward: 0.222 [0.045, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.046, 10.098], loss: 0.004057, mae: 0.070295, mean_q: 0.323833
 56000/100000: episode: 560, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 20.872, mean reward: 0.209 [0.016, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.357, 10.406], loss: 0.003820, mae: 0.068490, mean_q: 0.321329
 56100/100000: episode: 561, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 17.443, mean reward: 0.174 [0.025, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.023, 10.253], loss: 0.003772, mae: 0.067308, mean_q: 0.323825
 56200/100000: episode: 562, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.582, mean reward: 0.156 [0.018, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.591, 10.098], loss: 0.003935, mae: 0.068834, mean_q: 0.323264
 56300/100000: episode: 563, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 19.377, mean reward: 0.194 [0.013, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.496, 10.332], loss: 0.003761, mae: 0.068170, mean_q: 0.325714
 56400/100000: episode: 564, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 18.223, mean reward: 0.182 [0.007, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.695, 10.337], loss: 0.004066, mae: 0.070078, mean_q: 0.327752
 56500/100000: episode: 565, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 14.399, mean reward: 0.144 [0.001, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.203], loss: 0.003990, mae: 0.069654, mean_q: 0.323491
 56600/100000: episode: 566, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 15.570, mean reward: 0.156 [0.010, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.269], loss: 0.004050, mae: 0.069802, mean_q: 0.326540
 56700/100000: episode: 567, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 17.369, mean reward: 0.174 [0.013, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.137, 10.116], loss: 0.003974, mae: 0.069853, mean_q: 0.325967
 56800/100000: episode: 568, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 14.004, mean reward: 0.140 [0.021, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.565, 10.098], loss: 0.004168, mae: 0.070620, mean_q: 0.322298
 56900/100000: episode: 569, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 17.509, mean reward: 0.175 [0.033, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.381, 10.248], loss: 0.004313, mae: 0.072109, mean_q: 0.325595
 57000/100000: episode: 570, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 14.937, mean reward: 0.149 [0.011, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.591, 10.272], loss: 0.003937, mae: 0.068720, mean_q: 0.322233
 57100/100000: episode: 571, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 13.532, mean reward: 0.135 [0.008, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.176], loss: 0.004001, mae: 0.069472, mean_q: 0.319152
 57200/100000: episode: 572, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 15.285, mean reward: 0.153 [0.009, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.752, 10.255], loss: 0.003992, mae: 0.069962, mean_q: 0.319985
 57300/100000: episode: 573, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 17.153, mean reward: 0.172 [0.026, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.674, 10.098], loss: 0.003857, mae: 0.069005, mean_q: 0.315641
 57400/100000: episode: 574, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 16.902, mean reward: 0.169 [0.046, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.202, 10.098], loss: 0.003938, mae: 0.068565, mean_q: 0.318010
 57500/100000: episode: 575, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 18.094, mean reward: 0.181 [0.007, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.035, 10.098], loss: 0.003752, mae: 0.068132, mean_q: 0.313719
 57600/100000: episode: 576, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 14.440, mean reward: 0.144 [0.011, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.522, 10.098], loss: 0.004259, mae: 0.071481, mean_q: 0.318336
 57700/100000: episode: 577, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 16.369, mean reward: 0.164 [0.011, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.098], loss: 0.003974, mae: 0.068905, mean_q: 0.318335
 57800/100000: episode: 578, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 15.897, mean reward: 0.159 [0.012, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.447, 10.098], loss: 0.003845, mae: 0.068534, mean_q: 0.317117
 57900/100000: episode: 579, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.643, mean reward: 0.146 [0.003, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.305, 10.250], loss: 0.003800, mae: 0.067687, mean_q: 0.319041
 58000/100000: episode: 580, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 16.583, mean reward: 0.166 [0.027, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.868, 10.098], loss: 0.003929, mae: 0.068379, mean_q: 0.316300
 58100/100000: episode: 581, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 20.134, mean reward: 0.201 [0.007, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.390, 10.245], loss: 0.003918, mae: 0.068751, mean_q: 0.318278
 58200/100000: episode: 582, duration: 0.656s, episode steps: 100, steps per second: 153, episode reward: 17.670, mean reward: 0.177 [0.042, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.231, 10.098], loss: 0.004025, mae: 0.070314, mean_q: 0.317061
 58300/100000: episode: 583, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 13.762, mean reward: 0.138 [0.011, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.913, 10.098], loss: 0.003894, mae: 0.069210, mean_q: 0.313670
 58400/100000: episode: 584, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 14.715, mean reward: 0.147 [0.004, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.642, 10.124], loss: 0.003831, mae: 0.068359, mean_q: 0.311517
 58500/100000: episode: 585, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 15.479, mean reward: 0.155 [0.024, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.248, 10.263], loss: 0.004107, mae: 0.071092, mean_q: 0.318344
 58600/100000: episode: 586, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 16.588, mean reward: 0.166 [0.017, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.098], loss: 0.003857, mae: 0.068083, mean_q: 0.316305
 58700/100000: episode: 587, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 15.956, mean reward: 0.160 [0.010, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.446, 10.118], loss: 0.003784, mae: 0.068073, mean_q: 0.317635
 58800/100000: episode: 588, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 20.851, mean reward: 0.209 [0.012, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.482, 10.098], loss: 0.004114, mae: 0.070475, mean_q: 0.318085
 58900/100000: episode: 589, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 16.780, mean reward: 0.168 [0.004, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.937, 10.216], loss: 0.003848, mae: 0.069006, mean_q: 0.314586
 59000/100000: episode: 590, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 16.252, mean reward: 0.163 [0.022, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.386, 10.098], loss: 0.003936, mae: 0.069177, mean_q: 0.316604
 59100/100000: episode: 591, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 17.553, mean reward: 0.176 [0.014, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.312, 10.193], loss: 0.004046, mae: 0.071019, mean_q: 0.324018
 59200/100000: episode: 592, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 23.788, mean reward: 0.238 [0.014, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.272, 10.098], loss: 0.003739, mae: 0.067528, mean_q: 0.321904
 59300/100000: episode: 593, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 15.680, mean reward: 0.157 [0.005, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.744, 10.098], loss: 0.003803, mae: 0.069025, mean_q: 0.321042
 59400/100000: episode: 594, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 18.640, mean reward: 0.186 [0.027, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.401, 10.098], loss: 0.003859, mae: 0.068339, mean_q: 0.325290
 59500/100000: episode: 595, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 15.430, mean reward: 0.154 [0.013, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.556, 10.312], loss: 0.003986, mae: 0.070512, mean_q: 0.328235
 59600/100000: episode: 596, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 14.645, mean reward: 0.146 [0.023, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.188, 10.098], loss: 0.003966, mae: 0.069288, mean_q: 0.327463
 59700/100000: episode: 597, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 18.698, mean reward: 0.187 [0.038, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.415, 10.098], loss: 0.003986, mae: 0.069933, mean_q: 0.326698
 59800/100000: episode: 598, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 16.080, mean reward: 0.161 [0.022, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.424, 10.098], loss: 0.003914, mae: 0.069302, mean_q: 0.327673
 59900/100000: episode: 599, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 15.348, mean reward: 0.153 [0.017, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.098], loss: 0.003975, mae: 0.069439, mean_q: 0.330336
 60000/100000: episode: 600, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 20.868, mean reward: 0.209 [0.032, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.114], loss: 0.003986, mae: 0.069214, mean_q: 0.330777
 60100/100000: episode: 601, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 19.414, mean reward: 0.194 [0.021, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.188, 10.472], loss: 0.004055, mae: 0.069888, mean_q: 0.333095
 60200/100000: episode: 602, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 13.934, mean reward: 0.139 [0.010, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.320, 10.098], loss: 0.003954, mae: 0.068578, mean_q: 0.329733
 60300/100000: episode: 603, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 12.841, mean reward: 0.128 [0.013, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.110, 10.187], loss: 0.004192, mae: 0.070516, mean_q: 0.326195
 60400/100000: episode: 604, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 16.592, mean reward: 0.166 [0.016, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.562, 10.098], loss: 0.003615, mae: 0.066152, mean_q: 0.327608
 60500/100000: episode: 605, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 19.495, mean reward: 0.195 [0.009, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.990, 10.098], loss: 0.003936, mae: 0.068990, mean_q: 0.329466
 60600/100000: episode: 606, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 13.132, mean reward: 0.131 [0.014, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.044, 10.200], loss: 0.004176, mae: 0.070660, mean_q: 0.329565
 60700/100000: episode: 607, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 19.346, mean reward: 0.193 [0.005, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.016, 10.278], loss: 0.004101, mae: 0.069238, mean_q: 0.327462
 60800/100000: episode: 608, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 17.550, mean reward: 0.175 [0.007, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.226, 10.098], loss: 0.003835, mae: 0.068364, mean_q: 0.328607
 60900/100000: episode: 609, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 15.090, mean reward: 0.151 [0.011, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.098], loss: 0.004033, mae: 0.069772, mean_q: 0.327765
 61000/100000: episode: 610, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 18.669, mean reward: 0.187 [0.026, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.102, 10.181], loss: 0.004200, mae: 0.070718, mean_q: 0.326396
 61100/100000: episode: 611, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 24.375, mean reward: 0.244 [0.027, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.306, 10.416], loss: 0.003874, mae: 0.069296, mean_q: 0.327484
 61200/100000: episode: 612, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 16.163, mean reward: 0.162 [0.013, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.197, 10.164], loss: 0.003863, mae: 0.068260, mean_q: 0.329567
 61300/100000: episode: 613, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 15.098, mean reward: 0.151 [0.021, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.641, 10.252], loss: 0.003982, mae: 0.069650, mean_q: 0.325936
 61400/100000: episode: 614, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 22.311, mean reward: 0.223 [0.031, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.301, 10.098], loss: 0.003685, mae: 0.066875, mean_q: 0.332152
 61500/100000: episode: 615, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 24.164, mean reward: 0.242 [0.024, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.929, 10.098], loss: 0.003805, mae: 0.067455, mean_q: 0.330756
 61600/100000: episode: 616, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 19.900, mean reward: 0.199 [0.020, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.497, 10.428], loss: 0.003807, mae: 0.067269, mean_q: 0.338220
 61700/100000: episode: 617, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 16.245, mean reward: 0.162 [0.024, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.597, 10.098], loss: 0.003773, mae: 0.067355, mean_q: 0.340456
 61800/100000: episode: 618, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 18.008, mean reward: 0.180 [0.020, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.541, 10.099], loss: 0.003871, mae: 0.067812, mean_q: 0.336441
 61900/100000: episode: 619, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 17.810, mean reward: 0.178 [0.026, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.296], loss: 0.003766, mae: 0.067656, mean_q: 0.337081
 62000/100000: episode: 620, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 15.192, mean reward: 0.152 [0.002, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.484, 10.114], loss: 0.003815, mae: 0.068402, mean_q: 0.336472
 62100/100000: episode: 621, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 16.338, mean reward: 0.163 [0.016, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.487, 10.154], loss: 0.003805, mae: 0.067761, mean_q: 0.339621
 62200/100000: episode: 622, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 17.518, mean reward: 0.175 [0.018, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.207, 10.098], loss: 0.003949, mae: 0.069125, mean_q: 0.339032
 62300/100000: episode: 623, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 21.481, mean reward: 0.215 [0.030, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.100, 10.407], loss: 0.003660, mae: 0.066534, mean_q: 0.337026
 62400/100000: episode: 624, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 16.752, mean reward: 0.168 [0.016, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.633, 10.218], loss: 0.003941, mae: 0.069339, mean_q: 0.341409
 62500/100000: episode: 625, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 15.616, mean reward: 0.156 [0.014, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.553, 10.100], loss: 0.003934, mae: 0.069158, mean_q: 0.342012
 62600/100000: episode: 626, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.014, mean reward: 0.190 [0.031, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.249], loss: 0.004085, mae: 0.070732, mean_q: 0.341062
 62700/100000: episode: 627, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 16.333, mean reward: 0.163 [0.017, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.888, 10.107], loss: 0.004079, mae: 0.070034, mean_q: 0.344639
 62800/100000: episode: 628, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 15.098, mean reward: 0.151 [0.029, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.943, 10.105], loss: 0.004051, mae: 0.070356, mean_q: 0.340058
 62900/100000: episode: 629, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 16.357, mean reward: 0.164 [0.019, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.098], loss: 0.003860, mae: 0.068599, mean_q: 0.343959
 63000/100000: episode: 630, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 12.826, mean reward: 0.128 [0.009, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.544, 10.184], loss: 0.004002, mae: 0.069429, mean_q: 0.345389
 63100/100000: episode: 631, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 17.864, mean reward: 0.179 [0.029, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.098], loss: 0.003918, mae: 0.068610, mean_q: 0.341276
 63200/100000: episode: 632, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 15.540, mean reward: 0.155 [0.011, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.466, 10.171], loss: 0.004404, mae: 0.073057, mean_q: 0.341751
 63300/100000: episode: 633, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 11.476, mean reward: 0.115 [0.003, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.236, 10.107], loss: 0.003689, mae: 0.067185, mean_q: 0.342188
 63400/100000: episode: 634, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 20.492, mean reward: 0.205 [0.007, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.459, 10.150], loss: 0.003887, mae: 0.068968, mean_q: 0.343522
 63500/100000: episode: 635, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 13.729, mean reward: 0.137 [0.008, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.800, 10.155], loss: 0.003902, mae: 0.067936, mean_q: 0.338108
 63600/100000: episode: 636, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 13.677, mean reward: 0.137 [0.029, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.780, 10.130], loss: 0.004202, mae: 0.070454, mean_q: 0.338613
 63700/100000: episode: 637, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 13.986, mean reward: 0.140 [0.006, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.749, 10.098], loss: 0.003906, mae: 0.069177, mean_q: 0.339923
 63800/100000: episode: 638, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 17.205, mean reward: 0.172 [0.018, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.890, 10.098], loss: 0.003973, mae: 0.069079, mean_q: 0.336171
 63900/100000: episode: 639, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 19.799, mean reward: 0.198 [0.028, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.260, 10.098], loss: 0.004029, mae: 0.070430, mean_q: 0.339813
 64000/100000: episode: 640, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.997, mean reward: 0.150 [0.003, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.550, 10.219], loss: 0.003989, mae: 0.069570, mean_q: 0.337041
 64100/100000: episode: 641, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 13.133, mean reward: 0.131 [0.016, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.392, 10.098], loss: 0.004025, mae: 0.069181, mean_q: 0.337174
 64200/100000: episode: 642, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 18.221, mean reward: 0.182 [0.011, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.755, 10.241], loss: 0.003941, mae: 0.068586, mean_q: 0.336672
 64300/100000: episode: 643, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 16.450, mean reward: 0.164 [0.019, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.004242, mae: 0.071179, mean_q: 0.336576
 64400/100000: episode: 644, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 18.685, mean reward: 0.187 [0.022, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.832, 10.351], loss: 0.003871, mae: 0.068447, mean_q: 0.333670
 64500/100000: episode: 645, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 14.678, mean reward: 0.147 [0.030, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.736, 10.180], loss: 0.004171, mae: 0.071217, mean_q: 0.336680
 64600/100000: episode: 646, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 14.382, mean reward: 0.144 [0.011, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.144], loss: 0.003928, mae: 0.068426, mean_q: 0.331042
 64700/100000: episode: 647, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 14.495, mean reward: 0.145 [0.028, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.852, 10.098], loss: 0.003660, mae: 0.066151, mean_q: 0.335104
 64800/100000: episode: 648, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 13.871, mean reward: 0.139 [0.018, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.380, 10.098], loss: 0.004035, mae: 0.069414, mean_q: 0.333503
 64900/100000: episode: 649, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 16.670, mean reward: 0.167 [0.001, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.647, 10.365], loss: 0.003762, mae: 0.067650, mean_q: 0.324955
 65000/100000: episode: 650, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 13.309, mean reward: 0.133 [0.007, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.891, 10.098], loss: 0.003831, mae: 0.067836, mean_q: 0.330996
 65100/100000: episode: 651, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 17.053, mean reward: 0.171 [0.017, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.179, 10.098], loss: 0.003978, mae: 0.068805, mean_q: 0.325696
 65200/100000: episode: 652, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 16.772, mean reward: 0.168 [0.021, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.169, 10.164], loss: 0.003913, mae: 0.068559, mean_q: 0.324988
 65300/100000: episode: 653, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 19.656, mean reward: 0.197 [0.010, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.335, 10.098], loss: 0.003914, mae: 0.068709, mean_q: 0.326392
 65400/100000: episode: 654, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 13.895, mean reward: 0.139 [0.025, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.563, 10.113], loss: 0.003931, mae: 0.068430, mean_q: 0.327234
 65500/100000: episode: 655, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 16.551, mean reward: 0.166 [0.017, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.132, 10.174], loss: 0.003747, mae: 0.067158, mean_q: 0.329412
 65600/100000: episode: 656, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 16.457, mean reward: 0.165 [0.034, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.003769, mae: 0.067212, mean_q: 0.325337
 65700/100000: episode: 657, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 19.186, mean reward: 0.192 [0.020, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.623, 10.129], loss: 0.003896, mae: 0.068228, mean_q: 0.330335
 65800/100000: episode: 658, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.280, mean reward: 0.153 [0.022, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.098], loss: 0.003728, mae: 0.067073, mean_q: 0.329012
 65900/100000: episode: 659, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 13.978, mean reward: 0.140 [0.006, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.031, 10.176], loss: 0.003630, mae: 0.066016, mean_q: 0.329229
 66000/100000: episode: 660, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 14.693, mean reward: 0.147 [0.009, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.043, 10.098], loss: 0.003625, mae: 0.066061, mean_q: 0.328879
 66100/100000: episode: 661, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 13.727, mean reward: 0.137 [0.029, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.739, 10.156], loss: 0.003728, mae: 0.067515, mean_q: 0.324841
 66200/100000: episode: 662, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 13.404, mean reward: 0.134 [0.019, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.712, 10.292], loss: 0.003860, mae: 0.068847, mean_q: 0.315506
 66300/100000: episode: 663, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.357, mean reward: 0.164 [0.009, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.977, 10.160], loss: 0.003543, mae: 0.065295, mean_q: 0.317981
 66400/100000: episode: 664, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 15.781, mean reward: 0.158 [0.006, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.098], loss: 0.003671, mae: 0.066248, mean_q: 0.316307
 66500/100000: episode: 665, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 13.757, mean reward: 0.138 [0.011, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.379, 10.108], loss: 0.003890, mae: 0.068050, mean_q: 0.321651
 66600/100000: episode: 666, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 20.146, mean reward: 0.201 [0.003, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.280, 10.225], loss: 0.003712, mae: 0.066639, mean_q: 0.315975
 66700/100000: episode: 667, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 13.816, mean reward: 0.138 [0.004, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.048, 10.104], loss: 0.003728, mae: 0.068203, mean_q: 0.317033
 66800/100000: episode: 668, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 12.041, mean reward: 0.120 [0.017, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.134, 10.098], loss: 0.003441, mae: 0.064843, mean_q: 0.309179
 66900/100000: episode: 669, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 12.828, mean reward: 0.128 [0.015, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.207, 10.130], loss: 0.003584, mae: 0.066548, mean_q: 0.313694
 67000/100000: episode: 670, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 15.433, mean reward: 0.154 [0.006, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.165, 10.226], loss: 0.003653, mae: 0.067228, mean_q: 0.307452
 67100/100000: episode: 671, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 17.102, mean reward: 0.171 [0.006, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.104, 10.098], loss: 0.003527, mae: 0.065644, mean_q: 0.312794
 67200/100000: episode: 672, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 17.006, mean reward: 0.170 [0.013, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.209, 10.305], loss: 0.003738, mae: 0.067619, mean_q: 0.312487
 67300/100000: episode: 673, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 18.441, mean reward: 0.184 [0.004, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.400, 10.214], loss: 0.003382, mae: 0.064282, mean_q: 0.307361
 67400/100000: episode: 674, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 21.916, mean reward: 0.219 [0.030, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.654, 10.098], loss: 0.003397, mae: 0.064388, mean_q: 0.308187
 67500/100000: episode: 675, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 17.364, mean reward: 0.174 [0.020, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.963, 10.098], loss: 0.003672, mae: 0.066612, mean_q: 0.309357
 67600/100000: episode: 676, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 15.249, mean reward: 0.152 [0.017, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.494, 10.425], loss: 0.003618, mae: 0.065717, mean_q: 0.312475
 67700/100000: episode: 677, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 15.861, mean reward: 0.159 [0.015, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.550, 10.098], loss: 0.003527, mae: 0.065637, mean_q: 0.310652
 67800/100000: episode: 678, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 14.370, mean reward: 0.144 [0.017, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.511, 10.185], loss: 0.003466, mae: 0.065817, mean_q: 0.308241
 67900/100000: episode: 679, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 19.225, mean reward: 0.192 [0.048, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.743, 10.098], loss: 0.003369, mae: 0.064311, mean_q: 0.309628
 68000/100000: episode: 680, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 13.420, mean reward: 0.134 [0.010, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.753, 10.127], loss: 0.003767, mae: 0.067263, mean_q: 0.315945
 68100/100000: episode: 681, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 15.376, mean reward: 0.154 [0.030, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.605, 10.221], loss: 0.003576, mae: 0.066225, mean_q: 0.309258
 68200/100000: episode: 682, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 17.358, mean reward: 0.174 [0.028, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.120, 10.098], loss: 0.003372, mae: 0.063514, mean_q: 0.308271
 68300/100000: episode: 683, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 26.037, mean reward: 0.260 [0.030, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.838, 10.098], loss: 0.003370, mae: 0.064688, mean_q: 0.309720
 68400/100000: episode: 684, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 17.790, mean reward: 0.178 [0.015, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.299, 10.098], loss: 0.003577, mae: 0.065642, mean_q: 0.314199
 68500/100000: episode: 685, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 16.642, mean reward: 0.166 [0.017, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.152, 10.098], loss: 0.003375, mae: 0.064295, mean_q: 0.308793
 68600/100000: episode: 686, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.064, mean reward: 0.141 [0.003, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.149, 10.100], loss: 0.003634, mae: 0.066406, mean_q: 0.314916
 68700/100000: episode: 687, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 19.636, mean reward: 0.196 [0.006, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.747, 10.098], loss: 0.003730, mae: 0.066604, mean_q: 0.317005
 68800/100000: episode: 688, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.805, mean reward: 0.158 [0.035, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.327, 10.098], loss: 0.003556, mae: 0.065648, mean_q: 0.317612
 68900/100000: episode: 689, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.448, mean reward: 0.154 [0.014, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.446, 10.131], loss: 0.003611, mae: 0.066637, mean_q: 0.317774
 69000/100000: episode: 690, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 17.767, mean reward: 0.178 [0.016, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.555, 10.098], loss: 0.003728, mae: 0.067501, mean_q: 0.319743
 69100/100000: episode: 691, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 17.828, mean reward: 0.178 [0.023, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.535, 10.127], loss: 0.003470, mae: 0.064569, mean_q: 0.322479
 69200/100000: episode: 692, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.498, mean reward: 0.165 [0.010, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.323, 10.119], loss: 0.003529, mae: 0.066340, mean_q: 0.326755
 69300/100000: episode: 693, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.295, mean reward: 0.173 [0.015, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.130, 10.098], loss: 0.003569, mae: 0.066020, mean_q: 0.323149
 69400/100000: episode: 694, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 21.279, mean reward: 0.213 [0.026, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.525, 10.282], loss: 0.003489, mae: 0.064932, mean_q: 0.322323
 69500/100000: episode: 695, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 20.324, mean reward: 0.203 [0.045, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.789, 10.131], loss: 0.003509, mae: 0.065900, mean_q: 0.325121
 69600/100000: episode: 696, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 19.093, mean reward: 0.191 [0.039, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.590, 10.098], loss: 0.003329, mae: 0.063800, mean_q: 0.324486
 69700/100000: episode: 697, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 14.808, mean reward: 0.148 [0.011, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.175, 10.098], loss: 0.003379, mae: 0.063357, mean_q: 0.327801
 69800/100000: episode: 698, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.272, mean reward: 0.163 [0.007, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.818, 10.160], loss: 0.003493, mae: 0.064403, mean_q: 0.327427
 69900/100000: episode: 699, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 16.081, mean reward: 0.161 [0.030, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.679, 10.098], loss: 0.003609, mae: 0.065900, mean_q: 0.328061
 70000/100000: episode: 700, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.262, mean reward: 0.163 [0.003, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.888, 10.098], loss: 0.003441, mae: 0.064605, mean_q: 0.329568
 70100/100000: episode: 701, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.606, mean reward: 0.146 [0.007, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.668, 10.267], loss: 0.003836, mae: 0.069079, mean_q: 0.325683
 70200/100000: episode: 702, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 13.922, mean reward: 0.139 [0.021, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.285, 10.211], loss: 0.003731, mae: 0.067437, mean_q: 0.323647
 70300/100000: episode: 703, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 15.766, mean reward: 0.158 [0.016, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.478, 10.144], loss: 0.003595, mae: 0.066577, mean_q: 0.321852
 70400/100000: episode: 704, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.227, mean reward: 0.162 [0.009, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.298, 10.098], loss: 0.003674, mae: 0.066370, mean_q: 0.323579
 70500/100000: episode: 705, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.514, mean reward: 0.135 [0.014, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.474, 10.250], loss: 0.003595, mae: 0.065571, mean_q: 0.325445
 70600/100000: episode: 706, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 17.207, mean reward: 0.172 [0.009, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.098], loss: 0.003488, mae: 0.065124, mean_q: 0.323290
 70700/100000: episode: 707, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 16.192, mean reward: 0.162 [0.012, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.150, 10.217], loss: 0.003783, mae: 0.067679, mean_q: 0.321198
 70800/100000: episode: 708, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 14.986, mean reward: 0.150 [0.022, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.137, 10.098], loss: 0.003497, mae: 0.065421, mean_q: 0.322200
 70900/100000: episode: 709, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 15.013, mean reward: 0.150 [0.004, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.911, 10.098], loss: 0.003463, mae: 0.064725, mean_q: 0.321342
 71000/100000: episode: 710, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 17.537, mean reward: 0.175 [0.005, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.370, 10.098], loss: 0.003584, mae: 0.064698, mean_q: 0.318590
 71100/100000: episode: 711, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 13.869, mean reward: 0.139 [0.003, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.368, 10.250], loss: 0.003519, mae: 0.064880, mean_q: 0.322235
 71200/100000: episode: 712, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.525, mean reward: 0.145 [0.022, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.771, 10.139], loss: 0.003660, mae: 0.065572, mean_q: 0.324604
 71300/100000: episode: 713, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 17.781, mean reward: 0.178 [0.033, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.473, 10.140], loss: 0.003710, mae: 0.067116, mean_q: 0.323802
 71400/100000: episode: 714, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.824, mean reward: 0.148 [0.023, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.130, 10.146], loss: 0.003417, mae: 0.064032, mean_q: 0.320574
 71500/100000: episode: 715, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 13.610, mean reward: 0.136 [0.010, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.170], loss: 0.003786, mae: 0.067434, mean_q: 0.324819
 71600/100000: episode: 716, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.567, mean reward: 0.146 [0.014, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.416, 10.220], loss: 0.003753, mae: 0.067571, mean_q: 0.328201
 71700/100000: episode: 717, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 16.459, mean reward: 0.165 [0.019, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.106, 10.098], loss: 0.003545, mae: 0.065475, mean_q: 0.323820
 71800/100000: episode: 718, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 22.409, mean reward: 0.224 [0.040, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.980, 10.098], loss: 0.003714, mae: 0.067049, mean_q: 0.327977
 71900/100000: episode: 719, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 19.249, mean reward: 0.192 [0.018, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.988, 10.098], loss: 0.003616, mae: 0.065627, mean_q: 0.324021
 72000/100000: episode: 720, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 13.990, mean reward: 0.140 [0.007, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.080, 10.259], loss: 0.003526, mae: 0.065229, mean_q: 0.325002
 72100/100000: episode: 721, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 14.031, mean reward: 0.140 [0.007, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.805, 10.260], loss: 0.003619, mae: 0.066808, mean_q: 0.325798
 72200/100000: episode: 722, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 14.405, mean reward: 0.144 [0.012, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.126, 10.249], loss: 0.003654, mae: 0.065861, mean_q: 0.323914
 72300/100000: episode: 723, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 17.671, mean reward: 0.177 [0.025, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.354], loss: 0.003668, mae: 0.066497, mean_q: 0.327343
 72400/100000: episode: 724, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 20.160, mean reward: 0.202 [0.028, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.720, 10.098], loss: 0.003790, mae: 0.067702, mean_q: 0.324510
 72500/100000: episode: 725, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 18.919, mean reward: 0.189 [0.017, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.615, 10.416], loss: 0.003669, mae: 0.066301, mean_q: 0.327314
 72600/100000: episode: 726, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 15.273, mean reward: 0.153 [0.009, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.625, 10.387], loss: 0.003545, mae: 0.065274, mean_q: 0.322788
 72700/100000: episode: 727, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 15.264, mean reward: 0.153 [0.004, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.075, 10.253], loss: 0.003485, mae: 0.064876, mean_q: 0.326398
 72800/100000: episode: 728, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 18.158, mean reward: 0.182 [0.025, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.908, 10.275], loss: 0.003750, mae: 0.066413, mean_q: 0.326897
 72900/100000: episode: 729, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 20.709, mean reward: 0.207 [0.030, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.593, 10.313], loss: 0.003578, mae: 0.066313, mean_q: 0.328110
 73000/100000: episode: 730, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 14.783, mean reward: 0.148 [0.016, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.983, 10.098], loss: 0.003700, mae: 0.067598, mean_q: 0.330540
 73100/100000: episode: 731, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 16.801, mean reward: 0.168 [0.013, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.420, 10.098], loss: 0.003666, mae: 0.066110, mean_q: 0.325043
 73200/100000: episode: 732, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 16.708, mean reward: 0.167 [0.019, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.481, 10.098], loss: 0.003565, mae: 0.066187, mean_q: 0.330317
 73300/100000: episode: 733, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 15.355, mean reward: 0.154 [0.005, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.181], loss: 0.003995, mae: 0.069690, mean_q: 0.330499
 73400/100000: episode: 734, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 18.027, mean reward: 0.180 [0.021, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.081, 10.111], loss: 0.003729, mae: 0.067103, mean_q: 0.328645
 73500/100000: episode: 735, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 14.339, mean reward: 0.143 [0.015, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.897, 10.141], loss: 0.003814, mae: 0.068112, mean_q: 0.325194
 73600/100000: episode: 736, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 13.470, mean reward: 0.135 [0.012, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.104], loss: 0.003517, mae: 0.065512, mean_q: 0.329073
 73700/100000: episode: 737, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 17.223, mean reward: 0.172 [0.004, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.813, 10.098], loss: 0.003771, mae: 0.067104, mean_q: 0.326880
 73800/100000: episode: 738, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 15.270, mean reward: 0.153 [0.004, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.629, 10.177], loss: 0.003994, mae: 0.069983, mean_q: 0.329317
 73900/100000: episode: 739, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 14.819, mean reward: 0.148 [0.008, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.098], loss: 0.003781, mae: 0.067798, mean_q: 0.319375
 74000/100000: episode: 740, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 16.462, mean reward: 0.165 [0.026, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.098], loss: 0.003835, mae: 0.068283, mean_q: 0.320541
 74100/100000: episode: 741, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 15.803, mean reward: 0.158 [0.013, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.879, 10.098], loss: 0.003897, mae: 0.069272, mean_q: 0.322699
 74200/100000: episode: 742, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 17.528, mean reward: 0.175 [0.035, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.958, 10.098], loss: 0.003897, mae: 0.069031, mean_q: 0.322512
 74300/100000: episode: 743, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 22.905, mean reward: 0.229 [0.010, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.100], loss: 0.003678, mae: 0.067145, mean_q: 0.320612
 74400/100000: episode: 744, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 16.326, mean reward: 0.163 [0.044, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.524, 10.098], loss: 0.003791, mae: 0.067807, mean_q: 0.322967
 74500/100000: episode: 745, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 17.435, mean reward: 0.174 [0.016, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.453, 10.247], loss: 0.003645, mae: 0.067049, mean_q: 0.321166
 74600/100000: episode: 746, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 16.378, mean reward: 0.164 [0.011, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.485, 10.098], loss: 0.003742, mae: 0.066836, mean_q: 0.318756
 74700/100000: episode: 747, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.550, mean reward: 0.185 [0.036, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.098], loss: 0.003882, mae: 0.068732, mean_q: 0.320206
 74800/100000: episode: 748, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 15.802, mean reward: 0.158 [0.031, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.438, 10.098], loss: 0.003975, mae: 0.068961, mean_q: 0.322619
 74900/100000: episode: 749, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 22.390, mean reward: 0.224 [0.017, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.098], loss: 0.003904, mae: 0.068461, mean_q: 0.324504
 75000/100000: episode: 750, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.825, mean reward: 0.148 [0.036, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.155, 10.127], loss: 0.003788, mae: 0.067680, mean_q: 0.326213
 75100/100000: episode: 751, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 16.505, mean reward: 0.165 [0.007, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.481, 10.098], loss: 0.003836, mae: 0.068344, mean_q: 0.323114
 75200/100000: episode: 752, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 26.502, mean reward: 0.265 [0.016, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.501, 10.357], loss: 0.003586, mae: 0.066092, mean_q: 0.325763
 75300/100000: episode: 753, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 17.376, mean reward: 0.174 [0.017, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.335, 10.098], loss: 0.003809, mae: 0.067627, mean_q: 0.330296
 75400/100000: episode: 754, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 16.761, mean reward: 0.168 [0.016, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.667, 10.161], loss: 0.003964, mae: 0.069742, mean_q: 0.333270
 75500/100000: episode: 755, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 18.515, mean reward: 0.185 [0.019, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.629, 10.098], loss: 0.003630, mae: 0.066145, mean_q: 0.325071
 75600/100000: episode: 756, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 15.765, mean reward: 0.158 [0.016, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.814, 10.328], loss: 0.003738, mae: 0.067902, mean_q: 0.329694
 75700/100000: episode: 757, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 19.423, mean reward: 0.194 [0.043, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.499, 10.194], loss: 0.003875, mae: 0.069184, mean_q: 0.332293
 75800/100000: episode: 758, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.176, mean reward: 0.192 [0.031, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.867, 10.098], loss: 0.003812, mae: 0.067981, mean_q: 0.333639
 75900/100000: episode: 759, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 15.828, mean reward: 0.158 [0.012, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.374, 10.300], loss: 0.003992, mae: 0.069626, mean_q: 0.334555
 76000/100000: episode: 760, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 14.002, mean reward: 0.140 [0.007, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.433, 10.098], loss: 0.003769, mae: 0.067521, mean_q: 0.332340
 76100/100000: episode: 761, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.482, mean reward: 0.165 [0.009, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.812, 10.236], loss: 0.003752, mae: 0.067586, mean_q: 0.331545
 76200/100000: episode: 762, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 20.989, mean reward: 0.210 [0.024, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.782, 10.192], loss: 0.003762, mae: 0.067497, mean_q: 0.328818
 76300/100000: episode: 763, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 15.054, mean reward: 0.151 [0.016, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.282, 10.194], loss: 0.003765, mae: 0.067302, mean_q: 0.334516
 76400/100000: episode: 764, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 12.829, mean reward: 0.128 [0.014, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.702, 10.098], loss: 0.003585, mae: 0.066349, mean_q: 0.331405
 76500/100000: episode: 765, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 22.520, mean reward: 0.225 [0.006, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.571, 10.290], loss: 0.003573, mae: 0.066807, mean_q: 0.329881
 76600/100000: episode: 766, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 21.948, mean reward: 0.219 [0.040, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.098], loss: 0.003866, mae: 0.068702, mean_q: 0.340121
 76700/100000: episode: 767, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 18.435, mean reward: 0.184 [0.014, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.030, 10.098], loss: 0.003673, mae: 0.066624, mean_q: 0.341481
 76800/100000: episode: 768, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 15.286, mean reward: 0.153 [0.022, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.403, 10.108], loss: 0.003998, mae: 0.070033, mean_q: 0.338676
 76900/100000: episode: 769, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.333, mean reward: 0.143 [0.016, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.290, 10.193], loss: 0.003955, mae: 0.069423, mean_q: 0.337158
 77000/100000: episode: 770, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 18.311, mean reward: 0.183 [0.029, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.824, 10.316], loss: 0.004086, mae: 0.070502, mean_q: 0.337023
 77100/100000: episode: 771, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 18.144, mean reward: 0.181 [0.047, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.268, 10.359], loss: 0.003968, mae: 0.069797, mean_q: 0.339570
 77200/100000: episode: 772, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 19.192, mean reward: 0.192 [0.012, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.249, 10.237], loss: 0.003983, mae: 0.069891, mean_q: 0.338086
 77300/100000: episode: 773, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.101, mean reward: 0.151 [0.012, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.096, 10.098], loss: 0.003704, mae: 0.067089, mean_q: 0.340850
 77400/100000: episode: 774, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 17.173, mean reward: 0.172 [0.007, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.098], loss: 0.004081, mae: 0.070271, mean_q: 0.342152
 77500/100000: episode: 775, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 15.701, mean reward: 0.157 [0.012, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.122], loss: 0.004021, mae: 0.070213, mean_q: 0.339923
 77600/100000: episode: 776, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.560, mean reward: 0.146 [0.015, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.077, 10.147], loss: 0.004108, mae: 0.070129, mean_q: 0.336472
 77700/100000: episode: 777, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 18.886, mean reward: 0.189 [0.012, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.239], loss: 0.004031, mae: 0.069870, mean_q: 0.340378
 77800/100000: episode: 778, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.926, mean reward: 0.159 [0.019, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.146, 10.414], loss: 0.003903, mae: 0.069424, mean_q: 0.337262
 77900/100000: episode: 779, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 17.329, mean reward: 0.173 [0.023, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.579, 10.163], loss: 0.003968, mae: 0.069023, mean_q: 0.338261
 78000/100000: episode: 780, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 18.769, mean reward: 0.188 [0.021, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.070, 10.098], loss: 0.004046, mae: 0.070376, mean_q: 0.334593
 78100/100000: episode: 781, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.272, mean reward: 0.163 [0.030, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.227, 10.292], loss: 0.003864, mae: 0.068535, mean_q: 0.337953
 78200/100000: episode: 782, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 17.658, mean reward: 0.177 [0.012, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.102], loss: 0.003919, mae: 0.069111, mean_q: 0.333347
 78300/100000: episode: 783, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.288, mean reward: 0.173 [0.033, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.304, 10.214], loss: 0.004031, mae: 0.070242, mean_q: 0.339384
 78400/100000: episode: 784, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 16.961, mean reward: 0.170 [0.010, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.263], loss: 0.003799, mae: 0.068381, mean_q: 0.335500
 78500/100000: episode: 785, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 13.746, mean reward: 0.137 [0.016, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.098], loss: 0.003744, mae: 0.067720, mean_q: 0.337196
 78600/100000: episode: 786, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 23.998, mean reward: 0.240 [0.031, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.323, 10.098], loss: 0.003616, mae: 0.066683, mean_q: 0.340877
 78700/100000: episode: 787, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 19.020, mean reward: 0.190 [0.014, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.731, 10.098], loss: 0.003729, mae: 0.067508, mean_q: 0.347026
 78800/100000: episode: 788, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 16.712, mean reward: 0.167 [0.014, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.261, 10.289], loss: 0.003743, mae: 0.066616, mean_q: 0.346864
 78900/100000: episode: 789, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.752, mean reward: 0.158 [0.022, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.004127, mae: 0.070814, mean_q: 0.349130
 79000/100000: episode: 790, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 21.122, mean reward: 0.211 [0.014, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.723, 10.407], loss: 0.003817, mae: 0.067919, mean_q: 0.345650
 79100/100000: episode: 791, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 15.353, mean reward: 0.154 [0.007, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.098], loss: 0.003687, mae: 0.066707, mean_q: 0.344090
 79200/100000: episode: 792, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.896, mean reward: 0.149 [0.026, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.098], loss: 0.003903, mae: 0.069136, mean_q: 0.348393
 79300/100000: episode: 793, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 21.549, mean reward: 0.215 [0.025, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.122, 10.462], loss: 0.003819, mae: 0.068283, mean_q: 0.342853
 79400/100000: episode: 794, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 18.931, mean reward: 0.189 [0.014, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.910, 10.098], loss: 0.003694, mae: 0.067895, mean_q: 0.343135
 79500/100000: episode: 795, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 17.704, mean reward: 0.177 [0.017, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.502, 10.367], loss: 0.003739, mae: 0.066913, mean_q: 0.347333
 79600/100000: episode: 796, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 21.073, mean reward: 0.211 [0.009, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.299], loss: 0.003593, mae: 0.066656, mean_q: 0.347745
 79700/100000: episode: 797, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: 16.549, mean reward: 0.165 [0.010, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.500, 10.361], loss: 0.003860, mae: 0.069098, mean_q: 0.347276
 79800/100000: episode: 798, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 16.919, mean reward: 0.169 [0.011, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.995, 10.123], loss: 0.003700, mae: 0.067736, mean_q: 0.347931
 79900/100000: episode: 799, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 16.985, mean reward: 0.170 [0.027, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.955, 10.416], loss: 0.003847, mae: 0.068822, mean_q: 0.347552
[RESULT] FALSIFICATION!
 79944/100000: episode: 800, duration: 0.261s, episode steps: 44, steps per second: 169, episode reward: 17.238, mean reward: 0.392 [0.020, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.921 [-1.060, 4.981], loss: 0.003758, mae: 0.068499, mean_q: 0.350046
 80044/100000: episode: 801, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 16.874, mean reward: 0.169 [0.012, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.098], loss: 0.003955, mae: 0.069853, mean_q: 0.345613
 80144/100000: episode: 802, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 17.370, mean reward: 0.174 [0.023, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.098], loss: 0.003854, mae: 0.068095, mean_q: 0.346598
 80244/100000: episode: 803, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 14.245, mean reward: 0.142 [0.028, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.409, 10.151], loss: 0.003726, mae: 0.067467, mean_q: 0.344075
 80344/100000: episode: 804, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 14.327, mean reward: 0.143 [0.005, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.085, 10.098], loss: 0.003587, mae: 0.066333, mean_q: 0.340091
 80444/100000: episode: 805, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 14.439, mean reward: 0.144 [0.007, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.674, 10.185], loss: 0.003661, mae: 0.066737, mean_q: 0.342709
 80544/100000: episode: 806, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 21.530, mean reward: 0.215 [0.021, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.246, 10.098], loss: 0.018362, mae: 0.076410, mean_q: 0.343906
 80644/100000: episode: 807, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 20.471, mean reward: 0.205 [0.024, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.449, 10.132], loss: 0.003671, mae: 0.068024, mean_q: 0.336617
 80744/100000: episode: 808, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 17.386, mean reward: 0.174 [0.013, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.517, 10.098], loss: 0.003637, mae: 0.068214, mean_q: 0.342032
 80844/100000: episode: 809, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 14.835, mean reward: 0.148 [0.013, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.816, 10.174], loss: 0.018220, mae: 0.076966, mean_q: 0.345173
 80944/100000: episode: 810, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 17.050, mean reward: 0.171 [0.004, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.690, 10.165], loss: 0.018340, mae: 0.078369, mean_q: 0.340546
 81044/100000: episode: 811, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 14.843, mean reward: 0.148 [0.013, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.920, 10.110], loss: 0.003575, mae: 0.066833, mean_q: 0.337584
 81144/100000: episode: 812, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 15.181, mean reward: 0.152 [0.011, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.419, 10.098], loss: 0.018122, mae: 0.077240, mean_q: 0.339543
 81244/100000: episode: 813, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 22.993, mean reward: 0.230 [0.060, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.425, 10.314], loss: 0.003376, mae: 0.065331, mean_q: 0.337801
 81344/100000: episode: 814, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 19.242, mean reward: 0.192 [0.023, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.003392, mae: 0.065554, mean_q: 0.342214
 81444/100000: episode: 815, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 14.977, mean reward: 0.150 [0.007, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.079, 10.098], loss: 0.018118, mae: 0.076866, mean_q: 0.344465
 81544/100000: episode: 816, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 18.927, mean reward: 0.189 [0.007, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.098], loss: 0.003364, mae: 0.065366, mean_q: 0.341497
 81644/100000: episode: 817, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.874, mean reward: 0.149 [0.030, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.661, 10.098], loss: 0.003557, mae: 0.066665, mean_q: 0.340697
 81744/100000: episode: 818, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 15.156, mean reward: 0.152 [0.015, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.095, 10.285], loss: 0.003482, mae: 0.065523, mean_q: 0.341867
 81844/100000: episode: 819, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 17.285, mean reward: 0.173 [0.012, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.217], loss: 0.003443, mae: 0.065561, mean_q: 0.337714
 81944/100000: episode: 820, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 16.229, mean reward: 0.162 [0.011, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.073, 10.098], loss: 0.003647, mae: 0.067479, mean_q: 0.340801
 82044/100000: episode: 821, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 19.784, mean reward: 0.198 [0.014, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.487, 10.141], loss: 0.003734, mae: 0.068021, mean_q: 0.340702
 82144/100000: episode: 822, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 21.544, mean reward: 0.215 [0.024, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.393, 10.315], loss: 0.003580, mae: 0.066660, mean_q: 0.341419
 82244/100000: episode: 823, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 13.212, mean reward: 0.132 [0.003, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.098], loss: 0.032355, mae: 0.084601, mean_q: 0.348379
 82344/100000: episode: 824, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 17.110, mean reward: 0.171 [0.020, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.478, 10.315], loss: 0.003478, mae: 0.066506, mean_q: 0.339692
 82444/100000: episode: 825, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 23.368, mean reward: 0.234 [0.018, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.349, 10.098], loss: 0.003649, mae: 0.067265, mean_q: 0.342774
 82544/100000: episode: 826, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 14.411, mean reward: 0.144 [0.021, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.298, 10.171], loss: 0.003439, mae: 0.065720, mean_q: 0.346472
 82644/100000: episode: 827, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 14.557, mean reward: 0.146 [0.018, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.115], loss: 0.003565, mae: 0.067359, mean_q: 0.339446
 82744/100000: episode: 828, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 19.122, mean reward: 0.191 [0.011, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.279, 10.098], loss: 0.017897, mae: 0.076321, mean_q: 0.350727
 82844/100000: episode: 829, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.753, mean reward: 0.148 [0.009, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.198], loss: 0.003650, mae: 0.066114, mean_q: 0.343220
 82944/100000: episode: 830, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.429, mean reward: 0.174 [0.005, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.961, 10.165], loss: 0.017686, mae: 0.074722, mean_q: 0.345716
 83044/100000: episode: 831, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 16.240, mean reward: 0.162 [0.017, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.165, 10.098], loss: 0.017395, mae: 0.072477, mean_q: 0.343596
 83144/100000: episode: 832, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 13.016, mean reward: 0.130 [0.024, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.506, 10.109], loss: 0.031764, mae: 0.084031, mean_q: 0.346359
 83244/100000: episode: 833, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 14.204, mean reward: 0.142 [0.014, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.924, 10.123], loss: 0.003983, mae: 0.070307, mean_q: 0.340760
 83344/100000: episode: 834, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 13.962, mean reward: 0.140 [0.008, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.651, 10.098], loss: 0.017391, mae: 0.073519, mean_q: 0.338070
 83444/100000: episode: 835, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 18.116, mean reward: 0.181 [0.014, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.129, 10.098], loss: 0.044958, mae: 0.087412, mean_q: 0.348870
 83544/100000: episode: 836, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 12.798, mean reward: 0.128 [0.010, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.678, 10.217], loss: 0.003615, mae: 0.067268, mean_q: 0.340678
 83644/100000: episode: 837, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 15.868, mean reward: 0.159 [0.018, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.396, 10.098], loss: 0.003606, mae: 0.066622, mean_q: 0.339595
 83744/100000: episode: 838, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.798, mean reward: 0.168 [0.018, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.098], loss: 0.003599, mae: 0.067267, mean_q: 0.332032
 83844/100000: episode: 839, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.055, mean reward: 0.161 [0.019, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.246, 10.098], loss: 0.003551, mae: 0.066255, mean_q: 0.335868
 83944/100000: episode: 840, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.700, mean reward: 0.147 [0.011, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.918, 10.131], loss: 0.017586, mae: 0.073578, mean_q: 0.339259
 84044/100000: episode: 841, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 13.295, mean reward: 0.133 [0.011, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.581, 10.207], loss: 0.031187, mae: 0.081695, mean_q: 0.337711
 84144/100000: episode: 842, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 17.746, mean reward: 0.177 [0.032, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.406, 10.286], loss: 0.003447, mae: 0.065581, mean_q: 0.332193
 84244/100000: episode: 843, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.653, mean reward: 0.157 [0.007, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.941, 10.098], loss: 0.003308, mae: 0.064529, mean_q: 0.333638
 84344/100000: episode: 844, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.492, mean reward: 0.175 [0.026, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.507, 10.098], loss: 0.017181, mae: 0.071724, mean_q: 0.335284
 84444/100000: episode: 845, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 16.227, mean reward: 0.162 [0.021, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.544, 10.232], loss: 0.017809, mae: 0.078398, mean_q: 0.332912
 84544/100000: episode: 846, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 13.935, mean reward: 0.139 [0.015, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.984, 10.152], loss: 0.003634, mae: 0.067534, mean_q: 0.330187
 84644/100000: episode: 847, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 14.830, mean reward: 0.148 [0.013, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.268, 10.166], loss: 0.003435, mae: 0.065721, mean_q: 0.325869
 84744/100000: episode: 848, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 17.763, mean reward: 0.178 [0.018, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.022, 10.098], loss: 0.017386, mae: 0.073922, mean_q: 0.331501
 84844/100000: episode: 849, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 19.031, mean reward: 0.190 [0.014, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.829, 10.098], loss: 0.017416, mae: 0.074052, mean_q: 0.326117
 84944/100000: episode: 850, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.183, mean reward: 0.162 [0.009, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.956, 10.148], loss: 0.016745, mae: 0.072565, mean_q: 0.330645
 85044/100000: episode: 851, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.235, mean reward: 0.162 [0.011, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.437, 10.098], loss: 0.003434, mae: 0.065316, mean_q: 0.323239
 85144/100000: episode: 852, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 15.918, mean reward: 0.159 [0.022, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.818, 10.098], loss: 0.003432, mae: 0.064926, mean_q: 0.326569
 85244/100000: episode: 853, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.496, mean reward: 0.135 [0.020, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.106, 10.291], loss: 0.003380, mae: 0.064951, mean_q: 0.323180
 85344/100000: episode: 854, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 11.760, mean reward: 0.118 [0.015, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.098], loss: 0.003605, mae: 0.066709, mean_q: 0.329397
 85444/100000: episode: 855, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 12.853, mean reward: 0.129 [0.012, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.218, 10.121], loss: 0.003372, mae: 0.064035, mean_q: 0.326998
 85544/100000: episode: 856, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 17.391, mean reward: 0.174 [0.012, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.834, 10.187], loss: 0.003529, mae: 0.066405, mean_q: 0.321167
 85644/100000: episode: 857, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.460, mean reward: 0.145 [0.016, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.212], loss: 0.003439, mae: 0.065315, mean_q: 0.317430
 85744/100000: episode: 858, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 15.161, mean reward: 0.152 [0.026, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.490, 10.098], loss: 0.003330, mae: 0.064136, mean_q: 0.317245
 85844/100000: episode: 859, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 14.247, mean reward: 0.142 [0.005, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.969, 10.098], loss: 0.003546, mae: 0.065891, mean_q: 0.324142
 85944/100000: episode: 860, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 26.699, mean reward: 0.267 [0.044, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.485, 10.537], loss: 0.003625, mae: 0.067695, mean_q: 0.322231
 86044/100000: episode: 861, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 14.073, mean reward: 0.141 [0.015, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.698, 10.098], loss: 0.003542, mae: 0.066375, mean_q: 0.321960
 86144/100000: episode: 862, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 13.775, mean reward: 0.138 [0.013, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.342, 10.240], loss: 0.003859, mae: 0.069109, mean_q: 0.318343
 86244/100000: episode: 863, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 15.155, mean reward: 0.152 [0.015, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.098], loss: 0.003660, mae: 0.067459, mean_q: 0.319794
 86344/100000: episode: 864, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 16.719, mean reward: 0.167 [0.012, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.869, 10.338], loss: 0.003721, mae: 0.068398, mean_q: 0.319660
 86444/100000: episode: 865, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 16.664, mean reward: 0.167 [0.037, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.474, 10.098], loss: 0.003467, mae: 0.064801, mean_q: 0.318334
 86544/100000: episode: 866, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 14.885, mean reward: 0.149 [0.013, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.343, 10.307], loss: 0.003755, mae: 0.068663, mean_q: 0.318851
 86644/100000: episode: 867, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 19.214, mean reward: 0.192 [0.019, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-2.385, 10.098], loss: 0.003400, mae: 0.064995, mean_q: 0.320510
 86744/100000: episode: 868, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 15.331, mean reward: 0.153 [0.021, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.617, 10.098], loss: 0.003478, mae: 0.065493, mean_q: 0.312737
 86844/100000: episode: 869, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 15.081, mean reward: 0.151 [0.013, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.440, 10.353], loss: 0.003740, mae: 0.067240, mean_q: 0.319298
 86944/100000: episode: 870, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 12.739, mean reward: 0.127 [0.007, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.473, 10.234], loss: 0.003476, mae: 0.065332, mean_q: 0.317764
 87044/100000: episode: 871, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.479, mean reward: 0.145 [0.009, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.632, 10.150], loss: 0.003583, mae: 0.065849, mean_q: 0.313477
 87144/100000: episode: 872, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 16.443, mean reward: 0.164 [0.020, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.888, 10.098], loss: 0.003508, mae: 0.065027, mean_q: 0.309545
 87244/100000: episode: 873, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.574, mean reward: 0.146 [0.004, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.084, 10.098], loss: 0.003614, mae: 0.066099, mean_q: 0.313544
 87344/100000: episode: 874, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 17.003, mean reward: 0.170 [0.017, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.374, 10.179], loss: 0.003600, mae: 0.066152, mean_q: 0.311685
 87444/100000: episode: 875, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.500, mean reward: 0.165 [0.022, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.095, 10.149], loss: 0.003477, mae: 0.064986, mean_q: 0.308486
 87544/100000: episode: 876, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.548, mean reward: 0.155 [0.020, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.682, 10.113], loss: 0.003388, mae: 0.065068, mean_q: 0.307232
 87644/100000: episode: 877, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.143, mean reward: 0.161 [0.016, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.683, 10.098], loss: 0.003350, mae: 0.064238, mean_q: 0.305186
 87744/100000: episode: 878, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 15.955, mean reward: 0.160 [0.009, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.244], loss: 0.003481, mae: 0.065149, mean_q: 0.306837
 87844/100000: episode: 879, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.502, mean reward: 0.165 [0.009, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.147], loss: 0.003420, mae: 0.064671, mean_q: 0.306294
 87944/100000: episode: 880, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 18.239, mean reward: 0.182 [0.005, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.133, 10.248], loss: 0.003388, mae: 0.065288, mean_q: 0.305241
 88044/100000: episode: 881, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 16.364, mean reward: 0.164 [0.018, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.100, 10.098], loss: 0.003344, mae: 0.063914, mean_q: 0.304251
 88144/100000: episode: 882, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 21.926, mean reward: 0.219 [0.027, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.007, 10.098], loss: 0.003441, mae: 0.065107, mean_q: 0.307737
 88244/100000: episode: 883, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 19.497, mean reward: 0.195 [0.022, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.270, 10.098], loss: 0.003721, mae: 0.067043, mean_q: 0.312998
 88344/100000: episode: 884, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 13.892, mean reward: 0.139 [0.017, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.474, 10.098], loss: 0.003412, mae: 0.064647, mean_q: 0.310890
 88444/100000: episode: 885, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 16.479, mean reward: 0.165 [0.006, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.706, 10.098], loss: 0.003653, mae: 0.066072, mean_q: 0.307986
 88544/100000: episode: 886, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 20.455, mean reward: 0.205 [0.011, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.559], loss: 0.003695, mae: 0.066566, mean_q: 0.314861
 88644/100000: episode: 887, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 23.144, mean reward: 0.231 [0.014, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.132, 10.098], loss: 0.003853, mae: 0.068538, mean_q: 0.317097
 88744/100000: episode: 888, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 17.339, mean reward: 0.173 [0.045, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.644, 10.098], loss: 0.003554, mae: 0.065221, mean_q: 0.320842
 88844/100000: episode: 889, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 20.484, mean reward: 0.205 [0.032, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.003678, mae: 0.066856, mean_q: 0.320905
 88944/100000: episode: 890, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 13.862, mean reward: 0.139 [0.011, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.862, 10.204], loss: 0.003637, mae: 0.066999, mean_q: 0.319309
 89044/100000: episode: 891, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 18.925, mean reward: 0.189 [0.007, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.873, 10.098], loss: 0.003578, mae: 0.065997, mean_q: 0.318606
 89144/100000: episode: 892, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 13.634, mean reward: 0.136 [0.021, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.188], loss: 0.003751, mae: 0.067274, mean_q: 0.319527
 89244/100000: episode: 893, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 16.951, mean reward: 0.170 [0.028, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.741, 10.098], loss: 0.003525, mae: 0.066290, mean_q: 0.321898
 89344/100000: episode: 894, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 18.546, mean reward: 0.185 [0.022, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.944, 10.172], loss: 0.003684, mae: 0.067168, mean_q: 0.320347
 89444/100000: episode: 895, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 14.001, mean reward: 0.140 [0.023, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.659, 10.222], loss: 0.003665, mae: 0.066980, mean_q: 0.323536
 89544/100000: episode: 896, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 19.310, mean reward: 0.193 [0.031, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.286], loss: 0.003662, mae: 0.066701, mean_q: 0.326431
 89644/100000: episode: 897, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.174, mean reward: 0.162 [0.041, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.514, 10.205], loss: 0.003811, mae: 0.067519, mean_q: 0.326763
 89744/100000: episode: 898, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 13.808, mean reward: 0.138 [0.015, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.107], loss: 0.003684, mae: 0.067354, mean_q: 0.329161
 89844/100000: episode: 899, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 17.612, mean reward: 0.176 [0.005, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.139, 10.178], loss: 0.003706, mae: 0.067647, mean_q: 0.320469
 89944/100000: episode: 900, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 20.576, mean reward: 0.206 [0.007, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.583, 10.280], loss: 0.003638, mae: 0.066258, mean_q: 0.328557
 90044/100000: episode: 901, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 18.895, mean reward: 0.189 [0.048, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.601, 10.254], loss: 0.003740, mae: 0.067360, mean_q: 0.328873
 90144/100000: episode: 902, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 21.526, mean reward: 0.215 [0.042, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.388], loss: 0.003834, mae: 0.067717, mean_q: 0.326645
 90244/100000: episode: 903, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 17.245, mean reward: 0.172 [0.014, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.202], loss: 0.003665, mae: 0.066833, mean_q: 0.331178
 90344/100000: episode: 904, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 17.606, mean reward: 0.176 [0.016, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.216, 10.282], loss: 0.003547, mae: 0.065084, mean_q: 0.330746
 90444/100000: episode: 905, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 17.645, mean reward: 0.176 [0.001, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.570, 10.098], loss: 0.003698, mae: 0.066212, mean_q: 0.336011
 90544/100000: episode: 906, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 14.805, mean reward: 0.148 [0.004, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.925, 10.098], loss: 0.003621, mae: 0.066450, mean_q: 0.334089
 90644/100000: episode: 907, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 17.692, mean reward: 0.177 [0.017, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.375], loss: 0.003935, mae: 0.068733, mean_q: 0.330129
 90744/100000: episode: 908, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.028, mean reward: 0.150 [0.024, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.311, 10.098], loss: 0.004112, mae: 0.071266, mean_q: 0.333091
 90844/100000: episode: 909, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 18.540, mean reward: 0.185 [0.024, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.588, 10.241], loss: 0.003490, mae: 0.065523, mean_q: 0.330623
 90944/100000: episode: 910, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.139, mean reward: 0.171 [0.024, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.181, 10.098], loss: 0.003747, mae: 0.066436, mean_q: 0.336321
 91044/100000: episode: 911, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 20.142, mean reward: 0.201 [0.018, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.883, 10.098], loss: 0.003811, mae: 0.067369, mean_q: 0.331380
 91144/100000: episode: 912, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 18.826, mean reward: 0.188 [0.025, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.466, 10.098], loss: 0.003841, mae: 0.067547, mean_q: 0.336960
 91244/100000: episode: 913, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 23.385, mean reward: 0.234 [0.024, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.672, 10.098], loss: 0.004307, mae: 0.071462, mean_q: 0.336232
 91344/100000: episode: 914, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.359, mean reward: 0.164 [0.029, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.993, 10.198], loss: 0.003908, mae: 0.067875, mean_q: 0.340266
 91444/100000: episode: 915, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 25.325, mean reward: 0.253 [0.025, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.519, 10.098], loss: 0.003796, mae: 0.066927, mean_q: 0.339618
 91544/100000: episode: 916, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.752, mean reward: 0.148 [0.029, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.404, 10.098], loss: 0.003824, mae: 0.067681, mean_q: 0.340728
 91644/100000: episode: 917, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 18.844, mean reward: 0.188 [0.013, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.965, 10.421], loss: 0.004031, mae: 0.069873, mean_q: 0.345891
 91744/100000: episode: 918, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 13.258, mean reward: 0.133 [0.029, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.098], loss: 0.003709, mae: 0.065833, mean_q: 0.342783
 91844/100000: episode: 919, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.532, mean reward: 0.155 [0.010, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.625, 10.143], loss: 0.003806, mae: 0.066976, mean_q: 0.343844
 91944/100000: episode: 920, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 18.578, mean reward: 0.186 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.791, 10.098], loss: 0.003840, mae: 0.067539, mean_q: 0.348107
 92044/100000: episode: 921, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.005, mean reward: 0.170 [0.019, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.265, 10.098], loss: 0.004127, mae: 0.070864, mean_q: 0.347943
 92144/100000: episode: 922, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 14.386, mean reward: 0.144 [0.013, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.501, 10.327], loss: 0.003864, mae: 0.067820, mean_q: 0.343227
 92244/100000: episode: 923, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.687, mean reward: 0.167 [0.010, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.565, 10.196], loss: 0.003953, mae: 0.068134, mean_q: 0.344554
 92344/100000: episode: 924, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 17.761, mean reward: 0.178 [0.006, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.287, 10.300], loss: 0.003877, mae: 0.068286, mean_q: 0.348517
 92444/100000: episode: 925, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.206, mean reward: 0.182 [0.021, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.675, 10.098], loss: 0.003757, mae: 0.066790, mean_q: 0.346558
 92544/100000: episode: 926, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 16.954, mean reward: 0.170 [0.021, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.635, 10.098], loss: 0.003937, mae: 0.068188, mean_q: 0.346738
 92644/100000: episode: 927, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 13.447, mean reward: 0.134 [0.021, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.661, 10.163], loss: 0.003782, mae: 0.067489, mean_q: 0.348595
 92744/100000: episode: 928, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 14.524, mean reward: 0.145 [0.024, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.098], loss: 0.003930, mae: 0.068862, mean_q: 0.347166
 92844/100000: episode: 929, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 14.682, mean reward: 0.147 [0.015, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.014, 10.098], loss: 0.004022, mae: 0.068981, mean_q: 0.341306
 92944/100000: episode: 930, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 14.244, mean reward: 0.142 [0.005, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.814, 10.255], loss: 0.003982, mae: 0.068344, mean_q: 0.345038
 93044/100000: episode: 931, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 14.788, mean reward: 0.148 [0.011, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.516, 10.098], loss: 0.003989, mae: 0.069728, mean_q: 0.347877
 93144/100000: episode: 932, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.865, mean reward: 0.149 [0.017, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.501, 10.098], loss: 0.003906, mae: 0.067631, mean_q: 0.341452
 93244/100000: episode: 933, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 15.543, mean reward: 0.155 [0.011, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.596, 10.149], loss: 0.004134, mae: 0.069672, mean_q: 0.338704
 93344/100000: episode: 934, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 14.057, mean reward: 0.141 [0.004, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.658, 10.106], loss: 0.003818, mae: 0.067676, mean_q: 0.331070
 93444/100000: episode: 935, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 15.876, mean reward: 0.159 [0.018, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.708, 10.124], loss: 0.003847, mae: 0.067506, mean_q: 0.334925
 93544/100000: episode: 936, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 15.242, mean reward: 0.152 [0.028, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.757, 10.229], loss: 0.003699, mae: 0.066450, mean_q: 0.336839
 93644/100000: episode: 937, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 17.017, mean reward: 0.170 [0.028, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.191], loss: 0.003873, mae: 0.067780, mean_q: 0.331798
 93744/100000: episode: 938, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.502, mean reward: 0.165 [0.018, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.707, 10.266], loss: 0.003797, mae: 0.067618, mean_q: 0.333006
 93844/100000: episode: 939, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 12.312, mean reward: 0.123 [0.018, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.769, 10.197], loss: 0.003947, mae: 0.068337, mean_q: 0.332849
 93944/100000: episode: 940, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 18.110, mean reward: 0.181 [0.032, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.435, 10.098], loss: 0.003768, mae: 0.067088, mean_q: 0.333014
 94044/100000: episode: 941, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 18.601, mean reward: 0.186 [0.006, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.279, 10.098], loss: 0.004008, mae: 0.069352, mean_q: 0.330165
 94144/100000: episode: 942, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.127, mean reward: 0.151 [0.008, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.955, 10.098], loss: 0.004038, mae: 0.068556, mean_q: 0.328073
 94244/100000: episode: 943, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 25.171, mean reward: 0.252 [0.040, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.664, 10.268], loss: 0.003965, mae: 0.068779, mean_q: 0.331648
 94344/100000: episode: 944, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 12.132, mean reward: 0.121 [0.004, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.108, 10.098], loss: 0.004030, mae: 0.069769, mean_q: 0.336033
 94444/100000: episode: 945, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 16.122, mean reward: 0.161 [0.009, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.795, 10.098], loss: 0.003894, mae: 0.068297, mean_q: 0.331462
 94544/100000: episode: 946, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 23.714, mean reward: 0.237 [0.021, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.482, 10.098], loss: 0.003830, mae: 0.067345, mean_q: 0.330143
 94644/100000: episode: 947, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 10.931, mean reward: 0.109 [0.006, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.172, 10.098], loss: 0.003715, mae: 0.066612, mean_q: 0.333076
 94744/100000: episode: 948, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 19.034, mean reward: 0.190 [0.005, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.070, 10.407], loss: 0.003816, mae: 0.067430, mean_q: 0.334441
 94844/100000: episode: 949, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.898, mean reward: 0.149 [0.021, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.038, 10.098], loss: 0.003745, mae: 0.067300, mean_q: 0.332284
 94944/100000: episode: 950, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 13.870, mean reward: 0.139 [0.020, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.389, 10.098], loss: 0.003784, mae: 0.067316, mean_q: 0.333934
 95044/100000: episode: 951, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 16.517, mean reward: 0.165 [0.016, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.348, 10.098], loss: 0.004006, mae: 0.069988, mean_q: 0.333845
 95144/100000: episode: 952, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 18.205, mean reward: 0.182 [0.021, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.372, 10.098], loss: 0.003757, mae: 0.067600, mean_q: 0.328303
 95244/100000: episode: 953, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.524, mean reward: 0.155 [0.013, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.709, 10.098], loss: 0.003879, mae: 0.068267, mean_q: 0.328433
 95344/100000: episode: 954, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 13.270, mean reward: 0.133 [0.004, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.431, 10.136], loss: 0.003822, mae: 0.067538, mean_q: 0.324017
 95444/100000: episode: 955, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 13.229, mean reward: 0.132 [0.010, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.857, 10.254], loss: 0.003957, mae: 0.069086, mean_q: 0.329854
 95544/100000: episode: 956, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 13.069, mean reward: 0.131 [0.010, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.832, 10.098], loss: 0.003779, mae: 0.066249, mean_q: 0.322767
 95644/100000: episode: 957, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 15.059, mean reward: 0.151 [0.012, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.580, 10.098], loss: 0.004135, mae: 0.070419, mean_q: 0.326989
 95744/100000: episode: 958, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 18.287, mean reward: 0.183 [0.024, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.192], loss: 0.003795, mae: 0.067399, mean_q: 0.325765
 95844/100000: episode: 959, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 18.209, mean reward: 0.182 [0.022, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.088, 10.098], loss: 0.003732, mae: 0.067687, mean_q: 0.325398
 95944/100000: episode: 960, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 16.434, mean reward: 0.164 [0.021, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.467, 10.215], loss: 0.004193, mae: 0.071088, mean_q: 0.329015
 96044/100000: episode: 961, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 24.607, mean reward: 0.246 [0.009, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.343, 10.283], loss: 0.003663, mae: 0.067621, mean_q: 0.325897
 96144/100000: episode: 962, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 17.998, mean reward: 0.180 [0.006, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.501, 10.098], loss: 0.003940, mae: 0.068736, mean_q: 0.328951
 96244/100000: episode: 963, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 20.371, mean reward: 0.204 [0.026, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.284, 10.292], loss: 0.003991, mae: 0.069010, mean_q: 0.323550
 96344/100000: episode: 964, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 13.657, mean reward: 0.137 [0.012, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.126, 10.098], loss: 0.003829, mae: 0.068121, mean_q: 0.320517
 96444/100000: episode: 965, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.759, mean reward: 0.168 [0.024, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.099], loss: 0.003606, mae: 0.066085, mean_q: 0.318588
 96544/100000: episode: 966, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.463, mean reward: 0.165 [0.034, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.044, 10.098], loss: 0.003699, mae: 0.067031, mean_q: 0.317664
 96644/100000: episode: 967, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 19.822, mean reward: 0.198 [0.029, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.460], loss: 0.003645, mae: 0.067009, mean_q: 0.316537
 96744/100000: episode: 968, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 16.347, mean reward: 0.163 [0.026, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.650, 10.238], loss: 0.003618, mae: 0.067038, mean_q: 0.322670
 96844/100000: episode: 969, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 15.730, mean reward: 0.157 [0.015, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.456, 10.098], loss: 0.003679, mae: 0.066610, mean_q: 0.324128
 96944/100000: episode: 970, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 14.959, mean reward: 0.150 [0.026, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.679, 10.178], loss: 0.003674, mae: 0.067459, mean_q: 0.324355
 97044/100000: episode: 971, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 24.150, mean reward: 0.242 [0.047, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.839, 10.261], loss: 0.003703, mae: 0.067826, mean_q: 0.325116
 97144/100000: episode: 972, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 16.622, mean reward: 0.166 [0.012, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.135, 10.187], loss: 0.003557, mae: 0.065162, mean_q: 0.326725
 97244/100000: episode: 973, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 23.818, mean reward: 0.238 [0.047, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.647, 10.098], loss: 0.003697, mae: 0.066358, mean_q: 0.326772
 97344/100000: episode: 974, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 15.329, mean reward: 0.153 [0.019, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.231, 10.098], loss: 0.003684, mae: 0.067547, mean_q: 0.329225
 97444/100000: episode: 975, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 13.180, mean reward: 0.132 [0.011, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.416, 10.098], loss: 0.003677, mae: 0.066688, mean_q: 0.324942
 97544/100000: episode: 976, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 14.770, mean reward: 0.148 [0.018, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.316, 10.098], loss: 0.003675, mae: 0.067408, mean_q: 0.325833
 97644/100000: episode: 977, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 17.838, mean reward: 0.178 [0.023, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.000, 10.174], loss: 0.003579, mae: 0.066729, mean_q: 0.325601
 97744/100000: episode: 978, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 15.419, mean reward: 0.154 [0.019, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.098], loss: 0.003329, mae: 0.064126, mean_q: 0.323149
 97844/100000: episode: 979, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 18.963, mean reward: 0.190 [0.018, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.538, 10.098], loss: 0.003775, mae: 0.068002, mean_q: 0.324036
 97944/100000: episode: 980, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 14.894, mean reward: 0.149 [0.010, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.845, 10.098], loss: 0.003575, mae: 0.066860, mean_q: 0.331048
 98044/100000: episode: 981, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 15.761, mean reward: 0.158 [0.005, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.277], loss: 0.003536, mae: 0.065240, mean_q: 0.328682
 98144/100000: episode: 982, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 19.401, mean reward: 0.194 [0.010, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.269, 10.098], loss: 0.003502, mae: 0.065879, mean_q: 0.335488
 98244/100000: episode: 983, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.299, mean reward: 0.163 [0.009, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.526, 10.245], loss: 0.003784, mae: 0.068003, mean_q: 0.327108
 98344/100000: episode: 984, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 16.883, mean reward: 0.169 [0.012, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.752, 10.371], loss: 0.003522, mae: 0.066363, mean_q: 0.331966
 98444/100000: episode: 985, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 15.379, mean reward: 0.154 [0.016, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.309, 10.217], loss: 0.003420, mae: 0.064781, mean_q: 0.328226
 98544/100000: episode: 986, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.909, mean reward: 0.159 [0.017, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.098], loss: 0.003487, mae: 0.065757, mean_q: 0.329823
 98644/100000: episode: 987, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 13.521, mean reward: 0.135 [0.003, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.162], loss: 0.003653, mae: 0.066341, mean_q: 0.325949
 98744/100000: episode: 988, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.467, mean reward: 0.185 [0.035, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.731, 10.207], loss: 0.003699, mae: 0.067371, mean_q: 0.328973
 98844/100000: episode: 989, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 23.835, mean reward: 0.238 [0.012, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.098], loss: 0.003699, mae: 0.067665, mean_q: 0.326655
 98944/100000: episode: 990, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.231, mean reward: 0.172 [0.033, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.669, 10.231], loss: 0.003591, mae: 0.066203, mean_q: 0.333193
 99044/100000: episode: 991, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 18.392, mean reward: 0.184 [0.020, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.685, 10.098], loss: 0.003782, mae: 0.067982, mean_q: 0.332183
 99144/100000: episode: 992, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 17.930, mean reward: 0.179 [0.036, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.887, 10.098], loss: 0.003839, mae: 0.068193, mean_q: 0.340405
 99244/100000: episode: 993, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 15.447, mean reward: 0.154 [0.021, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.302, 10.098], loss: 0.003929, mae: 0.069399, mean_q: 0.332843
 99344/100000: episode: 994, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 15.795, mean reward: 0.158 [0.003, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.359, 10.098], loss: 0.003839, mae: 0.069138, mean_q: 0.335416
 99444/100000: episode: 995, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 15.903, mean reward: 0.159 [0.022, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.635, 10.233], loss: 0.003933, mae: 0.068848, mean_q: 0.330588
 99544/100000: episode: 996, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.199, mean reward: 0.172 [0.022, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.853, 10.098], loss: 0.003775, mae: 0.068366, mean_q: 0.330305
 99644/100000: episode: 997, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.299, mean reward: 0.173 [0.018, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.146, 10.098], loss: 0.004035, mae: 0.070501, mean_q: 0.334300
 99744/100000: episode: 998, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 13.538, mean reward: 0.135 [0.021, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.098], loss: 0.003840, mae: 0.068889, mean_q: 0.332014
 99844/100000: episode: 999, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.538, mean reward: 0.155 [0.011, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.800, 10.277], loss: 0.003845, mae: 0.069339, mean_q: 0.334987
 99944/100000: episode: 1000, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.543, mean reward: 0.165 [0.024, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.696, 10.098], loss: 0.004080, mae: 0.070914, mean_q: 0.337750
done, took 577.519 seconds
[Info] End Uniform Random Simulation.
