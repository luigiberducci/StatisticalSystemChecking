Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.171s, episode steps: 100, steps per second: 586, episode reward: 14.726, mean reward: 0.147 [0.026, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.309], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.060s, episode steps: 100, steps per second: 1673, episode reward: 16.233, mean reward: 0.162 [0.024, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.387, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.069s, episode steps: 100, steps per second: 1441, episode reward: 14.391, mean reward: 0.144 [0.027, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.594, 10.133], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.059s, episode steps: 100, steps per second: 1682, episode reward: 15.368, mean reward: 0.154 [0.005, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.227, 10.171], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 15.239, mean reward: 0.152 [0.009, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.095, 10.165], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.273s, episode steps: 100, steps per second: 79, episode reward: 12.654, mean reward: 0.127 [0.008, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.098], loss: 0.027599, mae: 0.151710, mean_q: 0.872433
   700/100000: episode: 7, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 18.479, mean reward: 0.185 [0.017, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.255, 10.098], loss: 0.007761, mae: 0.082642, mean_q: 0.633245
   800/100000: episode: 8, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 21.652, mean reward: 0.217 [0.021, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.739, 10.386], loss: 0.004878, mae: 0.073140, mean_q: 0.499052
   900/100000: episode: 9, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 16.755, mean reward: 0.168 [0.023, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.888, 10.183], loss: 0.004488, mae: 0.073109, mean_q: 0.431859
  1000/100000: episode: 10, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 17.011, mean reward: 0.170 [0.034, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.585, 10.098], loss: 0.004148, mae: 0.071404, mean_q: 0.390260
  1100/100000: episode: 11, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 14.984, mean reward: 0.150 [0.020, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.033, 10.144], loss: 0.003904, mae: 0.070231, mean_q: 0.363564
  1200/100000: episode: 12, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.004, mean reward: 0.150 [0.010, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.098], loss: 0.003866, mae: 0.070451, mean_q: 0.345758
  1300/100000: episode: 13, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 17.322, mean reward: 0.173 [0.016, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.634, 10.114], loss: 0.003886, mae: 0.069707, mean_q: 0.334254
  1400/100000: episode: 14, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.772, mean reward: 0.168 [0.019, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.050, 10.117], loss: 0.003925, mae: 0.070287, mean_q: 0.328904
  1500/100000: episode: 15, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 18.972, mean reward: 0.190 [0.012, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.338, 10.161], loss: 0.004009, mae: 0.071162, mean_q: 0.326263
  1600/100000: episode: 16, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 13.712, mean reward: 0.137 [0.015, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.398, 10.206], loss: 0.003906, mae: 0.070034, mean_q: 0.326008
  1700/100000: episode: 17, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 20.037, mean reward: 0.200 [0.017, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.577, 10.098], loss: 0.004001, mae: 0.070980, mean_q: 0.322941
  1800/100000: episode: 18, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 15.638, mean reward: 0.156 [0.026, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.273, 10.098], loss: 0.004145, mae: 0.071690, mean_q: 0.324478
  1900/100000: episode: 19, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 15.459, mean reward: 0.155 [0.013, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.158], loss: 0.004079, mae: 0.072172, mean_q: 0.324711
  2000/100000: episode: 20, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 17.817, mean reward: 0.178 [0.007, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.724, 10.330], loss: 0.003913, mae: 0.070244, mean_q: 0.323359
  2100/100000: episode: 21, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.642, mean reward: 0.166 [0.020, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.948, 10.245], loss: 0.004168, mae: 0.073334, mean_q: 0.326149
  2200/100000: episode: 22, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.616, mean reward: 0.166 [0.039, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.220], loss: 0.003776, mae: 0.069786, mean_q: 0.327374
  2300/100000: episode: 23, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 29.028, mean reward: 0.290 [0.040, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.780, 10.321], loss: 0.003965, mae: 0.071111, mean_q: 0.330244
  2400/100000: episode: 24, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.615, mean reward: 0.156 [0.028, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.515, 10.216], loss: 0.004317, mae: 0.074135, mean_q: 0.332201
  2500/100000: episode: 25, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.197, mean reward: 0.162 [0.013, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.286], loss: 0.003917, mae: 0.070262, mean_q: 0.329793
  2600/100000: episode: 26, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.256, mean reward: 0.153 [0.019, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.248, 10.098], loss: 0.004302, mae: 0.074084, mean_q: 0.335567
  2700/100000: episode: 27, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 20.227, mean reward: 0.202 [0.008, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.004106, mae: 0.072374, mean_q: 0.334516
  2800/100000: episode: 28, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.366, mean reward: 0.174 [0.052, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.137, 10.098], loss: 0.004116, mae: 0.072862, mean_q: 0.334372
  2900/100000: episode: 29, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 16.202, mean reward: 0.162 [0.020, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.534, 10.098], loss: 0.003902, mae: 0.070597, mean_q: 0.334820
  3000/100000: episode: 30, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.876, mean reward: 0.149 [0.004, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.227, 10.098], loss: 0.004295, mae: 0.074049, mean_q: 0.335927
  3100/100000: episode: 31, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 22.009, mean reward: 0.220 [0.042, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.192], loss: 0.004050, mae: 0.071574, mean_q: 0.334459
  3200/100000: episode: 32, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.286, mean reward: 0.163 [0.006, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.056, 10.098], loss: 0.004142, mae: 0.072499, mean_q: 0.338621
  3300/100000: episode: 33, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 14.232, mean reward: 0.142 [0.009, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.206, 10.098], loss: 0.003933, mae: 0.070575, mean_q: 0.333101
  3400/100000: episode: 34, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 19.044, mean reward: 0.190 [0.035, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.534, 10.098], loss: 0.004312, mae: 0.074168, mean_q: 0.334286
  3500/100000: episode: 35, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.965, mean reward: 0.180 [0.024, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.138], loss: 0.004307, mae: 0.074913, mean_q: 0.335117
  3600/100000: episode: 36, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 13.156, mean reward: 0.132 [0.009, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.560, 10.144], loss: 0.004337, mae: 0.073671, mean_q: 0.333002
  3700/100000: episode: 37, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 16.362, mean reward: 0.164 [0.014, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.667, 10.098], loss: 0.004228, mae: 0.073672, mean_q: 0.330736
  3800/100000: episode: 38, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.406, mean reward: 0.164 [0.021, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.645, 10.148], loss: 0.004149, mae: 0.072748, mean_q: 0.335739
  3900/100000: episode: 39, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 13.765, mean reward: 0.138 [0.009, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.199, 10.138], loss: 0.004077, mae: 0.071654, mean_q: 0.334193
  4000/100000: episode: 40, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.540, mean reward: 0.155 [0.016, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.790, 10.192], loss: 0.004173, mae: 0.072622, mean_q: 0.331285
  4100/100000: episode: 41, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 16.201, mean reward: 0.162 [0.029, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.709, 10.146], loss: 0.004206, mae: 0.072743, mean_q: 0.330853
  4200/100000: episode: 42, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.112, mean reward: 0.141 [0.017, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.659, 10.155], loss: 0.004607, mae: 0.075892, mean_q: 0.330995
  4300/100000: episode: 43, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.282, mean reward: 0.163 [0.029, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.766, 10.098], loss: 0.004184, mae: 0.073320, mean_q: 0.329310
  4400/100000: episode: 44, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 21.462, mean reward: 0.215 [0.025, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.386], loss: 0.004143, mae: 0.072583, mean_q: 0.332625
  4500/100000: episode: 45, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 15.122, mean reward: 0.151 [0.013, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.786, 10.243], loss: 0.004172, mae: 0.072594, mean_q: 0.331335
  4600/100000: episode: 46, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.466, mean reward: 0.135 [0.002, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.323, 10.254], loss: 0.004072, mae: 0.071920, mean_q: 0.335666
  4700/100000: episode: 47, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 15.607, mean reward: 0.156 [0.007, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.306, 10.098], loss: 0.004076, mae: 0.071828, mean_q: 0.330600
  4800/100000: episode: 48, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.158, mean reward: 0.182 [0.025, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.639, 10.227], loss: 0.003993, mae: 0.071164, mean_q: 0.331764
  4900/100000: episode: 49, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 20.588, mean reward: 0.206 [0.022, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.836, 10.359], loss: 0.004146, mae: 0.073492, mean_q: 0.334647
  5000/100000: episode: 50, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 19.927, mean reward: 0.199 [0.041, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.290, 10.365], loss: 0.004004, mae: 0.071540, mean_q: 0.333995
  5100/100000: episode: 51, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.583, mean reward: 0.166 [0.028, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.319, 10.174], loss: 0.004049, mae: 0.071505, mean_q: 0.330290
  5200/100000: episode: 52, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 20.543, mean reward: 0.205 [0.010, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.318, 10.098], loss: 0.004285, mae: 0.073958, mean_q: 0.330227
  5300/100000: episode: 53, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 14.623, mean reward: 0.146 [0.004, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.438, 10.314], loss: 0.003974, mae: 0.072065, mean_q: 0.330444
  5400/100000: episode: 54, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 13.857, mean reward: 0.139 [0.005, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.098], loss: 0.004328, mae: 0.073687, mean_q: 0.334079
  5500/100000: episode: 55, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.682, mean reward: 0.157 [0.021, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.636, 10.133], loss: 0.004268, mae: 0.074153, mean_q: 0.330420
  5600/100000: episode: 56, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 18.758, mean reward: 0.188 [0.016, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.350], loss: 0.003983, mae: 0.070984, mean_q: 0.331585
  5700/100000: episode: 57, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 15.716, mean reward: 0.157 [0.026, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.713, 10.098], loss: 0.003856, mae: 0.070906, mean_q: 0.335130
  5800/100000: episode: 58, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.285, mean reward: 0.173 [0.017, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.841, 10.098], loss: 0.004255, mae: 0.073991, mean_q: 0.336364
  5900/100000: episode: 59, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.579, mean reward: 0.166 [0.028, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.199, 10.206], loss: 0.004019, mae: 0.072019, mean_q: 0.335328
  6000/100000: episode: 60, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.211, mean reward: 0.152 [0.022, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.098], loss: 0.004042, mae: 0.071793, mean_q: 0.332942
  6100/100000: episode: 61, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.544, mean reward: 0.155 [0.007, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.862, 10.098], loss: 0.004017, mae: 0.072018, mean_q: 0.331776
  6200/100000: episode: 62, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 14.589, mean reward: 0.146 [0.015, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.133], loss: 0.003950, mae: 0.071348, mean_q: 0.331730
  6300/100000: episode: 63, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 20.572, mean reward: 0.206 [0.015, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.098], loss: 0.003892, mae: 0.070257, mean_q: 0.331030
  6400/100000: episode: 64, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 17.916, mean reward: 0.179 [0.021, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.241], loss: 0.003963, mae: 0.071790, mean_q: 0.332255
  6500/100000: episode: 65, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.676, mean reward: 0.157 [0.029, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.275], loss: 0.004054, mae: 0.071520, mean_q: 0.334229
  6600/100000: episode: 66, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.599, mean reward: 0.176 [0.017, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.226, 10.232], loss: 0.004014, mae: 0.071666, mean_q: 0.337605
  6700/100000: episode: 67, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.449, mean reward: 0.174 [0.019, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.687, 10.112], loss: 0.004041, mae: 0.072078, mean_q: 0.335376
  6800/100000: episode: 68, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.692, mean reward: 0.167 [0.016, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.790, 10.098], loss: 0.004417, mae: 0.075230, mean_q: 0.334985
  6900/100000: episode: 69, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 18.304, mean reward: 0.183 [0.013, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.835, 10.098], loss: 0.004187, mae: 0.073414, mean_q: 0.338575
  7000/100000: episode: 70, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.132, mean reward: 0.161 [0.024, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.242, 10.211], loss: 0.003841, mae: 0.070329, mean_q: 0.329608
  7100/100000: episode: 71, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 14.299, mean reward: 0.143 [0.006, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.025, 10.186], loss: 0.004075, mae: 0.071824, mean_q: 0.332372
  7200/100000: episode: 72, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 18.217, mean reward: 0.182 [0.024, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.324, 10.118], loss: 0.004076, mae: 0.071193, mean_q: 0.332330
  7300/100000: episode: 73, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.946, mean reward: 0.169 [0.036, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.156, 10.323], loss: 0.003901, mae: 0.070615, mean_q: 0.330486
  7400/100000: episode: 74, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 16.743, mean reward: 0.167 [0.015, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.464, 10.284], loss: 0.004039, mae: 0.071335, mean_q: 0.331044
  7500/100000: episode: 75, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 14.437, mean reward: 0.144 [0.023, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.280], loss: 0.004034, mae: 0.071163, mean_q: 0.332347
  7600/100000: episode: 76, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.278, mean reward: 0.163 [0.010, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.403, 10.098], loss: 0.003759, mae: 0.068717, mean_q: 0.330363
  7700/100000: episode: 77, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 22.245, mean reward: 0.222 [0.031, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.320, 10.098], loss: 0.003989, mae: 0.070905, mean_q: 0.329659
  7800/100000: episode: 78, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 19.959, mean reward: 0.200 [0.031, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.154, 10.341], loss: 0.003920, mae: 0.070830, mean_q: 0.327176
  7900/100000: episode: 79, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 17.452, mean reward: 0.175 [0.014, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.340, 10.098], loss: 0.003926, mae: 0.070016, mean_q: 0.328018
  8000/100000: episode: 80, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 20.325, mean reward: 0.203 [0.014, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.338], loss: 0.003925, mae: 0.070670, mean_q: 0.332306
  8100/100000: episode: 81, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 17.093, mean reward: 0.171 [0.019, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.584, 10.098], loss: 0.004074, mae: 0.071059, mean_q: 0.326401
  8200/100000: episode: 82, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 13.854, mean reward: 0.139 [0.031, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.168, 10.117], loss: 0.003903, mae: 0.070099, mean_q: 0.327533
  8300/100000: episode: 83, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 21.438, mean reward: 0.214 [0.031, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.685, 10.098], loss: 0.004194, mae: 0.072168, mean_q: 0.330727
  8400/100000: episode: 84, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.703, mean reward: 0.167 [0.026, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.310, 10.098], loss: 0.004041, mae: 0.070913, mean_q: 0.338104
  8500/100000: episode: 85, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.048, mean reward: 0.140 [0.007, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.004076, mae: 0.071769, mean_q: 0.331658
  8600/100000: episode: 86, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.246, mean reward: 0.182 [0.038, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.721, 10.348], loss: 0.003894, mae: 0.069522, mean_q: 0.335104
  8700/100000: episode: 87, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.619, mean reward: 0.156 [0.006, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.190], loss: 0.004148, mae: 0.072778, mean_q: 0.336842
  8800/100000: episode: 88, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 12.208, mean reward: 0.122 [0.023, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.011, 10.171], loss: 0.004233, mae: 0.072796, mean_q: 0.329196
  8900/100000: episode: 89, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.866, mean reward: 0.179 [0.036, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.641, 10.168], loss: 0.004051, mae: 0.072195, mean_q: 0.332715
  9000/100000: episode: 90, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.574, mean reward: 0.146 [0.007, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.290, 10.200], loss: 0.004161, mae: 0.072599, mean_q: 0.331231
  9100/100000: episode: 91, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 16.463, mean reward: 0.165 [0.011, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.098], loss: 0.004215, mae: 0.072309, mean_q: 0.332423
  9200/100000: episode: 92, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.686, mean reward: 0.177 [0.020, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.859, 10.098], loss: 0.003998, mae: 0.071114, mean_q: 0.331300
  9300/100000: episode: 93, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 15.555, mean reward: 0.156 [0.022, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.760, 10.098], loss: 0.004332, mae: 0.073564, mean_q: 0.336919
  9400/100000: episode: 94, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.069, mean reward: 0.191 [0.027, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.825, 10.296], loss: 0.003956, mae: 0.070506, mean_q: 0.333204
  9500/100000: episode: 95, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 14.162, mean reward: 0.142 [0.021, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.098], loss: 0.004144, mae: 0.072132, mean_q: 0.334275
  9600/100000: episode: 96, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 16.404, mean reward: 0.164 [0.024, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.084, 10.150], loss: 0.004042, mae: 0.071851, mean_q: 0.334607
  9700/100000: episode: 97, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 24.421, mean reward: 0.244 [0.019, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.098], loss: 0.003977, mae: 0.071120, mean_q: 0.334200
  9800/100000: episode: 98, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.560, mean reward: 0.176 [0.013, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.904, 10.373], loss: 0.004403, mae: 0.073264, mean_q: 0.334391
  9900/100000: episode: 99, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 15.438, mean reward: 0.154 [0.026, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.819, 10.098], loss: 0.004154, mae: 0.072271, mean_q: 0.334136
 10000/100000: episode: 100, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.008, mean reward: 0.150 [0.007, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.881, 10.098], loss: 0.004114, mae: 0.071424, mean_q: 0.330704
 10100/100000: episode: 101, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.557, mean reward: 0.166 [0.016, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.332, 10.098], loss: 0.004230, mae: 0.072454, mean_q: 0.332327
 10200/100000: episode: 102, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.907, mean reward: 0.159 [0.021, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.209, 10.098], loss: 0.003812, mae: 0.069153, mean_q: 0.329013
 10300/100000: episode: 103, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 14.915, mean reward: 0.149 [0.022, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.850, 10.104], loss: 0.003956, mae: 0.070308, mean_q: 0.332900
 10400/100000: episode: 104, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 20.157, mean reward: 0.202 [0.020, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.485, 10.098], loss: 0.004280, mae: 0.072598, mean_q: 0.328402
 10500/100000: episode: 105, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 14.607, mean reward: 0.146 [0.002, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.514, 10.203], loss: 0.004046, mae: 0.071447, mean_q: 0.331319
 10600/100000: episode: 106, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.399, mean reward: 0.154 [0.015, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.876, 10.098], loss: 0.004073, mae: 0.071302, mean_q: 0.329214
 10700/100000: episode: 107, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.660, mean reward: 0.137 [0.023, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.098], loss: 0.003959, mae: 0.070116, mean_q: 0.331491
 10800/100000: episode: 108, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.484, mean reward: 0.155 [0.024, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.594, 10.249], loss: 0.004232, mae: 0.072348, mean_q: 0.329553
 10900/100000: episode: 109, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 20.902, mean reward: 0.209 [0.020, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.219, 10.098], loss: 0.003979, mae: 0.071398, mean_q: 0.328752
 11000/100000: episode: 110, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 16.770, mean reward: 0.168 [0.001, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.534, 10.158], loss: 0.003987, mae: 0.070939, mean_q: 0.329509
 11100/100000: episode: 111, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 13.956, mean reward: 0.140 [0.016, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.702, 10.098], loss: 0.004095, mae: 0.071191, mean_q: 0.329450
 11200/100000: episode: 112, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.526, mean reward: 0.145 [0.016, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.346, 10.190], loss: 0.004119, mae: 0.071194, mean_q: 0.330066
 11300/100000: episode: 113, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 21.430, mean reward: 0.214 [0.032, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.240, 10.098], loss: 0.004166, mae: 0.072450, mean_q: 0.330295
 11400/100000: episode: 114, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.334, mean reward: 0.163 [0.023, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.067, 10.169], loss: 0.004036, mae: 0.071530, mean_q: 0.331412
 11500/100000: episode: 115, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 20.096, mean reward: 0.201 [0.040, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.843, 10.098], loss: 0.003996, mae: 0.070656, mean_q: 0.334211
 11600/100000: episode: 116, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 18.415, mean reward: 0.184 [0.020, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.136], loss: 0.004072, mae: 0.072013, mean_q: 0.333687
 11700/100000: episode: 117, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.952, mean reward: 0.180 [0.033, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.435, 10.259], loss: 0.003978, mae: 0.070893, mean_q: 0.333695
 11800/100000: episode: 118, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 19.163, mean reward: 0.192 [0.024, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.867, 10.135], loss: 0.003943, mae: 0.070702, mean_q: 0.335111
 11900/100000: episode: 119, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 18.120, mean reward: 0.181 [0.014, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.334, 10.098], loss: 0.004214, mae: 0.073129, mean_q: 0.335125
 12000/100000: episode: 120, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 13.211, mean reward: 0.132 [0.006, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.592, 10.130], loss: 0.004043, mae: 0.071408, mean_q: 0.334155
 12100/100000: episode: 121, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 19.656, mean reward: 0.197 [0.020, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.325, 10.098], loss: 0.003959, mae: 0.070362, mean_q: 0.335462
 12200/100000: episode: 122, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.804, mean reward: 0.188 [0.034, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.098], loss: 0.004080, mae: 0.071368, mean_q: 0.333981
 12300/100000: episode: 123, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.009, mean reward: 0.150 [0.022, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.660, 10.102], loss: 0.003927, mae: 0.070484, mean_q: 0.336857
 12400/100000: episode: 124, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 20.378, mean reward: 0.204 [0.033, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.565, 10.098], loss: 0.003964, mae: 0.071191, mean_q: 0.340901
 12500/100000: episode: 125, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 18.929, mean reward: 0.189 [0.026, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.230], loss: 0.003923, mae: 0.070336, mean_q: 0.334848
 12600/100000: episode: 126, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.032, mean reward: 0.170 [0.005, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.098], loss: 0.004137, mae: 0.071987, mean_q: 0.335284
 12700/100000: episode: 127, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 13.594, mean reward: 0.136 [0.007, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.227, 10.202], loss: 0.004029, mae: 0.070733, mean_q: 0.333315
 12800/100000: episode: 128, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.113, mean reward: 0.151 [0.013, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.003660, mae: 0.067310, mean_q: 0.332867
 12900/100000: episode: 129, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 15.428, mean reward: 0.154 [0.013, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.272, 10.233], loss: 0.004049, mae: 0.071105, mean_q: 0.336296
 13000/100000: episode: 130, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.242, mean reward: 0.162 [0.025, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.380, 10.098], loss: 0.003680, mae: 0.067558, mean_q: 0.329249
 13100/100000: episode: 131, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 22.096, mean reward: 0.221 [0.062, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.317], loss: 0.003732, mae: 0.068198, mean_q: 0.328212
 13200/100000: episode: 132, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 13.723, mean reward: 0.137 [0.012, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.128, 10.098], loss: 0.003669, mae: 0.068198, mean_q: 0.333075
 13300/100000: episode: 133, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 18.745, mean reward: 0.187 [0.004, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.550, 10.098], loss: 0.003745, mae: 0.067856, mean_q: 0.325266
 13400/100000: episode: 134, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 18.095, mean reward: 0.181 [0.008, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.141, 10.213], loss: 0.003898, mae: 0.070261, mean_q: 0.332240
 13500/100000: episode: 135, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.005, mean reward: 0.140 [0.010, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.986, 10.226], loss: 0.003880, mae: 0.070203, mean_q: 0.330591
 13600/100000: episode: 136, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.007, mean reward: 0.160 [0.033, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.803, 10.098], loss: 0.004036, mae: 0.071206, mean_q: 0.331869
 13700/100000: episode: 137, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 15.876, mean reward: 0.159 [0.028, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.098], loss: 0.003887, mae: 0.069898, mean_q: 0.333767
 13800/100000: episode: 138, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.438, mean reward: 0.154 [0.010, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.098], loss: 0.003938, mae: 0.070075, mean_q: 0.332108
 13900/100000: episode: 139, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.650, mean reward: 0.177 [0.015, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.966, 10.098], loss: 0.003915, mae: 0.069787, mean_q: 0.330490
 14000/100000: episode: 140, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.908, mean reward: 0.159 [0.013, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.140], loss: 0.003770, mae: 0.068856, mean_q: 0.334164
 14100/100000: episode: 141, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 13.637, mean reward: 0.136 [0.010, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.281, 10.098], loss: 0.003901, mae: 0.069817, mean_q: 0.338393
 14200/100000: episode: 142, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 18.098, mean reward: 0.181 [0.011, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.257, 10.111], loss: 0.003742, mae: 0.068470, mean_q: 0.332541
 14300/100000: episode: 143, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 13.915, mean reward: 0.139 [0.019, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.852, 10.224], loss: 0.003862, mae: 0.068648, mean_q: 0.332583
 14400/100000: episode: 144, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.079, mean reward: 0.151 [0.016, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.098], loss: 0.003728, mae: 0.068550, mean_q: 0.332163
 14500/100000: episode: 145, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 14.342, mean reward: 0.143 [0.016, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.103], loss: 0.003790, mae: 0.068274, mean_q: 0.331603
 14600/100000: episode: 146, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.691, mean reward: 0.177 [0.034, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.338, 10.207], loss: 0.003575, mae: 0.066152, mean_q: 0.329756
 14700/100000: episode: 147, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.867, mean reward: 0.149 [0.014, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.143, 10.308], loss: 0.003692, mae: 0.067695, mean_q: 0.326414
 14800/100000: episode: 148, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 16.054, mean reward: 0.161 [0.026, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.141], loss: 0.003619, mae: 0.067098, mean_q: 0.323486
 14900/100000: episode: 149, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 21.918, mean reward: 0.219 [0.017, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.898, 10.098], loss: 0.003995, mae: 0.069731, mean_q: 0.325618
[Info] New level: 0.6404080986976624 | Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.463s, episode steps: 100, steps per second: 22, episode reward: 16.655, mean reward: 0.167 [0.017, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.036, 10.098], loss: 0.003738, mae: 0.067524, mean_q: 0.325790
 15081/100000: episode: 151, duration: 0.419s, episode steps: 81, steps per second: 193, episode reward: 20.121, mean reward: 0.248 [0.037, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.637 [-1.199, 10.341], loss: 0.003827, mae: 0.068912, mean_q: 0.330166
 15092/100000: episode: 152, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 3.056, mean reward: 0.278 [0.174, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.301, 10.100], loss: 0.003818, mae: 0.066045, mean_q: 0.324094
 15113/100000: episode: 153, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 6.819, mean reward: 0.325 [0.086, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.065, 10.100], loss: 0.003349, mae: 0.064014, mean_q: 0.330326
 15133/100000: episode: 154, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.728, mean reward: 0.336 [0.192, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.386, 10.100], loss: 0.003741, mae: 0.067920, mean_q: 0.336025
 15167/100000: episode: 155, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 11.214, mean reward: 0.330 [0.128, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.651, 10.100], loss: 0.003882, mae: 0.068985, mean_q: 0.332285
 15178/100000: episode: 156, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 3.640, mean reward: 0.331 [0.257, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.173, 10.100], loss: 0.003751, mae: 0.067104, mean_q: 0.342635
 15188/100000: episode: 157, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 2.382, mean reward: 0.238 [0.156, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.290, 10.100], loss: 0.003773, mae: 0.068854, mean_q: 0.328712
 15208/100000: episode: 158, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 7.726, mean reward: 0.386 [0.293, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.377, 10.100], loss: 0.004208, mae: 0.071937, mean_q: 0.338508
 15218/100000: episode: 159, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 2.845, mean reward: 0.285 [0.212, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.563, 10.100], loss: 0.003698, mae: 0.070625, mean_q: 0.335331
 15229/100000: episode: 160, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 4.220, mean reward: 0.384 [0.312, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.391, 10.100], loss: 0.003638, mae: 0.067504, mean_q: 0.348188
 15266/100000: episode: 161, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 10.096, mean reward: 0.273 [0.189, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.314, 10.100], loss: 0.003924, mae: 0.068760, mean_q: 0.338757
 15277/100000: episode: 162, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 3.500, mean reward: 0.318 [0.260, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.235, 10.100], loss: 0.004001, mae: 0.069862, mean_q: 0.343506
 15311/100000: episode: 163, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 10.876, mean reward: 0.320 [0.189, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.808, 10.100], loss: 0.003974, mae: 0.068483, mean_q: 0.340685
 15392/100000: episode: 164, duration: 0.448s, episode steps: 81, steps per second: 181, episode reward: 14.445, mean reward: 0.178 [0.028, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.631 [-0.610, 10.131], loss: 0.003536, mae: 0.066758, mean_q: 0.349145
 15402/100000: episode: 165, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 3.082, mean reward: 0.308 [0.175, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.628, 10.100], loss: 0.003357, mae: 0.067191, mean_q: 0.355333
 15412/100000: episode: 166, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 2.791, mean reward: 0.279 [0.220, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.263, 10.100], loss: 0.004489, mae: 0.073847, mean_q: 0.340117
 15423/100000: episode: 167, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 3.899, mean reward: 0.354 [0.289, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.277, 10.100], loss: 0.004112, mae: 0.071003, mean_q: 0.329462
 15443/100000: episode: 168, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 6.179, mean reward: 0.309 [0.238, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.511, 10.100], loss: 0.004390, mae: 0.072281, mean_q: 0.353740
 15454/100000: episode: 169, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.167, mean reward: 0.379 [0.295, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.425, 10.100], loss: 0.003998, mae: 0.069040, mean_q: 0.343295
 15535/100000: episode: 170, duration: 0.423s, episode steps: 81, steps per second: 191, episode reward: 21.493, mean reward: 0.265 [0.045, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.633 [-0.763, 10.235], loss: 0.003769, mae: 0.068605, mean_q: 0.350057
 15556/100000: episode: 171, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 7.055, mean reward: 0.336 [0.230, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.324, 10.100], loss: 0.004268, mae: 0.073170, mean_q: 0.354270
 15585/100000: episode: 172, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 11.075, mean reward: 0.382 [0.251, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.358, 10.100], loss: 0.003375, mae: 0.063889, mean_q: 0.358323
 15605/100000: episode: 173, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 5.468, mean reward: 0.273 [0.116, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.149, 10.100], loss: 0.003559, mae: 0.067971, mean_q: 0.354746
 15626/100000: episode: 174, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 5.959, mean reward: 0.284 [0.206, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.386, 10.100], loss: 0.003588, mae: 0.066186, mean_q: 0.362967
 15660/100000: episode: 175, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 8.388, mean reward: 0.247 [0.116, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.852, 10.100], loss: 0.003899, mae: 0.068416, mean_q: 0.358928
 15671/100000: episode: 176, duration: 0.069s, episode steps: 11, steps per second: 158, episode reward: 4.390, mean reward: 0.399 [0.333, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.366, 10.100], loss: 0.004029, mae: 0.069392, mean_q: 0.367025
 15752/100000: episode: 177, duration: 0.423s, episode steps: 81, steps per second: 192, episode reward: 15.075, mean reward: 0.186 [0.026, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.635 [-1.322, 10.225], loss: 0.003945, mae: 0.068814, mean_q: 0.361866
 15772/100000: episode: 178, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 6.732, mean reward: 0.337 [0.234, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.206, 10.100], loss: 0.004297, mae: 0.074148, mean_q: 0.364575
 15783/100000: episode: 179, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.825, mean reward: 0.348 [0.207, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.387, 10.100], loss: 0.003721, mae: 0.068236, mean_q: 0.370552
 15804/100000: episode: 180, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 6.243, mean reward: 0.297 [0.170, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.137, 10.100], loss: 0.003732, mae: 0.068305, mean_q: 0.369298
 15885/100000: episode: 181, duration: 0.444s, episode steps: 81, steps per second: 183, episode reward: 16.451, mean reward: 0.203 [0.013, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.649 [-0.538, 10.306], loss: 0.003878, mae: 0.069238, mean_q: 0.363433
 15905/100000: episode: 182, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.028, mean reward: 0.301 [0.247, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.569, 10.100], loss: 0.004310, mae: 0.073147, mean_q: 0.372670
 15916/100000: episode: 183, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.325, mean reward: 0.302 [0.230, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.237, 10.100], loss: 0.004265, mae: 0.071453, mean_q: 0.369307
 15936/100000: episode: 184, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 6.306, mean reward: 0.315 [0.261, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.296, 10.100], loss: 0.004282, mae: 0.071841, mean_q: 0.364801
 15973/100000: episode: 185, duration: 0.203s, episode steps: 37, steps per second: 183, episode reward: 11.174, mean reward: 0.302 [0.087, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.348, 10.100], loss: 0.003476, mae: 0.066044, mean_q: 0.363598
 15993/100000: episode: 186, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 5.833, mean reward: 0.292 [0.131, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.282, 10.100], loss: 0.004130, mae: 0.072384, mean_q: 0.366115
 16004/100000: episode: 187, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 3.487, mean reward: 0.317 [0.265, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.262, 10.100], loss: 0.003721, mae: 0.067260, mean_q: 0.357704
 16014/100000: episode: 188, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.279, mean reward: 0.228 [0.188, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.980, 10.100], loss: 0.004565, mae: 0.073712, mean_q: 0.375988
 16095/100000: episode: 189, duration: 0.443s, episode steps: 81, steps per second: 183, episode reward: 15.377, mean reward: 0.190 [0.026, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.642 [-0.931, 10.100], loss: 0.003714, mae: 0.068019, mean_q: 0.369663
 16115/100000: episode: 190, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.006, mean reward: 0.300 [0.151, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.235, 10.100], loss: 0.004079, mae: 0.069933, mean_q: 0.369278
 16152/100000: episode: 191, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 13.387, mean reward: 0.362 [0.217, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.952, 10.100], loss: 0.003854, mae: 0.068147, mean_q: 0.376433
 16162/100000: episode: 192, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 3.145, mean reward: 0.314 [0.229, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.268, 10.100], loss: 0.003791, mae: 0.068561, mean_q: 0.355779
 16196/100000: episode: 193, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 10.370, mean reward: 0.305 [0.091, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.559, 10.100], loss: 0.004174, mae: 0.070419, mean_q: 0.381556
 16232/100000: episode: 194, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 9.207, mean reward: 0.256 [0.096, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.149, 10.100], loss: 0.004342, mae: 0.073197, mean_q: 0.377967
 16253/100000: episode: 195, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 6.469, mean reward: 0.308 [0.147, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.487, 10.100], loss: 0.003792, mae: 0.068812, mean_q: 0.373673
 16334/100000: episode: 196, duration: 0.424s, episode steps: 81, steps per second: 191, episode reward: 16.793, mean reward: 0.207 [0.019, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.628 [-0.274, 10.100], loss: 0.003683, mae: 0.066761, mean_q: 0.379724
 16415/100000: episode: 197, duration: 0.393s, episode steps: 81, steps per second: 206, episode reward: 13.865, mean reward: 0.171 [0.017, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.634 [-0.649, 10.100], loss: 0.003927, mae: 0.069153, mean_q: 0.382853
 16496/100000: episode: 198, duration: 0.410s, episode steps: 81, steps per second: 197, episode reward: 13.773, mean reward: 0.170 [0.050, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.639 [-1.003, 10.100], loss: 0.003867, mae: 0.068587, mean_q: 0.378353
 16516/100000: episode: 199, duration: 0.104s, episode steps: 20, steps per second: 191, episode reward: 5.712, mean reward: 0.286 [0.167, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.433, 10.100], loss: 0.003509, mae: 0.065905, mean_q: 0.372759
 16550/100000: episode: 200, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 11.441, mean reward: 0.336 [0.205, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.308, 10.100], loss: 0.003632, mae: 0.066847, mean_q: 0.383121
 16631/100000: episode: 201, duration: 0.423s, episode steps: 81, steps per second: 191, episode reward: 16.247, mean reward: 0.201 [0.013, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.630 [-0.866, 10.100], loss: 0.003583, mae: 0.066677, mean_q: 0.381793
 16660/100000: episode: 202, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 11.334, mean reward: 0.391 [0.276, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.500, 10.100], loss: 0.003311, mae: 0.063875, mean_q: 0.393533
 16697/100000: episode: 203, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 9.746, mean reward: 0.263 [0.067, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.235, 10.100], loss: 0.003781, mae: 0.067778, mean_q: 0.381086
 16731/100000: episode: 204, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 8.441, mean reward: 0.248 [0.058, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.467, 10.100], loss: 0.003765, mae: 0.067824, mean_q: 0.384608
 16742/100000: episode: 205, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 3.530, mean reward: 0.321 [0.210, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.209, 10.100], loss: 0.003726, mae: 0.067760, mean_q: 0.410971
 16779/100000: episode: 206, duration: 0.202s, episode steps: 37, steps per second: 184, episode reward: 13.776, mean reward: 0.372 [0.212, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.148, 10.100], loss: 0.003985, mae: 0.070037, mean_q: 0.387078
 16860/100000: episode: 207, duration: 0.415s, episode steps: 81, steps per second: 195, episode reward: 16.145, mean reward: 0.199 [0.030, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-0.963, 10.262], loss: 0.003956, mae: 0.070176, mean_q: 0.395396
 16871/100000: episode: 208, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 2.983, mean reward: 0.271 [0.201, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.495, 10.100], loss: 0.003367, mae: 0.064112, mean_q: 0.404032
 16900/100000: episode: 209, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 11.141, mean reward: 0.384 [0.238, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.085, 10.100], loss: 0.003616, mae: 0.065256, mean_q: 0.387700
 16936/100000: episode: 210, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 11.860, mean reward: 0.329 [0.194, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.525, 10.100], loss: 0.003731, mae: 0.067849, mean_q: 0.400851
 16973/100000: episode: 211, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 10.671, mean reward: 0.288 [0.138, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.100, 10.100], loss: 0.003073, mae: 0.061918, mean_q: 0.398704
 16994/100000: episode: 212, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 5.565, mean reward: 0.265 [0.138, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.227, 10.100], loss: 0.003158, mae: 0.063375, mean_q: 0.405941
 17005/100000: episode: 213, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 2.852, mean reward: 0.259 [0.218, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.347, 10.100], loss: 0.004036, mae: 0.069314, mean_q: 0.400777
 17041/100000: episode: 214, duration: 0.204s, episode steps: 36, steps per second: 177, episode reward: 7.413, mean reward: 0.206 [0.031, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.854, 10.100], loss: 0.003793, mae: 0.068401, mean_q: 0.406390
 17075/100000: episode: 215, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 6.603, mean reward: 0.194 [0.034, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.113, 10.100], loss: 0.004154, mae: 0.070321, mean_q: 0.394975
 17104/100000: episode: 216, duration: 0.173s, episode steps: 29, steps per second: 167, episode reward: 7.182, mean reward: 0.248 [0.092, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.561, 10.100], loss: 0.003872, mae: 0.068492, mean_q: 0.394268
 17115/100000: episode: 217, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 3.310, mean reward: 0.301 [0.239, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.457, 10.100], loss: 0.003519, mae: 0.066067, mean_q: 0.402276
 17135/100000: episode: 218, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 7.025, mean reward: 0.351 [0.204, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.972, 10.100], loss: 0.003829, mae: 0.067980, mean_q: 0.391119
 17171/100000: episode: 219, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 7.901, mean reward: 0.219 [0.038, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.575, 10.128], loss: 0.003447, mae: 0.065778, mean_q: 0.390555
 17252/100000: episode: 220, duration: 0.422s, episode steps: 81, steps per second: 192, episode reward: 12.013, mean reward: 0.148 [0.020, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.633 [-0.556, 10.134], loss: 0.003525, mae: 0.065328, mean_q: 0.400378
 17289/100000: episode: 221, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 11.585, mean reward: 0.313 [0.184, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.385, 10.100], loss: 0.003516, mae: 0.065344, mean_q: 0.406080
 17300/100000: episode: 222, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 3.248, mean reward: 0.295 [0.269, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.239, 10.100], loss: 0.003655, mae: 0.068620, mean_q: 0.406112
 17320/100000: episode: 223, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 6.471, mean reward: 0.324 [0.208, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.270, 10.100], loss: 0.003270, mae: 0.064642, mean_q: 0.400303
 17354/100000: episode: 224, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 13.377, mean reward: 0.393 [0.261, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.850, 10.100], loss: 0.003304, mae: 0.063719, mean_q: 0.406254
 17374/100000: episode: 225, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 4.879, mean reward: 0.244 [0.145, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.798, 10.100], loss: 0.003383, mae: 0.065374, mean_q: 0.414578
 17384/100000: episode: 226, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 3.464, mean reward: 0.346 [0.256, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.467, 10.100], loss: 0.003263, mae: 0.063156, mean_q: 0.430896
 17395/100000: episode: 227, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 3.970, mean reward: 0.361 [0.268, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.854, 10.100], loss: 0.003212, mae: 0.065488, mean_q: 0.429053
 17415/100000: episode: 228, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 6.478, mean reward: 0.324 [0.233, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.620, 10.100], loss: 0.003468, mae: 0.064980, mean_q: 0.409159
 17451/100000: episode: 229, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 9.834, mean reward: 0.273 [0.094, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.244, 10.100], loss: 0.003443, mae: 0.066277, mean_q: 0.408566
 17461/100000: episode: 230, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 3.056, mean reward: 0.306 [0.259, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.405, 10.100], loss: 0.003472, mae: 0.065877, mean_q: 0.415504
 17472/100000: episode: 231, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 3.547, mean reward: 0.322 [0.252, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.322, 10.100], loss: 0.003813, mae: 0.069411, mean_q: 0.405840
 17501/100000: episode: 232, duration: 0.148s, episode steps: 29, steps per second: 197, episode reward: 6.641, mean reward: 0.229 [0.050, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.480, 10.100], loss: 0.003865, mae: 0.068101, mean_q: 0.413828
 17535/100000: episode: 233, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 14.911, mean reward: 0.439 [0.245, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.729, 10.100], loss: 0.003721, mae: 0.067308, mean_q: 0.408851
 17546/100000: episode: 234, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 3.588, mean reward: 0.326 [0.254, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.788, 10.100], loss: 0.004058, mae: 0.071222, mean_q: 0.429862
 17627/100000: episode: 235, duration: 0.411s, episode steps: 81, steps per second: 197, episode reward: 21.262, mean reward: 0.262 [0.038, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.629 [-0.463, 10.100], loss: 0.003664, mae: 0.067341, mean_q: 0.417030
 17656/100000: episode: 236, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 8.183, mean reward: 0.282 [0.185, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.411, 10.100], loss: 0.003895, mae: 0.067918, mean_q: 0.419121
 17693/100000: episode: 237, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 10.904, mean reward: 0.295 [0.167, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.168, 10.100], loss: 0.003364, mae: 0.064284, mean_q: 0.426630
 17703/100000: episode: 238, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 2.229, mean reward: 0.223 [0.126, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.084, 10.100], loss: 0.004048, mae: 0.068600, mean_q: 0.419325
 17732/100000: episode: 239, duration: 0.171s, episode steps: 29, steps per second: 169, episode reward: 11.148, mean reward: 0.384 [0.223, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.267, 10.100], loss: 0.003777, mae: 0.068529, mean_q: 0.424946
[Info] New level: 0.759787917137146 | Considering 10/90 traces
 17769/100000: episode: 240, duration: 4.165s, episode steps: 37, steps per second: 9, episode reward: 13.578, mean reward: 0.367 [0.226, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.269, 10.100], loss: 0.003426, mae: 0.064982, mean_q: 0.421652
 17788/100000: episode: 241, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 8.840, mean reward: 0.465 [0.405, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.319, 10.100], loss: 0.003830, mae: 0.068792, mean_q: 0.425029
 17795/100000: episode: 242, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 3.312, mean reward: 0.473 [0.399, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.477, 10.100], loss: 0.004210, mae: 0.069831, mean_q: 0.430677
 17817/100000: episode: 243, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 8.490, mean reward: 0.386 [0.290, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.286, 10.100], loss: 0.003952, mae: 0.068875, mean_q: 0.449916
 17833/100000: episode: 244, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 5.445, mean reward: 0.340 [0.282, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.338, 10.100], loss: 0.003458, mae: 0.065083, mean_q: 0.426132
 17850/100000: episode: 245, duration: 0.116s, episode steps: 17, steps per second: 146, episode reward: 7.251, mean reward: 0.427 [0.350, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.475, 10.100], loss: 0.003780, mae: 0.068132, mean_q: 0.442117
 17867/100000: episode: 246, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 7.551, mean reward: 0.444 [0.348, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.554, 10.100], loss: 0.003507, mae: 0.067233, mean_q: 0.440067
 17884/100000: episode: 247, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 5.746, mean reward: 0.338 [0.245, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.253, 10.100], loss: 0.003027, mae: 0.062002, mean_q: 0.448032
 17891/100000: episode: 248, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 3.125, mean reward: 0.446 [0.402, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.402, 10.100], loss: 0.005065, mae: 0.072805, mean_q: 0.455723
 17916/100000: episode: 249, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 10.094, mean reward: 0.404 [0.326, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.673, 10.100], loss: 0.003265, mae: 0.063466, mean_q: 0.446733
 17932/100000: episode: 250, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 6.209, mean reward: 0.388 [0.333, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.198, 10.100], loss: 0.003274, mae: 0.062338, mean_q: 0.435698
 17949/100000: episode: 251, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 5.538, mean reward: 0.326 [0.250, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.982, 10.100], loss: 0.003351, mae: 0.065128, mean_q: 0.439042
 17966/100000: episode: 252, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 7.255, mean reward: 0.427 [0.280, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.510, 10.100], loss: 0.003209, mae: 0.063557, mean_q: 0.445118
 17988/100000: episode: 253, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 9.288, mean reward: 0.422 [0.324, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.302, 10.100], loss: 0.003310, mae: 0.064315, mean_q: 0.441890
 18005/100000: episode: 254, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 6.851, mean reward: 0.403 [0.205, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.392, 10.100], loss: 0.003186, mae: 0.062331, mean_q: 0.459597
 18014/100000: episode: 255, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.393, mean reward: 0.377 [0.349, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.448, 10.100], loss: 0.003016, mae: 0.061444, mean_q: 0.460375
 18036/100000: episode: 256, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 9.550, mean reward: 0.434 [0.297, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.207, 10.100], loss: 0.003528, mae: 0.066057, mean_q: 0.454122
 18058/100000: episode: 257, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 9.757, mean reward: 0.443 [0.231, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.271, 10.100], loss: 0.003353, mae: 0.064306, mean_q: 0.438416
 18075/100000: episode: 258, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 9.082, mean reward: 0.534 [0.440, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.628, 10.100], loss: 0.003862, mae: 0.066201, mean_q: 0.462734
 18092/100000: episode: 259, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 7.763, mean reward: 0.457 [0.391, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.320, 10.100], loss: 0.003492, mae: 0.066187, mean_q: 0.454082
 18114/100000: episode: 260, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 9.173, mean reward: 0.417 [0.289, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.273, 10.100], loss: 0.003556, mae: 0.065814, mean_q: 0.472484
 18131/100000: episode: 261, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 6.574, mean reward: 0.387 [0.318, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.477, 10.100], loss: 0.003347, mae: 0.066207, mean_q: 0.466589
 18148/100000: episode: 262, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 8.321, mean reward: 0.489 [0.349, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.646, 10.100], loss: 0.003995, mae: 0.069361, mean_q: 0.458436
 18157/100000: episode: 263, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 4.275, mean reward: 0.475 [0.380, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.544, 10.100], loss: 0.002797, mae: 0.058449, mean_q: 0.467825
 18164/100000: episode: 264, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 2.573, mean reward: 0.368 [0.309, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.450, 10.100], loss: 0.003114, mae: 0.059269, mean_q: 0.464685
 18186/100000: episode: 265, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 7.924, mean reward: 0.360 [0.163, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.079, 10.100], loss: 0.003433, mae: 0.064988, mean_q: 0.463639
 18195/100000: episode: 266, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 3.864, mean reward: 0.429 [0.335, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.598, 10.100], loss: 0.003384, mae: 0.064439, mean_q: 0.461184
 18204/100000: episode: 267, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 3.179, mean reward: 0.353 [0.290, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.465, 10.100], loss: 0.002867, mae: 0.060490, mean_q: 0.463785
 18213/100000: episode: 268, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 4.086, mean reward: 0.454 [0.331, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.633, 10.100], loss: 0.003258, mae: 0.061703, mean_q: 0.488673
 18238/100000: episode: 269, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 10.650, mean reward: 0.426 [0.326, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.797, 10.100], loss: 0.003004, mae: 0.060776, mean_q: 0.464638
 18257/100000: episode: 270, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 7.374, mean reward: 0.388 [0.309, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.439, 10.100], loss: 0.003122, mae: 0.061735, mean_q: 0.453833
 18274/100000: episode: 271, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 7.113, mean reward: 0.418 [0.370, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.393, 10.100], loss: 0.003312, mae: 0.061965, mean_q: 0.470143
 18291/100000: episode: 272, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 8.574, mean reward: 0.504 [0.446, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.342, 10.100], loss: 0.003622, mae: 0.063653, mean_q: 0.472422
 18313/100000: episode: 273, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 8.236, mean reward: 0.374 [0.184, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.928, 10.100], loss: 0.003972, mae: 0.070328, mean_q: 0.492588
 18393/100000: episode: 274, duration: 0.417s, episode steps: 80, steps per second: 192, episode reward: 22.870, mean reward: 0.286 [0.128, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.646 [-1.332, 10.100], loss: 0.003783, mae: 0.068250, mean_q: 0.492068
 18400/100000: episode: 275, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 3.077, mean reward: 0.440 [0.321, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.474, 10.100], loss: 0.002793, mae: 0.059296, mean_q: 0.488413
 18417/100000: episode: 276, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 7.650, mean reward: 0.450 [0.354, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.371, 10.100], loss: 0.003471, mae: 0.066192, mean_q: 0.497297
 18434/100000: episode: 277, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 7.955, mean reward: 0.468 [0.327, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.390, 10.100], loss: 0.003633, mae: 0.065971, mean_q: 0.495445
 18514/100000: episode: 278, duration: 0.432s, episode steps: 80, steps per second: 185, episode reward: 14.596, mean reward: 0.182 [0.032, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.640 [-1.249, 10.163], loss: 0.003757, mae: 0.067703, mean_q: 0.491517
 18539/100000: episode: 279, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 10.762, mean reward: 0.430 [0.344, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.142, 10.100], loss: 0.003919, mae: 0.067711, mean_q: 0.479668
 18548/100000: episode: 280, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 3.606, mean reward: 0.401 [0.351, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.472, 10.100], loss: 0.003165, mae: 0.064999, mean_q: 0.502792
 18570/100000: episode: 281, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 8.031, mean reward: 0.365 [0.166, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.134, 10.100], loss: 0.002938, mae: 0.059700, mean_q: 0.504141
 18595/100000: episode: 282, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 9.283, mean reward: 0.371 [0.293, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.309, 10.100], loss: 0.003684, mae: 0.067926, mean_q: 0.493182
 18602/100000: episode: 283, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 3.072, mean reward: 0.439 [0.374, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.478, 10.100], loss: 0.003149, mae: 0.059687, mean_q: 0.485851
 18609/100000: episode: 284, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 3.276, mean reward: 0.468 [0.424, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.878, 10.100], loss: 0.003838, mae: 0.069567, mean_q: 0.496131
 18626/100000: episode: 285, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 6.640, mean reward: 0.391 [0.266, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.323, 10.100], loss: 0.003786, mae: 0.067417, mean_q: 0.486757
 18642/100000: episode: 286, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 6.368, mean reward: 0.398 [0.340, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.714, 10.100], loss: 0.003606, mae: 0.066669, mean_q: 0.494970
 18722/100000: episode: 287, duration: 0.437s, episode steps: 80, steps per second: 183, episode reward: 20.819, mean reward: 0.260 [0.011, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.659 [-0.997, 10.350], loss: 0.003280, mae: 0.063511, mean_q: 0.501018
 18731/100000: episode: 288, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.537, mean reward: 0.393 [0.319, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.317, 10.100], loss: 0.003377, mae: 0.066692, mean_q: 0.525666
 18738/100000: episode: 289, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.512, mean reward: 0.502 [0.396, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.414, 10.100], loss: 0.003300, mae: 0.064730, mean_q: 0.515074
 18818/100000: episode: 290, duration: 0.435s, episode steps: 80, steps per second: 184, episode reward: 16.906, mean reward: 0.211 [0.008, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-1.119, 10.100], loss: 0.003728, mae: 0.067024, mean_q: 0.510366
 18840/100000: episode: 291, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 10.538, mean reward: 0.479 [0.405, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.433, 10.100], loss: 0.003341, mae: 0.065156, mean_q: 0.495536
 18862/100000: episode: 292, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 7.603, mean reward: 0.346 [0.197, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.249, 10.100], loss: 0.003566, mae: 0.065409, mean_q: 0.520161
 18942/100000: episode: 293, duration: 0.441s, episode steps: 80, steps per second: 181, episode reward: 21.080, mean reward: 0.263 [0.085, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.181, 10.100], loss: 0.003500, mae: 0.065822, mean_q: 0.511291
 18958/100000: episode: 294, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 7.883, mean reward: 0.493 [0.373, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.323, 10.100], loss: 0.003837, mae: 0.069471, mean_q: 0.495935
 19038/100000: episode: 295, duration: 0.393s, episode steps: 80, steps per second: 204, episode reward: 18.859, mean reward: 0.236 [0.009, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.653 [-0.559, 10.107], loss: 0.003277, mae: 0.064233, mean_q: 0.517569
 19060/100000: episode: 296, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 9.313, mean reward: 0.423 [0.068, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.988, 10.100], loss: 0.003327, mae: 0.063853, mean_q: 0.532326
 19076/100000: episode: 297, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 7.332, mean reward: 0.458 [0.414, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.374, 10.100], loss: 0.003289, mae: 0.062911, mean_q: 0.516059
 19156/100000: episode: 298, duration: 0.419s, episode steps: 80, steps per second: 191, episode reward: 20.668, mean reward: 0.258 [0.051, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.642 [-0.648, 10.156], loss: 0.003704, mae: 0.067859, mean_q: 0.527036
 19178/100000: episode: 299, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 7.771, mean reward: 0.353 [0.278, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.466, 10.100], loss: 0.003288, mae: 0.064006, mean_q: 0.527524
 19195/100000: episode: 300, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 6.990, mean reward: 0.411 [0.316, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.361, 10.100], loss: 0.003273, mae: 0.063167, mean_q: 0.524868
 19217/100000: episode: 301, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 11.243, mean reward: 0.511 [0.443, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.065, 10.100], loss: 0.003785, mae: 0.067349, mean_q: 0.537260
 19242/100000: episode: 302, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 8.654, mean reward: 0.346 [0.246, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.178, 10.100], loss: 0.003288, mae: 0.063651, mean_q: 0.534736
 19267/100000: episode: 303, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 11.713, mean reward: 0.469 [0.341, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.369, 10.100], loss: 0.003361, mae: 0.065428, mean_q: 0.526490
 19283/100000: episode: 304, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 6.863, mean reward: 0.429 [0.355, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.409, 10.100], loss: 0.004144, mae: 0.070243, mean_q: 0.509538
 19363/100000: episode: 305, duration: 0.400s, episode steps: 80, steps per second: 200, episode reward: 19.308, mean reward: 0.241 [0.013, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-1.570, 10.255], loss: 0.003730, mae: 0.068711, mean_q: 0.546360
 19388/100000: episode: 306, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 13.074, mean reward: 0.523 [0.386, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.328, 10.100], loss: 0.003898, mae: 0.069520, mean_q: 0.552625
 19404/100000: episode: 307, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 5.616, mean reward: 0.351 [0.232, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.295, 10.100], loss: 0.003192, mae: 0.063856, mean_q: 0.541129
 19426/100000: episode: 308, duration: 0.128s, episode steps: 22, steps per second: 173, episode reward: 7.953, mean reward: 0.361 [0.233, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.397, 10.100], loss: 0.003227, mae: 0.063400, mean_q: 0.538335
 19445/100000: episode: 309, duration: 0.126s, episode steps: 19, steps per second: 151, episode reward: 7.171, mean reward: 0.377 [0.258, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.179, 10.100], loss: 0.003741, mae: 0.069689, mean_q: 0.548649
 19467/100000: episode: 310, duration: 0.119s, episode steps: 22, steps per second: 186, episode reward: 8.003, mean reward: 0.364 [0.254, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.368, 10.100], loss: 0.004064, mae: 0.070721, mean_q: 0.572987
 19476/100000: episode: 311, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 3.282, mean reward: 0.365 [0.307, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.975, 10.100], loss: 0.003244, mae: 0.062975, mean_q: 0.531719
 19493/100000: episode: 312, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 3.969, mean reward: 0.233 [0.159, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.148, 10.100], loss: 0.003451, mae: 0.067849, mean_q: 0.548807
 19512/100000: episode: 313, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 9.876, mean reward: 0.520 [0.448, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.680, 10.100], loss: 0.003406, mae: 0.065424, mean_q: 0.532791
 19529/100000: episode: 314, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 8.611, mean reward: 0.507 [0.405, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.747, 10.100], loss: 0.003310, mae: 0.062630, mean_q: 0.558534
 19554/100000: episode: 315, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 7.980, mean reward: 0.319 [0.154, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.821, 10.100], loss: 0.003625, mae: 0.066159, mean_q: 0.570973
 19561/100000: episode: 316, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 3.525, mean reward: 0.504 [0.436, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.505, 10.100], loss: 0.003145, mae: 0.063513, mean_q: 0.561236
 19583/100000: episode: 317, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 8.950, mean reward: 0.407 [0.236, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.297, 10.100], loss: 0.003340, mae: 0.064744, mean_q: 0.564241
 19599/100000: episode: 318, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.400, mean reward: 0.400 [0.273, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.385, 10.100], loss: 0.003425, mae: 0.064539, mean_q: 0.562053
 19616/100000: episode: 319, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 8.853, mean reward: 0.521 [0.419, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.444, 10.100], loss: 0.004097, mae: 0.071009, mean_q: 0.570818
 19638/100000: episode: 320, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 9.826, mean reward: 0.447 [0.380, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.151, 10.100], loss: 0.003671, mae: 0.067838, mean_q: 0.558407
 19718/100000: episode: 321, duration: 0.410s, episode steps: 80, steps per second: 195, episode reward: 15.854, mean reward: 0.198 [0.033, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.654 [-0.772, 10.388], loss: 0.003455, mae: 0.065521, mean_q: 0.580241
 19798/100000: episode: 322, duration: 0.452s, episode steps: 80, steps per second: 177, episode reward: 14.403, mean reward: 0.180 [0.016, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.644 [-0.924, 10.159], loss: 0.003498, mae: 0.066006, mean_q: 0.577981
 19823/100000: episode: 323, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 10.365, mean reward: 0.415 [0.176, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.453, 10.100], loss: 0.003198, mae: 0.063169, mean_q: 0.564104
 19840/100000: episode: 324, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 8.058, mean reward: 0.474 [0.415, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.369, 10.100], loss: 0.003386, mae: 0.065286, mean_q: 0.561149
 19857/100000: episode: 325, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 7.008, mean reward: 0.412 [0.308, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.479, 10.100], loss: 0.003292, mae: 0.062838, mean_q: 0.567986
 19864/100000: episode: 326, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 3.353, mean reward: 0.479 [0.416, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.494, 10.100], loss: 0.003747, mae: 0.069757, mean_q: 0.611398
 19889/100000: episode: 327, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 11.552, mean reward: 0.462 [0.325, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.481, 10.100], loss: 0.003428, mae: 0.065153, mean_q: 0.573622
 19914/100000: episode: 328, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 10.207, mean reward: 0.408 [0.324, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.983, 10.100], loss: 0.003102, mae: 0.062364, mean_q: 0.577211
 19936/100000: episode: 329, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 9.014, mean reward: 0.410 [0.336, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.129, 10.100], loss: 0.003550, mae: 0.065598, mean_q: 0.562957
[Info] New level: 0.9545800685882568 | Considering 10/90 traces
 19953/100000: episode: 330, duration: 4.064s, episode steps: 17, steps per second: 4, episode reward: 8.103, mean reward: 0.477 [0.383, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.285, 10.100], loss: 0.003396, mae: 0.065485, mean_q: 0.591113
 19974/100000: episode: 331, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 7.856, mean reward: 0.374 [0.237, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.836, 10.100], loss: 0.003437, mae: 0.065726, mean_q: 0.577137
 19990/100000: episode: 332, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 8.319, mean reward: 0.520 [0.472, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.349, 10.100], loss: 0.003781, mae: 0.067348, mean_q: 0.579187
 20010/100000: episode: 333, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 9.155, mean reward: 0.458 [0.355, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.519, 10.100], loss: 0.003560, mae: 0.066773, mean_q: 0.580634
 20018/100000: episode: 334, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 4.069, mean reward: 0.509 [0.451, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.401, 10.100], loss: 0.003157, mae: 0.063423, mean_q: 0.590001
 20039/100000: episode: 335, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 9.587, mean reward: 0.457 [0.384, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.297, 10.100], loss: 0.003437, mae: 0.064605, mean_q: 0.579224
 20059/100000: episode: 336, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 9.882, mean reward: 0.494 [0.417, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.581, 10.100], loss: 0.003383, mae: 0.063683, mean_q: 0.589034
 20082/100000: episode: 337, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 12.009, mean reward: 0.522 [0.408, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.275, 10.100], loss: 0.003433, mae: 0.065667, mean_q: 0.591099
 20105/100000: episode: 338, duration: 0.114s, episode steps: 23, steps per second: 203, episode reward: 10.137, mean reward: 0.441 [0.280, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.449, 10.100], loss: 0.003295, mae: 0.063961, mean_q: 0.584548
 20128/100000: episode: 339, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 9.449, mean reward: 0.411 [0.330, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.503, 10.100], loss: 0.002918, mae: 0.060488, mean_q: 0.585976
 20136/100000: episode: 340, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 3.111, mean reward: 0.389 [0.330, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.416, 10.100], loss: 0.003270, mae: 0.061529, mean_q: 0.589105
 20157/100000: episode: 341, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 7.652, mean reward: 0.364 [0.279, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.559, 10.100], loss: 0.003485, mae: 0.067222, mean_q: 0.573443
 20178/100000: episode: 342, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 9.835, mean reward: 0.468 [0.360, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.976, 10.100], loss: 0.003432, mae: 0.065518, mean_q: 0.604412
 20199/100000: episode: 343, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 10.099, mean reward: 0.481 [0.399, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.603, 10.100], loss: 0.003236, mae: 0.064133, mean_q: 0.588441
 20215/100000: episode: 344, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 8.925, mean reward: 0.558 [0.475, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.523, 10.100], loss: 0.003311, mae: 0.064326, mean_q: 0.591910
 20223/100000: episode: 345, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 3.634, mean reward: 0.454 [0.409, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.718, 10.100], loss: 0.003608, mae: 0.067477, mean_q: 0.592968
 20246/100000: episode: 346, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 8.329, mean reward: 0.362 [0.232, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.161, 10.100], loss: 0.003773, mae: 0.067430, mean_q: 0.585671
 20263/100000: episode: 347, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 8.594, mean reward: 0.506 [0.327, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.874, 10.100], loss: 0.003467, mae: 0.066070, mean_q: 0.596895
 20271/100000: episode: 348, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.654, mean reward: 0.582 [0.539, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.580, 10.100], loss: 0.003734, mae: 0.065491, mean_q: 0.571213
 20292/100000: episode: 349, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 8.776, mean reward: 0.418 [0.297, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.492, 10.100], loss: 0.003679, mae: 0.068572, mean_q: 0.596346
 20306/100000: episode: 350, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 7.221, mean reward: 0.516 [0.445, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.494, 10.100], loss: 0.003203, mae: 0.062872, mean_q: 0.601949
 20322/100000: episode: 351, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 8.411, mean reward: 0.526 [0.455, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.392, 10.100], loss: 0.003758, mae: 0.067353, mean_q: 0.597535
 20338/100000: episode: 352, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 8.454, mean reward: 0.528 [0.475, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.549, 10.100], loss: 0.003206, mae: 0.064415, mean_q: 0.634135
 20346/100000: episode: 353, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.724, mean reward: 0.466 [0.384, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.533, 10.100], loss: 0.003056, mae: 0.061561, mean_q: 0.595936
 20367/100000: episode: 354, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 9.716, mean reward: 0.463 [0.386, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.515, 10.100], loss: 0.003095, mae: 0.062929, mean_q: 0.604442
 20384/100000: episode: 355, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 7.597, mean reward: 0.447 [0.361, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.235, 10.100], loss: 0.003344, mae: 0.064721, mean_q: 0.607482
 20392/100000: episode: 356, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 3.896, mean reward: 0.487 [0.440, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.517, 10.100], loss: 0.003245, mae: 0.062634, mean_q: 0.609268
 20413/100000: episode: 357, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 9.187, mean reward: 0.437 [0.365, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.578, 10.100], loss: 0.003762, mae: 0.067936, mean_q: 0.607610
 20429/100000: episode: 358, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 8.163, mean reward: 0.510 [0.372, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.227, 10.100], loss: 0.002890, mae: 0.058823, mean_q: 0.600087
 20449/100000: episode: 359, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 8.521, mean reward: 0.426 [0.314, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.341, 10.100], loss: 0.002621, mae: 0.057178, mean_q: 0.616250
 20469/100000: episode: 360, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 8.441, mean reward: 0.422 [0.254, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.249, 10.100], loss: 0.003337, mae: 0.065633, mean_q: 0.599640
 20477/100000: episode: 361, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 4.538, mean reward: 0.567 [0.528, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.439, 10.100], loss: 0.004201, mae: 0.071115, mean_q: 0.628348
 20485/100000: episode: 362, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 2.947, mean reward: 0.368 [0.278, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.544, 10.100], loss: 0.003005, mae: 0.063450, mean_q: 0.631471
 20506/100000: episode: 363, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 11.720, mean reward: 0.558 [0.411, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.634, 10.100], loss: 0.003428, mae: 0.065554, mean_q: 0.613309
 20522/100000: episode: 364, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 7.633, mean reward: 0.477 [0.381, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.731, 10.100], loss: 0.003341, mae: 0.063154, mean_q: 0.600692
 20542/100000: episode: 365, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 11.704, mean reward: 0.585 [0.465, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.430, 10.100], loss: 0.002914, mae: 0.059171, mean_q: 0.622065
 20558/100000: episode: 366, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 8.462, mean reward: 0.529 [0.452, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.633, 10.100], loss: 0.003479, mae: 0.065898, mean_q: 0.641414
 20579/100000: episode: 367, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 9.848, mean reward: 0.469 [0.376, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.135, 10.100], loss: 0.003501, mae: 0.065592, mean_q: 0.613388
 20595/100000: episode: 368, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 8.552, mean reward: 0.534 [0.478, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.329, 10.100], loss: 0.002938, mae: 0.059750, mean_q: 0.640547
 20612/100000: episode: 369, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 8.557, mean reward: 0.503 [0.395, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.705, 10.100], loss: 0.003141, mae: 0.060661, mean_q: 0.626873
 20628/100000: episode: 370, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 8.985, mean reward: 0.562 [0.459, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.393, 10.100], loss: 0.003226, mae: 0.063753, mean_q: 0.624242
 20645/100000: episode: 371, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 8.132, mean reward: 0.478 [0.423, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.493, 10.100], loss: 0.002908, mae: 0.059925, mean_q: 0.619036
 20653/100000: episode: 372, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 3.906, mean reward: 0.488 [0.347, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.387, 10.100], loss: 0.003173, mae: 0.063933, mean_q: 0.659327
[RESULT] FALSIFICATION!
 20663/100000: episode: 373, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 14.941, mean reward: 1.494 [0.483, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.379, 10.020], loss: 0.002680, mae: 0.058971, mean_q: 0.642245
 20680/100000: episode: 374, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 8.980, mean reward: 0.528 [0.440, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.406, 10.100], loss: 0.003789, mae: 0.067776, mean_q: 0.636223
 20694/100000: episode: 375, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 7.338, mean reward: 0.524 [0.472, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.371, 10.100], loss: 0.098133, mae: 0.113034, mean_q: 0.683289
 20714/100000: episode: 376, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 8.188, mean reward: 0.409 [0.272, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.251, 10.100], loss: 0.007650, mae: 0.100822, mean_q: 0.618032
 20730/100000: episode: 377, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 8.113, mean reward: 0.507 [0.385, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.411, 10.100], loss: 0.003912, mae: 0.071325, mean_q: 0.650762
 20744/100000: episode: 378, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 6.810, mean reward: 0.486 [0.398, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.102, 10.100], loss: 0.003958, mae: 0.069707, mean_q: 0.636016
 20752/100000: episode: 379, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 3.597, mean reward: 0.450 [0.416, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.429, 10.100], loss: 0.003666, mae: 0.067345, mean_q: 0.665578
 20775/100000: episode: 380, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 11.481, mean reward: 0.499 [0.364, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.568, 10.100], loss: 0.060690, mae: 0.096920, mean_q: 0.652950
 20785/100000: episode: 381, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 5.493, mean reward: 0.549 [0.500, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.839, 10.100], loss: 0.004171, mae: 0.071932, mean_q: 0.651007
 20795/100000: episode: 382, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 4.836, mean reward: 0.484 [0.413, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.290, 10.100], loss: 0.003582, mae: 0.066908, mean_q: 0.637460
 20816/100000: episode: 383, duration: 0.100s, episode steps: 21, steps per second: 209, episode reward: 8.435, mean reward: 0.402 [0.323, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.537, 10.100], loss: 0.003576, mae: 0.066797, mean_q: 0.653134
 20837/100000: episode: 384, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 8.156, mean reward: 0.388 [0.268, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.142, 10.100], loss: 0.003431, mae: 0.066256, mean_q: 0.662585
 20851/100000: episode: 385, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 7.256, mean reward: 0.518 [0.437, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.478, 10.100], loss: 0.002987, mae: 0.062032, mean_q: 0.660286
 20859/100000: episode: 386, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 4.013, mean reward: 0.502 [0.446, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.497, 10.100], loss: 0.004419, mae: 0.074125, mean_q: 0.685084
 20869/100000: episode: 387, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 4.702, mean reward: 0.470 [0.428, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.310, 10.100], loss: 0.004564, mae: 0.075025, mean_q: 0.656512
 20877/100000: episode: 388, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 3.438, mean reward: 0.430 [0.382, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.814, 10.100], loss: 0.003207, mae: 0.065304, mean_q: 0.668848
 20893/100000: episode: 389, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 7.701, mean reward: 0.481 [0.416, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.908, 10.100], loss: 0.003769, mae: 0.069306, mean_q: 0.653248
 20901/100000: episode: 390, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 4.258, mean reward: 0.532 [0.469, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.442, 10.100], loss: 0.004018, mae: 0.071205, mean_q: 0.643640
 20911/100000: episode: 391, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 4.796, mean reward: 0.480 [0.433, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.325, 10.100], loss: 0.003599, mae: 0.067673, mean_q: 0.646265
 20925/100000: episode: 392, duration: 0.097s, episode steps: 14, steps per second: 145, episode reward: 6.859, mean reward: 0.490 [0.404, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.940, 10.100], loss: 0.096226, mae: 0.106946, mean_q: 0.679560
 20941/100000: episode: 393, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 7.507, mean reward: 0.469 [0.420, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.417, 10.100], loss: 0.005084, mae: 0.079983, mean_q: 0.647906
 20957/100000: episode: 394, duration: 0.084s, episode steps: 16, steps per second: 192, episode reward: 7.471, mean reward: 0.467 [0.399, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.754, 10.100], loss: 0.003674, mae: 0.068253, mean_q: 0.655581
 20974/100000: episode: 395, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 8.208, mean reward: 0.483 [0.327, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.297, 10.100], loss: 0.003131, mae: 0.062268, mean_q: 0.652256
[RESULT] FALSIFICATION!
 20988/100000: episode: 396, duration: 0.087s, episode steps: 14, steps per second: 162, episode reward: 17.479, mean reward: 1.248 [0.517, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.562, 10.093], loss: 0.003613, mae: 0.067364, mean_q: 0.677705
 20996/100000: episode: 397, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 4.761, mean reward: 0.595 [0.520, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.471, 10.100], loss: 0.002785, mae: 0.060291, mean_q: 0.669082
 21017/100000: episode: 398, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 8.613, mean reward: 0.410 [0.238, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.257, 10.100], loss: 0.003034, mae: 0.061204, mean_q: 0.684837
 21033/100000: episode: 399, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 8.229, mean reward: 0.514 [0.480, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.906, 10.100], loss: 0.003290, mae: 0.063337, mean_q: 0.655791
 21054/100000: episode: 400, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 6.504, mean reward: 0.310 [0.111, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.255, 10.100], loss: 0.069506, mae: 0.118295, mean_q: 0.676284
 21062/100000: episode: 401, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 3.025, mean reward: 0.378 [0.345, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.402, 10.100], loss: 0.163510, mae: 0.130769, mean_q: 0.693727
 21079/100000: episode: 402, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 9.165, mean reward: 0.539 [0.492, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.513, 10.100], loss: 0.008130, mae: 0.100063, mean_q: 0.657764
 21100/100000: episode: 403, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 10.611, mean reward: 0.505 [0.459, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.377, 10.100], loss: 0.067026, mae: 0.108646, mean_q: 0.701827
 21120/100000: episode: 404, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 7.134, mean reward: 0.357 [0.248, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.358, 10.100], loss: 0.070747, mae: 0.106942, mean_q: 0.701602
 21140/100000: episode: 405, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 8.910, mean reward: 0.445 [0.391, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.332, 10.100], loss: 0.069602, mae: 0.106013, mean_q: 0.696813
 21150/100000: episode: 406, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 5.132, mean reward: 0.513 [0.419, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.345, 10.100], loss: 0.005368, mae: 0.078707, mean_q: 0.691883
 21166/100000: episode: 407, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 6.812, mean reward: 0.426 [0.331, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.322, 10.100], loss: 0.005248, mae: 0.074286, mean_q: 0.697433
 21182/100000: episode: 408, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 7.090, mean reward: 0.443 [0.374, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.279, 10.100], loss: 0.005421, mae: 0.078795, mean_q: 0.702403
 21196/100000: episode: 409, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 6.854, mean reward: 0.490 [0.391, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.314, 10.100], loss: 0.004848, mae: 0.072239, mean_q: 0.711785
 21212/100000: episode: 410, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 9.124, mean reward: 0.570 [0.492, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.444, 10.100], loss: 0.084442, mae: 0.102597, mean_q: 0.724790
 21229/100000: episode: 411, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 9.501, mean reward: 0.559 [0.443, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.252, 10.100], loss: 0.006351, mae: 0.083444, mean_q: 0.668325
 21243/100000: episode: 412, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 7.661, mean reward: 0.547 [0.470, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.337, 10.100], loss: 0.005306, mae: 0.080467, mean_q: 0.685801
 21253/100000: episode: 413, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 5.482, mean reward: 0.548 [0.511, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.702, 10.100], loss: 0.005017, mae: 0.076105, mean_q: 0.673039
 21273/100000: episode: 414, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 9.452, mean reward: 0.473 [0.385, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.070, 10.100], loss: 0.070816, mae: 0.113306, mean_q: 0.707175
 21290/100000: episode: 415, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 9.458, mean reward: 0.556 [0.471, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.473, 10.100], loss: 0.005711, mae: 0.078903, mean_q: 0.719167
 21300/100000: episode: 416, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 4.777, mean reward: 0.478 [0.418, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.560, 10.100], loss: 0.004913, mae: 0.073326, mean_q: 0.712932
[RESULT] FALSIFICATION!
 21316/100000: episode: 417, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 17.928, mean reward: 1.120 [0.454, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.493, 10.098], loss: 0.003977, mae: 0.067895, mean_q: 0.689704
 21332/100000: episode: 418, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 8.752, mean reward: 0.547 [0.489, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.547, 10.100], loss: 0.004302, mae: 0.069764, mean_q: 0.697856
 21346/100000: episode: 419, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 7.112, mean reward: 0.508 [0.418, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.326, 10.100], loss: 0.004747, mae: 0.072351, mean_q: 0.696225
[Info] Not found new level, current best level reached = 0.9545800685882568
 21363/100000: episode: 420, duration: 4.034s, episode steps: 17, steps per second: 4, episode reward: 7.700, mean reward: 0.453 [0.295, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.305, 10.100], loss: 0.081627, mae: 0.105464, mean_q: 0.722869
 21463/100000: episode: 421, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 17.747, mean reward: 0.177 [0.026, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.272], loss: 0.031648, mae: 0.089226, mean_q: 0.705672
 21563/100000: episode: 422, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.730, mean reward: 0.147 [0.007, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.156], loss: 0.044615, mae: 0.092119, mean_q: 0.719843
 21663/100000: episode: 423, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.513, mean reward: 0.175 [0.035, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.361], loss: 0.069403, mae: 0.108078, mean_q: 0.717930
 21763/100000: episode: 424, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.140, mean reward: 0.171 [0.005, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.013, 10.283], loss: 0.031588, mae: 0.087786, mean_q: 0.702658
 21863/100000: episode: 425, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.558, mean reward: 0.156 [0.014, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.042, 10.098], loss: 0.017554, mae: 0.075726, mean_q: 0.691637
 21963/100000: episode: 426, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 22.577, mean reward: 0.226 [0.007, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.325, 10.330], loss: 0.016878, mae: 0.073289, mean_q: 0.682917
 22063/100000: episode: 427, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 15.862, mean reward: 0.159 [0.006, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.608, 10.227], loss: 0.017274, mae: 0.075256, mean_q: 0.690284
 22163/100000: episode: 428, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 19.111, mean reward: 0.191 [0.021, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.209, 10.098], loss: 0.043509, mae: 0.087250, mean_q: 0.695811
 22263/100000: episode: 429, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.063, mean reward: 0.161 [0.008, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.970, 10.355], loss: 0.016786, mae: 0.073479, mean_q: 0.677993
 22363/100000: episode: 430, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.611, mean reward: 0.176 [0.017, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.406, 10.342], loss: 0.056238, mae: 0.094746, mean_q: 0.674716
 22463/100000: episode: 431, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 15.614, mean reward: 0.156 [0.021, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.759, 10.158], loss: 0.044221, mae: 0.095260, mean_q: 0.683708
 22563/100000: episode: 432, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 18.213, mean reward: 0.182 [0.013, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.645, 10.304], loss: 0.043514, mae: 0.090582, mean_q: 0.674048
 22663/100000: episode: 433, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 18.188, mean reward: 0.182 [0.010, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.948, 10.207], loss: 0.030419, mae: 0.082477, mean_q: 0.655417
 22763/100000: episode: 434, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.920, mean reward: 0.169 [0.036, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.714, 10.120], loss: 0.016694, mae: 0.074336, mean_q: 0.644012
 22863/100000: episode: 435, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.200, mean reward: 0.152 [0.013, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.014, 10.251], loss: 0.029401, mae: 0.075778, mean_q: 0.637141
 22963/100000: episode: 436, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.407, mean reward: 0.174 [0.017, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.356, 10.284], loss: 0.030582, mae: 0.087850, mean_q: 0.631644
 23063/100000: episode: 437, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.276, mean reward: 0.173 [0.041, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.684, 10.098], loss: 0.070213, mae: 0.110707, mean_q: 0.634524
 23163/100000: episode: 438, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 18.692, mean reward: 0.187 [0.012, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.098], loss: 0.042289, mae: 0.088583, mean_q: 0.614254
 23263/100000: episode: 439, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.890, mean reward: 0.159 [0.014, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.447, 10.210], loss: 0.042817, mae: 0.088266, mean_q: 0.602822
 23363/100000: episode: 440, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.113, mean reward: 0.171 [0.013, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.714, 10.135], loss: 0.016605, mae: 0.074805, mean_q: 0.604331
 23463/100000: episode: 441, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 17.574, mean reward: 0.176 [0.011, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.265, 10.319], loss: 0.081108, mae: 0.101745, mean_q: 0.599996
 23563/100000: episode: 442, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 14.569, mean reward: 0.146 [0.013, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.749, 10.098], loss: 0.016899, mae: 0.076191, mean_q: 0.591075
 23663/100000: episode: 443, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 17.460, mean reward: 0.175 [0.006, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.881, 10.312], loss: 0.003755, mae: 0.068085, mean_q: 0.587007
 23763/100000: episode: 444, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 13.816, mean reward: 0.138 [0.018, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.507, 10.130], loss: 0.016360, mae: 0.070458, mean_q: 0.573573
 23863/100000: episode: 445, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.871, mean reward: 0.169 [0.017, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.568, 10.114], loss: 0.030197, mae: 0.084292, mean_q: 0.580014
 23963/100000: episode: 446, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 21.229, mean reward: 0.212 [0.001, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.032, 10.098], loss: 0.017503, mae: 0.077095, mean_q: 0.567468
 24063/100000: episode: 447, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.152, mean reward: 0.162 [0.001, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.320, 10.177], loss: 0.044830, mae: 0.096497, mean_q: 0.569768
 24163/100000: episode: 448, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 18.452, mean reward: 0.185 [0.047, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.069, 10.098], loss: 0.004249, mae: 0.069998, mean_q: 0.549098
 24263/100000: episode: 449, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 18.455, mean reward: 0.185 [0.020, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.481, 10.227], loss: 0.017428, mae: 0.075947, mean_q: 0.554878
 24363/100000: episode: 450, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 18.248, mean reward: 0.182 [0.036, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.971, 10.187], loss: 0.055777, mae: 0.095146, mean_q: 0.553048
 24463/100000: episode: 451, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.164, mean reward: 0.162 [0.015, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.904, 10.198], loss: 0.017708, mae: 0.077871, mean_q: 0.531609
 24563/100000: episode: 452, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 19.120, mean reward: 0.191 [0.014, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.566, 10.313], loss: 0.029814, mae: 0.080264, mean_q: 0.528651
 24663/100000: episode: 453, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.134, mean reward: 0.151 [0.023, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.870, 10.293], loss: 0.054287, mae: 0.087420, mean_q: 0.537740
 24763/100000: episode: 454, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 14.195, mean reward: 0.142 [0.009, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.617, 10.121], loss: 0.017056, mae: 0.075347, mean_q: 0.529388
 24863/100000: episode: 455, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.304, mean reward: 0.153 [0.029, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.373, 10.098], loss: 0.029827, mae: 0.078347, mean_q: 0.507591
 24963/100000: episode: 456, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 13.496, mean reward: 0.135 [0.002, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.017171, mae: 0.077109, mean_q: 0.508179
 25063/100000: episode: 457, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.297, mean reward: 0.143 [0.018, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.841, 10.214], loss: 0.017126, mae: 0.076568, mean_q: 0.498834
 25163/100000: episode: 458, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.440, mean reward: 0.174 [0.010, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.220, 10.271], loss: 0.017105, mae: 0.075295, mean_q: 0.473506
 25263/100000: episode: 459, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 19.473, mean reward: 0.195 [0.047, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.024, 10.098], loss: 0.042327, mae: 0.087333, mean_q: 0.470548
 25363/100000: episode: 460, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.448, mean reward: 0.154 [0.009, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.166, 10.141], loss: 0.029890, mae: 0.081025, mean_q: 0.460656
 25463/100000: episode: 461, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 14.589, mean reward: 0.146 [0.017, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.497, 10.180], loss: 0.042430, mae: 0.090589, mean_q: 0.439940
 25563/100000: episode: 462, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 17.302, mean reward: 0.173 [0.003, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.922, 10.098], loss: 0.029498, mae: 0.079559, mean_q: 0.428397
 25663/100000: episode: 463, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.145, mean reward: 0.151 [0.019, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.222, 10.098], loss: 0.016862, mae: 0.075016, mean_q: 0.420188
 25763/100000: episode: 464, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.316, mean reward: 0.153 [0.025, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.351, 10.098], loss: 0.003912, mae: 0.067385, mean_q: 0.397118
 25863/100000: episode: 465, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.619, mean reward: 0.146 [0.022, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.986, 10.098], loss: 0.003744, mae: 0.068168, mean_q: 0.387038
 25963/100000: episode: 466, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.636, mean reward: 0.206 [0.040, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.187], loss: 0.016794, mae: 0.073512, mean_q: 0.385889
 26063/100000: episode: 467, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 20.369, mean reward: 0.204 [0.023, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.992, 10.440], loss: 0.029321, mae: 0.078211, mean_q: 0.377502
 26163/100000: episode: 468, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 14.502, mean reward: 0.145 [0.008, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.134], loss: 0.029215, mae: 0.080440, mean_q: 0.371168
 26263/100000: episode: 469, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.057, mean reward: 0.171 [0.014, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.510, 10.181], loss: 0.016884, mae: 0.077191, mean_q: 0.349425
 26363/100000: episode: 470, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.082, mean reward: 0.141 [0.009, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.779, 10.098], loss: 0.003823, mae: 0.067442, mean_q: 0.334743
 26463/100000: episode: 471, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 16.897, mean reward: 0.169 [0.007, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.020, 10.279], loss: 0.003658, mae: 0.067819, mean_q: 0.334846
 26563/100000: episode: 472, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 23.570, mean reward: 0.236 [0.020, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.887, 10.098], loss: 0.003690, mae: 0.067240, mean_q: 0.336504
 26663/100000: episode: 473, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.700, mean reward: 0.167 [0.014, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.851, 10.098], loss: 0.003610, mae: 0.067267, mean_q: 0.330416
 26763/100000: episode: 474, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.780, mean reward: 0.148 [0.024, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.155, 10.098], loss: 0.003670, mae: 0.068182, mean_q: 0.334523
 26863/100000: episode: 475, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.144, mean reward: 0.141 [0.011, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.479, 10.150], loss: 0.003641, mae: 0.067312, mean_q: 0.334445
 26963/100000: episode: 476, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 18.350, mean reward: 0.183 [0.016, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.490, 10.098], loss: 0.003608, mae: 0.067763, mean_q: 0.335019
 27063/100000: episode: 477, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.846, mean reward: 0.158 [0.006, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.324], loss: 0.003697, mae: 0.068078, mean_q: 0.332028
 27163/100000: episode: 478, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 20.666, mean reward: 0.207 [0.014, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.609, 10.098], loss: 0.003330, mae: 0.064184, mean_q: 0.331410
 27263/100000: episode: 479, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.114, mean reward: 0.171 [0.011, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.245, 10.274], loss: 0.003660, mae: 0.067325, mean_q: 0.331386
 27363/100000: episode: 480, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 16.050, mean reward: 0.161 [0.018, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.151], loss: 0.003370, mae: 0.065334, mean_q: 0.331813
 27463/100000: episode: 481, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.388, mean reward: 0.174 [0.017, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.931, 10.098], loss: 0.003692, mae: 0.068141, mean_q: 0.330343
 27563/100000: episode: 482, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.288, mean reward: 0.163 [0.005, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.388, 10.098], loss: 0.003717, mae: 0.067962, mean_q: 0.331208
 27663/100000: episode: 483, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 18.295, mean reward: 0.183 [0.019, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.235, 10.310], loss: 0.003600, mae: 0.067193, mean_q: 0.332759
 27763/100000: episode: 484, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 16.338, mean reward: 0.163 [0.008, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.423, 10.098], loss: 0.003462, mae: 0.066142, mean_q: 0.334018
 27863/100000: episode: 485, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 21.614, mean reward: 0.216 [0.042, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.985, 10.302], loss: 0.003466, mae: 0.065018, mean_q: 0.339516
 27963/100000: episode: 486, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.190, mean reward: 0.152 [0.008, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.141, 10.098], loss: 0.003745, mae: 0.068012, mean_q: 0.338169
 28063/100000: episode: 487, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.592, mean reward: 0.156 [0.006, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.361, 10.098], loss: 0.003557, mae: 0.066752, mean_q: 0.336131
 28163/100000: episode: 488, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.953, mean reward: 0.160 [0.009, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.156], loss: 0.003543, mae: 0.066287, mean_q: 0.331901
 28263/100000: episode: 489, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.469, mean reward: 0.145 [0.011, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.644, 10.116], loss: 0.003638, mae: 0.066545, mean_q: 0.334926
 28363/100000: episode: 490, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 21.265, mean reward: 0.213 [0.018, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.098], loss: 0.003617, mae: 0.066562, mean_q: 0.333301
 28463/100000: episode: 491, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 16.269, mean reward: 0.163 [0.005, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.237], loss: 0.003734, mae: 0.067793, mean_q: 0.335394
 28563/100000: episode: 492, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.962, mean reward: 0.160 [0.012, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.246, 10.098], loss: 0.003773, mae: 0.068718, mean_q: 0.330794
 28663/100000: episode: 493, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 17.998, mean reward: 0.180 [0.010, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.184], loss: 0.003661, mae: 0.066872, mean_q: 0.332362
 28763/100000: episode: 494, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 14.079, mean reward: 0.141 [0.008, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.966, 10.254], loss: 0.003681, mae: 0.067058, mean_q: 0.333143
 28863/100000: episode: 495, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.177, mean reward: 0.162 [0.011, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.592, 10.135], loss: 0.003541, mae: 0.066526, mean_q: 0.332821
 28963/100000: episode: 496, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.530, mean reward: 0.175 [0.019, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.460, 10.400], loss: 0.003483, mae: 0.066023, mean_q: 0.327924
 29063/100000: episode: 497, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.128, mean reward: 0.151 [0.012, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.951, 10.106], loss: 0.003541, mae: 0.066090, mean_q: 0.331073
 29163/100000: episode: 498, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.388, mean reward: 0.174 [0.021, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.635, 10.098], loss: 0.003596, mae: 0.066108, mean_q: 0.328058
 29263/100000: episode: 499, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 16.673, mean reward: 0.167 [0.019, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.577, 10.167], loss: 0.003780, mae: 0.067847, mean_q: 0.328522
 29363/100000: episode: 500, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 14.912, mean reward: 0.149 [0.014, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.003793, mae: 0.068391, mean_q: 0.327430
 29463/100000: episode: 501, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 19.632, mean reward: 0.196 [0.037, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.649, 10.098], loss: 0.003511, mae: 0.065844, mean_q: 0.328938
 29563/100000: episode: 502, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 20.092, mean reward: 0.201 [0.056, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.334, 10.291], loss: 0.003686, mae: 0.068055, mean_q: 0.329526
 29663/100000: episode: 503, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 15.389, mean reward: 0.154 [0.044, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.098], loss: 0.003686, mae: 0.067627, mean_q: 0.326908
 29763/100000: episode: 504, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.079, mean reward: 0.161 [0.016, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.804, 10.098], loss: 0.003492, mae: 0.066041, mean_q: 0.330925
 29863/100000: episode: 505, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 22.420, mean reward: 0.224 [0.040, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.085, 10.145], loss: 0.003838, mae: 0.068302, mean_q: 0.332517
 29963/100000: episode: 506, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.150, mean reward: 0.171 [0.028, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.829, 10.348], loss: 0.003659, mae: 0.067192, mean_q: 0.333982
 30063/100000: episode: 507, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.258, mean reward: 0.183 [0.018, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.136, 10.217], loss: 0.003617, mae: 0.066810, mean_q: 0.338052
 30163/100000: episode: 508, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.884, mean reward: 0.149 [0.003, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.934, 10.098], loss: 0.003736, mae: 0.067851, mean_q: 0.337158
 30263/100000: episode: 509, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 21.241, mean reward: 0.212 [0.018, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.862, 10.248], loss: 0.003637, mae: 0.067537, mean_q: 0.335801
 30363/100000: episode: 510, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.713, mean reward: 0.177 [0.008, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.943, 10.098], loss: 0.003724, mae: 0.068077, mean_q: 0.334664
 30463/100000: episode: 511, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.768, mean reward: 0.148 [0.014, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.172, 10.098], loss: 0.003692, mae: 0.068196, mean_q: 0.341156
 30563/100000: episode: 512, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 13.795, mean reward: 0.138 [0.015, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.879, 10.098], loss: 0.003642, mae: 0.067535, mean_q: 0.338339
 30663/100000: episode: 513, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 20.035, mean reward: 0.200 [0.023, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.022, 10.330], loss: 0.003566, mae: 0.066818, mean_q: 0.337077
 30763/100000: episode: 514, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 20.080, mean reward: 0.201 [0.011, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.706, 10.231], loss: 0.003543, mae: 0.065828, mean_q: 0.338982
 30863/100000: episode: 515, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 18.613, mean reward: 0.186 [0.041, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.367, 10.098], loss: 0.003586, mae: 0.066465, mean_q: 0.340914
 30963/100000: episode: 516, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.554, mean reward: 0.166 [0.011, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.061, 10.199], loss: 0.003882, mae: 0.068689, mean_q: 0.341165
 31063/100000: episode: 517, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.625, mean reward: 0.146 [0.008, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.861, 10.306], loss: 0.003506, mae: 0.065838, mean_q: 0.340073
 31163/100000: episode: 518, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.031, mean reward: 0.150 [0.014, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.112, 10.098], loss: 0.003741, mae: 0.068152, mean_q: 0.340653
 31263/100000: episode: 519, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 16.996, mean reward: 0.170 [0.005, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.956, 10.098], loss: 0.003780, mae: 0.068245, mean_q: 0.340160
[Info] Not found new level, current best level reached = inf
 31363/100000: episode: 520, duration: 4.479s, episode steps: 100, steps per second: 22, episode reward: 15.501, mean reward: 0.155 [0.014, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.472, 10.165], loss: 0.003753, mae: 0.067957, mean_q: 0.337433
 31463/100000: episode: 521, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.261, mean reward: 0.163 [0.016, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.098], loss: 0.003825, mae: 0.068738, mean_q: 0.339058
 31563/100000: episode: 522, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.855, mean reward: 0.159 [0.020, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.932, 10.148], loss: 0.003844, mae: 0.067809, mean_q: 0.339666
 31663/100000: episode: 523, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 15.457, mean reward: 0.155 [0.021, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.038, 10.136], loss: 0.003676, mae: 0.067406, mean_q: 0.334453
 31763/100000: episode: 524, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.887, mean reward: 0.189 [0.023, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.673, 10.098], loss: 0.003585, mae: 0.065984, mean_q: 0.336655
 31863/100000: episode: 525, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.733, mean reward: 0.157 [0.008, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.679, 10.103], loss: 0.003783, mae: 0.068059, mean_q: 0.334912
 31963/100000: episode: 526, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 16.669, mean reward: 0.167 [0.012, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.758, 10.178], loss: 0.003649, mae: 0.067224, mean_q: 0.335649
 32063/100000: episode: 527, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.414, mean reward: 0.164 [0.015, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.250, 10.098], loss: 0.003741, mae: 0.067291, mean_q: 0.335938
 32163/100000: episode: 528, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 19.353, mean reward: 0.194 [0.017, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.171, 10.098], loss: 0.003738, mae: 0.067205, mean_q: 0.336149
 32263/100000: episode: 529, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 18.033, mean reward: 0.180 [0.018, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.207, 10.098], loss: 0.003572, mae: 0.066424, mean_q: 0.334632
 32363/100000: episode: 530, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.030, mean reward: 0.150 [0.016, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.988, 10.148], loss: 0.003614, mae: 0.066462, mean_q: 0.330818
 32463/100000: episode: 531, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 16.646, mean reward: 0.166 [0.008, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.396, 10.368], loss: 0.003801, mae: 0.067731, mean_q: 0.333249
 32563/100000: episode: 532, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 19.162, mean reward: 0.192 [0.015, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.098], loss: 0.003732, mae: 0.067408, mean_q: 0.333882
 32663/100000: episode: 533, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.415, mean reward: 0.174 [0.015, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.530, 10.181], loss: 0.003567, mae: 0.066557, mean_q: 0.334766
 32763/100000: episode: 534, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 20.723, mean reward: 0.207 [0.028, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.437, 10.098], loss: 0.003597, mae: 0.066392, mean_q: 0.336051
 32863/100000: episode: 535, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.307, mean reward: 0.193 [0.051, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.401, 10.224], loss: 0.003938, mae: 0.068643, mean_q: 0.335159
 32963/100000: episode: 536, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.494, mean reward: 0.155 [0.016, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.972, 10.176], loss: 0.003787, mae: 0.067062, mean_q: 0.336151
 33063/100000: episode: 537, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 15.441, mean reward: 0.154 [0.015, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.535, 10.098], loss: 0.003660, mae: 0.066745, mean_q: 0.335421
 33163/100000: episode: 538, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 15.083, mean reward: 0.151 [0.015, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.777, 10.197], loss: 0.003578, mae: 0.066238, mean_q: 0.336767
 33263/100000: episode: 539, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 24.317, mean reward: 0.243 [0.011, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.157, 10.098], loss: 0.003703, mae: 0.067052, mean_q: 0.341080
 33363/100000: episode: 540, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.974, mean reward: 0.190 [0.010, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.385, 10.098], loss: 0.003589, mae: 0.066045, mean_q: 0.341356
 33463/100000: episode: 541, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 17.945, mean reward: 0.179 [0.002, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.837, 10.440], loss: 0.003485, mae: 0.066010, mean_q: 0.339076
 33563/100000: episode: 542, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 17.116, mean reward: 0.171 [0.011, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.365, 10.311], loss: 0.003440, mae: 0.064495, mean_q: 0.339602
 33663/100000: episode: 543, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.189, mean reward: 0.152 [0.009, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.924, 10.256], loss: 0.003580, mae: 0.066839, mean_q: 0.339648
 33763/100000: episode: 544, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 11.927, mean reward: 0.119 [0.006, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.607, 10.098], loss: 0.003547, mae: 0.066159, mean_q: 0.342130
 33863/100000: episode: 545, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 21.211, mean reward: 0.212 [0.016, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.370, 10.098], loss: 0.003624, mae: 0.066801, mean_q: 0.338794
 33963/100000: episode: 546, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 16.946, mean reward: 0.169 [0.015, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.108], loss: 0.003621, mae: 0.066697, mean_q: 0.342566
 34063/100000: episode: 547, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 17.449, mean reward: 0.174 [0.027, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.256, 10.098], loss: 0.003403, mae: 0.065426, mean_q: 0.339033
 34163/100000: episode: 548, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 15.098, mean reward: 0.151 [0.016, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.754, 10.262], loss: 0.003649, mae: 0.067000, mean_q: 0.340850
 34263/100000: episode: 549, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 12.589, mean reward: 0.126 [0.009, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.197], loss: 0.003579, mae: 0.066731, mean_q: 0.339749
 34363/100000: episode: 550, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 13.785, mean reward: 0.138 [0.012, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.964, 10.118], loss: 0.003676, mae: 0.068129, mean_q: 0.340287
 34463/100000: episode: 551, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 27.331, mean reward: 0.273 [0.032, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.759, 10.464], loss: 0.003580, mae: 0.066652, mean_q: 0.337028
 34563/100000: episode: 552, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 19.061, mean reward: 0.191 [0.030, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.935, 10.098], loss: 0.003875, mae: 0.069489, mean_q: 0.336516
 34663/100000: episode: 553, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 16.783, mean reward: 0.168 [0.011, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.730, 10.098], loss: 0.004016, mae: 0.071566, mean_q: 0.344293
 34763/100000: episode: 554, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 18.315, mean reward: 0.183 [0.020, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.678, 10.098], loss: 0.003748, mae: 0.068454, mean_q: 0.344961
 34863/100000: episode: 555, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 20.851, mean reward: 0.209 [0.008, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.110, 10.338], loss: 0.003743, mae: 0.067491, mean_q: 0.340540
 34963/100000: episode: 556, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.417, mean reward: 0.164 [0.013, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.214, 10.208], loss: 0.003826, mae: 0.069050, mean_q: 0.342731
 35063/100000: episode: 557, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 19.728, mean reward: 0.197 [0.021, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.568, 10.098], loss: 0.003753, mae: 0.068302, mean_q: 0.341929
 35163/100000: episode: 558, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 13.414, mean reward: 0.134 [0.007, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.098], loss: 0.003406, mae: 0.065093, mean_q: 0.344243
 35263/100000: episode: 559, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 18.205, mean reward: 0.182 [0.012, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.690, 10.379], loss: 0.003553, mae: 0.067041, mean_q: 0.336039
 35363/100000: episode: 560, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.880, mean reward: 0.179 [0.018, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.874, 10.098], loss: 0.003566, mae: 0.066417, mean_q: 0.340728
 35463/100000: episode: 561, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.622, mean reward: 0.156 [0.042, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.714, 10.178], loss: 0.003537, mae: 0.065990, mean_q: 0.342196
 35563/100000: episode: 562, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.868, mean reward: 0.179 [0.017, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.203, 10.298], loss: 0.003715, mae: 0.067715, mean_q: 0.340647
 35663/100000: episode: 563, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 16.471, mean reward: 0.165 [0.017, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.740, 10.166], loss: 0.003476, mae: 0.065903, mean_q: 0.338523
 35763/100000: episode: 564, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 19.108, mean reward: 0.191 [0.019, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.074, 10.098], loss: 0.003634, mae: 0.067208, mean_q: 0.336981
 35863/100000: episode: 565, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.794, mean reward: 0.178 [0.026, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.418, 10.098], loss: 0.003827, mae: 0.069472, mean_q: 0.341120
 35963/100000: episode: 566, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 20.943, mean reward: 0.209 [0.037, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.852, 10.204], loss: 0.003680, mae: 0.067563, mean_q: 0.339386
 36063/100000: episode: 567, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 12.931, mean reward: 0.129 [0.009, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.410, 10.098], loss: 0.003563, mae: 0.066404, mean_q: 0.342891
 36163/100000: episode: 568, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 21.968, mean reward: 0.220 [0.028, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.292, 10.506], loss: 0.003464, mae: 0.065050, mean_q: 0.340308
 36263/100000: episode: 569, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.968, mean reward: 0.160 [0.020, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.421, 10.227], loss: 0.003843, mae: 0.068400, mean_q: 0.345499
 36363/100000: episode: 570, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 17.312, mean reward: 0.173 [0.016, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.520, 10.112], loss: 0.003712, mae: 0.067991, mean_q: 0.346872
 36463/100000: episode: 571, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.095, mean reward: 0.151 [0.018, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.702, 10.098], loss: 0.003697, mae: 0.067203, mean_q: 0.345769
 36563/100000: episode: 572, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 20.428, mean reward: 0.204 [0.011, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.051, 10.098], loss: 0.003704, mae: 0.067630, mean_q: 0.345243
 36663/100000: episode: 573, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 20.483, mean reward: 0.205 [0.038, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.034, 10.098], loss: 0.003624, mae: 0.067233, mean_q: 0.345926
 36763/100000: episode: 574, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.435, mean reward: 0.144 [0.019, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.719, 10.098], loss: 0.003706, mae: 0.066767, mean_q: 0.347451
 36863/100000: episode: 575, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 13.617, mean reward: 0.136 [0.011, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.681, 10.098], loss: 0.003637, mae: 0.066544, mean_q: 0.350586
 36963/100000: episode: 576, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 12.923, mean reward: 0.129 [0.003, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.340, 10.104], loss: 0.003505, mae: 0.065184, mean_q: 0.346279
 37063/100000: episode: 577, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 22.375, mean reward: 0.224 [0.014, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.156], loss: 0.003622, mae: 0.066860, mean_q: 0.350459
 37163/100000: episode: 578, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.672, mean reward: 0.147 [0.017, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.702, 10.098], loss: 0.003581, mae: 0.066340, mean_q: 0.347661
 37263/100000: episode: 579, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 17.098, mean reward: 0.171 [0.020, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.377, 10.098], loss: 0.003612, mae: 0.066446, mean_q: 0.348248
 37363/100000: episode: 580, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.869, mean reward: 0.169 [0.015, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.950, 10.310], loss: 0.003573, mae: 0.065390, mean_q: 0.345682
 37463/100000: episode: 581, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 12.913, mean reward: 0.129 [0.011, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.771, 10.125], loss: 0.003555, mae: 0.065595, mean_q: 0.346372
 37563/100000: episode: 582, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 20.799, mean reward: 0.208 [0.019, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.075, 10.259], loss: 0.003615, mae: 0.066181, mean_q: 0.345577
 37663/100000: episode: 583, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 12.375, mean reward: 0.124 [0.003, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.677, 10.098], loss: 0.003529, mae: 0.066955, mean_q: 0.342367
 37763/100000: episode: 584, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 16.094, mean reward: 0.161 [0.002, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.380, 10.098], loss: 0.003674, mae: 0.067261, mean_q: 0.341738
 37863/100000: episode: 585, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 25.083, mean reward: 0.251 [0.033, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.990, 10.098], loss: 0.003602, mae: 0.066379, mean_q: 0.335134
 37963/100000: episode: 586, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.010, mean reward: 0.150 [0.008, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.169], loss: 0.003575, mae: 0.066758, mean_q: 0.339958
 38063/100000: episode: 587, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.924, mean reward: 0.159 [0.015, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.461, 10.098], loss: 0.003643, mae: 0.066503, mean_q: 0.342295
 38163/100000: episode: 588, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.810, mean reward: 0.168 [0.007, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.239, 10.098], loss: 0.003605, mae: 0.066511, mean_q: 0.342539
 38263/100000: episode: 589, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.809, mean reward: 0.158 [0.023, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.070, 10.231], loss: 0.003517, mae: 0.065567, mean_q: 0.338975
 38363/100000: episode: 590, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 13.423, mean reward: 0.134 [0.013, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.036, 10.098], loss: 0.003658, mae: 0.066849, mean_q: 0.338258
 38463/100000: episode: 591, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 12.784, mean reward: 0.128 [0.011, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.780, 10.348], loss: 0.003844, mae: 0.068690, mean_q: 0.335994
 38563/100000: episode: 592, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.913, mean reward: 0.159 [0.014, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.280, 10.132], loss: 0.003553, mae: 0.066201, mean_q: 0.332491
 38663/100000: episode: 593, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 17.948, mean reward: 0.179 [0.005, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.195, 10.098], loss: 0.003390, mae: 0.065266, mean_q: 0.334382
 38763/100000: episode: 594, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 14.392, mean reward: 0.144 [0.017, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.301], loss: 0.003402, mae: 0.064236, mean_q: 0.337905
 38863/100000: episode: 595, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 17.737, mean reward: 0.177 [0.037, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.374, 10.098], loss: 0.003586, mae: 0.066324, mean_q: 0.333494
 38963/100000: episode: 596, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.993, mean reward: 0.150 [0.015, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.929, 10.243], loss: 0.003819, mae: 0.068621, mean_q: 0.335090
 39063/100000: episode: 597, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 13.814, mean reward: 0.138 [0.019, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 0.003732, mae: 0.067254, mean_q: 0.330264
 39163/100000: episode: 598, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 19.591, mean reward: 0.196 [0.020, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.149, 10.114], loss: 0.003681, mae: 0.068193, mean_q: 0.331353
 39263/100000: episode: 599, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.396, mean reward: 0.154 [0.029, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.533, 10.150], loss: 0.003813, mae: 0.068138, mean_q: 0.337597
 39363/100000: episode: 600, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 16.195, mean reward: 0.162 [0.013, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.690, 10.185], loss: 0.003712, mae: 0.067072, mean_q: 0.337628
 39463/100000: episode: 601, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 18.921, mean reward: 0.189 [0.024, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.216, 10.193], loss: 0.003914, mae: 0.068670, mean_q: 0.332905
 39563/100000: episode: 602, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.348, mean reward: 0.153 [0.016, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.621, 10.387], loss: 0.003655, mae: 0.067666, mean_q: 0.329516
 39663/100000: episode: 603, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.802, mean reward: 0.148 [0.021, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.467, 10.098], loss: 0.003575, mae: 0.065478, mean_q: 0.328040
 39763/100000: episode: 604, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 14.335, mean reward: 0.143 [0.008, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.514, 10.119], loss: 0.004070, mae: 0.070605, mean_q: 0.334555
 39863/100000: episode: 605, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 12.719, mean reward: 0.127 [0.011, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.273, 10.259], loss: 0.003810, mae: 0.069374, mean_q: 0.329011
 39963/100000: episode: 606, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.020, mean reward: 0.140 [0.009, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.486, 10.098], loss: 0.003819, mae: 0.068602, mean_q: 0.323459
 40063/100000: episode: 607, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 14.808, mean reward: 0.148 [0.016, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.450, 10.146], loss: 0.003517, mae: 0.066071, mean_q: 0.327304
 40163/100000: episode: 608, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 16.744, mean reward: 0.167 [0.023, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.205, 10.098], loss: 0.003872, mae: 0.068843, mean_q: 0.324169
 40263/100000: episode: 609, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 15.614, mean reward: 0.156 [0.017, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.167], loss: 0.003657, mae: 0.066983, mean_q: 0.324093
 40363/100000: episode: 610, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 17.314, mean reward: 0.173 [0.028, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.516, 10.098], loss: 0.003772, mae: 0.068110, mean_q: 0.322134
 40463/100000: episode: 611, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 18.689, mean reward: 0.187 [0.036, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.025, 10.098], loss: 0.003834, mae: 0.068464, mean_q: 0.320085
 40563/100000: episode: 612, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 15.308, mean reward: 0.153 [0.008, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.261, 10.098], loss: 0.003960, mae: 0.069001, mean_q: 0.322215
 40663/100000: episode: 613, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.576, mean reward: 0.166 [0.009, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.098], loss: 0.003836, mae: 0.067923, mean_q: 0.326594
 40763/100000: episode: 614, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.052, mean reward: 0.181 [0.040, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.687, 10.098], loss: 0.003545, mae: 0.066194, mean_q: 0.323292
 40863/100000: episode: 615, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 20.257, mean reward: 0.203 [0.014, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.098], loss: 0.003708, mae: 0.067604, mean_q: 0.321573
 40963/100000: episode: 616, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 20.400, mean reward: 0.204 [0.006, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.172, 10.098], loss: 0.003756, mae: 0.068227, mean_q: 0.322899
 41063/100000: episode: 617, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.592, mean reward: 0.156 [0.022, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.517, 10.323], loss: 0.003565, mae: 0.066693, mean_q: 0.324597
 41163/100000: episode: 618, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 16.445, mean reward: 0.164 [0.013, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.149, 10.128], loss: 0.003543, mae: 0.065185, mean_q: 0.317419
 41263/100000: episode: 619, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.503, mean reward: 0.145 [0.001, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.835, 10.367], loss: 0.003557, mae: 0.065558, mean_q: 0.324832
[Info] Not found new level, current best level reached = inf
 41363/100000: episode: 620, duration: 4.437s, episode steps: 100, steps per second: 23, episode reward: 19.631, mean reward: 0.196 [0.009, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.160], loss: 0.003710, mae: 0.067587, mean_q: 0.324468
 41463/100000: episode: 621, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 21.289, mean reward: 0.213 [0.035, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.515, 10.338], loss: 0.003713, mae: 0.067498, mean_q: 0.324717
 41563/100000: episode: 622, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.104, mean reward: 0.141 [0.019, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.302, 10.098], loss: 0.003705, mae: 0.066752, mean_q: 0.321618
 41663/100000: episode: 623, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.536, mean reward: 0.185 [0.027, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.234], loss: 0.003702, mae: 0.067049, mean_q: 0.321603
 41763/100000: episode: 624, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.661, mean reward: 0.157 [0.004, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.265, 10.206], loss: 0.003659, mae: 0.066632, mean_q: 0.320451
 41863/100000: episode: 625, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.170, mean reward: 0.152 [0.022, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.112], loss: 0.003934, mae: 0.069207, mean_q: 0.323482
 41963/100000: episode: 626, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 16.412, mean reward: 0.164 [0.026, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.710, 10.098], loss: 0.003609, mae: 0.065828, mean_q: 0.323739
 42063/100000: episode: 627, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 17.559, mean reward: 0.176 [0.013, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.254], loss: 0.003686, mae: 0.067066, mean_q: 0.316975
 42163/100000: episode: 628, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 16.757, mean reward: 0.168 [0.030, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.098], loss: 0.003879, mae: 0.069316, mean_q: 0.319955
 42263/100000: episode: 629, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 18.948, mean reward: 0.189 [0.016, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.121, 10.135], loss: 0.003534, mae: 0.065989, mean_q: 0.317528
 42363/100000: episode: 630, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 23.837, mean reward: 0.238 [0.057, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.997, 10.303], loss: 0.003833, mae: 0.068501, mean_q: 0.325436
 42463/100000: episode: 631, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 20.054, mean reward: 0.201 [0.016, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.636, 10.501], loss: 0.003825, mae: 0.068839, mean_q: 0.328190
 42563/100000: episode: 632, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 22.406, mean reward: 0.224 [0.030, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.800, 10.125], loss: 0.003723, mae: 0.067376, mean_q: 0.328460
 42663/100000: episode: 633, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 17.827, mean reward: 0.178 [0.025, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.348, 10.098], loss: 0.003894, mae: 0.068929, mean_q: 0.334287
 42763/100000: episode: 634, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.706, mean reward: 0.167 [0.024, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.484, 10.098], loss: 0.003976, mae: 0.069056, mean_q: 0.330578
 42863/100000: episode: 635, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 16.898, mean reward: 0.169 [0.017, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.077, 10.310], loss: 0.003870, mae: 0.068860, mean_q: 0.328024
 42963/100000: episode: 636, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 18.312, mean reward: 0.183 [0.012, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.436, 10.098], loss: 0.003991, mae: 0.070291, mean_q: 0.326215
 43063/100000: episode: 637, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 13.424, mean reward: 0.134 [0.020, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.442, 10.207], loss: 0.003633, mae: 0.066429, mean_q: 0.326261
 43163/100000: episode: 638, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 17.532, mean reward: 0.175 [0.043, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.269, 10.151], loss: 0.003858, mae: 0.067945, mean_q: 0.331445
 43263/100000: episode: 639, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 13.104, mean reward: 0.131 [0.010, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.107], loss: 0.003897, mae: 0.068947, mean_q: 0.325986
 43363/100000: episode: 640, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 14.787, mean reward: 0.148 [0.010, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.418, 10.182], loss: 0.003812, mae: 0.068010, mean_q: 0.329615
 43463/100000: episode: 641, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 15.064, mean reward: 0.151 [0.011, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.119], loss: 0.004003, mae: 0.069821, mean_q: 0.332148
 43563/100000: episode: 642, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 13.834, mean reward: 0.138 [0.017, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.121, 10.098], loss: 0.003746, mae: 0.067375, mean_q: 0.330615
 43663/100000: episode: 643, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 17.677, mean reward: 0.177 [0.013, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.599, 10.270], loss: 0.004021, mae: 0.069731, mean_q: 0.328725
 43763/100000: episode: 644, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 17.120, mean reward: 0.171 [0.013, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.285, 10.235], loss: 0.003965, mae: 0.069275, mean_q: 0.329878
 43863/100000: episode: 645, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 26.166, mean reward: 0.262 [0.014, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.072, 10.427], loss: 0.003653, mae: 0.066384, mean_q: 0.327769
 43963/100000: episode: 646, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.439, mean reward: 0.164 [0.011, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.311, 10.098], loss: 0.003990, mae: 0.069421, mean_q: 0.335701
 44063/100000: episode: 647, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.960, mean reward: 0.150 [0.019, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.971, 10.235], loss: 0.003728, mae: 0.067801, mean_q: 0.333050
 44163/100000: episode: 648, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 17.215, mean reward: 0.172 [0.010, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.996, 10.295], loss: 0.003739, mae: 0.067330, mean_q: 0.329429
 44263/100000: episode: 649, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.092, mean reward: 0.131 [0.008, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.945, 10.149], loss: 0.003553, mae: 0.065422, mean_q: 0.331662
 44363/100000: episode: 650, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 20.664, mean reward: 0.207 [0.033, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.622, 10.308], loss: 0.003938, mae: 0.069812, mean_q: 0.336818
 44463/100000: episode: 651, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 17.669, mean reward: 0.177 [0.021, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.515, 10.311], loss: 0.003828, mae: 0.068190, mean_q: 0.336246
 44563/100000: episode: 652, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 19.957, mean reward: 0.200 [0.037, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.252], loss: 0.003566, mae: 0.065631, mean_q: 0.334293
 44663/100000: episode: 653, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 22.512, mean reward: 0.225 [0.044, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.055, 10.381], loss: 0.003712, mae: 0.067536, mean_q: 0.336326
 44763/100000: episode: 654, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.151, mean reward: 0.172 [0.012, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.508, 10.145], loss: 0.003718, mae: 0.067787, mean_q: 0.339769
 44863/100000: episode: 655, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.650, mean reward: 0.157 [0.016, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.208, 10.098], loss: 0.003819, mae: 0.067905, mean_q: 0.341108
 44963/100000: episode: 656, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 12.893, mean reward: 0.129 [0.003, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.284, 10.098], loss: 0.003515, mae: 0.065559, mean_q: 0.340897
 45063/100000: episode: 657, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.962, mean reward: 0.160 [0.012, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.183, 10.098], loss: 0.003844, mae: 0.068157, mean_q: 0.338919
 45163/100000: episode: 658, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.105, mean reward: 0.141 [0.004, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.950, 10.098], loss: 0.003795, mae: 0.067426, mean_q: 0.341390
 45263/100000: episode: 659, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 18.203, mean reward: 0.182 [0.023, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.885, 10.310], loss: 0.003570, mae: 0.065568, mean_q: 0.342049
 45363/100000: episode: 660, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 14.461, mean reward: 0.145 [0.013, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.390, 10.342], loss: 0.003564, mae: 0.065559, mean_q: 0.340816
 45463/100000: episode: 661, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.701, mean reward: 0.167 [0.005, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.280, 10.098], loss: 0.003595, mae: 0.065973, mean_q: 0.337692
 45563/100000: episode: 662, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 17.838, mean reward: 0.178 [0.011, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.496, 10.098], loss: 0.003567, mae: 0.065935, mean_q: 0.336393
 45663/100000: episode: 663, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.655, mean reward: 0.177 [0.003, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.361, 10.263], loss: 0.003630, mae: 0.067235, mean_q: 0.340754
 45763/100000: episode: 664, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.744, mean reward: 0.187 [0.013, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.673, 10.413], loss: 0.003434, mae: 0.065278, mean_q: 0.342212
 45863/100000: episode: 665, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 23.370, mean reward: 0.234 [0.025, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.098], loss: 0.003568, mae: 0.066193, mean_q: 0.339767
 45963/100000: episode: 666, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 14.071, mean reward: 0.141 [0.019, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.624, 10.098], loss: 0.003625, mae: 0.066286, mean_q: 0.342144
 46063/100000: episode: 667, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 18.327, mean reward: 0.183 [0.030, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.209], loss: 0.003744, mae: 0.067197, mean_q: 0.342018
 46163/100000: episode: 668, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 15.517, mean reward: 0.155 [0.015, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.650, 10.098], loss: 0.003776, mae: 0.067790, mean_q: 0.342406
 46263/100000: episode: 669, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.886, mean reward: 0.169 [0.012, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.660, 10.256], loss: 0.003520, mae: 0.065138, mean_q: 0.338738
 46363/100000: episode: 670, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 12.067, mean reward: 0.121 [0.036, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.859, 10.104], loss: 0.003673, mae: 0.067189, mean_q: 0.341298
 46463/100000: episode: 671, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 12.978, mean reward: 0.130 [0.009, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.628, 10.147], loss: 0.003689, mae: 0.067714, mean_q: 0.338108
 46563/100000: episode: 672, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.022, mean reward: 0.170 [0.002, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.704, 10.098], loss: 0.003648, mae: 0.067026, mean_q: 0.335709
 46663/100000: episode: 673, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.257, mean reward: 0.173 [0.028, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.523, 10.132], loss: 0.003491, mae: 0.065820, mean_q: 0.337785
 46763/100000: episode: 674, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.269, mean reward: 0.173 [0.005, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.098], loss: 0.003520, mae: 0.065256, mean_q: 0.335081
 46863/100000: episode: 675, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 18.040, mean reward: 0.180 [0.019, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.710, 10.098], loss: 0.003551, mae: 0.065711, mean_q: 0.334405
 46963/100000: episode: 676, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 13.908, mean reward: 0.139 [0.016, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.297, 10.191], loss: 0.003585, mae: 0.066378, mean_q: 0.340272
 47063/100000: episode: 677, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.904, mean reward: 0.159 [0.015, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.589, 10.212], loss: 0.003622, mae: 0.066510, mean_q: 0.339695
 47163/100000: episode: 678, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.160, mean reward: 0.182 [0.036, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.043, 10.240], loss: 0.003515, mae: 0.064885, mean_q: 0.335407
 47263/100000: episode: 679, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 19.438, mean reward: 0.194 [0.012, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.295, 10.190], loss: 0.003653, mae: 0.067014, mean_q: 0.334500
 47363/100000: episode: 680, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 18.104, mean reward: 0.181 [0.006, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.605, 10.637], loss: 0.003456, mae: 0.064675, mean_q: 0.333587
 47463/100000: episode: 681, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 13.802, mean reward: 0.138 [0.007, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.600, 10.159], loss: 0.003622, mae: 0.066819, mean_q: 0.335180
 47563/100000: episode: 682, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.177, mean reward: 0.152 [0.011, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.217], loss: 0.003743, mae: 0.066684, mean_q: 0.332487
 47663/100000: episode: 683, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.494, mean reward: 0.165 [0.017, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.970, 10.171], loss: 0.003703, mae: 0.067170, mean_q: 0.328163
 47763/100000: episode: 684, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.900, mean reward: 0.169 [0.006, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.098], loss: 0.003628, mae: 0.066082, mean_q: 0.326527
 47863/100000: episode: 685, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.556, mean reward: 0.146 [0.022, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.752, 10.098], loss: 0.003553, mae: 0.066286, mean_q: 0.330102
 47963/100000: episode: 686, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 16.243, mean reward: 0.162 [0.041, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.686, 10.098], loss: 0.003640, mae: 0.067085, mean_q: 0.327874
 48063/100000: episode: 687, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 16.912, mean reward: 0.169 [0.009, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.793, 10.098], loss: 0.003490, mae: 0.065705, mean_q: 0.328522
 48163/100000: episode: 688, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 22.312, mean reward: 0.223 [0.009, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.114], loss: 0.003584, mae: 0.066094, mean_q: 0.337247
 48263/100000: episode: 689, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 15.687, mean reward: 0.157 [0.015, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.157, 10.356], loss: 0.003517, mae: 0.066271, mean_q: 0.329961
 48363/100000: episode: 690, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.823, mean reward: 0.158 [0.026, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.365, 10.098], loss: 0.003507, mae: 0.065080, mean_q: 0.332490
 48463/100000: episode: 691, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.185, mean reward: 0.182 [0.029, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.072, 10.098], loss: 0.003590, mae: 0.066482, mean_q: 0.329333
 48563/100000: episode: 692, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 14.134, mean reward: 0.141 [0.008, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.735, 10.098], loss: 0.004101, mae: 0.070356, mean_q: 0.330504
 48663/100000: episode: 693, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.524, mean reward: 0.155 [0.014, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.126], loss: 0.003838, mae: 0.068613, mean_q: 0.333957
 48763/100000: episode: 694, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.047, mean reward: 0.150 [0.012, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.508, 10.138], loss: 0.003546, mae: 0.065828, mean_q: 0.327821
 48863/100000: episode: 695, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 19.515, mean reward: 0.195 [0.024, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.192, 10.366], loss: 0.003851, mae: 0.068592, mean_q: 0.326020
 48963/100000: episode: 696, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.847, mean reward: 0.158 [0.011, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.379, 10.107], loss: 0.003737, mae: 0.067402, mean_q: 0.328927
 49063/100000: episode: 697, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 28.149, mean reward: 0.281 [0.044, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.773, 10.375], loss: 0.003646, mae: 0.066703, mean_q: 0.332776
 49163/100000: episode: 698, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 19.018, mean reward: 0.190 [0.028, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.602, 10.271], loss: 0.003863, mae: 0.069049, mean_q: 0.338415
 49263/100000: episode: 699, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 18.490, mean reward: 0.185 [0.023, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.196], loss: 0.003705, mae: 0.067134, mean_q: 0.336176
 49363/100000: episode: 700, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.953, mean reward: 0.160 [0.005, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.276, 10.098], loss: 0.003894, mae: 0.069185, mean_q: 0.336184
 49463/100000: episode: 701, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.865, mean reward: 0.189 [0.019, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.616, 10.143], loss: 0.003828, mae: 0.068882, mean_q: 0.335544
 49563/100000: episode: 702, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.570, mean reward: 0.156 [0.010, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.121, 10.098], loss: 0.003626, mae: 0.066255, mean_q: 0.332183
 49663/100000: episode: 703, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 13.587, mean reward: 0.136 [0.015, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.098], loss: 0.003846, mae: 0.068690, mean_q: 0.334384
 49763/100000: episode: 704, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.704, mean reward: 0.157 [0.010, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.328], loss: 0.003868, mae: 0.068347, mean_q: 0.330925
 49863/100000: episode: 705, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 18.120, mean reward: 0.181 [0.024, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.482, 10.098], loss: 0.004027, mae: 0.070552, mean_q: 0.331426
 49963/100000: episode: 706, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.433, mean reward: 0.174 [0.005, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.138, 10.098], loss: 0.003784, mae: 0.068047, mean_q: 0.328190
 50063/100000: episode: 707, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 17.101, mean reward: 0.171 [0.020, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.176], loss: 0.003911, mae: 0.069305, mean_q: 0.325534
 50163/100000: episode: 708, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 19.052, mean reward: 0.191 [0.016, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.861, 10.218], loss: 0.003916, mae: 0.069314, mean_q: 0.334242
 50263/100000: episode: 709, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 13.773, mean reward: 0.138 [0.012, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.145, 10.154], loss: 0.003745, mae: 0.068563, mean_q: 0.330559
 50363/100000: episode: 710, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.669, mean reward: 0.157 [0.041, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.375, 10.166], loss: 0.003828, mae: 0.067864, mean_q: 0.332585
 50463/100000: episode: 711, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 14.572, mean reward: 0.146 [0.012, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.483, 10.211], loss: 0.004208, mae: 0.071870, mean_q: 0.332524
 50563/100000: episode: 712, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 17.990, mean reward: 0.180 [0.042, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.862, 10.424], loss: 0.003687, mae: 0.067687, mean_q: 0.327799
 50663/100000: episode: 713, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.251, mean reward: 0.153 [0.006, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.202], loss: 0.003944, mae: 0.069844, mean_q: 0.326849
 50763/100000: episode: 714, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.570, mean reward: 0.156 [0.008, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.034, 10.118], loss: 0.003589, mae: 0.066699, mean_q: 0.328477
 50863/100000: episode: 715, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.021, mean reward: 0.150 [0.023, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.245], loss: 0.003913, mae: 0.069039, mean_q: 0.326216
 50963/100000: episode: 716, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.810, mean reward: 0.158 [0.027, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.562, 10.209], loss: 0.003879, mae: 0.069742, mean_q: 0.323612
 51063/100000: episode: 717, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.719, mean reward: 0.167 [0.020, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.251, 10.180], loss: 0.004133, mae: 0.071007, mean_q: 0.324462
 51163/100000: episode: 718, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.013, mean reward: 0.160 [0.011, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.578, 10.098], loss: 0.003822, mae: 0.068019, mean_q: 0.323586
 51263/100000: episode: 719, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 12.993, mean reward: 0.130 [0.011, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.098], loss: 0.003605, mae: 0.066529, mean_q: 0.321138
[Info] Not found new level, current best level reached = inf
 51363/100000: episode: 720, duration: 4.460s, episode steps: 100, steps per second: 22, episode reward: 12.753, mean reward: 0.128 [0.007, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.967, 10.098], loss: 0.003745, mae: 0.067689, mean_q: 0.324333
 51463/100000: episode: 721, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.567, mean reward: 0.156 [0.009, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.258, 10.200], loss: 0.003830, mae: 0.068462, mean_q: 0.323778
 51563/100000: episode: 722, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 16.249, mean reward: 0.162 [0.002, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.340, 10.098], loss: 0.003881, mae: 0.068199, mean_q: 0.322296
 51663/100000: episode: 723, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.724, mean reward: 0.157 [0.018, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.240, 10.110], loss: 0.004052, mae: 0.070940, mean_q: 0.328805
 51763/100000: episode: 724, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 13.319, mean reward: 0.133 [0.013, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.103], loss: 0.003725, mae: 0.068195, mean_q: 0.325802
 51863/100000: episode: 725, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.776, mean reward: 0.168 [0.017, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.480, 10.098], loss: 0.003891, mae: 0.069437, mean_q: 0.325789
 51963/100000: episode: 726, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 12.370, mean reward: 0.124 [0.015, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.156], loss: 0.003940, mae: 0.069645, mean_q: 0.318576
 52063/100000: episode: 727, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 20.127, mean reward: 0.201 [0.029, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.214], loss: 0.003894, mae: 0.069634, mean_q: 0.323292
 52163/100000: episode: 728, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 15.301, mean reward: 0.153 [0.017, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.612, 10.107], loss: 0.003621, mae: 0.067255, mean_q: 0.322681
 52263/100000: episode: 729, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 16.878, mean reward: 0.169 [0.032, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.704, 10.255], loss: 0.003773, mae: 0.068026, mean_q: 0.319472
 52363/100000: episode: 730, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.581, mean reward: 0.146 [0.012, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.020, 10.098], loss: 0.003992, mae: 0.070400, mean_q: 0.322923
 52463/100000: episode: 731, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 17.670, mean reward: 0.177 [0.029, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.480, 10.272], loss: 0.003811, mae: 0.068314, mean_q: 0.321804
 52563/100000: episode: 732, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 19.148, mean reward: 0.191 [0.029, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.571, 10.098], loss: 0.003741, mae: 0.068072, mean_q: 0.325847
 52663/100000: episode: 733, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 17.492, mean reward: 0.175 [0.013, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.993, 10.098], loss: 0.003722, mae: 0.068047, mean_q: 0.325279
 52763/100000: episode: 734, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 23.314, mean reward: 0.233 [0.038, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.908, 10.098], loss: 0.003708, mae: 0.068043, mean_q: 0.323995
 52863/100000: episode: 735, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 15.546, mean reward: 0.155 [0.027, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.570, 10.165], loss: 0.003904, mae: 0.069361, mean_q: 0.328702
 52963/100000: episode: 736, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 16.506, mean reward: 0.165 [0.018, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.538, 10.278], loss: 0.003999, mae: 0.070722, mean_q: 0.331164
 53063/100000: episode: 737, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.506, mean reward: 0.165 [0.024, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.353, 10.360], loss: 0.003958, mae: 0.070127, mean_q: 0.326182
 53163/100000: episode: 738, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 19.963, mean reward: 0.200 [0.037, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.268, 10.098], loss: 0.003816, mae: 0.068898, mean_q: 0.323510
 53263/100000: episode: 739, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 15.918, mean reward: 0.159 [0.014, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.098], loss: 0.003753, mae: 0.067709, mean_q: 0.323336
 53363/100000: episode: 740, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 16.179, mean reward: 0.162 [0.011, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.473, 10.099], loss: 0.003815, mae: 0.067800, mean_q: 0.329219
 53463/100000: episode: 741, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.652, mean reward: 0.157 [0.025, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.382], loss: 0.003691, mae: 0.067601, mean_q: 0.326869
 53563/100000: episode: 742, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 17.079, mean reward: 0.171 [0.008, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.014, 10.098], loss: 0.003819, mae: 0.068177, mean_q: 0.327407
 53663/100000: episode: 743, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.832, mean reward: 0.188 [0.024, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.867, 10.347], loss: 0.003745, mae: 0.068893, mean_q: 0.331145
 53763/100000: episode: 744, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.485, mean reward: 0.185 [0.010, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.690, 10.292], loss: 0.003650, mae: 0.066688, mean_q: 0.330032
 53863/100000: episode: 745, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.992, mean reward: 0.180 [0.038, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.098], loss: 0.003903, mae: 0.069301, mean_q: 0.331203
 53963/100000: episode: 746, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 15.430, mean reward: 0.154 [0.010, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.334, 10.106], loss: 0.003853, mae: 0.068826, mean_q: 0.329690
 54063/100000: episode: 747, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.501, mean reward: 0.155 [0.010, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.327, 10.194], loss: 0.004084, mae: 0.071392, mean_q: 0.324343
 54163/100000: episode: 748, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.397, mean reward: 0.174 [0.019, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.895, 10.113], loss: 0.003844, mae: 0.068802, mean_q: 0.323160
 54263/100000: episode: 749, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.725, mean reward: 0.137 [0.018, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.312, 10.098], loss: 0.003812, mae: 0.068721, mean_q: 0.316309
 54363/100000: episode: 750, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 14.242, mean reward: 0.142 [0.013, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.738, 10.187], loss: 0.003729, mae: 0.068287, mean_q: 0.323029
 54463/100000: episode: 751, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 19.320, mean reward: 0.193 [0.007, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.177, 10.098], loss: 0.003475, mae: 0.064949, mean_q: 0.315288
 54563/100000: episode: 752, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 20.675, mean reward: 0.207 [0.049, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.779, 10.098], loss: 0.003777, mae: 0.068618, mean_q: 0.327366
 54663/100000: episode: 753, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.337, mean reward: 0.193 [0.018, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.190, 10.098], loss: 0.003803, mae: 0.068898, mean_q: 0.327350
 54763/100000: episode: 754, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 22.171, mean reward: 0.222 [0.006, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.756, 10.332], loss: 0.003694, mae: 0.067764, mean_q: 0.327740
 54863/100000: episode: 755, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 17.622, mean reward: 0.176 [0.020, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.830, 10.157], loss: 0.003892, mae: 0.068861, mean_q: 0.327138
 54963/100000: episode: 756, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 14.308, mean reward: 0.143 [0.022, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.732, 10.098], loss: 0.003808, mae: 0.069084, mean_q: 0.331007
 55063/100000: episode: 757, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 17.933, mean reward: 0.179 [0.015, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.098], loss: 0.003689, mae: 0.067390, mean_q: 0.328115
 55163/100000: episode: 758, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.741, mean reward: 0.147 [0.016, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.475, 10.251], loss: 0.003879, mae: 0.068899, mean_q: 0.325453
 55263/100000: episode: 759, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 17.616, mean reward: 0.176 [0.019, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.959, 10.312], loss: 0.003681, mae: 0.067514, mean_q: 0.324204
 55363/100000: episode: 760, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.393, mean reward: 0.154 [0.006, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.940, 10.121], loss: 0.003765, mae: 0.067968, mean_q: 0.325013
 55463/100000: episode: 761, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.383, mean reward: 0.164 [0.006, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.233, 10.349], loss: 0.003851, mae: 0.068882, mean_q: 0.333487
 55563/100000: episode: 762, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.443, mean reward: 0.144 [0.010, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.977, 10.098], loss: 0.003692, mae: 0.067221, mean_q: 0.326967
 55663/100000: episode: 763, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 18.194, mean reward: 0.182 [0.015, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.354, 10.098], loss: 0.003611, mae: 0.066777, mean_q: 0.329192
 55763/100000: episode: 764, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 17.705, mean reward: 0.177 [0.010, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.090, 10.098], loss: 0.003566, mae: 0.067033, mean_q: 0.327289
 55863/100000: episode: 765, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 19.127, mean reward: 0.191 [0.010, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.511, 10.136], loss: 0.003499, mae: 0.065084, mean_q: 0.330596
 55963/100000: episode: 766, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 18.230, mean reward: 0.182 [0.012, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.462, 10.345], loss: 0.003780, mae: 0.068300, mean_q: 0.332784
 56063/100000: episode: 767, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 15.209, mean reward: 0.152 [0.008, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.798, 10.106], loss: 0.003932, mae: 0.070096, mean_q: 0.327885
 56163/100000: episode: 768, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 19.097, mean reward: 0.191 [0.021, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.945, 10.349], loss: 0.003901, mae: 0.070106, mean_q: 0.330716
 56263/100000: episode: 769, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 25.720, mean reward: 0.257 [0.037, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.418, 10.352], loss: 0.003691, mae: 0.068017, mean_q: 0.332708
 56363/100000: episode: 770, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 15.176, mean reward: 0.152 [0.027, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.741, 10.271], loss: 0.003656, mae: 0.067483, mean_q: 0.335359
 56463/100000: episode: 771, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 21.200, mean reward: 0.212 [0.038, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.250], loss: 0.004034, mae: 0.071066, mean_q: 0.339814
 56563/100000: episode: 772, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 20.396, mean reward: 0.204 [0.012, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.484, 10.098], loss: 0.003857, mae: 0.069078, mean_q: 0.337276
 56663/100000: episode: 773, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 15.024, mean reward: 0.150 [0.016, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.114], loss: 0.003923, mae: 0.070009, mean_q: 0.339739
 56763/100000: episode: 774, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.080, mean reward: 0.171 [0.013, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.659, 10.234], loss: 0.003866, mae: 0.069470, mean_q: 0.338175
 56863/100000: episode: 775, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 17.280, mean reward: 0.173 [0.020, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.284, 10.098], loss: 0.003630, mae: 0.066616, mean_q: 0.339809
 56963/100000: episode: 776, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 18.962, mean reward: 0.190 [0.029, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.661, 10.366], loss: 0.003654, mae: 0.067014, mean_q: 0.339190
 57063/100000: episode: 777, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 17.250, mean reward: 0.172 [0.018, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.710, 10.098], loss: 0.003724, mae: 0.067354, mean_q: 0.343539
 57163/100000: episode: 778, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.878, mean reward: 0.179 [0.008, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.670, 10.123], loss: 0.003850, mae: 0.068213, mean_q: 0.341575
 57263/100000: episode: 779, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.918, mean reward: 0.159 [0.032, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.647, 10.098], loss: 0.003752, mae: 0.067610, mean_q: 0.344556
 57363/100000: episode: 780, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 22.087, mean reward: 0.221 [0.021, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.114, 10.300], loss: 0.003700, mae: 0.066571, mean_q: 0.346984
 57463/100000: episode: 781, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 19.409, mean reward: 0.194 [0.008, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.554, 10.427], loss: 0.003721, mae: 0.067052, mean_q: 0.345814
 57563/100000: episode: 782, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.928, mean reward: 0.149 [0.006, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.385, 10.249], loss: 0.003974, mae: 0.068832, mean_q: 0.344196
 57663/100000: episode: 783, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.802, mean reward: 0.168 [0.005, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.126], loss: 0.003810, mae: 0.067987, mean_q: 0.347008
 57763/100000: episode: 784, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.862, mean reward: 0.159 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.330, 10.208], loss: 0.003676, mae: 0.067769, mean_q: 0.340699
 57863/100000: episode: 785, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 19.444, mean reward: 0.194 [0.022, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.241, 10.098], loss: 0.003766, mae: 0.067613, mean_q: 0.343781
 57963/100000: episode: 786, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 24.217, mean reward: 0.242 [0.043, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.523], loss: 0.003967, mae: 0.069606, mean_q: 0.342622
 58063/100000: episode: 787, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.906, mean reward: 0.179 [0.040, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.275, 10.098], loss: 0.003746, mae: 0.067827, mean_q: 0.351366
 58163/100000: episode: 788, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 21.596, mean reward: 0.216 [0.042, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.579, 10.098], loss: 0.003736, mae: 0.067672, mean_q: 0.348372
 58263/100000: episode: 789, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.855, mean reward: 0.169 [0.007, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.250], loss: 0.003664, mae: 0.067047, mean_q: 0.350939
 58363/100000: episode: 790, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 15.625, mean reward: 0.156 [0.015, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.249], loss: 0.003707, mae: 0.066801, mean_q: 0.351055
 58463/100000: episode: 791, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 17.678, mean reward: 0.177 [0.021, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.048, 10.098], loss: 0.003875, mae: 0.069104, mean_q: 0.352634
 58563/100000: episode: 792, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.807, mean reward: 0.158 [0.023, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.753, 10.098], loss: 0.003849, mae: 0.068503, mean_q: 0.347730
 58663/100000: episode: 793, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.110, mean reward: 0.151 [0.005, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.896, 10.214], loss: 0.003935, mae: 0.069093, mean_q: 0.352306
 58763/100000: episode: 794, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.142, mean reward: 0.151 [0.026, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.717, 10.210], loss: 0.003696, mae: 0.067346, mean_q: 0.351113
 58863/100000: episode: 795, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.121, mean reward: 0.151 [0.020, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.382, 10.124], loss: 0.003628, mae: 0.066219, mean_q: 0.347974
 58963/100000: episode: 796, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.414, mean reward: 0.144 [0.027, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.404, 10.108], loss: 0.003814, mae: 0.067950, mean_q: 0.349727
 59063/100000: episode: 797, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.138, mean reward: 0.141 [0.010, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.928, 10.121], loss: 0.003945, mae: 0.068652, mean_q: 0.347758
 59163/100000: episode: 798, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 16.522, mean reward: 0.165 [0.006, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.068, 10.098], loss: 0.003927, mae: 0.069642, mean_q: 0.345656
 59263/100000: episode: 799, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 15.723, mean reward: 0.157 [0.008, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.098], loss: 0.003885, mae: 0.068360, mean_q: 0.347201
 59363/100000: episode: 800, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 15.439, mean reward: 0.154 [0.014, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.879, 10.145], loss: 0.003652, mae: 0.067134, mean_q: 0.345229
 59463/100000: episode: 801, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 20.177, mean reward: 0.202 [0.047, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.730, 10.188], loss: 0.003881, mae: 0.068749, mean_q: 0.343704
 59563/100000: episode: 802, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.648, mean reward: 0.156 [0.023, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.180, 10.098], loss: 0.003778, mae: 0.068299, mean_q: 0.343930
 59663/100000: episode: 803, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 17.057, mean reward: 0.171 [0.038, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.167, 10.213], loss: 0.003693, mae: 0.066378, mean_q: 0.346152
 59763/100000: episode: 804, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 18.066, mean reward: 0.181 [0.014, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.332, 10.367], loss: 0.003740, mae: 0.067857, mean_q: 0.343233
 59863/100000: episode: 805, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 17.756, mean reward: 0.178 [0.008, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.207, 10.098], loss: 0.003630, mae: 0.066573, mean_q: 0.342490
 59963/100000: episode: 806, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.172, mean reward: 0.162 [0.027, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.088, 10.098], loss: 0.003707, mae: 0.067403, mean_q: 0.339761
 60063/100000: episode: 807, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 17.514, mean reward: 0.175 [0.026, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.822, 10.264], loss: 0.003610, mae: 0.066537, mean_q: 0.339392
 60163/100000: episode: 808, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 15.188, mean reward: 0.152 [0.023, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.548, 10.101], loss: 0.003796, mae: 0.069022, mean_q: 0.342771
 60263/100000: episode: 809, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 21.806, mean reward: 0.218 [0.007, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.891, 10.167], loss: 0.003882, mae: 0.068819, mean_q: 0.338879
 60363/100000: episode: 810, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 12.334, mean reward: 0.123 [0.010, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.132], loss: 0.003846, mae: 0.068595, mean_q: 0.345058
 60463/100000: episode: 811, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.108, mean reward: 0.161 [0.023, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.286, 10.285], loss: 0.003803, mae: 0.068500, mean_q: 0.345267
 60563/100000: episode: 812, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 13.998, mean reward: 0.140 [0.011, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.004, 10.098], loss: 0.003795, mae: 0.067856, mean_q: 0.344122
 60663/100000: episode: 813, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 13.418, mean reward: 0.134 [0.013, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.555, 10.335], loss: 0.003511, mae: 0.066251, mean_q: 0.340035
 60763/100000: episode: 814, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 21.477, mean reward: 0.215 [0.068, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.982, 10.098], loss: 0.003744, mae: 0.067688, mean_q: 0.342396
 60863/100000: episode: 815, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 22.977, mean reward: 0.230 [0.020, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.457], loss: 0.003810, mae: 0.067941, mean_q: 0.341243
 60963/100000: episode: 816, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.413, mean reward: 0.184 [0.029, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.479, 10.154], loss: 0.003920, mae: 0.069199, mean_q: 0.347097
 61063/100000: episode: 817, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 13.269, mean reward: 0.133 [0.015, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.744, 10.236], loss: 0.003887, mae: 0.068071, mean_q: 0.345412
 61163/100000: episode: 818, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 16.439, mean reward: 0.164 [0.021, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.130], loss: 0.003948, mae: 0.070101, mean_q: 0.340591
 61263/100000: episode: 819, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.801, mean reward: 0.148 [0.010, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.806, 10.098], loss: 0.004097, mae: 0.070248, mean_q: 0.337481
[Info] Not found new level, current best level reached = inf
 61363/100000: episode: 820, duration: 4.483s, episode steps: 100, steps per second: 22, episode reward: 17.554, mean reward: 0.176 [0.013, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.014, 10.098], loss: 0.003790, mae: 0.068240, mean_q: 0.340774
 61463/100000: episode: 821, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 19.254, mean reward: 0.193 [0.027, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.845, 10.098], loss: 0.003615, mae: 0.066251, mean_q: 0.337303
 61563/100000: episode: 822, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 16.226, mean reward: 0.162 [0.016, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.117], loss: 0.003816, mae: 0.067960, mean_q: 0.336085
 61663/100000: episode: 823, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 17.559, mean reward: 0.176 [0.006, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.098], loss: 0.003950, mae: 0.070147, mean_q: 0.335447
 61763/100000: episode: 824, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 12.694, mean reward: 0.127 [0.002, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.705, 10.098], loss: 0.003989, mae: 0.069773, mean_q: 0.338500
 61863/100000: episode: 825, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.839, mean reward: 0.178 [0.030, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.260, 10.098], loss: 0.004027, mae: 0.069165, mean_q: 0.337862
 61963/100000: episode: 826, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.960, mean reward: 0.150 [0.003, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.375, 10.371], loss: 0.003729, mae: 0.067391, mean_q: 0.330271
 62063/100000: episode: 827, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.915, mean reward: 0.149 [0.014, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.042, 10.098], loss: 0.003845, mae: 0.068882, mean_q: 0.333867
 62163/100000: episode: 828, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 13.655, mean reward: 0.137 [0.023, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.575, 10.098], loss: 0.003697, mae: 0.067566, mean_q: 0.330608
 62263/100000: episode: 829, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 17.783, mean reward: 0.178 [0.029, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.806, 10.360], loss: 0.003721, mae: 0.067334, mean_q: 0.329410
 62363/100000: episode: 830, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 22.440, mean reward: 0.224 [0.068, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.817, 10.098], loss: 0.003683, mae: 0.067174, mean_q: 0.332163
 62463/100000: episode: 831, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.231, mean reward: 0.172 [0.017, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.730, 10.098], loss: 0.003949, mae: 0.068560, mean_q: 0.331771
 62563/100000: episode: 832, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 20.535, mean reward: 0.205 [0.008, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.319, 10.098], loss: 0.003775, mae: 0.069189, mean_q: 0.332764
 62663/100000: episode: 833, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 19.477, mean reward: 0.195 [0.002, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.242, 10.267], loss: 0.003842, mae: 0.069197, mean_q: 0.334422
 62763/100000: episode: 834, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 18.078, mean reward: 0.181 [0.017, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.098], loss: 0.003827, mae: 0.068536, mean_q: 0.340868
 62863/100000: episode: 835, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 25.083, mean reward: 0.251 [0.075, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.672, 10.212], loss: 0.003743, mae: 0.068039, mean_q: 0.335288
 62963/100000: episode: 836, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.279, mean reward: 0.173 [0.040, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.139, 10.315], loss: 0.003912, mae: 0.068949, mean_q: 0.335256
 63063/100000: episode: 837, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 17.565, mean reward: 0.176 [0.016, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.824, 10.098], loss: 0.003712, mae: 0.067600, mean_q: 0.335430
 63163/100000: episode: 838, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.201, mean reward: 0.152 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.098], loss: 0.003802, mae: 0.067307, mean_q: 0.333681
 63263/100000: episode: 839, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.020, mean reward: 0.170 [0.009, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.997, 10.098], loss: 0.003936, mae: 0.070025, mean_q: 0.334033
 63363/100000: episode: 840, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.184, mean reward: 0.152 [0.010, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.477, 10.105], loss: 0.003880, mae: 0.069137, mean_q: 0.333622
 63463/100000: episode: 841, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 25.467, mean reward: 0.255 [0.031, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.401, 10.209], loss: 0.003740, mae: 0.067767, mean_q: 0.333109
 63563/100000: episode: 842, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.167, mean reward: 0.142 [0.013, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.483, 10.103], loss: 0.003708, mae: 0.066917, mean_q: 0.331695
 63663/100000: episode: 843, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.204, mean reward: 0.162 [0.017, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.124, 10.256], loss: 0.003894, mae: 0.069196, mean_q: 0.332826
 63763/100000: episode: 844, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 13.741, mean reward: 0.137 [0.003, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.674, 10.255], loss: 0.003972, mae: 0.069467, mean_q: 0.338418
 63863/100000: episode: 845, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 15.046, mean reward: 0.150 [0.007, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.099], loss: 0.003758, mae: 0.067863, mean_q: 0.338252
 63963/100000: episode: 846, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 21.497, mean reward: 0.215 [0.016, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.390, 10.098], loss: 0.003998, mae: 0.069657, mean_q: 0.335592
 64063/100000: episode: 847, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 22.147, mean reward: 0.221 [0.030, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.094, 10.098], loss: 0.003751, mae: 0.067867, mean_q: 0.338524
 64163/100000: episode: 848, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 17.876, mean reward: 0.179 [0.044, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.801, 10.098], loss: 0.003828, mae: 0.068838, mean_q: 0.338994
 64263/100000: episode: 849, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 15.299, mean reward: 0.153 [0.030, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.171, 10.098], loss: 0.004133, mae: 0.070573, mean_q: 0.340451
 64363/100000: episode: 850, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.944, mean reward: 0.159 [0.027, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.098], loss: 0.003772, mae: 0.068295, mean_q: 0.336856
 64463/100000: episode: 851, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 17.289, mean reward: 0.173 [0.026, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.980, 10.098], loss: 0.003647, mae: 0.066755, mean_q: 0.336000
 64563/100000: episode: 852, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.892, mean reward: 0.189 [0.022, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.538, 10.263], loss: 0.004069, mae: 0.070215, mean_q: 0.339253
 64663/100000: episode: 853, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 20.969, mean reward: 0.210 [0.010, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.999, 10.098], loss: 0.003919, mae: 0.068988, mean_q: 0.339677
 64763/100000: episode: 854, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 18.249, mean reward: 0.182 [0.012, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.110], loss: 0.003968, mae: 0.069444, mean_q: 0.340095
 64863/100000: episode: 855, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 14.144, mean reward: 0.141 [0.021, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.331, 10.098], loss: 0.003623, mae: 0.066612, mean_q: 0.335978
 64963/100000: episode: 856, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.063, mean reward: 0.181 [0.010, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.984, 10.098], loss: 0.003785, mae: 0.067377, mean_q: 0.337165
 65063/100000: episode: 857, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 17.041, mean reward: 0.170 [0.028, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.109], loss: 0.003907, mae: 0.068236, mean_q: 0.343738
 65163/100000: episode: 858, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 14.700, mean reward: 0.147 [0.012, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.098], loss: 0.004164, mae: 0.071312, mean_q: 0.345653
 65263/100000: episode: 859, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 19.523, mean reward: 0.195 [0.033, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.451, 10.098], loss: 0.003917, mae: 0.069039, mean_q: 0.335621
 65363/100000: episode: 860, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 19.012, mean reward: 0.190 [0.023, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.391, 10.350], loss: 0.004019, mae: 0.069841, mean_q: 0.342640
 65463/100000: episode: 861, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.601, mean reward: 0.156 [0.012, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.750, 10.148], loss: 0.004195, mae: 0.071743, mean_q: 0.345814
 65563/100000: episode: 862, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 14.719, mean reward: 0.147 [0.025, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.970, 10.122], loss: 0.004243, mae: 0.070780, mean_q: 0.345654
 65663/100000: episode: 863, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 18.780, mean reward: 0.188 [0.030, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.865, 10.098], loss: 0.004054, mae: 0.070216, mean_q: 0.347284
 65763/100000: episode: 864, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 18.744, mean reward: 0.187 [0.020, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.161, 10.098], loss: 0.004173, mae: 0.071161, mean_q: 0.343807
 65863/100000: episode: 865, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 16.672, mean reward: 0.167 [0.013, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.447, 10.182], loss: 0.004160, mae: 0.070013, mean_q: 0.342579
 65963/100000: episode: 866, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 13.881, mean reward: 0.139 [0.003, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.898, 10.098], loss: 0.003973, mae: 0.070078, mean_q: 0.334625
 66063/100000: episode: 867, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.774, mean reward: 0.168 [0.009, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.593, 10.098], loss: 0.004089, mae: 0.070059, mean_q: 0.343295
 66163/100000: episode: 868, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 18.209, mean reward: 0.182 [0.038, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.184, 10.246], loss: 0.004116, mae: 0.070254, mean_q: 0.339789
 66263/100000: episode: 869, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 19.203, mean reward: 0.192 [0.019, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.184, 10.280], loss: 0.004119, mae: 0.070143, mean_q: 0.340309
 66363/100000: episode: 870, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.081, mean reward: 0.161 [0.022, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.531, 10.098], loss: 0.004150, mae: 0.070799, mean_q: 0.343778
 66463/100000: episode: 871, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 19.131, mean reward: 0.191 [0.015, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.883, 10.198], loss: 0.003917, mae: 0.069470, mean_q: 0.341191
 66563/100000: episode: 872, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.080, mean reward: 0.161 [0.006, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.265], loss: 0.004200, mae: 0.071601, mean_q: 0.340702
 66663/100000: episode: 873, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 19.703, mean reward: 0.197 [0.014, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.658, 10.098], loss: 0.004101, mae: 0.069701, mean_q: 0.343666
 66763/100000: episode: 874, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 19.874, mean reward: 0.199 [0.016, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.464, 10.154], loss: 0.004033, mae: 0.070108, mean_q: 0.346030
 66863/100000: episode: 875, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 17.275, mean reward: 0.173 [0.025, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.537, 10.216], loss: 0.003944, mae: 0.069522, mean_q: 0.344539
 66963/100000: episode: 876, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 18.326, mean reward: 0.183 [0.026, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.214, 10.280], loss: 0.004138, mae: 0.070153, mean_q: 0.349287
 67063/100000: episode: 877, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 15.438, mean reward: 0.154 [0.007, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.018, 10.165], loss: 0.003964, mae: 0.069330, mean_q: 0.348212
 67163/100000: episode: 878, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 14.566, mean reward: 0.146 [0.009, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.131, 10.217], loss: 0.003944, mae: 0.069772, mean_q: 0.350347
 67263/100000: episode: 879, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 15.441, mean reward: 0.154 [0.020, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.426, 10.119], loss: 0.003953, mae: 0.069525, mean_q: 0.344375
 67363/100000: episode: 880, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.605, mean reward: 0.156 [0.006, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.713, 10.098], loss: 0.004189, mae: 0.070535, mean_q: 0.344531
 67463/100000: episode: 881, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 18.188, mean reward: 0.182 [0.009, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.355, 10.098], loss: 0.004157, mae: 0.070534, mean_q: 0.344521
 67563/100000: episode: 882, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 21.904, mean reward: 0.219 [0.037, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.247, 10.098], loss: 0.004039, mae: 0.069866, mean_q: 0.348711
 67663/100000: episode: 883, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 18.689, mean reward: 0.187 [0.015, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.828, 10.098], loss: 0.003668, mae: 0.066117, mean_q: 0.343493
 67763/100000: episode: 884, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 19.295, mean reward: 0.193 [0.006, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.923, 10.098], loss: 0.004231, mae: 0.071478, mean_q: 0.345443
 67863/100000: episode: 885, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 18.008, mean reward: 0.180 [0.025, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.352], loss: 0.004200, mae: 0.071493, mean_q: 0.347800
 67963/100000: episode: 886, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 19.743, mean reward: 0.197 [0.026, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.243, 10.470], loss: 0.003881, mae: 0.068445, mean_q: 0.345817
 68063/100000: episode: 887, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.728, mean reward: 0.177 [0.016, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.905, 10.098], loss: 0.004088, mae: 0.070600, mean_q: 0.344035
 68163/100000: episode: 888, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 18.029, mean reward: 0.180 [0.012, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.128], loss: 0.003683, mae: 0.066246, mean_q: 0.341128
 68263/100000: episode: 889, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 14.905, mean reward: 0.149 [0.007, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.303, 10.225], loss: 0.003973, mae: 0.069543, mean_q: 0.342852
 68363/100000: episode: 890, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 12.695, mean reward: 0.127 [0.013, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.121], loss: 0.004211, mae: 0.071914, mean_q: 0.343077
 68463/100000: episode: 891, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.059, mean reward: 0.151 [0.015, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.662, 10.098], loss: 0.003957, mae: 0.069263, mean_q: 0.340105
 68563/100000: episode: 892, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 20.310, mean reward: 0.203 [0.024, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.098], loss: 0.003945, mae: 0.069928, mean_q: 0.338322
 68663/100000: episode: 893, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 14.453, mean reward: 0.145 [0.002, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.738, 10.098], loss: 0.004263, mae: 0.071487, mean_q: 0.344196
 68763/100000: episode: 894, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 15.503, mean reward: 0.155 [0.012, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.776, 10.098], loss: 0.004085, mae: 0.070218, mean_q: 0.341626
 68863/100000: episode: 895, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 13.235, mean reward: 0.132 [0.002, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.000, 10.098], loss: 0.004004, mae: 0.069142, mean_q: 0.343197
 68963/100000: episode: 896, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 16.722, mean reward: 0.167 [0.017, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.098], loss: 0.004097, mae: 0.071005, mean_q: 0.341308
 69063/100000: episode: 897, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 17.116, mean reward: 0.171 [0.015, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.924, 10.206], loss: 0.004058, mae: 0.069707, mean_q: 0.338552
 69163/100000: episode: 898, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.772, mean reward: 0.168 [0.006, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.439, 10.239], loss: 0.003922, mae: 0.068779, mean_q: 0.337107
 69263/100000: episode: 899, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 15.143, mean reward: 0.151 [0.010, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.745, 10.195], loss: 0.004181, mae: 0.072120, mean_q: 0.341301
 69363/100000: episode: 900, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 19.738, mean reward: 0.197 [0.037, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.806, 10.098], loss: 0.003853, mae: 0.068783, mean_q: 0.338425
 69463/100000: episode: 901, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 16.630, mean reward: 0.166 [0.007, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.184], loss: 0.004058, mae: 0.070355, mean_q: 0.340642
 69563/100000: episode: 902, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.459, mean reward: 0.155 [0.010, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.036, 10.149], loss: 0.003965, mae: 0.069393, mean_q: 0.336127
 69663/100000: episode: 903, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.092, mean reward: 0.151 [0.017, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.661, 10.257], loss: 0.003958, mae: 0.068686, mean_q: 0.338024
 69763/100000: episode: 904, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 12.390, mean reward: 0.124 [0.012, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.536, 10.098], loss: 0.003890, mae: 0.068617, mean_q: 0.332977
 69863/100000: episode: 905, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 15.460, mean reward: 0.155 [0.014, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.481, 10.207], loss: 0.003910, mae: 0.069237, mean_q: 0.335289
 69963/100000: episode: 906, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 19.381, mean reward: 0.194 [0.034, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.919, 10.367], loss: 0.003754, mae: 0.068904, mean_q: 0.327747
 70063/100000: episode: 907, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 13.971, mean reward: 0.140 [0.004, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.112, 10.166], loss: 0.004026, mae: 0.070009, mean_q: 0.335555
 70163/100000: episode: 908, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 17.617, mean reward: 0.176 [0.020, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.640, 10.123], loss: 0.004039, mae: 0.070578, mean_q: 0.332876
 70263/100000: episode: 909, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 19.167, mean reward: 0.192 [0.029, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.140], loss: 0.004102, mae: 0.070859, mean_q: 0.330893
 70363/100000: episode: 910, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 18.997, mean reward: 0.190 [0.015, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.535, 10.238], loss: 0.003806, mae: 0.067821, mean_q: 0.329110
 70463/100000: episode: 911, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.710, mean reward: 0.157 [0.021, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.522, 10.098], loss: 0.003887, mae: 0.068919, mean_q: 0.334743
 70563/100000: episode: 912, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 13.971, mean reward: 0.140 [0.013, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.098], loss: 0.003926, mae: 0.069194, mean_q: 0.336807
 70663/100000: episode: 913, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 16.536, mean reward: 0.165 [0.001, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.553, 10.098], loss: 0.003953, mae: 0.069918, mean_q: 0.335667
 70763/100000: episode: 914, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.142, mean reward: 0.191 [0.038, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.098], loss: 0.003776, mae: 0.068436, mean_q: 0.333901
 70863/100000: episode: 915, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 14.341, mean reward: 0.143 [0.028, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.601, 10.167], loss: 0.003460, mae: 0.065288, mean_q: 0.331801
 70963/100000: episode: 916, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.063, mean reward: 0.141 [0.010, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.502, 10.103], loss: 0.003752, mae: 0.067874, mean_q: 0.332358
 71063/100000: episode: 917, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 15.179, mean reward: 0.152 [0.009, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.050, 10.216], loss: 0.003613, mae: 0.066426, mean_q: 0.333929
 71163/100000: episode: 918, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 18.767, mean reward: 0.188 [0.019, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.912, 10.098], loss: 0.003755, mae: 0.067638, mean_q: 0.331670
 71263/100000: episode: 919, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.374, mean reward: 0.154 [0.024, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.101], loss: 0.003635, mae: 0.066655, mean_q: 0.329611
[Info] Not found new level, current best level reached = inf
 71363/100000: episode: 920, duration: 4.506s, episode steps: 100, steps per second: 22, episode reward: 14.249, mean reward: 0.142 [0.013, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.234, 10.098], loss: 0.003720, mae: 0.066967, mean_q: 0.327331
 71463/100000: episode: 921, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 14.538, mean reward: 0.145 [0.007, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.627, 10.098], loss: 0.003948, mae: 0.068973, mean_q: 0.329668
 71563/100000: episode: 922, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 14.777, mean reward: 0.148 [0.010, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.098], loss: 0.003545, mae: 0.066265, mean_q: 0.325923
 71663/100000: episode: 923, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 21.501, mean reward: 0.215 [0.033, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.336, 10.205], loss: 0.003983, mae: 0.070318, mean_q: 0.325663
 71763/100000: episode: 924, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 24.334, mean reward: 0.243 [0.019, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.745, 10.205], loss: 0.003606, mae: 0.066761, mean_q: 0.328216
 71863/100000: episode: 925, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 16.984, mean reward: 0.170 [0.020, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.289, 10.387], loss: 0.003741, mae: 0.067509, mean_q: 0.324121
 71963/100000: episode: 926, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.498, mean reward: 0.155 [0.031, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.445, 10.240], loss: 0.003627, mae: 0.066655, mean_q: 0.328342
 72063/100000: episode: 927, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 17.797, mean reward: 0.178 [0.011, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.046, 10.139], loss: 0.003643, mae: 0.067227, mean_q: 0.330132
 72163/100000: episode: 928, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 21.674, mean reward: 0.217 [0.030, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.549, 10.262], loss: 0.003765, mae: 0.068140, mean_q: 0.333349
 72263/100000: episode: 929, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 17.402, mean reward: 0.174 [0.023, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.987, 10.187], loss: 0.003696, mae: 0.066933, mean_q: 0.326694
 72363/100000: episode: 930, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 20.965, mean reward: 0.210 [0.011, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.354, 10.098], loss: 0.003793, mae: 0.067463, mean_q: 0.327561
 72463/100000: episode: 931, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.826, mean reward: 0.188 [0.026, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.286], loss: 0.003849, mae: 0.067879, mean_q: 0.332994
 72563/100000: episode: 932, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.058, mean reward: 0.151 [0.023, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.517, 10.098], loss: 0.004015, mae: 0.069716, mean_q: 0.330599
 72663/100000: episode: 933, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.433, mean reward: 0.164 [0.019, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.408, 10.350], loss: 0.003866, mae: 0.068091, mean_q: 0.330444
 72763/100000: episode: 934, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 13.948, mean reward: 0.139 [0.020, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.483, 10.134], loss: 0.003865, mae: 0.069005, mean_q: 0.332529
 72863/100000: episode: 935, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 14.528, mean reward: 0.145 [0.005, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.374, 10.130], loss: 0.003792, mae: 0.068031, mean_q: 0.333246
 72963/100000: episode: 936, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 20.459, mean reward: 0.205 [0.019, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.534, 10.424], loss: 0.003815, mae: 0.067773, mean_q: 0.324458
 73063/100000: episode: 937, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 13.839, mean reward: 0.138 [0.016, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.688, 10.098], loss: 0.003688, mae: 0.067211, mean_q: 0.324332
 73163/100000: episode: 938, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 11.958, mean reward: 0.120 [0.009, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.795, 10.098], loss: 0.003753, mae: 0.067713, mean_q: 0.323516
 73263/100000: episode: 939, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 15.843, mean reward: 0.158 [0.012, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.098], loss: 0.003819, mae: 0.068179, mean_q: 0.325287
 73363/100000: episode: 940, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 14.959, mean reward: 0.150 [0.013, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.044, 10.196], loss: 0.003738, mae: 0.067507, mean_q: 0.319861
 73463/100000: episode: 941, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 14.448, mean reward: 0.144 [0.030, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.580, 10.190], loss: 0.003682, mae: 0.067514, mean_q: 0.325053
 73563/100000: episode: 942, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.921, mean reward: 0.149 [0.002, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.098], loss: 0.003856, mae: 0.069532, mean_q: 0.322944
 73663/100000: episode: 943, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 13.149, mean reward: 0.131 [0.016, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.184], loss: 0.003831, mae: 0.069053, mean_q: 0.320853
 73763/100000: episode: 944, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 17.321, mean reward: 0.173 [0.008, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.840, 10.098], loss: 0.003764, mae: 0.067508, mean_q: 0.319676
 73863/100000: episode: 945, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 15.219, mean reward: 0.152 [0.009, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.775, 10.196], loss: 0.003662, mae: 0.066877, mean_q: 0.322538
 73963/100000: episode: 946, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.086, mean reward: 0.141 [0.026, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.864, 10.107], loss: 0.003659, mae: 0.066074, mean_q: 0.323688
 74063/100000: episode: 947, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.230, mean reward: 0.152 [0.012, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.279, 10.142], loss: 0.003776, mae: 0.067580, mean_q: 0.319037
 74163/100000: episode: 948, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 18.385, mean reward: 0.184 [0.026, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.534, 10.300], loss: 0.003975, mae: 0.068945, mean_q: 0.321185
 74263/100000: episode: 949, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 14.821, mean reward: 0.148 [0.007, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.091, 10.098], loss: 0.003948, mae: 0.068873, mean_q: 0.324731
 74363/100000: episode: 950, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.382, mean reward: 0.164 [0.016, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.743, 10.407], loss: 0.003426, mae: 0.064496, mean_q: 0.316837
 74463/100000: episode: 951, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 16.945, mean reward: 0.169 [0.002, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.678, 10.098], loss: 0.003541, mae: 0.065022, mean_q: 0.316415
 74563/100000: episode: 952, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 17.144, mean reward: 0.171 [0.032, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.857, 10.271], loss: 0.003655, mae: 0.065919, mean_q: 0.318027
 74663/100000: episode: 953, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 20.305, mean reward: 0.203 [0.012, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.351, 10.223], loss: 0.003623, mae: 0.065489, mean_q: 0.322659
 74763/100000: episode: 954, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 16.039, mean reward: 0.160 [0.005, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.949, 10.122], loss: 0.003757, mae: 0.067587, mean_q: 0.324981
 74863/100000: episode: 955, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 11.926, mean reward: 0.119 [0.019, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.422, 10.135], loss: 0.003928, mae: 0.069419, mean_q: 0.327864
 74963/100000: episode: 956, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 18.785, mean reward: 0.188 [0.017, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.953, 10.098], loss: 0.003650, mae: 0.065947, mean_q: 0.328020
 75063/100000: episode: 957, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 11.820, mean reward: 0.118 [0.010, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.061, 10.098], loss: 0.003832, mae: 0.066908, mean_q: 0.324516
 75163/100000: episode: 958, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 23.949, mean reward: 0.239 [0.040, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.240], loss: 0.003604, mae: 0.065098, mean_q: 0.329159
 75263/100000: episode: 959, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 11.074, mean reward: 0.111 [0.020, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.444, 10.098], loss: 0.003382, mae: 0.064034, mean_q: 0.320208
 75363/100000: episode: 960, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.139, mean reward: 0.171 [0.004, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.098], loss: 0.003809, mae: 0.067214, mean_q: 0.323426
 75463/100000: episode: 961, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.585, mean reward: 0.146 [0.035, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.185, 10.098], loss: 0.003739, mae: 0.067498, mean_q: 0.320427
 75563/100000: episode: 962, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 14.757, mean reward: 0.148 [0.010, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.995, 10.131], loss: 0.003771, mae: 0.067641, mean_q: 0.323089
 75663/100000: episode: 963, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 18.892, mean reward: 0.189 [0.013, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.116, 10.144], loss: 0.003783, mae: 0.067218, mean_q: 0.318081
 75763/100000: episode: 964, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 18.698, mean reward: 0.187 [0.014, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.681, 10.204], loss: 0.003782, mae: 0.067709, mean_q: 0.321156
 75863/100000: episode: 965, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 14.033, mean reward: 0.140 [0.010, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.635, 10.298], loss: 0.003712, mae: 0.066331, mean_q: 0.316385
 75963/100000: episode: 966, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.991, mean reward: 0.180 [0.022, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.425, 10.128], loss: 0.003779, mae: 0.067097, mean_q: 0.320232
 76063/100000: episode: 967, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 19.614, mean reward: 0.196 [0.040, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.418, 10.247], loss: 0.003571, mae: 0.065698, mean_q: 0.324048
 76163/100000: episode: 968, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.011, mean reward: 0.150 [0.030, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.812, 10.272], loss: 0.003729, mae: 0.067104, mean_q: 0.321009
 76263/100000: episode: 969, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 16.988, mean reward: 0.170 [0.013, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.205, 10.112], loss: 0.003871, mae: 0.067878, mean_q: 0.320757
 76363/100000: episode: 970, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 14.725, mean reward: 0.147 [0.006, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.224, 10.325], loss: 0.003823, mae: 0.067423, mean_q: 0.323070
 76463/100000: episode: 971, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.781, mean reward: 0.148 [0.011, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.877, 10.098], loss: 0.003726, mae: 0.067192, mean_q: 0.324230
 76563/100000: episode: 972, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 19.120, mean reward: 0.191 [0.013, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.193, 10.131], loss: 0.003836, mae: 0.067968, mean_q: 0.321511
 76663/100000: episode: 973, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 19.671, mean reward: 0.197 [0.012, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.416, 10.329], loss: 0.003989, mae: 0.068808, mean_q: 0.329641
 76763/100000: episode: 974, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.661, mean reward: 0.187 [0.023, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.862, 10.098], loss: 0.003768, mae: 0.067830, mean_q: 0.325715
 76863/100000: episode: 975, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.189, mean reward: 0.162 [0.009, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.098], loss: 0.003825, mae: 0.068013, mean_q: 0.323296
 76963/100000: episode: 976, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.157, mean reward: 0.172 [0.004, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.864, 10.229], loss: 0.003595, mae: 0.065872, mean_q: 0.320761
 77063/100000: episode: 977, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 17.406, mean reward: 0.174 [0.036, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.915, 10.239], loss: 0.003883, mae: 0.068539, mean_q: 0.321622
 77163/100000: episode: 978, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 21.404, mean reward: 0.214 [0.006, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.373, 10.098], loss: 0.003778, mae: 0.067630, mean_q: 0.321764
 77263/100000: episode: 979, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 19.367, mean reward: 0.194 [0.015, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.676, 10.255], loss: 0.003665, mae: 0.066267, mean_q: 0.313764
 77363/100000: episode: 980, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 18.496, mean reward: 0.185 [0.058, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.441, 10.403], loss: 0.003825, mae: 0.067584, mean_q: 0.320295
 77463/100000: episode: 981, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 13.766, mean reward: 0.138 [0.011, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.393, 10.165], loss: 0.003993, mae: 0.068516, mean_q: 0.323500
 77563/100000: episode: 982, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.190, mean reward: 0.162 [0.011, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.649, 10.098], loss: 0.004055, mae: 0.070230, mean_q: 0.325011
 77663/100000: episode: 983, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 14.797, mean reward: 0.148 [0.002, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.185, 10.329], loss: 0.003877, mae: 0.067792, mean_q: 0.317701
 77763/100000: episode: 984, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 17.355, mean reward: 0.174 [0.024, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.952, 10.265], loss: 0.004015, mae: 0.069877, mean_q: 0.315363
 77863/100000: episode: 985, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 23.472, mean reward: 0.235 [0.003, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.720, 10.098], loss: 0.003875, mae: 0.068340, mean_q: 0.320170
 77963/100000: episode: 986, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 14.243, mean reward: 0.142 [0.008, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.889, 10.098], loss: 0.003987, mae: 0.069038, mean_q: 0.326287
 78063/100000: episode: 987, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 16.154, mean reward: 0.162 [0.016, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.271, 10.330], loss: 0.004005, mae: 0.069791, mean_q: 0.324874
 78163/100000: episode: 988, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 14.070, mean reward: 0.141 [0.005, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.234, 10.098], loss: 0.003857, mae: 0.068323, mean_q: 0.324519
 78263/100000: episode: 989, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 14.586, mean reward: 0.146 [0.011, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.274, 10.207], loss: 0.003839, mae: 0.068770, mean_q: 0.322756
 78363/100000: episode: 990, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 15.385, mean reward: 0.154 [0.015, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.524, 10.129], loss: 0.004005, mae: 0.069462, mean_q: 0.321921
 78463/100000: episode: 991, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 20.592, mean reward: 0.206 [0.035, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.336, 10.098], loss: 0.004205, mae: 0.071283, mean_q: 0.327279
 78563/100000: episode: 992, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 14.433, mean reward: 0.144 [0.006, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.489, 10.271], loss: 0.003938, mae: 0.069363, mean_q: 0.325827
 78663/100000: episode: 993, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 17.626, mean reward: 0.176 [0.010, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.145, 10.098], loss: 0.003934, mae: 0.069396, mean_q: 0.332563
 78763/100000: episode: 994, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 24.788, mean reward: 0.248 [0.054, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.351], loss: 0.003749, mae: 0.067774, mean_q: 0.328742
 78863/100000: episode: 995, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 13.847, mean reward: 0.138 [0.007, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.897, 10.098], loss: 0.003992, mae: 0.069768, mean_q: 0.333839
 78963/100000: episode: 996, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.616, mean reward: 0.166 [0.019, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.098], loss: 0.004061, mae: 0.070202, mean_q: 0.332273
 79063/100000: episode: 997, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 18.609, mean reward: 0.186 [0.006, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.144, 10.098], loss: 0.004003, mae: 0.069576, mean_q: 0.335932
 79163/100000: episode: 998, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.905, mean reward: 0.149 [0.002, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.642, 10.257], loss: 0.003863, mae: 0.068346, mean_q: 0.331545
 79263/100000: episode: 999, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.564, mean reward: 0.146 [0.017, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.764, 10.120], loss: 0.003975, mae: 0.069167, mean_q: 0.336708
 79363/100000: episode: 1000, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 20.489, mean reward: 0.205 [0.023, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.612, 10.281], loss: 0.004252, mae: 0.072005, mean_q: 0.331876
 79463/100000: episode: 1001, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 22.125, mean reward: 0.221 [0.030, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.128, 10.098], loss: 0.003953, mae: 0.069419, mean_q: 0.333282
 79563/100000: episode: 1002, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 15.767, mean reward: 0.158 [0.024, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.098], loss: 0.004224, mae: 0.071812, mean_q: 0.337755
 79663/100000: episode: 1003, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.019, mean reward: 0.160 [0.017, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.526, 10.098], loss: 0.004078, mae: 0.070395, mean_q: 0.334850
 79763/100000: episode: 1004, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.257, mean reward: 0.163 [0.027, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.143, 10.344], loss: 0.004139, mae: 0.071663, mean_q: 0.335814
 79863/100000: episode: 1005, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 17.462, mean reward: 0.175 [0.002, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.913, 10.100], loss: 0.003833, mae: 0.068641, mean_q: 0.337772
 79963/100000: episode: 1006, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.482, mean reward: 0.155 [0.015, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.213, 10.098], loss: 0.003995, mae: 0.069945, mean_q: 0.335020
 80063/100000: episode: 1007, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.811, mean reward: 0.148 [0.016, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.240, 10.184], loss: 0.004226, mae: 0.071681, mean_q: 0.340993
 80163/100000: episode: 1008, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.935, mean reward: 0.169 [0.020, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.010, 10.098], loss: 0.004129, mae: 0.070997, mean_q: 0.335289
 80263/100000: episode: 1009, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.128, mean reward: 0.151 [0.014, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.313, 10.161], loss: 0.004461, mae: 0.074159, mean_q: 0.337488
 80363/100000: episode: 1010, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 15.204, mean reward: 0.152 [0.031, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.584, 10.140], loss: 0.003881, mae: 0.068755, mean_q: 0.332145
 80463/100000: episode: 1011, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 16.517, mean reward: 0.165 [0.018, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.873, 10.289], loss: 0.004401, mae: 0.073197, mean_q: 0.337387
 80563/100000: episode: 1012, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 18.287, mean reward: 0.183 [0.014, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.130], loss: 0.003947, mae: 0.069719, mean_q: 0.334678
 80663/100000: episode: 1013, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 23.995, mean reward: 0.240 [0.029, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.025, 10.262], loss: 0.003844, mae: 0.069107, mean_q: 0.332725
 80763/100000: episode: 1014, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 14.999, mean reward: 0.150 [0.018, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.302, 10.196], loss: 0.003905, mae: 0.068568, mean_q: 0.335903
 80863/100000: episode: 1015, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 14.522, mean reward: 0.145 [0.010, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.507, 10.173], loss: 0.004038, mae: 0.069877, mean_q: 0.332822
 80963/100000: episode: 1016, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 15.853, mean reward: 0.159 [0.016, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.061, 10.098], loss: 0.004194, mae: 0.071589, mean_q: 0.338903
 81063/100000: episode: 1017, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 14.897, mean reward: 0.149 [0.030, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.227, 10.162], loss: 0.004309, mae: 0.071665, mean_q: 0.331334
 81163/100000: episode: 1018, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.213, mean reward: 0.162 [0.006, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.746, 10.098], loss: 0.003993, mae: 0.070004, mean_q: 0.334302
 81263/100000: episode: 1019, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 20.708, mean reward: 0.207 [0.025, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.105, 10.362], loss: 0.004320, mae: 0.072279, mean_q: 0.336189
[Info] Not found new level, current best level reached = inf
 81363/100000: episode: 1020, duration: 4.545s, episode steps: 100, steps per second: 22, episode reward: 15.668, mean reward: 0.157 [0.018, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.136, 10.098], loss: 0.004025, mae: 0.070596, mean_q: 0.338108
 81463/100000: episode: 1021, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 16.389, mean reward: 0.164 [0.033, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.988, 10.098], loss: 0.004086, mae: 0.070529, mean_q: 0.339449
 81563/100000: episode: 1022, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 15.514, mean reward: 0.155 [0.013, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.802, 10.098], loss: 0.003944, mae: 0.069086, mean_q: 0.340711
 81663/100000: episode: 1023, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.571, mean reward: 0.166 [0.019, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.761, 10.098], loss: 0.004067, mae: 0.070718, mean_q: 0.336915
 81763/100000: episode: 1024, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.807, mean reward: 0.148 [0.009, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.340, 10.265], loss: 0.004270, mae: 0.071990, mean_q: 0.329056
 81863/100000: episode: 1025, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 15.561, mean reward: 0.156 [0.010, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.738, 10.098], loss: 0.004133, mae: 0.070767, mean_q: 0.334644
 81963/100000: episode: 1026, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.322, mean reward: 0.163 [0.013, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.128, 10.355], loss: 0.004326, mae: 0.072551, mean_q: 0.335219
 82063/100000: episode: 1027, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.936, mean reward: 0.169 [0.014, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.313, 10.229], loss: 0.004274, mae: 0.072818, mean_q: 0.337309
 82163/100000: episode: 1028, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 15.343, mean reward: 0.153 [0.014, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.606, 10.098], loss: 0.004125, mae: 0.071409, mean_q: 0.329777
 82263/100000: episode: 1029, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 19.030, mean reward: 0.190 [0.027, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.715, 10.122], loss: 0.004344, mae: 0.072531, mean_q: 0.331122
 82363/100000: episode: 1030, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 21.447, mean reward: 0.214 [0.009, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.030, 10.296], loss: 0.004469, mae: 0.074634, mean_q: 0.331542
 82463/100000: episode: 1031, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.821, mean reward: 0.168 [0.018, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.913, 10.098], loss: 0.004521, mae: 0.073841, mean_q: 0.336308
 82563/100000: episode: 1032, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.004, mean reward: 0.170 [0.039, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.343, 10.098], loss: 0.004319, mae: 0.072443, mean_q: 0.333127
 82663/100000: episode: 1033, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.731, mean reward: 0.147 [0.021, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.821, 10.184], loss: 0.004164, mae: 0.071342, mean_q: 0.334350
 82763/100000: episode: 1034, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 14.406, mean reward: 0.144 [0.019, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.313, 10.180], loss: 0.003996, mae: 0.069555, mean_q: 0.332360
 82863/100000: episode: 1035, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.144, mean reward: 0.151 [0.013, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.922, 10.098], loss: 0.004150, mae: 0.071277, mean_q: 0.335696
 82963/100000: episode: 1036, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 20.627, mean reward: 0.206 [0.006, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.933, 10.098], loss: 0.004134, mae: 0.070685, mean_q: 0.331066
 83063/100000: episode: 1037, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 19.573, mean reward: 0.196 [0.050, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.033, 10.322], loss: 0.004291, mae: 0.072270, mean_q: 0.331634
 83163/100000: episode: 1038, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.164, mean reward: 0.162 [0.016, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.103], loss: 0.004280, mae: 0.072727, mean_q: 0.330212
 83263/100000: episode: 1039, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 26.082, mean reward: 0.261 [0.025, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.341], loss: 0.003971, mae: 0.069533, mean_q: 0.331660
 83363/100000: episode: 1040, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 14.485, mean reward: 0.145 [0.031, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.080, 10.098], loss: 0.004039, mae: 0.070318, mean_q: 0.338168
 83463/100000: episode: 1041, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.943, mean reward: 0.159 [0.011, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.973, 10.165], loss: 0.004058, mae: 0.070199, mean_q: 0.334105
 83563/100000: episode: 1042, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 18.237, mean reward: 0.182 [0.008, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.647, 10.098], loss: 0.004259, mae: 0.072660, mean_q: 0.334953
 83663/100000: episode: 1043, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.480, mean reward: 0.155 [0.008, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.228], loss: 0.004209, mae: 0.071382, mean_q: 0.336778
 83763/100000: episode: 1044, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 26.947, mean reward: 0.269 [0.061, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.582, 10.098], loss: 0.003957, mae: 0.069140, mean_q: 0.331559
 83863/100000: episode: 1045, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.986, mean reward: 0.170 [0.017, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.939, 10.201], loss: 0.004068, mae: 0.070564, mean_q: 0.335489
 83963/100000: episode: 1046, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 20.262, mean reward: 0.203 [0.015, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.278, 10.309], loss: 0.004213, mae: 0.071291, mean_q: 0.342121
 84063/100000: episode: 1047, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 21.618, mean reward: 0.216 [0.017, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.836, 10.098], loss: 0.004158, mae: 0.070789, mean_q: 0.343174
 84163/100000: episode: 1048, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 13.857, mean reward: 0.139 [0.004, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.098], loss: 0.004141, mae: 0.071226, mean_q: 0.342417
 84263/100000: episode: 1049, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 13.570, mean reward: 0.136 [0.018, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.098], loss: 0.004232, mae: 0.071341, mean_q: 0.338314
 84363/100000: episode: 1050, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.430, mean reward: 0.164 [0.005, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.947, 10.369], loss: 0.004204, mae: 0.071413, mean_q: 0.337234
 84463/100000: episode: 1051, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 19.300, mean reward: 0.193 [0.009, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.327], loss: 0.004110, mae: 0.070574, mean_q: 0.339192
 84563/100000: episode: 1052, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 19.822, mean reward: 0.198 [0.033, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.353], loss: 0.004303, mae: 0.071760, mean_q: 0.336963
 84663/100000: episode: 1053, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.150, mean reward: 0.172 [0.029, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.202, 10.202], loss: 0.004095, mae: 0.070463, mean_q: 0.342256
 84763/100000: episode: 1054, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 19.565, mean reward: 0.196 [0.046, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.063, 10.098], loss: 0.004138, mae: 0.070108, mean_q: 0.343208
 84863/100000: episode: 1055, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 14.606, mean reward: 0.146 [0.032, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.091, 10.163], loss: 0.004369, mae: 0.072646, mean_q: 0.346914
 84963/100000: episode: 1056, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.647, mean reward: 0.186 [0.039, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.502, 10.408], loss: 0.004264, mae: 0.070871, mean_q: 0.342022
 85063/100000: episode: 1057, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.542, mean reward: 0.165 [0.015, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.034, 10.170], loss: 0.004261, mae: 0.071628, mean_q: 0.342085
 85163/100000: episode: 1058, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 17.181, mean reward: 0.172 [0.032, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.159, 10.137], loss: 0.003890, mae: 0.069378, mean_q: 0.337611
 85263/100000: episode: 1059, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.849, mean reward: 0.168 [0.010, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.570, 10.098], loss: 0.004080, mae: 0.070245, mean_q: 0.339723
 85363/100000: episode: 1060, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.926, mean reward: 0.159 [0.007, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.467, 10.107], loss: 0.004006, mae: 0.069874, mean_q: 0.337496
 85463/100000: episode: 1061, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 13.750, mean reward: 0.137 [0.011, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.994, 10.098], loss: 0.003886, mae: 0.069132, mean_q: 0.343346
 85563/100000: episode: 1062, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.939, mean reward: 0.159 [0.013, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.235, 10.184], loss: 0.004160, mae: 0.071068, mean_q: 0.338991
 85663/100000: episode: 1063, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.614, mean reward: 0.166 [0.018, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.408, 10.310], loss: 0.004101, mae: 0.070479, mean_q: 0.337846
 85763/100000: episode: 1064, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 21.891, mean reward: 0.219 [0.011, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.667, 10.316], loss: 0.004307, mae: 0.072224, mean_q: 0.341846
 85863/100000: episode: 1065, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 13.673, mean reward: 0.137 [0.008, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.118, 10.098], loss: 0.004057, mae: 0.070392, mean_q: 0.343282
 85963/100000: episode: 1066, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 16.699, mean reward: 0.167 [0.018, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.098], loss: 0.004269, mae: 0.070858, mean_q: 0.344310
 86063/100000: episode: 1067, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 16.504, mean reward: 0.165 [0.022, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.174, 10.098], loss: 0.003858, mae: 0.069297, mean_q: 0.340329
 86163/100000: episode: 1068, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 12.186, mean reward: 0.122 [0.013, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.232, 10.098], loss: 0.004154, mae: 0.071363, mean_q: 0.339293
 86263/100000: episode: 1069, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 21.014, mean reward: 0.210 [0.022, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.089, 10.359], loss: 0.004019, mae: 0.070242, mean_q: 0.341316
 86363/100000: episode: 1070, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.712, mean reward: 0.167 [0.013, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.724, 10.098], loss: 0.003939, mae: 0.069053, mean_q: 0.340497
 86463/100000: episode: 1071, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 19.207, mean reward: 0.192 [0.025, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.315, 10.098], loss: 0.003995, mae: 0.069728, mean_q: 0.339949
 86563/100000: episode: 1072, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.400, mean reward: 0.154 [0.008, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.703, 10.098], loss: 0.004120, mae: 0.071154, mean_q: 0.343371
 86663/100000: episode: 1073, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 17.255, mean reward: 0.173 [0.011, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.176, 10.187], loss: 0.004097, mae: 0.070613, mean_q: 0.342472
 86763/100000: episode: 1074, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 18.362, mean reward: 0.184 [0.024, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.442, 10.173], loss: 0.004187, mae: 0.071122, mean_q: 0.342561
 86863/100000: episode: 1075, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 14.398, mean reward: 0.144 [0.015, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.392, 10.098], loss: 0.004233, mae: 0.071705, mean_q: 0.340264
 86963/100000: episode: 1076, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 18.358, mean reward: 0.184 [0.024, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.098], loss: 0.004207, mae: 0.071808, mean_q: 0.344760
 87063/100000: episode: 1077, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.331, mean reward: 0.183 [0.017, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.114, 10.098], loss: 0.004299, mae: 0.072556, mean_q: 0.348499
 87163/100000: episode: 1078, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.051, mean reward: 0.191 [0.042, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.570, 10.098], loss: 0.004319, mae: 0.071956, mean_q: 0.344245
 87263/100000: episode: 1079, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 14.748, mean reward: 0.147 [0.002, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.182], loss: 0.004092, mae: 0.070679, mean_q: 0.341776
 87363/100000: episode: 1080, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.401, mean reward: 0.154 [0.017, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.831, 10.187], loss: 0.004356, mae: 0.072589, mean_q: 0.341438
 87463/100000: episode: 1081, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 14.047, mean reward: 0.140 [0.012, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.548, 10.151], loss: 0.004229, mae: 0.071643, mean_q: 0.338088
 87563/100000: episode: 1082, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.669, mean reward: 0.137 [0.016, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.968, 10.136], loss: 0.003920, mae: 0.068835, mean_q: 0.338534
 87663/100000: episode: 1083, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 17.671, mean reward: 0.177 [0.014, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.205, 10.165], loss: 0.004126, mae: 0.070598, mean_q: 0.340545
 87763/100000: episode: 1084, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.424, mean reward: 0.154 [0.019, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.336, 10.120], loss: 0.004069, mae: 0.069574, mean_q: 0.335934
 87863/100000: episode: 1085, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.287, mean reward: 0.153 [0.013, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.701, 10.198], loss: 0.004315, mae: 0.072613, mean_q: 0.340896
 87963/100000: episode: 1086, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.263, mean reward: 0.163 [0.011, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.858, 10.291], loss: 0.004128, mae: 0.070703, mean_q: 0.336577
 88063/100000: episode: 1087, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 15.252, mean reward: 0.153 [0.025, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.642, 10.189], loss: 0.004586, mae: 0.075419, mean_q: 0.340361
 88163/100000: episode: 1088, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 25.887, mean reward: 0.259 [0.029, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.594, 10.461], loss: 0.003849, mae: 0.068215, mean_q: 0.333618
 88263/100000: episode: 1089, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 19.153, mean reward: 0.192 [0.021, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.428, 10.185], loss: 0.004156, mae: 0.071263, mean_q: 0.341520
 88363/100000: episode: 1090, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 11.412, mean reward: 0.114 [0.015, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.176], loss: 0.003965, mae: 0.069797, mean_q: 0.341268
 88463/100000: episode: 1091, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.995, mean reward: 0.170 [0.014, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.855, 10.359], loss: 0.004020, mae: 0.069191, mean_q: 0.332956
 88563/100000: episode: 1092, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 15.632, mean reward: 0.156 [0.005, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.103], loss: 0.004124, mae: 0.070019, mean_q: 0.334859
 88663/100000: episode: 1093, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 16.959, mean reward: 0.170 [0.001, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.098], loss: 0.004114, mae: 0.070148, mean_q: 0.336009
 88763/100000: episode: 1094, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 18.393, mean reward: 0.184 [0.020, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.474, 10.098], loss: 0.003926, mae: 0.069028, mean_q: 0.336058
 88863/100000: episode: 1095, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.985, mean reward: 0.160 [0.007, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.620, 10.279], loss: 0.003850, mae: 0.068452, mean_q: 0.331764
 88963/100000: episode: 1096, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.099, mean reward: 0.141 [0.006, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.094, 10.176], loss: 0.003990, mae: 0.069085, mean_q: 0.330182
 89063/100000: episode: 1097, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 13.828, mean reward: 0.138 [0.011, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.240, 10.098], loss: 0.004131, mae: 0.070494, mean_q: 0.330381
 89163/100000: episode: 1098, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 13.554, mean reward: 0.136 [0.014, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.422, 10.205], loss: 0.003826, mae: 0.068275, mean_q: 0.328581
 89263/100000: episode: 1099, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 19.373, mean reward: 0.194 [0.035, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.209], loss: 0.004045, mae: 0.069690, mean_q: 0.330636
 89363/100000: episode: 1100, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 15.354, mean reward: 0.154 [0.013, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.157], loss: 0.004003, mae: 0.069683, mean_q: 0.330918
 89463/100000: episode: 1101, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 14.606, mean reward: 0.146 [0.009, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.308, 10.098], loss: 0.003956, mae: 0.068856, mean_q: 0.329679
 89563/100000: episode: 1102, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 17.926, mean reward: 0.179 [0.020, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.527, 10.287], loss: 0.003930, mae: 0.069405, mean_q: 0.329581
 89663/100000: episode: 1103, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.975, mean reward: 0.170 [0.015, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.825, 10.098], loss: 0.004004, mae: 0.069945, mean_q: 0.329441
 89763/100000: episode: 1104, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 15.841, mean reward: 0.158 [0.007, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.270, 10.235], loss: 0.004025, mae: 0.070171, mean_q: 0.322397
 89863/100000: episode: 1105, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 15.496, mean reward: 0.155 [0.011, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.191], loss: 0.004170, mae: 0.070572, mean_q: 0.329325
 89963/100000: episode: 1106, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 21.786, mean reward: 0.218 [0.034, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.740, 10.387], loss: 0.003901, mae: 0.069425, mean_q: 0.327505
 90063/100000: episode: 1107, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 14.985, mean reward: 0.150 [0.021, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.678, 10.192], loss: 0.004283, mae: 0.072162, mean_q: 0.327377
 90163/100000: episode: 1108, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 20.837, mean reward: 0.208 [0.017, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.570, 10.121], loss: 0.004107, mae: 0.070197, mean_q: 0.326144
 90263/100000: episode: 1109, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 18.488, mean reward: 0.185 [0.016, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.235], loss: 0.003896, mae: 0.068870, mean_q: 0.328312
 90363/100000: episode: 1110, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 17.249, mean reward: 0.172 [0.030, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.821, 10.295], loss: 0.004191, mae: 0.071944, mean_q: 0.329247
 90463/100000: episode: 1111, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 15.869, mean reward: 0.159 [0.019, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.177, 10.098], loss: 0.004104, mae: 0.070310, mean_q: 0.331837
 90563/100000: episode: 1112, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 17.132, mean reward: 0.171 [0.028, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.212, 10.098], loss: 0.003809, mae: 0.068475, mean_q: 0.328988
 90663/100000: episode: 1113, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 14.593, mean reward: 0.146 [0.002, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.167], loss: 0.003875, mae: 0.069127, mean_q: 0.329456
 90763/100000: episode: 1114, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.527, mean reward: 0.155 [0.018, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.125, 10.098], loss: 0.003914, mae: 0.068857, mean_q: 0.325229
 90863/100000: episode: 1115, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 17.072, mean reward: 0.171 [0.031, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.020, 10.098], loss: 0.004032, mae: 0.069820, mean_q: 0.329596
 90963/100000: episode: 1116, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.826, mean reward: 0.148 [0.015, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.792, 10.122], loss: 0.004058, mae: 0.070538, mean_q: 0.327946
 91063/100000: episode: 1117, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 19.366, mean reward: 0.194 [0.023, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.415, 10.098], loss: 0.003681, mae: 0.067243, mean_q: 0.324400
 91163/100000: episode: 1118, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 19.284, mean reward: 0.193 [0.043, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.129, 10.098], loss: 0.004101, mae: 0.070633, mean_q: 0.326639
 91263/100000: episode: 1119, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 16.220, mean reward: 0.162 [0.005, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.004, 10.347], loss: 0.003601, mae: 0.067091, mean_q: 0.324542
[Info] Not found new level, current best level reached = inf
 91363/100000: episode: 1120, duration: 4.499s, episode steps: 100, steps per second: 22, episode reward: 21.597, mean reward: 0.216 [0.009, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.313, 10.098], loss: 0.003809, mae: 0.067919, mean_q: 0.328112
 91463/100000: episode: 1121, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.944, mean reward: 0.169 [0.032, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.435, 10.098], loss: 0.003820, mae: 0.068414, mean_q: 0.325481
 91563/100000: episode: 1122, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 15.582, mean reward: 0.156 [0.028, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.962, 10.319], loss: 0.003842, mae: 0.068689, mean_q: 0.330918
 91663/100000: episode: 1123, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.400, mean reward: 0.174 [0.013, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.247, 10.287], loss: 0.003894, mae: 0.068856, mean_q: 0.329776
 91763/100000: episode: 1124, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 14.525, mean reward: 0.145 [0.006, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.658, 10.190], loss: 0.004265, mae: 0.072195, mean_q: 0.328221
 91863/100000: episode: 1125, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 14.068, mean reward: 0.141 [0.027, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.081, 10.098], loss: 0.003973, mae: 0.070029, mean_q: 0.329548
 91963/100000: episode: 1126, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 20.465, mean reward: 0.205 [0.030, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.677, 10.369], loss: 0.003677, mae: 0.067550, mean_q: 0.328623
 92063/100000: episode: 1127, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 24.163, mean reward: 0.242 [0.013, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.228, 10.098], loss: 0.003936, mae: 0.070040, mean_q: 0.334835
 92163/100000: episode: 1128, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.374, mean reward: 0.164 [0.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.289, 10.139], loss: 0.003931, mae: 0.069863, mean_q: 0.334161
 92263/100000: episode: 1129, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.486, mean reward: 0.175 [0.023, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.556, 10.193], loss: 0.003834, mae: 0.068371, mean_q: 0.330685
 92363/100000: episode: 1130, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.954, mean reward: 0.160 [0.044, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.160], loss: 0.003869, mae: 0.068623, mean_q: 0.328506
 92463/100000: episode: 1131, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.056, mean reward: 0.161 [0.018, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.979, 10.360], loss: 0.003800, mae: 0.068088, mean_q: 0.329886
 92563/100000: episode: 1132, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 22.878, mean reward: 0.229 [0.015, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.310, 10.216], loss: 0.003734, mae: 0.067470, mean_q: 0.331339
 92663/100000: episode: 1133, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 14.421, mean reward: 0.144 [0.009, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.281], loss: 0.004095, mae: 0.070512, mean_q: 0.338673
 92763/100000: episode: 1134, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.792, mean reward: 0.168 [0.011, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.802, 10.175], loss: 0.003603, mae: 0.065981, mean_q: 0.335629
 92863/100000: episode: 1135, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 22.757, mean reward: 0.228 [0.014, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.098], loss: 0.003505, mae: 0.066384, mean_q: 0.337648
 92963/100000: episode: 1136, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.031, mean reward: 0.160 [0.011, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.585, 10.181], loss: 0.003910, mae: 0.068984, mean_q: 0.337368
 93063/100000: episode: 1137, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 22.960, mean reward: 0.230 [0.031, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.064, 10.321], loss: 0.003794, mae: 0.069185, mean_q: 0.341642
 93163/100000: episode: 1138, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 13.215, mean reward: 0.132 [0.022, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.483, 10.218], loss: 0.003811, mae: 0.068524, mean_q: 0.338674
 93263/100000: episode: 1139, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 22.587, mean reward: 0.226 [0.002, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.809, 10.301], loss: 0.003644, mae: 0.066980, mean_q: 0.335740
 93363/100000: episode: 1140, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 17.719, mean reward: 0.177 [0.020, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.529, 10.099], loss: 0.003603, mae: 0.066176, mean_q: 0.342610
 93463/100000: episode: 1141, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 19.066, mean reward: 0.191 [0.024, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.099, 10.098], loss: 0.004082, mae: 0.070633, mean_q: 0.343469
 93563/100000: episode: 1142, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.919, mean reward: 0.159 [0.012, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.103], loss: 0.003753, mae: 0.067401, mean_q: 0.341167
 93663/100000: episode: 1143, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 15.456, mean reward: 0.155 [0.010, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.878, 10.268], loss: 0.003833, mae: 0.069643, mean_q: 0.338811
 93763/100000: episode: 1144, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 20.953, mean reward: 0.210 [0.033, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.331], loss: 0.004084, mae: 0.071630, mean_q: 0.337738
 93863/100000: episode: 1145, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 23.765, mean reward: 0.238 [0.033, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.410, 10.304], loss: 0.004035, mae: 0.070950, mean_q: 0.343393
 93963/100000: episode: 1146, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 14.564, mean reward: 0.146 [0.036, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.869, 10.137], loss: 0.004255, mae: 0.072792, mean_q: 0.347190
 94063/100000: episode: 1147, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 17.124, mean reward: 0.171 [0.038, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.286], loss: 0.003834, mae: 0.069057, mean_q: 0.339687
 94163/100000: episode: 1148, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.787, mean reward: 0.158 [0.014, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.501, 10.098], loss: 0.003889, mae: 0.068509, mean_q: 0.343721
 94263/100000: episode: 1149, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 12.879, mean reward: 0.129 [0.007, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.859, 10.134], loss: 0.003918, mae: 0.069319, mean_q: 0.349741
 94363/100000: episode: 1150, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 19.155, mean reward: 0.192 [0.014, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.650, 10.098], loss: 0.003853, mae: 0.069137, mean_q: 0.345965
 94463/100000: episode: 1151, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.278, mean reward: 0.153 [0.020, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.809, 10.154], loss: 0.003742, mae: 0.067487, mean_q: 0.347989
 94563/100000: episode: 1152, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 17.547, mean reward: 0.175 [0.018, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.443, 10.099], loss: 0.003801, mae: 0.068955, mean_q: 0.349362
 94663/100000: episode: 1153, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 25.981, mean reward: 0.260 [0.020, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.589, 10.098], loss: 0.003692, mae: 0.067656, mean_q: 0.341920
 94763/100000: episode: 1154, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 18.422, mean reward: 0.184 [0.014, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.400, 10.283], loss: 0.003617, mae: 0.066461, mean_q: 0.349236
 94863/100000: episode: 1155, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 15.718, mean reward: 0.157 [0.009, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.679, 10.098], loss: 0.003687, mae: 0.066902, mean_q: 0.349238
 94963/100000: episode: 1156, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 14.782, mean reward: 0.148 [0.023, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.053, 10.222], loss: 0.004018, mae: 0.070401, mean_q: 0.351079
 95063/100000: episode: 1157, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 13.292, mean reward: 0.133 [0.008, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.473, 10.098], loss: 0.003883, mae: 0.069551, mean_q: 0.350359
 95163/100000: episode: 1158, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.756, mean reward: 0.168 [0.012, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.460, 10.098], loss: 0.003887, mae: 0.069193, mean_q: 0.350340
 95263/100000: episode: 1159, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 18.664, mean reward: 0.187 [0.036, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.814, 10.278], loss: 0.003735, mae: 0.067302, mean_q: 0.349733
 95363/100000: episode: 1160, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.004, mean reward: 0.150 [0.023, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.098], loss: 0.003868, mae: 0.069060, mean_q: 0.346145
 95463/100000: episode: 1161, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.300, mean reward: 0.143 [0.009, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.421, 10.238], loss: 0.003601, mae: 0.066570, mean_q: 0.345009
 95563/100000: episode: 1162, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 14.629, mean reward: 0.146 [0.005, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.167], loss: 0.003835, mae: 0.069130, mean_q: 0.345847
 95663/100000: episode: 1163, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.339, mean reward: 0.173 [0.026, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.918, 10.499], loss: 0.003725, mae: 0.068153, mean_q: 0.347974
 95763/100000: episode: 1164, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 20.758, mean reward: 0.208 [0.027, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.465, 10.400], loss: 0.003688, mae: 0.067329, mean_q: 0.348745
 95863/100000: episode: 1165, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.725, mean reward: 0.147 [0.031, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.273, 10.098], loss: 0.003691, mae: 0.067665, mean_q: 0.348392
 95963/100000: episode: 1166, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 17.089, mean reward: 0.171 [0.017, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.228, 10.098], loss: 0.003805, mae: 0.068493, mean_q: 0.350278
 96063/100000: episode: 1167, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 18.550, mean reward: 0.186 [0.008, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.254], loss: 0.003903, mae: 0.069514, mean_q: 0.346737
 96163/100000: episode: 1168, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 19.074, mean reward: 0.191 [0.021, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.566, 10.098], loss: 0.003960, mae: 0.069045, mean_q: 0.347841
 96263/100000: episode: 1169, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 21.852, mean reward: 0.219 [0.026, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.644, 10.098], loss: 0.003808, mae: 0.068140, mean_q: 0.346191
 96363/100000: episode: 1170, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 14.745, mean reward: 0.147 [0.012, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.580, 10.188], loss: 0.003739, mae: 0.067188, mean_q: 0.342485
 96463/100000: episode: 1171, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 15.825, mean reward: 0.158 [0.009, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.243], loss: 0.003824, mae: 0.069762, mean_q: 0.345537
 96563/100000: episode: 1172, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 14.685, mean reward: 0.147 [0.010, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.410, 10.224], loss: 0.003708, mae: 0.067660, mean_q: 0.342431
 96663/100000: episode: 1173, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 13.870, mean reward: 0.139 [0.012, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.390, 10.098], loss: 0.003760, mae: 0.067988, mean_q: 0.344755
 96763/100000: episode: 1174, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 17.185, mean reward: 0.172 [0.022, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.401], loss: 0.003742, mae: 0.067349, mean_q: 0.346112
 96863/100000: episode: 1175, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 17.599, mean reward: 0.176 [0.026, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.391, 10.184], loss: 0.003760, mae: 0.067043, mean_q: 0.346895
 96963/100000: episode: 1176, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 23.500, mean reward: 0.235 [0.041, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.098], loss: 0.003851, mae: 0.068111, mean_q: 0.345191
 97063/100000: episode: 1177, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 24.163, mean reward: 0.242 [0.020, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.499, 10.286], loss: 0.003689, mae: 0.068069, mean_q: 0.348471
 97163/100000: episode: 1178, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 19.650, mean reward: 0.197 [0.003, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.651, 10.098], loss: 0.003808, mae: 0.067809, mean_q: 0.347793
 97263/100000: episode: 1179, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.357, mean reward: 0.154 [0.015, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.304, 10.259], loss: 0.003769, mae: 0.067637, mean_q: 0.349372
 97363/100000: episode: 1180, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.764, mean reward: 0.158 [0.016, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.366, 10.147], loss: 0.003586, mae: 0.066224, mean_q: 0.346740
 97463/100000: episode: 1181, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 15.144, mean reward: 0.151 [0.010, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.713, 10.136], loss: 0.003736, mae: 0.067383, mean_q: 0.348457
 97563/100000: episode: 1182, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 20.699, mean reward: 0.207 [0.022, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.742, 10.098], loss: 0.004001, mae: 0.069927, mean_q: 0.348839
 97663/100000: episode: 1183, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 16.379, mean reward: 0.164 [0.007, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.931, 10.098], loss: 0.003964, mae: 0.069846, mean_q: 0.343049
 97763/100000: episode: 1184, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.804, mean reward: 0.148 [0.019, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.188, 10.098], loss: 0.004016, mae: 0.069747, mean_q: 0.347547
 97863/100000: episode: 1185, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 18.872, mean reward: 0.189 [0.021, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.296, 10.308], loss: 0.003864, mae: 0.068442, mean_q: 0.347635
 97963/100000: episode: 1186, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 17.122, mean reward: 0.171 [0.002, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.154, 10.321], loss: 0.003599, mae: 0.065928, mean_q: 0.342668
 98063/100000: episode: 1187, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.161, mean reward: 0.182 [0.004, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.343, 10.194], loss: 0.003592, mae: 0.066418, mean_q: 0.344690
 98163/100000: episode: 1188, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 23.335, mean reward: 0.233 [0.018, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.222, 10.310], loss: 0.003668, mae: 0.066568, mean_q: 0.344372
 98263/100000: episode: 1189, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 16.124, mean reward: 0.161 [0.014, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.098], loss: 0.003807, mae: 0.068409, mean_q: 0.343234
 98363/100000: episode: 1190, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 18.432, mean reward: 0.184 [0.037, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.120, 10.415], loss: 0.003788, mae: 0.067855, mean_q: 0.348921
 98463/100000: episode: 1191, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 15.672, mean reward: 0.157 [0.008, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.483, 10.110], loss: 0.003796, mae: 0.067604, mean_q: 0.343653
 98563/100000: episode: 1192, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 14.790, mean reward: 0.148 [0.024, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.607, 10.098], loss: 0.003931, mae: 0.069149, mean_q: 0.345063
 98663/100000: episode: 1193, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 20.176, mean reward: 0.202 [0.030, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.098], loss: 0.003661, mae: 0.066628, mean_q: 0.344068
 98763/100000: episode: 1194, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 15.794, mean reward: 0.158 [0.006, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.430, 10.112], loss: 0.003864, mae: 0.069247, mean_q: 0.345438
 98863/100000: episode: 1195, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 15.828, mean reward: 0.158 [0.018, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.188], loss: 0.004097, mae: 0.070074, mean_q: 0.337456
 98963/100000: episode: 1196, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.381, mean reward: 0.134 [0.006, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.342, 10.243], loss: 0.003756, mae: 0.067987, mean_q: 0.335655
 99063/100000: episode: 1197, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 15.931, mean reward: 0.159 [0.008, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.471, 10.098], loss: 0.003764, mae: 0.067690, mean_q: 0.333191
 99163/100000: episode: 1198, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 19.176, mean reward: 0.192 [0.034, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.352, 10.159], loss: 0.003764, mae: 0.067583, mean_q: 0.340911
 99263/100000: episode: 1199, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 18.280, mean reward: 0.183 [0.027, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.440, 10.098], loss: 0.003941, mae: 0.069466, mean_q: 0.340473
 99363/100000: episode: 1200, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.134, mean reward: 0.161 [0.013, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.928, 10.098], loss: 0.004189, mae: 0.071500, mean_q: 0.345820
 99463/100000: episode: 1201, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.094, mean reward: 0.141 [0.017, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.222, 10.098], loss: 0.003806, mae: 0.068673, mean_q: 0.344337
 99563/100000: episode: 1202, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.322, mean reward: 0.153 [0.020, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.768, 10.098], loss: 0.003631, mae: 0.066082, mean_q: 0.341728
 99663/100000: episode: 1203, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 22.333, mean reward: 0.223 [0.012, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.429, 10.247], loss: 0.003878, mae: 0.068348, mean_q: 0.339531
 99763/100000: episode: 1204, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.948, mean reward: 0.189 [0.032, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.494, 10.098], loss: 0.003834, mae: 0.068502, mean_q: 0.341338
 99863/100000: episode: 1205, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 14.374, mean reward: 0.144 [0.017, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.555, 10.098], loss: 0.004293, mae: 0.073498, mean_q: 0.338861
 99963/100000: episode: 1206, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 16.602, mean reward: 0.166 [0.050, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.213], loss: 0.003881, mae: 0.068608, mean_q: 0.338676
done, took 578.772 seconds
[Info] End Importance Splitting.
