Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.175s, episode steps: 100, steps per second: 571, episode reward: 189.314, mean reward: 1.893 [1.450, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.988, 10.378], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.070s, episode steps: 100, steps per second: 1419, episode reward: 196.804, mean reward: 1.968 [1.472, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.310, 10.147], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.068s, episode steps: 100, steps per second: 1462, episode reward: 195.807, mean reward: 1.958 [1.475, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.418, 10.123], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.087s, episode steps: 100, steps per second: 1151, episode reward: 184.399, mean reward: 1.844 [1.464, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.670, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.083s, episode steps: 100, steps per second: 1201, episode reward: 184.354, mean reward: 1.844 [1.441, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.919, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 187.414, mean reward: 1.874 [1.436, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.666, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 185.261, mean reward: 1.853 [1.460, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.004, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 194.910, mean reward: 1.949 [1.455, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.328, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.069s, episode steps: 100, steps per second: 1453, episode reward: 191.066, mean reward: 1.911 [1.453, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.360, 10.208], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.072s, episode steps: 100, steps per second: 1392, episode reward: 196.711, mean reward: 1.967 [1.453, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.875, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 202.400, mean reward: 2.024 [1.473, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.378, 10.158], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 193.668, mean reward: 1.937 [1.484, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.586, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.085s, episode steps: 100, steps per second: 1183, episode reward: 204.599, mean reward: 2.046 [1.436, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.079s, episode steps: 100, steps per second: 1259, episode reward: 187.923, mean reward: 1.879 [1.445, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.905, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.065s, episode steps: 100, steps per second: 1540, episode reward: 186.747, mean reward: 1.867 [1.483, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.138, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 182.870, mean reward: 1.829 [1.448, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.533, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 192.606, mean reward: 1.926 [1.444, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.653, 10.107], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 192.491, mean reward: 1.925 [1.488, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.069, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 199.093, mean reward: 1.991 [1.461, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.264, 10.298], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 187.548, mean reward: 1.875 [1.447, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.211, 10.343], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.071s, episode steps: 100, steps per second: 1400, episode reward: 187.737, mean reward: 1.877 [1.457, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.967, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.090s, episode steps: 100, steps per second: 1117, episode reward: 184.484, mean reward: 1.845 [1.453, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.171, 10.311], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 193.556, mean reward: 1.936 [1.491, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.118, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 183.973, mean reward: 1.840 [1.478, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.977, 10.143], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 193.329, mean reward: 1.933 [1.485, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.917, 10.107], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.076s, episode steps: 100, steps per second: 1310, episode reward: 180.645, mean reward: 1.806 [1.461, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.650, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.068s, episode steps: 100, steps per second: 1463, episode reward: 196.913, mean reward: 1.969 [1.435, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.043, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 187.318, mean reward: 1.873 [1.459, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.665, 10.153], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 186.856, mean reward: 1.869 [1.532, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.209], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.065s, episode steps: 100, steps per second: 1531, episode reward: 198.679, mean reward: 1.987 [1.482, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.340, 10.213], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.065s, episode steps: 100, steps per second: 1546, episode reward: 183.290, mean reward: 1.833 [1.470, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.033, 10.295], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 210.605, mean reward: 2.106 [1.470, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.479, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 188.704, mean reward: 1.887 [1.482, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.985, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 187.430, mean reward: 1.874 [1.463, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.673, 10.157], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.073s, episode steps: 100, steps per second: 1364, episode reward: 183.555, mean reward: 1.836 [1.465, 2.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.063s, episode steps: 100, steps per second: 1583, episode reward: 195.982, mean reward: 1.960 [1.436, 6.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.923, 10.321], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 215.465, mean reward: 2.155 [1.433, 9.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.907, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 178.256, mean reward: 1.783 [1.486, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.239, 10.157], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 195.428, mean reward: 1.954 [1.481, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.774, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.069s, episode steps: 100, steps per second: 1456, episode reward: 187.931, mean reward: 1.879 [1.467, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.360, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.069s, episode steps: 100, steps per second: 1458, episode reward: 251.302, mean reward: 2.513 [1.506, 5.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.894, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 197.462, mean reward: 1.975 [1.463, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.122, 10.110], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 181.570, mean reward: 1.816 [1.472, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 192.215, mean reward: 1.922 [1.492, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.178, 10.190], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 189.783, mean reward: 1.898 [1.496, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.708, 10.246], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 228.614, mean reward: 2.286 [1.482, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 201.065, mean reward: 2.011 [1.460, 6.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.161, 10.125], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.065s, episode steps: 100, steps per second: 1549, episode reward: 184.403, mean reward: 1.844 [1.442, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.129, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 186.642, mean reward: 1.866 [1.464, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.974, 10.106], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 213.934, mean reward: 2.139 [1.456, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.925, 10.301], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.270s, episode steps: 100, steps per second: 79, episode reward: 221.702, mean reward: 2.217 [1.478, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.112, 10.121], loss: 0.286462, mae: 0.510968, mean_q: 1.878092
  5200/100000: episode: 52, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 198.617, mean reward: 1.986 [1.444, 5.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.148], loss: 0.108095, mae: 0.314805, mean_q: 2.769913
  5300/100000: episode: 53, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.244, mean reward: 1.842 [1.462, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.801, 10.098], loss: 0.116875, mae: 0.321696, mean_q: 3.164139
  5400/100000: episode: 54, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 184.382, mean reward: 1.844 [1.443, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.526, 10.178], loss: 0.127257, mae: 0.316898, mean_q: 3.412982
  5500/100000: episode: 55, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.613, mean reward: 1.856 [1.438, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.516, 10.098], loss: 0.120673, mae: 0.319965, mean_q: 3.569677
  5600/100000: episode: 56, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 216.684, mean reward: 2.167 [1.457, 6.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.883, 10.098], loss: 0.112816, mae: 0.311788, mean_q: 3.675884
  5700/100000: episode: 57, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 184.360, mean reward: 1.844 [1.455, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.568, 10.105], loss: 0.121412, mae: 0.317732, mean_q: 3.747991
  5800/100000: episode: 58, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 181.434, mean reward: 1.814 [1.442, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.900, 10.098], loss: 0.106364, mae: 0.310822, mean_q: 3.784667
  5900/100000: episode: 59, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 190.459, mean reward: 1.905 [1.476, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.147, 10.133], loss: 0.134644, mae: 0.320535, mean_q: 3.809265
  6000/100000: episode: 60, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 190.662, mean reward: 1.907 [1.460, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.142, 10.249], loss: 0.131367, mae: 0.324344, mean_q: 3.826087
  6100/100000: episode: 61, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.101, mean reward: 1.871 [1.486, 5.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.117, 10.098], loss: 0.131449, mae: 0.322824, mean_q: 3.840674
  6200/100000: episode: 62, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.585, mean reward: 1.826 [1.485, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.609, 10.156], loss: 0.126812, mae: 0.321546, mean_q: 3.849886
  6300/100000: episode: 63, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.453, mean reward: 1.885 [1.444, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.753, 10.098], loss: 0.121972, mae: 0.313402, mean_q: 3.829674
  6400/100000: episode: 64, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 200.373, mean reward: 2.004 [1.469, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.061, 10.098], loss: 0.100643, mae: 0.304696, mean_q: 3.824372
  6500/100000: episode: 65, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.564, mean reward: 1.956 [1.456, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.341, 10.353], loss: 0.118058, mae: 0.311240, mean_q: 3.853936
  6600/100000: episode: 66, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 210.104, mean reward: 2.101 [1.458, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.691, 10.098], loss: 0.118377, mae: 0.315422, mean_q: 3.837344
  6700/100000: episode: 67, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 190.259, mean reward: 1.903 [1.467, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.752, 10.098], loss: 0.139302, mae: 0.325780, mean_q: 3.846134
  6800/100000: episode: 68, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 200.998, mean reward: 2.010 [1.473, 5.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.793, 10.098], loss: 0.138628, mae: 0.333139, mean_q: 3.849544
  6900/100000: episode: 69, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 188.237, mean reward: 1.882 [1.455, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.586, 10.098], loss: 0.123088, mae: 0.316309, mean_q: 3.842717
  7000/100000: episode: 70, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 202.116, mean reward: 2.021 [1.465, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.771, 10.443], loss: 0.108124, mae: 0.307787, mean_q: 3.849878
  7100/100000: episode: 71, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 184.283, mean reward: 1.843 [1.465, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.098], loss: 0.107465, mae: 0.313442, mean_q: 3.847712
  7200/100000: episode: 72, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 180.835, mean reward: 1.808 [1.445, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.604, 10.098], loss: 0.123608, mae: 0.312919, mean_q: 3.844344
  7300/100000: episode: 73, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.060, mean reward: 1.931 [1.469, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.615, 10.221], loss: 0.125284, mae: 0.326873, mean_q: 3.841319
  7400/100000: episode: 74, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 229.435, mean reward: 2.294 [1.490, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.383, 10.330], loss: 0.126230, mae: 0.328825, mean_q: 3.864654
  7500/100000: episode: 75, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 180.343, mean reward: 1.803 [1.464, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.793, 10.131], loss: 0.123719, mae: 0.325432, mean_q: 3.855096
  7600/100000: episode: 76, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 191.360, mean reward: 1.914 [1.442, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.311, 10.098], loss: 0.107930, mae: 0.311602, mean_q: 3.864410
  7700/100000: episode: 77, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.437, mean reward: 1.824 [1.454, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.233, 10.110], loss: 0.123306, mae: 0.316455, mean_q: 3.856281
  7800/100000: episode: 78, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 187.859, mean reward: 1.879 [1.450, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.520, 10.331], loss: 0.124954, mae: 0.317176, mean_q: 3.843215
  7900/100000: episode: 79, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 192.949, mean reward: 1.929 [1.452, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.893, 10.379], loss: 0.118814, mae: 0.319239, mean_q: 3.846516
  8000/100000: episode: 80, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 206.278, mean reward: 2.063 [1.478, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.535, 10.307], loss: 0.121682, mae: 0.325989, mean_q: 3.869921
  8100/100000: episode: 81, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 186.718, mean reward: 1.867 [1.464, 5.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.699, 10.113], loss: 0.099952, mae: 0.300171, mean_q: 3.863125
  8200/100000: episode: 82, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.832, mean reward: 2.008 [1.472, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.261, 10.098], loss: 0.118034, mae: 0.320116, mean_q: 3.879526
  8300/100000: episode: 83, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 206.578, mean reward: 2.066 [1.460, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.903, 10.246], loss: 0.138847, mae: 0.327721, mean_q: 3.879893
  8400/100000: episode: 84, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 201.567, mean reward: 2.016 [1.434, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.337, 10.098], loss: 0.121887, mae: 0.319240, mean_q: 3.867490
  8500/100000: episode: 85, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.930, mean reward: 1.939 [1.491, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.319, 10.137], loss: 0.121197, mae: 0.320496, mean_q: 3.863226
  8600/100000: episode: 86, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 186.676, mean reward: 1.867 [1.463, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.722, 10.098], loss: 0.115016, mae: 0.323672, mean_q: 3.886760
  8700/100000: episode: 87, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 187.169, mean reward: 1.872 [1.474, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.713, 10.098], loss: 0.109192, mae: 0.319076, mean_q: 3.878076
  8800/100000: episode: 88, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 210.494, mean reward: 2.105 [1.497, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.887, 10.215], loss: 0.123048, mae: 0.327026, mean_q: 3.890630
  8900/100000: episode: 89, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 188.712, mean reward: 1.887 [1.499, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.164], loss: 0.099671, mae: 0.306225, mean_q: 3.882965
  9000/100000: episode: 90, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.214, mean reward: 1.892 [1.479, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.098], loss: 0.105491, mae: 0.310279, mean_q: 3.882922
  9100/100000: episode: 91, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 196.287, mean reward: 1.963 [1.465, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.308, 10.409], loss: 0.110119, mae: 0.317256, mean_q: 3.878142
  9200/100000: episode: 92, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 182.292, mean reward: 1.823 [1.434, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.204, 10.098], loss: 0.095538, mae: 0.300585, mean_q: 3.859348
  9300/100000: episode: 93, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 194.460, mean reward: 1.945 [1.469, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.061, 10.486], loss: 0.097766, mae: 0.308924, mean_q: 3.860927
  9400/100000: episode: 94, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 337.888, mean reward: 3.379 [1.504, 15.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.218, 10.098], loss: 0.117316, mae: 0.325678, mean_q: 3.870087
  9500/100000: episode: 95, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 196.569, mean reward: 1.966 [1.480, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.361, 10.098], loss: 0.190688, mae: 0.365509, mean_q: 3.922530
  9600/100000: episode: 96, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 193.510, mean reward: 1.935 [1.436, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.827, 10.128], loss: 0.195964, mae: 0.338352, mean_q: 3.891908
  9700/100000: episode: 97, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 205.336, mean reward: 2.053 [1.481, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.566, 10.098], loss: 0.220094, mae: 0.355517, mean_q: 3.919573
  9800/100000: episode: 98, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 239.320, mean reward: 2.393 [1.535, 4.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.976, 10.098], loss: 0.171587, mae: 0.333004, mean_q: 3.908918
  9900/100000: episode: 99, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 178.184, mean reward: 1.782 [1.447, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.604, 10.170], loss: 0.213202, mae: 0.354738, mean_q: 3.929920
 10000/100000: episode: 100, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.424, mean reward: 1.854 [1.439, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.444, 10.226], loss: 0.212924, mae: 0.345119, mean_q: 3.905524
 10100/100000: episode: 101, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.934, mean reward: 1.849 [1.453, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.880, 10.161], loss: 0.174450, mae: 0.340960, mean_q: 3.902763
 10200/100000: episode: 102, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.575, mean reward: 1.886 [1.471, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.098], loss: 0.184800, mae: 0.342161, mean_q: 3.893053
 10300/100000: episode: 103, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 187.187, mean reward: 1.872 [1.464, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.133], loss: 0.155310, mae: 0.332759, mean_q: 3.897182
 10400/100000: episode: 104, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 217.753, mean reward: 2.178 [1.456, 6.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.978, 10.098], loss: 0.152802, mae: 0.330594, mean_q: 3.890764
 10500/100000: episode: 105, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 185.652, mean reward: 1.857 [1.448, 5.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.256], loss: 0.156322, mae: 0.339875, mean_q: 3.910096
 10600/100000: episode: 106, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 214.824, mean reward: 2.148 [1.464, 9.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.090, 10.098], loss: 0.147772, mae: 0.338852, mean_q: 3.928536
 10700/100000: episode: 107, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.646, mean reward: 1.876 [1.454, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.744, 10.109], loss: 0.227162, mae: 0.358340, mean_q: 3.916525
 10800/100000: episode: 108, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 195.629, mean reward: 1.956 [1.475, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.098], loss: 0.179963, mae: 0.348628, mean_q: 3.913551
 10900/100000: episode: 109, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 190.163, mean reward: 1.902 [1.454, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.734, 10.098], loss: 0.149035, mae: 0.340194, mean_q: 3.898106
 11000/100000: episode: 110, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 178.007, mean reward: 1.780 [1.465, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.915, 10.197], loss: 0.142954, mae: 0.323873, mean_q: 3.900405
 11100/100000: episode: 111, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.778, mean reward: 1.948 [1.459, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.503, 10.098], loss: 0.198531, mae: 0.345207, mean_q: 3.911182
 11200/100000: episode: 112, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 194.028, mean reward: 1.940 [1.491, 6.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.986, 10.107], loss: 0.228344, mae: 0.369260, mean_q: 3.942982
 11300/100000: episode: 113, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 215.258, mean reward: 2.153 [1.454, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.232, 10.111], loss: 0.159167, mae: 0.337634, mean_q: 3.908633
 11400/100000: episode: 114, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 185.275, mean reward: 1.853 [1.436, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.575, 10.098], loss: 0.212091, mae: 0.361734, mean_q: 3.917025
 11500/100000: episode: 115, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 187.271, mean reward: 1.873 [1.446, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.600, 10.098], loss: 0.206229, mae: 0.344760, mean_q: 3.907391
 11600/100000: episode: 116, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 216.664, mean reward: 2.167 [1.444, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.305, 10.098], loss: 0.151301, mae: 0.344869, mean_q: 3.903429
 11700/100000: episode: 117, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.007, mean reward: 1.960 [1.467, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.770, 10.358], loss: 0.167883, mae: 0.342383, mean_q: 3.907635
 11800/100000: episode: 118, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 207.512, mean reward: 2.075 [1.508, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.521, 10.098], loss: 0.164644, mae: 0.353228, mean_q: 3.912348
 11900/100000: episode: 119, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 204.607, mean reward: 2.046 [1.441, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.560, 10.124], loss: 0.156601, mae: 0.349333, mean_q: 3.925312
 12000/100000: episode: 120, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 177.428, mean reward: 1.774 [1.443, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.815, 10.174], loss: 0.190183, mae: 0.353015, mean_q: 3.923340
 12100/100000: episode: 121, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 179.996, mean reward: 1.800 [1.453, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.103], loss: 0.157879, mae: 0.350671, mean_q: 3.925286
 12200/100000: episode: 122, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.408, mean reward: 1.924 [1.484, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.362, 10.098], loss: 0.176351, mae: 0.348513, mean_q: 3.921038
 12300/100000: episode: 123, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 175.683, mean reward: 1.757 [1.455, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.188, 10.192], loss: 0.143505, mae: 0.334570, mean_q: 3.918052
 12400/100000: episode: 124, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 196.873, mean reward: 1.969 [1.465, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.793, 10.098], loss: 0.132298, mae: 0.321790, mean_q: 3.896696
 12500/100000: episode: 125, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 203.335, mean reward: 2.033 [1.466, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.183, 10.098], loss: 0.180917, mae: 0.352613, mean_q: 3.926502
 12600/100000: episode: 126, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 196.107, mean reward: 1.961 [1.505, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.066, 10.098], loss: 0.160486, mae: 0.332463, mean_q: 3.906973
 12700/100000: episode: 127, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 192.548, mean reward: 1.925 [1.513, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.210, 10.150], loss: 0.151304, mae: 0.342533, mean_q: 3.908476
 12800/100000: episode: 128, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 196.230, mean reward: 1.962 [1.455, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.230, 10.098], loss: 0.188525, mae: 0.356932, mean_q: 3.936266
 12900/100000: episode: 129, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 186.887, mean reward: 1.869 [1.435, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.098], loss: 0.164182, mae: 0.338740, mean_q: 3.931631
 13000/100000: episode: 130, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 211.814, mean reward: 2.118 [1.466, 7.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.374, 10.165], loss: 0.311905, mae: 0.393676, mean_q: 3.947880
 13100/100000: episode: 131, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 194.188, mean reward: 1.942 [1.524, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.968, 10.295], loss: 0.147592, mae: 0.322709, mean_q: 3.909090
 13200/100000: episode: 132, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.772, mean reward: 1.978 [1.446, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.394, 10.176], loss: 0.147741, mae: 0.327757, mean_q: 3.891503
 13300/100000: episode: 133, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 237.032, mean reward: 2.370 [1.492, 7.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.812, 10.398], loss: 0.163699, mae: 0.346038, mean_q: 3.908781
 13400/100000: episode: 134, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 214.365, mean reward: 2.144 [1.467, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.193, 10.098], loss: 0.131498, mae: 0.335624, mean_q: 3.911609
 13500/100000: episode: 135, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 192.603, mean reward: 1.926 [1.442, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.826, 10.109], loss: 0.200557, mae: 0.363783, mean_q: 3.932753
 13600/100000: episode: 136, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 202.894, mean reward: 2.029 [1.492, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.079, 10.098], loss: 0.171466, mae: 0.339767, mean_q: 3.918584
 13700/100000: episode: 137, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 198.043, mean reward: 1.980 [1.457, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.967, 10.098], loss: 0.175616, mae: 0.348230, mean_q: 3.917390
 13800/100000: episode: 138, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 184.865, mean reward: 1.849 [1.463, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.098], loss: 0.190089, mae: 0.354594, mean_q: 3.931226
 13900/100000: episode: 139, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 180.553, mean reward: 1.806 [1.434, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.599, 10.098], loss: 0.126636, mae: 0.322305, mean_q: 3.909163
 14000/100000: episode: 140, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 182.198, mean reward: 1.822 [1.508, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.098], loss: 0.126244, mae: 0.325408, mean_q: 3.898237
 14100/100000: episode: 141, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 183.934, mean reward: 1.839 [1.484, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.098], loss: 0.149660, mae: 0.337862, mean_q: 3.910875
 14200/100000: episode: 142, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 194.656, mean reward: 1.947 [1.503, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.409, 10.098], loss: 0.163553, mae: 0.345502, mean_q: 3.912710
 14300/100000: episode: 143, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 211.230, mean reward: 2.112 [1.430, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.283, 10.359], loss: 0.155541, mae: 0.339332, mean_q: 3.926311
 14400/100000: episode: 144, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 197.897, mean reward: 1.979 [1.466, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.728, 10.281], loss: 0.122186, mae: 0.314261, mean_q: 3.887788
 14500/100000: episode: 145, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 205.305, mean reward: 2.053 [1.505, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.468, 10.362], loss: 0.137736, mae: 0.334547, mean_q: 3.899413
 14600/100000: episode: 146, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 210.077, mean reward: 2.101 [1.478, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.743, 10.286], loss: 0.142701, mae: 0.333882, mean_q: 3.892843
 14700/100000: episode: 147, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 182.924, mean reward: 1.829 [1.483, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.242], loss: 0.118717, mae: 0.326873, mean_q: 3.878562
 14800/100000: episode: 148, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 191.434, mean reward: 1.914 [1.471, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.098], loss: 0.129248, mae: 0.318663, mean_q: 3.873957
 14900/100000: episode: 149, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 184.085, mean reward: 1.841 [1.441, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.451, 10.244], loss: 0.127657, mae: 0.322385, mean_q: 3.877256
[Info] 1-TH LEVEL FOUND: 4.878106594085693, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.922s, episode steps: 100, steps per second: 20, episode reward: 199.312, mean reward: 1.993 [1.482, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.947, 10.098], loss: 0.109271, mae: 0.309947, mean_q: 3.859338
 15012/100000: episode: 151, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 28.096, mean reward: 2.341 [1.835, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.647, 10.100], loss: 0.121868, mae: 0.320472, mean_q: 3.849801
 15039/100000: episode: 152, duration: 0.156s, episode steps: 27, steps per second: 174, episode reward: 82.782, mean reward: 3.066 [2.367, 6.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.236, 10.100], loss: 0.083817, mae: 0.291622, mean_q: 3.856072
 15061/100000: episode: 153, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 43.443, mean reward: 1.975 [1.439, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.100], loss: 0.103831, mae: 0.304941, mean_q: 3.852046
[Info] FALSIFICATION!
 15079/100000: episode: 154, duration: 0.477s, episode steps: 18, steps per second: 38, episode reward: 1126.233, mean reward: 62.569 [4.262, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.127, 9.404], loss: 0.105617, mae: 0.321431, mean_q: 3.890402
 15106/100000: episode: 155, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 62.731, mean reward: 2.323 [1.926, 4.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.799, 10.100], loss: 0.193850, mae: 0.370852, mean_q: 3.923149
 15116/100000: episode: 156, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 24.916, mean reward: 2.492 [2.043, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.214, 10.100], loss: 0.097452, mae: 0.308224, mean_q: 3.872918
 15139/100000: episode: 157, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 60.944, mean reward: 2.650 [1.971, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.267, 10.100], loss: 0.149549, mae: 0.351101, mean_q: 3.908236
 15162/100000: episode: 158, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 70.177, mean reward: 3.051 [2.129, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.251, 10.100], loss: 670.475891, mae: 1.949616, mean_q: 4.250414
 15184/100000: episode: 159, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 58.804, mean reward: 2.673 [2.041, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.263, 10.100], loss: 1.017086, mae: 1.044729, mean_q: 4.017505
 15200/100000: episode: 160, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 37.912, mean reward: 2.369 [1.945, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.399, 10.100], loss: 0.176455, mae: 0.425374, mean_q: 3.927226
 15220/100000: episode: 161, duration: 0.111s, episode steps: 20, steps per second: 179, episode reward: 48.365, mean reward: 2.418 [1.968, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.112, 10.100], loss: 771.059204, mae: 1.998223, mean_q: 4.202941
 15230/100000: episode: 162, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 21.921, mean reward: 2.192 [1.870, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.260, 10.100], loss: 3.947383, mae: 2.440877, mean_q: 6.655979
 15250/100000: episode: 163, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 58.948, mean reward: 2.947 [2.194, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.120, 10.100], loss: 0.576316, mae: 0.777275, mean_q: 4.041858
 15260/100000: episode: 164, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 23.232, mean reward: 2.323 [1.685, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.362, 10.100], loss: 0.257926, mae: 0.476895, mean_q: 4.210690
 15295/100000: episode: 165, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 129.429, mean reward: 3.698 [1.992, 6.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.720, 10.100], loss: 0.166233, mae: 0.393896, mean_q: 4.120673
 15318/100000: episode: 166, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 52.109, mean reward: 2.266 [1.675, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.100], loss: 0.281186, mae: 0.459823, mean_q: 4.246242
 15331/100000: episode: 167, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 29.582, mean reward: 2.276 [1.961, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.092, 10.100], loss: 0.138204, mae: 0.373142, mean_q: 4.131448
 15353/100000: episode: 168, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 47.393, mean reward: 2.154 [1.725, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.261, 10.100], loss: 0.142253, mae: 0.357558, mean_q: 4.126904
 15366/100000: episode: 169, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 37.950, mean reward: 2.919 [2.442, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.381, 10.100], loss: 0.217505, mae: 0.382518, mean_q: 4.069730
 15378/100000: episode: 170, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 30.569, mean reward: 2.547 [1.886, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.068, 10.100], loss: 0.143603, mae: 0.330961, mean_q: 4.043839
 15390/100000: episode: 171, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 29.578, mean reward: 2.465 [1.913, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.161, 10.100], loss: 0.252234, mae: 0.419788, mean_q: 4.103259
 15412/100000: episode: 172, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 56.064, mean reward: 2.548 [1.937, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.144, 10.100], loss: 0.146888, mae: 0.391675, mean_q: 4.176612
 15422/100000: episode: 173, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 32.925, mean reward: 3.292 [2.037, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.350, 10.100], loss: 0.179737, mae: 0.374994, mean_q: 4.060411
 15438/100000: episode: 174, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 37.152, mean reward: 2.322 [1.858, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.691, 10.100], loss: 0.147592, mae: 0.366140, mean_q: 4.039637
 15454/100000: episode: 175, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 37.398, mean reward: 2.337 [2.034, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.392, 10.100], loss: 961.475037, mae: 2.498814, mean_q: 4.335249
 15467/100000: episode: 176, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 31.882, mean reward: 2.452 [2.010, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.358, 10.100], loss: 3.410228, mae: 2.027634, mean_q: 6.182308
 15480/100000: episode: 177, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 32.318, mean reward: 2.486 [2.020, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.885, 10.100], loss: 0.969641, mae: 0.952854, mean_q: 4.497220
 15490/100000: episode: 178, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 24.093, mean reward: 2.409 [1.900, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.457, 10.100], loss: 0.544506, mae: 0.776429, mean_q: 3.826609
 15525/100000: episode: 179, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 297.433, mean reward: 8.498 [3.483, 26.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.451, 10.100], loss: 0.246727, mae: 0.453078, mean_q: 4.107122
 15552/100000: episode: 180, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 75.346, mean reward: 2.791 [2.109, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.476, 10.100], loss: 0.510405, mae: 0.450824, mean_q: 4.194477
 15568/100000: episode: 181, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 39.035, mean reward: 2.440 [2.115, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.618, 10.100], loss: 1.160613, mae: 0.528374, mean_q: 4.316783
 15578/100000: episode: 182, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 21.171, mean reward: 2.117 [1.696, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.227, 10.100], loss: 0.175600, mae: 0.392296, mean_q: 4.314746
 15605/100000: episode: 183, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 57.544, mean reward: 2.131 [1.564, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.486, 10.182], loss: 570.115967, mae: 1.716341, mean_q: 4.404565
 15625/100000: episode: 184, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 53.657, mean reward: 2.683 [1.947, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.913, 10.100], loss: 767.922058, mae: 3.173280, mean_q: 5.882167
 15647/100000: episode: 185, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 48.054, mean reward: 2.184 [1.782, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.193, 10.100], loss: 2.123456, mae: 1.455261, mean_q: 5.724609
 15660/100000: episode: 186, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 32.872, mean reward: 2.529 [1.509, 5.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.394, 10.160], loss: 1.215318, mae: 0.750954, mean_q: 4.352456
 15676/100000: episode: 187, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 34.448, mean reward: 2.153 [1.807, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.462, 10.100], loss: 0.819777, mae: 0.606031, mean_q: 4.146625
 15703/100000: episode: 188, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 62.987, mean reward: 2.333 [1.718, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.715, 10.100], loss: 0.404756, mae: 0.480466, mean_q: 4.391849
 15716/100000: episode: 189, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 26.660, mean reward: 2.051 [1.740, 2.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.156, 10.100], loss: 0.406967, mae: 0.455816, mean_q: 4.309001
 15728/100000: episode: 190, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 33.771, mean reward: 2.814 [2.184, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.230, 10.100], loss: 0.380527, mae: 0.447763, mean_q: 4.282096
 15748/100000: episode: 191, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 56.090, mean reward: 2.804 [2.160, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.308, 10.100], loss: 0.456255, mae: 0.491500, mean_q: 4.396962
 15764/100000: episode: 192, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 33.028, mean reward: 2.064 [1.880, 2.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.551, 10.100], loss: 0.173550, mae: 0.351631, mean_q: 4.204224
 15776/100000: episode: 193, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 26.689, mean reward: 2.224 [1.776, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.073, 10.100], loss: 0.367551, mae: 0.437251, mean_q: 4.250823
 15796/100000: episode: 194, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 62.171, mean reward: 3.109 [2.249, 6.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.558, 10.100], loss: 0.169602, mae: 0.357541, mean_q: 4.247687
 15806/100000: episode: 195, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 20.056, mean reward: 2.006 [1.728, 2.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.245, 10.100], loss: 0.705241, mae: 0.449263, mean_q: 4.234538
 15841/100000: episode: 196, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 192.274, mean reward: 5.494 [3.722, 9.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.501, 10.100], loss: 0.253432, mae: 0.396547, mean_q: 4.231246
 15851/100000: episode: 197, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 17.349, mean reward: 1.735 [1.583, 2.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.275, 10.100], loss: 0.359132, mae: 0.462077, mean_q: 4.371681
 15867/100000: episode: 198, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 36.768, mean reward: 2.298 [1.599, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.211, 10.100], loss: 0.456318, mae: 0.466209, mean_q: 4.358201
 15879/100000: episode: 199, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 31.169, mean reward: 2.597 [2.424, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.525, 10.100], loss: 0.293994, mae: 0.446855, mean_q: 4.347326
 15892/100000: episode: 200, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 31.289, mean reward: 2.407 [1.861, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.165, 10.100], loss: 0.214996, mae: 0.393433, mean_q: 4.256812
 15914/100000: episode: 201, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 39.130, mean reward: 1.779 [1.468, 2.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.140, 10.108], loss: 0.258983, mae: 0.433957, mean_q: 4.263163
 15924/100000: episode: 202, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 24.060, mean reward: 2.406 [2.111, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.843, 10.100], loss: 0.704362, mae: 0.434654, mean_q: 4.236144
 15940/100000: episode: 203, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 46.294, mean reward: 2.893 [2.316, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.499, 10.100], loss: 0.326625, mae: 0.447812, mean_q: 4.269217
 15952/100000: episode: 204, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 28.170, mean reward: 2.347 [1.934, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.252, 10.100], loss: 0.331689, mae: 0.444232, mean_q: 4.320446
 15975/100000: episode: 205, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 58.809, mean reward: 2.557 [1.854, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.941, 10.100], loss: 0.273753, mae: 0.439584, mean_q: 4.288064
 15985/100000: episode: 206, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 17.716, mean reward: 1.772 [1.624, 1.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.287, 10.100], loss: 0.358236, mae: 0.472260, mean_q: 4.315077
 16007/100000: episode: 207, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 47.297, mean reward: 2.150 [1.907, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.160, 10.100], loss: 0.231265, mae: 0.405457, mean_q: 4.240650
 16029/100000: episode: 208, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 46.559, mean reward: 2.116 [1.555, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.327, 10.100], loss: 0.287510, mae: 0.422791, mean_q: 4.237160
 16056/100000: episode: 209, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 73.214, mean reward: 2.712 [2.182, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.158, 10.100], loss: 0.719701, mae: 0.515461, mean_q: 4.334796
 16083/100000: episode: 210, duration: 0.150s, episode steps: 27, steps per second: 181, episode reward: 55.314, mean reward: 2.049 [1.585, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.412, 10.100], loss: 570.790771, mae: 2.081302, mean_q: 4.956773
 16103/100000: episode: 211, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 50.451, mean reward: 2.523 [2.076, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.737, 10.100], loss: 1.333052, mae: 0.948092, mean_q: 4.806278
 16130/100000: episode: 212, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 82.442, mean reward: 3.053 [1.828, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.497, 10.100], loss: 0.330390, mae: 0.514921, mean_q: 4.118242
 16157/100000: episode: 213, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 75.812, mean reward: 2.808 [2.246, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.094, 10.100], loss: 0.652413, mae: 0.479040, mean_q: 4.305414
 16167/100000: episode: 214, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 22.692, mean reward: 2.269 [1.823, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.363, 10.100], loss: 0.736834, mae: 0.458062, mean_q: 4.279422
 16190/100000: episode: 215, duration: 0.110s, episode steps: 23, steps per second: 208, episode reward: 59.550, mean reward: 2.589 [2.044, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.668, 10.100], loss: 0.468241, mae: 0.443962, mean_q: 4.287770
 16206/100000: episode: 216, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 47.326, mean reward: 2.958 [1.888, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.948, 10.100], loss: 0.777431, mae: 0.511293, mean_q: 4.418328
 16233/100000: episode: 217, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 67.627, mean reward: 2.505 [2.021, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.112, 10.100], loss: 0.316201, mae: 0.463017, mean_q: 4.340534
 16249/100000: episode: 218, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 45.948, mean reward: 2.872 [1.945, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.440, 10.100], loss: 0.505705, mae: 0.430587, mean_q: 4.304930
 16276/100000: episode: 219, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 66.343, mean reward: 2.457 [1.803, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.617, 10.100], loss: 0.252764, mae: 0.425489, mean_q: 4.312299
 16288/100000: episode: 220, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 28.242, mean reward: 2.354 [2.105, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.207, 10.100], loss: 0.836578, mae: 0.490859, mean_q: 4.316196
 16298/100000: episode: 221, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 23.640, mean reward: 2.364 [1.800, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.267, 10.100], loss: 0.245707, mae: 0.432046, mean_q: 4.419002
 16314/100000: episode: 222, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 41.518, mean reward: 2.595 [1.930, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.369, 10.100], loss: 1.173419, mae: 0.474837, mean_q: 4.371675
 16330/100000: episode: 223, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 40.150, mean reward: 2.509 [2.198, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.471, 10.100], loss: 0.773005, mae: 0.540524, mean_q: 4.468710
 16343/100000: episode: 224, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 29.874, mean reward: 2.298 [1.795, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.257, 10.100], loss: 0.252069, mae: 0.455593, mean_q: 4.396920
 16355/100000: episode: 225, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 23.177, mean reward: 1.931 [1.704, 2.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.100], loss: 0.288630, mae: 0.443489, mean_q: 4.240559
 16368/100000: episode: 226, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 33.350, mean reward: 2.565 [1.994, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.294, 10.100], loss: 0.275526, mae: 0.436807, mean_q: 4.389889
 16378/100000: episode: 227, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 21.018, mean reward: 2.102 [1.828, 2.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.298, 10.100], loss: 0.773027, mae: 0.535424, mean_q: 4.375304
 16388/100000: episode: 228, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 18.541, mean reward: 1.854 [1.689, 2.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.162, 10.100], loss: 0.911422, mae: 0.511467, mean_q: 4.438559
 16401/100000: episode: 229, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 24.376, mean reward: 1.875 [1.568, 2.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.069, 10.100], loss: 0.869575, mae: 0.484334, mean_q: 4.313988
 16423/100000: episode: 230, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 46.091, mean reward: 2.095 [1.607, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.175, 10.100], loss: 0.276643, mae: 0.421797, mean_q: 4.289524
 16445/100000: episode: 231, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 66.615, mean reward: 3.028 [1.591, 5.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.845, 10.100], loss: 0.228690, mae: 0.421098, mean_q: 4.323793
 16457/100000: episode: 232, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 26.938, mean reward: 2.245 [2.009, 2.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.226, 10.100], loss: 0.226278, mae: 0.401863, mean_q: 4.259562
 16467/100000: episode: 233, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 18.659, mean reward: 1.866 [1.702, 2.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.243, 10.100], loss: 0.983317, mae: 0.453695, mean_q: 4.234930
 16477/100000: episode: 234, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 23.133, mean reward: 2.313 [1.728, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.224, 10.100], loss: 0.283190, mae: 0.424953, mean_q: 4.310489
 16493/100000: episode: 235, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 43.581, mean reward: 2.724 [1.958, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.332, 10.100], loss: 0.547815, mae: 0.509055, mean_q: 4.399241
 16516/100000: episode: 236, duration: 0.130s, episode steps: 23, steps per second: 178, episode reward: 80.910, mean reward: 3.518 [2.209, 6.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.934, 10.100], loss: 0.370299, mae: 0.446854, mean_q: 4.253762
 16538/100000: episode: 237, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 44.791, mean reward: 2.036 [1.535, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.257, 10.100], loss: 0.199360, mae: 0.385878, mean_q: 4.267581
 16573/100000: episode: 238, duration: 0.221s, episode steps: 35, steps per second: 158, episode reward: 274.401, mean reward: 7.840 [2.978, 37.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.889, 10.100], loss: 0.463177, mae: 0.452076, mean_q: 4.274513
 16583/100000: episode: 239, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 25.594, mean reward: 2.559 [2.022, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.337, 10.100], loss: 0.336025, mae: 0.483144, mean_q: 4.336383
[Info] Complete ISplit Iteration
[Info] Levels: [4.8781066, 8.182871]
[Info] Cond. Prob: [0.1, 0.03]
[Info] Error Prob: 0.003

 16603/100000: episode: 240, duration: 4.295s, episode steps: 20, steps per second: 5, episode reward: 51.811, mean reward: 2.591 [2.235, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.084, 10.100], loss: 0.510251, mae: 0.532251, mean_q: 4.447176
 16703/100000: episode: 241, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 190.358, mean reward: 1.904 [1.471, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.530, 10.098], loss: 154.626099, mae: 1.073506, mean_q: 4.741377
 16803/100000: episode: 242, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.427, mean reward: 2.004 [1.464, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.509, 10.098], loss: 154.864059, mae: 1.088326, mean_q: 4.733584
 16903/100000: episode: 243, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.491, mean reward: 1.935 [1.449, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.584, 10.268], loss: 154.665649, mae: 1.118460, mean_q: 4.890970
 17003/100000: episode: 244, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 307.619, mean reward: 3.076 [1.550, 10.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.480, 10.098], loss: 154.053070, mae: 0.880804, mean_q: 4.600198
 17103/100000: episode: 245, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 189.707, mean reward: 1.897 [1.460, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.114], loss: 0.722362, mae: 0.674427, mean_q: 4.718962
 17203/100000: episode: 246, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 195.022, mean reward: 1.950 [1.444, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.386, 10.139], loss: 0.578366, mae: 0.501004, mean_q: 4.557653
 17303/100000: episode: 247, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 187.298, mean reward: 1.873 [1.489, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.033, 10.264], loss: 0.720882, mae: 0.518698, mean_q: 4.520135
 17403/100000: episode: 248, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.946, mean reward: 1.879 [1.486, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.440, 10.126], loss: 154.353531, mae: 1.093898, mean_q: 4.800966
 17503/100000: episode: 249, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 181.941, mean reward: 1.819 [1.446, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.687, 10.144], loss: 154.223160, mae: 1.070166, mean_q: 4.854357
 17603/100000: episode: 250, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 200.841, mean reward: 2.008 [1.469, 4.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.954, 10.198], loss: 307.106140, mae: 1.714943, mean_q: 5.231868
 17703/100000: episode: 251, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 207.532, mean reward: 2.075 [1.501, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.098], loss: 0.653571, mae: 0.496716, mean_q: 4.575799
 17803/100000: episode: 252, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 228.611, mean reward: 2.286 [1.513, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.829, 10.098], loss: 154.066757, mae: 1.065176, mean_q: 4.776129
 17903/100000: episode: 253, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 227.081, mean reward: 2.271 [1.484, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.910, 10.588], loss: 154.064987, mae: 1.163952, mean_q: 5.033946
 18003/100000: episode: 254, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 186.139, mean reward: 1.861 [1.480, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.937, 10.098], loss: 0.485780, mae: 0.526007, mean_q: 4.635802
 18103/100000: episode: 255, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.386, mean reward: 1.894 [1.457, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.601, 10.149], loss: 0.541413, mae: 0.509452, mean_q: 4.571101
 18203/100000: episode: 256, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.131, mean reward: 1.851 [1.476, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.468, 10.193], loss: 306.974396, mae: 1.605440, mean_q: 5.195217
 18303/100000: episode: 257, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 189.618, mean reward: 1.896 [1.440, 6.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.752, 10.098], loss: 153.852341, mae: 1.069022, mean_q: 4.894287
 18403/100000: episode: 258, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 205.627, mean reward: 2.056 [1.451, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.262], loss: 306.515594, mae: 1.757174, mean_q: 5.251033
 18503/100000: episode: 259, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 208.939, mean reward: 2.089 [1.439, 4.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.098], loss: 153.147720, mae: 0.854571, mean_q: 4.855055
 18603/100000: episode: 260, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.694, mean reward: 1.857 [1.479, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.110], loss: 459.843903, mae: 2.717326, mean_q: 6.121182
 18703/100000: episode: 261, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.330, mean reward: 1.833 [1.439, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.300], loss: 0.494326, mae: 0.542086, mean_q: 4.737245
 18803/100000: episode: 262, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 193.072, mean reward: 1.931 [1.453, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.331, 10.134], loss: 153.274582, mae: 1.069545, mean_q: 4.988638
 18903/100000: episode: 263, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.462, mean reward: 1.935 [1.455, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.873, 10.098], loss: 0.471616, mae: 0.508229, mean_q: 4.609849
 19003/100000: episode: 264, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.099, mean reward: 1.881 [1.469, 4.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.430, 10.253], loss: 306.174255, mae: 1.747442, mean_q: 5.292984
 19103/100000: episode: 265, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 187.480, mean reward: 1.875 [1.466, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.101, 10.144], loss: 152.854095, mae: 0.873723, mean_q: 4.657157
 19203/100000: episode: 266, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.721, mean reward: 1.907 [1.464, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.314], loss: 1.142159, mae: 0.769674, mean_q: 4.867790
 19303/100000: episode: 267, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.500, mean reward: 1.895 [1.435, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.445, 10.098], loss: 153.058624, mae: 1.018121, mean_q: 4.798245
 19403/100000: episode: 268, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.945, mean reward: 2.009 [1.475, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.090, 10.098], loss: 152.483109, mae: 1.066409, mean_q: 4.937374
 19503/100000: episode: 269, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 220.068, mean reward: 2.201 [1.491, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.441, 10.098], loss: 0.400398, mae: 0.476594, mean_q: 4.531279
 19603/100000: episode: 270, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 187.695, mean reward: 1.877 [1.464, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.878, 10.098], loss: 0.831153, mae: 0.514758, mean_q: 4.534279
 19703/100000: episode: 271, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 190.688, mean reward: 1.907 [1.462, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.393, 10.413], loss: 458.959564, mae: 2.279260, mean_q: 5.466290
 19803/100000: episode: 272, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.868, mean reward: 1.929 [1.440, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.799, 10.297], loss: 0.728221, mae: 0.539301, mean_q: 4.812051
 19903/100000: episode: 273, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.511, mean reward: 1.875 [1.455, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: 0.392004, mae: 0.480977, mean_q: 4.586924
 20003/100000: episode: 274, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 187.948, mean reward: 1.879 [1.455, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.592, 10.291], loss: 0.731725, mae: 0.502858, mean_q: 4.542193
 20103/100000: episode: 275, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 209.130, mean reward: 2.091 [1.452, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.283, 10.098], loss: 0.365445, mae: 0.447931, mean_q: 4.399975
 20203/100000: episode: 276, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.368, mean reward: 1.904 [1.472, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.620, 10.248], loss: 0.329183, mae: 0.415697, mean_q: 4.333866
 20303/100000: episode: 277, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.866, mean reward: 1.899 [1.478, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.295, 10.098], loss: 0.495189, mae: 0.440716, mean_q: 4.335984
 20403/100000: episode: 278, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.824, mean reward: 1.878 [1.476, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.823, 10.248], loss: 0.323975, mae: 0.421326, mean_q: 4.279569
 20503/100000: episode: 279, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 204.620, mean reward: 2.046 [1.456, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.474, 10.357], loss: 0.569095, mae: 0.436359, mean_q: 4.214219
 20603/100000: episode: 280, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.492, mean reward: 1.935 [1.491, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.916, 10.128], loss: 0.202372, mae: 0.384007, mean_q: 4.171270
 20703/100000: episode: 281, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.686, mean reward: 1.907 [1.455, 6.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.311, 10.098], loss: 0.509072, mae: 0.404341, mean_q: 4.156553
 20803/100000: episode: 282, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 190.637, mean reward: 1.906 [1.473, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.333, 10.098], loss: 0.212016, mae: 0.373243, mean_q: 4.118472
 20903/100000: episode: 283, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 208.333, mean reward: 2.083 [1.465, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.794, 10.198], loss: 0.329520, mae: 0.389468, mean_q: 4.103468
 21003/100000: episode: 284, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.742, mean reward: 1.857 [1.449, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.056, 10.098], loss: 0.364679, mae: 0.380749, mean_q: 4.064313
 21103/100000: episode: 285, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 208.120, mean reward: 2.081 [1.440, 5.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.204], loss: 0.479065, mae: 0.381510, mean_q: 4.050636
 21203/100000: episode: 286, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 183.482, mean reward: 1.835 [1.441, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.154], loss: 0.528332, mae: 0.397370, mean_q: 4.061531
 21303/100000: episode: 287, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.726, mean reward: 1.817 [1.466, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.882, 10.098], loss: 0.182225, mae: 0.352481, mean_q: 4.023526
 21403/100000: episode: 288, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 177.241, mean reward: 1.772 [1.463, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.098], loss: 0.201571, mae: 0.359757, mean_q: 4.004612
 21503/100000: episode: 289, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.913, mean reward: 1.869 [1.477, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.456, 10.306], loss: 0.193377, mae: 0.360506, mean_q: 3.988513
 21603/100000: episode: 290, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.545, mean reward: 1.865 [1.470, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.233], loss: 0.106385, mae: 0.318012, mean_q: 3.917467
 21703/100000: episode: 291, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 208.857, mean reward: 2.089 [1.440, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.226, 10.292], loss: 0.120666, mae: 0.317481, mean_q: 3.903957
 21803/100000: episode: 292, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 244.656, mean reward: 2.447 [1.501, 5.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.348, 10.338], loss: 0.115734, mae: 0.322240, mean_q: 3.927142
 21903/100000: episode: 293, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 217.213, mean reward: 2.172 [1.488, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.484, 10.098], loss: 0.109034, mae: 0.316970, mean_q: 3.904875
 22003/100000: episode: 294, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.698, mean reward: 1.847 [1.470, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.796, 10.266], loss: 0.108603, mae: 0.314717, mean_q: 3.881322
 22103/100000: episode: 295, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.848, mean reward: 1.918 [1.508, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.098], loss: 0.104825, mae: 0.315256, mean_q: 3.871193
 22203/100000: episode: 296, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.433, mean reward: 1.934 [1.446, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.719, 10.292], loss: 0.110092, mae: 0.324740, mean_q: 3.885082
 22303/100000: episode: 297, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.857, mean reward: 1.879 [1.468, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.203], loss: 0.098565, mae: 0.307275, mean_q: 3.865480
 22403/100000: episode: 298, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 205.918, mean reward: 2.059 [1.461, 4.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.205, 10.098], loss: 0.092874, mae: 0.300980, mean_q: 3.855525
 22503/100000: episode: 299, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 192.757, mean reward: 1.928 [1.489, 4.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.742, 10.098], loss: 0.101746, mae: 0.312379, mean_q: 3.886665
 22603/100000: episode: 300, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 179.348, mean reward: 1.793 [1.474, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.273, 10.174], loss: 0.099486, mae: 0.307735, mean_q: 3.871775
 22703/100000: episode: 301, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 203.275, mean reward: 2.033 [1.473, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.516, 10.379], loss: 0.093361, mae: 0.299291, mean_q: 3.866873
 22803/100000: episode: 302, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.479, mean reward: 1.895 [1.492, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.942, 10.098], loss: 0.094797, mae: 0.299757, mean_q: 3.878642
 22903/100000: episode: 303, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 197.279, mean reward: 1.973 [1.439, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.761, 10.204], loss: 0.101127, mae: 0.298193, mean_q: 3.848239
 23003/100000: episode: 304, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.780, mean reward: 1.988 [1.483, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.781, 10.098], loss: 0.083669, mae: 0.292436, mean_q: 3.844257
 23103/100000: episode: 305, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 199.599, mean reward: 1.996 [1.439, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.684, 10.130], loss: 0.095402, mae: 0.293436, mean_q: 3.834669
 23203/100000: episode: 306, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 200.709, mean reward: 2.007 [1.461, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.575, 10.267], loss: 0.084541, mae: 0.290772, mean_q: 3.838416
 23303/100000: episode: 307, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.396, mean reward: 1.954 [1.463, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.572, 10.098], loss: 0.087387, mae: 0.299849, mean_q: 3.854928
 23403/100000: episode: 308, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 178.738, mean reward: 1.787 [1.444, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.148], loss: 0.099259, mae: 0.299294, mean_q: 3.845055
 23503/100000: episode: 309, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 190.366, mean reward: 1.904 [1.466, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.651, 10.193], loss: 0.103327, mae: 0.305573, mean_q: 3.851700
 23603/100000: episode: 310, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.790, mean reward: 1.848 [1.460, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.132], loss: 0.084435, mae: 0.291002, mean_q: 3.843195
 23703/100000: episode: 311, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 221.065, mean reward: 2.211 [1.455, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.311, 10.183], loss: 0.092449, mae: 0.304871, mean_q: 3.847352
 23803/100000: episode: 312, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 184.741, mean reward: 1.847 [1.436, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.361], loss: 0.102176, mae: 0.307381, mean_q: 3.861382
 23903/100000: episode: 313, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 185.717, mean reward: 1.857 [1.453, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.789, 10.268], loss: 0.100561, mae: 0.313608, mean_q: 3.877621
 24003/100000: episode: 314, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 193.529, mean reward: 1.935 [1.477, 5.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.129], loss: 0.086280, mae: 0.296564, mean_q: 3.867427
 24103/100000: episode: 315, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.553, mean reward: 2.026 [1.434, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.402, 10.098], loss: 0.095248, mae: 0.300938, mean_q: 3.855361
 24203/100000: episode: 316, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.872, mean reward: 1.899 [1.472, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.742, 10.175], loss: 0.087929, mae: 0.294648, mean_q: 3.850450
 24303/100000: episode: 317, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 181.470, mean reward: 1.815 [1.453, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.699, 10.194], loss: 0.096997, mae: 0.303278, mean_q: 3.873264
 24403/100000: episode: 318, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 176.805, mean reward: 1.768 [1.451, 2.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.204, 10.214], loss: 0.094047, mae: 0.303224, mean_q: 3.857553
 24503/100000: episode: 319, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.093, mean reward: 1.881 [1.432, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.519, 10.098], loss: 0.091648, mae: 0.296309, mean_q: 3.839071
 24603/100000: episode: 320, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 257.705, mean reward: 2.577 [1.504, 5.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.517, 10.537], loss: 0.090056, mae: 0.300098, mean_q: 3.856925
 24703/100000: episode: 321, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.604, mean reward: 1.956 [1.457, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.162, 10.244], loss: 0.099746, mae: 0.306743, mean_q: 3.856662
 24803/100000: episode: 322, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.237, mean reward: 1.892 [1.489, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.733, 10.138], loss: 0.099722, mae: 0.302986, mean_q: 3.869870
 24903/100000: episode: 323, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.615, mean reward: 2.036 [1.471, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.830, 10.098], loss: 0.083036, mae: 0.288046, mean_q: 3.848263
 25003/100000: episode: 324, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.661, mean reward: 1.857 [1.496, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.098], loss: 0.089110, mae: 0.298717, mean_q: 3.849881
 25103/100000: episode: 325, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.715, mean reward: 1.977 [1.443, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.633, 10.177], loss: 0.098224, mae: 0.302342, mean_q: 3.866156
 25203/100000: episode: 326, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.628, mean reward: 1.946 [1.463, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.098], loss: 0.101664, mae: 0.299722, mean_q: 3.850699
 25303/100000: episode: 327, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 222.241, mean reward: 2.222 [1.445, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.634, 10.098], loss: 0.094398, mae: 0.300246, mean_q: 3.866567
 25403/100000: episode: 328, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 201.038, mean reward: 2.010 [1.547, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.799, 10.291], loss: 0.092308, mae: 0.297062, mean_q: 3.875417
 25503/100000: episode: 329, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 186.741, mean reward: 1.867 [1.455, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.098], loss: 0.094000, mae: 0.301123, mean_q: 3.881497
 25603/100000: episode: 330, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 195.169, mean reward: 1.952 [1.440, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.449, 10.240], loss: 0.093533, mae: 0.300671, mean_q: 3.860099
 25703/100000: episode: 331, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 181.305, mean reward: 1.813 [1.501, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.615, 10.098], loss: 0.096550, mae: 0.309847, mean_q: 3.880662
 25803/100000: episode: 332, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.371, mean reward: 1.864 [1.442, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.216], loss: 0.094868, mae: 0.300270, mean_q: 3.867134
 25903/100000: episode: 333, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.899, mean reward: 1.819 [1.449, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.098], loss: 0.081215, mae: 0.291387, mean_q: 3.848912
 26003/100000: episode: 334, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.348, mean reward: 1.903 [1.448, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.598, 10.270], loss: 0.094517, mae: 0.301260, mean_q: 3.857120
 26103/100000: episode: 335, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 186.451, mean reward: 1.865 [1.442, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.548, 10.098], loss: 0.076671, mae: 0.281848, mean_q: 3.836685
 26203/100000: episode: 336, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 207.254, mean reward: 2.073 [1.440, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.249, 10.126], loss: 0.092645, mae: 0.302006, mean_q: 3.848191
 26303/100000: episode: 337, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 177.200, mean reward: 1.772 [1.459, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.154], loss: 0.091622, mae: 0.305879, mean_q: 3.857404
 26403/100000: episode: 338, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.390, mean reward: 1.964 [1.453, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.545, 10.307], loss: 0.087704, mae: 0.296627, mean_q: 3.861004
 26503/100000: episode: 339, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 195.617, mean reward: 1.956 [1.459, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.266, 10.098], loss: 0.100419, mae: 0.313451, mean_q: 3.884835
[Info] 1-TH LEVEL FOUND: 5.199102401733398, Considering 10/90 traces
 26603/100000: episode: 340, duration: 4.516s, episode steps: 100, steps per second: 22, episode reward: 192.029, mean reward: 1.920 [1.434, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.098], loss: 0.093436, mae: 0.306653, mean_q: 3.870942
 26643/100000: episode: 341, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 154.069, mean reward: 3.852 [2.194, 22.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.807, 10.367], loss: 0.083793, mae: 0.280145, mean_q: 3.848898
 26737/100000: episode: 342, duration: 0.481s, episode steps: 94, steps per second: 196, episode reward: 180.060, mean reward: 1.916 [1.441, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.854, 10.274], loss: 0.089379, mae: 0.299201, mean_q: 3.861564
 26831/100000: episode: 343, duration: 0.480s, episode steps: 94, steps per second: 196, episode reward: 201.314, mean reward: 2.142 [1.484, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.594, 10.100], loss: 0.093324, mae: 0.305699, mean_q: 3.867773
 26862/100000: episode: 344, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 89.496, mean reward: 2.887 [2.054, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.370, 10.100], loss: 0.115299, mae: 0.341456, mean_q: 3.906854
 26956/100000: episode: 345, duration: 0.504s, episode steps: 94, steps per second: 186, episode reward: 176.227, mean reward: 1.875 [1.507, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-1.039, 10.140], loss: 0.103671, mae: 0.320336, mean_q: 3.885334
 26977/100000: episode: 346, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 70.731, mean reward: 3.368 [2.040, 5.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.937, 10.100], loss: 0.093926, mae: 0.307439, mean_q: 3.835865
 26998/100000: episode: 347, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 49.469, mean reward: 2.356 [1.891, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.152, 10.100], loss: 0.119624, mae: 0.322049, mean_q: 3.942501
 27038/100000: episode: 348, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 118.003, mean reward: 2.950 [2.094, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.517, 10.468], loss: 0.088854, mae: 0.295531, mean_q: 3.863515
 27078/100000: episode: 349, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 86.150, mean reward: 2.154 [1.558, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.247, 10.100], loss: 0.294526, mae: 0.387465, mean_q: 3.907855
 27085/100000: episode: 350, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 19.432, mean reward: 2.776 [2.235, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.289, 10.504], loss: 0.087021, mae: 0.295365, mean_q: 3.749609
 27092/100000: episode: 351, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 20.942, mean reward: 2.992 [2.627, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.460], loss: 0.144245, mae: 0.363279, mean_q: 3.984987
 27123/100000: episode: 352, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 93.920, mean reward: 3.030 [1.759, 5.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.275, 10.100], loss: 0.085859, mae: 0.305896, mean_q: 3.893412
 27154/100000: episode: 353, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 106.680, mean reward: 3.441 [1.824, 7.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.605, 10.100], loss: 0.095863, mae: 0.308169, mean_q: 3.904768
 27191/100000: episode: 354, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 115.262, mean reward: 3.115 [1.799, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.304, 10.311], loss: 0.104802, mae: 0.303034, mean_q: 3.922502
 27225/100000: episode: 355, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 80.834, mean reward: 2.377 [1.587, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.945, 10.220], loss: 0.102210, mae: 0.320448, mean_q: 3.960631
 27256/100000: episode: 356, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 89.524, mean reward: 2.888 [2.136, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.509, 10.100], loss: 0.114072, mae: 0.315095, mean_q: 3.952308
 27296/100000: episode: 357, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 133.876, mean reward: 3.347 [2.176, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.687, 10.368], loss: 0.118043, mae: 0.333319, mean_q: 3.961255
 27333/100000: episode: 358, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 99.303, mean reward: 2.684 [2.052, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.922, 10.405], loss: 0.119668, mae: 0.328547, mean_q: 4.021595
 27373/100000: episode: 359, duration: 0.224s, episode steps: 40, steps per second: 178, episode reward: 91.116, mean reward: 2.278 [1.728, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.791, 10.363], loss: 0.292188, mae: 0.378697, mean_q: 3.993739
 27407/100000: episode: 360, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 83.454, mean reward: 2.455 [1.462, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.439, 10.108], loss: 0.290025, mae: 0.353622, mean_q: 4.002825
 27438/100000: episode: 361, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 85.297, mean reward: 2.752 [2.000, 4.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.534, 10.100], loss: 0.349272, mae: 0.404809, mean_q: 4.083381
 27478/100000: episode: 362, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 91.000, mean reward: 2.275 [1.813, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.527, 10.314], loss: 0.122478, mae: 0.346483, mean_q: 4.031171
 27572/100000: episode: 363, duration: 0.494s, episode steps: 94, steps per second: 190, episode reward: 184.035, mean reward: 1.958 [1.437, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.826, 10.365], loss: 0.266619, mae: 0.363290, mean_q: 4.042632
 27603/100000: episode: 364, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 80.543, mean reward: 2.598 [1.761, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.108, 10.100], loss: 0.110394, mae: 0.333428, mean_q: 3.998080
 27643/100000: episode: 365, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 188.807, mean reward: 4.720 [2.172, 14.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.140, 10.726], loss: 0.268717, mae: 0.356537, mean_q: 4.018730
 27677/100000: episode: 366, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 93.863, mean reward: 2.761 [2.085, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.314, 10.438], loss: 0.123744, mae: 0.349132, mean_q: 4.098283
 27717/100000: episode: 367, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 130.108, mean reward: 3.253 [2.340, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.878, 10.481], loss: 0.179963, mae: 0.376337, mean_q: 4.101309
 27748/100000: episode: 368, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 98.977, mean reward: 3.193 [2.151, 5.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.673, 10.100], loss: 0.357430, mae: 0.392607, mean_q: 4.142022
 27788/100000: episode: 369, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 112.351, mean reward: 2.809 [1.979, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.650, 10.486], loss: 0.117357, mae: 0.339902, mean_q: 4.111478
 27799/100000: episode: 370, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 36.082, mean reward: 3.280 [2.434, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.464], loss: 0.077142, mae: 0.287072, mean_q: 3.969491
 27806/100000: episode: 371, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 20.834, mean reward: 2.976 [2.622, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.473], loss: 0.119715, mae: 0.363986, mean_q: 4.165822
 27837/100000: episode: 372, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 87.091, mean reward: 2.809 [2.107, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.648, 10.100], loss: 0.105872, mae: 0.319045, mean_q: 4.111533
 27874/100000: episode: 373, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 121.633, mean reward: 3.287 [2.540, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.796, 10.580], loss: 0.170184, mae: 0.376287, mean_q: 4.177742
 27881/100000: episode: 374, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 17.941, mean reward: 2.563 [2.365, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.387], loss: 0.122219, mae: 0.343084, mean_q: 4.246574
 27892/100000: episode: 375, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 31.667, mean reward: 2.879 [2.561, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.414], loss: 0.139359, mae: 0.372755, mean_q: 4.210652
 27899/100000: episode: 376, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 21.502, mean reward: 3.072 [2.476, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.518], loss: 0.295202, mae: 0.379242, mean_q: 4.283168
 27939/100000: episode: 377, duration: 0.209s, episode steps: 40, steps per second: 192, episode reward: 143.143, mean reward: 3.579 [2.667, 5.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.358, 10.500], loss: 0.314073, mae: 0.395395, mean_q: 4.213182
 27968/100000: episode: 378, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 95.001, mean reward: 3.276 [2.426, 6.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.667, 10.100], loss: 0.125906, mae: 0.341041, mean_q: 4.151998
 27989/100000: episode: 379, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 85.522, mean reward: 4.072 [2.308, 9.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.478, 10.100], loss: 0.142077, mae: 0.390957, mean_q: 4.271360
 28020/100000: episode: 380, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 95.009, mean reward: 3.065 [2.003, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.314, 10.100], loss: 0.164836, mae: 0.390493, mean_q: 4.248663
 28054/100000: episode: 381, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 74.519, mean reward: 2.192 [1.679, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.568, 10.237], loss: 0.348311, mae: 0.442166, mean_q: 4.232695
 28065/100000: episode: 382, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 32.391, mean reward: 2.945 [2.247, 4.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.526], loss: 0.134439, mae: 0.344622, mean_q: 4.046550
 28159/100000: episode: 383, duration: 0.483s, episode steps: 94, steps per second: 195, episode reward: 179.337, mean reward: 1.908 [1.453, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-1.157, 10.100], loss: 0.210191, mae: 0.384734, mean_q: 4.203609
 28196/100000: episode: 384, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 104.610, mean reward: 2.827 [2.043, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.355], loss: 0.131901, mae: 0.369234, mean_q: 4.254622
 28230/100000: episode: 385, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 88.670, mean reward: 2.608 [2.099, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.333, 10.393], loss: 0.402541, mae: 0.448574, mean_q: 4.322159
 28261/100000: episode: 386, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 102.838, mean reward: 3.317 [2.347, 4.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.774, 10.100], loss: 0.176350, mae: 0.406931, mean_q: 4.361484
 28301/100000: episode: 387, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 112.078, mean reward: 2.802 [1.884, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.078, 10.349], loss: 0.197698, mae: 0.405971, mean_q: 4.311746
 28395/100000: episode: 388, duration: 0.513s, episode steps: 94, steps per second: 183, episode reward: 187.064, mean reward: 1.990 [1.467, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.048, 10.100], loss: 0.143238, mae: 0.376537, mean_q: 4.297471
 28416/100000: episode: 389, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 48.382, mean reward: 2.304 [1.942, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.343, 10.100], loss: 0.168324, mae: 0.384392, mean_q: 4.347449
 28445/100000: episode: 390, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 58.547, mean reward: 2.019 [1.470, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.675, 10.213], loss: 0.228211, mae: 0.407684, mean_q: 4.325081
 28452/100000: episode: 391, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 18.154, mean reward: 2.593 [2.401, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.405], loss: 0.169646, mae: 0.393698, mean_q: 4.285121
 28492/100000: episode: 392, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 81.130, mean reward: 2.028 [1.491, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.157, 10.157], loss: 0.154679, mae: 0.352060, mean_q: 4.244043
 28499/100000: episode: 393, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 21.542, mean reward: 3.077 [2.727, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.506], loss: 0.148212, mae: 0.407765, mean_q: 4.463601
 28533/100000: episode: 394, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 68.349, mean reward: 2.010 [1.456, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.596, 10.166], loss: 0.198904, mae: 0.404678, mean_q: 4.327116
 28570/100000: episode: 395, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 84.120, mean reward: 2.274 [1.579, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.521, 10.306], loss: 0.137957, mae: 0.361299, mean_q: 4.282002
 28610/100000: episode: 396, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 121.916, mean reward: 3.048 [1.728, 7.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.681, 10.421], loss: 0.290627, mae: 0.392844, mean_q: 4.343345
 28621/100000: episode: 397, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 32.344, mean reward: 2.940 [2.202, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.346, 10.511], loss: 0.216777, mae: 0.452083, mean_q: 4.260492
 28658/100000: episode: 398, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 94.882, mean reward: 2.564 [1.735, 6.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.480, 10.214], loss: 0.157292, mae: 0.396515, mean_q: 4.318131
 28687/100000: episode: 399, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 101.229, mean reward: 3.491 [2.525, 5.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.800, 10.100], loss: 0.177879, mae: 0.413521, mean_q: 4.424978
 28721/100000: episode: 400, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 79.848, mean reward: 2.348 [1.854, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.510, 10.268], loss: 0.152755, mae: 0.397948, mean_q: 4.361161
 28761/100000: episode: 401, duration: 0.220s, episode steps: 40, steps per second: 181, episode reward: 153.441, mean reward: 3.836 [1.852, 19.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.718, 10.318], loss: 0.145043, mae: 0.383991, mean_q: 4.343312
 28855/100000: episode: 402, duration: 0.492s, episode steps: 94, steps per second: 191, episode reward: 183.735, mean reward: 1.955 [1.493, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.771, 10.100], loss: 0.154462, mae: 0.392431, mean_q: 4.376493
 28895/100000: episode: 403, duration: 0.210s, episode steps: 40, steps per second: 190, episode reward: 110.265, mean reward: 2.757 [1.877, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.508, 10.391], loss: 0.168718, mae: 0.395179, mean_q: 4.338157
 28989/100000: episode: 404, duration: 0.507s, episode steps: 94, steps per second: 185, episode reward: 236.317, mean reward: 2.514 [1.525, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.562, 10.326], loss: 0.236502, mae: 0.414895, mean_q: 4.419966
 29010/100000: episode: 405, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 56.874, mean reward: 2.708 [2.199, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.208, 10.100], loss: 0.649376, mae: 0.476181, mean_q: 4.575191
 29031/100000: episode: 406, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 73.686, mean reward: 3.509 [2.361, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.347, 10.100], loss: 0.182441, mae: 0.419740, mean_q: 4.445382
 29125/100000: episode: 407, duration: 0.506s, episode steps: 94, steps per second: 186, episode reward: 177.721, mean reward: 1.891 [1.447, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.240, 10.100], loss: 0.223961, mae: 0.418554, mean_q: 4.430031
 29132/100000: episode: 408, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 20.573, mean reward: 2.939 [2.750, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.593, 10.416], loss: 0.192698, mae: 0.445901, mean_q: 4.467375
 29161/100000: episode: 409, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 82.743, mean reward: 2.853 [2.415, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.460, 10.100], loss: 0.184815, mae: 0.411542, mean_q: 4.453973
 29192/100000: episode: 410, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 71.681, mean reward: 2.312 [1.600, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.693, 10.100], loss: 0.170381, mae: 0.403654, mean_q: 4.416623
 29232/100000: episode: 411, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 111.623, mean reward: 2.791 [2.201, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.348, 10.449], loss: 0.185516, mae: 0.411496, mean_q: 4.535388
 29261/100000: episode: 412, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 131.418, mean reward: 4.532 [2.669, 10.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.585, 10.100], loss: 0.409978, mae: 0.463522, mean_q: 4.525157
 29272/100000: episode: 413, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 40.240, mean reward: 3.658 [2.965, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.614, 10.503], loss: 0.175022, mae: 0.423725, mean_q: 4.431631
 29283/100000: episode: 414, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 34.007, mean reward: 3.092 [2.363, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.735, 10.420], loss: 0.218906, mae: 0.405786, mean_q: 4.475602
 29294/100000: episode: 415, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 26.234, mean reward: 2.385 [2.190, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.056, 10.410], loss: 0.773590, mae: 0.537346, mean_q: 4.718225
 29305/100000: episode: 416, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 28.592, mean reward: 2.599 [1.812, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.237], loss: 0.243963, mae: 0.440207, mean_q: 4.472034
 29334/100000: episode: 417, duration: 0.163s, episode steps: 29, steps per second: 177, episode reward: 89.815, mean reward: 3.097 [2.293, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.210, 10.100], loss: 0.185801, mae: 0.410308, mean_q: 4.509624
 29363/100000: episode: 418, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 80.382, mean reward: 2.772 [1.851, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.133, 10.100], loss: 0.381439, mae: 0.470056, mean_q: 4.546344
 29384/100000: episode: 419, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 77.435, mean reward: 3.687 [2.195, 5.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-2.287, 10.100], loss: 0.463066, mae: 0.446482, mean_q: 4.599980
 29405/100000: episode: 420, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 71.648, mean reward: 3.412 [2.202, 6.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.086, 10.100], loss: 0.398941, mae: 0.456028, mean_q: 4.605755
 29499/100000: episode: 421, duration: 0.498s, episode steps: 94, steps per second: 189, episode reward: 186.203, mean reward: 1.981 [1.456, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.552, 10.100], loss: 0.330036, mae: 0.447742, mean_q: 4.610604
 29539/100000: episode: 422, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 106.265, mean reward: 2.657 [1.865, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.727, 10.482], loss: 0.195919, mae: 0.406465, mean_q: 4.535183
 29633/100000: episode: 423, duration: 0.479s, episode steps: 94, steps per second: 196, episode reward: 209.830, mean reward: 2.232 [1.452, 6.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.976, 10.100], loss: 0.223116, mae: 0.429080, mean_q: 4.596839
 29673/100000: episode: 424, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 110.658, mean reward: 2.766 [2.000, 4.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.300, 10.393], loss: 0.185452, mae: 0.424659, mean_q: 4.593875
 29680/100000: episode: 425, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 21.137, mean reward: 3.020 [2.721, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.465], loss: 0.132292, mae: 0.341090, mean_q: 4.427730
 29774/100000: episode: 426, duration: 0.472s, episode steps: 94, steps per second: 199, episode reward: 184.170, mean reward: 1.959 [1.460, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.249, 10.100], loss: 0.248552, mae: 0.436548, mean_q: 4.582106
 29868/100000: episode: 427, duration: 0.510s, episode steps: 94, steps per second: 184, episode reward: 178.074, mean reward: 1.894 [1.475, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.494, 10.100], loss: 0.312108, mae: 0.427362, mean_q: 4.570219
 29902/100000: episode: 428, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 88.976, mean reward: 2.617 [1.929, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.551, 10.331], loss: 0.372831, mae: 0.470954, mean_q: 4.619966
 29913/100000: episode: 429, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 43.582, mean reward: 3.962 [2.592, 7.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.477, 10.446], loss: 0.170274, mae: 0.431648, mean_q: 4.381249
[Info] 2-TH LEVEL FOUND: 7.601591110229492, Considering 10/90 traces
 29920/100000: episode: 430, duration: 4.091s, episode steps: 7, steps per second: 2, episode reward: 29.345, mean reward: 4.192 [2.757, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.567], loss: 0.312692, mae: 0.547467, mean_q: 4.864419
 29948/100000: episode: 431, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 134.091, mean reward: 4.789 [2.259, 12.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.423, 10.397], loss: 0.247457, mae: 0.449143, mean_q: 4.653741
 29970/100000: episode: 432, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 72.517, mean reward: 3.296 [2.575, 4.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.047, 10.467], loss: 0.403286, mae: 0.461937, mean_q: 4.715058
 29992/100000: episode: 433, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 88.109, mean reward: 4.005 [3.157, 6.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.815, 10.534], loss: 0.241212, mae: 0.469457, mean_q: 4.626308
 29995/100000: episode: 434, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 11.344, mean reward: 3.781 [3.692, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.514], loss: 0.138166, mae: 0.414558, mean_q: 4.780540
 29998/100000: episode: 435, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 16.644, mean reward: 5.548 [4.321, 6.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.035, 10.640], loss: 0.165113, mae: 0.398893, mean_q: 4.459874
 30001/100000: episode: 436, duration: 0.024s, episode steps: 3, steps per second: 127, episode reward: 14.597, mean reward: 4.866 [3.841, 6.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.452], loss: 0.212764, mae: 0.402839, mean_q: 4.699061
 30030/100000: episode: 437, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 132.886, mean reward: 4.582 [1.930, 11.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.103, 10.377], loss: 0.488327, mae: 0.474821, mean_q: 4.710715
 30059/100000: episode: 438, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 125.675, mean reward: 4.334 [2.209, 11.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.351, 10.389], loss: 0.318942, mae: 0.498359, mean_q: 4.810661
 30087/100000: episode: 439, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 159.068, mean reward: 5.681 [2.772, 22.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.904, 10.466], loss: 0.500169, mae: 0.499148, mean_q: 4.780079
 30109/100000: episode: 440, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 65.070, mean reward: 2.958 [2.290, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.405], loss: 0.221506, mae: 0.439629, mean_q: 4.683187
 30135/100000: episode: 441, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 106.819, mean reward: 4.108 [2.568, 7.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.007, 10.489], loss: 0.283592, mae: 0.480431, mean_q: 4.787889
 30154/100000: episode: 442, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 77.334, mean reward: 4.070 [2.531, 5.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.069, 10.406], loss: 0.515233, mae: 0.530306, mean_q: 4.820726
 30157/100000: episode: 443, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 12.302, mean reward: 4.101 [3.803, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.525], loss: 0.385636, mae: 0.572491, mean_q: 4.565630
 30179/100000: episode: 444, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 76.616, mean reward: 3.483 [2.702, 6.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.551, 10.456], loss: 0.372442, mae: 0.518465, mean_q: 4.836444
 30182/100000: episode: 445, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 12.174, mean reward: 4.058 [3.531, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.484], loss: 0.224368, mae: 0.480566, mean_q: 5.076672
 30185/100000: episode: 446, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 14.331, mean reward: 4.777 [4.520, 4.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.415], loss: 0.266626, mae: 0.490505, mean_q: 4.765572
 30207/100000: episode: 447, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 66.789, mean reward: 3.036 [2.227, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.340, 10.434], loss: 0.497453, mae: 0.505322, mean_q: 4.785250
 30226/100000: episode: 448, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 50.041, mean reward: 2.634 [2.053, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.396], loss: 0.366531, mae: 0.569333, mean_q: 4.980510
 30229/100000: episode: 449, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 11.429, mean reward: 3.810 [3.625, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.035, 10.520], loss: 1.725513, mae: 0.537985, mean_q: 4.363628
 30232/100000: episode: 450, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 14.337, mean reward: 4.779 [4.297, 5.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.475], loss: 0.251316, mae: 0.490826, mean_q: 4.899261
 30254/100000: episode: 451, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 97.547, mean reward: 4.434 [2.971, 7.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.527, 10.561], loss: 0.281843, mae: 0.537213, mean_q: 4.823151
 30267/100000: episode: 452, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 53.607, mean reward: 4.124 [3.566, 5.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.025, 10.100], loss: 0.294279, mae: 0.485818, mean_q: 4.959271
 30296/100000: episode: 453, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 108.119, mean reward: 3.728 [2.097, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.430], loss: 0.209625, mae: 0.437728, mean_q: 4.771274
 30322/100000: episode: 454, duration: 0.176s, episode steps: 26, steps per second: 148, episode reward: 91.051, mean reward: 3.502 [1.991, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.295], loss: 0.304498, mae: 0.481065, mean_q: 4.817016
 30344/100000: episode: 455, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 83.162, mean reward: 3.780 [2.053, 6.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.791, 10.371], loss: 0.231866, mae: 0.470699, mean_q: 4.817128
 30366/100000: episode: 456, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 71.247, mean reward: 3.239 [1.853, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.481, 10.424], loss: 0.303932, mae: 0.505866, mean_q: 4.919649
 30392/100000: episode: 457, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 89.156, mean reward: 3.429 [2.042, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.423], loss: 0.606342, mae: 0.551125, mean_q: 4.874804
 30405/100000: episode: 458, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 88.055, mean reward: 6.773 [4.177, 9.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.475, 10.100], loss: 0.535712, mae: 0.630896, mean_q: 5.079705
 30431/100000: episode: 459, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 212.458, mean reward: 8.171 [2.659, 25.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.583], loss: 0.265191, mae: 0.481569, mean_q: 4.979492
 30442/100000: episode: 460, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 33.342, mean reward: 3.031 [2.682, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.560], loss: 0.306991, mae: 0.494642, mean_q: 4.940812
 30464/100000: episode: 461, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 87.996, mean reward: 4.000 [2.626, 6.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.445], loss: 0.237618, mae: 0.473627, mean_q: 4.914763
 30477/100000: episode: 462, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 76.620, mean reward: 5.894 [3.820, 8.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.461, 10.100], loss: 0.567793, mae: 0.539013, mean_q: 5.194214
 30499/100000: episode: 463, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 95.532, mean reward: 4.342 [2.757, 7.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.930, 10.428], loss: 0.368090, mae: 0.532070, mean_q: 5.033545
 30510/100000: episode: 464, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 39.391, mean reward: 3.581 [3.083, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.535], loss: 0.421586, mae: 0.559009, mean_q: 4.942152
 30523/100000: episode: 465, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 59.662, mean reward: 4.589 [3.498, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.269, 10.100], loss: 0.322995, mae: 0.537680, mean_q: 5.162835
 30536/100000: episode: 466, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 42.196, mean reward: 3.246 [2.448, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.029, 10.100], loss: 0.352760, mae: 0.468434, mean_q: 5.103700
 30558/100000: episode: 467, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 59.650, mean reward: 2.711 [2.224, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.516, 10.398], loss: 0.525620, mae: 0.523894, mean_q: 5.016225
 30561/100000: episode: 468, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 12.563, mean reward: 4.188 [3.728, 4.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.408], loss: 0.590349, mae: 0.697164, mean_q: 5.396446
 30574/100000: episode: 469, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 59.005, mean reward: 4.539 [3.562, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.500, 10.100], loss: 0.252616, mae: 0.465940, mean_q: 4.933207
 30587/100000: episode: 470, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 68.261, mean reward: 5.251 [4.291, 6.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.541, 10.100], loss: 0.688295, mae: 0.549985, mean_q: 5.000736
 30600/100000: episode: 471, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 54.320, mean reward: 4.178 [2.549, 6.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.388, 10.100], loss: 0.514119, mae: 0.550374, mean_q: 5.124359
 30622/100000: episode: 472, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 72.431, mean reward: 3.292 [2.149, 4.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.374], loss: 0.655951, mae: 0.576629, mean_q: 5.238329
 30625/100000: episode: 473, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 13.411, mean reward: 4.470 [4.373, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.517], loss: 0.267222, mae: 0.536483, mean_q: 5.043210
 30651/100000: episode: 474, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 97.440, mean reward: 3.748 [2.858, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.524], loss: 0.522096, mae: 0.587286, mean_q: 5.214730
 30679/100000: episode: 475, duration: 0.165s, episode steps: 28, steps per second: 169, episode reward: 135.479, mean reward: 4.839 [3.528, 7.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.564], loss: 0.487463, mae: 0.567855, mean_q: 5.230072
 30695/100000: episode: 476, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 96.987, mean reward: 6.062 [3.968, 11.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.530, 10.100], loss: 0.564175, mae: 0.612246, mean_q: 5.129388
 30717/100000: episode: 477, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 96.559, mean reward: 4.389 [3.218, 5.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.161, 10.450], loss: 0.570004, mae: 0.555915, mean_q: 5.147599
 30739/100000: episode: 478, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 128.066, mean reward: 5.821 [2.541, 14.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.204, 10.604], loss: 0.775249, mae: 0.675090, mean_q: 5.282979
 30758/100000: episode: 479, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 73.247, mean reward: 3.855 [2.931, 6.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.289, 10.537], loss: 0.275423, mae: 0.538668, mean_q: 5.297121
 30777/100000: episode: 480, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 54.878, mean reward: 2.888 [2.087, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.438], loss: 0.716380, mae: 0.645555, mean_q: 5.390231
 30796/100000: episode: 481, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 52.110, mean reward: 2.743 [2.239, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.354], loss: 0.607630, mae: 0.599906, mean_q: 5.383921
 30818/100000: episode: 482, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 70.080, mean reward: 3.185 [2.479, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.750, 10.411], loss: 0.532070, mae: 0.575923, mean_q: 5.398470
 30844/100000: episode: 483, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 58.650, mean reward: 2.256 [1.507, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.373, 10.172], loss: 0.751004, mae: 0.634848, mean_q: 5.474718
 30847/100000: episode: 484, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 15.217, mean reward: 5.072 [4.629, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.558], loss: 0.319339, mae: 0.633640, mean_q: 5.519919
 30866/100000: episode: 485, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 80.130, mean reward: 4.217 [2.744, 5.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.707, 10.502], loss: 0.843455, mae: 0.746748, mean_q: 5.457361
 30885/100000: episode: 486, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 47.438, mean reward: 2.497 [1.761, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.345], loss: 0.544235, mae: 0.611060, mean_q: 5.410230
 30896/100000: episode: 487, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 50.752, mean reward: 4.614 [2.624, 6.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.579], loss: 0.349664, mae: 0.555784, mean_q: 5.341708
 30925/100000: episode: 488, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 133.319, mean reward: 4.597 [2.063, 11.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.378], loss: 0.472375, mae: 0.601689, mean_q: 5.456857
 30936/100000: episode: 489, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 38.497, mean reward: 3.500 [2.802, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.073, 10.473], loss: 0.497341, mae: 0.641066, mean_q: 5.387236
 30949/100000: episode: 490, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 182.524, mean reward: 14.040 [3.284, 53.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.531, 10.100], loss: 0.754769, mae: 0.689592, mean_q: 5.603436
 30971/100000: episode: 491, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 64.016, mean reward: 2.910 [2.088, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.398, 10.378], loss: 0.643795, mae: 0.641256, mean_q: 5.423897
 30990/100000: episode: 492, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 65.000, mean reward: 3.421 [2.674, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.420], loss: 0.472320, mae: 0.599838, mean_q: 5.461587
 31019/100000: episode: 493, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 98.529, mean reward: 3.398 [2.536, 5.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.397], loss: 1.963763, mae: 0.761260, mean_q: 5.640687
 31035/100000: episode: 494, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 120.443, mean reward: 7.528 [3.470, 16.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.447, 10.100], loss: 0.465405, mae: 0.676524, mean_q: 5.303020
 31038/100000: episode: 495, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 19.061, mean reward: 6.354 [6.217, 6.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.122, 10.513], loss: 2.485114, mae: 0.826531, mean_q: 5.693512
 31060/100000: episode: 496, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 69.665, mean reward: 3.167 [2.304, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.437, 10.387], loss: 0.487027, mae: 0.675511, mean_q: 5.614603
 31089/100000: episode: 497, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 255.014, mean reward: 8.794 [2.648, 38.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.107, 10.452], loss: 2.456432, mae: 0.937550, mean_q: 5.755754
 31111/100000: episode: 498, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 59.512, mean reward: 2.705 [2.133, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.127, 10.434], loss: 0.604977, mae: 0.638809, mean_q: 5.519863
 31127/100000: episode: 499, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 79.473, mean reward: 4.967 [3.588, 10.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.196, 10.100], loss: 5.038083, mae: 0.928137, mean_q: 5.772273
 31138/100000: episode: 500, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 34.680, mean reward: 3.153 [2.553, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.674, 10.440], loss: 0.959246, mae: 0.932613, mean_q: 5.772598
 31166/100000: episode: 501, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 123.980, mean reward: 4.428 [2.493, 6.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.248, 10.386], loss: 2.297634, mae: 0.954180, mean_q: 5.683352
 31177/100000: episode: 502, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 42.472, mean reward: 3.861 [3.115, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.523], loss: 0.697610, mae: 0.771486, mean_q: 5.645555
 31203/100000: episode: 503, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 84.257, mean reward: 3.241 [2.221, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.031, 10.326], loss: 1.636394, mae: 0.868879, mean_q: 5.704478
 31225/100000: episode: 504, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 156.292, mean reward: 7.104 [2.902, 11.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.110, 10.674], loss: 1.055616, mae: 0.739200, mean_q: 5.738209
 31228/100000: episode: 505, duration: 0.023s, episode steps: 3, steps per second: 128, episode reward: 17.486, mean reward: 5.829 [5.741, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.477], loss: 0.621691, mae: 0.760159, mean_q: 5.875887
 31241/100000: episode: 506, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 51.103, mean reward: 3.931 [2.894, 5.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.285, 10.100], loss: 2.611827, mae: 0.897980, mean_q: 5.843137
 31260/100000: episode: 507, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 72.340, mean reward: 3.807 [2.879, 6.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.881, 10.462], loss: 0.494496, mae: 0.652788, mean_q: 5.733049
 31286/100000: episode: 508, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 119.995, mean reward: 4.615 [2.045, 9.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.460, 10.329], loss: 0.734792, mae: 0.723509, mean_q: 5.833946
 31308/100000: episode: 509, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 88.421, mean reward: 4.019 [2.408, 6.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.294, 10.553], loss: 0.785281, mae: 0.679432, mean_q: 5.452681
 31319/100000: episode: 510, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 28.457, mean reward: 2.587 [1.943, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.398], loss: 0.571037, mae: 0.713713, mean_q: 5.925589
 31345/100000: episode: 511, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 72.851, mean reward: 2.802 [2.048, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.056, 10.381], loss: 0.425067, mae: 0.610298, mean_q: 5.553443
 31373/100000: episode: 512, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 241.364, mean reward: 8.620 [3.992, 17.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.137, 10.598], loss: 1.336206, mae: 0.850512, mean_q: 5.918804
 31392/100000: episode: 513, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 68.648, mean reward: 3.613 [2.391, 5.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.633, 10.403], loss: 1.084629, mae: 0.744994, mean_q: 5.907775
 31418/100000: episode: 514, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 71.811, mean reward: 2.762 [1.949, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.528, 10.268], loss: 1.299047, mae: 0.827190, mean_q: 5.933200
 31429/100000: episode: 515, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 32.759, mean reward: 2.978 [2.515, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.652, 10.418], loss: 0.910059, mae: 0.753367, mean_q: 6.228816
 31451/100000: episode: 516, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 73.211, mean reward: 3.328 [2.310, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.566], loss: 0.830063, mae: 0.707845, mean_q: 5.760513
 31454/100000: episode: 517, duration: 0.028s, episode steps: 3, steps per second: 107, episode reward: 12.076, mean reward: 4.025 [3.627, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.428], loss: 0.439944, mae: 0.619883, mean_q: 5.755709
 31476/100000: episode: 518, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 67.356, mean reward: 3.062 [2.390, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.777, 10.356], loss: 3.014198, mae: 0.985580, mean_q: 6.172148
 31492/100000: episode: 519, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 77.068, mean reward: 4.817 [3.327, 9.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.567, 10.100], loss: 1.396969, mae: 0.900993, mean_q: 5.876534
[Info] 3-TH LEVEL FOUND: 11.07097339630127, Considering 10/90 traces
 31495/100000: episode: 520, duration: 4.108s, episode steps: 3, steps per second: 1, episode reward: 11.717, mean reward: 3.906 [3.468, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.425], loss: 1.285660, mae: 0.853697, mean_q: 5.970198
 31504/100000: episode: 521, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 94.466, mean reward: 10.496 [4.587, 22.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.890, 10.100], loss: 0.663233, mae: 0.710832, mean_q: 5.905336
 31515/100000: episode: 522, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 52.892, mean reward: 4.808 [3.423, 6.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.399, 10.100], loss: 3.645480, mae: 0.879264, mean_q: 6.015600
 31526/100000: episode: 523, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 56.444, mean reward: 5.131 [3.412, 7.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.603, 10.100], loss: 1.620329, mae: 0.892704, mean_q: 5.825099
 31529/100000: episode: 524, duration: 0.027s, episode steps: 3, steps per second: 111, episode reward: 29.160, mean reward: 9.720 [7.515, 12.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.606, 10.100], loss: 0.668457, mae: 0.769086, mean_q: 5.553314
 31534/100000: episode: 525, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 103.049, mean reward: 20.610 [6.831, 51.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.631, 10.100], loss: 1.453553, mae: 0.887165, mean_q: 6.120780
 31539/100000: episode: 526, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 27.086, mean reward: 5.417 [3.923, 7.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.380, 10.100], loss: 2.347614, mae: 0.998625, mean_q: 6.019911
 31544/100000: episode: 527, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 31.638, mean reward: 6.328 [4.863, 9.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.524, 10.100], loss: 1.176753, mae: 0.851119, mean_q: 5.854760
 31555/100000: episode: 528, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 80.049, mean reward: 7.277 [3.488, 20.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.612, 10.100], loss: 0.899694, mae: 0.781845, mean_q: 6.105973
 31558/100000: episode: 529, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 9.583, mean reward: 3.194 [2.869, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.469, 10.100], loss: 0.907217, mae: 0.977262, mean_q: 6.562325
 31566/100000: episode: 530, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 37.057, mean reward: 4.632 [3.905, 6.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.560, 10.100], loss: 0.661936, mae: 0.686685, mean_q: 5.682620
 31576/100000: episode: 531, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 45.877, mean reward: 4.588 [3.817, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.467, 10.100], loss: 4.676663, mae: 1.229028, mean_q: 6.767162
 31586/100000: episode: 532, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 133.863, mean reward: 13.386 [6.747, 33.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.672, 10.100], loss: 2.760426, mae: 1.056401, mean_q: 6.185667
 31595/100000: episode: 533, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 61.914, mean reward: 6.879 [3.702, 13.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.976, 10.100], loss: 1.170135, mae: 0.866669, mean_q: 6.046831
 31598/100000: episode: 534, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 18.378, mean reward: 6.126 [5.676, 6.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.466, 10.100], loss: 1.833124, mae: 1.079911, mean_q: 6.959101
 31607/100000: episode: 535, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 42.056, mean reward: 4.673 [4.061, 6.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.443, 10.100], loss: 2.096473, mae: 0.896885, mean_q: 5.956323
 31612/100000: episode: 536, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 46.182, mean reward: 9.236 [4.332, 12.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.494, 10.100], loss: 2.942090, mae: 1.157386, mean_q: 6.696218
 31615/100000: episode: 537, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 15.634, mean reward: 5.211 [4.866, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.481, 10.100], loss: 0.528824, mae: 0.745084, mean_q: 6.169084
 31626/100000: episode: 538, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 63.744, mean reward: 5.795 [4.000, 7.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.779, 10.100], loss: 1.316378, mae: 0.957988, mean_q: 6.581600
 31635/100000: episode: 539, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 38.873, mean reward: 4.319 [3.664, 5.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.554, 10.100], loss: 0.857135, mae: 0.846261, mean_q: 5.925193
 31638/100000: episode: 540, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.982, mean reward: 2.994 [2.713, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.529, 10.100], loss: 0.473770, mae: 0.657495, mean_q: 5.961237
 31648/100000: episode: 541, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 50.551, mean reward: 5.055 [4.389, 5.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.434, 10.100], loss: 0.797628, mae: 0.773143, mean_q: 6.102483
 31659/100000: episode: 542, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 66.006, mean reward: 6.001 [3.812, 11.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.503, 10.100], loss: 1.207982, mae: 0.853614, mean_q: 6.265488
 31670/100000: episode: 543, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 60.562, mean reward: 5.506 [3.859, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.518, 10.100], loss: 1.688287, mae: 0.948593, mean_q: 6.305771
 31675/100000: episode: 544, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 41.816, mean reward: 8.363 [6.192, 11.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.475, 10.100], loss: 2.119694, mae: 1.193931, mean_q: 6.727939
 31678/100000: episode: 545, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 11.077, mean reward: 3.692 [3.118, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.570, 10.100], loss: 0.786965, mae: 0.833659, mean_q: 6.401249
 31687/100000: episode: 546, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 57.737, mean reward: 6.415 [4.123, 9.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.576, 10.100], loss: 1.978082, mae: 0.851898, mean_q: 6.171627
 31697/100000: episode: 547, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 62.331, mean reward: 6.233 [4.858, 9.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.519, 10.100], loss: 0.897696, mae: 0.853096, mean_q: 6.395184
 31707/100000: episode: 548, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 69.798, mean reward: 6.980 [4.441, 10.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.293, 10.100], loss: 1.646525, mae: 1.033728, mean_q: 6.350600
 31717/100000: episode: 549, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 71.466, mean reward: 7.147 [5.138, 8.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.600, 10.100], loss: 0.944834, mae: 0.827988, mean_q: 6.248782
 31727/100000: episode: 550, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 54.580, mean reward: 5.458 [4.095, 6.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.530, 10.100], loss: 0.560157, mae: 0.673997, mean_q: 6.036070
 31730/100000: episode: 551, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 13.612, mean reward: 4.537 [4.349, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.508, 10.100], loss: 12.285583, mae: 1.324661, mean_q: 6.251442
 31738/100000: episode: 552, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 28.859, mean reward: 3.607 [2.986, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.428, 10.100], loss: 3.072090, mae: 1.105152, mean_q: 6.769953
 31743/100000: episode: 553, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 76.530, mean reward: 15.306 [7.664, 36.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.457, 10.100], loss: 1.851055, mae: 0.998801, mean_q: 6.346372
 31748/100000: episode: 554, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 72.240, mean reward: 14.448 [8.253, 27.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.449, 10.100], loss: 0.659094, mae: 0.781223, mean_q: 5.791017
 31758/100000: episode: 555, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 100.574, mean reward: 10.057 [4.802, 18.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.430, 10.100], loss: 0.819055, mae: 0.811584, mean_q: 6.222662
 31767/100000: episode: 556, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 41.166, mean reward: 4.574 [3.408, 5.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.467, 10.100], loss: 0.975568, mae: 0.822774, mean_q: 6.363023
 31778/100000: episode: 557, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 56.228, mean reward: 5.112 [4.460, 5.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.544, 10.100], loss: 8.230048, mae: 1.333801, mean_q: 6.651966
 31781/100000: episode: 558, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 9.760, mean reward: 3.253 [3.005, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.508, 10.100], loss: 2.098026, mae: 1.460794, mean_q: 7.302332
[Info] FALSIFICATION!
 31782/100000: episode: 559, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.270, 9.911], loss: 1.437151, mae: 0.929700, mean_q: 6.374477
 31790/100000: episode: 560, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 54.128, mean reward: 6.766 [4.106, 12.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.599, 10.100], loss: 2.245546, mae: 1.080115, mean_q: 6.273340
 31793/100000: episode: 561, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 10.264, mean reward: 3.421 [2.988, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.511, 10.100], loss: 4.176072, mae: 1.219163, mean_q: 6.050877
 31796/100000: episode: 562, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 8.940, mean reward: 2.980 [2.492, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.589, 10.100], loss: 4.360098, mae: 1.136838, mean_q: 6.349091
 31805/100000: episode: 563, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 43.432, mean reward: 4.826 [3.474, 6.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.368, 10.100], loss: 2.527617, mae: 1.144369, mean_q: 6.858160
 31816/100000: episode: 564, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 49.410, mean reward: 4.492 [3.814, 5.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.559, 10.100], loss: 3.792912, mae: 1.278530, mean_q: 6.919791
 31826/100000: episode: 565, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 35.152, mean reward: 3.515 [2.269, 4.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.515, 10.100], loss: 0.935988, mae: 0.869314, mean_q: 6.498734
 31831/100000: episode: 566, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 32.738, mean reward: 6.548 [5.378, 9.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.550, 10.100], loss: 1.231511, mae: 0.897601, mean_q: 6.216197
[Info] FALSIFICATION!
 31833/100000: episode: 567, duration: 0.181s, episode steps: 2, steps per second: 11, episode reward: 1054.601, mean reward: 527.300 [54.601, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.271, 9.952], loss: 0.546865, mae: 0.744109, mean_q: 6.142418
 31836/100000: episode: 568, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 9.166, mean reward: 3.055 [2.653, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.552, 10.100], loss: 0.586158, mae: 0.764990, mean_q: 6.264645
 31841/100000: episode: 569, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 36.861, mean reward: 7.372 [5.823, 8.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.437, 10.100], loss: 1.335810, mae: 1.021915, mean_q: 6.803967
 31852/100000: episode: 570, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 51.839, mean reward: 4.713 [3.966, 5.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.427, 10.100], loss: 4.449246, mae: 1.128144, mean_q: 6.449944
 31863/100000: episode: 571, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 44.004, mean reward: 4.000 [3.406, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.540, 10.100], loss: 1392.275757, mae: 4.572820, mean_q: 7.566277
 31874/100000: episode: 572, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 45.734, mean reward: 4.158 [3.430, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.345, 10.100], loss: 9.902465, mae: 2.665586, mean_q: 8.167420
 31883/100000: episode: 573, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 34.425, mean reward: 3.825 [3.227, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.443, 10.100], loss: 5.034632, mae: 1.829637, mean_q: 6.234563
 31892/100000: episode: 574, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 44.628, mean reward: 4.959 [4.092, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.496, 10.100], loss: 3.272279, mae: 1.470979, mean_q: 6.111272
 31903/100000: episode: 575, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 59.931, mean reward: 5.448 [4.080, 8.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.360, 10.100], loss: 2.451362, mae: 1.076678, mean_q: 6.511319
 31913/100000: episode: 576, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 59.555, mean reward: 5.955 [3.402, 12.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.392, 10.100], loss: 2.505751, mae: 1.092335, mean_q: 6.992230
 31922/100000: episode: 577, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 57.154, mean reward: 6.350 [4.267, 8.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.388, 10.100], loss: 4.218260, mae: 1.269063, mean_q: 6.756494
 31933/100000: episode: 578, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 49.317, mean reward: 4.483 [3.738, 6.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.450, 10.100], loss: 6.113377, mae: 1.273465, mean_q: 6.861614
 31941/100000: episode: 579, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 43.776, mean reward: 5.472 [4.015, 7.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.730, 10.100], loss: 1.469238, mae: 1.051681, mean_q: 7.208963
 31952/100000: episode: 580, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 56.503, mean reward: 5.137 [3.809, 12.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.628, 10.100], loss: 1393.954468, mae: 5.601283, mean_q: 8.871163
 31955/100000: episode: 581, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 44.368, mean reward: 14.789 [5.908, 27.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.575, 10.100], loss: 8.324496, mae: 2.670687, mean_q: 8.511252
 31966/100000: episode: 582, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 73.788, mean reward: 6.708 [3.560, 12.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.909, 10.100], loss: 3.661611, mae: 1.474870, mean_q: 7.294079
 31969/100000: episode: 583, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 9.974, mean reward: 3.325 [2.976, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.526, 10.100], loss: 1.576900, mae: 1.292874, mean_q: 5.915418
 31979/100000: episode: 584, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 53.318, mean reward: 5.332 [3.529, 9.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.514, 10.100], loss: 5.777845, mae: 1.441349, mean_q: 5.814343
 31989/100000: episode: 585, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 71.376, mean reward: 7.138 [4.227, 19.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.554, 10.100], loss: 3049.937256, mae: 7.452700, mean_q: 6.932013
 31992/100000: episode: 586, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 9.013, mean reward: 3.004 [2.778, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.456, 10.100], loss: 13.630406, mae: 3.799733, mean_q: 10.065207
 31997/100000: episode: 587, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 20.980, mean reward: 4.196 [3.485, 4.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.409, 10.100], loss: 22.771420, mae: 4.485662, mean_q: 10.595434
 32008/100000: episode: 588, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 61.324, mean reward: 5.575 [3.421, 8.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.454, 10.100], loss: 9.596108, mae: 2.883655, mean_q: 9.040434
[Info] FALSIFICATION!
 32013/100000: episode: 589, duration: 0.303s, episode steps: 5, steps per second: 16, episode reward: 1063.300, mean reward: 212.660 [9.305, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.398, 10.046], loss: 3.209633, mae: 1.630273, mean_q: 7.654454
 32016/100000: episode: 590, duration: 0.023s, episode steps: 3, steps per second: 128, episode reward: 9.210, mean reward: 3.070 [2.875, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.475, 10.100], loss: 3.460727, mae: 1.684657, mean_q: 6.367747
 32026/100000: episode: 591, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 46.627, mean reward: 4.663 [3.719, 5.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.500, 10.100], loss: 1531.523315, mae: 4.521705, mean_q: 6.599021
 32036/100000: episode: 592, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 64.255, mean reward: 6.426 [4.721, 12.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.481, 10.100], loss: 4.502622, mae: 1.701689, mean_q: 8.116527
 32047/100000: episode: 593, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 74.894, mean reward: 6.809 [3.862, 9.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.692, 10.100], loss: 1378.512329, mae: 4.662022, mean_q: 8.723545
 32050/100000: episode: 594, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 16.886, mean reward: 5.629 [5.333, 6.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.785, 10.100], loss: 7.079973, mae: 2.396770, mean_q: 9.120976
 32053/100000: episode: 595, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 12.992, mean reward: 4.331 [3.871, 4.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.542, 10.100], loss: 4.939158, mae: 2.240481, mean_q: 8.918296
 32063/100000: episode: 596, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 66.834, mean reward: 6.683 [4.909, 10.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.530, 10.100], loss: 4.258491, mae: 1.798925, mean_q: 8.253373
 32074/100000: episode: 597, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 71.028, mean reward: 6.457 [4.275, 8.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.569, 10.100], loss: 1383.265381, mae: 4.207547, mean_q: 7.582103
 32084/100000: episode: 598, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 70.150, mean reward: 7.015 [5.068, 13.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.477, 10.100], loss: 6.107266, mae: 2.144332, mean_q: 8.956625
 32094/100000: episode: 599, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 69.156, mean reward: 6.916 [5.652, 9.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.472, 10.100], loss: 7.012244, mae: 1.855970, mean_q: 8.127800
 32099/100000: episode: 600, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 35.144, mean reward: 7.029 [5.631, 8.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.428, 10.100], loss: 3.211422, mae: 1.589001, mean_q: 8.129802
 32110/100000: episode: 601, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 41.573, mean reward: 3.779 [3.313, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.457, 10.100], loss: 1387.858765, mae: 4.482956, mean_q: 8.190732
 32115/100000: episode: 602, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 27.748, mean reward: 5.550 [3.834, 7.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.365, 10.100], loss: 4.510223, mae: 1.974175, mean_q: 8.457817
 32124/100000: episode: 603, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 35.057, mean reward: 3.895 [3.080, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.477, 10.100], loss: 9.931301, mae: 2.879699, mean_q: 9.658901
 32132/100000: episode: 604, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 34.496, mean reward: 4.312 [3.271, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.475, 10.100], loss: 7.883533, mae: 2.527511, mean_q: 9.321993
 32135/100000: episode: 605, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 7.406, mean reward: 2.469 [2.336, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.415, 10.100], loss: 4.099043, mae: 1.740430, mean_q: 8.282409
 32146/100000: episode: 606, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 49.050, mean reward: 4.459 [3.525, 7.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.484, 10.100], loss: 3.364831, mae: 1.510599, mean_q: 8.044168
 32157/100000: episode: 607, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 59.671, mean reward: 5.425 [3.793, 7.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.556, 10.100], loss: 1376.444458, mae: 4.862961, mean_q: 8.157929
 32165/100000: episode: 608, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 55.243, mean reward: 6.905 [5.154, 13.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.425, 10.100], loss: 3733.018066, mae: 11.615089, mean_q: 10.862684
 32175/100000: episode: 609, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 59.483, mean reward: 5.948 [4.366, 15.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.497, 10.100], loss: 45.492004, mae: 6.561211, mean_q: 13.473677
[Info] Complete ISplit Iteration
[Info] Levels: [5.1991024, 7.601591, 11.070973, 24.688372]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.58]
[Info] Error Prob: 0.0005800000000000001

 32178/100000: episode: 610, duration: 4.145s, episode steps: 3, steps per second: 1, episode reward: 8.114, mean reward: 2.705 [2.590, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.565, 10.100], loss: 26.009806, mae: 5.136851, mean_q: 12.081019
 32278/100000: episode: 611, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 197.749, mean reward: 1.977 [1.487, 6.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.098], loss: 305.736206, mae: 2.494282, mean_q: 8.350208
 32378/100000: episode: 612, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 182.978, mean reward: 1.830 [1.454, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.166, 10.120], loss: 307.599457, mae: 2.251698, mean_q: 8.246790
 32478/100000: episode: 613, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.586, mean reward: 1.896 [1.466, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.452, 10.157], loss: 307.046906, mae: 1.996817, mean_q: 7.571533
 32578/100000: episode: 614, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 182.721, mean reward: 1.827 [1.441, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.099], loss: 458.815460, mae: 2.966476, mean_q: 8.496476
 32678/100000: episode: 615, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.939, mean reward: 1.839 [1.442, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.573, 10.142], loss: 154.962875, mae: 1.644522, mean_q: 7.352998
 32778/100000: episode: 616, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 189.110, mean reward: 1.891 [1.498, 5.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.919, 10.098], loss: 3.291149, mae: 1.226123, mean_q: 7.074011
 32878/100000: episode: 617, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 180.000, mean reward: 1.800 [1.469, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.085, 10.098], loss: 3.387518, mae: 1.203367, mean_q: 6.744247
 32978/100000: episode: 618, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 211.948, mean reward: 2.119 [1.476, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.849, 10.311], loss: 307.313232, mae: 2.057957, mean_q: 7.174854
 33078/100000: episode: 619, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.904, mean reward: 1.899 [1.500, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.868, 10.098], loss: 156.529068, mae: 1.813654, mean_q: 7.306741
 33178/100000: episode: 620, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 190.710, mean reward: 1.907 [1.461, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.098], loss: 154.879791, mae: 1.552410, mean_q: 6.983818
 33278/100000: episode: 621, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 198.712, mean reward: 1.987 [1.453, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.970, 10.468], loss: 155.230865, mae: 1.626849, mean_q: 6.904478
 33378/100000: episode: 622, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 178.802, mean reward: 1.788 [1.465, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.135, 10.180], loss: 2.846034, mae: 1.120739, mean_q: 6.712100
 33478/100000: episode: 623, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 196.179, mean reward: 1.962 [1.472, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.057, 10.098], loss: 155.776001, mae: 1.629961, mean_q: 6.868048
 33578/100000: episode: 624, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 186.679, mean reward: 1.867 [1.469, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.244, 10.098], loss: 2.255654, mae: 1.052073, mean_q: 6.497358
 33678/100000: episode: 625, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.179, mean reward: 1.892 [1.434, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.490, 10.098], loss: 609.455872, mae: 2.935239, mean_q: 7.339192
 33778/100000: episode: 626, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 189.004, mean reward: 1.890 [1.455, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.185, 10.098], loss: 307.803253, mae: 2.393193, mean_q: 7.772076
 33878/100000: episode: 627, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 209.046, mean reward: 2.090 [1.452, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.636, 10.380], loss: 155.361206, mae: 1.612214, mean_q: 6.985383
 33978/100000: episode: 628, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 187.518, mean reward: 1.875 [1.439, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.131], loss: 154.089081, mae: 1.498557, mean_q: 6.716299
 34078/100000: episode: 629, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 193.021, mean reward: 1.930 [1.507, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.098], loss: 156.360229, mae: 1.851324, mean_q: 7.419850
 34178/100000: episode: 630, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.963, mean reward: 1.930 [1.453, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.040, 10.098], loss: 155.706894, mae: 1.532881, mean_q: 6.868082
 34278/100000: episode: 631, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 212.832, mean reward: 2.128 [1.479, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.354, 10.223], loss: 1.983565, mae: 1.000123, mean_q: 6.344722
 34378/100000: episode: 632, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 213.955, mean reward: 2.140 [1.546, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.333], loss: 2.020761, mae: 0.915639, mean_q: 6.239955
 34478/100000: episode: 633, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 203.674, mean reward: 2.037 [1.483, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.098], loss: 308.213379, mae: 2.027313, mean_q: 6.773710
 34578/100000: episode: 634, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 183.255, mean reward: 1.833 [1.483, 2.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.207, 10.098], loss: 3.401743, mae: 1.351282, mean_q: 6.600309
 34678/100000: episode: 635, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.196, mean reward: 1.832 [1.463, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.679, 10.346], loss: 1057.709106, mae: 4.522067, mean_q: 8.174024
 34778/100000: episode: 636, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 217.573, mean reward: 2.176 [1.482, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.599, 10.522], loss: 606.100342, mae: 3.427253, mean_q: 7.935332
 34878/100000: episode: 637, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.380, mean reward: 1.984 [1.500, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.928, 10.098], loss: 606.826721, mae: 3.741596, mean_q: 8.951042
 34978/100000: episode: 638, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.644, mean reward: 1.936 [1.488, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.543, 10.319], loss: 2.473482, mae: 1.143752, mean_q: 6.782183
 35078/100000: episode: 639, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 215.766, mean reward: 2.158 [1.436, 5.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.770, 10.098], loss: 456.154175, mae: 2.572008, mean_q: 7.300212
 35178/100000: episode: 640, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 225.969, mean reward: 2.260 [1.468, 5.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.693, 10.350], loss: 305.904907, mae: 2.006864, mean_q: 6.656737
 35278/100000: episode: 641, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.146, mean reward: 1.921 [1.467, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.425, 10.346], loss: 605.573608, mae: 3.115356, mean_q: 7.495142
 35378/100000: episode: 642, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 206.512, mean reward: 2.065 [1.459, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.523, 10.098], loss: 303.950500, mae: 2.199335, mean_q: 6.932547
 35478/100000: episode: 643, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 201.772, mean reward: 2.018 [1.492, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.964, 10.412], loss: 304.959595, mae: 2.160970, mean_q: 6.696288
 35578/100000: episode: 644, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.763, mean reward: 1.898 [1.450, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.599, 10.098], loss: 455.178131, mae: 2.469965, mean_q: 6.819776
 35678/100000: episode: 645, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.320, mean reward: 1.953 [1.443, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.816, 10.275], loss: 605.525513, mae: 3.094257, mean_q: 7.266262
 35778/100000: episode: 646, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 201.130, mean reward: 2.011 [1.499, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.734, 10.455], loss: 4.017251, mae: 1.266690, mean_q: 6.145178
 35878/100000: episode: 647, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.896, mean reward: 1.839 [1.454, 2.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.988, 10.147], loss: 2.437281, mae: 0.980245, mean_q: 5.666091
 35978/100000: episode: 648, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.910, mean reward: 1.849 [1.450, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.135, 10.198], loss: 306.681763, mae: 1.940354, mean_q: 6.159441
 36078/100000: episode: 649, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.125, mean reward: 1.811 [1.450, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.417, 10.158], loss: 303.464172, mae: 1.827149, mean_q: 5.924231
 36178/100000: episode: 650, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.962, mean reward: 1.910 [1.455, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.098], loss: 603.174500, mae: 2.601559, mean_q: 6.117739
 36278/100000: episode: 651, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 178.771, mean reward: 1.788 [1.453, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.281, 10.178], loss: 3.964204, mae: 1.311549, mean_q: 5.571587
 36378/100000: episode: 652, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 181.914, mean reward: 1.819 [1.449, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.706, 10.168], loss: 752.845337, mae: 3.021226, mean_q: 6.166096
 36478/100000: episode: 653, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 182.965, mean reward: 1.830 [1.449, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.408, 10.098], loss: 3.411356, mae: 1.281562, mean_q: 5.401201
 36578/100000: episode: 654, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 184.860, mean reward: 1.849 [1.453, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.207], loss: 455.562805, mae: 2.783859, mean_q: 5.988812
 36678/100000: episode: 655, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 194.820, mean reward: 1.948 [1.491, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.698, 10.125], loss: 151.312119, mae: 1.147433, mean_q: 4.689281
 36778/100000: episode: 656, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 255.370, mean reward: 2.554 [1.459, 8.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.495, 10.516], loss: 0.994539, mae: 0.665841, mean_q: 4.429498
 36878/100000: episode: 657, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 199.222, mean reward: 1.992 [1.500, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.537, 10.158], loss: 0.567859, mae: 0.524727, mean_q: 4.255087
 36978/100000: episode: 658, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.770, mean reward: 1.888 [1.465, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.694, 10.130], loss: 0.352506, mae: 0.446364, mean_q: 4.082530
 37078/100000: episode: 659, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 213.757, mean reward: 2.138 [1.443, 9.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.098], loss: 0.233050, mae: 0.388755, mean_q: 3.947647
 37178/100000: episode: 660, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 191.342, mean reward: 1.913 [1.469, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.962, 10.098], loss: 0.184011, mae: 0.376470, mean_q: 3.863117
 37278/100000: episode: 661, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 183.334, mean reward: 1.833 [1.446, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.547, 10.164], loss: 0.136804, mae: 0.340729, mean_q: 3.846242
 37378/100000: episode: 662, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 216.984, mean reward: 2.170 [1.508, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.327, 10.098], loss: 0.132010, mae: 0.342941, mean_q: 3.874370
 37478/100000: episode: 663, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 198.860, mean reward: 1.989 [1.486, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.586, 10.098], loss: 0.151229, mae: 0.358216, mean_q: 3.874338
 37578/100000: episode: 664, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 235.437, mean reward: 2.354 [1.493, 6.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.312, 10.392], loss: 0.151040, mae: 0.357175, mean_q: 3.903571
 37678/100000: episode: 665, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 217.269, mean reward: 2.173 [1.445, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.112], loss: 0.149848, mae: 0.350371, mean_q: 3.903950
 37778/100000: episode: 666, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 175.193, mean reward: 1.752 [1.442, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.234, 10.098], loss: 0.151741, mae: 0.348468, mean_q: 3.885120
 37878/100000: episode: 667, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 211.269, mean reward: 2.113 [1.501, 6.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.761, 10.281], loss: 0.144607, mae: 0.353305, mean_q: 3.930768
 37978/100000: episode: 668, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 201.140, mean reward: 2.011 [1.458, 6.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.015, 10.121], loss: 0.147690, mae: 0.355352, mean_q: 3.923706
 38078/100000: episode: 669, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.709, mean reward: 1.847 [1.458, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.676, 10.144], loss: 0.154250, mae: 0.355454, mean_q: 3.915126
 38178/100000: episode: 670, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.971, mean reward: 1.900 [1.458, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.945, 10.098], loss: 0.150542, mae: 0.354374, mean_q: 3.936131
 38278/100000: episode: 671, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.023, mean reward: 1.820 [1.467, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.525, 10.310], loss: 0.140155, mae: 0.348163, mean_q: 3.914110
 38378/100000: episode: 672, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 197.807, mean reward: 1.978 [1.474, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.264, 10.197], loss: 0.134883, mae: 0.345768, mean_q: 3.917626
 38478/100000: episode: 673, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 202.444, mean reward: 2.024 [1.471, 7.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.549, 10.207], loss: 0.134671, mae: 0.342917, mean_q: 3.909632
 38578/100000: episode: 674, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 178.235, mean reward: 1.782 [1.472, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.519, 10.098], loss: 0.137340, mae: 0.348383, mean_q: 3.921995
 38678/100000: episode: 675, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 181.810, mean reward: 1.818 [1.438, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.302, 10.319], loss: 0.112858, mae: 0.323120, mean_q: 3.895850
 38778/100000: episode: 676, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.591, mean reward: 2.016 [1.480, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.922, 10.098], loss: 0.137017, mae: 0.346224, mean_q: 3.930617
 38878/100000: episode: 677, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.972, mean reward: 1.870 [1.453, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.461, 10.098], loss: 0.137156, mae: 0.333469, mean_q: 3.901410
 38978/100000: episode: 678, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 207.188, mean reward: 2.072 [1.507, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.468, 10.098], loss: 0.153752, mae: 0.346226, mean_q: 3.937127
 39078/100000: episode: 679, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.665, mean reward: 1.927 [1.479, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.940, 10.117], loss: 0.119419, mae: 0.327687, mean_q: 3.899841
 39178/100000: episode: 680, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 182.583, mean reward: 1.826 [1.455, 2.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.820, 10.116], loss: 0.121962, mae: 0.331392, mean_q: 3.904294
 39278/100000: episode: 681, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 231.785, mean reward: 2.318 [1.461, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.615, 10.343], loss: 0.127161, mae: 0.336653, mean_q: 3.910356
 39378/100000: episode: 682, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 184.684, mean reward: 1.847 [1.469, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.187, 10.098], loss: 0.140041, mae: 0.338278, mean_q: 3.917836
 39478/100000: episode: 683, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.556, mean reward: 1.846 [1.482, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.228], loss: 0.126670, mae: 0.330108, mean_q: 3.905772
 39578/100000: episode: 684, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.862, mean reward: 1.809 [1.466, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.851, 10.098], loss: 0.117226, mae: 0.320917, mean_q: 3.903668
 39678/100000: episode: 685, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 195.434, mean reward: 1.954 [1.459, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.862, 10.098], loss: 0.147633, mae: 0.339918, mean_q: 3.909202
 39778/100000: episode: 686, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.472, mean reward: 1.895 [1.443, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.098], loss: 0.131300, mae: 0.324056, mean_q: 3.909268
 39878/100000: episode: 687, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 187.786, mean reward: 1.878 [1.459, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.923, 10.098], loss: 0.120261, mae: 0.323272, mean_q: 3.900039
 39978/100000: episode: 688, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 182.776, mean reward: 1.828 [1.482, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.576, 10.132], loss: 0.131045, mae: 0.337055, mean_q: 3.899081
 40078/100000: episode: 689, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 188.542, mean reward: 1.885 [1.437, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.318], loss: 0.112081, mae: 0.323462, mean_q: 3.898105
 40178/100000: episode: 690, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.604, mean reward: 1.906 [1.470, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.790, 10.177], loss: 0.129993, mae: 0.324498, mean_q: 3.869709
 40278/100000: episode: 691, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 212.407, mean reward: 2.124 [1.480, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.290, 10.098], loss: 0.100585, mae: 0.305846, mean_q: 3.860178
 40378/100000: episode: 692, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 192.211, mean reward: 1.922 [1.507, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.029, 10.169], loss: 0.113609, mae: 0.317835, mean_q: 3.875175
 40478/100000: episode: 693, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 185.412, mean reward: 1.854 [1.491, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.098], loss: 0.124297, mae: 0.315435, mean_q: 3.869576
 40578/100000: episode: 694, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.933, mean reward: 1.989 [1.470, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.688, 10.098], loss: 0.107076, mae: 0.306324, mean_q: 3.865031
 40678/100000: episode: 695, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.582, mean reward: 1.866 [1.458, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.601, 10.098], loss: 0.108139, mae: 0.307415, mean_q: 3.845961
 40778/100000: episode: 696, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 193.327, mean reward: 1.933 [1.461, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.773, 10.098], loss: 0.110500, mae: 0.307228, mean_q: 3.848324
 40878/100000: episode: 697, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 176.701, mean reward: 1.767 [1.461, 2.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.060, 10.171], loss: 0.113109, mae: 0.307459, mean_q: 3.843021
 40978/100000: episode: 698, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 196.851, mean reward: 1.969 [1.495, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.659, 10.098], loss: 0.114234, mae: 0.296708, mean_q: 3.824849
 41078/100000: episode: 699, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.878, mean reward: 1.899 [1.448, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.169], loss: 0.094079, mae: 0.298515, mean_q: 3.855985
 41178/100000: episode: 700, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 207.478, mean reward: 2.075 [1.484, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.652, 10.383], loss: 0.116979, mae: 0.303375, mean_q: 3.850734
 41278/100000: episode: 701, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 214.374, mean reward: 2.144 [1.471, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.622, 10.255], loss: 0.125269, mae: 0.318288, mean_q: 3.855175
 41378/100000: episode: 702, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 217.199, mean reward: 2.172 [1.437, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.535, 10.098], loss: 0.116208, mae: 0.322297, mean_q: 3.892756
 41478/100000: episode: 703, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 195.241, mean reward: 1.952 [1.459, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.248], loss: 0.119376, mae: 0.320622, mean_q: 3.886850
 41578/100000: episode: 704, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.898, mean reward: 1.959 [1.471, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.693, 10.098], loss: 0.098218, mae: 0.304906, mean_q: 3.875852
 41678/100000: episode: 705, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.646, mean reward: 1.916 [1.434, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.238, 10.271], loss: 0.098877, mae: 0.300987, mean_q: 3.867865
 41778/100000: episode: 706, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 187.217, mean reward: 1.872 [1.468, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.429, 10.321], loss: 0.107222, mae: 0.302811, mean_q: 3.875865
 41878/100000: episode: 707, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 190.710, mean reward: 1.907 [1.493, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.698, 10.101], loss: 0.106575, mae: 0.309259, mean_q: 3.868951
 41978/100000: episode: 708, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 186.936, mean reward: 1.869 [1.435, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.187], loss: 0.100050, mae: 0.300266, mean_q: 3.863033
 42078/100000: episode: 709, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.272, mean reward: 1.893 [1.448, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.504, 10.142], loss: 0.097450, mae: 0.304547, mean_q: 3.850832
[Info] 1-TH LEVEL FOUND: 6.007113933563232, Considering 10/90 traces
 42178/100000: episode: 710, duration: 4.513s, episode steps: 100, steps per second: 22, episode reward: 182.174, mean reward: 1.822 [1.490, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.366, 10.162], loss: 0.091418, mae: 0.297691, mean_q: 3.846152
 42183/100000: episode: 711, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 19.834, mean reward: 3.967 [3.390, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.384, 10.100], loss: 0.098938, mae: 0.305407, mean_q: 3.899648
 42211/100000: episode: 712, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 125.555, mean reward: 4.484 [2.529, 8.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.144, 10.598], loss: 0.090636, mae: 0.293115, mean_q: 3.829191
 42221/100000: episode: 713, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 23.631, mean reward: 2.363 [1.926, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.209, 10.100], loss: 0.082141, mae: 0.278430, mean_q: 3.845439
 42249/100000: episode: 714, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 66.733, mean reward: 2.383 [1.746, 4.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.260], loss: 0.082508, mae: 0.293292, mean_q: 3.873816
 42263/100000: episode: 715, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 42.729, mean reward: 3.052 [2.273, 5.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.460], loss: 0.106577, mae: 0.295632, mean_q: 3.854989
 42288/100000: episode: 716, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 58.836, mean reward: 2.353 [1.905, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.413], loss: 0.098882, mae: 0.304607, mean_q: 3.876473
 42315/100000: episode: 717, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 66.578, mean reward: 2.466 [1.812, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.754, 10.360], loss: 0.129153, mae: 0.327589, mean_q: 3.880732
 42330/100000: episode: 718, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 36.954, mean reward: 2.464 [2.065, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.432], loss: 0.121685, mae: 0.325662, mean_q: 3.919903
 42335/100000: episode: 719, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 14.748, mean reward: 2.950 [2.451, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.331, 10.100], loss: 0.098737, mae: 0.313748, mean_q: 3.841537
 42375/100000: episode: 720, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 356.341, mean reward: 8.909 [2.683, 168.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-1.383, 10.531], loss: 0.105420, mae: 0.320483, mean_q: 3.947947
 42400/100000: episode: 721, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 67.351, mean reward: 2.694 [2.144, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.165, 10.381], loss: 0.121126, mae: 0.321958, mean_q: 3.911678
 42434/100000: episode: 722, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 158.073, mean reward: 4.649 [2.586, 10.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.439, 10.684], loss: 0.171352, mae: 0.351825, mean_q: 3.973108
 42449/100000: episode: 723, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 38.395, mean reward: 2.560 [2.196, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.144, 10.371], loss: 0.144200, mae: 0.364344, mean_q: 4.025970
 42459/100000: episode: 724, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 23.773, mean reward: 2.377 [2.022, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.153, 10.100], loss: 0.154258, mae: 0.339645, mean_q: 3.855640
 42484/100000: episode: 725, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 59.548, mean reward: 2.382 [2.004, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.216, 10.356], loss: 0.187263, mae: 0.372950, mean_q: 4.083507
 42498/100000: episode: 726, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 39.920, mean reward: 2.851 [2.285, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.379], loss: 0.267423, mae: 0.413609, mean_q: 4.133523
 42526/100000: episode: 727, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 79.716, mean reward: 2.847 [2.330, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.358], loss: 0.123436, mae: 0.336801, mean_q: 3.909985
 42547/100000: episode: 728, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 48.703, mean reward: 2.319 [1.702, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.627, 10.233], loss: 0.166262, mae: 0.348655, mean_q: 3.996536
 42587/100000: episode: 729, duration: 0.207s, episode steps: 40, steps per second: 194, episode reward: 95.006, mean reward: 2.375 [1.880, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.531, 10.272], loss: 0.185452, mae: 0.365235, mean_q: 4.072902
 42615/100000: episode: 730, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 84.799, mean reward: 3.029 [2.408, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.397, 10.484], loss: 0.217599, mae: 0.386315, mean_q: 3.973903
 42643/100000: episode: 731, duration: 0.162s, episode steps: 28, steps per second: 172, episode reward: 70.108, mean reward: 2.504 [1.686, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.369, 10.256], loss: 0.116374, mae: 0.339068, mean_q: 4.012572
 42677/100000: episode: 732, duration: 0.173s, episode steps: 34, steps per second: 196, episode reward: 103.830, mean reward: 3.054 [2.069, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.445, 10.354], loss: 0.153395, mae: 0.361882, mean_q: 4.023502
 42698/100000: episode: 733, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 49.133, mean reward: 2.340 [1.774, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.431], loss: 19.619297, mae: 0.680994, mean_q: 4.189315
 42708/100000: episode: 734, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 37.293, mean reward: 3.729 [2.495, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.332, 10.100], loss: 1.812884, mae: 0.924243, mean_q: 4.607881
 42723/100000: episode: 735, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 53.682, mean reward: 3.579 [2.123, 10.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.565, 10.601], loss: 0.265852, mae: 0.489612, mean_q: 3.799463
 42733/100000: episode: 736, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 28.244, mean reward: 2.824 [2.103, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.320, 10.100], loss: 0.440666, mae: 0.505057, mean_q: 3.879022
 42747/100000: episode: 737, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 31.950, mean reward: 2.282 [1.922, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.374, 10.404], loss: 0.247832, mae: 0.439350, mean_q: 3.948794
 42775/100000: episode: 738, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 74.077, mean reward: 2.646 [1.899, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.454, 10.266], loss: 0.269010, mae: 0.439112, mean_q: 3.974207
 42785/100000: episode: 739, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 26.349, mean reward: 2.635 [2.132, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.719, 10.100], loss: 43.484543, mae: 0.954968, mean_q: 4.110410
 42825/100000: episode: 740, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 134.249, mean reward: 3.356 [1.709, 8.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.152, 10.264], loss: 21.576550, mae: 0.998182, mean_q: 4.257661
 42865/100000: episode: 741, duration: 0.206s, episode steps: 40, steps per second: 195, episode reward: 89.980, mean reward: 2.250 [1.576, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.703, 10.155], loss: 10.746758, mae: 0.650314, mean_q: 4.285989
 42880/100000: episode: 742, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 37.736, mean reward: 2.516 [2.024, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.431], loss: 0.387765, mae: 0.509546, mean_q: 4.196592
 42895/100000: episode: 743, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 47.585, mean reward: 3.172 [2.392, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.798, 10.472], loss: 0.241993, mae: 0.407042, mean_q: 3.951995
 42922/100000: episode: 744, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 87.081, mean reward: 3.225 [2.261, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.226, 10.577], loss: 0.162784, mae: 0.382904, mean_q: 4.062054
 42962/100000: episode: 745, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 187.734, mean reward: 4.693 [2.420, 13.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.173, 10.396], loss: 0.154336, mae: 0.360320, mean_q: 4.069535
 42989/100000: episode: 746, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 80.768, mean reward: 2.991 [2.408, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.447], loss: 0.192311, mae: 0.387815, mean_q: 4.121595
 43010/100000: episode: 747, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 56.373, mean reward: 2.684 [2.245, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.594, 10.468], loss: 0.217138, mae: 0.381565, mean_q: 4.110425
 43035/100000: episode: 748, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 63.512, mean reward: 2.540 [1.647, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.076, 10.252], loss: 0.183950, mae: 0.376378, mean_q: 4.072198
 43069/100000: episode: 749, duration: 0.198s, episode steps: 34, steps per second: 171, episode reward: 89.866, mean reward: 2.643 [2.029, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.032, 10.353], loss: 0.157914, mae: 0.371327, mean_q: 4.110989
 43090/100000: episode: 750, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 68.108, mean reward: 3.243 [2.080, 6.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.632], loss: 0.206562, mae: 0.403561, mean_q: 4.185361
 43111/100000: episode: 751, duration: 0.104s, episode steps: 21, steps per second: 203, episode reward: 55.374, mean reward: 2.637 [2.101, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.051, 10.327], loss: 0.255589, mae: 0.389911, mean_q: 4.169586
 43116/100000: episode: 752, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 18.902, mean reward: 3.780 [3.430, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.327, 10.100], loss: 0.346354, mae: 0.430679, mean_q: 4.042149
 43137/100000: episode: 753, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 51.537, mean reward: 2.454 [1.936, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.412, 10.386], loss: 0.274741, mae: 0.416001, mean_q: 4.322759
 43164/100000: episode: 754, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 87.367, mean reward: 3.236 [2.253, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.638, 10.435], loss: 0.182177, mae: 0.405002, mean_q: 4.140607
 43198/100000: episode: 755, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 109.592, mean reward: 3.223 [2.184, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.464], loss: 0.221669, mae: 0.392957, mean_q: 4.187612
 43232/100000: episode: 756, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 97.480, mean reward: 2.867 [2.270, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.409, 10.398], loss: 0.219542, mae: 0.388205, mean_q: 4.179396
 43260/100000: episode: 757, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 70.120, mean reward: 2.504 [1.911, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.189, 10.373], loss: 0.219986, mae: 0.393433, mean_q: 4.192626
 43285/100000: episode: 758, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 67.501, mean reward: 2.700 [2.081, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-2.036, 10.310], loss: 0.217867, mae: 0.394262, mean_q: 4.225334
 43313/100000: episode: 759, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 91.920, mean reward: 3.283 [2.066, 5.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.974, 10.478], loss: 0.207386, mae: 0.372956, mean_q: 4.149048
 43328/100000: episode: 760, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 54.610, mean reward: 3.641 [2.284, 4.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.100, 10.482], loss: 0.193463, mae: 0.395800, mean_q: 4.255100
 43342/100000: episode: 761, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 41.281, mean reward: 2.949 [2.261, 6.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.321, 10.438], loss: 0.206635, mae: 0.386232, mean_q: 4.203139
 43352/100000: episode: 762, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 24.775, mean reward: 2.477 [1.993, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.221, 10.100], loss: 0.251623, mae: 0.418867, mean_q: 4.289023
 43367/100000: episode: 763, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 39.693, mean reward: 2.646 [1.800, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.106, 10.322], loss: 0.440299, mae: 0.495855, mean_q: 4.442637
 43401/100000: episode: 764, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 77.948, mean reward: 2.293 [1.701, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.039, 10.257], loss: 0.210354, mae: 0.407963, mean_q: 4.270000
 43426/100000: episode: 765, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 66.551, mean reward: 2.662 [2.057, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.394], loss: 0.160352, mae: 0.380232, mean_q: 4.229302
 43431/100000: episode: 766, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 21.452, mean reward: 4.290 [3.589, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.422, 10.100], loss: 0.274619, mae: 0.424607, mean_q: 4.320544
 43441/100000: episode: 767, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 25.937, mean reward: 2.594 [2.181, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.331, 10.100], loss: 0.175026, mae: 0.365944, mean_q: 4.288403
 43475/100000: episode: 768, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 81.041, mean reward: 2.384 [1.621, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.199, 10.194], loss: 12.869238, mae: 0.848800, mean_q: 4.561451
 43515/100000: episode: 769, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 108.827, mean reward: 2.721 [1.829, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.874, 10.312], loss: 0.358382, mae: 0.493652, mean_q: 4.231828
 43540/100000: episode: 770, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 64.742, mean reward: 2.590 [1.651, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.311], loss: 0.316032, mae: 0.448973, mean_q: 4.248752
 43580/100000: episode: 771, duration: 0.210s, episode steps: 40, steps per second: 190, episode reward: 105.425, mean reward: 2.636 [1.746, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.263, 10.316], loss: 11.185853, mae: 0.743999, mean_q: 4.482962
 43595/100000: episode: 772, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 35.711, mean reward: 2.381 [1.963, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.494, 10.361], loss: 0.202699, mae: 0.435907, mean_q: 4.256579
 43629/100000: episode: 773, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 80.407, mean reward: 2.365 [1.627, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.590, 10.407], loss: 12.985146, mae: 0.861799, mean_q: 4.505901
 43639/100000: episode: 774, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 23.053, mean reward: 2.305 [2.037, 2.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.805, 10.100], loss: 0.288632, mae: 0.493806, mean_q: 4.223886
 43660/100000: episode: 775, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 57.949, mean reward: 2.759 [1.949, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.059, 10.319], loss: 0.257764, mae: 0.443178, mean_q: 4.311919
 43675/100000: episode: 776, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 46.847, mean reward: 3.123 [2.261, 5.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.524], loss: 0.215017, mae: 0.414694, mean_q: 4.239875
 43689/100000: episode: 777, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 39.610, mean reward: 2.829 [2.195, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.146, 10.348], loss: 0.291984, mae: 0.444207, mean_q: 4.328275
 43710/100000: episode: 778, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 48.564, mean reward: 2.313 [1.791, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.500, 10.310], loss: 0.233131, mae: 0.411443, mean_q: 4.363359
 43750/100000: episode: 779, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 110.214, mean reward: 2.755 [1.980, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.334, 10.452], loss: 0.195831, mae: 0.400317, mean_q: 4.361492
 43777/100000: episode: 780, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 60.938, mean reward: 2.257 [1.736, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.353, 10.332], loss: 0.284947, mae: 0.421165, mean_q: 4.364142
 43792/100000: episode: 781, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 36.102, mean reward: 2.407 [2.003, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.357], loss: 0.272940, mae: 0.426178, mean_q: 4.395707
 43826/100000: episode: 782, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 87.119, mean reward: 2.562 [1.651, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.338, 10.293], loss: 12.484750, mae: 0.774524, mean_q: 4.527145
 43866/100000: episode: 783, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 105.204, mean reward: 2.630 [1.557, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.488, 10.245], loss: 0.228074, mae: 0.420010, mean_q: 4.404423
 43906/100000: episode: 784, duration: 0.213s, episode steps: 40, steps per second: 187, episode reward: 96.911, mean reward: 2.423 [1.651, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.594, 10.290], loss: 0.212112, mae: 0.402798, mean_q: 4.404427
 43946/100000: episode: 785, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 125.074, mean reward: 3.127 [1.824, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.208, 10.268], loss: 10.761290, mae: 0.728308, mean_q: 4.691674
 43973/100000: episode: 786, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 68.353, mean reward: 2.532 [1.961, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.467, 10.408], loss: 0.377984, mae: 0.536190, mean_q: 4.236038
 43978/100000: episode: 787, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 19.982, mean reward: 3.996 [3.314, 4.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.429, 10.100], loss: 0.217785, mae: 0.434277, mean_q: 4.535793
 44012/100000: episode: 788, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 95.204, mean reward: 2.800 [2.028, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.934, 10.418], loss: 12.458327, mae: 0.699100, mean_q: 4.624382
 44052/100000: episode: 789, duration: 0.217s, episode steps: 40, steps per second: 185, episode reward: 123.980, mean reward: 3.099 [1.728, 7.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.947, 10.288], loss: 0.353719, mae: 0.536811, mean_q: 4.390153
 44067/100000: episode: 790, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 42.078, mean reward: 2.805 [2.266, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.452], loss: 0.241939, mae: 0.438547, mean_q: 4.516699
 44082/100000: episode: 791, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 47.226, mean reward: 3.148 [2.181, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.491], loss: 0.193139, mae: 0.409564, mean_q: 4.405836
 44109/100000: episode: 792, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 76.970, mean reward: 2.851 [1.812, 5.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.236, 10.383], loss: 15.499583, mae: 0.845245, mean_q: 4.759829
 44136/100000: episode: 793, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 83.025, mean reward: 3.075 [1.887, 9.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.121, 10.331], loss: 0.295647, mae: 0.475601, mean_q: 4.330522
 44170/100000: episode: 794, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: 84.148, mean reward: 2.475 [1.915, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.153, 10.368], loss: 0.266118, mae: 0.466287, mean_q: 4.523933
 44204/100000: episode: 795, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 131.197, mean reward: 3.859 [2.695, 6.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.486], loss: 0.204215, mae: 0.408338, mean_q: 4.497966
 44218/100000: episode: 796, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 35.255, mean reward: 2.518 [2.113, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.443], loss: 0.208132, mae: 0.415287, mean_q: 4.417600
 44223/100000: episode: 797, duration: 0.042s, episode steps: 5, steps per second: 120, episode reward: 19.432, mean reward: 3.886 [3.400, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.374, 10.100], loss: 0.226426, mae: 0.391135, mean_q: 4.450943
 44238/100000: episode: 798, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 43.921, mean reward: 2.928 [2.232, 4.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.432], loss: 27.862270, mae: 1.147296, mean_q: 4.947486
 44278/100000: episode: 799, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 142.612, mean reward: 3.565 [1.536, 9.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.408, 10.219], loss: 0.411392, mae: 0.561346, mean_q: 4.427296
[Info] 2-TH LEVEL FOUND: 7.148224353790283, Considering 10/90 traces
 44293/100000: episode: 800, duration: 4.121s, episode steps: 15, steps per second: 4, episode reward: 41.771, mean reward: 2.785 [2.005, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.412], loss: 0.264602, mae: 0.466076, mean_q: 4.522664
 44330/100000: episode: 801, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 200.416, mean reward: 5.417 [2.118, 13.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.183, 10.542], loss: 0.228797, mae: 0.440158, mean_q: 4.557784
 44336/100000: episode: 802, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 18.396, mean reward: 3.066 [2.768, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.427], loss: 0.194664, mae: 0.418124, mean_q: 4.518071
 44373/100000: episode: 803, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 193.452, mean reward: 5.228 [2.777, 16.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.523, 10.413], loss: 0.232140, mae: 0.425348, mean_q: 4.593049
 44410/100000: episode: 804, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 140.616, mean reward: 3.800 [1.778, 6.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.156, 10.332], loss: 0.332771, mae: 0.471134, mean_q: 4.643672
 44422/100000: episode: 805, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 63.535, mean reward: 5.295 [4.506, 7.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.176, 10.553], loss: 33.454674, mae: 0.877751, mean_q: 4.604127
 44428/100000: episode: 806, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 20.544, mean reward: 3.424 [3.084, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.535], loss: 2.277216, mae: 1.311157, mean_q: 5.678593
 44434/100000: episode: 807, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 23.864, mean reward: 3.977 [2.774, 5.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.528], loss: 0.427554, mae: 0.592526, mean_q: 4.663528
 44471/100000: episode: 808, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 115.086, mean reward: 3.110 [1.845, 6.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.498, 10.100], loss: 11.831668, mae: 0.800182, mean_q: 4.740387
 44508/100000: episode: 809, duration: 0.206s, episode steps: 37, steps per second: 179, episode reward: 163.868, mean reward: 4.429 [2.153, 23.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.400, 10.447], loss: 0.296038, mae: 0.473285, mean_q: 4.729633
 44521/100000: episode: 810, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 80.701, mean reward: 6.208 [4.731, 8.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.361, 10.594], loss: 0.357054, mae: 0.498066, mean_q: 4.775851
 44558/100000: episode: 811, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 92.804, mean reward: 2.508 [1.576, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.501, 10.191], loss: 0.348636, mae: 0.461992, mean_q: 4.725761
 44595/100000: episode: 812, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 106.151, mean reward: 2.869 [2.433, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.102, 10.441], loss: 0.269416, mae: 0.442904, mean_q: 4.706340
 44632/100000: episode: 813, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 125.394, mean reward: 3.389 [1.770, 7.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.127, 10.257], loss: 0.252335, mae: 0.447832, mean_q: 4.794920
 44644/100000: episode: 814, duration: 0.080s, episode steps: 12, steps per second: 151, episode reward: 62.869, mean reward: 5.239 [4.063, 7.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.088, 10.622], loss: 0.206460, mae: 0.424861, mean_q: 4.811198
 44681/100000: episode: 815, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 169.692, mean reward: 4.586 [3.346, 6.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.575, 10.569], loss: 0.550285, mae: 0.509394, mean_q: 4.861674
 44709/100000: episode: 816, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 87.454, mean reward: 3.123 [1.836, 7.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.172, 10.328], loss: 0.248159, mae: 0.426872, mean_q: 4.707685
 44746/100000: episode: 817, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 108.704, mean reward: 2.938 [2.024, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.194, 10.377], loss: 0.314981, mae: 0.450524, mean_q: 4.887475
 44759/100000: episode: 818, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 88.939, mean reward: 6.841 [4.367, 9.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.669], loss: 0.267938, mae: 0.480734, mean_q: 5.007083
 44765/100000: episode: 819, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 22.266, mean reward: 3.711 [3.179, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.548], loss: 0.487610, mae: 0.524571, mean_q: 4.876174
 44802/100000: episode: 820, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 118.134, mean reward: 3.193 [1.674, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.562, 10.282], loss: 0.242146, mae: 0.463110, mean_q: 4.896463
 44814/100000: episode: 821, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 48.920, mean reward: 4.077 [3.241, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.390, 10.459], loss: 0.238806, mae: 0.460867, mean_q: 4.869359
 44826/100000: episode: 822, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 84.437, mean reward: 7.036 [4.868, 10.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.476, 10.566], loss: 0.300535, mae: 0.493601, mean_q: 4.951949
 44832/100000: episode: 823, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 26.011, mean reward: 4.335 [3.456, 7.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.478], loss: 0.343933, mae: 0.541390, mean_q: 5.282809
 44869/100000: episode: 824, duration: 0.185s, episode steps: 37, steps per second: 199, episode reward: 167.473, mean reward: 4.526 [1.682, 15.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.521, 10.306], loss: 0.321669, mae: 0.481141, mean_q: 4.948088
 44906/100000: episode: 825, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 105.626, mean reward: 2.855 [2.283, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.816, 10.456], loss: 0.322411, mae: 0.471563, mean_q: 4.910453
 44934/100000: episode: 826, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 92.424, mean reward: 3.301 [2.007, 7.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.173, 10.377], loss: 0.368663, mae: 0.491167, mean_q: 4.940554
 44946/100000: episode: 827, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 45.249, mean reward: 3.771 [2.648, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.217, 10.373], loss: 0.404287, mae: 0.529794, mean_q: 5.100118
 44956/100000: episode: 828, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 53.920, mean reward: 5.392 [3.228, 7.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.635], loss: 42.945919, mae: 1.750076, mean_q: 5.986949
 44968/100000: episode: 829, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 60.559, mean reward: 5.047 [4.245, 5.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.588], loss: 0.924299, mae: 0.781927, mean_q: 4.670767
 44973/100000: episode: 830, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 18.071, mean reward: 3.614 [3.122, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.064, 10.480], loss: 1.270760, mae: 0.861348, mean_q: 5.275371
 44983/100000: episode: 831, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 259.086, mean reward: 25.909 [4.185, 94.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.188, 10.678], loss: 1.006533, mae: 0.697217, mean_q: 5.381870
 44993/100000: episode: 832, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 40.195, mean reward: 4.019 [3.308, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.161, 10.496], loss: 0.437522, mae: 0.581863, mean_q: 5.014431
 45021/100000: episode: 833, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 101.863, mean reward: 3.638 [2.241, 5.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.179, 10.417], loss: 0.345494, mae: 0.522594, mean_q: 5.209672
 45033/100000: episode: 834, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 68.255, mean reward: 5.688 [3.911, 7.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-1.484, 10.585], loss: 10.581614, mae: 0.794825, mean_q: 5.371943
 45070/100000: episode: 835, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 159.651, mean reward: 4.315 [2.162, 11.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.015, 10.403], loss: 0.497213, mae: 0.622941, mean_q: 5.141210
 45075/100000: episode: 836, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 17.035, mean reward: 3.407 [2.878, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.447], loss: 0.460364, mae: 0.565003, mean_q: 5.287534
 45088/100000: episode: 837, duration: 0.070s, episode steps: 13, steps per second: 184, episode reward: 91.847, mean reward: 7.065 [4.338, 13.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-1.141, 10.590], loss: 0.423458, mae: 0.574679, mean_q: 5.239129
 45125/100000: episode: 838, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 148.153, mean reward: 4.004 [2.552, 21.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.125, 10.505], loss: 11.233560, mae: 0.639095, mean_q: 5.235720
 45162/100000: episode: 839, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 140.588, mean reward: 3.800 [2.020, 16.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.119, 10.364], loss: 0.867360, mae: 0.830213, mean_q: 5.216858
 45174/100000: episode: 840, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 65.020, mean reward: 5.418 [3.819, 8.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.552], loss: 0.604599, mae: 0.663563, mean_q: 5.383037
 45186/100000: episode: 841, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 92.980, mean reward: 7.748 [4.076, 12.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.650], loss: 0.329372, mae: 0.559438, mean_q: 5.375914
 45198/100000: episode: 842, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 51.165, mean reward: 4.264 [3.375, 5.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.509], loss: 0.918419, mae: 0.616774, mean_q: 5.178945
 45204/100000: episode: 843, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 22.666, mean reward: 3.778 [3.169, 4.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.479], loss: 20.649466, mae: 1.054672, mean_q: 5.448953
[Info] FALSIFICATION!
 45208/100000: episode: 844, duration: 0.181s, episode steps: 4, steps per second: 22, episode reward: 1034.163, mean reward: 258.541 [5.534, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.264, 10.256], loss: 1.092629, mae: 0.950228, mean_q: 6.035043
 45245/100000: episode: 845, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 204.495, mean reward: 5.527 [3.521, 10.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.129, 10.618], loss: 12.002831, mae: 0.976375, mean_q: 5.366050
 45282/100000: episode: 846, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 111.899, mean reward: 3.024 [1.695, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.666, 10.247], loss: 4.383824, mae: 0.905080, mean_q: 5.612362
 45319/100000: episode: 847, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 117.554, mean reward: 3.177 [1.952, 8.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.380, 10.376], loss: 0.809535, mae: 0.622925, mean_q: 5.519625
 45347/100000: episode: 848, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 83.386, mean reward: 2.978 [2.086, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.070, 10.379], loss: 5.460306, mae: 0.936113, mean_q: 5.640842
 45384/100000: episode: 849, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 114.450, mean reward: 3.093 [2.025, 6.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.358, 10.394], loss: 0.531359, mae: 0.610558, mean_q: 5.324333
 45421/100000: episode: 850, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 140.010, mean reward: 3.784 [2.394, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.699, 10.485], loss: 0.577456, mae: 0.564040, mean_q: 5.428623
 45427/100000: episode: 851, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 20.580, mean reward: 3.430 [3.206, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.468], loss: 12.297939, mae: 0.924568, mean_q: 5.369931
 45464/100000: episode: 852, duration: 0.197s, episode steps: 37, steps per second: 187, episode reward: 89.630, mean reward: 2.422 [1.524, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.868, 10.100], loss: 0.467706, mae: 0.587936, mean_q: 5.470369
 45469/100000: episode: 853, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 29.171, mean reward: 5.834 [4.558, 8.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.660], loss: 0.727020, mae: 0.642587, mean_q: 5.581377
 45479/100000: episode: 854, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 64.572, mean reward: 6.457 [3.902, 10.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.655], loss: 0.604750, mae: 0.652489, mean_q: 5.469760
 45516/100000: episode: 855, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 123.904, mean reward: 3.349 [1.759, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.753, 10.269], loss: 0.693265, mae: 0.653539, mean_q: 5.541151
 45544/100000: episode: 856, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 109.840, mean reward: 3.923 [2.667, 7.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.538], loss: 548.462585, mae: 3.213713, mean_q: 6.730690
 45557/100000: episode: 857, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 126.943, mean reward: 9.765 [5.050, 19.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.589], loss: 3.076048, mae: 1.659402, mean_q: 4.527987
 45594/100000: episode: 858, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 119.502, mean reward: 3.230 [2.412, 5.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.562, 10.418], loss: 8.231459, mae: 2.123634, mean_q: 6.223202
 45607/100000: episode: 859, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 45.345, mean reward: 3.488 [2.863, 5.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.341, 10.449], loss: 2.157897, mae: 1.322747, mean_q: 6.015987
 45617/100000: episode: 860, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 46.368, mean reward: 4.637 [3.440, 5.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.613], loss: 1.028183, mae: 1.037955, mean_q: 5.874246
 45654/100000: episode: 861, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 157.427, mean reward: 4.255 [2.525, 7.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.637, 10.411], loss: 420.708618, mae: 1.920426, mean_q: 5.595885
 45667/100000: episode: 862, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 94.749, mean reward: 7.288 [4.301, 9.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.653], loss: 8.100545, mae: 2.911081, mean_q: 8.290157
[Info] FALSIFICATION!
 45677/100000: episode: 863, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 1065.097, mean reward: 106.510 [5.022, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-1.094, 8.384], loss: 1.952528, mae: 1.149264, mean_q: 4.854718
 45690/100000: episode: 864, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 118.504, mean reward: 9.116 [4.473, 14.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.720], loss: 1.700935, mae: 1.068083, mean_q: 4.849069
 45727/100000: episode: 865, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 148.530, mean reward: 4.014 [2.331, 9.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.839, 10.419], loss: 417.535461, mae: 2.515081, mean_q: 6.673470
[Info] FALSIFICATION!
 45756/100000: episode: 866, duration: 0.403s, episode steps: 29, steps per second: 72, episode reward: 1414.763, mean reward: 48.785 [4.245, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.155, 10.616], loss: 4.350689, mae: 1.177051, mean_q: 5.686490
 45768/100000: episode: 867, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 73.904, mean reward: 6.159 [3.490, 8.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.642], loss: 1307.762207, mae: 5.848562, mean_q: 8.644780
 45780/100000: episode: 868, duration: 0.060s, episode steps: 12, steps per second: 202, episode reward: 39.826, mean reward: 3.319 [2.358, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.346], loss: 3.467236, mae: 1.681483, mean_q: 7.311654
 45792/100000: episode: 869, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 62.447, mean reward: 5.204 [3.896, 7.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.569], loss: 1.409144, mae: 1.019736, mean_q: 5.560812
 45804/100000: episode: 870, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 100.802, mean reward: 8.400 [4.672, 18.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.727], loss: 1.357189, mae: 0.918555, mean_q: 5.559602
 45814/100000: episode: 871, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 57.086, mean reward: 5.709 [4.693, 6.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.580], loss: 1.211502, mae: 0.905028, mean_q: 6.029423
 45826/100000: episode: 872, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 53.474, mean reward: 4.456 [2.902, 6.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.415], loss: 12.928425, mae: 1.326896, mean_q: 6.250741
 45838/100000: episode: 873, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 38.686, mean reward: 3.224 [2.377, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.262, 10.403], loss: 1.288028, mae: 1.125800, mean_q: 6.375931
 45875/100000: episode: 874, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 151.578, mean reward: 4.097 [2.138, 11.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.173, 10.451], loss: 6.959310, mae: 1.201936, mean_q: 6.532700
 45912/100000: episode: 875, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 115.835, mean reward: 3.131 [2.302, 5.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.299, 10.418], loss: 413.881439, mae: 2.756596, mean_q: 7.520613
 45924/100000: episode: 876, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 64.927, mean reward: 5.411 [4.447, 7.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.562], loss: 1.275455, mae: 1.140074, mean_q: 5.181605
[Info] FALSIFICATION!
 45936/100000: episode: 877, duration: 0.350s, episode steps: 12, steps per second: 34, episode reward: 1145.686, mean reward: 95.474 [3.749, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.751 [-0.637, 8.617], loss: 2.276038, mae: 1.139196, mean_q: 5.915014
 45942/100000: episode: 878, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 26.547, mean reward: 4.424 [3.503, 6.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.795, 10.609], loss: 4.226537, mae: 1.306669, mean_q: 6.521835
 45948/100000: episode: 879, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 24.852, mean reward: 4.142 [2.793, 6.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.439, 10.382], loss: 1.854402, mae: 1.258635, mean_q: 6.766295
 45960/100000: episode: 880, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 68.400, mean reward: 5.700 [2.530, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.551], loss: 1.217120, mae: 0.995301, mean_q: 6.348172
 45972/100000: episode: 881, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 65.647, mean reward: 5.471 [3.472, 9.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.554], loss: 1266.590942, mae: 5.159886, mean_q: 8.215810
 45982/100000: episode: 882, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 41.895, mean reward: 4.189 [3.199, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.577], loss: 8.121150, mae: 2.614319, mean_q: 9.002266
 45995/100000: episode: 883, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 119.518, mean reward: 9.194 [4.762, 23.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.633], loss: 3.018102, mae: 1.445577, mean_q: 5.734156
 46008/100000: episode: 884, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 82.445, mean reward: 6.342 [3.995, 12.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.463, 10.547], loss: 1182.570190, mae: 3.707384, mean_q: 5.568027
 46018/100000: episode: 885, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 45.641, mean reward: 4.564 [3.464, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.252, 10.626], loss: 9.851661, mae: 2.234394, mean_q: 7.957297
 46055/100000: episode: 886, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 128.942, mean reward: 3.485 [2.270, 5.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.606, 10.498], loss: 2.931878, mae: 1.411205, mean_q: 6.716142
 46083/100000: episode: 887, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 85.149, mean reward: 3.041 [1.599, 6.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.069, 10.249], loss: 4.618781, mae: 1.152858, mean_q: 6.447551
 46095/100000: episode: 888, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 47.350, mean reward: 3.946 [2.871, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.418], loss: 1265.631470, mae: 3.651258, mean_q: 6.587269
 46107/100000: episode: 889, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 57.939, mean reward: 4.828 [3.823, 6.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.544], loss: 6.450727, mae: 2.730216, mean_q: 9.319504
[Info] Complete ISplit Iteration
[Info] Levels: [6.007114, 7.1482244, 14.393145]
[Info] Cond. Prob: [0.1, 0.1, 0.11]
[Info] Error Prob: 0.0011000000000000003

 46144/100000: episode: 890, duration: 4.517s, episode steps: 37, steps per second: 8, episode reward: 226.670, mean reward: 6.126 [3.263, 12.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.476, 10.664], loss: 3.893450, mae: 1.160527, mean_q: 6.361802
 46244/100000: episode: 891, duration: 0.512s, episode steps: 100, steps per second: 196, episode reward: 212.070, mean reward: 2.121 [1.464, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.530, 10.300], loss: 156.312347, mae: 1.859378, mean_q: 7.223087
 46344/100000: episode: 892, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.989, mean reward: 1.940 [1.456, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.121, 10.371], loss: 613.892822, mae: 3.724909, mean_q: 7.709287
 46444/100000: episode: 893, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 204.833, mean reward: 2.048 [1.445, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.248], loss: 449.405365, mae: 2.999634, mean_q: 7.700655
 46544/100000: episode: 894, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 176.516, mean reward: 1.765 [1.437, 2.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.176], loss: 461.595123, mae: 2.917980, mean_q: 7.940997
 46644/100000: episode: 895, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 193.075, mean reward: 1.931 [1.459, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.585, 10.304], loss: 910.747559, mae: 4.900936, mean_q: 9.190235
 46744/100000: episode: 896, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.582, mean reward: 1.916 [1.458, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.300, 10.164], loss: 612.746399, mae: 3.790728, mean_q: 8.477842
 46844/100000: episode: 897, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.490, mean reward: 1.835 [1.446, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.218], loss: 598.792114, mae: 3.708426, mean_q: 8.729007
 46944/100000: episode: 898, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 173.751, mean reward: 1.738 [1.435, 2.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.541, 10.098], loss: 453.750183, mae: 3.235585, mean_q: 7.629604
 47044/100000: episode: 899, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 181.183, mean reward: 1.812 [1.442, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.212, 10.161], loss: 757.843384, mae: 4.341877, mean_q: 8.710075
 47144/100000: episode: 900, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.910, mean reward: 1.899 [1.449, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.237, 10.140], loss: 457.659882, mae: 3.428813, mean_q: 8.407972
 47244/100000: episode: 901, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.750, mean reward: 1.938 [1.469, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.778, 10.235], loss: 153.632034, mae: 1.609245, mean_q: 6.668925
 47344/100000: episode: 902, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.964, mean reward: 2.020 [1.458, 6.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.301, 10.098], loss: 4.575759, mae: 1.436317, mean_q: 6.872969
 47444/100000: episode: 903, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 184.479, mean reward: 1.845 [1.470, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.607, 10.194], loss: 449.339813, mae: 2.827242, mean_q: 7.484560
 47544/100000: episode: 904, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 178.536, mean reward: 1.785 [1.457, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.189], loss: 598.108582, mae: 3.301122, mean_q: 7.486997
 47644/100000: episode: 905, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.598, mean reward: 2.006 [1.479, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.098], loss: 152.950806, mae: 2.082616, mean_q: 7.091010
 47744/100000: episode: 906, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 203.658, mean reward: 2.037 [1.453, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.925, 10.098], loss: 308.169891, mae: 2.235389, mean_q: 6.982416
 47844/100000: episode: 907, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 204.009, mean reward: 2.040 [1.462, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.817, 10.359], loss: 598.400085, mae: 3.194384, mean_q: 7.415773
 47944/100000: episode: 908, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 175.138, mean reward: 1.751 [1.447, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.579, 10.195], loss: 154.506165, mae: 2.255664, mean_q: 7.048447
 48044/100000: episode: 909, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.908, mean reward: 1.869 [1.467, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.219, 10.282], loss: 453.312134, mae: 2.966837, mean_q: 7.416110
 48144/100000: episode: 910, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 245.312, mean reward: 2.453 [1.510, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.409, 10.098], loss: 5.742030, mae: 1.283405, mean_q: 6.099932
 48244/100000: episode: 911, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.702, mean reward: 1.937 [1.491, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.051, 10.098], loss: 307.347870, mae: 2.163103, mean_q: 6.879157
 48344/100000: episode: 912, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 201.605, mean reward: 2.016 [1.457, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.640, 10.323], loss: 595.787598, mae: 2.920601, mean_q: 6.918851
 48444/100000: episode: 913, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.452, mean reward: 1.905 [1.468, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.884, 10.098], loss: 302.032532, mae: 2.822610, mean_q: 7.146912
 48544/100000: episode: 914, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.214, mean reward: 1.872 [1.453, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.098], loss: 595.884033, mae: 3.445819, mean_q: 7.392709
 48644/100000: episode: 915, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.524, mean reward: 1.885 [1.439, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.599, 10.098], loss: 304.563995, mae: 2.377782, mean_q: 6.998311
 48744/100000: episode: 916, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.999, mean reward: 2.030 [1.459, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.572, 10.174], loss: 753.109070, mae: 4.060475, mean_q: 7.898357
 48844/100000: episode: 917, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 252.478, mean reward: 2.525 [1.492, 8.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.753, 10.098], loss: 449.778320, mae: 2.943504, mean_q: 7.302135
 48944/100000: episode: 918, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.015, mean reward: 1.920 [1.467, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.098], loss: 455.324463, mae: 2.846781, mean_q: 7.203604
 49044/100000: episode: 919, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.449, mean reward: 1.984 [1.467, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.789, 10.350], loss: 593.046997, mae: 3.584697, mean_q: 7.736197
 49144/100000: episode: 920, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 186.628, mean reward: 1.866 [1.456, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.462, 10.098], loss: 150.185577, mae: 1.385230, mean_q: 5.785305
 49244/100000: episode: 921, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.485, mean reward: 1.825 [1.446, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.198], loss: 152.750229, mae: 1.845840, mean_q: 6.595149
 49344/100000: episode: 922, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 205.727, mean reward: 2.057 [1.491, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.306, 10.386], loss: 150.115906, mae: 1.902506, mean_q: 6.508646
 49444/100000: episode: 923, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 189.221, mean reward: 1.892 [1.474, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.374, 10.271], loss: 304.089111, mae: 2.141760, mean_q: 6.258143
 49544/100000: episode: 924, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 224.480, mean reward: 2.245 [1.465, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.459, 10.359], loss: 5.579817, mae: 1.050669, mean_q: 5.669437
 49644/100000: episode: 925, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.956, mean reward: 1.940 [1.475, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.229, 10.296], loss: 441.027252, mae: 2.333358, mean_q: 6.338842
 49744/100000: episode: 926, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 196.402, mean reward: 1.964 [1.486, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.381, 10.098], loss: 298.067413, mae: 2.355127, mean_q: 6.649105
 49844/100000: episode: 927, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 201.020, mean reward: 2.010 [1.464, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.095, 10.328], loss: 157.931000, mae: 1.574250, mean_q: 5.849894
 49944/100000: episode: 928, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.065, mean reward: 1.921 [1.437, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.098], loss: 149.215607, mae: 1.338356, mean_q: 5.609445
 50044/100000: episode: 929, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 209.592, mean reward: 2.096 [1.457, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.614, 10.098], loss: 300.543488, mae: 1.997671, mean_q: 5.795004
 50144/100000: episode: 930, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.703, mean reward: 1.807 [1.457, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.495, 10.098], loss: 152.134979, mae: 1.243299, mean_q: 5.264236
 50244/100000: episode: 931, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.068, mean reward: 1.981 [1.448, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.330], loss: 2.245974, mae: 0.716716, mean_q: 4.939137
 50344/100000: episode: 932, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 188.997, mean reward: 1.890 [1.440, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.271, 10.159], loss: 438.932739, mae: 2.591374, mean_q: 5.992292
 50444/100000: episode: 933, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 180.290, mean reward: 1.803 [1.474, 2.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.824, 10.098], loss: 153.253326, mae: 1.216871, mean_q: 5.095912
 50544/100000: episode: 934, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 187.699, mean reward: 1.877 [1.452, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.325], loss: 453.248993, mae: 2.201657, mean_q: 5.427052
 50644/100000: episode: 935, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.776, mean reward: 1.888 [1.442, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.552, 10.098], loss: 150.538635, mae: 1.533842, mean_q: 5.144283
 50744/100000: episode: 936, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 234.460, mean reward: 2.345 [1.490, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.865, 10.098], loss: 151.792770, mae: 1.038855, mean_q: 4.697696
 50844/100000: episode: 937, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 181.625, mean reward: 1.816 [1.452, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.352, 10.098], loss: 0.793536, mae: 0.530613, mean_q: 4.331757
 50944/100000: episode: 938, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 185.840, mean reward: 1.858 [1.434, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.065, 10.383], loss: 0.554894, mae: 0.434485, mean_q: 4.088312
 51044/100000: episode: 939, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 247.682, mean reward: 2.477 [1.448, 10.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.741, 10.098], loss: 0.225394, mae: 0.371747, mean_q: 3.995207
 51144/100000: episode: 940, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 193.991, mean reward: 1.940 [1.448, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.336, 10.098], loss: 0.183071, mae: 0.347475, mean_q: 3.909133
 51244/100000: episode: 941, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 199.332, mean reward: 1.993 [1.444, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.701, 10.221], loss: 0.103702, mae: 0.309914, mean_q: 3.865362
 51344/100000: episode: 942, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.871, mean reward: 1.889 [1.442, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.689, 10.100], loss: 0.113097, mae: 0.320457, mean_q: 3.886976
 51444/100000: episode: 943, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 224.689, mean reward: 2.247 [1.494, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.442], loss: 0.098503, mae: 0.310016, mean_q: 3.873594
 51544/100000: episode: 944, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 215.116, mean reward: 2.151 [1.450, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.835, 10.416], loss: 0.110784, mae: 0.322900, mean_q: 3.893643
 51644/100000: episode: 945, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 205.913, mean reward: 2.059 [1.460, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.544, 10.197], loss: 0.109796, mae: 0.319883, mean_q: 3.894527
 51744/100000: episode: 946, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 195.031, mean reward: 1.950 [1.476, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.238, 10.135], loss: 0.124830, mae: 0.332454, mean_q: 3.897864
 51844/100000: episode: 947, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 214.025, mean reward: 2.140 [1.519, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.075, 10.098], loss: 0.115483, mae: 0.322049, mean_q: 3.901144
 51944/100000: episode: 948, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 189.666, mean reward: 1.897 [1.466, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.978, 10.098], loss: 0.112397, mae: 0.311452, mean_q: 3.900617
 52044/100000: episode: 949, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 198.358, mean reward: 1.984 [1.459, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.359, 10.098], loss: 0.115774, mae: 0.325034, mean_q: 3.942334
 52144/100000: episode: 950, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.413, mean reward: 1.994 [1.452, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.656, 10.147], loss: 0.095855, mae: 0.313790, mean_q: 3.929752
 52244/100000: episode: 951, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.944, mean reward: 1.879 [1.451, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.907, 10.098], loss: 0.124456, mae: 0.333746, mean_q: 3.936422
 52344/100000: episode: 952, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 210.797, mean reward: 2.108 [1.473, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.521, 10.098], loss: 0.120445, mae: 0.328970, mean_q: 3.962380
 52444/100000: episode: 953, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 206.227, mean reward: 2.062 [1.463, 4.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.314, 10.098], loss: 0.119361, mae: 0.327784, mean_q: 3.941664
 52544/100000: episode: 954, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 184.003, mean reward: 1.840 [1.456, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.499, 10.150], loss: 0.105641, mae: 0.322700, mean_q: 3.946836
 52644/100000: episode: 955, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.916, mean reward: 1.959 [1.483, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.104645, mae: 0.317663, mean_q: 3.944397
 52744/100000: episode: 956, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 195.097, mean reward: 1.951 [1.513, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.601, 10.098], loss: 0.106902, mae: 0.321385, mean_q: 3.950324
 52844/100000: episode: 957, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.817, mean reward: 1.988 [1.475, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.864, 10.098], loss: 0.103546, mae: 0.314686, mean_q: 3.939840
 52944/100000: episode: 958, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.934, mean reward: 1.879 [1.479, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.283, 10.207], loss: 0.105843, mae: 0.315353, mean_q: 3.950454
 53044/100000: episode: 959, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.670, mean reward: 1.847 [1.454, 6.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.163], loss: 0.110591, mae: 0.320513, mean_q: 3.957921
 53144/100000: episode: 960, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.480, mean reward: 1.935 [1.485, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.073, 10.098], loss: 0.111262, mae: 0.327833, mean_q: 3.930123
 53244/100000: episode: 961, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 199.044, mean reward: 1.990 [1.469, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.289, 10.153], loss: 0.110890, mae: 0.318955, mean_q: 3.913448
 53344/100000: episode: 962, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.146, mean reward: 1.861 [1.447, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.562, 10.108], loss: 0.108014, mae: 0.312869, mean_q: 3.939685
 53444/100000: episode: 963, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.242, mean reward: 1.852 [1.443, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.044, 10.236], loss: 0.137883, mae: 0.333357, mean_q: 3.955206
 53544/100000: episode: 964, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.192, mean reward: 1.882 [1.449, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.688, 10.214], loss: 0.132150, mae: 0.334670, mean_q: 3.945366
 53644/100000: episode: 965, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.299, mean reward: 1.933 [1.477, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.533, 10.226], loss: 0.105499, mae: 0.314887, mean_q: 3.955269
 53744/100000: episode: 966, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.672, mean reward: 2.007 [1.448, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.098], loss: 0.107138, mae: 0.312395, mean_q: 3.944130
 53844/100000: episode: 967, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.241, mean reward: 1.942 [1.449, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.641, 10.131], loss: 0.104297, mae: 0.314146, mean_q: 3.919550
 53944/100000: episode: 968, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.195, mean reward: 1.912 [1.458, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.794, 10.098], loss: 0.112038, mae: 0.314934, mean_q: 3.911445
 54044/100000: episode: 969, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 202.657, mean reward: 2.027 [1.445, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.829, 10.098], loss: 0.097455, mae: 0.303697, mean_q: 3.890716
 54144/100000: episode: 970, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.828, mean reward: 1.898 [1.481, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.761, 10.279], loss: 0.104404, mae: 0.305095, mean_q: 3.901608
 54244/100000: episode: 971, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 191.804, mean reward: 1.918 [1.465, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.023, 10.098], loss: 0.093180, mae: 0.303613, mean_q: 3.895656
 54344/100000: episode: 972, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.608, mean reward: 1.856 [1.482, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.819, 10.231], loss: 0.088193, mae: 0.294690, mean_q: 3.904647
 54444/100000: episode: 973, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.577, mean reward: 1.916 [1.481, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.198, 10.098], loss: 0.088840, mae: 0.296408, mean_q: 3.887960
 54544/100000: episode: 974, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 261.042, mean reward: 2.610 [1.474, 5.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.999, 10.463], loss: 0.092165, mae: 0.302883, mean_q: 3.886807
 54644/100000: episode: 975, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.903, mean reward: 1.899 [1.455, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.692, 10.098], loss: 0.128136, mae: 0.330706, mean_q: 3.920339
 54744/100000: episode: 976, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 208.077, mean reward: 2.081 [1.452, 7.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.023, 10.098], loss: 0.094311, mae: 0.297589, mean_q: 3.906683
 54844/100000: episode: 977, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.076, mean reward: 1.991 [1.437, 5.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.219], loss: 0.113838, mae: 0.312185, mean_q: 3.920827
 54944/100000: episode: 978, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 199.405, mean reward: 1.994 [1.460, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.020, 10.098], loss: 0.105619, mae: 0.314743, mean_q: 3.936991
 55044/100000: episode: 979, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.647, mean reward: 1.866 [1.458, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.575, 10.098], loss: 0.111698, mae: 0.313385, mean_q: 3.909006
 55144/100000: episode: 980, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 207.076, mean reward: 2.071 [1.482, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.098], loss: 0.094866, mae: 0.303365, mean_q: 3.918395
 55244/100000: episode: 981, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 183.083, mean reward: 1.831 [1.436, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.641, 10.098], loss: 0.098408, mae: 0.305802, mean_q: 3.905951
 55344/100000: episode: 982, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.241, mean reward: 1.912 [1.434, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.501, 10.098], loss: 0.097421, mae: 0.310333, mean_q: 3.914587
 55444/100000: episode: 983, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.329, mean reward: 1.993 [1.519, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.299], loss: 0.095903, mae: 0.307214, mean_q: 3.930940
 55544/100000: episode: 984, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 197.503, mean reward: 1.975 [1.514, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.789, 10.098], loss: 0.096570, mae: 0.309301, mean_q: 3.899893
 55644/100000: episode: 985, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 198.859, mean reward: 1.989 [1.480, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.502, 10.223], loss: 0.130204, mae: 0.328194, mean_q: 3.955440
 55744/100000: episode: 986, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.335, mean reward: 1.923 [1.474, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.818, 10.098], loss: 0.107059, mae: 0.297553, mean_q: 3.905892
 55844/100000: episode: 987, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.716, mean reward: 1.887 [1.449, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.706, 10.098], loss: 0.093608, mae: 0.310256, mean_q: 3.928097
 55944/100000: episode: 988, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.146, mean reward: 1.931 [1.476, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.112192, mae: 0.313966, mean_q: 3.938928
 56044/100000: episode: 989, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 188.628, mean reward: 1.886 [1.464, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.127, 10.190], loss: 0.084266, mae: 0.291201, mean_q: 3.895662
[Info] 1-TH LEVEL FOUND: 5.945566654205322, Considering 10/90 traces
 56144/100000: episode: 990, duration: 4.599s, episode steps: 100, steps per second: 22, episode reward: 212.674, mean reward: 2.127 [1.475, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.223, 10.098], loss: 0.093028, mae: 0.302864, mean_q: 3.906969
 56166/100000: episode: 991, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 46.083, mean reward: 2.095 [1.731, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.238, 10.233], loss: 0.118536, mae: 0.323079, mean_q: 3.952136
 56186/100000: episode: 992, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 89.021, mean reward: 4.451 [2.860, 8.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.507, 10.100], loss: 0.088369, mae: 0.296364, mean_q: 3.924282
 56221/100000: episode: 993, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 115.594, mean reward: 3.303 [1.930, 10.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.933, 10.304], loss: 0.101695, mae: 0.303561, mean_q: 3.934554
 56256/100000: episode: 994, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 87.258, mean reward: 2.493 [1.901, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.473, 10.260], loss: 0.103246, mae: 0.317413, mean_q: 3.965876
 56296/100000: episode: 995, duration: 0.229s, episode steps: 40, steps per second: 174, episode reward: 123.025, mean reward: 3.076 [2.051, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.269, 10.469], loss: 0.099632, mae: 0.315434, mean_q: 3.970124
 56321/100000: episode: 996, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 121.906, mean reward: 4.876 [1.842, 13.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.191, 10.100], loss: 0.134739, mae: 0.322835, mean_q: 4.016256
 56346/100000: episode: 997, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 127.690, mean reward: 5.108 [1.720, 11.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.496, 10.100], loss: 0.117260, mae: 0.308910, mean_q: 3.911778
 56365/100000: episode: 998, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 43.159, mean reward: 2.272 [1.834, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.551, 10.309], loss: 0.103994, mae: 0.327292, mean_q: 3.994111
 56387/100000: episode: 999, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 53.005, mean reward: 2.409 [1.802, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.518, 10.260], loss: 0.148335, mae: 0.352351, mean_q: 4.070217
 56406/100000: episode: 1000, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 49.315, mean reward: 2.596 [2.033, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.848, 10.470], loss: 0.087866, mae: 0.292234, mean_q: 3.954361
 56441/100000: episode: 1001, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 78.181, mean reward: 2.234 [1.684, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.692, 10.261], loss: 0.134104, mae: 0.333962, mean_q: 4.015205
 56481/100000: episode: 1002, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 124.690, mean reward: 3.117 [2.278, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.427, 10.403], loss: 0.133026, mae: 0.328833, mean_q: 3.993502
 56521/100000: episode: 1003, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 110.672, mean reward: 2.767 [1.997, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.076, 10.360], loss: 0.171807, mae: 0.347940, mean_q: 4.066020
 56547/100000: episode: 1004, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 93.662, mean reward: 3.602 [2.243, 6.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.411, 10.100], loss: 0.251733, mae: 0.383114, mean_q: 4.061443
 56572/100000: episode: 1005, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 72.570, mean reward: 2.903 [1.968, 8.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.401, 10.100], loss: 0.129473, mae: 0.320691, mean_q: 4.016350
 56586/100000: episode: 1006, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 37.249, mean reward: 2.661 [1.974, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.256, 10.339], loss: 0.258083, mae: 0.380287, mean_q: 4.166543
 56600/100000: episode: 1007, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 30.387, mean reward: 2.171 [1.929, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.371, 10.346], loss: 0.232912, mae: 0.380267, mean_q: 4.088421
 56619/100000: episode: 1008, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 69.213, mean reward: 3.643 [2.067, 5.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.807, 10.611], loss: 0.121312, mae: 0.334088, mean_q: 4.099717
 56638/100000: episode: 1009, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 37.546, mean reward: 1.976 [1.659, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.755, 10.230], loss: 0.253523, mae: 0.423468, mean_q: 4.172492
 56647/100000: episode: 1010, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 30.315, mean reward: 3.368 [3.014, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.509], loss: 0.129017, mae: 0.349575, mean_q: 3.922549
 56672/100000: episode: 1011, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 67.843, mean reward: 2.714 [1.927, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.075, 10.100], loss: 0.135710, mae: 0.347170, mean_q: 4.084592
 56692/100000: episode: 1012, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 44.757, mean reward: 2.238 [1.830, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.194, 10.100], loss: 0.136623, mae: 0.343956, mean_q: 4.110918
 56732/100000: episode: 1013, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 80.346, mean reward: 2.009 [1.513, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.061, 10.209], loss: 0.250308, mae: 0.380109, mean_q: 4.141629
 56757/100000: episode: 1014, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 88.185, mean reward: 3.527 [1.814, 6.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.560, 10.100], loss: 0.215870, mae: 0.367716, mean_q: 4.126991
 56781/100000: episode: 1015, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 66.345, mean reward: 2.764 [1.896, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.932, 10.357], loss: 0.152451, mae: 0.365830, mean_q: 4.099226
 56807/100000: episode: 1016, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 80.664, mean reward: 3.102 [1.600, 5.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.582, 10.100], loss: 0.123713, mae: 0.324497, mean_q: 4.066776
 56842/100000: episode: 1017, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 73.389, mean reward: 2.097 [1.851, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.172, 10.246], loss: 0.166434, mae: 0.362551, mean_q: 4.114823
 56851/100000: episode: 1018, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 23.692, mean reward: 2.632 [2.429, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.348], loss: 0.123087, mae: 0.316326, mean_q: 3.871011
 56877/100000: episode: 1019, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 64.693, mean reward: 2.488 [1.761, 4.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.300, 10.100], loss: 0.190424, mae: 0.361390, mean_q: 4.178852
 56896/100000: episode: 1020, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 43.519, mean reward: 2.290 [1.854, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.419], loss: 0.150175, mae: 0.356573, mean_q: 4.081467
 56920/100000: episode: 1021, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 64.571, mean reward: 2.690 [2.031, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.359], loss: 0.160171, mae: 0.360057, mean_q: 4.080835
 56942/100000: episode: 1022, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 53.995, mean reward: 2.454 [1.822, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.448, 10.346], loss: 0.158320, mae: 0.369503, mean_q: 4.205969
 56977/100000: episode: 1023, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 85.115, mean reward: 2.432 [1.796, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.164, 10.334], loss: 0.196335, mae: 0.382622, mean_q: 4.218442
 56997/100000: episode: 1024, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 69.868, mean reward: 3.493 [1.984, 9.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.543, 10.100], loss: 0.194065, mae: 0.379058, mean_q: 4.139239
 57006/100000: episode: 1025, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 31.906, mean reward: 3.545 [3.101, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.479], loss: 0.155038, mae: 0.330769, mean_q: 4.154842
 57026/100000: episode: 1026, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 52.726, mean reward: 2.636 [2.166, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.401, 10.100], loss: 0.170988, mae: 0.359206, mean_q: 4.132663
 57035/100000: episode: 1027, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 27.406, mean reward: 3.045 [2.483, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.685, 10.415], loss: 0.241276, mae: 0.398822, mean_q: 4.237164
 57059/100000: episode: 1028, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 73.204, mean reward: 3.050 [2.123, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.086, 10.429], loss: 0.214939, mae: 0.376079, mean_q: 4.123249
 57084/100000: episode: 1029, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 70.578, mean reward: 2.823 [1.933, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.872, 10.100], loss: 0.193335, mae: 0.384703, mean_q: 4.103651
 57104/100000: episode: 1030, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 59.438, mean reward: 2.972 [2.275, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.164, 10.100], loss: 0.188806, mae: 0.398157, mean_q: 4.256303
 57126/100000: episode: 1031, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 70.738, mean reward: 3.215 [1.908, 6.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.508], loss: 0.234302, mae: 0.386742, mean_q: 4.224194
 57150/100000: episode: 1032, duration: 0.120s, episode steps: 24, steps per second: 199, episode reward: 93.564, mean reward: 3.899 [2.172, 7.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.618], loss: 0.247339, mae: 0.421795, mean_q: 4.286720
 57170/100000: episode: 1033, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 70.870, mean reward: 3.543 [2.522, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.438, 10.100], loss: 0.156087, mae: 0.373440, mean_q: 4.174742
 57192/100000: episode: 1034, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 52.170, mean reward: 2.371 [1.885, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.334], loss: 0.228552, mae: 0.397158, mean_q: 4.279611
 57227/100000: episode: 1035, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 86.115, mean reward: 2.460 [1.770, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.702, 10.307], loss: 0.220907, mae: 0.392308, mean_q: 4.256732
 57249/100000: episode: 1036, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 77.915, mean reward: 3.542 [2.232, 6.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.454, 10.513], loss: 0.197030, mae: 0.390661, mean_q: 4.206490
 57275/100000: episode: 1037, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 89.465, mean reward: 3.441 [2.273, 5.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.255, 10.100], loss: 0.213062, mae: 0.387039, mean_q: 4.271881
 57299/100000: episode: 1038, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 67.445, mean reward: 2.810 [1.955, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.592, 10.500], loss: 0.250769, mae: 0.391377, mean_q: 4.322139
 57339/100000: episode: 1039, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 104.348, mean reward: 2.609 [1.550, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.920, 10.148], loss: 0.246559, mae: 0.404775, mean_q: 4.353843
 57348/100000: episode: 1040, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 29.313, mean reward: 3.257 [2.439, 5.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.573], loss: 0.251064, mae: 0.392395, mean_q: 4.319522
 57357/100000: episode: 1041, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 36.147, mean reward: 4.016 [3.109, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.544], loss: 0.156596, mae: 0.357003, mean_q: 4.188664
 57379/100000: episode: 1042, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 45.110, mean reward: 2.050 [1.464, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.562, 10.100], loss: 0.205446, mae: 0.407270, mean_q: 4.316979
 57414/100000: episode: 1043, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: 97.115, mean reward: 2.775 [1.607, 6.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.954, 10.215], loss: 0.220336, mae: 0.391671, mean_q: 4.322952
 57434/100000: episode: 1044, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 72.195, mean reward: 3.610 [2.267, 5.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.508, 10.100], loss: 0.136967, mae: 0.350006, mean_q: 4.269361
 57474/100000: episode: 1045, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 87.026, mean reward: 2.176 [1.612, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.765, 10.198], loss: 0.262034, mae: 0.414794, mean_q: 4.354920
 57498/100000: episode: 1046, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 81.190, mean reward: 3.383 [2.011, 9.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.557, 10.401], loss: 0.190527, mae: 0.418778, mean_q: 4.372379
 57523/100000: episode: 1047, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 82.093, mean reward: 3.284 [2.109, 5.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.708, 10.100], loss: 0.210753, mae: 0.384143, mean_q: 4.334633
 57548/100000: episode: 1048, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 170.219, mean reward: 6.809 [1.949, 55.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.528, 10.100], loss: 0.284844, mae: 0.453075, mean_q: 4.394796
 57572/100000: episode: 1049, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 59.103, mean reward: 2.463 [1.871, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.650, 10.297], loss: 0.203047, mae: 0.372476, mean_q: 4.291045
 57597/100000: episode: 1050, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 83.373, mean reward: 3.335 [1.797, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.382, 10.100], loss: 0.180282, mae: 0.407435, mean_q: 4.324111
 57617/100000: episode: 1051, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 50.146, mean reward: 2.507 [2.044, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.471, 10.100], loss: 0.270015, mae: 0.421007, mean_q: 4.361304
 57643/100000: episode: 1052, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 78.318, mean reward: 3.012 [2.216, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.247, 10.100], loss: 0.236039, mae: 0.418177, mean_q: 4.468400
 57657/100000: episode: 1053, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 40.852, mean reward: 2.918 [2.271, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.677, 10.420], loss: 3.117196, mae: 0.512662, mean_q: 4.275611
 57682/100000: episode: 1054, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 68.862, mean reward: 2.754 [1.721, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.259, 10.100], loss: 0.455941, mae: 0.581828, mean_q: 4.560218
 57722/100000: episode: 1055, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 92.490, mean reward: 2.312 [1.736, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-1.031, 10.305], loss: 0.246323, mae: 0.428888, mean_q: 4.431939
 57736/100000: episode: 1056, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 38.064, mean reward: 2.719 [2.232, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.465], loss: 0.244075, mae: 0.438425, mean_q: 4.527472
 57750/100000: episode: 1057, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 29.540, mean reward: 2.110 [1.774, 2.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.490, 10.268], loss: 0.207276, mae: 0.405807, mean_q: 4.460794
 57775/100000: episode: 1058, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 77.183, mean reward: 3.087 [1.891, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.586, 10.100], loss: 2.024377, mae: 0.628498, mean_q: 4.538115
 57795/100000: episode: 1059, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 66.444, mean reward: 3.322 [2.270, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.381, 10.100], loss: 0.284882, mae: 0.453846, mean_q: 4.579425
 57821/100000: episode: 1060, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 81.979, mean reward: 3.153 [1.676, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.386, 10.100], loss: 0.229380, mae: 0.418410, mean_q: 4.437717
 57845/100000: episode: 1061, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 108.349, mean reward: 4.515 [2.450, 13.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.151, 10.600], loss: 0.270811, mae: 0.440257, mean_q: 4.513223
 57865/100000: episode: 1062, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 61.763, mean reward: 3.088 [2.378, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.419, 10.100], loss: 2.365613, mae: 0.602786, mean_q: 4.692981
 57879/100000: episode: 1063, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 37.043, mean reward: 2.646 [2.020, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.250], loss: 0.154467, mae: 0.380091, mean_q: 4.308069
 57898/100000: episode: 1064, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 45.188, mean reward: 2.378 [1.965, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.548, 10.336], loss: 0.202826, mae: 0.440344, mean_q: 4.623977
 57922/100000: episode: 1065, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 48.837, mean reward: 2.035 [1.626, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.510, 10.261], loss: 0.316865, mae: 0.439016, mean_q: 4.497512
 57947/100000: episode: 1066, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 167.121, mean reward: 6.685 [1.997, 22.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.787, 10.100], loss: 0.346601, mae: 0.430238, mean_q: 4.526774
 57956/100000: episode: 1067, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 22.922, mean reward: 2.547 [2.219, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.409], loss: 0.150806, mae: 0.366140, mean_q: 4.484700
 57991/100000: episode: 1068, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 105.401, mean reward: 3.011 [1.977, 7.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.678, 10.413], loss: 0.198698, mae: 0.399822, mean_q: 4.524267
 58016/100000: episode: 1069, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 83.807, mean reward: 3.352 [2.228, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.012, 10.100], loss: 0.436263, mae: 0.480657, mean_q: 4.661658
 58051/100000: episode: 1070, duration: 0.187s, episode steps: 35, steps per second: 188, episode reward: 82.055, mean reward: 2.344 [1.877, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.490, 10.308], loss: 0.281260, mae: 0.433695, mean_q: 4.545830
 58076/100000: episode: 1071, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 67.448, mean reward: 2.698 [1.748, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.444, 10.100], loss: 0.370198, mae: 0.460362, mean_q: 4.531265
 58096/100000: episode: 1072, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 70.843, mean reward: 3.542 [2.270, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.855, 10.100], loss: 4.300260, mae: 0.695441, mean_q: 4.789472
 58120/100000: episode: 1073, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 60.690, mean reward: 2.529 [1.698, 7.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.840, 10.261], loss: 0.407590, mae: 0.497914, mean_q: 4.703614
 58142/100000: episode: 1074, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 53.615, mean reward: 2.437 [1.966, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-2.165, 10.321], loss: 0.622484, mae: 0.516632, mean_q: 4.687363
 58168/100000: episode: 1075, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 83.228, mean reward: 3.201 [1.978, 6.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.249, 10.100], loss: 0.217752, mae: 0.431889, mean_q: 4.632023
 58177/100000: episode: 1076, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 26.985, mean reward: 2.998 [2.421, 4.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.637, 10.345], loss: 0.235441, mae: 0.429619, mean_q: 4.707094
 58217/100000: episode: 1077, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 98.768, mean reward: 2.469 [1.754, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.192, 10.230], loss: 0.276163, mae: 0.473097, mean_q: 4.709510
 58236/100000: episode: 1078, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 79.904, mean reward: 4.205 [1.793, 9.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.636], loss: 0.369464, mae: 0.509837, mean_q: 4.688373
 58245/100000: episode: 1079, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 27.414, mean reward: 3.046 [2.787, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.455], loss: 0.189380, mae: 0.403046, mean_q: 4.652918
[Info] 2-TH LEVEL FOUND: 8.764392852783203, Considering 10/90 traces
 58285/100000: episode: 1080, duration: 4.271s, episode steps: 40, steps per second: 9, episode reward: 102.093, mean reward: 2.552 [1.932, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.513, 10.357], loss: 0.259746, mae: 0.437788, mean_q: 4.665704
 58293/100000: episode: 1081, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 32.750, mean reward: 4.094 [2.711, 6.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.454, 10.100], loss: 0.205798, mae: 0.444909, mean_q: 4.724151
 58305/100000: episode: 1082, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 57.282, mean reward: 4.774 [3.565, 8.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.854, 10.100], loss: 0.737062, mae: 0.483926, mean_q: 4.552761
 58313/100000: episode: 1083, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 24.818, mean reward: 3.102 [2.591, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.489, 10.100], loss: 0.450206, mae: 0.518182, mean_q: 4.865999
 58331/100000: episode: 1084, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 103.297, mean reward: 5.739 [3.111, 11.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.471, 10.100], loss: 0.251306, mae: 0.445314, mean_q: 4.682611
 58337/100000: episode: 1085, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 56.928, mean reward: 9.488 [4.304, 24.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.424, 10.100], loss: 0.226604, mae: 0.446263, mean_q: 4.910619
 58346/100000: episode: 1086, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 30.423, mean reward: 3.380 [2.628, 5.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.170, 10.100], loss: 0.291921, mae: 0.397230, mean_q: 4.512916
 58364/100000: episode: 1087, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 71.717, mean reward: 3.984 [2.777, 6.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.438, 10.100], loss: 0.394541, mae: 0.527640, mean_q: 4.937803
 58374/100000: episode: 1088, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 70.487, mean reward: 7.049 [4.691, 15.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.631, 10.100], loss: 0.335949, mae: 0.491720, mean_q: 4.620892
 58386/100000: episode: 1089, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 109.604, mean reward: 9.134 [5.352, 18.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.995, 10.100], loss: 0.594264, mae: 0.554214, mean_q: 4.794751
 58392/100000: episode: 1090, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 52.167, mean reward: 8.695 [5.337, 17.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.298, 10.100], loss: 0.568840, mae: 0.646322, mean_q: 5.148147
 58398/100000: episode: 1091, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 46.662, mean reward: 7.777 [5.491, 12.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.410, 10.100], loss: 0.474442, mae: 0.505149, mean_q: 4.729915
 58410/100000: episode: 1092, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 63.989, mean reward: 5.332 [3.649, 8.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.351, 10.100], loss: 0.503993, mae: 0.502192, mean_q: 4.738188
 58419/100000: episode: 1093, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 38.251, mean reward: 4.250 [3.100, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.430, 10.100], loss: 4.825180, mae: 0.692463, mean_q: 4.861132
 58429/100000: episode: 1094, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 39.193, mean reward: 3.919 [2.638, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.108, 10.100], loss: 0.910403, mae: 0.655385, mean_q: 4.991356
 58447/100000: episode: 1095, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 128.975, mean reward: 7.165 [2.875, 12.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.751, 10.100], loss: 0.928655, mae: 0.608826, mean_q: 4.986430
 58456/100000: episode: 1096, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 36.722, mean reward: 4.080 [3.116, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.368, 10.100], loss: 4.715425, mae: 0.834716, mean_q: 5.214788
 58462/100000: episode: 1097, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 36.606, mean reward: 6.101 [4.809, 9.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.434, 10.100], loss: 0.674733, mae: 0.604452, mean_q: 4.626574
 58470/100000: episode: 1098, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 29.750, mean reward: 3.719 [2.731, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.520, 10.100], loss: 0.478422, mae: 0.552163, mean_q: 4.602534
[Info] FALSIFICATION!
 58480/100000: episode: 1099, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 1174.190, mean reward: 117.419 [5.640, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.426, 10.093], loss: 0.398652, mae: 0.534208, mean_q: 4.861828
 58495/100000: episode: 1100, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 120.512, mean reward: 8.034 [4.319, 18.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.489, 10.100], loss: 0.986285, mae: 0.652032, mean_q: 5.048193
 58510/100000: episode: 1101, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 97.688, mean reward: 6.513 [4.834, 7.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.493, 10.100], loss: 0.402366, mae: 0.523022, mean_q: 4.957091
 58518/100000: episode: 1102, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 26.240, mean reward: 3.280 [2.741, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.408, 10.100], loss: 0.732377, mae: 0.622676, mean_q: 5.140703
 58530/100000: episode: 1103, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 39.316, mean reward: 3.276 [2.596, 5.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.808, 10.100], loss: 0.507395, mae: 0.541449, mean_q: 4.857371
 58538/100000: episode: 1104, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 28.163, mean reward: 3.520 [2.648, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.792, 10.100], loss: 0.379274, mae: 0.494433, mean_q: 4.783334
 58548/100000: episode: 1105, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 44.063, mean reward: 4.406 [3.068, 6.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.402, 10.100], loss: 1532.482666, mae: 4.830086, mean_q: 6.482229
 58557/100000: episode: 1106, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 29.720, mean reward: 3.302 [3.116, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.345, 10.100], loss: 8.980496, mae: 2.702189, mean_q: 7.556410
 58575/100000: episode: 1107, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 92.122, mean reward: 5.118 [3.187, 7.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.393, 10.100], loss: 3.457631, mae: 0.949687, mean_q: 4.940194
 58585/100000: episode: 1108, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 69.927, mean reward: 6.993 [4.589, 10.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.347, 10.100], loss: 0.846684, mae: 0.729088, mean_q: 4.906748
 58593/100000: episode: 1109, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 22.562, mean reward: 2.820 [2.431, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.527, 10.100], loss: 0.981517, mae: 0.791396, mean_q: 5.158890
 58599/100000: episode: 1110, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 47.161, mean reward: 7.860 [4.340, 10.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.380, 10.100], loss: 0.891494, mae: 0.703963, mean_q: 5.264909
 58609/100000: episode: 1111, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 40.990, mean reward: 4.099 [3.063, 5.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.371, 10.100], loss: 1.169295, mae: 0.672002, mean_q: 5.111288
 58621/100000: episode: 1112, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 104.079, mean reward: 8.673 [5.405, 13.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.484, 10.100], loss: 0.909904, mae: 0.744497, mean_q: 5.026743
 58630/100000: episode: 1113, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 36.088, mean reward: 4.010 [3.333, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.289, 10.100], loss: 0.782188, mae: 0.750988, mean_q: 5.286543
 58639/100000: episode: 1114, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 43.156, mean reward: 4.795 [3.428, 6.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.783, 10.100], loss: 17.536566, mae: 1.046178, mean_q: 5.357124
 58651/100000: episode: 1115, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 75.003, mean reward: 6.250 [4.256, 8.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.567, 10.100], loss: 0.921192, mae: 0.779040, mean_q: 5.520686
 58660/100000: episode: 1116, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 42.591, mean reward: 4.732 [3.749, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.602, 10.100], loss: 0.613152, mae: 0.666051, mean_q: 5.117730
 58670/100000: episode: 1117, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 44.911, mean reward: 4.491 [3.444, 6.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.467, 10.100], loss: 1.474016, mae: 0.874710, mean_q: 5.587220
 58685/100000: episode: 1118, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 81.679, mean reward: 5.445 [3.825, 8.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.494, 10.100], loss: 5.899966, mae: 0.837517, mean_q: 5.237997
 58693/100000: episode: 1119, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 28.478, mean reward: 3.560 [2.745, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.360, 10.100], loss: 1.034533, mae: 0.831962, mean_q: 5.530914
 58699/100000: episode: 1120, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 38.617, mean reward: 6.436 [5.248, 8.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.487, 10.100], loss: 0.585771, mae: 0.615924, mean_q: 5.390044
 58708/100000: episode: 1121, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 32.701, mean reward: 3.633 [3.180, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.445, 10.100], loss: 0.881554, mae: 0.714483, mean_q: 5.199209
 58720/100000: episode: 1122, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 60.209, mean reward: 5.017 [4.061, 7.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.430, 10.100], loss: 0.563092, mae: 0.653641, mean_q: 5.302312
 58735/100000: episode: 1123, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 95.459, mean reward: 6.364 [4.767, 9.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.790, 10.100], loss: 11.375661, mae: 0.957701, mean_q: 5.342226
 58741/100000: episode: 1124, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 43.015, mean reward: 7.169 [3.916, 13.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.414, 10.100], loss: 25.737379, mae: 1.352764, mean_q: 5.533011
 58759/100000: episode: 1125, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 63.310, mean reward: 3.517 [3.071, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.464, 10.100], loss: 1.047813, mae: 0.825914, mean_q: 5.616949
 58769/100000: episode: 1126, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 58.614, mean reward: 5.861 [4.869, 7.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.412, 10.100], loss: 1.923479, mae: 0.833153, mean_q: 5.366325
 58781/100000: episode: 1127, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 61.671, mean reward: 5.139 [2.963, 9.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.520, 10.100], loss: 1.450252, mae: 0.753327, mean_q: 5.124181
 58789/100000: episode: 1128, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 26.762, mean reward: 3.345 [3.024, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.372, 10.100], loss: 1.124951, mae: 0.795520, mean_q: 5.391905
 58797/100000: episode: 1129, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 30.935, mean reward: 3.867 [2.928, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.414, 10.100], loss: 1.122107, mae: 0.801666, mean_q: 5.383752
 58809/100000: episode: 1130, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 45.589, mean reward: 3.799 [2.907, 5.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.414, 10.100], loss: 0.809315, mae: 0.750566, mean_q: 5.634487
 58819/100000: episode: 1131, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 58.518, mean reward: 5.852 [4.452, 8.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.726, 10.100], loss: 0.853384, mae: 0.675875, mean_q: 5.423309
 58828/100000: episode: 1132, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 41.296, mean reward: 4.588 [3.300, 6.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.347, 10.100], loss: 1.008657, mae: 0.702922, mean_q: 5.397350
[Info] FALSIFICATION!
 58840/100000: episode: 1133, duration: 0.221s, episode steps: 12, steps per second: 54, episode reward: 1084.864, mean reward: 90.405 [3.794, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.767, 10.082], loss: 1.521406, mae: 0.762961, mean_q: 5.479359
 58846/100000: episode: 1134, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 37.222, mean reward: 6.204 [5.984, 6.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.370, 10.100], loss: 0.611168, mae: 0.615127, mean_q: 5.312557
 58858/100000: episode: 1135, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 60.310, mean reward: 5.026 [3.215, 9.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.310, 10.100], loss: 13.211459, mae: 0.942812, mean_q: 5.487888
 58870/100000: episode: 1136, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 54.262, mean reward: 4.522 [3.459, 9.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.441, 10.100], loss: 1.167915, mae: 0.889466, mean_q: 5.791216
 58888/100000: episode: 1137, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 149.654, mean reward: 8.314 [2.647, 19.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.582, 10.100], loss: 1695.360840, mae: 5.925291, mean_q: 7.535123
 58906/100000: episode: 1138, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 99.616, mean reward: 5.534 [3.079, 10.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.531, 10.100], loss: 8.147195, mae: 2.298793, mean_q: 7.018704
 58924/100000: episode: 1139, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 81.733, mean reward: 4.541 [3.227, 6.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.726, 10.100], loss: 2.092144, mae: 1.199560, mean_q: 4.969024
 58933/100000: episode: 1140, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 35.405, mean reward: 3.934 [3.376, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.425, 10.100], loss: 5.209010, mae: 0.913779, mean_q: 5.963071
 58945/100000: episode: 1141, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 117.291, mean reward: 9.774 [6.111, 17.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.631, 10.100], loss: 0.726980, mae: 0.715951, mean_q: 5.493400
 58953/100000: episode: 1142, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 32.707, mean reward: 4.088 [3.127, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.506, 10.100], loss: 0.959739, mae: 0.804071, mean_q: 5.811253
 58961/100000: episode: 1143, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 31.428, mean reward: 3.928 [3.214, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.211, 10.100], loss: 1905.387451, mae: 4.910636, mean_q: 5.900128
 58973/100000: episode: 1144, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 68.774, mean reward: 5.731 [4.738, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.373, 10.100], loss: 3.487696, mae: 1.722692, mean_q: 6.773707
 58982/100000: episode: 1145, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 40.854, mean reward: 4.539 [3.571, 5.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.381, 10.100], loss: 2.400707, mae: 1.282466, mean_q: 6.299422
 58992/100000: episode: 1146, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 34.358, mean reward: 3.436 [2.747, 5.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.452, 10.100], loss: 1.300914, mae: 0.917027, mean_q: 5.953978
[Info] FALSIFICATION!
 59006/100000: episode: 1147, duration: 0.325s, episode steps: 14, steps per second: 43, episode reward: 1136.273, mean reward: 81.162 [2.898, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.870, 10.067], loss: 0.668493, mae: 0.692879, mean_q: 5.490766
 59016/100000: episode: 1148, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 61.090, mean reward: 6.109 [4.044, 8.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.472, 10.100], loss: 1528.896118, mae: 4.105754, mean_q: 5.960639
 59024/100000: episode: 1149, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 32.774, mean reward: 4.097 [2.461, 8.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.581, 10.100], loss: 3.676041, mae: 1.718838, mean_q: 7.026730
 59032/100000: episode: 1150, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 24.605, mean reward: 3.076 [2.437, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.290, 10.100], loss: 2.518185, mae: 1.336668, mean_q: 6.537408
[Info] FALSIFICATION!
 59041/100000: episode: 1151, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 1081.611, mean reward: 120.179 [6.026, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.405, 10.098], loss: 1.684152, mae: 1.002222, mean_q: 6.086284
 59049/100000: episode: 1152, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 25.275, mean reward: 3.159 [2.547, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.422, 10.100], loss: 1.161214, mae: 0.887238, mean_q: 5.890752
 59059/100000: episode: 1153, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 33.909, mean reward: 3.391 [3.033, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.448, 10.100], loss: 0.879951, mae: 0.757729, mean_q: 5.654021
 59077/100000: episode: 1154, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 260.823, mean reward: 14.490 [2.764, 147.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.536, 10.100], loss: 3.716577, mae: 0.972830, mean_q: 5.641675
 59085/100000: episode: 1155, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 25.163, mean reward: 3.145 [2.433, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.416, 10.100], loss: 1.524066, mae: 0.849287, mean_q: 5.915493
 59097/100000: episode: 1156, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 75.441, mean reward: 6.287 [4.321, 8.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.580, 10.100], loss: 2524.330566, mae: 6.161644, mean_q: 6.167692
[Info] FALSIFICATION!
 59108/100000: episode: 1157, duration: 0.326s, episode steps: 11, steps per second: 34, episode reward: 1104.087, mean reward: 100.372 [4.066, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.460, 10.067], loss: 2697.114746, mae: 11.123635, mean_q: 11.430337
 59114/100000: episode: 1158, duration: 0.048s, episode steps: 6, steps per second: 126, episode reward: 53.505, mean reward: 8.917 [5.033, 13.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.585, 10.100], loss: 42.477272, mae: 6.099973, mean_q: 11.987773
 59126/100000: episode: 1159, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 51.719, mean reward: 4.310 [2.971, 5.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.373, 10.100], loss: 1243.377319, mae: 5.221079, mean_q: 7.916241
 59132/100000: episode: 1160, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 46.348, mean reward: 7.725 [6.559, 9.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.430, 10.100], loss: 4.997344, mae: 1.796505, mean_q: 6.471101
 59141/100000: episode: 1161, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 37.234, mean reward: 4.137 [2.528, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.511, 10.100], loss: 5.173407, mae: 1.707671, mean_q: 6.779700
 59156/100000: episode: 1162, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 105.482, mean reward: 7.032 [3.871, 10.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.506, 10.100], loss: 2.795086, mae: 1.263304, mean_q: 5.977731
 59164/100000: episode: 1163, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 22.278, mean reward: 2.785 [2.136, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.540, 10.100], loss: 1.941242, mae: 1.131561, mean_q: 5.832753
 59179/100000: episode: 1164, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 251.421, mean reward: 16.761 [4.113, 59.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.513, 10.100], loss: 4.354380, mae: 1.113522, mean_q: 5.749207
 59187/100000: episode: 1165, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 28.835, mean reward: 3.604 [2.851, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.518, 10.100], loss: 41.667160, mae: 1.711836, mean_q: 6.325366
 59202/100000: episode: 1166, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 105.902, mean reward: 7.060 [3.571, 12.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.532, 10.100], loss: 1014.274597, mae: 3.980439, mean_q: 7.517128
 59220/100000: episode: 1167, duration: 0.098s, episode steps: 18, steps per second: 185, episode reward: 111.773, mean reward: 6.210 [2.562, 14.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.933, 10.100], loss: 8.272167, mae: 1.997296, mean_q: 7.672132
 59228/100000: episode: 1168, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 24.927, mean reward: 3.116 [2.694, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.522, 10.100], loss: 3.109141, mae: 1.314117, mean_q: 6.666530
 59237/100000: episode: 1169, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 40.626, mean reward: 4.514 [3.402, 5.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.615, 10.100], loss: 1.250594, mae: 0.979761, mean_q: 6.299842
[Info] Complete ISplit Iteration
[Info] Levels: [5.9455667, 8.764393, 12.2744]
[Info] Cond. Prob: [0.1, 0.1, 0.72]
[Info] Error Prob: 0.0072000000000000015

 59247/100000: episode: 1170, duration: 4.341s, episode steps: 10, steps per second: 2, episode reward: 37.934, mean reward: 3.793 [2.598, 5.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.209, 10.100], loss: 2.119873, mae: 1.073172, mean_q: 6.217981
 59347/100000: episode: 1171, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 202.736, mean reward: 2.027 [1.472, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.098], loss: 910.989441, mae: 4.366869, mean_q: 8.575273
 59447/100000: episode: 1172, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.672, mean reward: 1.887 [1.444, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.731, 10.098], loss: 154.503052, mae: 1.882014, mean_q: 6.970242
 59547/100000: episode: 1173, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 190.150, mean reward: 1.901 [1.474, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.616, 10.098], loss: 759.179688, mae: 3.779479, mean_q: 8.238910
 59647/100000: episode: 1174, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 236.202, mean reward: 2.362 [1.539, 4.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.556, 10.266], loss: 312.413544, mae: 2.193924, mean_q: 7.589131
 59747/100000: episode: 1175, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.963, mean reward: 1.910 [1.431, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.098], loss: 5.084002, mae: 1.141211, mean_q: 6.811868
 59847/100000: episode: 1176, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.888, mean reward: 1.959 [1.471, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.942, 10.142], loss: 906.253601, mae: 4.148796, mean_q: 8.408341
 59947/100000: episode: 1177, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 206.087, mean reward: 2.061 [1.455, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.032, 10.292], loss: 607.446533, mae: 3.595376, mean_q: 8.604883
 60047/100000: episode: 1178, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 205.445, mean reward: 2.054 [1.484, 5.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.339, 10.098], loss: 301.667664, mae: 2.249187, mean_q: 7.609784
 60147/100000: episode: 1179, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 182.716, mean reward: 1.827 [1.451, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.431, 10.239], loss: 305.476868, mae: 2.190446, mean_q: 7.447636
 60247/100000: episode: 1180, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 217.786, mean reward: 2.178 [1.450, 5.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.098], loss: 304.666412, mae: 1.804754, mean_q: 6.723128
 60347/100000: episode: 1181, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 208.670, mean reward: 2.087 [1.449, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.599, 10.098], loss: 453.485168, mae: 3.050700, mean_q: 8.285947
 60447/100000: episode: 1182, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 206.013, mean reward: 2.060 [1.459, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.931, 10.176], loss: 760.092957, mae: 3.828961, mean_q: 8.383889
 60547/100000: episode: 1183, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 202.210, mean reward: 2.022 [1.481, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.187, 10.181], loss: 602.852905, mae: 3.186114, mean_q: 8.127387
 60647/100000: episode: 1184, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.157, mean reward: 1.942 [1.459, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.898, 10.178], loss: 597.163330, mae: 3.235415, mean_q: 8.579860
 60747/100000: episode: 1185, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 200.108, mean reward: 2.001 [1.473, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.677, 10.332], loss: 596.267517, mae: 3.181735, mean_q: 8.186101
 60847/100000: episode: 1186, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 192.120, mean reward: 1.921 [1.481, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.671, 10.131], loss: 159.724701, mae: 1.945505, mean_q: 7.569386
 60947/100000: episode: 1187, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.733, mean reward: 1.907 [1.479, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.873, 10.254], loss: 906.197876, mae: 4.424146, mean_q: 8.778119
 61047/100000: episode: 1188, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.538, mean reward: 1.925 [1.481, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 157.705353, mae: 1.940276, mean_q: 7.612772
 61147/100000: episode: 1189, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 183.561, mean reward: 1.836 [1.459, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.788, 10.125], loss: 454.821808, mae: 2.704855, mean_q: 7.828071
 61247/100000: episode: 1190, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 205.595, mean reward: 2.056 [1.492, 6.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.911, 10.237], loss: 455.117645, mae: 2.579726, mean_q: 7.584456
 61347/100000: episode: 1191, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.753, mean reward: 1.928 [1.455, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.257, 10.098], loss: 314.836731, mae: 2.335504, mean_q: 7.333255
 61447/100000: episode: 1192, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.035, mean reward: 1.990 [1.436, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.163], loss: 312.526733, mae: 2.355756, mean_q: 7.794340
 61547/100000: episode: 1193, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 184.015, mean reward: 1.840 [1.494, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.759, 10.098], loss: 904.268372, mae: 4.035193, mean_q: 8.328565
 61647/100000: episode: 1194, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.096, mean reward: 1.961 [1.497, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.493, 10.098], loss: 153.146774, mae: 1.785342, mean_q: 7.283232
 61747/100000: episode: 1195, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.508, mean reward: 1.905 [1.471, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.305, 10.098], loss: 151.276871, mae: 1.556290, mean_q: 6.649446
 61847/100000: episode: 1196, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 179.031, mean reward: 1.790 [1.441, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.642, 10.107], loss: 453.379150, mae: 2.337197, mean_q: 6.929792
 61947/100000: episode: 1197, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.815, mean reward: 1.918 [1.490, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.990, 10.098], loss: 10.954320, mae: 1.398257, mean_q: 6.630577
 62047/100000: episode: 1198, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.440, mean reward: 1.874 [1.494, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.658, 10.306], loss: 158.651688, mae: 1.544597, mean_q: 6.570621
 62147/100000: episode: 1199, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 188.434, mean reward: 1.884 [1.458, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.134, 10.098], loss: 157.797546, mae: 1.559412, mean_q: 6.518463
 62247/100000: episode: 1200, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 199.702, mean reward: 1.997 [1.453, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.342, 10.098], loss: 744.984619, mae: 3.341331, mean_q: 7.373239
 62347/100000: episode: 1201, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 202.622, mean reward: 2.026 [1.440, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.574, 10.369], loss: 308.971710, mae: 2.055433, mean_q: 6.739356
 62447/100000: episode: 1202, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 203.553, mean reward: 2.036 [1.465, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.567, 10.338], loss: 1193.551514, mae: 4.947260, mean_q: 8.437345
 62547/100000: episode: 1203, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 197.183, mean reward: 1.972 [1.486, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.639, 10.098], loss: 451.747925, mae: 3.046378, mean_q: 8.125016
 62647/100000: episode: 1204, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.767, mean reward: 1.818 [1.466, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.938, 10.239], loss: 162.264206, mae: 1.879480, mean_q: 7.123097
 62747/100000: episode: 1205, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.398, mean reward: 1.924 [1.468, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.169, 10.302], loss: 602.378296, mae: 2.766544, mean_q: 7.183945
 62847/100000: episode: 1206, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 210.995, mean reward: 2.110 [1.502, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.259, 10.207], loss: 453.881561, mae: 2.542746, mean_q: 7.175995
 62947/100000: episode: 1207, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.796, mean reward: 1.898 [1.468, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.523, 10.098], loss: 892.813660, mae: 4.299040, mean_q: 8.539287
 63047/100000: episode: 1208, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.162, mean reward: 1.962 [1.473, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.108, 10.215], loss: 592.135925, mae: 2.708868, mean_q: 7.267445
 63147/100000: episode: 1209, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 183.503, mean reward: 1.835 [1.458, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.590, 10.128], loss: 301.798157, mae: 2.401944, mean_q: 7.457145
 63247/100000: episode: 1210, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 185.325, mean reward: 1.853 [1.439, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.564, 10.098], loss: 448.435699, mae: 2.267828, mean_q: 6.433591
 63347/100000: episode: 1211, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.931, mean reward: 2.029 [1.444, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.098], loss: 160.764908, mae: 1.677856, mean_q: 6.129545
 63447/100000: episode: 1212, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.163, mean reward: 1.962 [1.500, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.190, 10.325], loss: 730.386414, mae: 3.616355, mean_q: 7.506851
 63547/100000: episode: 1213, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.972, mean reward: 2.020 [1.471, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.098], loss: 6.134289, mae: 1.051762, mean_q: 5.381404
 63647/100000: episode: 1214, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 214.879, mean reward: 2.149 [1.459, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.463, 10.108], loss: 148.665161, mae: 1.298001, mean_q: 5.525881
 63747/100000: episode: 1215, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.958, mean reward: 1.950 [1.460, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.724, 10.108], loss: 600.621277, mae: 2.707962, mean_q: 6.219177
 63847/100000: episode: 1216, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 218.110, mean reward: 2.181 [1.460, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.609, 10.098], loss: 302.795685, mae: 1.697419, mean_q: 5.512262
 63947/100000: episode: 1217, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.483, mean reward: 1.895 [1.455, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.217], loss: 441.270386, mae: 1.953797, mean_q: 5.394691
 64047/100000: episode: 1218, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.085, mean reward: 1.961 [1.471, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.935, 10.098], loss: 150.873871, mae: 1.263361, mean_q: 5.159429
 64147/100000: episode: 1219, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.497, mean reward: 2.055 [1.485, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.837, 10.359], loss: 0.615543, mae: 0.485996, mean_q: 4.218668
 64247/100000: episode: 1220, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 178.597, mean reward: 1.786 [1.441, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.459, 10.343], loss: 0.195417, mae: 0.393512, mean_q: 3.939307
 64347/100000: episode: 1221, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 174.268, mean reward: 1.743 [1.461, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.143], loss: 0.174678, mae: 0.381496, mean_q: 3.884478
 64447/100000: episode: 1222, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.977, mean reward: 1.960 [1.522, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.098], loss: 0.165672, mae: 0.383895, mean_q: 3.895603
 64547/100000: episode: 1223, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.747, mean reward: 1.897 [1.479, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.263, 10.098], loss: 0.154487, mae: 0.367782, mean_q: 3.871810
 64647/100000: episode: 1224, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.905, mean reward: 1.789 [1.436, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.744, 10.104], loss: 0.161782, mae: 0.367398, mean_q: 3.878266
 64747/100000: episode: 1225, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 226.755, mean reward: 2.268 [1.455, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.327, 10.098], loss: 0.139620, mae: 0.357768, mean_q: 3.874100
 64847/100000: episode: 1226, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 191.672, mean reward: 1.917 [1.491, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.583, 10.098], loss: 0.159085, mae: 0.373107, mean_q: 3.867916
 64947/100000: episode: 1227, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 187.215, mean reward: 1.872 [1.449, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.098], loss: 0.168693, mae: 0.360876, mean_q: 3.866261
 65047/100000: episode: 1228, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.260, mean reward: 1.883 [1.472, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.264, 10.098], loss: 0.186656, mae: 0.373464, mean_q: 3.845807
 65147/100000: episode: 1229, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 232.185, mean reward: 2.322 [1.457, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.098], loss: 0.122661, mae: 0.338935, mean_q: 3.854453
 65247/100000: episode: 1230, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 209.005, mean reward: 2.090 [1.600, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.615, 10.339], loss: 0.131490, mae: 0.348961, mean_q: 3.835248
 65347/100000: episode: 1231, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 215.379, mean reward: 2.154 [1.434, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.244, 10.406], loss: 0.112374, mae: 0.336671, mean_q: 3.871343
 65447/100000: episode: 1232, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 209.135, mean reward: 2.091 [1.456, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.026, 10.257], loss: 0.117429, mae: 0.334881, mean_q: 3.856084
 65547/100000: episode: 1233, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 227.062, mean reward: 2.271 [1.522, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.762, 10.199], loss: 0.115771, mae: 0.336341, mean_q: 3.882431
 65647/100000: episode: 1234, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 199.350, mean reward: 1.993 [1.472, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.656, 10.136], loss: 0.123666, mae: 0.344180, mean_q: 3.886518
 65747/100000: episode: 1235, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.163, mean reward: 1.852 [1.450, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.696, 10.098], loss: 0.119755, mae: 0.338221, mean_q: 3.873883
 65847/100000: episode: 1236, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.759, mean reward: 1.988 [1.447, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.166, 10.098], loss: 0.109207, mae: 0.330759, mean_q: 3.883749
 65947/100000: episode: 1237, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 196.503, mean reward: 1.965 [1.465, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.707, 10.098], loss: 0.106299, mae: 0.327835, mean_q: 3.871372
 66047/100000: episode: 1238, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.072, mean reward: 1.911 [1.498, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.098], loss: 0.105068, mae: 0.323569, mean_q: 3.870935
 66147/100000: episode: 1239, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.550, mean reward: 1.896 [1.448, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.666, 10.098], loss: 0.107632, mae: 0.319400, mean_q: 3.864772
 66247/100000: episode: 1240, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.909, mean reward: 1.959 [1.543, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.164, 10.206], loss: 0.103700, mae: 0.321930, mean_q: 3.874851
 66347/100000: episode: 1241, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.980, mean reward: 1.870 [1.472, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.549, 10.126], loss: 0.101076, mae: 0.315830, mean_q: 3.875579
 66447/100000: episode: 1242, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 206.432, mean reward: 2.064 [1.512, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.698, 10.098], loss: 0.095723, mae: 0.310430, mean_q: 3.891284
 66547/100000: episode: 1243, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.434, mean reward: 1.954 [1.454, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.660, 10.184], loss: 0.087068, mae: 0.300707, mean_q: 3.888871
 66647/100000: episode: 1244, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.236, mean reward: 1.902 [1.476, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.795, 10.098], loss: 0.105072, mae: 0.321359, mean_q: 3.897684
 66747/100000: episode: 1245, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.537, mean reward: 1.965 [1.441, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.098], loss: 0.093390, mae: 0.309415, mean_q: 3.886529
 66847/100000: episode: 1246, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 234.595, mean reward: 2.346 [1.435, 8.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.276, 10.189], loss: 0.112757, mae: 0.327057, mean_q: 3.911280
 66947/100000: episode: 1247, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 180.766, mean reward: 1.808 [1.450, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.705, 10.278], loss: 0.114242, mae: 0.321312, mean_q: 3.895772
 67047/100000: episode: 1248, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.902, mean reward: 1.939 [1.445, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.796, 10.480], loss: 0.132515, mae: 0.333421, mean_q: 3.931211
 67147/100000: episode: 1249, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 201.653, mean reward: 2.017 [1.465, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.066, 10.295], loss: 0.119624, mae: 0.333121, mean_q: 3.933507
 67247/100000: episode: 1250, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 204.279, mean reward: 2.043 [1.470, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.614, 10.123], loss: 0.113537, mae: 0.325049, mean_q: 3.914061
 67347/100000: episode: 1251, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 213.670, mean reward: 2.137 [1.480, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.714, 10.173], loss: 0.101108, mae: 0.314270, mean_q: 3.907569
 67447/100000: episode: 1252, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.443, mean reward: 1.934 [1.432, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.592, 10.216], loss: 0.107494, mae: 0.328564, mean_q: 3.922848
 67547/100000: episode: 1253, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.802, mean reward: 1.878 [1.456, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.592, 10.152], loss: 0.100024, mae: 0.316916, mean_q: 3.910927
 67647/100000: episode: 1254, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.096, mean reward: 1.841 [1.475, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.987, 10.231], loss: 0.105870, mae: 0.325119, mean_q: 3.925416
 67747/100000: episode: 1255, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.825, mean reward: 2.008 [1.474, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.505, 10.192], loss: 0.100408, mae: 0.313139, mean_q: 3.920895
 67847/100000: episode: 1256, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.958, mean reward: 1.910 [1.455, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.342], loss: 0.119959, mae: 0.323363, mean_q: 3.926019
 67947/100000: episode: 1257, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 201.404, mean reward: 2.014 [1.497, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.143], loss: 0.095227, mae: 0.315021, mean_q: 3.901958
 68047/100000: episode: 1258, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.051, mean reward: 1.851 [1.466, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.820, 10.098], loss: 0.093932, mae: 0.304786, mean_q: 3.894068
 68147/100000: episode: 1259, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 184.936, mean reward: 1.849 [1.449, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.838, 10.312], loss: 0.107119, mae: 0.314262, mean_q: 3.891957
 68247/100000: episode: 1260, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 226.101, mean reward: 2.261 [1.464, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.301, 10.098], loss: 0.096263, mae: 0.311161, mean_q: 3.907368
 68347/100000: episode: 1261, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.364, mean reward: 2.004 [1.462, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.447, 10.386], loss: 0.093664, mae: 0.308928, mean_q: 3.916274
 68447/100000: episode: 1262, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.575, mean reward: 1.886 [1.454, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.927, 10.194], loss: 0.105973, mae: 0.316734, mean_q: 3.942559
 68547/100000: episode: 1263, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.890, mean reward: 2.029 [1.455, 6.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.198, 10.213], loss: 0.104345, mae: 0.323053, mean_q: 3.920433
 68647/100000: episode: 1264, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.218, mean reward: 1.842 [1.441, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.100], loss: 0.113231, mae: 0.314045, mean_q: 3.935266
 68747/100000: episode: 1265, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.658, mean reward: 1.847 [1.454, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.697, 10.098], loss: 0.106392, mae: 0.314505, mean_q: 3.927655
 68847/100000: episode: 1266, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 186.902, mean reward: 1.869 [1.493, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.349, 10.098], loss: 0.093729, mae: 0.308831, mean_q: 3.923031
 68947/100000: episode: 1267, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.321, mean reward: 1.933 [1.507, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.369, 10.114], loss: 0.113443, mae: 0.313878, mean_q: 3.927211
 69047/100000: episode: 1268, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 208.171, mean reward: 2.082 [1.542, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.716, 10.178], loss: 0.109492, mae: 0.316343, mean_q: 3.915089
 69147/100000: episode: 1269, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 202.923, mean reward: 2.029 [1.500, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.771, 10.098], loss: 0.092703, mae: 0.310837, mean_q: 3.919513
[Info] 1-TH LEVEL FOUND: 4.845114231109619, Considering 10/90 traces
 69247/100000: episode: 1270, duration: 4.506s, episode steps: 100, steps per second: 22, episode reward: 200.623, mean reward: 2.006 [1.459, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.021, 10.098], loss: 0.100146, mae: 0.299749, mean_q: 3.904249
 69291/100000: episode: 1271, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 84.411, mean reward: 1.918 [1.467, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.389, 10.187], loss: 0.092560, mae: 0.304636, mean_q: 3.922539
 69335/100000: episode: 1272, duration: 0.231s, episode steps: 44, steps per second: 191, episode reward: 86.846, mean reward: 1.974 [1.503, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.055, 10.147], loss: 0.100491, mae: 0.310691, mean_q: 3.927779
 69349/100000: episode: 1273, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 37.725, mean reward: 2.695 [2.004, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.682, 10.397], loss: 0.072777, mae: 0.291698, mean_q: 3.952189
 69401/100000: episode: 1274, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 108.961, mean reward: 2.095 [1.436, 5.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.745, 10.149], loss: 0.105041, mae: 0.307515, mean_q: 3.919470
 69445/100000: episode: 1275, duration: 0.240s, episode steps: 44, steps per second: 184, episode reward: 81.911, mean reward: 1.862 [1.483, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.179, 10.165], loss: 0.088126, mae: 0.304733, mean_q: 3.930280
 69482/100000: episode: 1276, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 88.172, mean reward: 2.383 [1.777, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.441, 10.436], loss: 0.106496, mae: 0.312578, mean_q: 3.939788
 69492/100000: episode: 1277, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 22.264, mean reward: 2.226 [1.849, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.692, 10.322], loss: 0.115789, mae: 0.336699, mean_q: 3.951807
 69497/100000: episode: 1278, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 18.544, mean reward: 3.709 [2.916, 5.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.483], loss: 0.073757, mae: 0.286942, mean_q: 3.934866
 69511/100000: episode: 1279, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 33.093, mean reward: 2.364 [1.943, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.384], loss: 0.093676, mae: 0.309010, mean_q: 3.942480
 69563/100000: episode: 1280, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 166.527, mean reward: 3.202 [1.908, 5.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.644, 10.378], loss: 0.112925, mae: 0.325001, mean_q: 3.963084
 69607/100000: episode: 1281, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 99.704, mean reward: 2.266 [1.736, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.330, 10.249], loss: 0.103131, mae: 0.312047, mean_q: 3.952974
 69621/100000: episode: 1282, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 31.345, mean reward: 2.239 [1.851, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.267], loss: 0.087767, mae: 0.293795, mean_q: 3.937393
 69626/100000: episode: 1283, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 16.356, mean reward: 3.271 [2.931, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.404], loss: 0.169641, mae: 0.401275, mean_q: 4.062406
 69635/100000: episode: 1284, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 23.477, mean reward: 2.609 [2.277, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.163, 10.445], loss: 0.076961, mae: 0.300899, mean_q: 4.004772
 69679/100000: episode: 1285, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 88.720, mean reward: 2.016 [1.573, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.085, 10.275], loss: 0.104434, mae: 0.330783, mean_q: 3.973180
 69731/100000: episode: 1286, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 108.012, mean reward: 2.077 [1.512, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.797, 10.141], loss: 0.088257, mae: 0.307737, mean_q: 3.950297
 69737/100000: episode: 1287, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 13.406, mean reward: 2.234 [1.969, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.387], loss: 0.087378, mae: 0.320021, mean_q: 3.972759
 69781/100000: episode: 1288, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 81.052, mean reward: 1.842 [1.480, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.994, 10.300], loss: 0.089434, mae: 0.304619, mean_q: 3.971533
 69818/100000: episode: 1289, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 73.220, mean reward: 1.979 [1.610, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.569, 10.262], loss: 0.101752, mae: 0.321418, mean_q: 3.988619
 69856/100000: episode: 1290, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 116.939, mean reward: 3.077 [2.260, 4.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.877, 10.575], loss: 0.098692, mae: 0.315922, mean_q: 3.985539
 69865/100000: episode: 1291, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 27.080, mean reward: 3.009 [2.066, 5.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.670, 10.377], loss: 0.090043, mae: 0.303531, mean_q: 3.960504
 69909/100000: episode: 1292, duration: 0.232s, episode steps: 44, steps per second: 190, episode reward: 101.204, mean reward: 2.300 [1.708, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.682, 10.334], loss: 0.094715, mae: 0.308085, mean_q: 3.999132
 69918/100000: episode: 1293, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 22.517, mean reward: 2.502 [2.316, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.222, 10.390], loss: 0.106993, mae: 0.333040, mean_q: 3.937300
 69970/100000: episode: 1294, duration: 0.298s, episode steps: 52, steps per second: 174, episode reward: 129.652, mean reward: 2.493 [1.580, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.075, 10.412], loss: 0.103852, mae: 0.325968, mean_q: 3.983065
 70008/100000: episode: 1295, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 114.983, mean reward: 3.026 [1.745, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.049, 10.342], loss: 0.138012, mae: 0.332873, mean_q: 4.012840
 70018/100000: episode: 1296, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 18.572, mean reward: 1.857 [1.602, 2.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.246], loss: 0.072706, mae: 0.285507, mean_q: 3.941084
 70032/100000: episode: 1297, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 30.000, mean reward: 2.143 [1.524, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.122, 10.232], loss: 0.127727, mae: 0.359572, mean_q: 4.012847
 70037/100000: episode: 1298, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 14.806, mean reward: 2.961 [2.362, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.364], loss: 0.089593, mae: 0.322753, mean_q: 4.024816
 70074/100000: episode: 1299, duration: 0.191s, episode steps: 37, steps per second: 193, episode reward: 91.988, mean reward: 2.486 [1.801, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.265, 10.381], loss: 0.125293, mae: 0.335023, mean_q: 4.020366
 70079/100000: episode: 1300, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 17.583, mean reward: 3.517 [2.580, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.103, 10.324], loss: 0.083993, mae: 0.284047, mean_q: 3.991540
 70085/100000: episode: 1301, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 14.966, mean reward: 2.494 [1.890, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.150, 10.356], loss: 0.092384, mae: 0.318836, mean_q: 4.022352
 70099/100000: episode: 1302, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 28.342, mean reward: 2.024 [1.891, 2.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.285], loss: 0.099638, mae: 0.330912, mean_q: 4.040021
 70104/100000: episode: 1303, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 13.146, mean reward: 2.629 [2.237, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.457], loss: 0.087654, mae: 0.272431, mean_q: 3.897263
 70130/100000: episode: 1304, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 78.357, mean reward: 3.014 [2.232, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.722, 10.375], loss: 0.137940, mae: 0.345323, mean_q: 4.026289
 70140/100000: episode: 1305, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 20.060, mean reward: 2.006 [1.638, 2.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.302], loss: 0.106599, mae: 0.321672, mean_q: 3.992236
 70150/100000: episode: 1306, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 21.635, mean reward: 2.163 [1.759, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.632, 10.381], loss: 0.076292, mae: 0.285312, mean_q: 3.966873
 70156/100000: episode: 1307, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 14.856, mean reward: 2.476 [1.850, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.553, 10.237], loss: 0.104767, mae: 0.324818, mean_q: 3.967277
 70165/100000: episode: 1308, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 22.937, mean reward: 2.549 [2.044, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.356], loss: 0.180484, mae: 0.365751, mean_q: 4.009294
 70217/100000: episode: 1309, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 238.274, mean reward: 4.582 [1.723, 88.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.429, 10.382], loss: 0.121984, mae: 0.340885, mean_q: 4.020970
 70223/100000: episode: 1310, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 13.498, mean reward: 2.250 [1.979, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.369], loss: 0.362154, mae: 0.428200, mean_q: 4.095318
 70232/100000: episode: 1311, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 22.043, mean reward: 2.449 [2.059, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.351], loss: 13.339197, mae: 0.958508, mean_q: 4.533735
 70242/100000: episode: 1312, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 22.305, mean reward: 2.231 [1.957, 2.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.036, 10.322], loss: 0.164405, mae: 0.398001, mean_q: 3.851102
 70248/100000: episode: 1313, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 14.085, mean reward: 2.347 [1.935, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.441], loss: 0.165110, mae: 0.395130, mean_q: 4.164579
 70274/100000: episode: 1314, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 65.031, mean reward: 2.501 [1.896, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.294, 10.265], loss: 4.649653, mae: 0.591570, mean_q: 4.154140
 70300/100000: episode: 1315, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 67.216, mean reward: 2.585 [1.932, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.045, 10.462], loss: 0.152883, mae: 0.371579, mean_q: 4.120200
 70305/100000: episode: 1316, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 16.200, mean reward: 3.240 [2.801, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.461], loss: 0.159522, mae: 0.397227, mean_q: 4.125173
 70310/100000: episode: 1317, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 18.743, mean reward: 3.749 [2.760, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.566], loss: 0.158629, mae: 0.357248, mean_q: 4.135946
 70336/100000: episode: 1318, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 54.221, mean reward: 2.085 [1.453, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.285, 10.275], loss: 0.122380, mae: 0.355796, mean_q: 4.095208
 70345/100000: episode: 1319, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 32.882, mean reward: 3.654 [2.675, 6.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.518], loss: 0.129346, mae: 0.326924, mean_q: 4.002070
 70359/100000: episode: 1320, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 28.260, mean reward: 2.019 [1.679, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.645, 10.221], loss: 0.113717, mae: 0.353872, mean_q: 4.115431
 70364/100000: episode: 1321, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 16.497, mean reward: 3.299 [2.457, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.504], loss: 0.140839, mae: 0.328481, mean_q: 4.006063
 70402/100000: episode: 1322, duration: 0.214s, episode steps: 38, steps per second: 177, episode reward: 112.094, mean reward: 2.950 [1.537, 5.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.267, 10.225], loss: 3.263442, mae: 0.488155, mean_q: 4.155594
 70408/100000: episode: 1323, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 13.482, mean reward: 2.247 [1.857, 2.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.400], loss: 0.190250, mae: 0.428734, mean_q: 3.807822
 70460/100000: episode: 1324, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 142.287, mean reward: 2.736 [2.062, 5.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.454, 10.301], loss: 0.147629, mae: 0.353531, mean_q: 4.068349
 70465/100000: episode: 1325, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 15.562, mean reward: 3.112 [2.852, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.457], loss: 0.469226, mae: 0.462020, mean_q: 4.117939
 70491/100000: episode: 1326, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 58.660, mean reward: 2.256 [1.460, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.216, 10.100], loss: 0.171170, mae: 0.385970, mean_q: 4.075521
 70505/100000: episode: 1327, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 31.598, mean reward: 2.257 [1.801, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.348, 10.303], loss: 0.151996, mae: 0.351713, mean_q: 4.099608
 70542/100000: episode: 1328, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 95.165, mean reward: 2.572 [1.906, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.384, 10.305], loss: 0.134916, mae: 0.350393, mean_q: 4.053655
 70586/100000: episode: 1329, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 86.768, mean reward: 1.972 [1.487, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.428, 10.105], loss: 0.197080, mae: 0.393211, mean_q: 4.101557
 70596/100000: episode: 1330, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 21.247, mean reward: 2.125 [1.908, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.309], loss: 0.093833, mae: 0.323288, mean_q: 4.085116
 70633/100000: episode: 1331, duration: 0.216s, episode steps: 37, steps per second: 172, episode reward: 122.791, mean reward: 3.319 [2.271, 7.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.816, 10.666], loss: 0.133019, mae: 0.360406, mean_q: 4.105425
 70671/100000: episode: 1332, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 119.983, mean reward: 3.157 [2.044, 10.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.252, 10.372], loss: 0.130797, mae: 0.353468, mean_q: 4.089631
 70677/100000: episode: 1333, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 14.749, mean reward: 2.458 [2.090, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.389], loss: 0.092016, mae: 0.300342, mean_q: 4.116591
 70714/100000: episode: 1334, duration: 0.217s, episode steps: 37, steps per second: 171, episode reward: 86.113, mean reward: 2.327 [1.478, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.035, 10.134], loss: 0.166578, mae: 0.382093, mean_q: 4.136738
 70724/100000: episode: 1335, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 19.416, mean reward: 1.942 [1.599, 2.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.326], loss: 0.123555, mae: 0.374721, mean_q: 4.177522
 70768/100000: episode: 1336, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 103.564, mean reward: 2.354 [1.926, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.928, 10.362], loss: 0.122919, mae: 0.347903, mean_q: 4.113255
 70774/100000: episode: 1337, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 12.857, mean reward: 2.143 [1.931, 2.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.354], loss: 0.150328, mae: 0.375086, mean_q: 4.101213
 70780/100000: episode: 1338, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 14.084, mean reward: 2.347 [1.902, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.795, 10.244], loss: 0.144013, mae: 0.369843, mean_q: 4.132291
 70786/100000: episode: 1339, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 13.046, mean reward: 2.174 [1.982, 2.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.389], loss: 0.101995, mae: 0.316462, mean_q: 4.119886
 70800/100000: episode: 1340, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 31.862, mean reward: 2.276 [1.703, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.227], loss: 0.169547, mae: 0.380713, mean_q: 4.150279
 70814/100000: episode: 1341, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 31.151, mean reward: 2.225 [1.590, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.939, 10.192], loss: 0.104818, mae: 0.346744, mean_q: 4.134053
 70852/100000: episode: 1342, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 108.315, mean reward: 2.850 [2.040, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.689, 10.414], loss: 0.181798, mae: 0.377611, mean_q: 4.135332
 70904/100000: episode: 1343, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 137.152, mean reward: 2.638 [1.486, 10.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.853, 10.185], loss: 2.399383, mae: 0.445288, mean_q: 4.174320
 70956/100000: episode: 1344, duration: 0.282s, episode steps: 52, steps per second: 185, episode reward: 122.112, mean reward: 2.348 [1.737, 5.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.608, 10.326], loss: 0.147803, mae: 0.383524, mean_q: 4.132314
 70961/100000: episode: 1345, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 13.663, mean reward: 2.733 [2.235, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.428], loss: 0.252240, mae: 0.421859, mean_q: 4.083330
 71013/100000: episode: 1346, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 123.085, mean reward: 2.367 [1.440, 7.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.680, 10.112], loss: 2.529465, mae: 0.509700, mean_q: 4.268540
 71039/100000: episode: 1347, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 55.602, mean reward: 2.139 [1.517, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.200, 10.183], loss: 0.151573, mae: 0.402341, mean_q: 4.228456
 71048/100000: episode: 1348, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 26.141, mean reward: 2.905 [2.537, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.345, 10.419], loss: 0.142407, mae: 0.369159, mean_q: 4.199066
 71058/100000: episode: 1349, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 21.511, mean reward: 2.151 [1.716, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.460, 10.287], loss: 0.107469, mae: 0.346193, mean_q: 4.256146
 71102/100000: episode: 1350, duration: 0.242s, episode steps: 44, steps per second: 181, episode reward: 84.992, mean reward: 1.932 [1.565, 2.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.071, 10.261], loss: 0.200774, mae: 0.390729, mean_q: 4.191577
 71140/100000: episode: 1351, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 124.527, mean reward: 3.277 [2.054, 4.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.732, 10.410], loss: 3.181703, mae: 0.447778, mean_q: 4.216179
 71149/100000: episode: 1352, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 28.188, mean reward: 3.132 [2.559, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.544], loss: 0.252303, mae: 0.550434, mean_q: 4.449485
 71175/100000: episode: 1353, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 64.098, mean reward: 2.465 [1.991, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.396, 10.391], loss: 0.168826, mae: 0.398469, mean_q: 4.183171
 71212/100000: episode: 1354, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 76.360, mean reward: 2.064 [1.565, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.357, 10.245], loss: 0.154159, mae: 0.390836, mean_q: 4.277105
 71217/100000: episode: 1355, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 15.879, mean reward: 3.176 [2.752, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.429], loss: 0.133789, mae: 0.376535, mean_q: 4.289272
 71231/100000: episode: 1356, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 35.879, mean reward: 2.563 [1.931, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.237], loss: 0.141664, mae: 0.380757, mean_q: 4.236872
 71283/100000: episode: 1357, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 111.916, mean reward: 2.152 [1.470, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.796, 10.102], loss: 0.164091, mae: 0.394054, mean_q: 4.239987
 71309/100000: episode: 1358, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 65.640, mean reward: 2.525 [1.767, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.350], loss: 0.185971, mae: 0.390718, mean_q: 4.290195
 71346/100000: episode: 1359, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 85.212, mean reward: 2.303 [1.753, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.538, 10.401], loss: 0.148160, mae: 0.362707, mean_q: 4.234061
[Info] 2-TH LEVEL FOUND: 5.9843339920043945, Considering 11/89 traces
 71390/100000: episode: 1360, duration: 4.314s, episode steps: 44, steps per second: 10, episode reward: 80.300, mean reward: 1.825 [1.505, 2.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.233, 10.107], loss: 0.151514, mae: 0.375567, mean_q: 4.241684
 71428/100000: episode: 1361, duration: 0.203s, episode steps: 38, steps per second: 188, episode reward: 124.671, mean reward: 3.281 [1.832, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.091, 10.455], loss: 3.230998, mae: 0.491372, mean_q: 4.314117
 71466/100000: episode: 1362, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 94.584, mean reward: 2.489 [2.027, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.147, 10.333], loss: 0.247359, mae: 0.441775, mean_q: 4.307713
 71504/100000: episode: 1363, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 91.059, mean reward: 2.396 [1.483, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.386, 10.159], loss: 0.234155, mae: 0.418003, mean_q: 4.299249
 71542/100000: episode: 1364, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 140.828, mean reward: 3.706 [2.099, 6.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.319, 10.443], loss: 0.239855, mae: 0.430868, mean_q: 4.279721
 71564/100000: episode: 1365, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 66.653, mean reward: 3.030 [2.081, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.808, 10.494], loss: 0.189807, mae: 0.391463, mean_q: 4.323044
 71602/100000: episode: 1366, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 286.310, mean reward: 7.534 [3.272, 27.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.102, 10.630], loss: 0.252019, mae: 0.422301, mean_q: 4.305487
 71640/100000: episode: 1367, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 146.162, mean reward: 3.846 [2.032, 8.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.793, 10.376], loss: 3.672943, mae: 0.630380, mean_q: 4.529481
 71678/100000: episode: 1368, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 107.214, mean reward: 2.821 [1.832, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.519, 10.291], loss: 0.462814, mae: 0.545795, mean_q: 4.386619
 71716/100000: episode: 1369, duration: 0.195s, episode steps: 38, steps per second: 194, episode reward: 96.719, mean reward: 2.545 [1.966, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.338, 10.414], loss: 0.251924, mae: 0.441204, mean_q: 4.406914
 71719/100000: episode: 1370, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 9.586, mean reward: 3.195 [2.955, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.114, 10.454], loss: 0.168741, mae: 0.394226, mean_q: 4.312151
 71757/100000: episode: 1371, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 115.544, mean reward: 3.041 [2.427, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.103, 10.447], loss: 3.686182, mae: 0.610531, mean_q: 4.522912
 71795/100000: episode: 1372, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 108.387, mean reward: 2.852 [1.825, 5.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.168, 10.340], loss: 0.498920, mae: 0.473279, mean_q: 4.478447
 71833/100000: episode: 1373, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 118.874, mean reward: 3.128 [2.160, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.050, 10.428], loss: 0.250652, mae: 0.462010, mean_q: 4.451552
 71871/100000: episode: 1374, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 89.128, mean reward: 2.345 [1.523, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.125, 10.100], loss: 0.343264, mae: 0.492553, mean_q: 4.514127
 71909/100000: episode: 1375, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 92.484, mean reward: 2.434 [1.711, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.204, 10.229], loss: 0.221753, mae: 0.436214, mean_q: 4.393953
 71947/100000: episode: 1376, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 114.221, mean reward: 3.006 [1.754, 6.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.409, 10.208], loss: 3.386330, mae: 0.601666, mean_q: 4.547739
 71985/100000: episode: 1377, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 110.306, mean reward: 2.903 [1.973, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.476, 10.310], loss: 0.560379, mae: 0.527222, mean_q: 4.546632
 72023/100000: episode: 1378, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 84.394, mean reward: 2.221 [1.532, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.330, 10.202], loss: 0.306417, mae: 0.463593, mean_q: 4.505177
 72061/100000: episode: 1379, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 86.903, mean reward: 2.287 [1.463, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.169, 10.100], loss: 3.496284, mae: 0.677856, mean_q: 4.587335
 72064/100000: episode: 1380, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 7.928, mean reward: 2.643 [2.386, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.131, 10.401], loss: 0.323965, mae: 0.603394, mean_q: 4.860444
 72069/100000: episode: 1381, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 15.599, mean reward: 3.120 [2.805, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.137, 10.391], loss: 0.162239, mae: 0.423286, mean_q: 4.474500
 72072/100000: episode: 1382, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 6.619, mean reward: 2.206 [2.181, 2.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.348], loss: 0.226866, mae: 0.502288, mean_q: 4.474273
 72110/100000: episode: 1383, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 103.972, mean reward: 2.736 [1.763, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.365, 10.255], loss: 0.317167, mae: 0.485702, mean_q: 4.523976
 72148/100000: episode: 1384, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 119.670, mean reward: 3.149 [2.202, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.587, 10.439], loss: 0.267106, mae: 0.462265, mean_q: 4.529395
 72186/100000: episode: 1385, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 118.565, mean reward: 3.120 [2.276, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.297, 10.499], loss: 3.335878, mae: 0.587743, mean_q: 4.609751
 72189/100000: episode: 1386, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 8.553, mean reward: 2.851 [2.300, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.496], loss: 0.166248, mae: 0.394589, mean_q: 4.415356
 72211/100000: episode: 1387, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 140.086, mean reward: 6.368 [2.584, 36.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.702, 10.532], loss: 5.497133, mae: 0.597927, mean_q: 4.587528
 72249/100000: episode: 1388, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 128.720, mean reward: 3.387 [2.454, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.672, 10.411], loss: 0.387888, mae: 0.534170, mean_q: 4.625937
 72287/100000: episode: 1389, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 131.871, mean reward: 3.470 [2.342, 6.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.040, 10.406], loss: 0.693894, mae: 0.489101, mean_q: 4.611641
 72290/100000: episode: 1390, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 8.441, mean reward: 2.814 [2.680, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.442], loss: 0.119332, mae: 0.370500, mean_q: 4.598186
 72318/100000: episode: 1391, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 76.156, mean reward: 2.720 [1.862, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.589, 10.331], loss: 0.228864, mae: 0.441423, mean_q: 4.596352
 72356/100000: episode: 1392, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 100.258, mean reward: 2.638 [2.035, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.532, 10.342], loss: 0.206535, mae: 0.430581, mean_q: 4.524654
 72394/100000: episode: 1393, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 86.081, mean reward: 2.265 [1.457, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.329, 10.100], loss: 0.555164, mae: 0.509710, mean_q: 4.657246
 72416/100000: episode: 1394, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 57.878, mean reward: 2.631 [1.559, 7.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.136, 10.472], loss: 1.093754, mae: 0.532950, mean_q: 4.599226
 72454/100000: episode: 1395, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 80.186, mean reward: 2.110 [1.471, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.385, 10.100], loss: 0.385603, mae: 0.507136, mean_q: 4.565055
 72492/100000: episode: 1396, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 161.538, mean reward: 4.251 [2.745, 7.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.825, 10.524], loss: 0.474123, mae: 0.546047, mean_q: 4.784915
 72530/100000: episode: 1397, duration: 0.219s, episode steps: 38, steps per second: 174, episode reward: 95.238, mean reward: 2.506 [1.733, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.192, 10.226], loss: 0.696861, mae: 0.505721, mean_q: 4.664509
 72533/100000: episode: 1398, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 7.891, mean reward: 2.630 [2.478, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.666, 10.370], loss: 38.915718, mae: 1.342687, mean_q: 4.754074
 72561/100000: episode: 1399, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 125.640, mean reward: 4.487 [3.027, 13.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.235, 10.543], loss: 0.329341, mae: 0.553975, mean_q: 4.801257
 72599/100000: episode: 1400, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 144.585, mean reward: 3.805 [2.015, 6.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.519, 10.385], loss: 1.214349, mae: 0.600157, mean_q: 4.785389
 72637/100000: episode: 1401, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 122.188, mean reward: 3.215 [2.292, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.533, 10.396], loss: 0.365313, mae: 0.541768, mean_q: 4.805146
 72675/100000: episode: 1402, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 92.743, mean reward: 2.441 [2.006, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-1.015, 10.331], loss: 3.496115, mae: 0.644881, mean_q: 4.770392
 72703/100000: episode: 1403, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 158.529, mean reward: 5.662 [3.542, 20.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.109, 10.555], loss: 1.212974, mae: 0.574276, mean_q: 4.845539
 72706/100000: episode: 1404, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 8.098, mean reward: 2.699 [2.622, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.440], loss: 0.297631, mae: 0.457817, mean_q: 4.356388
 72709/100000: episode: 1405, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 7.709, mean reward: 2.570 [2.293, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.175, 10.416], loss: 0.213031, mae: 0.493836, mean_q: 4.860689
 72712/100000: episode: 1406, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 10.065, mean reward: 3.355 [2.972, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.536], loss: 1.006853, mae: 0.664280, mean_q: 4.966052
 72717/100000: episode: 1407, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 13.910, mean reward: 2.782 [2.363, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.127, 10.367], loss: 0.239128, mae: 0.492406, mean_q: 4.845108
 72755/100000: episode: 1408, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 137.974, mean reward: 3.631 [2.440, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.884, 10.568], loss: 3.515609, mae: 0.660791, mean_q: 4.851228
 72793/100000: episode: 1409, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 94.450, mean reward: 2.486 [1.470, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.746, 10.170], loss: 0.367164, mae: 0.539356, mean_q: 4.836051
 72821/100000: episode: 1410, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 87.052, mean reward: 3.109 [1.961, 7.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.077, 10.375], loss: 0.377308, mae: 0.520704, mean_q: 4.831142
 72859/100000: episode: 1411, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 94.487, mean reward: 2.486 [1.665, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.479, 10.212], loss: 1.416032, mae: 0.654970, mean_q: 4.903352
 72897/100000: episode: 1412, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 141.387, mean reward: 3.721 [2.590, 6.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.270, 10.422], loss: 0.365815, mae: 0.535493, mean_q: 4.873898
 72919/100000: episode: 1413, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 54.522, mean reward: 2.478 [2.052, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.320, 10.397], loss: 5.481257, mae: 0.606471, mean_q: 4.896781
 72957/100000: episode: 1414, duration: 0.226s, episode steps: 38, steps per second: 168, episode reward: 104.178, mean reward: 2.742 [2.059, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.891, 10.396], loss: 0.475166, mae: 0.684138, mean_q: 5.013587
 72962/100000: episode: 1415, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 15.431, mean reward: 3.086 [2.860, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.444], loss: 0.292912, mae: 0.605271, mean_q: 5.230054
 73000/100000: episode: 1416, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 135.380, mean reward: 3.563 [2.575, 5.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.434, 10.399], loss: 0.415401, mae: 0.568516, mean_q: 4.947115
 73022/100000: episode: 1417, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 87.224, mean reward: 3.965 [2.910, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.280, 10.492], loss: 0.645993, mae: 0.540248, mean_q: 4.970398
 73060/100000: episode: 1418, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 95.238, mean reward: 2.506 [1.522, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.342, 10.188], loss: 0.994110, mae: 0.600281, mean_q: 5.055618
 73098/100000: episode: 1419, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 88.136, mean reward: 2.319 [1.719, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.944, 10.233], loss: 0.251156, mae: 0.485365, mean_q: 4.994053
 73120/100000: episode: 1420, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 68.501, mean reward: 3.114 [2.474, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.569, 10.357], loss: 0.387434, mae: 0.570360, mean_q: 4.981336
 73158/100000: episode: 1421, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 115.998, mean reward: 3.053 [2.166, 8.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.925, 10.316], loss: 0.600303, mae: 0.547654, mean_q: 5.048991
 73161/100000: episode: 1422, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 7.964, mean reward: 2.655 [2.352, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.453], loss: 0.262261, mae: 0.535266, mean_q: 5.125266
 73183/100000: episode: 1423, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 103.533, mean reward: 4.706 [2.939, 7.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.732, 10.592], loss: 1.105446, mae: 0.575990, mean_q: 4.962311
 73221/100000: episode: 1424, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 90.441, mean reward: 2.380 [1.538, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.772, 10.163], loss: 3.600137, mae: 0.706499, mean_q: 5.178508
 73243/100000: episode: 1425, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 81.296, mean reward: 3.695 [2.263, 9.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.678], loss: 5.466118, mae: 0.668890, mean_q: 4.923249
 73271/100000: episode: 1426, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 102.513, mean reward: 3.661 [2.866, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.514], loss: 0.674929, mae: 0.614735, mean_q: 5.114684
 73309/100000: episode: 1427, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 112.631, mean reward: 2.964 [1.888, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.053, 10.326], loss: 0.302087, mae: 0.523747, mean_q: 5.048171
 73347/100000: episode: 1428, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 75.891, mean reward: 1.997 [1.500, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.421, 10.111], loss: 0.371426, mae: 0.566002, mean_q: 5.048241
 73369/100000: episode: 1429, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 58.632, mean reward: 2.665 [2.089, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.347, 10.316], loss: 0.366851, mae: 0.542817, mean_q: 5.088569
 73407/100000: episode: 1430, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 124.652, mean reward: 3.280 [1.987, 7.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.502, 10.377], loss: 3.922833, mae: 0.680647, mean_q: 5.248595
 73445/100000: episode: 1431, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 89.197, mean reward: 2.347 [1.489, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.119, 10.210], loss: 0.497414, mae: 0.596705, mean_q: 5.135180
 73483/100000: episode: 1432, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 106.494, mean reward: 2.802 [2.150, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.426, 10.444], loss: 4.159521, mae: 0.738533, mean_q: 5.139983
 73521/100000: episode: 1433, duration: 0.203s, episode steps: 38, steps per second: 188, episode reward: 79.839, mean reward: 2.101 [1.540, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.035, 10.174], loss: 0.320936, mae: 0.521856, mean_q: 5.059425
 73559/100000: episode: 1434, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 89.224, mean reward: 2.348 [1.745, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.077, 10.356], loss: 0.336508, mae: 0.546511, mean_q: 5.157980
 73597/100000: episode: 1435, duration: 0.203s, episode steps: 38, steps per second: 188, episode reward: 118.598, mean reward: 3.121 [1.864, 4.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.856, 10.323], loss: 0.435591, mae: 0.585789, mean_q: 5.161615
 73600/100000: episode: 1436, duration: 0.022s, episode steps: 3, steps per second: 133, episode reward: 12.306, mean reward: 4.102 [3.646, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.567], loss: 0.281439, mae: 0.546258, mean_q: 5.179217
 73638/100000: episode: 1437, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 79.470, mean reward: 2.091 [1.498, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.164, 10.120], loss: 0.289529, mae: 0.509290, mean_q: 5.100363
 73676/100000: episode: 1438, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 98.873, mean reward: 2.602 [1.508, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.585, 10.162], loss: 0.380835, mae: 0.548187, mean_q: 5.124058
 73681/100000: episode: 1439, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 17.091, mean reward: 3.418 [2.907, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.374], loss: 0.657813, mae: 0.609070, mean_q: 5.214853
 73719/100000: episode: 1440, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 105.473, mean reward: 2.776 [1.656, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.334, 10.211], loss: 0.291582, mae: 0.516605, mean_q: 5.171871
 73722/100000: episode: 1441, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 9.602, mean reward: 3.201 [2.684, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.516], loss: 0.374991, mae: 0.607114, mean_q: 4.915531
 73760/100000: episode: 1442, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 79.831, mean reward: 2.101 [1.492, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.735, 10.154], loss: 0.689353, mae: 0.580812, mean_q: 5.150752
 73782/100000: episode: 1443, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 68.627, mean reward: 3.119 [2.436, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.466], loss: 0.955381, mae: 0.656594, mean_q: 5.361776
 73820/100000: episode: 1444, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 124.543, mean reward: 3.277 [1.762, 11.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.207, 10.335], loss: 0.425263, mae: 0.573833, mean_q: 5.236747
 73842/100000: episode: 1445, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 59.659, mean reward: 2.712 [1.723, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.218], loss: 0.366480, mae: 0.575204, mean_q: 5.154267
 73847/100000: episode: 1446, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 15.859, mean reward: 3.172 [2.933, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.458], loss: 0.393970, mae: 0.573430, mean_q: 5.274180
 73885/100000: episode: 1447, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 94.019, mean reward: 2.474 [1.705, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.383, 10.379], loss: 0.971168, mae: 0.623350, mean_q: 5.284572
 73907/100000: episode: 1448, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 69.589, mean reward: 3.163 [2.334, 5.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.553, 10.601], loss: 0.351039, mae: 0.543925, mean_q: 5.273617
[Info] 3-TH LEVEL FOUND: 9.43798828125, Considering 10/90 traces
 73945/100000: episode: 1449, duration: 4.227s, episode steps: 38, steps per second: 9, episode reward: 122.879, mean reward: 3.234 [2.144, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.038, 10.595], loss: 7.099189, mae: 0.906278, mean_q: 5.253361
 73958/100000: episode: 1450, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 41.794, mean reward: 3.215 [2.655, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.448], loss: 0.612568, mae: 0.726338, mean_q: 5.572083
 73972/100000: episode: 1451, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 67.354, mean reward: 4.811 [3.252, 7.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.191, 10.519], loss: 0.479659, mae: 0.645133, mean_q: 5.201801
 73984/100000: episode: 1452, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 46.191, mean reward: 3.849 [3.083, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.289, 10.613], loss: 0.243169, mae: 0.479572, mean_q: 5.008377
 74010/100000: episode: 1453, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 96.579, mean reward: 3.715 [1.817, 5.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.920, 10.375], loss: 0.397261, mae: 0.585667, mean_q: 5.247591
 74035/100000: episode: 1454, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 99.012, mean reward: 3.960 [2.595, 5.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.406, 10.218], loss: 5.815504, mae: 0.931567, mean_q: 5.378052
 74051/100000: episode: 1455, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 52.921, mean reward: 3.308 [2.484, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.357], loss: 0.413834, mae: 0.626696, mean_q: 5.340604
 74067/100000: episode: 1456, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 55.152, mean reward: 3.447 [2.353, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.461], loss: 0.366859, mae: 0.583181, mean_q: 5.360473
 74093/100000: episode: 1457, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 72.511, mean reward: 2.789 [1.670, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.306], loss: 0.948857, mae: 0.582435, mean_q: 5.388813
 74117/100000: episode: 1458, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 120.532, mean reward: 5.022 [2.923, 8.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.555, 10.605], loss: 0.737880, mae: 0.591394, mean_q: 5.321271
 74130/100000: episode: 1459, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 104.510, mean reward: 8.039 [4.290, 16.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.579], loss: 0.247911, mae: 0.556493, mean_q: 5.511472
 74145/100000: episode: 1460, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 322.676, mean reward: 21.512 [3.783, 247.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.016, 10.563], loss: 0.646289, mae: 0.674633, mean_q: 5.320302
 74170/100000: episode: 1461, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 95.193, mean reward: 3.808 [2.404, 11.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.424], loss: 76.151985, mae: 1.657781, mean_q: 5.680313
 74196/100000: episode: 1462, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 150.448, mean reward: 5.786 [3.494, 20.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-2.180, 10.667], loss: 1.319443, mae: 1.210030, mean_q: 5.422088
[Info] FALSIFICATION!
 74203/100000: episode: 1463, duration: 0.260s, episode steps: 7, steps per second: 27, episode reward: 1205.139, mean reward: 172.163 [5.918, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.014, 9.951], loss: 0.694078, mae: 0.897461, mean_q: 6.040417
 74229/100000: episode: 1464, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 1613.981, mean reward: 62.076 [3.222, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.137, 10.725], loss: 0.623463, mae: 0.669754, mean_q: 5.515687
 74252/100000: episode: 1465, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 114.531, mean reward: 4.980 [2.973, 11.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.558], loss: 0.949240, mae: 0.739271, mean_q: 5.633540
 74278/100000: episode: 1466, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 80.099, mean reward: 3.081 [2.388, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.460, 10.437], loss: 47.843571, mae: 1.323374, mean_q: 5.779737
 74302/100000: episode: 1467, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 109.939, mean reward: 4.581 [2.210, 8.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.042, 10.396], loss: 1.889924, mae: 0.996219, mean_q: 5.893023
 74317/100000: episode: 1468, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 62.781, mean reward: 4.185 [3.388, 5.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.710, 10.553], loss: 83.189720, mae: 1.949670, mean_q: 6.883222
 74340/100000: episode: 1469, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 89.438, mean reward: 3.889 [2.652, 7.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.117, 10.408], loss: 14.705899, mae: 1.198359, mean_q: 5.693790
 74352/100000: episode: 1470, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 44.966, mean reward: 3.747 [3.276, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.294, 10.514], loss: 102.629860, mae: 1.530175, mean_q: 5.787089
 74375/100000: episode: 1471, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 150.200, mean reward: 6.530 [3.673, 18.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.395, 10.577], loss: 1.705989, mae: 1.334175, mean_q: 5.944115
 74401/100000: episode: 1472, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 116.129, mean reward: 4.467 [2.199, 11.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.525, 10.424], loss: 12.503077, mae: 1.088444, mean_q: 6.174833
 74413/100000: episode: 1473, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 42.609, mean reward: 3.551 [2.910, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.398, 10.545], loss: 24.820831, mae: 1.050204, mean_q: 5.730840
 74438/100000: episode: 1474, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 76.660, mean reward: 3.066 [2.409, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.268, 10.466], loss: 1.014913, mae: 0.860755, mean_q: 5.870478
 74452/100000: episode: 1475, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 125.322, mean reward: 8.952 [3.588, 26.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.224, 10.558], loss: 2.064938, mae: 0.795997, mean_q: 5.860193
 74464/100000: episode: 1476, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 83.619, mean reward: 6.968 [3.273, 15.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.759, 10.618], loss: 0.645247, mae: 0.691715, mean_q: 5.900493
 74477/100000: episode: 1477, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 149.153, mean reward: 11.473 [3.850, 29.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.722], loss: 1195.836792, mae: 5.258237, mean_q: 8.457104
 74492/100000: episode: 1478, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 68.696, mean reward: 4.580 [3.053, 6.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.575], loss: 1089.333252, mae: 4.028391, mean_q: 6.502908
 74518/100000: episode: 1479, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 151.760, mean reward: 5.837 [3.362, 10.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.197, 10.571], loss: 13.245401, mae: 1.858744, mean_q: 7.109665
 74533/100000: episode: 1480, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 93.960, mean reward: 6.264 [4.210, 9.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.558], loss: 63.159477, mae: 1.477558, mean_q: 6.360002
 74559/100000: episode: 1481, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 96.517, mean reward: 3.712 [2.669, 5.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.612], loss: 64.182419, mae: 1.795794, mean_q: 6.965813
 74575/100000: episode: 1482, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 76.018, mean reward: 4.751 [3.292, 6.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.081, 10.595], loss: 2.911574, mae: 1.019368, mean_q: 6.406282
 74587/100000: episode: 1483, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 46.887, mean reward: 3.907 [2.651, 6.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.418], loss: 0.468531, mae: 0.664630, mean_q: 6.219145
 74613/100000: episode: 1484, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 108.251, mean reward: 4.164 [2.216, 11.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.138, 10.389], loss: 57.670341, mae: 1.465986, mean_q: 6.398629
 74637/100000: episode: 1485, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 169.607, mean reward: 7.067 [3.394, 18.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.179, 10.572], loss: 1285.509644, mae: 5.878634, mean_q: 9.304313
 74650/100000: episode: 1486, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 51.652, mean reward: 3.973 [2.701, 5.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.318, 10.374], loss: 1.477416, mae: 1.238161, mean_q: 5.742313
 74675/100000: episode: 1487, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 96.769, mean reward: 3.871 [2.285, 5.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.433], loss: 25.660654, mae: 1.697186, mean_q: 6.597725
 74690/100000: episode: 1488, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 68.702, mean reward: 4.580 [2.716, 6.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.604], loss: 101.557884, mae: 2.178733, mean_q: 7.425478
 74716/100000: episode: 1489, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 131.080, mean reward: 5.042 [2.824, 8.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.812, 10.504], loss: 642.988586, mae: 3.326132, mean_q: 7.903306
 74740/100000: episode: 1490, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 88.379, mean reward: 3.682 [2.549, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.372, 10.523], loss: 2.886757, mae: 1.495848, mean_q: 6.316723
 74764/100000: episode: 1491, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 66.810, mean reward: 2.784 [1.929, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.156, 10.405], loss: 638.843872, mae: 3.387130, mean_q: 8.150329
 74790/100000: episode: 1492, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 136.911, mean reward: 5.266 [3.025, 8.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.067, 10.476], loss: 1.520349, mae: 1.195711, mean_q: 6.682599
 74814/100000: episode: 1493, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 74.820, mean reward: 3.117 [2.175, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.406, 10.342], loss: 0.993813, mae: 0.865104, mean_q: 6.835536
 74829/100000: episode: 1494, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 101.163, mean reward: 6.744 [3.756, 9.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.384, 10.547], loss: 1.204270, mae: 0.915134, mean_q: 6.473443
 74855/100000: episode: 1495, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 92.426, mean reward: 3.555 [1.865, 6.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.860, 10.347], loss: 50.070671, mae: 1.494182, mean_q: 7.162254
 74881/100000: episode: 1496, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 101.695, mean reward: 3.911 [2.997, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.513, 10.522], loss: 1.123128, mae: 0.812600, mean_q: 6.323606
 74906/100000: episode: 1497, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 77.174, mean reward: 3.087 [2.412, 4.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.051, 10.402], loss: 612.995178, mae: 3.237877, mean_q: 8.263859
 74932/100000: episode: 1498, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 82.221, mean reward: 3.162 [2.106, 6.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.422, 10.413], loss: 1.482607, mae: 0.954738, mean_q: 6.159383
 74958/100000: episode: 1499, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 105.483, mean reward: 4.057 [2.366, 6.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.201, 10.549], loss: 588.465454, mae: 2.999294, mean_q: 7.947320
 74973/100000: episode: 1500, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 55.746, mean reward: 3.716 [2.450, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.518], loss: 1.386945, mae: 0.972325, mean_q: 6.557353
 74998/100000: episode: 1501, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 116.640, mean reward: 4.666 [3.045, 7.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.277, 10.575], loss: 23.886406, mae: 1.248026, mean_q: 6.403648
 75013/100000: episode: 1502, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 62.124, mean reward: 4.142 [3.036, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.619], loss: 20.056318, mae: 1.566749, mean_q: 7.580535
 75039/100000: episode: 1503, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 88.464, mean reward: 3.402 [2.062, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.366], loss: 22.474672, mae: 1.192179, mean_q: 6.735628
 75055/100000: episode: 1504, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 55.987, mean reward: 3.499 [2.520, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.574, 10.446], loss: 965.198242, mae: 2.854876, mean_q: 6.872918
 75071/100000: episode: 1505, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 75.627, mean reward: 4.727 [3.546, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.564], loss: 951.189636, mae: 3.971113, mean_q: 8.421417
 75097/100000: episode: 1506, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 120.975, mean reward: 4.653 [2.896, 6.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.080, 10.564], loss: 49.536320, mae: 1.462453, mean_q: 6.776359
 75109/100000: episode: 1507, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 40.711, mean reward: 3.393 [2.768, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.491], loss: 1289.957153, mae: 5.060227, mean_q: 8.772667
 75124/100000: episode: 1508, duration: 0.090s, episode steps: 15, steps per second: 168, episode reward: 54.781, mean reward: 3.652 [2.892, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.128, 10.594], loss: 2041.262939, mae: 6.581643, mean_q: 8.989373
 75137/100000: episode: 1509, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 54.221, mean reward: 4.171 [3.022, 5.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.920, 10.500], loss: 78.437187, mae: 4.461941, mean_q: 11.020471
 75163/100000: episode: 1510, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 117.979, mean reward: 4.538 [1.969, 6.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.374], loss: 1.251749, mae: 1.177503, mean_q: 6.431759
 75186/100000: episode: 1511, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 134.221, mean reward: 5.836 [2.911, 12.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.111, 10.478], loss: 1.362118, mae: 1.035335, mean_q: 7.521064
 75202/100000: episode: 1512, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 68.571, mean reward: 4.286 [2.869, 6.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.540], loss: 1.753643, mae: 0.853845, mean_q: 6.848632
 75228/100000: episode: 1513, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 119.874, mean reward: 4.611 [2.900, 6.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.724, 10.501], loss: 587.500793, mae: 3.285650, mean_q: 8.870812
 75244/100000: episode: 1514, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 103.221, mean reward: 6.451 [3.571, 13.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.368, 10.608], loss: 19.403391, mae: 1.350186, mean_q: 6.703230
 75258/100000: episode: 1515, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 101.108, mean reward: 7.222 [3.882, 11.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.850, 10.642], loss: 1169.533447, mae: 4.593242, mean_q: 8.398462
 75272/100000: episode: 1516, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 82.854, mean reward: 5.918 [4.434, 9.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.430, 10.627], loss: 91.076462, mae: 2.928751, mean_q: 9.131778
 75298/100000: episode: 1517, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 119.861, mean reward: 4.610 [2.516, 7.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.480], loss: 13.769181, mae: 1.209913, mean_q: 7.011861
 75311/100000: episode: 1518, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 55.777, mean reward: 4.291 [3.006, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.467], loss: 2.050575, mae: 1.122046, mean_q: 7.428264
 75336/100000: episode: 1519, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 92.703, mean reward: 3.708 [2.800, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.601], loss: 50.246967, mae: 1.334430, mean_q: 7.283350
 75351/100000: episode: 1520, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 62.472, mean reward: 4.165 [3.141, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.629], loss: 1.159271, mae: 0.935952, mean_q: 7.225248
 75377/100000: episode: 1521, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 133.478, mean reward: 5.134 [2.122, 13.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.892, 10.237], loss: 1.273113, mae: 0.915612, mean_q: 6.975201
 75403/100000: episode: 1522, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 129.489, mean reward: 4.980 [3.093, 7.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.616], loss: 49.083450, mae: 1.536320, mean_q: 7.519776
 75428/100000: episode: 1523, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 114.843, mean reward: 4.594 [3.162, 9.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.376, 10.458], loss: 619.165771, mae: 2.375268, mean_q: 7.371691
 75452/100000: episode: 1524, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 206.401, mean reward: 8.600 [2.791, 63.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.451, 10.614], loss: 41.651859, mae: 2.415843, mean_q: 8.425216
 75466/100000: episode: 1525, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 70.161, mean reward: 5.011 [2.567, 7.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.059, 10.582], loss: 88.918373, mae: 1.664690, mean_q: 6.700759
 75490/100000: episode: 1526, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 144.707, mean reward: 6.029 [3.458, 10.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.372, 10.494], loss: 658.913940, mae: 4.351624, mean_q: 9.734911
 75503/100000: episode: 1527, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 57.394, mean reward: 4.415 [3.579, 5.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.347, 10.530], loss: 94.877083, mae: 2.109365, mean_q: 6.917373
 75526/100000: episode: 1528, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 99.475, mean reward: 4.325 [3.081, 6.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.086, 10.514], loss: 66.897499, mae: 1.767472, mean_q: 7.900497
 75550/100000: episode: 1529, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 96.073, mean reward: 4.003 [3.190, 5.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.263, 10.563], loss: 14.987713, mae: 1.418810, mean_q: 7.482912
 75576/100000: episode: 1530, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 141.990, mean reward: 5.461 [3.552, 10.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.115, 10.623], loss: 1.851318, mae: 1.048774, mean_q: 7.109238
 75599/100000: episode: 1531, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 157.657, mean reward: 6.855 [4.963, 9.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.684], loss: 41.743748, mae: 1.498572, mean_q: 7.610271
 75625/100000: episode: 1532, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 88.150, mean reward: 3.390 [2.296, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.325, 10.401], loss: 36.669674, mae: 1.388186, mean_q: 7.580497
 75640/100000: episode: 1533, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 96.270, mean reward: 6.418 [4.325, 12.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.459, 10.586], loss: 101.860809, mae: 2.149479, mean_q: 7.530715
 75665/100000: episode: 1534, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 129.680, mean reward: 5.187 [2.900, 7.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.766, 10.655], loss: 50.449886, mae: 1.494003, mean_q: 7.789924
 75691/100000: episode: 1535, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 166.753, mean reward: 6.414 [2.914, 17.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.553], loss: 2.172532, mae: 1.121348, mean_q: 7.498607
 75714/100000: episode: 1536, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 118.170, mean reward: 5.138 [3.300, 9.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.190, 10.549], loss: 3.991869, mae: 1.192203, mean_q: 7.411238
 75729/100000: episode: 1537, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 83.831, mean reward: 5.589 [3.053, 9.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.559], loss: 1032.536743, mae: 4.550693, mean_q: 9.591158
 75752/100000: episode: 1538, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 80.189, mean reward: 3.486 [2.090, 6.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.402], loss: 16.050526, mae: 1.454932, mean_q: 7.554669
[Info] Complete ISplit Iteration
[Info] Levels: [4.845114, 5.984334, 9.437988, 15.160434]
[Info] Cond. Prob: [0.1, 0.11, 0.1, 0.34]
[Info] Error Prob: 0.00037400000000000004

 75775/100000: episode: 1539, duration: 4.380s, episode steps: 23, steps per second: 5, episode reward: 214.940, mean reward: 9.345 [4.426, 19.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.605, 10.580], loss: 675.120789, mae: 3.733309, mean_q: 8.879622
 75875/100000: episode: 1540, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 176.139, mean reward: 1.761 [1.448, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.777, 10.098], loss: 327.865906, mae: 2.303302, mean_q: 8.099195
 75975/100000: episode: 1541, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 197.893, mean reward: 1.979 [1.443, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.098], loss: 471.520508, mae: 2.917684, mean_q: 8.465631
 76075/100000: episode: 1542, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 230.690, mean reward: 2.307 [1.469, 14.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.143, 10.098], loss: 470.812378, mae: 3.067440, mean_q: 8.727521
 76175/100000: episode: 1543, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.839, mean reward: 1.818 [1.443, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.131], loss: 313.867157, mae: 2.387012, mean_q: 8.272624
 76275/100000: episode: 1544, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 182.700, mean reward: 1.827 [1.518, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.098], loss: 350.978424, mae: 2.828667, mean_q: 8.616502
 76375/100000: episode: 1545, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.661, mean reward: 1.917 [1.452, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.274], loss: 4.710620, mae: 1.130133, mean_q: 7.645318
 76475/100000: episode: 1546, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.450, mean reward: 1.895 [1.463, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.031, 10.158], loss: 202.382324, mae: 2.257539, mean_q: 8.022442
 76575/100000: episode: 1547, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.641, mean reward: 1.896 [1.455, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.244, 10.098], loss: 158.203491, mae: 1.768282, mean_q: 7.721773
 76675/100000: episode: 1548, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.746, mean reward: 1.927 [1.448, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.131, 10.301], loss: 171.879883, mae: 1.733618, mean_q: 7.363933
 76775/100000: episode: 1549, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 196.766, mean reward: 1.968 [1.517, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.613, 10.254], loss: 176.475021, mae: 1.876384, mean_q: 7.755179
 76875/100000: episode: 1550, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.141, mean reward: 1.931 [1.485, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.251], loss: 306.743195, mae: 2.107299, mean_q: 7.693279
 76975/100000: episode: 1551, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 183.479, mean reward: 1.835 [1.448, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.880, 10.098], loss: 789.036499, mae: 3.878847, mean_q: 8.809541
 77075/100000: episode: 1552, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 206.633, mean reward: 2.066 [1.457, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.354, 10.119], loss: 313.268585, mae: 2.018547, mean_q: 7.777139
 77175/100000: episode: 1553, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.172, mean reward: 1.952 [1.464, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.395, 10.214], loss: 165.687012, mae: 1.559478, mean_q: 7.225621
 77275/100000: episode: 1554, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 190.226, mean reward: 1.902 [1.470, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.228, 10.153], loss: 23.656195, mae: 1.437100, mean_q: 7.250939
 77375/100000: episode: 1555, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 211.840, mean reward: 2.118 [1.455, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.188, 10.169], loss: 320.391632, mae: 2.353496, mean_q: 7.702983
 77475/100000: episode: 1556, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 216.862, mean reward: 2.169 [1.498, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.722, 10.439], loss: 19.488503, mae: 1.184008, mean_q: 6.769166
 77575/100000: episode: 1557, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 180.480, mean reward: 1.805 [1.452, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.440, 10.173], loss: 180.746490, mae: 1.777555, mean_q: 7.383634
 77675/100000: episode: 1558, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 227.614, mean reward: 2.276 [1.457, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.392, 10.098], loss: 1.399027, mae: 0.875342, mean_q: 6.315601
 77775/100000: episode: 1559, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 199.754, mean reward: 1.998 [1.446, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.242, 10.098], loss: 153.866638, mae: 1.380648, mean_q: 6.724174
 77875/100000: episode: 1560, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.960, mean reward: 1.920 [1.456, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.136, 10.113], loss: 332.261414, mae: 2.227470, mean_q: 7.193000
 77975/100000: episode: 1561, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.172, mean reward: 1.902 [1.448, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.837, 10.098], loss: 177.437927, mae: 1.705580, mean_q: 6.976935
 78075/100000: episode: 1562, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 212.915, mean reward: 2.129 [1.528, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.753, 10.098], loss: 316.392700, mae: 1.980200, mean_q: 7.004419
 78175/100000: episode: 1563, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 200.038, mean reward: 2.000 [1.467, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.127], loss: 342.112000, mae: 2.573652, mean_q: 7.671535
 78275/100000: episode: 1564, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 199.585, mean reward: 1.996 [1.439, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.414, 10.098], loss: 177.676346, mae: 1.734769, mean_q: 7.110358
 78375/100000: episode: 1565, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 196.385, mean reward: 1.964 [1.500, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.375, 10.171], loss: 4.045084, mae: 0.882131, mean_q: 6.339227
 78475/100000: episode: 1566, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 229.958, mean reward: 2.300 [1.438, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.740, 10.474], loss: 168.954529, mae: 1.622702, mean_q: 6.857438
 78575/100000: episode: 1567, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 202.796, mean reward: 2.028 [1.463, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.569, 10.098], loss: 4.618707, mae: 0.891933, mean_q: 6.195935
 78675/100000: episode: 1568, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 182.551, mean reward: 1.826 [1.477, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.892, 10.098], loss: 177.258118, mae: 1.685269, mean_q: 6.666940
 78775/100000: episode: 1569, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.210, mean reward: 1.952 [1.454, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.700, 10.110], loss: 314.693390, mae: 2.008726, mean_q: 7.012437
 78875/100000: episode: 1570, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 220.680, mean reward: 2.207 [1.443, 11.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.954, 10.098], loss: 170.413437, mae: 1.311619, mean_q: 6.475764
 78975/100000: episode: 1571, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 219.469, mean reward: 2.195 [1.452, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.487, 10.132], loss: 340.790558, mae: 2.561558, mean_q: 7.400248
 79075/100000: episode: 1572, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.399, mean reward: 1.934 [1.447, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.188], loss: 173.067535, mae: 1.549130, mean_q: 6.654700
 79175/100000: episode: 1573, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 203.227, mean reward: 2.032 [1.462, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.006, 10.098], loss: 1.542379, mae: 0.745819, mean_q: 6.038476
 79275/100000: episode: 1574, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 205.066, mean reward: 2.051 [1.456, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.180, 10.100], loss: 0.814786, mae: 0.655843, mean_q: 5.673270
 79375/100000: episode: 1575, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 193.219, mean reward: 1.932 [1.457, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.288, 10.262], loss: 0.808508, mae: 0.651041, mean_q: 5.500986
 79475/100000: episode: 1576, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 189.680, mean reward: 1.897 [1.460, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.295, 10.230], loss: 0.731141, mae: 0.618901, mean_q: 5.325425
 79575/100000: episode: 1577, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 188.881, mean reward: 1.889 [1.466, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.442, 10.143], loss: 0.573820, mae: 0.577121, mean_q: 5.204408
 79675/100000: episode: 1578, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 210.026, mean reward: 2.100 [1.441, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.289], loss: 0.506441, mae: 0.543602, mean_q: 5.066355
 79775/100000: episode: 1579, duration: 1.290s, episode steps: 100, steps per second: 78, episode reward: 183.996, mean reward: 1.840 [1.468, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.881, 10.159], loss: 1.946266, mae: 0.582271, mean_q: 4.965441
 79875/100000: episode: 1580, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 181.923, mean reward: 1.819 [1.464, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.005, 10.098], loss: 0.442791, mae: 0.516198, mean_q: 4.857692
 79975/100000: episode: 1581, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.800, mean reward: 1.958 [1.473, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.345, 10.098], loss: 0.560999, mae: 0.527740, mean_q: 4.834345
 80075/100000: episode: 1582, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 215.861, mean reward: 2.159 [1.446, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.346, 10.312], loss: 0.485748, mae: 0.496387, mean_q: 4.725230
 80175/100000: episode: 1583, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 191.401, mean reward: 1.914 [1.441, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.700, 10.109], loss: 0.858635, mae: 0.477495, mean_q: 4.626722
 80275/100000: episode: 1584, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.647, mean reward: 1.896 [1.454, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.590, 10.098], loss: 0.392228, mae: 0.446677, mean_q: 4.520854
 80375/100000: episode: 1585, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.967, mean reward: 1.960 [1.508, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.383, 10.171], loss: 0.341065, mae: 0.430832, mean_q: 4.416471
 80475/100000: episode: 1586, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.941, mean reward: 1.799 [1.453, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.860, 10.098], loss: 0.239352, mae: 0.384999, mean_q: 4.259761
 80575/100000: episode: 1587, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.871, mean reward: 1.839 [1.461, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.385, 10.098], loss: 0.285143, mae: 0.383928, mean_q: 4.162035
 80675/100000: episode: 1588, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 185.307, mean reward: 1.853 [1.466, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.484, 10.098], loss: 0.238103, mae: 0.360854, mean_q: 4.059891
 80775/100000: episode: 1589, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 195.438, mean reward: 1.954 [1.516, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.561, 10.195], loss: 0.132662, mae: 0.323014, mean_q: 3.954274
 80875/100000: episode: 1590, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 195.712, mean reward: 1.957 [1.441, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.953, 10.098], loss: 0.112684, mae: 0.299667, mean_q: 3.914230
 80975/100000: episode: 1591, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 195.888, mean reward: 1.959 [1.471, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.613, 10.098], loss: 0.113984, mae: 0.319468, mean_q: 3.920068
 81075/100000: episode: 1592, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 185.894, mean reward: 1.859 [1.447, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.957, 10.139], loss: 0.105302, mae: 0.314474, mean_q: 3.892063
 81175/100000: episode: 1593, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 185.414, mean reward: 1.854 [1.445, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.763, 10.180], loss: 0.119674, mae: 0.313516, mean_q: 3.907525
 81275/100000: episode: 1594, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 207.435, mean reward: 2.074 [1.540, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.133], loss: 0.102925, mae: 0.310878, mean_q: 3.891750
 81375/100000: episode: 1595, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 196.314, mean reward: 1.963 [1.458, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.114, 10.295], loss: 0.134076, mae: 0.313532, mean_q: 3.924681
 81475/100000: episode: 1596, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 184.976, mean reward: 1.850 [1.463, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.242, 10.164], loss: 0.096451, mae: 0.309637, mean_q: 3.908335
 81575/100000: episode: 1597, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 178.789, mean reward: 1.788 [1.439, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.844, 10.254], loss: 0.126954, mae: 0.318718, mean_q: 3.913136
 81675/100000: episode: 1598, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 182.800, mean reward: 1.828 [1.448, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.767, 10.098], loss: 0.102654, mae: 0.310712, mean_q: 3.896831
 81775/100000: episode: 1599, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 197.871, mean reward: 1.979 [1.480, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.098], loss: 0.110726, mae: 0.314761, mean_q: 3.902328
 81875/100000: episode: 1600, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.790, mean reward: 1.938 [1.493, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.216, 10.248], loss: 0.103910, mae: 0.317559, mean_q: 3.923154
 81975/100000: episode: 1601, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 206.471, mean reward: 2.065 [1.463, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.954, 10.226], loss: 0.095122, mae: 0.307305, mean_q: 3.915041
 82075/100000: episode: 1602, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 191.071, mean reward: 1.911 [1.451, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.098], loss: 0.103909, mae: 0.306785, mean_q: 3.913769
 82175/100000: episode: 1603, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 222.116, mean reward: 2.221 [1.509, 5.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.929, 10.098], loss: 0.085400, mae: 0.294722, mean_q: 3.909503
 82275/100000: episode: 1604, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.923, mean reward: 1.929 [1.459, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.670, 10.098], loss: 0.102781, mae: 0.307253, mean_q: 3.922806
 82375/100000: episode: 1605, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 193.425, mean reward: 1.934 [1.464, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.273, 10.098], loss: 0.105412, mae: 0.311115, mean_q: 3.923376
 82475/100000: episode: 1606, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 175.430, mean reward: 1.754 [1.446, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.217, 10.098], loss: 0.090095, mae: 0.296758, mean_q: 3.893508
 82575/100000: episode: 1607, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.350, mean reward: 1.933 [1.487, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.408, 10.098], loss: 0.092024, mae: 0.293329, mean_q: 3.876043
 82675/100000: episode: 1608, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 182.031, mean reward: 1.820 [1.460, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.623, 10.098], loss: 0.083717, mae: 0.287739, mean_q: 3.877107
 82775/100000: episode: 1609, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 202.525, mean reward: 2.025 [1.471, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.519, 10.129], loss: 0.084123, mae: 0.292526, mean_q: 3.866534
 82875/100000: episode: 1610, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 202.404, mean reward: 2.024 [1.460, 8.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.886, 10.183], loss: 0.085969, mae: 0.297024, mean_q: 3.878101
 82975/100000: episode: 1611, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 208.339, mean reward: 2.083 [1.504, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.069, 10.098], loss: 0.137618, mae: 0.321685, mean_q: 3.915776
 83075/100000: episode: 1612, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 207.834, mean reward: 2.078 [1.524, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.067, 10.098], loss: 0.101837, mae: 0.304275, mean_q: 3.886656
 83175/100000: episode: 1613, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 195.647, mean reward: 1.956 [1.465, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.572, 10.354], loss: 0.103306, mae: 0.296364, mean_q: 3.891845
 83275/100000: episode: 1614, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 212.952, mean reward: 2.130 [1.523, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.991, 10.098], loss: 0.109280, mae: 0.309153, mean_q: 3.904499
 83375/100000: episode: 1615, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 190.928, mean reward: 1.909 [1.444, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.415, 10.200], loss: 0.093025, mae: 0.297125, mean_q: 3.901620
 83475/100000: episode: 1616, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 191.126, mean reward: 1.911 [1.477, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.283, 10.098], loss: 0.103695, mae: 0.301865, mean_q: 3.876473
 83575/100000: episode: 1617, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.005, mean reward: 1.900 [1.440, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.125, 10.213], loss: 0.089273, mae: 0.282470, mean_q: 3.864243
 83675/100000: episode: 1618, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.072, mean reward: 1.931 [1.447, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.296, 10.217], loss: 0.110688, mae: 0.307791, mean_q: 3.871603
 83775/100000: episode: 1619, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 209.251, mean reward: 2.093 [1.450, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.106], loss: 0.090446, mae: 0.293015, mean_q: 3.863401
 83875/100000: episode: 1620, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 189.435, mean reward: 1.894 [1.446, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.117, 10.253], loss: 0.123967, mae: 0.302697, mean_q: 3.877821
 83975/100000: episode: 1621, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 202.124, mean reward: 2.021 [1.495, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.551, 10.098], loss: 0.101447, mae: 0.295054, mean_q: 3.865247
 84075/100000: episode: 1622, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 191.851, mean reward: 1.919 [1.456, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.198, 10.098], loss: 0.085518, mae: 0.291034, mean_q: 3.880707
 84175/100000: episode: 1623, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 199.118, mean reward: 1.991 [1.501, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.969, 10.098], loss: 0.087580, mae: 0.290918, mean_q: 3.851171
 84275/100000: episode: 1624, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 209.269, mean reward: 2.093 [1.449, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.789, 10.098], loss: 0.084505, mae: 0.290300, mean_q: 3.861188
 84375/100000: episode: 1625, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 212.783, mean reward: 2.128 [1.481, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.317], loss: 0.076153, mae: 0.282850, mean_q: 3.858662
 84475/100000: episode: 1626, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 185.964, mean reward: 1.860 [1.508, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.605, 10.098], loss: 0.078138, mae: 0.284379, mean_q: 3.839385
 84575/100000: episode: 1627, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 179.894, mean reward: 1.799 [1.456, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.606, 10.098], loss: 0.075242, mae: 0.277909, mean_q: 3.846591
 84675/100000: episode: 1628, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.899, mean reward: 1.909 [1.440, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.809, 10.272], loss: 0.103670, mae: 0.299239, mean_q: 3.863375
 84775/100000: episode: 1629, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 213.011, mean reward: 2.130 [1.457, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.685, 10.455], loss: 0.079515, mae: 0.291053, mean_q: 3.856270
 84875/100000: episode: 1630, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 186.960, mean reward: 1.870 [1.441, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.332, 10.317], loss: 0.095425, mae: 0.301198, mean_q: 3.870667
 84975/100000: episode: 1631, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.354, mean reward: 1.984 [1.508, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.320, 10.098], loss: 0.087420, mae: 0.285232, mean_q: 3.862857
 85075/100000: episode: 1632, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.682, mean reward: 1.907 [1.458, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.823, 10.098], loss: 0.092091, mae: 0.291491, mean_q: 3.857038
 85175/100000: episode: 1633, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 201.066, mean reward: 2.011 [1.478, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.612, 10.283], loss: 0.101418, mae: 0.297886, mean_q: 3.862394
 85275/100000: episode: 1634, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 272.165, mean reward: 2.722 [1.457, 9.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.040, 10.098], loss: 0.100423, mae: 0.298789, mean_q: 3.862283
 85375/100000: episode: 1635, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 181.554, mean reward: 1.816 [1.451, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.348, 10.155], loss: 0.101943, mae: 0.303244, mean_q: 3.906759
 85475/100000: episode: 1636, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 204.516, mean reward: 2.045 [1.458, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.593, 10.098], loss: 0.116859, mae: 0.305150, mean_q: 3.894466
 85575/100000: episode: 1637, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 179.324, mean reward: 1.793 [1.456, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.879, 10.247], loss: 0.090067, mae: 0.292665, mean_q: 3.884978
 85675/100000: episode: 1638, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.180, mean reward: 2.052 [1.445, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.807, 10.137], loss: 0.085297, mae: 0.299117, mean_q: 3.886879
[Info] 1-TH LEVEL FOUND: 5.35355281829834, Considering 10/90 traces
 85775/100000: episode: 1639, duration: 4.829s, episode steps: 100, steps per second: 21, episode reward: 185.989, mean reward: 1.860 [1.489, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.878, 10.206], loss: 0.097163, mae: 0.300961, mean_q: 3.918839
 85784/100000: episode: 1640, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 28.643, mean reward: 3.183 [2.429, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.348, 10.100], loss: 0.106212, mae: 0.304159, mean_q: 3.867532
 85794/100000: episode: 1641, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 23.599, mean reward: 2.360 [2.010, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.325], loss: 0.076218, mae: 0.289256, mean_q: 3.889850
 85812/100000: episode: 1642, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 57.548, mean reward: 3.197 [2.226, 4.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.283, 10.100], loss: 0.084631, mae: 0.305688, mean_q: 3.899463
 85830/100000: episode: 1643, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 54.366, mean reward: 3.020 [2.384, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.580, 10.100], loss: 0.097464, mae: 0.316920, mean_q: 3.890406
 85839/100000: episode: 1644, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 24.044, mean reward: 2.672 [2.149, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.302, 10.100], loss: 0.092302, mae: 0.283246, mean_q: 3.876656
 85857/100000: episode: 1645, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 75.648, mean reward: 4.203 [2.357, 6.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.479, 10.100], loss: 0.154361, mae: 0.344545, mean_q: 3.969687
 85882/100000: episode: 1646, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 110.542, mean reward: 4.422 [2.681, 15.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.494, 10.100], loss: 0.108841, mae: 0.303473, mean_q: 3.919365
 85890/100000: episode: 1647, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 19.014, mean reward: 2.377 [1.726, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.270, 10.100], loss: 0.124262, mae: 0.298007, mean_q: 3.957654
 85902/100000: episode: 1648, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 31.813, mean reward: 2.651 [2.294, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.251, 10.100], loss: 0.103605, mae: 0.321950, mean_q: 3.954169
 85910/100000: episode: 1649, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 16.394, mean reward: 2.049 [1.836, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.268, 10.100], loss: 0.078395, mae: 0.271668, mean_q: 3.926773
 85935/100000: episode: 1650, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 107.108, mean reward: 4.284 [2.550, 7.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.115, 10.100], loss: 0.091155, mae: 0.305853, mean_q: 3.964741
 85945/100000: episode: 1651, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 22.868, mean reward: 2.287 [2.090, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.362], loss: 0.126188, mae: 0.333901, mean_q: 3.951719
 85954/100000: episode: 1652, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 20.966, mean reward: 2.330 [1.923, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.498, 10.100], loss: 0.097603, mae: 0.301524, mean_q: 3.970521
 85966/100000: episode: 1653, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 30.910, mean reward: 2.576 [2.223, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.258, 10.100], loss: 0.117080, mae: 0.320865, mean_q: 3.950124
 85974/100000: episode: 1654, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 19.108, mean reward: 2.388 [2.060, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.286, 10.100], loss: 0.091830, mae: 0.290593, mean_q: 3.996395
 85992/100000: episode: 1655, duration: 0.133s, episode steps: 18, steps per second: 135, episode reward: 77.175, mean reward: 4.287 [2.540, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.517, 10.100], loss: 0.125882, mae: 0.315236, mean_q: 3.970340
 86002/100000: episode: 1656, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 27.032, mean reward: 2.703 [2.095, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.448], loss: 0.170316, mae: 0.340132, mean_q: 3.949537
 86011/100000: episode: 1657, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 22.790, mean reward: 2.532 [1.950, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.133, 10.100], loss: 0.088323, mae: 0.299659, mean_q: 4.020441
 86029/100000: episode: 1658, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 92.025, mean reward: 5.112 [2.757, 9.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.996, 10.100], loss: 0.128283, mae: 0.347426, mean_q: 4.027651
 86046/100000: episode: 1659, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 52.302, mean reward: 3.077 [1.822, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.544, 10.100], loss: 0.086543, mae: 0.295777, mean_q: 3.930017
 86058/100000: episode: 1660, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 36.593, mean reward: 3.049 [2.327, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.443, 10.100], loss: 0.161544, mae: 0.382338, mean_q: 4.096522
 86083/100000: episode: 1661, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 81.112, mean reward: 3.244 [2.508, 10.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.808, 10.477], loss: 0.181532, mae: 0.337113, mean_q: 4.035292
 86101/100000: episode: 1662, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 66.523, mean reward: 3.696 [2.738, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.423, 10.100], loss: 0.158918, mae: 0.355098, mean_q: 4.053883
 86125/100000: episode: 1663, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 64.058, mean reward: 2.669 [1.545, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.071, 10.245], loss: 0.094274, mae: 0.318497, mean_q: 4.060179
 86134/100000: episode: 1664, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 19.326, mean reward: 2.147 [1.831, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.133, 10.100], loss: 0.183938, mae: 0.368426, mean_q: 4.014372
 86159/100000: episode: 1665, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 73.949, mean reward: 2.958 [2.191, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.068, 10.331], loss: 0.110845, mae: 0.320952, mean_q: 4.050590
 86177/100000: episode: 1666, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 55.097, mean reward: 3.061 [2.297, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.458, 10.100], loss: 0.167501, mae: 0.345960, mean_q: 4.050822
 86194/100000: episode: 1667, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 41.942, mean reward: 2.467 [1.781, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.590, 10.100], loss: 0.162772, mae: 0.359560, mean_q: 4.118387
 86203/100000: episode: 1668, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 18.780, mean reward: 2.087 [1.862, 2.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.213, 10.100], loss: 0.123257, mae: 0.330912, mean_q: 4.091410
 86212/100000: episode: 1669, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 22.490, mean reward: 2.499 [2.103, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.215, 10.100], loss: 0.183729, mae: 0.354459, mean_q: 4.145640
 86237/100000: episode: 1670, duration: 0.155s, episode steps: 25, steps per second: 161, episode reward: 91.768, mean reward: 3.671 [2.410, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.369, 10.596], loss: 0.110019, mae: 0.329432, mean_q: 4.005643
 86262/100000: episode: 1671, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 71.867, mean reward: 2.875 [1.746, 4.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.267], loss: 0.104320, mae: 0.310735, mean_q: 4.038980
 86286/100000: episode: 1672, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 50.607, mean reward: 2.109 [1.593, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.288, 10.181], loss: 0.108943, mae: 0.324514, mean_q: 4.059744
 86295/100000: episode: 1673, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 21.907, mean reward: 2.434 [2.028, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.428, 10.100], loss: 0.145966, mae: 0.361947, mean_q: 4.152511
 86303/100000: episode: 1674, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 19.686, mean reward: 2.461 [2.096, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.343, 10.100], loss: 0.122189, mae: 0.318399, mean_q: 4.141649
 86327/100000: episode: 1675, duration: 0.162s, episode steps: 24, steps per second: 148, episode reward: 44.943, mean reward: 1.873 [1.574, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.136, 10.300], loss: 0.125224, mae: 0.337106, mean_q: 4.118829
 86336/100000: episode: 1676, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 23.226, mean reward: 2.581 [1.945, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.350, 10.100], loss: 0.287731, mae: 0.452800, mean_q: 4.212026
 86345/100000: episode: 1677, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 20.480, mean reward: 2.276 [2.089, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.151, 10.100], loss: 0.114605, mae: 0.329015, mean_q: 4.020546
 86363/100000: episode: 1678, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 65.411, mean reward: 3.634 [2.507, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.270, 10.100], loss: 0.105041, mae: 0.343788, mean_q: 4.166158
 86387/100000: episode: 1679, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 68.824, mean reward: 2.868 [2.011, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.338, 10.348], loss: 0.153904, mae: 0.363588, mean_q: 4.126015
 86411/100000: episode: 1680, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 53.071, mean reward: 2.211 [1.785, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.521, 10.360], loss: 0.116278, mae: 0.340578, mean_q: 4.077297
 86419/100000: episode: 1681, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 16.706, mean reward: 2.088 [1.782, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.309, 10.100], loss: 0.094704, mae: 0.301052, mean_q: 4.066500
 86428/100000: episode: 1682, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 25.687, mean reward: 2.854 [2.334, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.340, 10.100], loss: 0.114692, mae: 0.337197, mean_q: 4.050873
 86446/100000: episode: 1683, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 50.064, mean reward: 2.781 [1.865, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.178, 10.100], loss: 0.143995, mae: 0.352157, mean_q: 4.116905
 86463/100000: episode: 1684, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 42.960, mean reward: 2.527 [2.124, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.269, 10.100], loss: 0.182215, mae: 0.374888, mean_q: 4.157391
 86488/100000: episode: 1685, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 83.261, mean reward: 3.330 [2.618, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.777, 10.454], loss: 0.260671, mae: 0.411857, mean_q: 4.172637
 86497/100000: episode: 1686, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 17.972, mean reward: 1.997 [1.764, 2.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.268, 10.100], loss: 0.165173, mae: 0.373622, mean_q: 4.177732
 86509/100000: episode: 1687, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 30.378, mean reward: 2.531 [2.219, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.208, 10.100], loss: 0.167174, mae: 0.365773, mean_q: 4.102158
 86533/100000: episode: 1688, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 64.049, mean reward: 2.669 [2.235, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.508, 10.382], loss: 0.132752, mae: 0.353141, mean_q: 4.189930
 86557/100000: episode: 1689, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 83.730, mean reward: 3.489 [2.639, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.277, 10.426], loss: 0.126857, mae: 0.363860, mean_q: 4.238220
 86569/100000: episode: 1690, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 32.784, mean reward: 2.732 [2.085, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.111, 10.100], loss: 0.271036, mae: 0.398582, mean_q: 4.271481
 86593/100000: episode: 1691, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 54.207, mean reward: 2.259 [1.900, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.312, 10.318], loss: 0.181771, mae: 0.377395, mean_q: 4.195449
 86602/100000: episode: 1692, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 23.894, mean reward: 2.655 [1.981, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.221, 10.100], loss: 0.144140, mae: 0.360222, mean_q: 4.133152
 86626/100000: episode: 1693, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 56.609, mean reward: 2.359 [1.591, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.166, 10.256], loss: 0.140255, mae: 0.361583, mean_q: 4.243240
 86638/100000: episode: 1694, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 33.761, mean reward: 2.813 [2.407, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.354, 10.100], loss: 0.316246, mae: 0.365171, mean_q: 4.188536
 86662/100000: episode: 1695, duration: 0.145s, episode steps: 24, steps per second: 165, episode reward: 53.713, mean reward: 2.238 [1.700, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.031, 10.195], loss: 0.155184, mae: 0.390980, mean_q: 4.201974
 86671/100000: episode: 1696, duration: 0.062s, episode steps: 9, steps per second: 144, episode reward: 21.249, mean reward: 2.361 [1.796, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.221, 10.100], loss: 0.296160, mae: 0.410859, mean_q: 4.147669
 86689/100000: episode: 1697, duration: 0.122s, episode steps: 18, steps per second: 148, episode reward: 67.986, mean reward: 3.777 [2.970, 5.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.367, 10.100], loss: 0.153461, mae: 0.372642, mean_q: 4.272712
 86714/100000: episode: 1698, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 75.737, mean reward: 3.029 [2.251, 6.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.275, 10.100], loss: 0.147391, mae: 0.364139, mean_q: 4.225283
 86723/100000: episode: 1699, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 36.739, mean reward: 4.082 [2.192, 6.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.484, 10.100], loss: 0.250007, mae: 0.398067, mean_q: 4.356249
 86740/100000: episode: 1700, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 40.877, mean reward: 2.405 [1.928, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.290, 10.100], loss: 0.199886, mae: 0.371479, mean_q: 4.243832
 86757/100000: episode: 1701, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 53.953, mean reward: 3.174 [1.885, 5.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.513, 10.100], loss: 0.260818, mae: 0.406364, mean_q: 4.293233
 86769/100000: episode: 1702, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 33.810, mean reward: 2.817 [2.396, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.306, 10.100], loss: 0.245858, mae: 0.442593, mean_q: 4.497513
 86777/100000: episode: 1703, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 15.163, mean reward: 1.895 [1.679, 2.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.118, 10.100], loss: 0.245825, mae: 0.399260, mean_q: 4.163482
 86787/100000: episode: 1704, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 24.227, mean reward: 2.423 [2.089, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.091, 10.363], loss: 0.220611, mae: 0.429984, mean_q: 4.278297
 86812/100000: episode: 1705, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 60.965, mean reward: 2.439 [1.838, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.136, 10.254], loss: 0.159111, mae: 0.387503, mean_q: 4.239776
 86824/100000: episode: 1706, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 41.593, mean reward: 3.466 [2.749, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.340, 10.100], loss: 0.166029, mae: 0.375292, mean_q: 4.244812
 86836/100000: episode: 1707, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 30.185, mean reward: 2.515 [2.243, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.250, 10.100], loss: 0.274810, mae: 0.435578, mean_q: 4.381501
 86861/100000: episode: 1708, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 70.099, mean reward: 2.804 [1.867, 7.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.379, 10.100], loss: 0.226477, mae: 0.395994, mean_q: 4.249520
 86869/100000: episode: 1709, duration: 0.062s, episode steps: 8, steps per second: 130, episode reward: 17.272, mean reward: 2.159 [1.926, 2.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.545, 10.100], loss: 0.116018, mae: 0.355913, mean_q: 4.346925
 86879/100000: episode: 1710, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 29.057, mean reward: 2.906 [2.083, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.475, 10.470], loss: 0.165540, mae: 0.407311, mean_q: 4.352880
 86888/100000: episode: 1711, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 22.384, mean reward: 2.487 [2.324, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.330, 10.100], loss: 0.154927, mae: 0.370369, mean_q: 4.176142
 86913/100000: episode: 1712, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 73.964, mean reward: 2.959 [1.865, 6.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.723, 10.362], loss: 0.261352, mae: 0.444880, mean_q: 4.431557
 86923/100000: episode: 1713, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 39.972, mean reward: 3.997 [2.204, 8.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.701, 10.282], loss: 0.223318, mae: 0.393460, mean_q: 4.240979
 86948/100000: episode: 1714, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 64.928, mean reward: 2.597 [1.856, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.330], loss: 0.183156, mae: 0.385991, mean_q: 4.362695
 86972/100000: episode: 1715, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 73.234, mean reward: 3.051 [2.410, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.381], loss: 0.167184, mae: 0.378350, mean_q: 4.246809
 86982/100000: episode: 1716, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 28.024, mean reward: 2.802 [2.085, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.126, 10.388], loss: 0.148298, mae: 0.368313, mean_q: 4.326905
 86991/100000: episode: 1717, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 20.970, mean reward: 2.330 [2.035, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.387, 10.100], loss: 0.254882, mae: 0.384135, mean_q: 4.318704
 87015/100000: episode: 1718, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 51.580, mean reward: 2.149 [1.797, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.064, 10.312], loss: 0.247391, mae: 0.413887, mean_q: 4.345301
 87040/100000: episode: 1719, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 87.651, mean reward: 3.506 [1.893, 6.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.328], loss: 0.219034, mae: 0.398354, mean_q: 4.403533
 87064/100000: episode: 1720, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 60.551, mean reward: 2.523 [1.839, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.386, 10.304], loss: 0.190990, mae: 0.414115, mean_q: 4.324771
 87081/100000: episode: 1721, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 46.699, mean reward: 2.747 [1.821, 4.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.820, 10.100], loss: 0.128924, mae: 0.350910, mean_q: 4.298159
 87105/100000: episode: 1722, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 53.605, mean reward: 2.234 [1.646, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.116, 10.234], loss: 0.183238, mae: 0.383434, mean_q: 4.366217
 87113/100000: episode: 1723, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 20.294, mean reward: 2.537 [1.894, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.510, 10.100], loss: 0.147356, mae: 0.353074, mean_q: 4.310775
 87138/100000: episode: 1724, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 69.185, mean reward: 2.767 [1.619, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.383, 10.226], loss: 0.186724, mae: 0.409573, mean_q: 4.405624
 87155/100000: episode: 1725, duration: 0.124s, episode steps: 17, steps per second: 138, episode reward: 48.936, mean reward: 2.879 [2.265, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.224, 10.100], loss: 0.230626, mae: 0.397322, mean_q: 4.318615
 87165/100000: episode: 1726, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 21.688, mean reward: 2.169 [1.741, 2.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.464, 10.352], loss: 0.216528, mae: 0.421632, mean_q: 4.399364
 87175/100000: episode: 1727, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 26.605, mean reward: 2.661 [2.277, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.395], loss: 0.155310, mae: 0.347505, mean_q: 4.328752
 87200/100000: episode: 1728, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 76.808, mean reward: 3.072 [1.705, 5.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.221, 10.100], loss: 0.202131, mae: 0.406291, mean_q: 4.348713
[Info] 2-TH LEVEL FOUND: 7.905514240264893, Considering 10/90 traces
 87225/100000: episode: 1729, duration: 4.547s, episode steps: 25, steps per second: 5, episode reward: 90.580, mean reward: 3.623 [2.030, 5.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.263, 10.100], loss: 0.164356, mae: 0.380635, mean_q: 4.336034
 87242/100000: episode: 1730, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 55.738, mean reward: 3.279 [2.460, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.747, 10.506], loss: 0.161510, mae: 0.381355, mean_q: 4.370807
 87262/100000: episode: 1731, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 63.419, mean reward: 3.171 [1.760, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.298], loss: 0.245321, mae: 0.445824, mean_q: 4.380675
 87278/100000: episode: 1732, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 45.737, mean reward: 2.859 [2.235, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.681, 10.364], loss: 0.183477, mae: 0.401864, mean_q: 4.365776
 87287/100000: episode: 1733, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 37.004, mean reward: 4.112 [2.987, 6.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.636, 10.100], loss: 0.135764, mae: 0.376949, mean_q: 4.334165
 87303/100000: episode: 1734, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 72.129, mean reward: 4.508 [2.986, 5.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.563], loss: 0.168614, mae: 0.408626, mean_q: 4.509470
 87313/100000: episode: 1735, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 70.237, mean reward: 7.024 [3.157, 27.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.484, 10.100], loss: 0.117006, mae: 0.323132, mean_q: 4.329808
 87329/100000: episode: 1736, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 52.690, mean reward: 3.293 [2.591, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.669, 10.398], loss: 0.139710, mae: 0.365304, mean_q: 4.314830
 87341/100000: episode: 1737, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 51.003, mean reward: 4.250 [2.957, 7.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.171, 10.100], loss: 0.158007, mae: 0.380803, mean_q: 4.430233
 87353/100000: episode: 1738, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 54.310, mean reward: 4.526 [3.383, 6.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.251, 10.100], loss: 0.183312, mae: 0.423308, mean_q: 4.325991
 87363/100000: episode: 1739, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 53.661, mean reward: 5.366 [4.289, 7.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.439, 10.100], loss: 0.367659, mae: 0.477144, mean_q: 4.644113
 87376/100000: episode: 1740, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 107.004, mean reward: 8.231 [3.456, 15.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.641, 10.100], loss: 0.224217, mae: 0.460844, mean_q: 4.460685
 87389/100000: episode: 1741, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 76.474, mean reward: 5.883 [2.987, 13.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.511, 10.100], loss: 0.187433, mae: 0.417348, mean_q: 4.479590
 87406/100000: episode: 1742, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 66.373, mean reward: 3.904 [2.844, 7.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.318, 10.545], loss: 0.382521, mae: 0.478425, mean_q: 4.599234
 87418/100000: episode: 1743, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 82.043, mean reward: 6.837 [2.910, 34.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.403, 10.100], loss: 0.300766, mae: 0.472606, mean_q: 4.444775
 87435/100000: episode: 1744, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 111.632, mean reward: 6.567 [3.327, 15.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.554, 10.696], loss: 0.307649, mae: 0.445211, mean_q: 4.607836
 87447/100000: episode: 1745, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 48.083, mean reward: 4.007 [2.685, 5.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.389, 10.100], loss: 0.435617, mae: 0.483270, mean_q: 4.542754
 87459/100000: episode: 1746, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 62.832, mean reward: 5.236 [4.115, 7.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.510, 10.100], loss: 0.235212, mae: 0.443890, mean_q: 4.602376
 87476/100000: episode: 1747, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 67.632, mean reward: 3.978 [2.891, 5.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.613, 10.498], loss: 0.954261, mae: 0.495518, mean_q: 4.556350
 87485/100000: episode: 1748, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 29.159, mean reward: 3.240 [2.408, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.604, 10.100], loss: 0.267061, mae: 0.501165, mean_q: 4.594751
 87497/100000: episode: 1749, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 66.908, mean reward: 5.576 [3.422, 12.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.439, 10.100], loss: 0.541471, mae: 0.539484, mean_q: 4.649759
 87507/100000: episode: 1750, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 36.128, mean reward: 3.613 [3.156, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.213, 10.100], loss: 0.335521, mae: 0.515429, mean_q: 4.702680
 87517/100000: episode: 1751, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 50.740, mean reward: 5.074 [3.947, 5.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.413, 10.100], loss: 0.337074, mae: 0.497373, mean_q: 4.718184
 87529/100000: episode: 1752, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 59.484, mean reward: 4.957 [3.269, 6.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.639, 10.100], loss: 0.289654, mae: 0.485315, mean_q: 4.772056
 87545/100000: episode: 1753, duration: 0.116s, episode steps: 16, steps per second: 138, episode reward: 51.630, mean reward: 3.227 [2.300, 5.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.762, 10.456], loss: 0.254511, mae: 0.438863, mean_q: 4.512767
 87561/100000: episode: 1754, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 61.793, mean reward: 3.862 [2.961, 5.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.295, 10.529], loss: 0.387916, mae: 0.534475, mean_q: 4.775611
 87581/100000: episode: 1755, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 95.934, mean reward: 4.797 [3.301, 6.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.249, 10.525], loss: 0.299092, mae: 0.487310, mean_q: 4.616624
 87601/100000: episode: 1756, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 71.400, mean reward: 3.570 [2.194, 5.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.917, 10.473], loss: 0.357473, mae: 0.543896, mean_q: 4.923330
 87613/100000: episode: 1757, duration: 0.090s, episode steps: 12, steps per second: 133, episode reward: 45.960, mean reward: 3.830 [2.980, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.148, 10.100], loss: 0.126514, mae: 0.355970, mean_q: 4.584136
 87623/100000: episode: 1758, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 59.403, mean reward: 5.940 [3.578, 16.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.445, 10.100], loss: 0.389829, mae: 0.486592, mean_q: 4.751378
 87635/100000: episode: 1759, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 36.333, mean reward: 3.028 [2.576, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.320, 10.100], loss: 0.175091, mae: 0.416465, mean_q: 4.663065
 87652/100000: episode: 1760, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 56.723, mean reward: 3.337 [2.811, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.463], loss: 0.217654, mae: 0.456018, mean_q: 4.696723
 87668/100000: episode: 1761, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 53.333, mean reward: 3.333 [2.149, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.199, 10.451], loss: 0.852304, mae: 0.520112, mean_q: 4.683245
 87688/100000: episode: 1762, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 106.845, mean reward: 5.342 [3.241, 8.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.662, 10.486], loss: 0.359575, mae: 0.518198, mean_q: 4.831670
 87701/100000: episode: 1763, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 46.071, mean reward: 3.544 [3.000, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.359, 10.100], loss: 0.523140, mae: 0.547076, mean_q: 4.854908
 87710/100000: episode: 1764, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 28.137, mean reward: 3.126 [2.618, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.357, 10.100], loss: 0.366780, mae: 0.581985, mean_q: 5.001338
 87726/100000: episode: 1765, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 34.246, mean reward: 2.140 [1.856, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.092, 10.313], loss: 0.306705, mae: 0.510029, mean_q: 4.611605
 87746/100000: episode: 1766, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 69.147, mean reward: 3.457 [2.449, 6.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.445], loss: 0.360871, mae: 0.510368, mean_q: 4.800299
 87758/100000: episode: 1767, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 43.479, mean reward: 3.623 [2.834, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.327, 10.100], loss: 0.299555, mae: 0.481586, mean_q: 4.756431
 87774/100000: episode: 1768, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 46.233, mean reward: 2.890 [2.571, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.799, 10.404], loss: 0.369973, mae: 0.507128, mean_q: 4.898773
 87783/100000: episode: 1769, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 45.783, mean reward: 5.087 [2.745, 8.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.620, 10.100], loss: 0.338615, mae: 0.471186, mean_q: 4.837406
 87803/100000: episode: 1770, duration: 0.208s, episode steps: 20, steps per second: 96, episode reward: 54.846, mean reward: 2.742 [1.832, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.301], loss: 0.971917, mae: 0.586048, mean_q: 4.924925
 87823/100000: episode: 1771, duration: 0.138s, episode steps: 20, steps per second: 145, episode reward: 154.708, mean reward: 7.735 [3.478, 13.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.068, 10.626], loss: 0.910376, mae: 0.590940, mean_q: 4.978597
 87843/100000: episode: 1772, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 368.086, mean reward: 18.404 [3.690, 253.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.836, 10.522], loss: 0.630333, mae: 0.592530, mean_q: 4.872631
 87856/100000: episode: 1773, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 69.497, mean reward: 5.346 [3.437, 9.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.433, 10.100], loss: 1.250913, mae: 0.562744, mean_q: 4.856209
 87876/100000: episode: 1774, duration: 0.196s, episode steps: 20, steps per second: 102, episode reward: 71.878, mean reward: 3.594 [2.736, 5.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.601, 10.558], loss: 0.454328, mae: 0.552899, mean_q: 5.066010
 87889/100000: episode: 1775, duration: 0.151s, episode steps: 13, steps per second: 86, episode reward: 74.014, mean reward: 5.693 [3.382, 10.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.494, 10.100], loss: 0.250378, mae: 0.467539, mean_q: 4.914945
 87901/100000: episode: 1776, duration: 0.137s, episode steps: 12, steps per second: 87, episode reward: 65.799, mean reward: 5.483 [3.469, 10.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.434, 10.100], loss: 0.227500, mae: 0.459675, mean_q: 4.878283
 87917/100000: episode: 1777, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 42.951, mean reward: 2.684 [2.324, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.675, 10.403], loss: 0.337413, mae: 0.518861, mean_q: 4.961663
 87933/100000: episode: 1778, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 49.260, mean reward: 3.079 [1.961, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.389], loss: 0.510387, mae: 0.574905, mean_q: 5.091775
 87945/100000: episode: 1779, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 40.611, mean reward: 3.384 [2.590, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.492, 10.100], loss: 0.482362, mae: 0.538134, mean_q: 4.992268
 87954/100000: episode: 1780, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 40.905, mean reward: 4.545 [2.848, 6.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.508, 10.100], loss: 0.418522, mae: 0.573740, mean_q: 5.138159
 87970/100000: episode: 1781, duration: 0.184s, episode steps: 16, steps per second: 87, episode reward: 51.283, mean reward: 3.205 [2.588, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.303, 10.483], loss: 1.034898, mae: 0.618532, mean_q: 5.181476
 87982/100000: episode: 1782, duration: 0.129s, episode steps: 12, steps per second: 93, episode reward: 84.825, mean reward: 7.069 [3.449, 14.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.565, 10.100], loss: 0.687164, mae: 0.583083, mean_q: 5.007048
 88002/100000: episode: 1783, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 93.054, mean reward: 4.653 [2.036, 14.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.488], loss: 0.628190, mae: 0.599481, mean_q: 5.061887
 88019/100000: episode: 1784, duration: 0.099s, episode steps: 17, steps per second: 173, episode reward: 53.474, mean reward: 3.146 [2.766, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.103, 10.486], loss: 0.785787, mae: 0.627839, mean_q: 5.019195
 88029/100000: episode: 1785, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 42.995, mean reward: 4.300 [3.249, 7.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.340, 10.100], loss: 0.479144, mae: 0.528189, mean_q: 4.987073
 88049/100000: episode: 1786, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 72.474, mean reward: 3.624 [2.387, 6.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.078, 10.344], loss: 0.653463, mae: 0.580627, mean_q: 5.140326
 88058/100000: episode: 1787, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 29.101, mean reward: 3.233 [2.575, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.456, 10.100], loss: 0.818480, mae: 0.653215, mean_q: 4.986154
[Info] FALSIFICATION!
 88066/100000: episode: 1788, duration: 0.338s, episode steps: 8, steps per second: 24, episode reward: 1020.265, mean reward: 127.533 [2.384, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.608, 10.098], loss: 0.337646, mae: 0.548214, mean_q: 5.080631
 88078/100000: episode: 1789, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 43.694, mean reward: 3.641 [2.823, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.535, 10.100], loss: 0.466811, mae: 0.540675, mean_q: 5.226690
 88091/100000: episode: 1790, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 43.788, mean reward: 3.368 [2.705, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.371, 10.100], loss: 0.856609, mae: 0.670027, mean_q: 5.274472
 88101/100000: episode: 1791, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 47.832, mean reward: 4.783 [3.481, 6.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.451, 10.100], loss: 0.430863, mae: 0.552062, mean_q: 4.968184
 88110/100000: episode: 1792, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 24.743, mean reward: 2.749 [2.291, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.289, 10.100], loss: 0.344118, mae: 0.496637, mean_q: 4.728542
 88127/100000: episode: 1793, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 49.649, mean reward: 2.921 [2.170, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.312, 10.332], loss: 0.352031, mae: 0.557951, mean_q: 5.190818
 88137/100000: episode: 1794, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 45.330, mean reward: 4.533 [3.265, 6.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.498, 10.100], loss: 0.594065, mae: 0.615634, mean_q: 5.101283
 88157/100000: episode: 1795, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 71.950, mean reward: 3.597 [2.789, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.344], loss: 51.116600, mae: 1.709291, mean_q: 5.829628
 88167/100000: episode: 1796, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 40.487, mean reward: 4.049 [3.277, 5.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.478, 10.100], loss: 1539.726440, mae: 4.205863, mean_q: 4.622150
 88176/100000: episode: 1797, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 33.368, mean reward: 3.708 [2.532, 7.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.581, 10.100], loss: 8.014668, mae: 2.891299, mean_q: 6.737974
 88185/100000: episode: 1798, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 33.035, mean reward: 3.671 [2.790, 5.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.375, 10.100], loss: 2.850024, mae: 1.714441, mean_q: 5.866531
 88202/100000: episode: 1799, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 51.881, mean reward: 3.052 [2.670, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.448], loss: 1.719531, mae: 1.212166, mean_q: 4.165386
 88222/100000: episode: 1800, duration: 0.143s, episode steps: 20, steps per second: 140, episode reward: 74.233, mean reward: 3.712 [2.565, 7.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.295, 10.340], loss: 1.107751, mae: 0.922291, mean_q: 5.240755
 88238/100000: episode: 1801, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 48.194, mean reward: 3.012 [2.296, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.373], loss: 0.633945, mae: 0.820739, mean_q: 5.488004
 88251/100000: episode: 1802, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 52.872, mean reward: 4.067 [3.441, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.482, 10.100], loss: 1.162484, mae: 0.816024, mean_q: 5.375610
 88261/100000: episode: 1803, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 41.532, mean reward: 4.153 [2.938, 5.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.313, 10.100], loss: 0.837099, mae: 0.752740, mean_q: 5.331687
 88277/100000: episode: 1804, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 60.798, mean reward: 3.800 [2.238, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.087, 10.482], loss: 0.577612, mae: 0.668820, mean_q: 5.247261
 88293/100000: episode: 1805, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 61.867, mean reward: 3.867 [2.971, 4.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.319, 10.485], loss: 0.924909, mae: 0.757093, mean_q: 5.377140
 88309/100000: episode: 1806, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 49.926, mean reward: 3.120 [2.457, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.537, 10.487], loss: 0.839110, mae: 0.716081, mean_q: 5.302890
 88319/100000: episode: 1807, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 36.905, mean reward: 3.690 [3.179, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.507, 10.100], loss: 1.776257, mae: 0.774636, mean_q: 5.221601
 88328/100000: episode: 1808, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 26.825, mean reward: 2.981 [2.636, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.415, 10.100], loss: 108.435493, mae: 1.502784, mean_q: 5.419985
 88340/100000: episode: 1809, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 53.759, mean reward: 4.480 [3.411, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.465, 10.100], loss: 80.688072, mae: 1.653514, mean_q: 5.750502
 88356/100000: episode: 1810, duration: 0.102s, episode steps: 16, steps per second: 157, episode reward: 43.539, mean reward: 2.721 [2.264, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.123, 10.407], loss: 1.531835, mae: 1.108987, mean_q: 6.098082
 88368/100000: episode: 1811, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 54.508, mean reward: 4.542 [3.537, 8.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.403, 10.100], loss: 0.496489, mae: 0.574402, mean_q: 5.232970
 88378/100000: episode: 1812, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 76.909, mean reward: 7.691 [4.926, 15.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.412, 10.100], loss: 2.048195, mae: 1.311773, mean_q: 5.703455
 88395/100000: episode: 1813, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 57.808, mean reward: 3.400 [2.792, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.529, 10.459], loss: 1.250473, mae: 0.912190, mean_q: 5.322234
 88412/100000: episode: 1814, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 65.251, mean reward: 3.838 [3.139, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.557], loss: 1.598520, mae: 0.855757, mean_q: 5.385655
 88425/100000: episode: 1815, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 71.600, mean reward: 5.508 [2.865, 8.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.559, 10.100], loss: 0.756620, mae: 0.738558, mean_q: 5.276357
 88435/100000: episode: 1816, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 70.137, mean reward: 7.014 [4.799, 10.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.376, 10.100], loss: 1528.156372, mae: 4.083368, mean_q: 5.775529
 88451/100000: episode: 1817, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 47.169, mean reward: 2.948 [2.352, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.419], loss: 5.706966, mae: 2.288040, mean_q: 7.049539
 88468/100000: episode: 1818, duration: 0.128s, episode steps: 17, steps per second: 133, episode reward: 105.936, mean reward: 6.232 [3.200, 22.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.882, 10.539], loss: 57.485359, mae: 1.445830, mean_q: 5.665142
[Info] Complete ISplit Iteration
[Info] Levels: [5.353553, 7.9055142, 12.845469]
[Info] Cond. Prob: [0.1, 0.1, 0.07]
[Info] Error Prob: 0.0007000000000000002

 88477/100000: episode: 1819, duration: 4.318s, episode steps: 9, steps per second: 2, episode reward: 25.304, mean reward: 2.812 [2.423, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.673, 10.100], loss: 2.200963, mae: 1.526384, mean_q: 5.784464
 88577/100000: episode: 1820, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 185.297, mean reward: 1.853 [1.435, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.302, 10.134], loss: 10.709057, mae: 0.871387, mean_q: 5.513962
 88677/100000: episode: 1821, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 210.477, mean reward: 2.105 [1.499, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.084, 10.098], loss: 10.766371, mae: 0.906673, mean_q: 5.573277
 88777/100000: episode: 1822, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.753, mean reward: 1.888 [1.464, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.277], loss: 154.438675, mae: 1.408291, mean_q: 5.799177
 88877/100000: episode: 1823, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 210.891, mean reward: 2.109 [1.479, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.447, 10.098], loss: 0.945079, mae: 0.746069, mean_q: 5.436975
 88977/100000: episode: 1824, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 246.125, mean reward: 2.461 [1.461, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.318, 10.098], loss: 0.712807, mae: 0.699988, mean_q: 5.389209
 89077/100000: episode: 1825, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.393, mean reward: 2.024 [1.485, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.036, 10.306], loss: 1.237817, mae: 0.771975, mean_q: 5.434620
 89177/100000: episode: 1826, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 189.449, mean reward: 1.894 [1.489, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.972, 10.205], loss: 306.973419, mae: 1.806902, mean_q: 5.939654
 89277/100000: episode: 1827, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 193.811, mean reward: 1.938 [1.451, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.098], loss: 154.011322, mae: 1.439894, mean_q: 5.905105
 89377/100000: episode: 1828, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.400, mean reward: 2.054 [1.551, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.668, 10.098], loss: 1.119526, mae: 0.755435, mean_q: 5.527983
 89477/100000: episode: 1829, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.813, mean reward: 2.008 [1.460, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 163.544830, mae: 1.387643, mean_q: 5.787330
 89577/100000: episode: 1830, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 188.513, mean reward: 1.885 [1.487, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.167, 10.098], loss: 20.935322, mae: 1.165768, mean_q: 5.844198
 89677/100000: episode: 1831, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 200.526, mean reward: 2.005 [1.478, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.682, 10.165], loss: 2.042775, mae: 1.104529, mean_q: 5.995279
 89777/100000: episode: 1832, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 194.184, mean reward: 1.942 [1.444, 4.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.911, 10.098], loss: 163.389923, mae: 1.427076, mean_q: 5.964898
 89877/100000: episode: 1833, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 185.413, mean reward: 1.854 [1.451, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.234], loss: 0.994713, mae: 0.823276, mean_q: 5.594321
 89977/100000: episode: 1834, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.597, mean reward: 1.896 [1.466, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.222], loss: 316.753693, mae: 2.067017, mean_q: 6.320436
 90077/100000: episode: 1835, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 204.404, mean reward: 2.044 [1.472, 4.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.098], loss: 10.857862, mae: 0.924014, mean_q: 5.714856
 90177/100000: episode: 1836, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.770, mean reward: 1.938 [1.440, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.394, 10.098], loss: 10.634630, mae: 0.908931, mean_q: 5.578670
 90277/100000: episode: 1837, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 207.244, mean reward: 2.072 [1.485, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.239, 10.098], loss: 173.215607, mae: 1.465778, mean_q: 5.855331
 90377/100000: episode: 1838, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 184.306, mean reward: 1.843 [1.449, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.681, 10.194], loss: 316.323120, mae: 2.110450, mean_q: 6.428980
 90477/100000: episode: 1839, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 190.878, mean reward: 1.909 [1.477, 2.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.589, 10.098], loss: 163.071533, mae: 1.456760, mean_q: 6.028653
 90577/100000: episode: 1840, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.129, mean reward: 1.921 [1.495, 4.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.709, 10.118], loss: 162.922485, mae: 1.497847, mean_q: 6.049434
 90677/100000: episode: 1841, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 185.169, mean reward: 1.852 [1.461, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.539, 10.147], loss: 1.382115, mae: 0.874805, mean_q: 5.652590
 90777/100000: episode: 1842, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.853, mean reward: 1.849 [1.456, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.543, 10.145], loss: 0.894783, mae: 0.754285, mean_q: 5.405838
 90877/100000: episode: 1843, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 200.386, mean reward: 2.004 [1.465, 5.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.798, 10.194], loss: 10.631320, mae: 0.860540, mean_q: 5.335844
 90977/100000: episode: 1844, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 190.276, mean reward: 1.903 [1.487, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.250, 10.098], loss: 163.115494, mae: 1.436809, mean_q: 5.731419
 91077/100000: episode: 1845, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 193.226, mean reward: 1.932 [1.453, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.099], loss: 10.812030, mae: 0.874108, mean_q: 5.414623
 91177/100000: episode: 1846, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 177.851, mean reward: 1.779 [1.471, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.868, 10.098], loss: 162.849258, mae: 1.373265, mean_q: 5.643532
 91277/100000: episode: 1847, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 185.267, mean reward: 1.853 [1.459, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.225, 10.125], loss: 19.895653, mae: 0.965468, mean_q: 5.401884
 91377/100000: episode: 1848, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.328, mean reward: 1.933 [1.452, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.098], loss: 10.689837, mae: 0.847542, mean_q: 5.337376
 91477/100000: episode: 1849, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.850, mean reward: 1.979 [1.510, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.863, 10.098], loss: 10.668937, mae: 0.824029, mean_q: 5.211362
 91577/100000: episode: 1850, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 181.218, mean reward: 1.812 [1.476, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.205], loss: 0.925021, mae: 0.692404, mean_q: 5.166648
 91677/100000: episode: 1851, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.622, mean reward: 1.886 [1.435, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.358, 10.221], loss: 153.332977, mae: 1.265588, mean_q: 5.432861
 91777/100000: episode: 1852, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.928, mean reward: 1.939 [1.439, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.814, 10.098], loss: 0.729418, mae: 0.636412, mean_q: 5.029240
 91877/100000: episode: 1853, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 194.742, mean reward: 1.947 [1.465, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.817, 10.251], loss: 0.943333, mae: 0.658293, mean_q: 5.016435
 91977/100000: episode: 1854, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 224.984, mean reward: 2.250 [1.472, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.972, 10.564], loss: 314.419586, mae: 1.949942, mean_q: 5.743592
 92077/100000: episode: 1855, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 196.587, mean reward: 1.966 [1.463, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.116], loss: 0.903945, mae: 0.688157, mean_q: 4.958188
 92177/100000: episode: 1856, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 192.537, mean reward: 1.925 [1.464, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.669, 10.101], loss: 0.967134, mae: 0.664711, mean_q: 4.984065
 92277/100000: episode: 1857, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.568, mean reward: 1.896 [1.450, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.232, 10.098], loss: 10.480416, mae: 0.742271, mean_q: 4.924865
 92377/100000: episode: 1858, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 187.075, mean reward: 1.871 [1.496, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.627, 10.098], loss: 0.727938, mae: 0.584408, mean_q: 4.723608
 92477/100000: episode: 1859, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 209.573, mean reward: 2.096 [1.513, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.315], loss: 190.896332, mae: 1.440924, mean_q: 5.120440
 92577/100000: episode: 1860, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 179.546, mean reward: 1.795 [1.457, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.298, 10.263], loss: 302.644714, mae: 1.699255, mean_q: 5.324679
 92677/100000: episode: 1861, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 194.693, mean reward: 1.947 [1.496, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.126], loss: 309.751068, mae: 1.683810, mean_q: 5.141054
 92777/100000: episode: 1862, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 225.085, mean reward: 2.251 [1.490, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.529, 10.098], loss: 21.178513, mae: 1.179872, mean_q: 5.000671
 92877/100000: episode: 1863, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 201.300, mean reward: 2.013 [1.447, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.504, 10.193], loss: 151.265121, mae: 0.979491, mean_q: 4.601838
 92977/100000: episode: 1864, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.801, mean reward: 1.968 [1.452, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.437, 10.098], loss: 151.473206, mae: 1.075560, mean_q: 4.621051
 93077/100000: episode: 1865, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.316, mean reward: 1.853 [1.466, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.771, 10.098], loss: 0.700900, mae: 0.513382, mean_q: 4.301896
 93177/100000: episode: 1866, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 182.996, mean reward: 1.830 [1.494, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.160], loss: 0.477260, mae: 0.459830, mean_q: 4.158046
 93277/100000: episode: 1867, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.815, mean reward: 1.958 [1.483, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.421, 10.273], loss: 0.345735, mae: 0.412721, mean_q: 4.048366
 93377/100000: episode: 1868, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 193.481, mean reward: 1.935 [1.433, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.389, 10.098], loss: 0.213817, mae: 0.360507, mean_q: 3.945290
 93477/100000: episode: 1869, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 176.044, mean reward: 1.760 [1.452, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.220, 10.222], loss: 0.172543, mae: 0.332906, mean_q: 3.884429
 93577/100000: episode: 1870, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 183.524, mean reward: 1.835 [1.461, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.124, 10.143], loss: 0.108262, mae: 0.304331, mean_q: 3.859610
 93677/100000: episode: 1871, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 178.600, mean reward: 1.786 [1.443, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.803, 10.131], loss: 0.122661, mae: 0.319523, mean_q: 3.872373
 93777/100000: episode: 1872, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 229.121, mean reward: 2.291 [1.505, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.447, 10.098], loss: 0.124392, mae: 0.313006, mean_q: 3.868560
 93877/100000: episode: 1873, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 189.260, mean reward: 1.893 [1.457, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.622, 10.098], loss: 0.133112, mae: 0.309143, mean_q: 3.874890
 93977/100000: episode: 1874, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.533, mean reward: 2.045 [1.457, 4.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.517, 10.130], loss: 0.108558, mae: 0.302883, mean_q: 3.848391
 94077/100000: episode: 1875, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 182.389, mean reward: 1.824 [1.466, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.347, 10.124], loss: 0.112275, mae: 0.312654, mean_q: 3.856442
 94177/100000: episode: 1876, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.439, mean reward: 1.964 [1.461, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.103, 10.098], loss: 0.106709, mae: 0.306366, mean_q: 3.852679
 94277/100000: episode: 1877, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 192.321, mean reward: 1.923 [1.461, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.261, 10.213], loss: 0.129175, mae: 0.321203, mean_q: 3.878033
 94377/100000: episode: 1878, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 185.968, mean reward: 1.860 [1.491, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.305], loss: 0.125006, mae: 0.304159, mean_q: 3.847205
 94477/100000: episode: 1879, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.651, mean reward: 1.967 [1.490, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.697, 10.098], loss: 0.101098, mae: 0.295526, mean_q: 3.835394
 94577/100000: episode: 1880, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.240, mean reward: 1.852 [1.449, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.216], loss: 0.102860, mae: 0.298541, mean_q: 3.818716
 94677/100000: episode: 1881, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 205.856, mean reward: 2.059 [1.491, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.164], loss: 0.105554, mae: 0.309805, mean_q: 3.841367
 94777/100000: episode: 1882, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 200.949, mean reward: 2.009 [1.459, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.887, 10.098], loss: 0.097168, mae: 0.295711, mean_q: 3.816104
 94877/100000: episode: 1883, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 212.366, mean reward: 2.124 [1.484, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.276, 10.098], loss: 0.089572, mae: 0.292332, mean_q: 3.831357
 94977/100000: episode: 1884, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.585, mean reward: 1.986 [1.475, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.129, 10.124], loss: 0.102782, mae: 0.300170, mean_q: 3.851919
 95077/100000: episode: 1885, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 181.869, mean reward: 1.819 [1.475, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.189], loss: 0.094516, mae: 0.288130, mean_q: 3.846346
 95177/100000: episode: 1886, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 198.089, mean reward: 1.981 [1.508, 5.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.553, 10.098], loss: 0.099223, mae: 0.297736, mean_q: 3.839860
 95277/100000: episode: 1887, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.040, mean reward: 1.970 [1.493, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.218, 10.221], loss: 0.096575, mae: 0.293450, mean_q: 3.835067
 95377/100000: episode: 1888, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.061, mean reward: 1.951 [1.493, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.071, 10.432], loss: 0.100447, mae: 0.294949, mean_q: 3.845674
 95477/100000: episode: 1889, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 187.566, mean reward: 1.876 [1.493, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.195], loss: 0.097519, mae: 0.293422, mean_q: 3.838090
 95577/100000: episode: 1890, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 236.710, mean reward: 2.367 [1.518, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.305, 10.098], loss: 0.092456, mae: 0.291195, mean_q: 3.852579
 95677/100000: episode: 1891, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.553, mean reward: 1.876 [1.466, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.045, 10.154], loss: 0.086069, mae: 0.287680, mean_q: 3.852753
 95777/100000: episode: 1892, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 183.210, mean reward: 1.832 [1.466, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.407, 10.098], loss: 0.090952, mae: 0.289249, mean_q: 3.850748
 95877/100000: episode: 1893, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 199.742, mean reward: 1.997 [1.470, 6.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.829, 10.098], loss: 0.082945, mae: 0.286840, mean_q: 3.863194
 95977/100000: episode: 1894, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 210.385, mean reward: 2.104 [1.470, 6.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.107, 10.222], loss: 0.098948, mae: 0.298694, mean_q: 3.875473
 96077/100000: episode: 1895, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.505, mean reward: 1.905 [1.468, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.180, 10.356], loss: 0.099942, mae: 0.297924, mean_q: 3.872406
 96177/100000: episode: 1896, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 194.522, mean reward: 1.945 [1.466, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.223, 10.250], loss: 0.099886, mae: 0.292331, mean_q: 3.865162
 96277/100000: episode: 1897, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.086, mean reward: 1.921 [1.437, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.888, 10.122], loss: 0.099135, mae: 0.295722, mean_q: 3.872732
 96377/100000: episode: 1898, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 183.551, mean reward: 1.836 [1.487, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.696, 10.157], loss: 0.098561, mae: 0.298185, mean_q: 3.854069
 96477/100000: episode: 1899, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.091, mean reward: 1.901 [1.452, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.286, 10.334], loss: 0.092626, mae: 0.285282, mean_q: 3.853256
 96577/100000: episode: 1900, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 210.201, mean reward: 2.102 [1.456, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.929, 10.237], loss: 0.101739, mae: 0.303010, mean_q: 3.880079
 96677/100000: episode: 1901, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 209.073, mean reward: 2.091 [1.494, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.907, 10.098], loss: 0.093172, mae: 0.296918, mean_q: 3.882551
 96777/100000: episode: 1902, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 176.642, mean reward: 1.766 [1.453, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.838, 10.098], loss: 0.115829, mae: 0.313633, mean_q: 3.892705
 96877/100000: episode: 1903, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 193.486, mean reward: 1.935 [1.443, 6.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.416, 10.100], loss: 0.107918, mae: 0.311347, mean_q: 3.897245
 96977/100000: episode: 1904, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 207.898, mean reward: 2.079 [1.470, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.097, 10.235], loss: 0.099628, mae: 0.299551, mean_q: 3.887462
 97077/100000: episode: 1905, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.871, mean reward: 1.879 [1.463, 5.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.170], loss: 0.112560, mae: 0.309184, mean_q: 3.862618
 97177/100000: episode: 1906, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 207.439, mean reward: 2.074 [1.494, 7.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.891, 10.225], loss: 0.101101, mae: 0.301727, mean_q: 3.867838
 97277/100000: episode: 1907, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 184.226, mean reward: 1.842 [1.441, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.470, 10.112], loss: 0.098962, mae: 0.299710, mean_q: 3.867826
 97377/100000: episode: 1908, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 182.881, mean reward: 1.829 [1.445, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.648, 10.098], loss: 0.113829, mae: 0.309330, mean_q: 3.872327
 97477/100000: episode: 1909, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 210.906, mean reward: 2.109 [1.499, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.617, 10.098], loss: 0.118507, mae: 0.319743, mean_q: 3.885499
 97577/100000: episode: 1910, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.067, mean reward: 1.941 [1.459, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.767, 10.098], loss: 0.108908, mae: 0.309162, mean_q: 3.873793
 97677/100000: episode: 1911, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 216.525, mean reward: 2.165 [1.549, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.800, 10.141], loss: 0.115374, mae: 0.310235, mean_q: 3.877869
 97777/100000: episode: 1912, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.641, mean reward: 1.926 [1.495, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.087, 10.290], loss: 0.107768, mae: 0.317059, mean_q: 3.867338
 97877/100000: episode: 1913, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 210.114, mean reward: 2.101 [1.462, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.433, 10.331], loss: 0.109432, mae: 0.302064, mean_q: 3.856956
 97977/100000: episode: 1914, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 179.232, mean reward: 1.792 [1.450, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.065, 10.139], loss: 0.108760, mae: 0.319364, mean_q: 3.873004
 98077/100000: episode: 1915, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.214, mean reward: 1.802 [1.487, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.595, 10.098], loss: 0.103624, mae: 0.312547, mean_q: 3.861108
 98177/100000: episode: 1916, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.698, mean reward: 1.857 [1.460, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.374, 10.098], loss: 0.118608, mae: 0.316550, mean_q: 3.878932
 98277/100000: episode: 1917, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 184.365, mean reward: 1.844 [1.441, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.108431, mae: 0.309896, mean_q: 3.871788
 98377/100000: episode: 1918, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 208.385, mean reward: 2.084 [1.492, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.091, 10.098], loss: 0.118117, mae: 0.318148, mean_q: 3.884043
[Info] 1-TH LEVEL FOUND: 5.212669372558594, Considering 10/90 traces
 98477/100000: episode: 1919, duration: 5.265s, episode steps: 100, steps per second: 19, episode reward: 182.840, mean reward: 1.828 [1.458, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.126], loss: 0.107090, mae: 0.309981, mean_q: 3.862281
 98483/100000: episode: 1920, duration: 0.109s, episode steps: 6, steps per second: 55, episode reward: 15.841, mean reward: 2.640 [2.413, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.375], loss: 0.131484, mae: 0.339733, mean_q: 3.835512
 98512/100000: episode: 1921, duration: 0.356s, episode steps: 29, steps per second: 81, episode reward: 70.185, mean reward: 2.420 [1.588, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.113, 10.100], loss: 0.120139, mae: 0.314598, mean_q: 3.878379
 98541/100000: episode: 1922, duration: 0.243s, episode steps: 29, steps per second: 119, episode reward: 108.089, mean reward: 3.727 [2.045, 7.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.450, 10.100], loss: 0.103581, mae: 0.311923, mean_q: 3.879659
 98561/100000: episode: 1923, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 43.541, mean reward: 2.177 [1.694, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.192, 10.100], loss: 0.136404, mae: 0.319536, mean_q: 3.908061
 98566/100000: episode: 1924, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 14.153, mean reward: 2.831 [2.636, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.439, 10.100], loss: 0.115452, mae: 0.334412, mean_q: 3.948343
 98597/100000: episode: 1925, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 89.692, mean reward: 2.893 [1.716, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.170, 10.100], loss: 0.114396, mae: 0.315737, mean_q: 3.918863
 98690/100000: episode: 1926, duration: 0.570s, episode steps: 93, steps per second: 163, episode reward: 174.605, mean reward: 1.877 [1.453, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.787, 10.125], loss: 0.121335, mae: 0.331476, mean_q: 3.912433
 98728/100000: episode: 1927, duration: 0.226s, episode steps: 38, steps per second: 168, episode reward: 106.235, mean reward: 2.796 [1.487, 10.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.400, 10.150], loss: 0.103509, mae: 0.318287, mean_q: 3.890499
 98748/100000: episode: 1928, duration: 0.146s, episode steps: 20, steps per second: 137, episode reward: 44.122, mean reward: 2.206 [1.677, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.134, 10.100], loss: 0.113137, mae: 0.328695, mean_q: 3.920897
 98841/100000: episode: 1929, duration: 0.516s, episode steps: 93, steps per second: 180, episode reward: 182.891, mean reward: 1.967 [1.450, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.734, 10.100], loss: 0.124411, mae: 0.329864, mean_q: 3.925002
 98870/100000: episode: 1930, duration: 0.173s, episode steps: 29, steps per second: 167, episode reward: 74.226, mean reward: 2.560 [1.752, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.431, 10.100], loss: 0.098308, mae: 0.314938, mean_q: 3.884143
 98890/100000: episode: 1931, duration: 0.135s, episode steps: 20, steps per second: 148, episode reward: 61.753, mean reward: 3.088 [2.169, 6.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.239, 10.100], loss: 0.116689, mae: 0.328113, mean_q: 3.891581
 98917/100000: episode: 1932, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 73.060, mean reward: 2.706 [2.046, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.398], loss: 0.135735, mae: 0.347204, mean_q: 3.956819
 98944/100000: episode: 1933, duration: 0.183s, episode steps: 27, steps per second: 148, episode reward: 82.118, mean reward: 3.041 [2.160, 6.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.582], loss: 0.097920, mae: 0.307651, mean_q: 3.906039
 98964/100000: episode: 1934, duration: 0.134s, episode steps: 20, steps per second: 149, episode reward: 50.934, mean reward: 2.547 [1.925, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.638, 10.100], loss: 0.143990, mae: 0.338944, mean_q: 3.943099
 99002/100000: episode: 1935, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 93.638, mean reward: 2.464 [1.695, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.060, 10.100], loss: 0.119712, mae: 0.323122, mean_q: 3.944891
 99022/100000: episode: 1936, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 45.032, mean reward: 2.252 [1.642, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.121, 10.100], loss: 0.107399, mae: 0.330375, mean_q: 3.952492
 99028/100000: episode: 1937, duration: 0.048s, episode steps: 6, steps per second: 124, episode reward: 15.849, mean reward: 2.642 [2.260, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.222, 10.400], loss: 0.159748, mae: 0.370533, mean_q: 3.991994
 99055/100000: episode: 1938, duration: 0.160s, episode steps: 27, steps per second: 168, episode reward: 130.369, mean reward: 4.828 [2.907, 6.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.473, 10.573], loss: 0.129982, mae: 0.323570, mean_q: 3.977136
 99086/100000: episode: 1939, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 110.499, mean reward: 3.564 [2.396, 5.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.517, 10.100], loss: 0.113088, mae: 0.331352, mean_q: 3.996302
 99180/100000: episode: 1940, duration: 0.545s, episode steps: 94, steps per second: 173, episode reward: 181.413, mean reward: 1.930 [1.488, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-0.937, 10.352], loss: 0.140660, mae: 0.333081, mean_q: 3.979956
 99201/100000: episode: 1941, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 49.353, mean reward: 2.350 [1.907, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.221, 10.100], loss: 0.105913, mae: 0.315939, mean_q: 4.024665
 99222/100000: episode: 1942, duration: 0.162s, episode steps: 21, steps per second: 130, episode reward: 75.907, mean reward: 3.615 [2.575, 7.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.314, 10.100], loss: 0.143423, mae: 0.343234, mean_q: 4.074881
 99249/100000: episode: 1943, duration: 0.192s, episode steps: 27, steps per second: 141, episode reward: 73.205, mean reward: 2.711 [1.670, 4.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.290], loss: 0.123696, mae: 0.350348, mean_q: 4.013603
 99269/100000: episode: 1944, duration: 0.131s, episode steps: 20, steps per second: 153, episode reward: 54.781, mean reward: 2.739 [2.042, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.360, 10.100], loss: 0.135779, mae: 0.336323, mean_q: 3.987843
 99275/100000: episode: 1945, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 14.532, mean reward: 2.422 [2.195, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.415], loss: 0.161529, mae: 0.358002, mean_q: 3.998549
 99302/100000: episode: 1946, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 69.124, mean reward: 2.560 [1.820, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.358], loss: 0.119834, mae: 0.336803, mean_q: 4.029611
 99333/100000: episode: 1947, duration: 0.204s, episode steps: 31, steps per second: 152, episode reward: 97.876, mean reward: 3.157 [2.375, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.370, 10.100], loss: 0.139914, mae: 0.359132, mean_q: 4.046988
 99360/100000: episode: 1948, duration: 0.195s, episode steps: 27, steps per second: 138, episode reward: 91.375, mean reward: 3.384 [2.461, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.706, 10.478], loss: 0.133181, mae: 0.351911, mean_q: 4.132460
 99391/100000: episode: 1949, duration: 0.462s, episode steps: 31, steps per second: 67, episode reward: 104.003, mean reward: 3.355 [1.996, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.438, 10.100], loss: 0.150422, mae: 0.363759, mean_q: 4.086613
 99418/100000: episode: 1950, duration: 0.318s, episode steps: 27, steps per second: 85, episode reward: 111.005, mean reward: 4.111 [2.825, 6.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.301, 10.542], loss: 0.148415, mae: 0.360924, mean_q: 4.091805
 99511/100000: episode: 1951, duration: 0.756s, episode steps: 93, steps per second: 123, episode reward: 182.394, mean reward: 1.961 [1.467, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.891, 10.382], loss: 0.162906, mae: 0.360279, mean_q: 4.115842
 99532/100000: episode: 1952, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 54.011, mean reward: 2.572 [2.058, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.711, 10.100], loss: 0.151944, mae: 0.377096, mean_q: 4.122022
 99559/100000: episode: 1953, duration: 0.181s, episode steps: 27, steps per second: 149, episode reward: 148.063, mean reward: 5.484 [2.909, 9.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.123, 10.471], loss: 0.220694, mae: 0.406389, mean_q: 4.237059
 99579/100000: episode: 1954, duration: 0.136s, episode steps: 20, steps per second: 147, episode reward: 54.905, mean reward: 2.745 [2.126, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.158, 10.100], loss: 0.158377, mae: 0.357883, mean_q: 4.109466
 99672/100000: episode: 1955, duration: 0.632s, episode steps: 93, steps per second: 147, episode reward: 175.145, mean reward: 1.883 [1.447, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-0.204, 10.100], loss: 0.187688, mae: 0.389136, mean_q: 4.139238
 99678/100000: episode: 1956, duration: 0.045s, episode steps: 6, steps per second: 133, episode reward: 20.658, mean reward: 3.443 [2.683, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.420, 10.303], loss: 0.178920, mae: 0.414423, mean_q: 4.146760
 99684/100000: episode: 1957, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 18.706, mean reward: 3.118 [2.367, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.412], loss: 0.188611, mae: 0.386676, mean_q: 4.090277
 99777/100000: episode: 1958, duration: 0.575s, episode steps: 93, steps per second: 162, episode reward: 185.141, mean reward: 1.991 [1.496, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.405, 10.179], loss: 0.174145, mae: 0.383229, mean_q: 4.172002
 99783/100000: episode: 1959, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 15.514, mean reward: 2.586 [1.989, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.054, 10.451], loss: 0.157888, mae: 0.358288, mean_q: 4.019874
 99803/100000: episode: 1960, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 51.336, mean reward: 2.567 [1.960, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.310, 10.100], loss: 0.147093, mae: 0.359653, mean_q: 4.137849
 99809/100000: episode: 1961, duration: 0.051s, episode steps: 6, steps per second: 117, episode reward: 16.600, mean reward: 2.767 [2.222, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.408], loss: 0.152295, mae: 0.364278, mean_q: 4.147215
 99836/100000: episode: 1962, duration: 0.176s, episode steps: 27, steps per second: 154, episode reward: 81.333, mean reward: 3.012 [2.559, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.142, 10.530], loss: 0.136353, mae: 0.347611, mean_q: 4.108170
 99874/100000: episode: 1963, duration: 0.229s, episode steps: 38, steps per second: 166, episode reward: 112.917, mean reward: 2.971 [1.710, 5.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.166, 10.100], loss: 0.168773, mae: 0.366862, mean_q: 4.206341
 99968/100000: episode: 1964, duration: 0.560s, episode steps: 94, steps per second: 168, episode reward: 166.708, mean reward: 1.773 [1.449, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.788, 10.100], loss: 0.190831, mae: 0.384226, mean_q: 4.195065
 99988/100000: episode: 1965, duration: 0.239s, episode steps: 20, steps per second: 84, episode reward: 56.283, mean reward: 2.814 [2.123, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.169, 10.100], loss: 0.144435, mae: 0.360583, mean_q: 4.109604
done, took 613.673 seconds
[Info] End Importance Splitting. Falsification occurred 15 times.
