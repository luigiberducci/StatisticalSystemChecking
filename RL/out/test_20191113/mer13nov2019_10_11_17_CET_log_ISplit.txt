Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.184s, episode steps: 100, steps per second: 543, episode reward: 179.209, mean reward: 1.792 [1.443, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.258], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.060s, episode steps: 100, steps per second: 1671, episode reward: 186.642, mean reward: 1.866 [1.474, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.688, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 210.866, mean reward: 2.109 [1.501, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.561, 10.456], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1662, episode reward: 186.194, mean reward: 1.862 [1.463, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.558, 10.389], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 191.975, mean reward: 1.920 [1.478, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.387, 10.112], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 185.756, mean reward: 1.858 [1.435, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.118, 10.153], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 206.489, mean reward: 2.065 [1.570, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.254], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 215.666, mean reward: 2.157 [1.467, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.324, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.088s, episode steps: 100, steps per second: 1143, episode reward: 208.381, mean reward: 2.084 [1.472, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 211.816, mean reward: 2.118 [1.513, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.176, 10.153], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.060s, episode steps: 100, steps per second: 1663, episode reward: 189.406, mean reward: 1.894 [1.442, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.906, 10.321], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.060s, episode steps: 100, steps per second: 1663, episode reward: 175.554, mean reward: 1.756 [1.449, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.431, 10.117], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.068s, episode steps: 100, steps per second: 1473, episode reward: 214.623, mean reward: 2.146 [1.501, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-2.115, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.059s, episode steps: 100, steps per second: 1692, episode reward: 187.787, mean reward: 1.878 [1.458, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.740, 10.190], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.059s, episode steps: 100, steps per second: 1684, episode reward: 189.878, mean reward: 1.899 [1.461, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.303, 10.182], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.060s, episode steps: 100, steps per second: 1664, episode reward: 186.119, mean reward: 1.861 [1.491, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.341, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 184.049, mean reward: 1.840 [1.454, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.660, 10.104], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.066s, episode steps: 100, steps per second: 1518, episode reward: 188.161, mean reward: 1.882 [1.453, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.540, 10.284], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 185.479, mean reward: 1.855 [1.452, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.753, 10.239], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 198.538, mean reward: 1.985 [1.451, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.009, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.065s, episode steps: 100, steps per second: 1530, episode reward: 189.690, mean reward: 1.897 [1.444, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.060s, episode steps: 100, steps per second: 1680, episode reward: 190.903, mean reward: 1.909 [1.462, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.947, 10.230], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.080s, episode steps: 100, steps per second: 1257, episode reward: 180.632, mean reward: 1.806 [1.450, 2.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.015, 10.179], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.067s, episode steps: 100, steps per second: 1497, episode reward: 194.636, mean reward: 1.946 [1.488, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.002, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 205.302, mean reward: 2.053 [1.471, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.513, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 201.311, mean reward: 2.013 [1.485, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.528, 10.455], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 182.404, mean reward: 1.824 [1.452, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.815, 10.104], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.062s, episode steps: 100, steps per second: 1612, episode reward: 190.701, mean reward: 1.907 [1.472, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.950, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 192.686, mean reward: 1.927 [1.451, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.191, 10.257], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.067s, episode steps: 100, steps per second: 1482, episode reward: 200.774, mean reward: 2.008 [1.528, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.695, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 208.906, mean reward: 2.089 [1.464, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.435, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 183.002, mean reward: 1.830 [1.436, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.064, 10.151], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 182.072, mean reward: 1.821 [1.470, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.137], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.067s, episode steps: 100, steps per second: 1495, episode reward: 227.667, mean reward: 2.277 [1.506, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.038, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 214.021, mean reward: 2.140 [1.471, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.816, 10.254], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.060s, episode steps: 100, steps per second: 1662, episode reward: 175.266, mean reward: 1.753 [1.442, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.216], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 190.014, mean reward: 1.900 [1.462, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.683, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 182.300, mean reward: 1.823 [1.446, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.151, 10.239], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1639, episode reward: 197.824, mean reward: 1.978 [1.457, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.839, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 209.817, mean reward: 2.098 [1.460, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.216], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 188.195, mean reward: 1.882 [1.465, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.519, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.068s, episode steps: 100, steps per second: 1469, episode reward: 189.885, mean reward: 1.899 [1.444, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.465, 10.247], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.067s, episode steps: 100, steps per second: 1486, episode reward: 190.341, mean reward: 1.903 [1.453, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.213, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.060s, episode steps: 100, steps per second: 1672, episode reward: 190.144, mean reward: 1.901 [1.452, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.645, 10.249], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.072s, episode steps: 100, steps per second: 1393, episode reward: 200.783, mean reward: 2.008 [1.515, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.417], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.061s, episode steps: 100, steps per second: 1632, episode reward: 197.312, mean reward: 1.973 [1.441, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.135, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 186.962, mean reward: 1.870 [1.455, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.446, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 187.328, mean reward: 1.873 [1.518, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.431, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 179.460, mean reward: 1.795 [1.445, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.090, 10.305], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.068s, episode steps: 100, steps per second: 1479, episode reward: 189.128, mean reward: 1.891 [1.472, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.126], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 192.823, mean reward: 1.928 [1.449, 6.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.598, 10.296], loss: 0.154394, mae: 0.397851, mean_q: 2.836863
  5200/100000: episode: 52, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 202.799, mean reward: 2.028 [1.443, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.249], loss: 0.103497, mae: 0.309598, mean_q: 3.219343
  5300/100000: episode: 53, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 191.745, mean reward: 1.917 [1.458, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.251], loss: 0.101416, mae: 0.306847, mean_q: 3.421985
  5400/100000: episode: 54, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 197.920, mean reward: 1.979 [1.487, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.580, 10.179], loss: 0.096823, mae: 0.302868, mean_q: 3.585048
  5500/100000: episode: 55, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 189.172, mean reward: 1.892 [1.449, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.573, 10.226], loss: 0.110235, mae: 0.313754, mean_q: 3.695380
  5600/100000: episode: 56, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.348, mean reward: 1.893 [1.452, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.682, 10.098], loss: 0.100247, mae: 0.309029, mean_q: 3.741071
  5700/100000: episode: 57, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.249, mean reward: 1.892 [1.453, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.803, 10.098], loss: 0.107851, mae: 0.324029, mean_q: 3.794855
  5800/100000: episode: 58, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 201.864, mean reward: 2.019 [1.470, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.997, 10.306], loss: 0.098498, mae: 0.307879, mean_q: 3.793706
  5900/100000: episode: 59, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 192.018, mean reward: 1.920 [1.456, 4.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.639, 10.098], loss: 0.101242, mae: 0.312904, mean_q: 3.801403
  6000/100000: episode: 60, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 183.959, mean reward: 1.840 [1.469, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.433, 10.150], loss: 0.103598, mae: 0.312156, mean_q: 3.821374
  6100/100000: episode: 61, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 183.096, mean reward: 1.831 [1.446, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.249, 10.184], loss: 0.091979, mae: 0.300261, mean_q: 3.818222
  6200/100000: episode: 62, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 205.849, mean reward: 2.058 [1.468, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.374], loss: 0.087001, mae: 0.295381, mean_q: 3.825902
  6300/100000: episode: 63, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.014, mean reward: 1.960 [1.502, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.894, 10.331], loss: 0.081812, mae: 0.283815, mean_q: 3.801524
  6400/100000: episode: 64, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 181.300, mean reward: 1.813 [1.437, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.307, 10.181], loss: 0.092123, mae: 0.297087, mean_q: 3.801411
  6500/100000: episode: 65, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 191.171, mean reward: 1.912 [1.473, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.036, 10.098], loss: 0.079336, mae: 0.286148, mean_q: 3.791856
  6600/100000: episode: 66, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 191.645, mean reward: 1.916 [1.490, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.404, 10.098], loss: 0.090160, mae: 0.294646, mean_q: 3.804065
  6700/100000: episode: 67, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 183.139, mean reward: 1.831 [1.448, 2.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.240], loss: 0.081825, mae: 0.289943, mean_q: 3.782434
  6800/100000: episode: 68, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 212.160, mean reward: 2.122 [1.512, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.361, 10.098], loss: 0.099621, mae: 0.309185, mean_q: 3.814210
  6900/100000: episode: 69, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 229.246, mean reward: 2.292 [1.447, 6.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.467, 10.344], loss: 0.098117, mae: 0.308879, mean_q: 3.833597
  7000/100000: episode: 70, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 200.033, mean reward: 2.000 [1.445, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.319, 10.365], loss: 0.100292, mae: 0.311284, mean_q: 3.837255
  7100/100000: episode: 71, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 198.996, mean reward: 1.990 [1.487, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.027, 10.159], loss: 0.096270, mae: 0.302893, mean_q: 3.828952
  7200/100000: episode: 72, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 193.329, mean reward: 1.933 [1.473, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.098], loss: 0.102474, mae: 0.311721, mean_q: 3.850288
  7300/100000: episode: 73, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 204.344, mean reward: 2.043 [1.460, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.239, 10.278], loss: 0.098161, mae: 0.309808, mean_q: 3.833849
  7400/100000: episode: 74, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 180.275, mean reward: 1.803 [1.450, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.619, 10.103], loss: 0.098270, mae: 0.307184, mean_q: 3.854088
  7500/100000: episode: 75, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 182.239, mean reward: 1.822 [1.455, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.011, 10.184], loss: 0.107605, mae: 0.318201, mean_q: 3.856460
  7600/100000: episode: 76, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 199.535, mean reward: 1.995 [1.481, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.272], loss: 0.106395, mae: 0.315767, mean_q: 3.846021
  7700/100000: episode: 77, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 202.288, mean reward: 2.023 [1.502, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.714, 10.098], loss: 0.106432, mae: 0.311884, mean_q: 3.857060
  7800/100000: episode: 78, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 190.206, mean reward: 1.902 [1.456, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.542, 10.098], loss: 0.099457, mae: 0.310155, mean_q: 3.841746
  7900/100000: episode: 79, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 189.818, mean reward: 1.898 [1.440, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.289], loss: 0.117071, mae: 0.327117, mean_q: 3.866246
  8000/100000: episode: 80, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 188.348, mean reward: 1.883 [1.476, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.147, 10.150], loss: 0.099369, mae: 0.299717, mean_q: 3.832903
  8100/100000: episode: 81, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 194.110, mean reward: 1.941 [1.478, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.763, 10.115], loss: 0.104013, mae: 0.308982, mean_q: 3.837683
  8200/100000: episode: 82, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 226.960, mean reward: 2.270 [1.573, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.515, 10.342], loss: 0.093553, mae: 0.298556, mean_q: 3.830768
  8300/100000: episode: 83, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 212.853, mean reward: 2.129 [1.460, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.142, 10.366], loss: 0.091854, mae: 0.306932, mean_q: 3.849650
  8400/100000: episode: 84, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 232.400, mean reward: 2.324 [1.456, 8.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.888, 10.344], loss: 0.093556, mae: 0.299590, mean_q: 3.832673
  8500/100000: episode: 85, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 187.300, mean reward: 1.873 [1.449, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.049, 10.159], loss: 0.110032, mae: 0.310123, mean_q: 3.837503
  8600/100000: episode: 86, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 218.698, mean reward: 2.187 [1.546, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.275], loss: 0.125199, mae: 0.321238, mean_q: 3.852843
  8700/100000: episode: 87, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 188.570, mean reward: 1.886 [1.465, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.098], loss: 0.124748, mae: 0.322954, mean_q: 3.849195
  8800/100000: episode: 88, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 202.441, mean reward: 2.024 [1.508, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.434, 10.418], loss: 0.100544, mae: 0.309360, mean_q: 3.852153
  8900/100000: episode: 89, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 187.434, mean reward: 1.874 [1.440, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.507, 10.157], loss: 0.127965, mae: 0.329500, mean_q: 3.882978
  9000/100000: episode: 90, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 184.241, mean reward: 1.842 [1.431, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.672, 10.098], loss: 0.123857, mae: 0.317536, mean_q: 3.873698
  9100/100000: episode: 91, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 186.148, mean reward: 1.861 [1.483, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.096, 10.299], loss: 0.107094, mae: 0.305247, mean_q: 3.865937
  9200/100000: episode: 92, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 189.237, mean reward: 1.892 [1.451, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.077, 10.098], loss: 0.119296, mae: 0.315192, mean_q: 3.865663
  9300/100000: episode: 93, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 197.034, mean reward: 1.970 [1.518, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.654, 10.149], loss: 0.121620, mae: 0.313925, mean_q: 3.858896
  9400/100000: episode: 94, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 185.926, mean reward: 1.859 [1.491, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.501, 10.197], loss: 0.133999, mae: 0.328002, mean_q: 3.888958
  9500/100000: episode: 95, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 194.821, mean reward: 1.948 [1.441, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.525, 10.098], loss: 0.111447, mae: 0.312285, mean_q: 3.858067
  9600/100000: episode: 96, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 188.797, mean reward: 1.888 [1.470, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.749, 10.098], loss: 0.125079, mae: 0.328165, mean_q: 3.862941
  9700/100000: episode: 97, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 195.542, mean reward: 1.955 [1.449, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.517, 10.220], loss: 0.110079, mae: 0.308619, mean_q: 3.855227
  9800/100000: episode: 98, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.326, mean reward: 2.003 [1.495, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.841, 10.291], loss: 0.126418, mae: 0.328106, mean_q: 3.874736
  9900/100000: episode: 99, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 182.523, mean reward: 1.825 [1.453, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.098], loss: 0.111779, mae: 0.310344, mean_q: 3.871037
 10000/100000: episode: 100, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 187.748, mean reward: 1.877 [1.488, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.748, 10.292], loss: 0.106447, mae: 0.312485, mean_q: 3.848525
 10100/100000: episode: 101, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 197.640, mean reward: 1.976 [1.509, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.965, 10.098], loss: 0.112988, mae: 0.310778, mean_q: 3.849964
 10200/100000: episode: 102, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 187.396, mean reward: 1.874 [1.463, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.867, 10.370], loss: 0.116624, mae: 0.318227, mean_q: 3.855637
 10300/100000: episode: 103, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 190.978, mean reward: 1.910 [1.446, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.467, 10.098], loss: 0.125344, mae: 0.330422, mean_q: 3.865327
 10400/100000: episode: 104, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 200.056, mean reward: 2.001 [1.481, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.852, 10.238], loss: 0.110872, mae: 0.313761, mean_q: 3.853802
 10500/100000: episode: 105, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 222.690, mean reward: 2.227 [1.447, 8.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.993, 10.491], loss: 0.107187, mae: 0.307860, mean_q: 3.851255
 10600/100000: episode: 106, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 205.248, mean reward: 2.052 [1.445, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.140, 10.098], loss: 0.115357, mae: 0.318020, mean_q: 3.874697
 10700/100000: episode: 107, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 217.081, mean reward: 2.171 [1.533, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.109, 10.281], loss: 0.104362, mae: 0.314184, mean_q: 3.872761
 10800/100000: episode: 108, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 196.491, mean reward: 1.965 [1.442, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.315], loss: 0.115562, mae: 0.320679, mean_q: 3.893047
 10900/100000: episode: 109, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 189.491, mean reward: 1.895 [1.450, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.699, 10.180], loss: 0.124689, mae: 0.327666, mean_q: 3.876996
 11000/100000: episode: 110, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 193.757, mean reward: 1.938 [1.475, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.473, 10.272], loss: 0.119804, mae: 0.323865, mean_q: 3.892566
 11100/100000: episode: 111, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 189.693, mean reward: 1.897 [1.462, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.525, 10.098], loss: 0.135652, mae: 0.336700, mean_q: 3.901854
 11200/100000: episode: 112, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 194.662, mean reward: 1.947 [1.492, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.473, 10.098], loss: 0.110791, mae: 0.317998, mean_q: 3.885839
 11300/100000: episode: 113, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.488, mean reward: 1.895 [1.435, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.338, 10.238], loss: 0.103714, mae: 0.310172, mean_q: 3.868994
 11400/100000: episode: 114, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 192.210, mean reward: 1.922 [1.458, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.388, 10.354], loss: 0.103140, mae: 0.308316, mean_q: 3.880952
 11500/100000: episode: 115, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 183.558, mean reward: 1.836 [1.435, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.386, 10.098], loss: 0.125814, mae: 0.333958, mean_q: 3.884981
 11600/100000: episode: 116, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 187.347, mean reward: 1.873 [1.504, 4.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.413, 10.334], loss: 0.123904, mae: 0.332782, mean_q: 3.903792
 11700/100000: episode: 117, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.612, mean reward: 1.926 [1.467, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.654, 10.146], loss: 0.139015, mae: 0.338574, mean_q: 3.883025
 11800/100000: episode: 118, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.749, mean reward: 1.947 [1.440, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.454, 10.098], loss: 0.107572, mae: 0.313961, mean_q: 3.876519
 11900/100000: episode: 119, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.067, mean reward: 1.911 [1.469, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.999, 10.098], loss: 0.103291, mae: 0.305664, mean_q: 3.863613
 12000/100000: episode: 120, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.251, mean reward: 2.053 [1.471, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.124, 10.271], loss: 0.107033, mae: 0.321676, mean_q: 3.882472
 12100/100000: episode: 121, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.167, mean reward: 1.862 [1.503, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.209, 10.098], loss: 0.099146, mae: 0.313230, mean_q: 3.868126
 12200/100000: episode: 122, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 202.845, mean reward: 2.028 [1.449, 16.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.098], loss: 0.106978, mae: 0.304278, mean_q: 3.876952
 12300/100000: episode: 123, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 208.780, mean reward: 2.088 [1.471, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.970, 10.098], loss: 0.109269, mae: 0.318606, mean_q: 3.867435
 12400/100000: episode: 124, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 194.288, mean reward: 1.943 [1.471, 5.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.246, 10.274], loss: 0.159117, mae: 0.333773, mean_q: 3.891277
 12500/100000: episode: 125, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 183.561, mean reward: 1.836 [1.449, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.730, 10.098], loss: 0.100641, mae: 0.300808, mean_q: 3.863535
 12600/100000: episode: 126, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.269, mean reward: 1.833 [1.433, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.129, 10.098], loss: 0.125363, mae: 0.311321, mean_q: 3.858981
 12700/100000: episode: 127, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 196.284, mean reward: 1.963 [1.456, 4.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.426, 10.098], loss: 0.137606, mae: 0.316644, mean_q: 3.876152
 12800/100000: episode: 128, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 202.769, mean reward: 2.028 [1.458, 5.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.614, 10.098], loss: 0.203561, mae: 0.333889, mean_q: 3.880921
 12900/100000: episode: 129, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.336, mean reward: 1.903 [1.467, 4.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.201], loss: 0.127350, mae: 0.314560, mean_q: 3.870874
 13000/100000: episode: 130, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.735, mean reward: 1.927 [1.469, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.971, 10.098], loss: 0.135190, mae: 0.316908, mean_q: 3.868825
 13100/100000: episode: 131, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.426, mean reward: 1.864 [1.472, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.276, 10.098], loss: 0.113210, mae: 0.312322, mean_q: 3.868065
 13200/100000: episode: 132, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 237.063, mean reward: 2.371 [1.437, 5.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.627, 10.098], loss: 0.107304, mae: 0.312754, mean_q: 3.868351
 13300/100000: episode: 133, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.489, mean reward: 1.915 [1.441, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.125], loss: 0.169518, mae: 0.325578, mean_q: 3.874738
 13400/100000: episode: 134, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.492, mean reward: 1.905 [1.471, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.084, 10.137], loss: 0.098758, mae: 0.306181, mean_q: 3.848471
 13500/100000: episode: 135, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 194.100, mean reward: 1.941 [1.455, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.464, 10.098], loss: 0.098260, mae: 0.303372, mean_q: 3.849259
 13600/100000: episode: 136, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 189.455, mean reward: 1.895 [1.473, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.024, 10.098], loss: 0.098670, mae: 0.304777, mean_q: 3.837155
 13700/100000: episode: 137, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 203.734, mean reward: 2.037 [1.468, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.103], loss: 0.141793, mae: 0.323620, mean_q: 3.851906
 13800/100000: episode: 138, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 208.734, mean reward: 2.087 [1.508, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.918, 10.098], loss: 0.109021, mae: 0.316389, mean_q: 3.844030
 13900/100000: episode: 139, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 199.593, mean reward: 1.996 [1.461, 3.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.233, 10.406], loss: 0.116455, mae: 0.316541, mean_q: 3.843611
 14000/100000: episode: 140, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 188.186, mean reward: 1.882 [1.464, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.696, 10.137], loss: 0.173581, mae: 0.318188, mean_q: 3.859254
 14100/100000: episode: 141, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 187.031, mean reward: 1.870 [1.451, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.215, 10.098], loss: 0.206175, mae: 0.337472, mean_q: 3.874713
 14200/100000: episode: 142, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 215.255, mean reward: 2.153 [1.460, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.098], loss: 0.138118, mae: 0.326145, mean_q: 3.870551
 14300/100000: episode: 143, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.206, mean reward: 1.842 [1.468, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.132, 10.194], loss: 0.103418, mae: 0.314535, mean_q: 3.865179
 14400/100000: episode: 144, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.058, mean reward: 1.841 [1.459, 4.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.135, 10.098], loss: 0.171247, mae: 0.331878, mean_q: 3.854346
 14500/100000: episode: 145, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.739, mean reward: 1.977 [1.482, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.217, 10.103], loss: 0.118742, mae: 0.320828, mean_q: 3.874831
 14600/100000: episode: 146, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 215.133, mean reward: 2.151 [1.439, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.668, 10.359], loss: 0.178466, mae: 0.318306, mean_q: 3.857026
 14700/100000: episode: 147, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 184.745, mean reward: 1.847 [1.458, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.557, 10.278], loss: 0.098825, mae: 0.307646, mean_q: 3.852982
 14800/100000: episode: 148, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 211.948, mean reward: 2.119 [1.453, 5.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.201, 10.305], loss: 0.122222, mae: 0.329506, mean_q: 3.882358
 14900/100000: episode: 149, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 180.674, mean reward: 1.807 [1.450, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.985, 10.132], loss: 0.108096, mae: 0.315078, mean_q: 3.857795
[Info] 1-TH LEVEL FOUND: 4.609077453613281, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.154s, episode steps: 100, steps per second: 19, episode reward: 199.245, mean reward: 1.992 [1.457, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.039, 10.168], loss: 0.182338, mae: 0.332371, mean_q: 3.882524
 15017/100000: episode: 151, duration: 0.131s, episode steps: 17, steps per second: 130, episode reward: 157.023, mean reward: 9.237 [2.597, 44.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.678, 10.595], loss: 0.131451, mae: 0.327120, mean_q: 3.871398
 15045/100000: episode: 152, duration: 0.200s, episode steps: 28, steps per second: 140, episode reward: 59.854, mean reward: 2.138 [1.626, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.328, 10.298], loss: 0.233794, mae: 0.361555, mean_q: 3.926487
 15067/100000: episode: 153, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 58.160, mean reward: 2.644 [1.976, 5.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.183, 10.422], loss: 0.434482, mae: 0.393478, mean_q: 3.934867
 15096/100000: episode: 154, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 69.607, mean reward: 2.400 [1.859, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.098, 10.291], loss: 1.030039, mae: 0.405984, mean_q: 3.951039
 15125/100000: episode: 155, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 58.779, mean reward: 2.027 [1.482, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.922, 10.120], loss: 0.386765, mae: 0.367417, mean_q: 3.905000
 15161/100000: episode: 156, duration: 0.170s, episode steps: 36, steps per second: 211, episode reward: 72.541, mean reward: 2.015 [1.466, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.424, 10.100], loss: 1.179526, mae: 0.469374, mean_q: 4.009391
 15202/100000: episode: 157, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 108.008, mean reward: 2.634 [1.526, 5.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.157, 10.427], loss: 0.287236, mae: 0.356107, mean_q: 3.925675
 15231/100000: episode: 158, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 56.970, mean reward: 1.964 [1.468, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.415, 10.111], loss: 0.134905, mae: 0.348634, mean_q: 3.897421
 15248/100000: episode: 159, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 48.198, mean reward: 2.835 [2.300, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.405], loss: 0.112240, mae: 0.323223, mean_q: 3.907266
 15270/100000: episode: 160, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 47.366, mean reward: 2.153 [1.756, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.395], loss: 0.128922, mae: 0.331392, mean_q: 3.903615
 15311/100000: episode: 161, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 73.183, mean reward: 1.785 [1.458, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.694, 10.194], loss: 0.123291, mae: 0.341366, mean_q: 3.916766
 15339/100000: episode: 162, duration: 0.130s, episode steps: 28, steps per second: 216, episode reward: 55.418, mean reward: 1.979 [1.465, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.537, 10.148], loss: 0.149224, mae: 0.322829, mean_q: 3.909574
 15370/100000: episode: 163, duration: 0.151s, episode steps: 31, steps per second: 206, episode reward: 58.996, mean reward: 1.903 [1.466, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.279, 10.132], loss: 0.391351, mae: 0.395162, mean_q: 3.948866
 15399/100000: episode: 164, duration: 0.143s, episode steps: 29, steps per second: 202, episode reward: 58.644, mean reward: 2.022 [1.471, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.933, 10.100], loss: 1.061884, mae: 0.446214, mean_q: 4.002177
 15421/100000: episode: 165, duration: 0.103s, episode steps: 22, steps per second: 214, episode reward: 52.315, mean reward: 2.378 [1.889, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.457], loss: 0.151239, mae: 0.351873, mean_q: 3.915433
 15448/100000: episode: 166, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 51.588, mean reward: 1.911 [1.484, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.112, 10.100], loss: 0.188269, mae: 0.345051, mean_q: 3.932303
 15475/100000: episode: 167, duration: 0.202s, episode steps: 27, steps per second: 134, episode reward: 52.218, mean reward: 1.934 [1.527, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.272, 10.131], loss: 0.236761, mae: 0.369804, mean_q: 3.961662
 15516/100000: episode: 168, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 81.251, mean reward: 1.982 [1.455, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.658, 10.280], loss: 0.220774, mae: 0.362047, mean_q: 3.945033
 15545/100000: episode: 169, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 62.721, mean reward: 2.163 [1.632, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.896, 10.215], loss: 0.268586, mae: 0.363812, mean_q: 3.959806
 15574/100000: episode: 170, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 56.614, mean reward: 1.952 [1.531, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.699, 10.100], loss: 0.184078, mae: 0.342760, mean_q: 3.914989
 15596/100000: episode: 171, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 46.122, mean reward: 2.096 [1.752, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.339], loss: 0.133497, mae: 0.342649, mean_q: 3.940193
 15624/100000: episode: 172, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 58.502, mean reward: 2.089 [1.649, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.205, 10.212], loss: 1.077455, mae: 0.392329, mean_q: 3.931127
 15641/100000: episode: 173, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 53.292, mean reward: 3.135 [2.569, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.462], loss: 0.156521, mae: 0.400132, mean_q: 3.934626
 15673/100000: episode: 174, duration: 0.150s, episode steps: 32, steps per second: 213, episode reward: 75.240, mean reward: 2.351 [1.673, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.261], loss: 0.165113, mae: 0.348288, mean_q: 3.945272
 15709/100000: episode: 175, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 93.836, mean reward: 2.607 [1.579, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.536, 10.184], loss: 0.139187, mae: 0.322432, mean_q: 3.879895
 15736/100000: episode: 176, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 57.876, mean reward: 2.144 [1.575, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.239, 10.282], loss: 0.170688, mae: 0.364915, mean_q: 3.944494
 15768/100000: episode: 177, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 68.117, mean reward: 2.129 [1.597, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.077, 10.254], loss: 0.946867, mae: 0.402577, mean_q: 3.993346
 15800/100000: episode: 178, duration: 0.152s, episode steps: 32, steps per second: 210, episode reward: 77.389, mean reward: 2.418 [2.003, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.630, 10.379], loss: 0.145585, mae: 0.354702, mean_q: 3.895490
 15832/100000: episode: 179, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 67.289, mean reward: 2.103 [1.564, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.292, 10.206], loss: 0.115652, mae: 0.340112, mean_q: 3.931527
 15868/100000: episode: 180, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 95.029, mean reward: 2.640 [1.832, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.286, 10.428], loss: 0.302472, mae: 0.370397, mean_q: 3.958992
 15904/100000: episode: 181, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 68.993, mean reward: 1.916 [1.525, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.475, 10.127], loss: 0.125874, mae: 0.346188, mean_q: 3.943553
 15921/100000: episode: 182, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 52.614, mean reward: 3.095 [2.456, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.235, 10.381], loss: 0.115575, mae: 0.358832, mean_q: 3.981223
 15943/100000: episode: 183, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 51.436, mean reward: 2.338 [1.795, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.353], loss: 1.329574, mae: 0.475585, mean_q: 4.021076
 15960/100000: episode: 184, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 49.412, mean reward: 2.907 [2.384, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.469], loss: 0.229534, mae: 0.410208, mean_q: 3.957388
 15996/100000: episode: 185, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 77.331, mean reward: 2.148 [1.696, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.799, 10.413], loss: 1.528854, mae: 0.444327, mean_q: 3.978877
 16018/100000: episode: 186, duration: 0.102s, episode steps: 22, steps per second: 215, episode reward: 44.518, mean reward: 2.024 [1.503, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.720, 10.170], loss: 0.227338, mae: 0.425788, mean_q: 4.038744
 16050/100000: episode: 187, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 68.234, mean reward: 2.132 [1.616, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.807, 10.216], loss: 0.280241, mae: 0.439443, mean_q: 3.963230
 16078/100000: episode: 188, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 70.804, mean reward: 2.529 [1.830, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.379], loss: 0.129002, mae: 0.346473, mean_q: 3.956714
 16107/100000: episode: 189, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 58.577, mean reward: 2.020 [1.488, 2.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.176, 10.100], loss: 0.108566, mae: 0.329038, mean_q: 3.964261
 16136/100000: episode: 190, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 82.455, mean reward: 2.843 [2.071, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.741, 10.188], loss: 0.294034, mae: 0.393483, mean_q: 3.975517
 16153/100000: episode: 191, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 43.847, mean reward: 2.579 [2.090, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.859, 10.300], loss: 0.221005, mae: 0.393563, mean_q: 4.037634
 16182/100000: episode: 192, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 74.972, mean reward: 2.585 [1.942, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.338], loss: 0.125980, mae: 0.348853, mean_q: 3.932358
 16211/100000: episode: 193, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 65.592, mean reward: 2.262 [1.656, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.217], loss: 0.203853, mae: 0.387775, mean_q: 3.993119
 16252/100000: episode: 194, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 89.452, mean reward: 2.182 [1.658, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.884, 10.100], loss: 0.154854, mae: 0.355286, mean_q: 3.979778
 16269/100000: episode: 195, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 53.130, mean reward: 3.125 [2.493, 4.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.419], loss: 0.217404, mae: 0.382769, mean_q: 3.999457
 16296/100000: episode: 196, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 47.942, mean reward: 1.776 [1.456, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.080, 10.210], loss: 0.210186, mae: 0.360985, mean_q: 4.053383
 16337/100000: episode: 197, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 79.216, mean reward: 1.932 [1.447, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.035, 10.100], loss: 0.141598, mae: 0.347917, mean_q: 3.973835
 16354/100000: episode: 198, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 56.765, mean reward: 3.339 [2.314, 4.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.111, 10.363], loss: 0.301751, mae: 0.366236, mean_q: 4.015446
 16390/100000: episode: 199, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 70.498, mean reward: 1.958 [1.557, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.847, 10.226], loss: 0.173556, mae: 0.379971, mean_q: 4.034115
 16422/100000: episode: 200, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 61.822, mean reward: 1.932 [1.504, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.761, 10.100], loss: 0.166987, mae: 0.365163, mean_q: 3.993888
 16454/100000: episode: 201, duration: 0.201s, episode steps: 32, steps per second: 159, episode reward: 75.945, mean reward: 2.373 [1.787, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.395], loss: 0.913782, mae: 0.421513, mean_q: 4.048542
 16485/100000: episode: 202, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 60.458, mean reward: 1.950 [1.542, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.570, 10.162], loss: 0.249643, mae: 0.389127, mean_q: 3.966359
 16513/100000: episode: 203, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 48.429, mean reward: 1.730 [1.479, 2.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.364, 10.152], loss: 0.204748, mae: 0.371259, mean_q: 3.996091
 16542/100000: episode: 204, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 69.414, mean reward: 2.394 [1.732, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.392], loss: 0.252508, mae: 0.394645, mean_q: 4.030819
 16578/100000: episode: 205, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 78.018, mean reward: 2.167 [1.594, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.941, 10.196], loss: 0.214426, mae: 0.393563, mean_q: 4.059283
 16609/100000: episode: 206, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 61.793, mean reward: 1.993 [1.502, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.297, 10.295], loss: 0.141587, mae: 0.348450, mean_q: 4.034899
 16637/100000: episode: 207, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 55.119, mean reward: 1.969 [1.556, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.169, 10.174], loss: 0.159648, mae: 0.359536, mean_q: 4.033602
 16678/100000: episode: 208, duration: 0.267s, episode steps: 41, steps per second: 153, episode reward: 80.643, mean reward: 1.967 [1.502, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.753, 10.215], loss: 0.113348, mae: 0.344494, mean_q: 4.003947
 16719/100000: episode: 209, duration: 0.261s, episode steps: 41, steps per second: 157, episode reward: 89.236, mean reward: 2.176 [1.587, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.934, 10.252], loss: 0.384643, mae: 0.387166, mean_q: 4.050234
 16746/100000: episode: 210, duration: 0.197s, episode steps: 27, steps per second: 137, episode reward: 50.629, mean reward: 1.875 [1.458, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.403, 10.258], loss: 0.489293, mae: 0.450539, mean_q: 4.105597
 16778/100000: episode: 211, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 64.980, mean reward: 2.031 [1.494, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.359, 10.135], loss: 0.151741, mae: 0.368199, mean_q: 3.970088
 16814/100000: episode: 212, duration: 0.228s, episode steps: 36, steps per second: 158, episode reward: 74.899, mean reward: 2.081 [1.640, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.030, 10.333], loss: 0.118487, mae: 0.333107, mean_q: 3.995192
 16843/100000: episode: 213, duration: 0.186s, episode steps: 29, steps per second: 156, episode reward: 68.870, mean reward: 2.375 [1.647, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.130, 10.289], loss: 0.479988, mae: 0.411134, mean_q: 4.072032
 16879/100000: episode: 214, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 68.251, mean reward: 1.896 [1.476, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.428, 10.134], loss: 0.971474, mae: 0.426712, mean_q: 4.046589
 16920/100000: episode: 215, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 70.909, mean reward: 1.729 [1.442, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.390, 10.114], loss: 0.208524, mae: 0.371922, mean_q: 4.039826
 16949/100000: episode: 216, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 53.802, mean reward: 1.855 [1.462, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.525, 10.211], loss: 0.295437, mae: 0.373663, mean_q: 4.032622
 16990/100000: episode: 217, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 78.543, mean reward: 1.916 [1.524, 2.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.831, 10.357], loss: 0.210605, mae: 0.394446, mean_q: 4.069839
 17019/100000: episode: 218, duration: 0.233s, episode steps: 29, steps per second: 125, episode reward: 69.652, mean reward: 2.402 [1.814, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.120, 10.225], loss: 0.173007, mae: 0.377521, mean_q: 4.020939
 17051/100000: episode: 219, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 72.178, mean reward: 2.256 [1.626, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.289, 10.361], loss: 0.146083, mae: 0.360211, mean_q: 4.077968
 17080/100000: episode: 220, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 59.500, mean reward: 2.052 [1.695, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.327], loss: 0.942555, mae: 0.448033, mean_q: 4.114917
 17109/100000: episode: 221, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 67.756, mean reward: 2.336 [1.523, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.724, 10.197], loss: 0.165003, mae: 0.379269, mean_q: 3.948317
 17138/100000: episode: 222, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 80.286, mean reward: 2.768 [1.743, 5.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.486, 10.273], loss: 0.125656, mae: 0.340490, mean_q: 4.047102
 17160/100000: episode: 223, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 50.256, mean reward: 2.284 [1.801, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.904, 10.412], loss: 1.212945, mae: 0.464519, mean_q: 4.142839
 17192/100000: episode: 224, duration: 0.223s, episode steps: 32, steps per second: 143, episode reward: 79.273, mean reward: 2.477 [1.747, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.303, 10.312], loss: 0.252588, mae: 0.409700, mean_q: 4.006185
 17223/100000: episode: 225, duration: 0.206s, episode steps: 31, steps per second: 151, episode reward: 65.601, mean reward: 2.116 [1.731, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.289], loss: 0.149366, mae: 0.371941, mean_q: 4.078475
 17251/100000: episode: 226, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 52.333, mean reward: 1.869 [1.441, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.307, 10.166], loss: 0.137919, mae: 0.362268, mean_q: 4.063546
 17287/100000: episode: 227, duration: 0.218s, episode steps: 36, steps per second: 165, episode reward: 66.139, mean reward: 1.837 [1.464, 2.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.711, 10.102], loss: 0.127717, mae: 0.346666, mean_q: 4.067930
 17315/100000: episode: 228, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 60.229, mean reward: 2.151 [1.703, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.289], loss: 0.150203, mae: 0.350981, mean_q: 4.068809
 17346/100000: episode: 229, duration: 0.191s, episode steps: 31, steps per second: 163, episode reward: 61.164, mean reward: 1.973 [1.574, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.675, 10.167], loss: 0.313758, mae: 0.383480, mean_q: 4.100564
 17387/100000: episode: 230, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 72.549, mean reward: 1.769 [1.479, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.340, 10.100], loss: 0.132219, mae: 0.340430, mean_q: 4.064992
 17419/100000: episode: 231, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 82.154, mean reward: 2.567 [2.058, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.325], loss: 0.328413, mae: 0.406940, mean_q: 4.039481
 17455/100000: episode: 232, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 83.925, mean reward: 2.331 [1.652, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.559, 10.350], loss: 0.122822, mae: 0.359920, mean_q: 4.068474
 17487/100000: episode: 233, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 75.968, mean reward: 2.374 [1.831, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.551, 10.252], loss: 0.860182, mae: 0.451930, mean_q: 4.143637
 17516/100000: episode: 234, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 64.242, mean reward: 2.215 [1.742, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.341, 10.264], loss: 0.144507, mae: 0.344360, mean_q: 4.057173
 17548/100000: episode: 235, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 80.920, mean reward: 2.529 [2.105, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.833, 10.373], loss: 0.208949, mae: 0.400061, mean_q: 4.159534
 17576/100000: episode: 236, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 53.879, mean reward: 1.924 [1.459, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.256, 10.166], loss: 0.145799, mae: 0.343504, mean_q: 4.126702
 17617/100000: episode: 237, duration: 0.297s, episode steps: 41, steps per second: 138, episode reward: 70.337, mean reward: 1.716 [1.432, 2.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.865, 10.100], loss: 0.307521, mae: 0.415169, mean_q: 4.111979
 17634/100000: episode: 238, duration: 0.111s, episode steps: 17, steps per second: 153, episode reward: 54.848, mean reward: 3.226 [2.387, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.159, 10.461], loss: 0.174568, mae: 0.370116, mean_q: 4.128774
 17663/100000: episode: 239, duration: 0.213s, episode steps: 29, steps per second: 136, episode reward: 64.216, mean reward: 2.214 [1.611, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.691, 10.189], loss: 0.286783, mae: 0.359216, mean_q: 4.095578
[Info] 2-TH LEVEL FOUND: 6.482072353363037, Considering 11/89 traces
 17704/100000: episode: 240, duration: 4.814s, episode steps: 41, steps per second: 9, episode reward: 76.362, mean reward: 1.862 [1.449, 2.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.230, 10.375], loss: 1.209318, mae: 0.445628, mean_q: 4.148417
 17715/100000: episode: 241, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 35.208, mean reward: 3.201 [2.700, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.513], loss: 0.198699, mae: 0.412869, mean_q: 3.944144
 17732/100000: episode: 242, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 52.372, mean reward: 3.081 [2.243, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.322, 10.571], loss: 0.142929, mae: 0.383686, mean_q: 4.116643
 17749/100000: episode: 243, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 53.883, mean reward: 3.170 [2.491, 4.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.561, 10.483], loss: 0.085292, mae: 0.311546, mean_q: 4.080351
 17766/100000: episode: 244, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 42.074, mean reward: 2.475 [2.127, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.347], loss: 0.106544, mae: 0.323223, mean_q: 4.137355
 17783/100000: episode: 245, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 65.614, mean reward: 3.860 [2.775, 9.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.817, 10.430], loss: 0.434580, mae: 0.384629, mean_q: 4.112180
 17800/100000: episode: 246, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 51.825, mean reward: 3.049 [2.116, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.535, 10.392], loss: 0.187910, mae: 0.397035, mean_q: 4.096983
 17817/100000: episode: 247, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 48.277, mean reward: 2.840 [2.141, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.161, 10.445], loss: 0.158146, mae: 0.347404, mean_q: 4.050748
 17828/100000: episode: 248, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 43.570, mean reward: 3.961 [2.675, 7.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.542], loss: 0.255452, mae: 0.404538, mean_q: 4.157946
 17839/100000: episode: 249, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 45.750, mean reward: 4.159 [2.727, 8.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.663], loss: 0.174171, mae: 0.367029, mean_q: 4.157005
 17856/100000: episode: 250, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 58.797, mean reward: 3.459 [2.484, 7.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.123, 10.624], loss: 0.135144, mae: 0.381375, mean_q: 4.152009
 17873/100000: episode: 251, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 70.603, mean reward: 4.153 [3.030, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.496], loss: 0.245472, mae: 0.413569, mean_q: 4.271485
 17890/100000: episode: 252, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 48.082, mean reward: 2.828 [2.545, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.311, 10.394], loss: 0.128677, mae: 0.359079, mean_q: 4.123546
 17907/100000: episode: 253, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 72.963, mean reward: 4.292 [2.889, 7.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.423], loss: 0.179673, mae: 0.368389, mean_q: 4.124274
 17924/100000: episode: 254, duration: 0.125s, episode steps: 17, steps per second: 137, episode reward: 49.261, mean reward: 2.898 [2.157, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.822, 10.350], loss: 0.192642, mae: 0.374292, mean_q: 4.259021
 17941/100000: episode: 255, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 44.637, mean reward: 2.626 [2.253, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.486, 10.339], loss: 0.116506, mae: 0.345094, mean_q: 4.148725
 17958/100000: episode: 256, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 64.564, mean reward: 3.798 [2.474, 6.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.065, 10.583], loss: 0.123822, mae: 0.351755, mean_q: 4.175144
 17975/100000: episode: 257, duration: 0.138s, episode steps: 17, steps per second: 123, episode reward: 53.662, mean reward: 3.157 [2.395, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.429], loss: 0.190292, mae: 0.355396, mean_q: 4.192605
 17992/100000: episode: 258, duration: 0.127s, episode steps: 17, steps per second: 134, episode reward: 60.968, mean reward: 3.586 [2.935, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.546, 10.590], loss: 0.132643, mae: 0.362377, mean_q: 4.290674
 18009/100000: episode: 259, duration: 0.109s, episode steps: 17, steps per second: 155, episode reward: 57.957, mean reward: 3.409 [3.052, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.517], loss: 0.127779, mae: 0.355752, mean_q: 4.229834
 18026/100000: episode: 260, duration: 0.121s, episode steps: 17, steps per second: 140, episode reward: 51.481, mean reward: 3.028 [1.982, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.860, 10.338], loss: 0.204506, mae: 0.400351, mean_q: 4.185428
 18037/100000: episode: 261, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 37.614, mean reward: 3.419 [2.515, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.131, 10.367], loss: 0.132698, mae: 0.368293, mean_q: 4.297232
 18054/100000: episode: 262, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 59.628, mean reward: 3.508 [2.708, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.051, 10.419], loss: 0.171398, mae: 0.373818, mean_q: 4.246195
 18065/100000: episode: 263, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 29.226, mean reward: 2.657 [2.318, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.444], loss: 0.118704, mae: 0.360450, mean_q: 4.193600
 18082/100000: episode: 264, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 61.017, mean reward: 3.589 [2.260, 6.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.525], loss: 0.101215, mae: 0.330861, mean_q: 4.292096
 18099/100000: episode: 265, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 46.555, mean reward: 2.739 [2.337, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.442], loss: 0.149349, mae: 0.379039, mean_q: 4.252681
 18116/100000: episode: 266, duration: 0.120s, episode steps: 17, steps per second: 141, episode reward: 107.201, mean reward: 6.306 [3.059, 10.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.221, 10.686], loss: 0.130634, mae: 0.355544, mean_q: 4.252018
 18133/100000: episode: 267, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 64.678, mean reward: 3.805 [2.530, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.405, 10.569], loss: 0.193032, mae: 0.387877, mean_q: 4.219029
 18150/100000: episode: 268, duration: 0.131s, episode steps: 17, steps per second: 130, episode reward: 74.703, mean reward: 4.394 [3.061, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.599], loss: 1.375077, mae: 0.535836, mean_q: 4.296748
 18161/100000: episode: 269, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 36.240, mean reward: 3.295 [2.494, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.432], loss: 2.148097, mae: 0.627159, mean_q: 4.519229
 18178/100000: episode: 270, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 48.715, mean reward: 2.866 [2.014, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.224, 10.331], loss: 0.333484, mae: 0.557967, mean_q: 4.386193
 18195/100000: episode: 271, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 61.643, mean reward: 3.626 [2.887, 6.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.642, 10.473], loss: 0.302411, mae: 0.495110, mean_q: 4.328322
 18212/100000: episode: 272, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 43.850, mean reward: 2.579 [2.083, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.428], loss: 0.435428, mae: 0.420159, mean_q: 4.265165
 18229/100000: episode: 273, duration: 0.111s, episode steps: 17, steps per second: 153, episode reward: 61.898, mean reward: 3.641 [2.414, 5.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.598], loss: 0.254520, mae: 0.459944, mean_q: 4.353207
 18246/100000: episode: 274, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 69.916, mean reward: 4.113 [2.733, 6.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.534], loss: 1.402434, mae: 0.524341, mean_q: 4.459042
 18263/100000: episode: 275, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 54.958, mean reward: 3.233 [2.376, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.089, 10.475], loss: 0.245257, mae: 0.436829, mean_q: 4.391364
 18280/100000: episode: 276, duration: 0.119s, episode steps: 17, steps per second: 143, episode reward: 62.307, mean reward: 3.665 [2.953, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.611, 10.450], loss: 0.137241, mae: 0.388050, mean_q: 4.345112
 18297/100000: episode: 277, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 46.726, mean reward: 2.749 [2.137, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.041, 10.297], loss: 0.161032, mae: 0.389546, mean_q: 4.385419
 18314/100000: episode: 278, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 45.453, mean reward: 2.674 [2.119, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.722, 10.386], loss: 0.181471, mae: 0.409817, mean_q: 4.493263
 18331/100000: episode: 279, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 60.892, mean reward: 3.582 [2.203, 10.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.468], loss: 0.127125, mae: 0.360056, mean_q: 4.404644
 18348/100000: episode: 280, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 52.887, mean reward: 3.111 [2.411, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.735, 10.497], loss: 0.156094, mae: 0.380220, mean_q: 4.370805
 18365/100000: episode: 281, duration: 0.133s, episode steps: 17, steps per second: 128, episode reward: 48.611, mean reward: 2.859 [2.328, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.048, 10.466], loss: 0.182800, mae: 0.389515, mean_q: 4.377720
 18382/100000: episode: 282, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 54.859, mean reward: 3.227 [2.459, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.578], loss: 0.162341, mae: 0.372700, mean_q: 4.350783
 18399/100000: episode: 283, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 71.223, mean reward: 4.190 [3.146, 5.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.411, 10.558], loss: 1.324787, mae: 0.509277, mean_q: 4.583606
 18416/100000: episode: 284, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 56.603, mean reward: 3.330 [2.384, 5.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.157, 10.471], loss: 0.252302, mae: 0.442978, mean_q: 4.412318
 18433/100000: episode: 285, duration: 0.119s, episode steps: 17, steps per second: 143, episode reward: 79.180, mean reward: 4.658 [2.981, 7.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.620], loss: 0.266995, mae: 0.453829, mean_q: 4.562018
 18450/100000: episode: 286, duration: 0.117s, episode steps: 17, steps per second: 145, episode reward: 80.210, mean reward: 4.718 [2.802, 7.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.207, 10.618], loss: 0.214287, mae: 0.436431, mean_q: 4.423396
 18467/100000: episode: 287, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 48.141, mean reward: 2.832 [2.381, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.700, 10.425], loss: 0.478191, mae: 0.457848, mean_q: 4.560458
 18484/100000: episode: 288, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 60.707, mean reward: 3.571 [2.599, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.647, 10.491], loss: 0.242196, mae: 0.479149, mean_q: 4.465055
 18501/100000: episode: 289, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 73.822, mean reward: 4.342 [2.624, 8.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.272, 10.471], loss: 0.736402, mae: 0.509147, mean_q: 4.482305
 18518/100000: episode: 290, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 73.601, mean reward: 4.329 [3.047, 5.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.703, 10.588], loss: 0.228005, mae: 0.422072, mean_q: 4.523884
 18535/100000: episode: 291, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 59.332, mean reward: 3.490 [2.689, 4.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.545], loss: 0.261076, mae: 0.449039, mean_q: 4.547338
 18552/100000: episode: 292, duration: 0.135s, episode steps: 17, steps per second: 126, episode reward: 45.946, mean reward: 2.703 [2.382, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.315, 10.362], loss: 0.207017, mae: 0.425606, mean_q: 4.562640
 18569/100000: episode: 293, duration: 0.120s, episode steps: 17, steps per second: 142, episode reward: 49.077, mean reward: 2.887 [2.468, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.173, 10.434], loss: 1.604284, mae: 0.564162, mean_q: 4.752328
 18586/100000: episode: 294, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 60.899, mean reward: 3.582 [2.595, 6.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.184, 10.490], loss: 0.450278, mae: 0.653301, mean_q: 4.653619
 18603/100000: episode: 295, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 59.956, mean reward: 3.527 [2.520, 8.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.719, 10.448], loss: 0.292905, mae: 0.504931, mean_q: 4.653597
 18620/100000: episode: 296, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 55.454, mean reward: 3.262 [2.206, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.538], loss: 0.268206, mae: 0.468536, mean_q: 4.538639
 18637/100000: episode: 297, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 57.352, mean reward: 3.374 [2.507, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.517], loss: 0.334959, mae: 0.470579, mean_q: 4.615155
 18654/100000: episode: 298, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 48.506, mean reward: 2.853 [2.347, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.427], loss: 0.262179, mae: 0.487386, mean_q: 4.773869
 18671/100000: episode: 299, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 61.602, mean reward: 3.624 [2.536, 4.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.187, 10.507], loss: 0.141261, mae: 0.379028, mean_q: 4.577877
 18688/100000: episode: 300, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 70.932, mean reward: 4.172 [3.039, 7.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.658], loss: 0.237771, mae: 0.446725, mean_q: 4.759724
 18705/100000: episode: 301, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 49.718, mean reward: 2.925 [2.461, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.142, 10.417], loss: 0.263289, mae: 0.461364, mean_q: 4.674763
 18722/100000: episode: 302, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 55.975, mean reward: 3.293 [2.601, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.458], loss: 0.203917, mae: 0.429667, mean_q: 4.714969
 18739/100000: episode: 303, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 52.749, mean reward: 3.103 [2.532, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.470], loss: 0.218411, mae: 0.436513, mean_q: 4.640478
 18756/100000: episode: 304, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 60.954, mean reward: 3.586 [2.407, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.422, 10.589], loss: 0.296331, mae: 0.450438, mean_q: 4.668474
 18773/100000: episode: 305, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 49.365, mean reward: 2.904 [1.961, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-1.185, 10.318], loss: 0.149001, mae: 0.380916, mean_q: 4.651373
 18784/100000: episode: 306, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 36.860, mean reward: 3.351 [2.533, 5.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.596], loss: 0.221918, mae: 0.440852, mean_q: 4.576710
 18801/100000: episode: 307, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 46.423, mean reward: 2.731 [2.063, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.349], loss: 0.196313, mae: 0.425787, mean_q: 4.708900
 18818/100000: episode: 308, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 48.256, mean reward: 2.839 [1.674, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.514, 10.234], loss: 0.260854, mae: 0.446073, mean_q: 4.716938
 18835/100000: episode: 309, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 49.989, mean reward: 2.941 [2.617, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.121, 10.436], loss: 0.160250, mae: 0.402366, mean_q: 4.649287
 18852/100000: episode: 310, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 49.742, mean reward: 2.926 [2.448, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.386], loss: 0.208943, mae: 0.395627, mean_q: 4.650341
 18869/100000: episode: 311, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 51.962, mean reward: 3.057 [2.349, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.206, 10.312], loss: 0.208135, mae: 0.431181, mean_q: 4.817662
 18886/100000: episode: 312, duration: 0.120s, episode steps: 17, steps per second: 142, episode reward: 59.201, mean reward: 3.482 [2.624, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.496, 10.541], loss: 0.174446, mae: 0.418559, mean_q: 4.706796
 18897/100000: episode: 313, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 48.170, mean reward: 4.379 [2.846, 7.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.572], loss: 0.339031, mae: 0.517510, mean_q: 4.759273
 18914/100000: episode: 314, duration: 0.121s, episode steps: 17, steps per second: 141, episode reward: 42.019, mean reward: 2.472 [1.987, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.182, 10.300], loss: 0.181613, mae: 0.405374, mean_q: 4.676304
 18931/100000: episode: 315, duration: 0.123s, episode steps: 17, steps per second: 138, episode reward: 59.874, mean reward: 3.522 [2.722, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.060, 10.450], loss: 0.315238, mae: 0.493719, mean_q: 4.728290
 18948/100000: episode: 316, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 51.981, mean reward: 3.058 [2.313, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.418], loss: 0.330811, mae: 0.525479, mean_q: 4.935581
 18965/100000: episode: 317, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 54.052, mean reward: 3.180 [2.776, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.078, 10.482], loss: 0.606896, mae: 0.582581, mean_q: 4.817073
 18982/100000: episode: 318, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 58.792, mean reward: 3.458 [2.339, 5.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.262, 10.363], loss: 1.408794, mae: 0.602893, mean_q: 5.008772
 18999/100000: episode: 319, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 48.959, mean reward: 2.880 [2.283, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.296, 10.396], loss: 0.275026, mae: 0.490450, mean_q: 4.583225
 19016/100000: episode: 320, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 63.227, mean reward: 3.719 [2.963, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.489], loss: 2.631816, mae: 0.900178, mean_q: 4.917238
 19033/100000: episode: 321, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 50.751, mean reward: 2.985 [2.048, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.288], loss: 0.418524, mae: 0.620778, mean_q: 4.842830
 19050/100000: episode: 322, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 70.706, mean reward: 4.159 [3.173, 6.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.536], loss: 0.263607, mae: 0.518088, mean_q: 4.763319
 19067/100000: episode: 323, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 57.661, mean reward: 3.392 [2.062, 6.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.648, 10.394], loss: 0.251551, mae: 0.489206, mean_q: 4.860017
 19084/100000: episode: 324, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 47.636, mean reward: 2.802 [2.083, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.339], loss: 0.288550, mae: 0.464303, mean_q: 4.758951
 19101/100000: episode: 325, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 56.366, mean reward: 3.316 [2.657, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.544], loss: 0.287899, mae: 0.497324, mean_q: 4.836067
 19118/100000: episode: 326, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 60.770, mean reward: 3.575 [2.598, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.520], loss: 0.291443, mae: 0.468234, mean_q: 4.902974
 19135/100000: episode: 327, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 41.208, mean reward: 2.424 [1.719, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.436, 10.272], loss: 1.281006, mae: 0.493298, mean_q: 4.784512
 19152/100000: episode: 328, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 64.591, mean reward: 3.799 [3.020, 4.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.691, 10.555], loss: 0.266702, mae: 0.455701, mean_q: 4.839406
[Info] 3-TH LEVEL FOUND: 8.618775367736816, Considering 10/90 traces
 19169/100000: episode: 329, duration: 4.193s, episode steps: 17, steps per second: 4, episode reward: 64.296, mean reward: 3.782 [2.858, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.208, 10.519], loss: 0.334831, mae: 0.477649, mean_q: 4.902864
 19179/100000: episode: 330, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 52.628, mean reward: 5.263 [3.856, 8.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.300, 10.646], loss: 0.315583, mae: 0.470371, mean_q: 4.901542
 19187/100000: episode: 331, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 39.855, mean reward: 4.982 [3.922, 8.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.462, 10.502], loss: 0.215070, mae: 0.455145, mean_q: 4.975292
 19196/100000: episode: 332, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 80.095, mean reward: 8.899 [3.575, 27.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.703, 10.491], loss: 0.184548, mae: 0.423935, mean_q: 5.029445
 19207/100000: episode: 333, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 109.311, mean reward: 9.937 [4.433, 22.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.112, 10.689], loss: 0.648895, mae: 0.496976, mean_q: 4.921827
 19216/100000: episode: 334, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 44.996, mean reward: 5.000 [3.708, 7.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.036, 10.639], loss: 0.230534, mae: 0.431678, mean_q: 4.880251
 19225/100000: episode: 335, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 40.565, mean reward: 4.507 [3.413, 7.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.686, 10.420], loss: 0.417781, mae: 0.540583, mean_q: 5.142982
 19237/100000: episode: 336, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 42.253, mean reward: 3.521 [2.436, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.471], loss: 0.367764, mae: 0.463370, mean_q: 4.943401
 19241/100000: episode: 337, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 20.781, mean reward: 5.195 [4.659, 5.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.567], loss: 0.393910, mae: 0.420790, mean_q: 4.661829
 19252/100000: episode: 338, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 57.197, mean reward: 5.200 [3.666, 6.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.582], loss: 0.251763, mae: 0.446173, mean_q: 5.052071
 19262/100000: episode: 339, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 39.667, mean reward: 3.967 [3.371, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.350, 10.573], loss: 0.204746, mae: 0.448292, mean_q: 4.822078
 19273/100000: episode: 340, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 58.267, mean reward: 5.297 [4.116, 7.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.578], loss: 0.248126, mae: 0.476311, mean_q: 4.869827
 19281/100000: episode: 341, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 51.323, mean reward: 6.415 [4.243, 12.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.655], loss: 0.486728, mae: 0.575406, mean_q: 5.304771
 19291/100000: episode: 342, duration: 0.062s, episode steps: 10, steps per second: 163, episode reward: 44.411, mean reward: 4.441 [2.978, 5.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.174, 10.492], loss: 0.174210, mae: 0.451431, mean_q: 5.085484
 19301/100000: episode: 343, duration: 0.073s, episode steps: 10, steps per second: 136, episode reward: 67.227, mean reward: 6.723 [3.759, 14.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.511, 10.550], loss: 0.212021, mae: 0.419102, mean_q: 4.795163
 19311/100000: episode: 344, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 36.102, mean reward: 3.610 [2.547, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.402], loss: 0.374866, mae: 0.540120, mean_q: 5.166994
 19314/100000: episode: 345, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 43.639, mean reward: 14.546 [10.053, 20.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.101, 10.693], loss: 0.247121, mae: 0.507170, mean_q: 5.202670
 19324/100000: episode: 346, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 53.233, mean reward: 5.323 [4.036, 9.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.480], loss: 0.380220, mae: 0.523505, mean_q: 5.107509
 19328/100000: episode: 347, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 27.305, mean reward: 6.826 [5.484, 7.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.479, 10.596], loss: 0.255912, mae: 0.421615, mean_q: 4.678225
 19331/100000: episode: 348, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 14.071, mean reward: 4.690 [4.595, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.522], loss: 0.291642, mae: 0.530717, mean_q: 5.127488
 19340/100000: episode: 349, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 33.223, mean reward: 3.691 [2.780, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.466, 10.308], loss: 0.304296, mae: 0.465858, mean_q: 5.056916
 19348/100000: episode: 350, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 43.540, mean reward: 5.442 [3.550, 9.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.171, 10.561], loss: 0.320437, mae: 0.520595, mean_q: 4.996495
 19356/100000: episode: 351, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 35.549, mean reward: 4.444 [3.133, 5.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-1.294, 10.583], loss: 0.210712, mae: 0.438734, mean_q: 4.832106
 19367/100000: episode: 352, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 64.449, mean reward: 5.859 [4.289, 8.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.618], loss: 1.265418, mae: 0.715110, mean_q: 5.071951
 19375/100000: episode: 353, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 23.123, mean reward: 2.890 [2.283, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-1.021, 10.448], loss: 0.414154, mae: 0.604510, mean_q: 5.334729
 19386/100000: episode: 354, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 61.486, mean reward: 5.590 [4.077, 9.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.657], loss: 0.281454, mae: 0.528561, mean_q: 4.938298
 19397/100000: episode: 355, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 101.454, mean reward: 9.223 [5.016, 24.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.213, 10.568], loss: 0.790962, mae: 0.622129, mean_q: 5.120397
 19401/100000: episode: 356, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 31.095, mean reward: 7.774 [5.784, 10.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.580], loss: 0.643961, mae: 0.589739, mean_q: 5.074965
 19412/100000: episode: 357, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 48.621, mean reward: 4.420 [2.990, 7.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.278, 10.464], loss: 0.324612, mae: 0.507495, mean_q: 5.142118
 19422/100000: episode: 358, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 71.589, mean reward: 7.159 [3.693, 12.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.591, 10.546], loss: 0.323818, mae: 0.531029, mean_q: 5.175907
 19432/100000: episode: 359, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 63.442, mean reward: 6.344 [4.300, 7.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.641], loss: 0.570414, mae: 0.613337, mean_q: 5.327865
 19441/100000: episode: 360, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 30.189, mean reward: 3.354 [2.653, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.545], loss: 2.610520, mae: 0.756657, mean_q: 5.326094
 19445/100000: episode: 361, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 27.748, mean reward: 6.937 [6.339, 8.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.655], loss: 4.849330, mae: 0.898646, mean_q: 5.324858
 19449/100000: episode: 362, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 36.873, mean reward: 9.218 [6.363, 10.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.652], loss: 0.549602, mae: 0.609759, mean_q: 5.206677
 19453/100000: episode: 363, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 22.102, mean reward: 5.525 [5.020, 6.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.604], loss: 0.352815, mae: 0.536425, mean_q: 5.355783
 19465/100000: episode: 364, duration: 0.079s, episode steps: 12, steps per second: 153, episode reward: 75.596, mean reward: 6.300 [5.071, 8.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.516, 10.583], loss: 1.089051, mae: 0.689866, mean_q: 5.305059
 19477/100000: episode: 365, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 51.507, mean reward: 4.292 [2.873, 6.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.472], loss: 0.797931, mae: 0.645744, mean_q: 5.431822
 19489/100000: episode: 366, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 65.542, mean reward: 5.462 [4.057, 6.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.618], loss: 2.047008, mae: 0.905750, mean_q: 5.321074
 19498/100000: episode: 367, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 31.743, mean reward: 3.527 [2.792, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.575, 10.450], loss: 0.900390, mae: 0.783273, mean_q: 5.439727
 19507/100000: episode: 368, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 60.239, mean reward: 6.693 [3.926, 15.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.254, 10.549], loss: 0.504262, mae: 0.659958, mean_q: 5.190385
 19518/100000: episode: 369, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 41.622, mean reward: 3.784 [2.641, 6.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.510], loss: 0.475658, mae: 0.619651, mean_q: 5.283835
 19528/100000: episode: 370, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 73.954, mean reward: 7.395 [5.095, 10.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.725, 10.617], loss: 0.844323, mae: 0.596684, mean_q: 5.332263
 19537/100000: episode: 371, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 50.789, mean reward: 5.643 [3.765, 8.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.834, 10.621], loss: 0.581106, mae: 0.591297, mean_q: 5.434042
 19541/100000: episode: 372, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 21.143, mean reward: 5.286 [4.676, 6.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.035, 10.616], loss: 0.424884, mae: 0.554660, mean_q: 5.481748
 19553/100000: episode: 373, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 47.880, mean reward: 3.990 [2.788, 5.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.558], loss: 0.527288, mae: 0.649782, mean_q: 5.345968
 19564/100000: episode: 374, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 57.028, mean reward: 5.184 [3.619, 8.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.027, 10.490], loss: 0.709684, mae: 0.699690, mean_q: 5.308208
 19573/100000: episode: 375, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 51.008, mean reward: 5.668 [4.534, 8.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.183, 10.673], loss: 1.076694, mae: 0.685996, mean_q: 5.458553
 19577/100000: episode: 376, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 20.842, mean reward: 5.211 [4.667, 5.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.458], loss: 0.321678, mae: 0.548730, mean_q: 4.865126
 19588/100000: episode: 377, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 49.512, mean reward: 4.501 [3.704, 5.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.885, 10.572], loss: 0.256700, mae: 0.526697, mean_q: 5.436737
 19600/100000: episode: 378, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 57.449, mean reward: 4.787 [2.677, 7.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.594], loss: 0.481376, mae: 0.626608, mean_q: 5.406903
 19612/100000: episode: 379, duration: 0.102s, episode steps: 12, steps per second: 117, episode reward: 58.025, mean reward: 4.835 [2.936, 9.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.753, 10.663], loss: 0.467225, mae: 0.609739, mean_q: 5.380577
 19621/100000: episode: 380, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 45.585, mean reward: 5.065 [3.391, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.238, 10.557], loss: 0.978669, mae: 0.702415, mean_q: 5.382274
 19631/100000: episode: 381, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 48.145, mean reward: 4.814 [3.866, 5.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.568], loss: 0.455015, mae: 0.620982, mean_q: 5.630284
 19643/100000: episode: 382, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 144.836, mean reward: 12.070 [3.004, 45.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.588], loss: 0.814863, mae: 0.652656, mean_q: 5.298547
 19653/100000: episode: 383, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 54.041, mean reward: 5.404 [4.594, 6.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.567], loss: 0.622966, mae: 0.685525, mean_q: 5.626054
 19664/100000: episode: 384, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 68.785, mean reward: 6.253 [4.646, 8.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.599], loss: 1.185853, mae: 0.808750, mean_q: 5.633382
 19672/100000: episode: 385, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 40.406, mean reward: 5.051 [3.346, 8.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.444], loss: 0.956161, mae: 0.725967, mean_q: 5.562914
 19676/100000: episode: 386, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 21.643, mean reward: 5.411 [4.738, 6.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.535], loss: 0.280706, mae: 0.542183, mean_q: 5.196539
 19679/100000: episode: 387, duration: 0.030s, episode steps: 3, steps per second: 101, episode reward: 20.937, mean reward: 6.979 [5.815, 7.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.556], loss: 0.485891, mae: 0.583153, mean_q: 5.045578
 19687/100000: episode: 388, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 32.286, mean reward: 4.036 [2.990, 5.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.612], loss: 0.828778, mae: 0.589699, mean_q: 5.432629
 19699/100000: episode: 389, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 109.207, mean reward: 9.101 [4.944, 17.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.687], loss: 3.563467, mae: 0.812543, mean_q: 5.588350
 19702/100000: episode: 390, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 21.597, mean reward: 7.199 [6.612, 7.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.628], loss: 1.207726, mae: 1.141543, mean_q: 6.238668
 19714/100000: episode: 391, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 65.453, mean reward: 5.454 [3.675, 8.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.511], loss: 0.993717, mae: 0.887984, mean_q: 5.423892
 19724/100000: episode: 392, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 79.091, mean reward: 7.909 [4.778, 11.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.650, 10.666], loss: 0.758540, mae: 0.592937, mean_q: 5.556636
 19736/100000: episode: 393, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 44.908, mean reward: 3.742 [3.141, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.058, 10.473], loss: 0.479060, mae: 0.615108, mean_q: 5.594646
 19747/100000: episode: 394, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 162.882, mean reward: 14.807 [4.304, 44.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-1.005, 10.593], loss: 2.492197, mae: 0.891740, mean_q: 5.718500
 19759/100000: episode: 395, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 62.624, mean reward: 5.219 [3.956, 7.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.288, 10.524], loss: 2.946670, mae: 0.970082, mean_q: 5.818797
 19763/100000: episode: 396, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 26.074, mean reward: 6.519 [5.121, 7.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.645], loss: 0.459113, mae: 0.650479, mean_q: 5.143987
 19773/100000: episode: 397, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 52.125, mean reward: 5.212 [4.060, 6.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.680, 10.542], loss: 0.861059, mae: 0.762137, mean_q: 5.662330
 19785/100000: episode: 398, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 42.553, mean reward: 3.546 [2.543, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.577], loss: 1.015396, mae: 0.672843, mean_q: 5.746476
 19795/100000: episode: 399, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 40.604, mean reward: 4.060 [3.514, 5.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.581], loss: 0.607498, mae: 0.709840, mean_q: 5.699430
 19803/100000: episode: 400, duration: 0.060s, episode steps: 8, steps per second: 133, episode reward: 26.679, mean reward: 3.335 [2.729, 4.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.499], loss: 0.676520, mae: 0.632768, mean_q: 5.493146
 19815/100000: episode: 401, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 43.825, mean reward: 3.652 [2.949, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.636, 10.500], loss: 0.412611, mae: 0.617355, mean_q: 5.776685
 19824/100000: episode: 402, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 45.081, mean reward: 5.009 [3.651, 6.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.592], loss: 0.681500, mae: 0.665782, mean_q: 5.582484
 19827/100000: episode: 403, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 22.639, mean reward: 7.546 [6.814, 8.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.664], loss: 0.358047, mae: 0.549523, mean_q: 5.468121
 19836/100000: episode: 404, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 46.768, mean reward: 5.196 [3.490, 8.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.344, 10.662], loss: 0.820287, mae: 0.698202, mean_q: 5.819854
 19848/100000: episode: 405, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 119.505, mean reward: 9.959 [5.049, 44.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.097, 10.773], loss: 1.497435, mae: 0.871038, mean_q: 5.874949
 19851/100000: episode: 406, duration: 0.020s, episode steps: 3, steps per second: 154, episode reward: 20.144, mean reward: 6.715 [6.260, 7.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.592], loss: 0.606721, mae: 0.683950, mean_q: 5.483274
 19863/100000: episode: 407, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 81.384, mean reward: 6.782 [4.349, 9.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.428, 10.506], loss: 2.570964, mae: 0.957583, mean_q: 5.905149
 19873/100000: episode: 408, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 50.477, mean reward: 5.048 [3.797, 6.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.539], loss: 1.308270, mae: 0.757469, mean_q: 5.610921
 19882/100000: episode: 409, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 55.695, mean reward: 6.188 [3.221, 8.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-1.323, 10.683], loss: 0.728439, mae: 0.704220, mean_q: 5.592387
 19892/100000: episode: 410, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 56.016, mean reward: 5.602 [3.831, 7.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.519], loss: 3.187373, mae: 1.053969, mean_q: 6.116099
 19902/100000: episode: 411, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 144.221, mean reward: 14.422 [4.160, 44.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.500, 10.619], loss: 0.543963, mae: 0.686032, mean_q: 5.708488
 19912/100000: episode: 412, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 49.743, mean reward: 4.974 [3.639, 6.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.105, 10.596], loss: 0.528424, mae: 0.674846, mean_q: 5.714072
 19922/100000: episode: 413, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 37.147, mean reward: 3.715 [3.018, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.365], loss: 1.969174, mae: 0.864800, mean_q: 6.057542
 19934/100000: episode: 414, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 40.892, mean reward: 3.408 [2.924, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.516], loss: 4.342196, mae: 0.982093, mean_q: 5.938507
 19944/100000: episode: 415, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 47.128, mean reward: 4.713 [3.607, 6.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.366, 10.591], loss: 0.502447, mae: 0.654396, mean_q: 5.694815
 19953/100000: episode: 416, duration: 0.061s, episode steps: 9, steps per second: 146, episode reward: 33.046, mean reward: 3.672 [3.110, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.289, 10.564], loss: 7.100203, mae: 1.267739, mean_q: 6.122959
 19961/100000: episode: 417, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 36.586, mean reward: 4.573 [3.818, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.571], loss: 1.620611, mae: 1.228284, mean_q: 5.674008
 19971/100000: episode: 418, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 56.079, mean reward: 5.608 [4.061, 9.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.514], loss: 1.307249, mae: 0.963247, mean_q: 6.165184
[Info] 4-TH LEVEL FOUND: 13.944093704223633, Considering 10/90 traces
 19979/100000: episode: 419, duration: 4.077s, episode steps: 8, steps per second: 2, episode reward: 38.588, mean reward: 4.824 [3.433, 7.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-1.406, 10.442], loss: 5.650314, mae: 0.905526, mean_q: 5.668606
 19981/100000: episode: 420, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 11.044, mean reward: 5.522 [5.168, 5.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.595], loss: 3.501896, mae: 1.775997, mean_q: 7.913361
 19987/100000: episode: 421, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 46.005, mean reward: 7.668 [5.404, 9.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.588], loss: 1.004144, mae: 0.815345, mean_q: 5.819467
 19994/100000: episode: 422, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 50.569, mean reward: 7.224 [4.606, 12.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.668], loss: 4.088187, mae: 0.920020, mean_q: 5.939326
 20002/100000: episode: 423, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 91.168, mean reward: 11.396 [8.067, 20.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.534], loss: 3.146780, mae: 0.921103, mean_q: 5.841703
 20008/100000: episode: 424, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 65.259, mean reward: 10.877 [8.559, 13.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.715], loss: 0.651879, mae: 0.694586, mean_q: 5.984594
 20016/100000: episode: 425, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 90.903, mean reward: 11.363 [7.009, 21.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.270, 10.582], loss: 3.909226, mae: 1.165558, mean_q: 6.313845
 20021/100000: episode: 426, duration: 0.026s, episode steps: 5, steps per second: 189, episode reward: 31.671, mean reward: 6.334 [5.512, 7.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.589], loss: 0.716094, mae: 0.798793, mean_q: 5.217043
 20029/100000: episode: 427, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 137.055, mean reward: 17.132 [6.630, 38.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.613, 10.685], loss: 1.330078, mae: 0.838256, mean_q: 6.268810
 20034/100000: episode: 428, duration: 0.038s, episode steps: 5, steps per second: 133, episode reward: 36.944, mean reward: 7.389 [5.520, 10.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.948, 10.520], loss: 0.728226, mae: 0.720909, mean_q: 6.191560
 20038/100000: episode: 429, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 16.796, mean reward: 4.199 [3.617, 5.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.748, 10.574], loss: 1.962656, mae: 1.002820, mean_q: 6.147649
 20040/100000: episode: 430, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 15.314, mean reward: 7.657 [6.741, 8.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.580], loss: 1.035656, mae: 0.826467, mean_q: 6.423312
 20042/100000: episode: 431, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 19.839, mean reward: 9.920 [9.860, 9.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.559], loss: 0.436811, mae: 0.659191, mean_q: 5.727668
 20044/100000: episode: 432, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 12.200, mean reward: 6.100 [4.317, 7.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.035, 10.666], loss: 0.592834, mae: 0.708307, mean_q: 5.273817
 20050/100000: episode: 433, duration: 0.046s, episode steps: 6, steps per second: 132, episode reward: 54.521, mean reward: 9.087 [3.952, 16.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.574], loss: 1.444637, mae: 1.067056, mean_q: 6.425947
 20052/100000: episode: 434, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 13.803, mean reward: 6.902 [6.188, 7.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.035, 10.667], loss: 5.862535, mae: 1.462184, mean_q: 6.472965
 20059/100000: episode: 435, duration: 0.054s, episode steps: 7, steps per second: 130, episode reward: 51.018, mean reward: 7.288 [5.785, 10.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.619], loss: 0.457371, mae: 0.634060, mean_q: 5.852487
 20067/100000: episode: 436, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 66.287, mean reward: 8.286 [5.709, 10.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.569], loss: 0.806463, mae: 0.685568, mean_q: 6.049387
 20072/100000: episode: 437, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 34.294, mean reward: 6.859 [5.604, 8.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.507, 10.609], loss: 2.445168, mae: 0.830332, mean_q: 5.920187
 20077/100000: episode: 438, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 29.896, mean reward: 5.979 [4.463, 7.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.618], loss: 4.719741, mae: 1.096341, mean_q: 6.469508
 20084/100000: episode: 439, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 35.179, mean reward: 5.026 [4.103, 5.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.508], loss: 0.731361, mae: 0.807564, mean_q: 5.973504
 20092/100000: episode: 440, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 86.440, mean reward: 10.805 [5.713, 21.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.752], loss: 0.797862, mae: 0.841117, mean_q: 6.345179
 20099/100000: episode: 441, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 58.192, mean reward: 8.313 [5.839, 14.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.141, 10.682], loss: 3.106818, mae: 0.980981, mean_q: 6.332618
 20106/100000: episode: 442, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 38.852, mean reward: 5.550 [4.545, 6.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.571], loss: 1.601688, mae: 1.012591, mean_q: 6.593557
 20108/100000: episode: 443, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 23.831, mean reward: 11.915 [10.648, 13.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.329, 10.469], loss: 0.353873, mae: 0.601247, mean_q: 5.786090
 20110/100000: episode: 444, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 31.328, mean reward: 15.664 [13.227, 18.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.711], loss: 1.250141, mae: 0.989882, mean_q: 6.156734
 20115/100000: episode: 445, duration: 0.059s, episode steps: 5, steps per second: 85, episode reward: 39.824, mean reward: 7.965 [7.440, 8.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.607], loss: 1.120584, mae: 0.730585, mean_q: 6.252551
 20117/100000: episode: 446, duration: 0.035s, episode steps: 2, steps per second: 57, episode reward: 10.822, mean reward: 5.411 [4.912, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.546], loss: 0.914741, mae: 0.950828, mean_q: 6.380848
 20119/100000: episode: 447, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 12.427, mean reward: 6.214 [5.994, 6.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.588], loss: 2.403796, mae: 1.176441, mean_q: 6.767379
 20123/100000: episode: 448, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 20.940, mean reward: 5.235 [4.462, 5.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.625], loss: 0.895064, mae: 0.736270, mean_q: 5.746990
 20125/100000: episode: 449, duration: 0.029s, episode steps: 2, steps per second: 69, episode reward: 13.210, mean reward: 6.605 [5.634, 7.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.588], loss: 0.872292, mae: 0.865518, mean_q: 5.809536
 20131/100000: episode: 450, duration: 0.067s, episode steps: 6, steps per second: 89, episode reward: 36.966, mean reward: 6.161 [5.743, 6.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.615], loss: 1.156738, mae: 0.898245, mean_q: 6.440794
 20137/100000: episode: 451, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 38.547, mean reward: 6.424 [5.628, 8.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.576], loss: 0.882372, mae: 0.730670, mean_q: 6.209377
[Info] FALSIFICATION!
 20138/100000: episode: 452, duration: 0.494s, episode steps: 1, steps per second: 2, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.009, 10.224], loss: 1.688439, mae: 0.980440, mean_q: 5.746338
 20146/100000: episode: 453, duration: 0.081s, episode steps: 8, steps per second: 99, episode reward: 95.706, mean reward: 11.963 [5.735, 20.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.387, 10.662], loss: 0.704924, mae: 0.808537, mean_q: 6.637988
 20154/100000: episode: 454, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 543.065, mean reward: 67.883 [10.048, 214.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.373, 10.652], loss: 1918.709961, mae: 7.727290, mean_q: 9.857190
 20156/100000: episode: 455, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 14.120, mean reward: 7.060 [6.840, 7.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.617], loss: 8.220751, mae: 3.182833, mean_q: 8.454444
[Info] FALSIFICATION!
 20162/100000: episode: 456, duration: 0.304s, episode steps: 6, steps per second: 20, episode reward: 1128.177, mean reward: 188.030 [8.860, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-1.510, 10.779], loss: 5.633663, mae: 2.494528, mean_q: 4.185068
 20166/100000: episode: 457, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 27.722, mean reward: 6.930 [6.375, 8.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.647], loss: 4.557163, mae: 2.017079, mean_q: 5.401998
 20168/100000: episode: 458, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 9.340, mean reward: 4.670 [4.265, 5.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.035, 10.590], loss: 3.002706, mae: 2.044694, mean_q: 7.204040
 20172/100000: episode: 459, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 18.293, mean reward: 4.573 [4.023, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.574], loss: 5.190483, mae: 2.713098, mean_q: 7.901916
 20178/100000: episode: 460, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 53.667, mean reward: 8.944 [5.741, 20.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.579, 10.555], loss: 2.843343, mae: 1.782282, mean_q: 7.270895
 20185/100000: episode: 461, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 161.442, mean reward: 23.063 [6.976, 50.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.348, 10.688], loss: 2207.059814, mae: 5.928904, mean_q: 6.171735
 20190/100000: episode: 462, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 47.844, mean reward: 9.569 [8.033, 13.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.534], loss: 23.834253, mae: 3.947898, mean_q: 9.634480
 20192/100000: episode: 463, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 10.308, mean reward: 5.154 [5.033, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.035, 10.586], loss: 10.043347, mae: 4.236370, mean_q: 10.526985
 20196/100000: episode: 464, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 37.086, mean reward: 9.271 [4.500, 12.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.714], loss: 4.643265, mae: 2.733514, mean_q: 8.979404
 20198/100000: episode: 465, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 7.801, mean reward: 3.900 [3.722, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.543], loss: 1.955575, mae: 1.510405, mean_q: 6.852589
 20203/100000: episode: 466, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 145.128, mean reward: 29.026 [7.490, 97.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.464], loss: 2.288407, mae: 1.339842, mean_q: 5.894300
[Info] FALSIFICATION!
 20208/100000: episode: 467, duration: 0.231s, episode steps: 5, steps per second: 22, episode reward: 1040.525, mean reward: 208.105 [8.961, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-1.699, 10.630], loss: 1.491812, mae: 1.316600, mean_q: 5.034039
 20215/100000: episode: 468, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 45.848, mean reward: 6.550 [3.799, 8.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.564], loss: 22.115702, mae: 1.755732, mean_q: 5.391653
 20217/100000: episode: 469, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 23.652, mean reward: 11.826 [11.226, 12.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.679], loss: 1.230724, mae: 0.971195, mean_q: 5.812554
 20223/100000: episode: 470, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 52.553, mean reward: 8.759 [6.127, 14.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.564], loss: 2560.757568, mae: 7.976744, mean_q: 8.682897
 20229/100000: episode: 471, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 29.565, mean reward: 4.927 [3.399, 5.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.546], loss: 13.589421, mae: 4.768755, mean_q: 11.301675
 20234/100000: episode: 472, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 38.909, mean reward: 7.782 [6.203, 9.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.680], loss: 2.211141, mae: 1.614057, mean_q: 7.758218
 20238/100000: episode: 473, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 20.738, mean reward: 5.184 [4.492, 5.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.733, 10.493], loss: 1.861246, mae: 1.137110, mean_q: 5.969721
[Info] FALSIFICATION!
 20243/100000: episode: 474, duration: 0.213s, episode steps: 5, steps per second: 24, episode reward: 1100.167, mean reward: 220.033 [19.466, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.737], loss: 2.104631, mae: 1.298618, mean_q: 5.495487
 20250/100000: episode: 475, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 38.774, mean reward: 5.539 [4.499, 6.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.502], loss: 52.185535, mae: 2.024064, mean_q: 6.171249
 20255/100000: episode: 476, duration: 0.033s, episode steps: 5, steps per second: 149, episode reward: 109.832, mean reward: 21.966 [7.647, 55.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.495, 10.655], loss: 3059.466309, mae: 8.849689, mean_q: 8.508946
 20257/100000: episode: 477, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 9.469, mean reward: 4.734 [4.560, 4.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.508], loss: 18.792774, mae: 5.264910, mean_q: 11.882625
 20264/100000: episode: 478, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 46.069, mean reward: 6.581 [5.562, 8.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.601], loss: 10.104463, mae: 3.528301, mean_q: 10.064131
 20268/100000: episode: 479, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 25.522, mean reward: 6.381 [4.115, 11.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.447, 10.487], loss: 1.576680, mae: 1.256222, mean_q: 7.085256
 20276/100000: episode: 480, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 106.759, mean reward: 13.345 [7.363, 21.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.716], loss: 3.022500, mae: 1.354359, mean_q: 5.863665
 20281/100000: episode: 481, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 24.684, mean reward: 4.937 [3.580, 6.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.568], loss: 3212.973145, mae: 8.751516, mean_q: 6.103861
 20288/100000: episode: 482, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 81.642, mean reward: 11.663 [5.643, 36.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.380, 10.682], loss: 6.285469, mae: 2.790282, mean_q: 9.514531
 20294/100000: episode: 483, duration: 0.043s, episode steps: 6, steps per second: 141, episode reward: 38.713, mean reward: 6.452 [4.631, 7.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.669], loss: 6.572322, mae: 2.693588, mean_q: 9.163401
 20300/100000: episode: 484, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 30.976, mean reward: 5.163 [4.812, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.565], loss: 5118.684082, mae: 13.074276, mean_q: 7.820901
 20308/100000: episode: 485, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 183.169, mean reward: 22.896 [8.318, 43.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.162, 10.567], loss: 1895.530273, mae: 11.160276, mean_q: 14.364981
 20313/100000: episode: 486, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 431.641, mean reward: 86.328 [10.293, 201.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.654], loss: 26.753790, mae: 6.161530, mean_q: 13.863899
 20319/100000: episode: 487, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 31.234, mean reward: 5.206 [4.090, 7.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.087, 10.591], loss: 66.390800, mae: 2.986624, mean_q: 7.065763
 20324/100000: episode: 488, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 112.179, mean reward: 22.436 [9.653, 37.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.707], loss: 4.451973, mae: 2.116866, mean_q: 5.564106
 20326/100000: episode: 489, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 12.993, mean reward: 6.496 [5.694, 7.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.652], loss: 3.183942, mae: 1.674323, mean_q: 5.153160
 20328/100000: episode: 490, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 8.534, mean reward: 4.267 [4.133, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.563], loss: 3.470522, mae: 1.911552, mean_q: 6.032480
 20334/100000: episode: 491, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 51.140, mean reward: 8.523 [6.952, 10.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.134, 10.623], loss: 8.409928, mae: 1.776211, mean_q: 6.646133
 20336/100000: episode: 492, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 22.576, mean reward: 11.288 [9.663, 12.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.035, 10.710], loss: 1.787211, mae: 1.379334, mean_q: 6.874664
 20342/100000: episode: 493, duration: 0.048s, episode steps: 6, steps per second: 124, episode reward: 523.658, mean reward: 87.276 [7.751, 467.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.469, 10.642], loss: 564.391663, mae: 4.195018, mean_q: 7.860152
 20344/100000: episode: 494, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 13.158, mean reward: 6.579 [6.041, 7.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.579], loss: 20.970882, mae: 2.924530, mean_q: 8.399807
 20348/100000: episode: 495, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 25.226, mean reward: 6.306 [4.975, 7.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.627], loss: 10.844658, mae: 2.885975, mean_q: 9.304593
 20353/100000: episode: 496, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 114.341, mean reward: 22.868 [10.330, 48.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.738], loss: 119.108910, mae: 3.913776, mean_q: 10.039680
 20355/100000: episode: 497, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 8.161, mean reward: 4.080 [3.927, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.509], loss: 1653.562622, mae: 9.792124, mean_q: 9.732454
 20361/100000: episode: 498, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 72.459, mean reward: 12.077 [7.103, 17.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.668], loss: 6.140776, mae: 2.562991, mean_q: 9.773755
 20368/100000: episode: 499, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 35.943, mean reward: 5.135 [4.132, 6.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.632], loss: 11.157187, mae: 1.830739, mean_q: 8.302895
 20374/100000: episode: 500, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 29.517, mean reward: 4.919 [4.207, 5.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.552], loss: 16.705036, mae: 2.124385, mean_q: 7.084139
[Info] FALSIFICATION!
 20376/100000: episode: 501, duration: 0.312s, episode steps: 2, steps per second: 6, episode reward: 1009.660, mean reward: 504.830 [9.660, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.016, 10.756], loss: 1.789376, mae: 1.300893, mean_q: 7.283587
 20378/100000: episode: 502, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.956, mean reward: 5.478 [5.364, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.600], loss: 1.993085, mae: 1.281720, mean_q: 7.353204
 20383/100000: episode: 503, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 156.783, mean reward: 31.357 [13.669, 50.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.750], loss: 4.126584, mae: 1.416683, mean_q: 7.880968
 20389/100000: episode: 504, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 55.194, mean reward: 9.199 [5.480, 18.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.340, 10.724], loss: 7.373800, mae: 1.729552, mean_q: 8.040783
 20394/100000: episode: 505, duration: 0.039s, episode steps: 5, steps per second: 127, episode reward: 1042.105, mean reward: 208.421 [7.647, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.396], loss: 4.883800, mae: 1.480388, mean_q: 8.257240
 20396/100000: episode: 506, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 13.770, mean reward: 6.885 [6.238, 7.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.637], loss: 7597.206543, mae: 16.856596, mean_q: 7.007885
[Info] FALSIFICATION!
 20402/100000: episode: 507, duration: 0.324s, episode steps: 6, steps per second: 19, episode reward: 1066.410, mean reward: 177.735 [6.780, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.015, 10.756], loss: 57.701843, mae: 3.659958, mean_q: 10.227246
 20407/100000: episode: 508, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 26.849, mean reward: 5.370 [3.965, 7.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.569], loss: 3038.604248, mae: 10.873493, mean_q: 11.557265
[Info] Complete ISplit Iteration
[Info] Levels: [4.6090775, 6.4820724, 8.618775, 13.944094, 19.545715]
[Info] Cond. Prob: [0.1, 0.11, 0.1, 0.1, 1.0]
[Info] Error Prob: 0.00011000000000000002

 20409/100000: episode: 509, duration: 4.774s, episode steps: 2, steps per second: 0, episode reward: 8.949, mean reward: 4.475 [4.138, 4.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.035, 10.483], loss: 12.369791, mae: 4.028592, mean_q: 11.888397
 20509/100000: episode: 510, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.236, mean reward: 1.902 [1.470, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.777, 10.098], loss: 325.985321, mae: 2.996036, mean_q: 8.938026
 20609/100000: episode: 511, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.507, mean reward: 1.835 [1.456, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.118, 10.114], loss: 389.946594, mae: 3.468978, mean_q: 9.105518
 20709/100000: episode: 512, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 185.891, mean reward: 1.859 [1.432, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.925, 10.098], loss: 1227.908081, mae: 6.388879, mean_q: 10.714114
 20809/100000: episode: 513, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 199.522, mean reward: 1.995 [1.457, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.093, 10.185], loss: 809.597412, mae: 5.175253, mean_q: 9.407708
 20909/100000: episode: 514, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 191.719, mean reward: 1.917 [1.465, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.529, 10.211], loss: 359.308868, mae: 3.610578, mean_q: 9.494883
 21009/100000: episode: 515, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 201.104, mean reward: 2.011 [1.449, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.371, 10.098], loss: 642.347290, mae: 4.008693, mean_q: 9.666778
 21109/100000: episode: 516, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.341, mean reward: 1.813 [1.435, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.765, 10.213], loss: 781.615845, mae: 4.321000, mean_q: 9.509129
 21209/100000: episode: 517, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 235.898, mean reward: 2.359 [1.523, 7.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.034, 10.497], loss: 451.870239, mae: 3.862135, mean_q: 9.663377
 21309/100000: episode: 518, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.170, mean reward: 1.832 [1.440, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.080, 10.127], loss: 911.450562, mae: 4.774694, mean_q: 10.066739
 21409/100000: episode: 519, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 198.302, mean reward: 1.983 [1.493, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.062, 10.098], loss: 1383.898315, mae: 6.260991, mean_q: 10.833678
 21509/100000: episode: 520, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.189, mean reward: 1.892 [1.451, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.098], loss: 528.852661, mae: 4.397633, mean_q: 10.062711
 21609/100000: episode: 521, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.599, mean reward: 1.836 [1.437, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.098], loss: 782.127747, mae: 4.105830, mean_q: 9.595761
 21709/100000: episode: 522, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 269.091, mean reward: 2.691 [1.444, 8.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.392, 10.098], loss: 469.487701, mae: 3.537527, mean_q: 9.406334
 21809/100000: episode: 523, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 192.447, mean reward: 1.924 [1.450, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.653, 10.098], loss: 353.166107, mae: 2.905803, mean_q: 8.760815
 21909/100000: episode: 524, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 195.023, mean reward: 1.950 [1.503, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.513, 10.244], loss: 796.827209, mae: 4.559149, mean_q: 9.779545
 22009/100000: episode: 525, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.603, mean reward: 1.806 [1.439, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.247], loss: 636.817200, mae: 4.259530, mean_q: 9.871600
 22109/100000: episode: 526, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.040, mean reward: 1.890 [1.475, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.538, 10.108], loss: 652.473145, mae: 4.342274, mean_q: 9.935612
 22209/100000: episode: 527, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 220.796, mean reward: 2.208 [1.535, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.184, 10.134], loss: 615.142639, mae: 3.620310, mean_q: 9.355715
 22309/100000: episode: 528, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 194.618, mean reward: 1.946 [1.457, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.123, 10.231], loss: 997.636414, mae: 5.040294, mean_q: 9.964819
 22409/100000: episode: 529, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 187.323, mean reward: 1.873 [1.444, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.438, 10.111], loss: 1233.668701, mae: 6.379841, mean_q: 11.417572
 22509/100000: episode: 530, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 198.977, mean reward: 1.990 [1.444, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.503, 10.258], loss: 1065.436035, mae: 6.216723, mean_q: 11.203180
 22609/100000: episode: 531, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 206.689, mean reward: 2.067 [1.454, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.739, 10.098], loss: 573.752197, mae: 4.234094, mean_q: 9.846376
 22709/100000: episode: 532, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 176.387, mean reward: 1.764 [1.464, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.181], loss: 937.137634, mae: 5.146305, mean_q: 10.664273
 22809/100000: episode: 533, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 200.413, mean reward: 2.004 [1.467, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.888, 10.098], loss: 663.260559, mae: 4.360577, mean_q: 9.599132
 22909/100000: episode: 534, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 195.217, mean reward: 1.952 [1.497, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.147, 10.098], loss: 797.378540, mae: 4.593916, mean_q: 9.686642
 23009/100000: episode: 535, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.517, mean reward: 1.915 [1.452, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.389, 10.277], loss: 1191.629272, mae: 5.616159, mean_q: 10.410757
 23109/100000: episode: 536, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 213.753, mean reward: 2.138 [1.489, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.926, 10.098], loss: 523.250488, mae: 3.754967, mean_q: 9.031180
 23209/100000: episode: 537, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 200.680, mean reward: 2.007 [1.468, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.950, 10.191], loss: 457.544220, mae: 2.774980, mean_q: 8.174714
 23309/100000: episode: 538, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.188, mean reward: 1.872 [1.494, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.185], loss: 498.528717, mae: 3.385623, mean_q: 8.791867
 23409/100000: episode: 539, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: 198.023, mean reward: 1.980 [1.482, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.159, 10.407], loss: 486.565460, mae: 3.327758, mean_q: 8.851160
 23509/100000: episode: 540, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.363, mean reward: 1.934 [1.470, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.219, 10.314], loss: 608.385437, mae: 3.279862, mean_q: 8.238973
 23609/100000: episode: 541, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 201.586, mean reward: 2.016 [1.460, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.814, 10.098], loss: 796.218018, mae: 4.586973, mean_q: 9.080163
 23709/100000: episode: 542, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 185.689, mean reward: 1.857 [1.448, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.859, 10.098], loss: 783.187012, mae: 4.607523, mean_q: 9.709060
 23809/100000: episode: 543, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.090, mean reward: 1.811 [1.485, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.378, 10.157], loss: 663.782593, mae: 3.561466, mean_q: 8.771441
 23909/100000: episode: 544, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 221.949, mean reward: 2.219 [1.494, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.474], loss: 794.353577, mae: 4.106318, mean_q: 8.654330
 24009/100000: episode: 545, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 185.614, mean reward: 1.856 [1.448, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.732, 10.136], loss: 1407.170288, mae: 6.564629, mean_q: 10.621784
 24109/100000: episode: 546, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.522, mean reward: 1.895 [1.445, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.532, 10.159], loss: 806.882874, mae: 4.704328, mean_q: 9.895817
 24209/100000: episode: 547, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 176.577, mean reward: 1.766 [1.438, 2.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.730, 10.149], loss: 752.515259, mae: 4.018852, mean_q: 8.969961
 24309/100000: episode: 548, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 187.062, mean reward: 1.871 [1.445, 5.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.586, 10.207], loss: 503.608307, mae: 3.330827, mean_q: 8.137114
 24409/100000: episode: 549, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 197.163, mean reward: 1.972 [1.468, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.812, 10.390], loss: 206.240524, mae: 2.282269, mean_q: 7.569201
 24509/100000: episode: 550, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 190.255, mean reward: 1.903 [1.447, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.228, 10.098], loss: 316.865356, mae: 2.410192, mean_q: 7.199485
 24609/100000: episode: 551, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 180.470, mean reward: 1.805 [1.454, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.173, 10.132], loss: 460.951416, mae: 2.705386, mean_q: 6.972151
 24709/100000: episode: 552, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 201.058, mean reward: 2.011 [1.439, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.999, 10.098], loss: 485.947113, mae: 2.978702, mean_q: 7.355974
 24809/100000: episode: 553, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 204.773, mean reward: 2.048 [1.434, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.648, 10.340], loss: 1186.860107, mae: 4.859814, mean_q: 7.842797
 24909/100000: episode: 554, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 185.517, mean reward: 1.855 [1.448, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.043, 10.159], loss: 1595.845947, mae: 6.595397, mean_q: 9.150422
 25009/100000: episode: 555, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 191.926, mean reward: 1.919 [1.482, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.878, 10.098], loss: 1146.141602, mae: 5.610070, mean_q: 8.676111
 25109/100000: episode: 556, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 188.713, mean reward: 1.887 [1.443, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.948, 10.098], loss: 606.646118, mae: 3.152089, mean_q: 6.805717
 25209/100000: episode: 557, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 192.367, mean reward: 1.924 [1.456, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.293, 10.291], loss: 153.389404, mae: 1.283549, mean_q: 5.319861
 25309/100000: episode: 558, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 193.007, mean reward: 1.930 [1.472, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.286], loss: 177.916061, mae: 1.137210, mean_q: 4.763052
 25409/100000: episode: 559, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 211.126, mean reward: 2.111 [1.479, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.358, 10.139], loss: 139.018600, mae: 0.853366, mean_q: 4.245486
 25509/100000: episode: 560, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.806, mean reward: 1.848 [1.451, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.016, 10.098], loss: 0.492435, mae: 0.390855, mean_q: 4.065391
 25609/100000: episode: 561, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.881, mean reward: 1.929 [1.440, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.126, 10.099], loss: 0.198936, mae: 0.335566, mean_q: 3.966635
 25709/100000: episode: 562, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 210.643, mean reward: 2.106 [1.444, 6.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.947, 10.098], loss: 0.153021, mae: 0.315978, mean_q: 3.945442
 25809/100000: episode: 563, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 188.359, mean reward: 1.884 [1.450, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.741, 10.098], loss: 0.178485, mae: 0.327186, mean_q: 3.921518
 25909/100000: episode: 564, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.376, mean reward: 1.914 [1.442, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.687, 10.098], loss: 0.144274, mae: 0.321327, mean_q: 3.913910
 26009/100000: episode: 565, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 210.004, mean reward: 2.100 [1.459, 4.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.026, 10.098], loss: 0.117903, mae: 0.303522, mean_q: 3.885799
 26109/100000: episode: 566, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.584, mean reward: 1.816 [1.449, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.331, 10.256], loss: 0.110813, mae: 0.308791, mean_q: 3.906009
 26209/100000: episode: 567, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 176.999, mean reward: 1.770 [1.435, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.992, 10.162], loss: 0.124027, mae: 0.305701, mean_q: 3.894944
 26309/100000: episode: 568, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.987, mean reward: 1.880 [1.456, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.749, 10.098], loss: 0.138932, mae: 0.328233, mean_q: 3.899070
 26409/100000: episode: 569, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 206.782, mean reward: 2.068 [1.475, 4.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.098], loss: 0.096341, mae: 0.297122, mean_q: 3.870877
 26509/100000: episode: 570, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 221.524, mean reward: 2.215 [1.521, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.375, 10.098], loss: 0.106901, mae: 0.300406, mean_q: 3.887070
 26609/100000: episode: 571, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.809, mean reward: 1.848 [1.452, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.282, 10.107], loss: 0.122268, mae: 0.319622, mean_q: 3.899606
 26709/100000: episode: 572, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 183.279, mean reward: 1.833 [1.461, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.178, 10.144], loss: 0.094218, mae: 0.292133, mean_q: 3.867181
 26809/100000: episode: 573, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.775, mean reward: 1.988 [1.472, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.820, 10.476], loss: 0.087020, mae: 0.286593, mean_q: 3.853438
 26909/100000: episode: 574, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 188.063, mean reward: 1.881 [1.465, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.098], loss: 0.124393, mae: 0.295060, mean_q: 3.860040
 27009/100000: episode: 575, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 182.168, mean reward: 1.822 [1.441, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 0.098591, mae: 0.295236, mean_q: 3.883731
 27109/100000: episode: 576, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 191.003, mean reward: 1.910 [1.457, 4.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.170], loss: 0.097190, mae: 0.281136, mean_q: 3.832332
 27209/100000: episode: 577, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: 189.829, mean reward: 1.898 [1.454, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.451, 10.098], loss: 0.093831, mae: 0.291557, mean_q: 3.848413
 27309/100000: episode: 578, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: 196.495, mean reward: 1.965 [1.455, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.990, 10.159], loss: 0.111534, mae: 0.300981, mean_q: 3.859334
 27409/100000: episode: 579, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 194.511, mean reward: 1.945 [1.444, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.396, 10.098], loss: 0.113918, mae: 0.306895, mean_q: 3.848752
 27509/100000: episode: 580, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 182.557, mean reward: 1.826 [1.442, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.110161, mae: 0.305448, mean_q: 3.836805
 27609/100000: episode: 581, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 194.371, mean reward: 1.944 [1.508, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.325, 10.272], loss: 0.099764, mae: 0.291745, mean_q: 3.829261
 27709/100000: episode: 582, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 180.475, mean reward: 1.805 [1.437, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.124], loss: 0.089776, mae: 0.283303, mean_q: 3.825082
 27809/100000: episode: 583, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 293.365, mean reward: 2.934 [1.540, 10.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.721, 10.347], loss: 0.100930, mae: 0.297679, mean_q: 3.831897
 27909/100000: episode: 584, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.783, mean reward: 1.938 [1.454, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.903, 10.216], loss: 0.111756, mae: 0.307177, mean_q: 3.861846
 28009/100000: episode: 585, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 189.118, mean reward: 1.891 [1.456, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.303, 10.357], loss: 0.107838, mae: 0.301013, mean_q: 3.869535
 28109/100000: episode: 586, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.334, mean reward: 1.913 [1.468, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.376, 10.249], loss: 0.090460, mae: 0.288251, mean_q: 3.850457
 28209/100000: episode: 587, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 235.508, mean reward: 2.355 [1.442, 6.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.976, 10.098], loss: 0.108631, mae: 0.302454, mean_q: 3.864742
 28309/100000: episode: 588, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 179.108, mean reward: 1.791 [1.474, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.187], loss: 0.110269, mae: 0.301724, mean_q: 3.862125
 28409/100000: episode: 589, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.494, mean reward: 1.915 [1.451, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.623, 10.098], loss: 0.143903, mae: 0.324356, mean_q: 3.885620
 28509/100000: episode: 590, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 186.521, mean reward: 1.865 [1.454, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.486, 10.171], loss: 0.110719, mae: 0.303449, mean_q: 3.855234
 28609/100000: episode: 591, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 209.612, mean reward: 2.096 [1.522, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.682, 10.098], loss: 0.109341, mae: 0.302773, mean_q: 3.855602
 28709/100000: episode: 592, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.299, mean reward: 1.863 [1.454, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.645, 10.098], loss: 0.107842, mae: 0.293310, mean_q: 3.864417
 28809/100000: episode: 593, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 182.306, mean reward: 1.823 [1.445, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.529, 10.098], loss: 0.105979, mae: 0.301321, mean_q: 3.869115
 28909/100000: episode: 594, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 202.689, mean reward: 2.027 [1.474, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.907, 10.098], loss: 0.107948, mae: 0.308885, mean_q: 3.864976
 29009/100000: episode: 595, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 181.625, mean reward: 1.816 [1.453, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.559, 10.110], loss: 0.101741, mae: 0.291443, mean_q: 3.863445
 29109/100000: episode: 596, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.303, mean reward: 1.913 [1.452, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.862, 10.098], loss: 0.106750, mae: 0.307166, mean_q: 3.878196
 29209/100000: episode: 597, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.637, mean reward: 1.976 [1.467, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.417, 10.393], loss: 0.098852, mae: 0.293430, mean_q: 3.852304
 29309/100000: episode: 598, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 200.234, mean reward: 2.002 [1.478, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.290, 10.148], loss: 0.095563, mae: 0.290593, mean_q: 3.854741
 29409/100000: episode: 599, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 184.492, mean reward: 1.845 [1.439, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.098], loss: 0.115881, mae: 0.301276, mean_q: 3.852430
 29509/100000: episode: 600, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 198.081, mean reward: 1.981 [1.468, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.560, 10.375], loss: 0.090733, mae: 0.288749, mean_q: 3.860095
 29609/100000: episode: 601, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 193.169, mean reward: 1.932 [1.448, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.859, 10.305], loss: 0.084323, mae: 0.281712, mean_q: 3.856478
 29709/100000: episode: 602, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 193.615, mean reward: 1.936 [1.485, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.364, 10.168], loss: 0.099868, mae: 0.291147, mean_q: 3.837196
 29809/100000: episode: 603, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 195.427, mean reward: 1.954 [1.448, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.458, 10.098], loss: 0.100081, mae: 0.298905, mean_q: 3.851234
 29909/100000: episode: 604, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 213.736, mean reward: 2.137 [1.492, 5.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.513, 10.324], loss: 0.096889, mae: 0.281032, mean_q: 3.841899
 30009/100000: episode: 605, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.297, mean reward: 1.983 [1.462, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.816, 10.182], loss: 0.105874, mae: 0.295253, mean_q: 3.861196
 30109/100000: episode: 606, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 194.036, mean reward: 1.940 [1.473, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.066, 10.098], loss: 0.093632, mae: 0.288664, mean_q: 3.854310
 30209/100000: episode: 607, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 189.085, mean reward: 1.891 [1.433, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.332, 10.146], loss: 0.096909, mae: 0.297481, mean_q: 3.876635
 30309/100000: episode: 608, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 196.377, mean reward: 1.964 [1.472, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.897, 10.100], loss: 0.099387, mae: 0.298765, mean_q: 3.848533
[Info] 1-TH LEVEL FOUND: 5.623072147369385, Considering 10/90 traces
 30409/100000: episode: 609, duration: 5.494s, episode steps: 100, steps per second: 18, episode reward: 192.208, mean reward: 1.922 [1.439, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.185], loss: 0.097856, mae: 0.293256, mean_q: 3.868134
 30423/100000: episode: 610, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 45.028, mean reward: 3.216 [2.623, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.203, 10.455], loss: 0.084654, mae: 0.288612, mean_q: 3.845388
 30493/100000: episode: 611, duration: 0.345s, episode steps: 70, steps per second: 203, episode reward: 175.527, mean reward: 2.508 [1.496, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-0.512, 10.435], loss: 0.103567, mae: 0.300010, mean_q: 3.875666
 30512/100000: episode: 612, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 38.103, mean reward: 2.005 [1.538, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.184], loss: 0.130366, mae: 0.322509, mean_q: 3.921976
 30531/100000: episode: 613, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 70.188, mean reward: 3.694 [2.147, 15.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.264, 10.356], loss: 0.095598, mae: 0.303886, mean_q: 3.915841
 30601/100000: episode: 614, duration: 0.400s, episode steps: 70, steps per second: 175, episode reward: 146.534, mean reward: 2.093 [1.477, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.720, 10.297], loss: 0.104156, mae: 0.299450, mean_q: 3.902601
 30671/100000: episode: 615, duration: 0.387s, episode steps: 70, steps per second: 181, episode reward: 173.905, mean reward: 2.484 [1.862, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.671, 10.385], loss: 0.142627, mae: 0.306212, mean_q: 3.920010
 30696/100000: episode: 616, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 53.339, mean reward: 2.134 [1.789, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.392, 10.296], loss: 0.082851, mae: 0.283569, mean_q: 3.889402
 30720/100000: episode: 617, duration: 0.132s, episode steps: 24, steps per second: 183, episode reward: 89.644, mean reward: 3.735 [2.345, 6.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.136, 10.483], loss: 0.149408, mae: 0.356124, mean_q: 3.958486
 30744/100000: episode: 618, duration: 0.116s, episode steps: 24, steps per second: 206, episode reward: 64.706, mean reward: 2.696 [1.972, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.414], loss: 0.094642, mae: 0.299982, mean_q: 3.929033
 30763/100000: episode: 619, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 41.370, mean reward: 2.177 [1.886, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.312, 10.275], loss: 0.114285, mae: 0.312325, mean_q: 3.951889
 30782/100000: episode: 620, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 43.894, mean reward: 2.310 [1.808, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.250], loss: 0.093306, mae: 0.293918, mean_q: 3.922642
 30793/100000: episode: 621, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 30.222, mean reward: 2.747 [2.091, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.343, 10.100], loss: 0.103362, mae: 0.313474, mean_q: 3.941394
 30831/100000: episode: 622, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 73.328, mean reward: 1.930 [1.501, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.209, 10.167], loss: 0.102956, mae: 0.304024, mean_q: 3.937924
 30845/100000: episode: 623, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 39.886, mean reward: 2.849 [2.259, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.523], loss: 0.120088, mae: 0.308278, mean_q: 3.943889
 30870/100000: episode: 624, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 77.609, mean reward: 3.104 [2.608, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.214, 10.504], loss: 0.124651, mae: 0.320757, mean_q: 3.920043
 30889/100000: episode: 625, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 40.832, mean reward: 2.149 [1.827, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.172, 10.236], loss: 0.100102, mae: 0.292286, mean_q: 3.935127
 30900/100000: episode: 626, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 39.988, mean reward: 3.635 [2.465, 10.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.294, 10.100], loss: 0.119769, mae: 0.314083, mean_q: 3.990213
 30914/100000: episode: 627, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 37.885, mean reward: 2.706 [2.085, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.580, 10.472], loss: 0.326386, mae: 0.344818, mean_q: 3.951915
 30938/100000: episode: 628, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 55.501, mean reward: 2.313 [1.743, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.111, 10.295], loss: 0.118354, mae: 0.323323, mean_q: 3.951332
 30949/100000: episode: 629, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 39.882, mean reward: 3.626 [2.103, 7.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.601, 10.100], loss: 0.125878, mae: 0.336368, mean_q: 4.005429
 30987/100000: episode: 630, duration: 0.186s, episode steps: 38, steps per second: 205, episode reward: 87.608, mean reward: 2.305 [1.490, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.053, 10.171], loss: 0.163723, mae: 0.340881, mean_q: 3.988583
 31006/100000: episode: 631, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 43.078, mean reward: 2.267 [1.860, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.618, 10.333], loss: 0.301746, mae: 0.368695, mean_q: 4.056704
 31050/100000: episode: 632, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 143.693, mean reward: 3.266 [2.075, 5.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.688, 10.383], loss: 0.121866, mae: 0.331903, mean_q: 3.969919
 31075/100000: episode: 633, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 61.676, mean reward: 2.467 [1.777, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.029, 10.278], loss: 0.189377, mae: 0.340676, mean_q: 3.973898
 31145/100000: episode: 634, duration: 0.348s, episode steps: 70, steps per second: 201, episode reward: 193.926, mean reward: 2.770 [1.697, 10.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.871, 10.503], loss: 0.161197, mae: 0.354018, mean_q: 4.022533
 31183/100000: episode: 635, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 83.268, mean reward: 2.191 [1.522, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.300, 10.100], loss: 0.177406, mae: 0.352632, mean_q: 4.061649
 31221/100000: episode: 636, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 76.393, mean reward: 2.010 [1.559, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.929, 10.100], loss: 0.184107, mae: 0.360356, mean_q: 4.133991
 31265/100000: episode: 637, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 96.608, mean reward: 2.196 [1.639, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.422, 10.227], loss: 0.124529, mae: 0.337009, mean_q: 4.046449
 31279/100000: episode: 638, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 38.115, mean reward: 2.723 [2.150, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.368], loss: 0.191881, mae: 0.364795, mean_q: 4.098279
 31349/100000: episode: 639, duration: 0.358s, episode steps: 70, steps per second: 195, episode reward: 144.867, mean reward: 2.070 [1.482, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.922, 10.136], loss: 0.143209, mae: 0.343528, mean_q: 4.083207
 31360/100000: episode: 640, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 35.378, mean reward: 3.216 [2.474, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.334, 10.100], loss: 0.108163, mae: 0.340079, mean_q: 4.055338
 31384/100000: episode: 641, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 76.310, mean reward: 3.180 [2.502, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.344, 10.100], loss: 0.113642, mae: 0.328115, mean_q: 4.023643
 31422/100000: episode: 642, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 118.048, mean reward: 3.107 [2.236, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.546, 10.361], loss: 0.173421, mae: 0.346459, mean_q: 4.054877
 31466/100000: episode: 643, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 118.196, mean reward: 2.686 [1.557, 4.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.189, 10.235], loss: 0.101142, mae: 0.315469, mean_q: 4.030493
 31510/100000: episode: 644, duration: 0.215s, episode steps: 44, steps per second: 205, episode reward: 111.413, mean reward: 2.532 [1.911, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.423, 10.420], loss: 0.198695, mae: 0.352008, mean_q: 4.064159
 31529/100000: episode: 645, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 47.182, mean reward: 2.483 [1.975, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.756, 10.351], loss: 0.390486, mae: 0.415965, mean_q: 4.208209
 31548/100000: episode: 646, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 56.906, mean reward: 2.995 [1.832, 5.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.819, 10.286], loss: 0.272861, mae: 0.368160, mean_q: 4.134430
 31559/100000: episode: 647, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 28.013, mean reward: 2.547 [2.186, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.693, 10.100], loss: 0.420427, mae: 0.408325, mean_q: 4.171334
 31583/100000: episode: 648, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 58.104, mean reward: 2.421 [2.037, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.720, 10.349], loss: 0.124492, mae: 0.342861, mean_q: 4.103257
 31627/100000: episode: 649, duration: 0.223s, episode steps: 44, steps per second: 198, episode reward: 84.617, mean reward: 1.923 [1.533, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.484, 10.196], loss: 0.165876, mae: 0.337258, mean_q: 4.156466
 31646/100000: episode: 650, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 42.159, mean reward: 2.219 [1.876, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.341], loss: 0.250221, mae: 0.351341, mean_q: 4.138515
 31657/100000: episode: 651, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 30.938, mean reward: 2.813 [2.213, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.453, 10.100], loss: 0.373575, mae: 0.404564, mean_q: 4.187007
 31676/100000: episode: 652, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 40.796, mean reward: 2.147 [1.832, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.514, 10.232], loss: 0.145278, mae: 0.362780, mean_q: 4.139408
 31700/100000: episode: 653, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 84.577, mean reward: 3.524 [2.312, 6.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.240, 10.100], loss: 0.117194, mae: 0.340064, mean_q: 4.098338
 31719/100000: episode: 654, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 51.684, mean reward: 2.720 [2.130, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.270, 10.510], loss: 0.148108, mae: 0.338691, mean_q: 4.090225
 31727/100000: episode: 655, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 20.253, mean reward: 2.532 [2.195, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.430], loss: 0.083127, mae: 0.303166, mean_q: 4.099883
 31751/100000: episode: 656, duration: 0.123s, episode steps: 24, steps per second: 194, episode reward: 66.112, mean reward: 2.755 [2.296, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.455], loss: 0.199979, mae: 0.364548, mean_q: 4.121340
 31821/100000: episode: 657, duration: 0.345s, episode steps: 70, steps per second: 203, episode reward: 135.321, mean reward: 1.933 [1.447, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.384, 10.100], loss: 0.191809, mae: 0.370736, mean_q: 4.155313
 31840/100000: episode: 658, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 39.934, mean reward: 2.102 [1.795, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-1.600, 10.313], loss: 0.298097, mae: 0.382788, mean_q: 4.187390
 31864/100000: episode: 659, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 115.920, mean reward: 4.830 [3.136, 9.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.207, 10.100], loss: 0.158378, mae: 0.359755, mean_q: 4.179032
 31902/100000: episode: 660, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 86.284, mean reward: 2.271 [1.776, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.303, 10.265], loss: 0.153341, mae: 0.374949, mean_q: 4.206423
 31916/100000: episode: 661, duration: 0.101s, episode steps: 14, steps per second: 139, episode reward: 45.658, mean reward: 3.261 [2.540, 4.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.772, 10.417], loss: 0.123445, mae: 0.337685, mean_q: 4.174949
 31935/100000: episode: 662, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 37.549, mean reward: 1.976 [1.638, 2.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.286, 10.291], loss: 0.145551, mae: 0.365951, mean_q: 4.249310
 31946/100000: episode: 663, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 24.739, mean reward: 2.249 [2.010, 2.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.248, 10.100], loss: 0.171355, mae: 0.402727, mean_q: 4.187358
 32016/100000: episode: 664, duration: 0.373s, episode steps: 70, steps per second: 188, episode reward: 136.709, mean reward: 1.953 [1.468, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-1.212, 10.100], loss: 0.175167, mae: 0.379315, mean_q: 4.224973
 32027/100000: episode: 665, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 36.107, mean reward: 3.282 [2.824, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.666, 10.100], loss: 0.128726, mae: 0.358334, mean_q: 4.285204
 32046/100000: episode: 666, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 45.810, mean reward: 2.411 [2.008, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.703, 10.442], loss: 0.173083, mae: 0.375262, mean_q: 4.220650
 32065/100000: episode: 667, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 50.531, mean reward: 2.660 [1.963, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.205, 10.403], loss: 0.197543, mae: 0.397216, mean_q: 4.218815
 32103/100000: episode: 668, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 87.159, mean reward: 2.294 [1.635, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.904, 10.229], loss: 0.151810, mae: 0.354208, mean_q: 4.205399
 32114/100000: episode: 669, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 42.186, mean reward: 3.835 [2.395, 6.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.489, 10.100], loss: 0.116832, mae: 0.337159, mean_q: 4.233634
 32125/100000: episode: 670, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 45.011, mean reward: 4.092 [2.948, 6.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.538, 10.100], loss: 0.170509, mae: 0.385449, mean_q: 4.305489
 32139/100000: episode: 671, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 30.788, mean reward: 2.199 [1.593, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.339, 10.200], loss: 0.133354, mae: 0.354864, mean_q: 4.294953
 32183/100000: episode: 672, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 127.039, mean reward: 2.887 [2.204, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.388, 10.380], loss: 0.210413, mae: 0.384574, mean_q: 4.240033
 32197/100000: episode: 673, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 40.444, mean reward: 2.889 [2.074, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.226, 10.435], loss: 0.176747, mae: 0.375772, mean_q: 4.244910
 32222/100000: episode: 674, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: 98.075, mean reward: 3.923 [2.019, 9.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.330], loss: 0.208342, mae: 0.400664, mean_q: 4.308491
 32236/100000: episode: 675, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 38.565, mean reward: 2.755 [2.273, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.399], loss: 0.151984, mae: 0.368047, mean_q: 4.204025
 32244/100000: episode: 676, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 20.507, mean reward: 2.563 [2.245, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.426], loss: 0.230746, mae: 0.422695, mean_q: 4.250021
 32258/100000: episode: 677, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 37.070, mean reward: 2.648 [2.192, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.380, 10.322], loss: 0.233399, mae: 0.417798, mean_q: 4.291699
 32266/100000: episode: 678, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 20.179, mean reward: 2.522 [2.141, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.412], loss: 0.120579, mae: 0.347529, mean_q: 4.234497
 32277/100000: episode: 679, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 31.162, mean reward: 2.833 [1.914, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.367, 10.100], loss: 0.167604, mae: 0.367669, mean_q: 4.267274
 32347/100000: episode: 680, duration: 0.390s, episode steps: 70, steps per second: 179, episode reward: 143.519, mean reward: 2.050 [1.447, 4.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.573, 10.223], loss: 0.236663, mae: 0.399572, mean_q: 4.297717
 32355/100000: episode: 681, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 21.632, mean reward: 2.704 [2.077, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.454], loss: 0.200515, mae: 0.453465, mean_q: 4.405312
 32363/100000: episode: 682, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 29.272, mean reward: 3.659 [2.530, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.568, 10.611], loss: 0.106859, mae: 0.331538, mean_q: 4.242288
 32374/100000: episode: 683, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 27.999, mean reward: 2.545 [2.173, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.508, 10.100], loss: 0.144358, mae: 0.364138, mean_q: 4.267128
 32398/100000: episode: 684, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 57.278, mean reward: 2.387 [1.921, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.547, 10.397], loss: 0.191565, mae: 0.399172, mean_q: 4.271682
 32422/100000: episode: 685, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 50.439, mean reward: 2.102 [1.833, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.342], loss: 0.173295, mae: 0.391600, mean_q: 4.305393
 32446/100000: episode: 686, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 81.617, mean reward: 3.401 [2.065, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.324, 10.100], loss: 0.206082, mae: 0.409059, mean_q: 4.373669
 32471/100000: episode: 687, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 71.638, mean reward: 2.866 [1.864, 4.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.326, 10.393], loss: 0.221787, mae: 0.402208, mean_q: 4.332617
 32482/100000: episode: 688, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 53.205, mean reward: 4.837 [2.486, 13.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.643, 10.100], loss: 0.142634, mae: 0.368778, mean_q: 4.152605
 32520/100000: episode: 689, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 86.742, mean reward: 2.283 [1.795, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.174, 10.256], loss: 0.215069, mae: 0.411628, mean_q: 4.393202
 32544/100000: episode: 690, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 56.739, mean reward: 2.364 [1.922, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.969, 10.436], loss: 0.174710, mae: 0.395617, mean_q: 4.340097
 32582/100000: episode: 691, duration: 0.183s, episode steps: 38, steps per second: 207, episode reward: 82.379, mean reward: 2.168 [1.474, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.645, 10.210], loss: 0.214692, mae: 0.406239, mean_q: 4.353277
 32607/100000: episode: 692, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 72.616, mean reward: 2.905 [2.198, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.269, 10.366], loss: 0.312865, mae: 0.472904, mean_q: 4.413798
 32631/100000: episode: 693, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 72.477, mean reward: 3.020 [2.238, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.320, 10.100], loss: 0.241748, mae: 0.432634, mean_q: 4.379436
 32645/100000: episode: 694, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 39.666, mean reward: 2.833 [2.143, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.041, 10.436], loss: 0.210497, mae: 0.396157, mean_q: 4.402540
 32689/100000: episode: 695, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 102.967, mean reward: 2.340 [1.822, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.361, 10.307], loss: 0.177880, mae: 0.376969, mean_q: 4.277837
 32727/100000: episode: 696, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 82.429, mean reward: 2.169 [1.546, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.026, 10.245], loss: 0.299074, mae: 0.398432, mean_q: 4.364527
 32752/100000: episode: 697, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 62.746, mean reward: 2.510 [1.849, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.427], loss: 0.230376, mae: 0.413381, mean_q: 4.371969
 32776/100000: episode: 698, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 54.641, mean reward: 2.277 [1.582, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.200, 10.205], loss: 0.319730, mae: 0.431643, mean_q: 4.399585
[Info] 2-TH LEVEL FOUND: 7.552290916442871, Considering 10/90 traces
 32846/100000: episode: 699, duration: 4.377s, episode steps: 70, steps per second: 16, episode reward: 148.415, mean reward: 2.120 [1.461, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.839, 10.317], loss: 0.187060, mae: 0.397714, mean_q: 4.388431
 32855/100000: episode: 700, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 33.112, mean reward: 3.679 [3.044, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.557], loss: 0.155312, mae: 0.400058, mean_q: 4.401516
 32866/100000: episode: 701, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 63.034, mean reward: 5.730 [4.756, 7.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.549, 10.100], loss: 0.108068, mae: 0.341654, mean_q: 4.245078
 32895/100000: episode: 702, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 118.836, mean reward: 4.098 [1.945, 11.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.735, 10.331], loss: 0.169714, mae: 0.375888, mean_q: 4.378573
 32917/100000: episode: 703, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 80.558, mean reward: 3.662 [3.017, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.581, 10.404], loss: 0.230484, mae: 0.423626, mean_q: 4.437700
 32924/100000: episode: 704, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 21.469, mean reward: 3.067 [2.421, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.386], loss: 0.643086, mae: 0.513925, mean_q: 4.351427
 32953/100000: episode: 705, duration: 0.134s, episode steps: 29, steps per second: 216, episode reward: 69.956, mean reward: 2.412 [1.441, 4.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.789, 10.100], loss: 0.228223, mae: 0.447146, mean_q: 4.417959
 32962/100000: episode: 706, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 22.286, mean reward: 2.476 [2.104, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.305, 10.359], loss: 0.290548, mae: 0.428703, mean_q: 4.386888
 32992/100000: episode: 707, duration: 0.148s, episode steps: 30, steps per second: 202, episode reward: 132.949, mean reward: 4.432 [2.754, 7.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.334, 10.533], loss: 0.287230, mae: 0.419381, mean_q: 4.488859
 32999/100000: episode: 708, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 17.699, mean reward: 2.528 [2.421, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.375], loss: 0.174191, mae: 0.407438, mean_q: 4.441147
 33006/100000: episode: 709, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 22.694, mean reward: 3.242 [2.273, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.529], loss: 0.293618, mae: 0.483909, mean_q: 4.459412
 33017/100000: episode: 710, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 48.429, mean reward: 4.403 [3.758, 6.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.457, 10.100], loss: 0.295422, mae: 0.439078, mean_q: 4.535835
 33028/100000: episode: 711, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 50.369, mean reward: 4.579 [2.676, 8.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.105, 10.100], loss: 0.344702, mae: 0.490655, mean_q: 4.625640
 33035/100000: episode: 712, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 24.275, mean reward: 3.468 [2.420, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.979, 10.448], loss: 0.170712, mae: 0.390886, mean_q: 4.496247
 33044/100000: episode: 713, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 27.472, mean reward: 3.052 [2.597, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.555, 10.477], loss: 0.220055, mae: 0.414026, mean_q: 4.506618
 33066/100000: episode: 714, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 99.097, mean reward: 4.504 [2.856, 9.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.660, 10.429], loss: 0.293595, mae: 0.460143, mean_q: 4.485394
 33096/100000: episode: 715, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 86.346, mean reward: 2.878 [1.782, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.443, 10.324], loss: 0.191959, mae: 0.417230, mean_q: 4.507065
 33105/100000: episode: 716, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 24.347, mean reward: 2.705 [2.005, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.422], loss: 0.161264, mae: 0.408499, mean_q: 4.531163
 33112/100000: episode: 717, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 22.104, mean reward: 3.158 [2.271, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.296, 10.475], loss: 0.286465, mae: 0.428434, mean_q: 4.351847
 33118/100000: episode: 718, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 30.518, mean reward: 5.086 [4.548, 5.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.454], loss: 0.616116, mae: 0.540837, mean_q: 4.673111
 33145/100000: episode: 719, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 72.583, mean reward: 2.688 [2.068, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.047, 10.298], loss: 0.242979, mae: 0.428619, mean_q: 4.473459
 33167/100000: episode: 720, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 75.117, mean reward: 3.414 [2.269, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.613, 10.372], loss: 0.247085, mae: 0.428584, mean_q: 4.480199
 33194/100000: episode: 721, duration: 0.128s, episode steps: 27, steps per second: 210, episode reward: 110.053, mean reward: 4.076 [2.641, 6.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.484, 10.493], loss: 0.248913, mae: 0.454568, mean_q: 4.568830
 33205/100000: episode: 722, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 57.199, mean reward: 5.200 [3.828, 10.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.345, 10.100], loss: 0.215485, mae: 0.411987, mean_q: 4.381667
 33234/100000: episode: 723, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 89.791, mean reward: 3.096 [1.915, 5.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.439, 10.341], loss: 0.185295, mae: 0.415700, mean_q: 4.492797
 33263/100000: episode: 724, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 91.392, mean reward: 3.151 [1.778, 7.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.383], loss: 0.258048, mae: 0.461162, mean_q: 4.552391
 33290/100000: episode: 725, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 77.359, mean reward: 2.865 [2.123, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.076, 10.404], loss: 0.174432, mae: 0.410452, mean_q: 4.539126
 33297/100000: episode: 726, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 22.825, mean reward: 3.261 [2.812, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.228, 10.502], loss: 0.259164, mae: 0.460508, mean_q: 4.568450
 33308/100000: episode: 727, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 56.274, mean reward: 5.116 [3.473, 10.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.640, 10.100], loss: 0.210070, mae: 0.445400, mean_q: 4.622210
 33338/100000: episode: 728, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 96.427, mean reward: 3.214 [2.300, 6.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.512, 10.441], loss: 0.259984, mae: 0.462428, mean_q: 4.594810
 33367/100000: episode: 729, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 94.228, mean reward: 3.249 [2.131, 8.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.177, 10.490], loss: 0.251821, mae: 0.435777, mean_q: 4.654896
 33394/100000: episode: 730, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 70.203, mean reward: 2.600 [1.931, 5.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.999, 10.417], loss: 0.297626, mae: 0.460756, mean_q: 4.617548
 33416/100000: episode: 731, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 62.456, mean reward: 2.839 [1.757, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.206, 10.332], loss: 0.220361, mae: 0.451245, mean_q: 4.638483
 33446/100000: episode: 732, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 84.904, mean reward: 2.830 [1.680, 6.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.423, 10.232], loss: 0.285649, mae: 0.478190, mean_q: 4.625719
 33457/100000: episode: 733, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 57.099, mean reward: 5.191 [3.738, 6.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.510, 10.100], loss: 0.340344, mae: 0.507854, mean_q: 4.729082
 33466/100000: episode: 734, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 25.017, mean reward: 2.780 [2.558, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.405], loss: 0.313163, mae: 0.502631, mean_q: 4.706961
 33488/100000: episode: 735, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 96.944, mean reward: 4.407 [2.785, 15.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.829, 10.477], loss: 0.323325, mae: 0.450163, mean_q: 4.636609
 33495/100000: episode: 736, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 18.216, mean reward: 2.602 [2.410, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.427], loss: 0.272736, mae: 0.451155, mean_q: 4.646557
 33501/100000: episode: 737, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 20.935, mean reward: 3.489 [2.573, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.421], loss: 0.209146, mae: 0.448569, mean_q: 4.632381
 33507/100000: episode: 738, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 30.034, mean reward: 5.006 [3.591, 7.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-1.378, 10.530], loss: 0.406164, mae: 0.493479, mean_q: 4.687974
 33518/100000: episode: 739, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 57.998, mean reward: 5.273 [3.449, 6.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.667, 10.100], loss: 0.440612, mae: 0.468264, mean_q: 4.559746
 33524/100000: episode: 740, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 27.623, mean reward: 4.604 [3.355, 7.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.530], loss: 0.347857, mae: 0.605583, mean_q: 5.094892
 33551/100000: episode: 741, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 118.326, mean reward: 4.382 [2.733, 6.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.576], loss: 0.265475, mae: 0.467910, mean_q: 4.692387
 33580/100000: episode: 742, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 69.198, mean reward: 2.386 [1.833, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.085, 10.308], loss: 0.206474, mae: 0.442392, mean_q: 4.636671
 33610/100000: episode: 743, duration: 0.144s, episode steps: 30, steps per second: 209, episode reward: 98.280, mean reward: 3.276 [2.680, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.228, 10.407], loss: 0.245818, mae: 0.447871, mean_q: 4.660171
 33619/100000: episode: 744, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 30.693, mean reward: 3.410 [2.800, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.775, 10.315], loss: 0.381266, mae: 0.503960, mean_q: 4.845808
 33628/100000: episode: 745, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 19.708, mean reward: 2.190 [1.801, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.292], loss: 0.487142, mae: 0.532912, mean_q: 4.848756
 33637/100000: episode: 746, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 24.803, mean reward: 2.756 [2.266, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.297], loss: 0.263445, mae: 0.482511, mean_q: 4.558688
 33667/100000: episode: 747, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 76.489, mean reward: 2.550 [1.908, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.461, 10.321], loss: 0.390213, mae: 0.506049, mean_q: 4.738127
 33678/100000: episode: 748, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 62.480, mean reward: 5.680 [4.222, 9.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.456, 10.100], loss: 0.269181, mae: 0.484532, mean_q: 4.768886
 33707/100000: episode: 749, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 82.404, mean reward: 2.842 [1.975, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.342], loss: 0.258541, mae: 0.491132, mean_q: 4.738653
 33736/100000: episode: 750, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 92.027, mean reward: 3.173 [2.365, 5.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.040, 10.542], loss: 0.458591, mae: 0.536879, mean_q: 4.824410
 33747/100000: episode: 751, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 84.593, mean reward: 7.690 [5.370, 10.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.423, 10.100], loss: 0.192707, mae: 0.445508, mean_q: 4.807869
 33769/100000: episode: 752, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 65.035, mean reward: 2.956 [2.465, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.500, 10.474], loss: 0.326892, mae: 0.535887, mean_q: 4.919500
 33776/100000: episode: 753, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 22.056, mean reward: 3.151 [2.721, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.843, 10.470], loss: 0.235216, mae: 0.465305, mean_q: 4.898628
 33798/100000: episode: 754, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 83.815, mean reward: 3.810 [2.583, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.571, 10.507], loss: 0.385730, mae: 0.513684, mean_q: 4.909853
 33827/100000: episode: 755, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 83.109, mean reward: 2.866 [1.591, 7.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.744, 10.192], loss: 0.363319, mae: 0.531061, mean_q: 4.958686
 33836/100000: episode: 756, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 26.107, mean reward: 2.901 [2.239, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.472], loss: 0.352312, mae: 0.558877, mean_q: 4.785351
 33863/100000: episode: 757, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 85.719, mean reward: 3.175 [2.110, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.370], loss: 0.368088, mae: 0.536091, mean_q: 5.038461
 33890/100000: episode: 758, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 73.392, mean reward: 2.718 [2.151, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.300, 10.425], loss: 0.326565, mae: 0.512659, mean_q: 4.841965
 33917/100000: episode: 759, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 113.540, mean reward: 4.205 [2.756, 8.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.061, 10.510], loss: 0.292244, mae: 0.490633, mean_q: 4.936668
 33923/100000: episode: 760, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 24.401, mean reward: 4.067 [3.483, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.556], loss: 0.262433, mae: 0.481963, mean_q: 4.841013
 33945/100000: episode: 761, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 71.594, mean reward: 3.254 [2.542, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.171, 10.484], loss: 0.425284, mae: 0.499985, mean_q: 4.952236
 33952/100000: episode: 762, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 15.897, mean reward: 2.271 [2.124, 2.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.354], loss: 0.295648, mae: 0.515469, mean_q: 4.828650
 33981/100000: episode: 763, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 127.434, mean reward: 4.394 [2.715, 8.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.078, 10.555], loss: 0.359443, mae: 0.509017, mean_q: 4.927884
 34008/100000: episode: 764, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 82.383, mean reward: 3.051 [2.287, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.885, 10.456], loss: 0.397751, mae: 0.513972, mean_q: 4.926068
 34015/100000: episode: 765, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 16.040, mean reward: 2.291 [2.105, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.360], loss: 0.268647, mae: 0.514835, mean_q: 5.171478
 34024/100000: episode: 766, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 29.565, mean reward: 3.285 [2.704, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.557], loss: 0.348079, mae: 0.534934, mean_q: 4.867692
 34046/100000: episode: 767, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 84.640, mean reward: 3.847 [2.340, 5.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.235, 10.459], loss: 0.471532, mae: 0.577399, mean_q: 5.065165
 34055/100000: episode: 768, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 29.078, mean reward: 3.231 [2.656, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.155, 10.520], loss: 0.664994, mae: 0.649114, mean_q: 5.125002
 34061/100000: episode: 769, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 35.884, mean reward: 5.981 [3.754, 8.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.552], loss: 0.416435, mae: 0.559849, mean_q: 5.000691
 34072/100000: episode: 770, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 66.311, mean reward: 6.028 [4.051, 7.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.557, 10.100], loss: 0.503798, mae: 0.577392, mean_q: 5.144401
 34102/100000: episode: 771, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 78.139, mean reward: 2.605 [1.722, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.090, 10.273], loss: 0.268116, mae: 0.500695, mean_q: 5.030622
 34111/100000: episode: 772, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 26.754, mean reward: 2.973 [2.527, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.506], loss: 0.454829, mae: 0.631752, mean_q: 5.304737
 34133/100000: episode: 773, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 108.928, mean reward: 4.951 [2.924, 7.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.406, 10.587], loss: 0.361229, mae: 0.560634, mean_q: 5.104620
 34155/100000: episode: 774, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 75.100, mean reward: 3.414 [2.260, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.656, 10.422], loss: 0.410779, mae: 0.556275, mean_q: 5.001009
 34185/100000: episode: 775, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 85.209, mean reward: 2.840 [1.719, 11.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.756, 10.238], loss: 0.462686, mae: 0.612325, mean_q: 5.132096
 34191/100000: episode: 776, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 54.974, mean reward: 9.162 [4.023, 15.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.087, 10.491], loss: 0.412966, mae: 0.578124, mean_q: 5.269059
 34220/100000: episode: 777, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 163.501, mean reward: 5.638 [3.688, 11.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.267, 10.498], loss: 0.394247, mae: 0.533790, mean_q: 5.084193
 34231/100000: episode: 778, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 78.022, mean reward: 7.093 [3.908, 19.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.408, 10.100], loss: 0.395893, mae: 0.591594, mean_q: 5.252830
 34242/100000: episode: 779, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 68.591, mean reward: 6.236 [3.391, 9.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.402, 10.100], loss: 0.453060, mae: 0.584682, mean_q: 5.049512
 34248/100000: episode: 780, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 23.893, mean reward: 3.982 [3.695, 4.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.527], loss: 0.747111, mae: 0.677164, mean_q: 5.372990
 34257/100000: episode: 781, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 25.968, mean reward: 2.885 [2.471, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.454], loss: 0.953148, mae: 0.707352, mean_q: 5.344147
 34284/100000: episode: 782, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 99.043, mean reward: 3.668 [2.291, 6.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.405, 10.611], loss: 0.400019, mae: 0.576798, mean_q: 5.125482
 34311/100000: episode: 783, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 62.160, mean reward: 2.302 [1.566, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.592, 10.194], loss: 0.702590, mae: 0.655809, mean_q: 5.189989
 34333/100000: episode: 784, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 60.953, mean reward: 2.771 [2.149, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.442], loss: 0.445000, mae: 0.604311, mean_q: 5.288761
 34355/100000: episode: 785, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 56.168, mean reward: 2.553 [1.877, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.402, 10.254], loss: 0.488554, mae: 0.602256, mean_q: 5.200235
 34366/100000: episode: 786, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 321.544, mean reward: 29.231 [4.160, 225.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.556, 10.100], loss: 0.700472, mae: 0.648027, mean_q: 5.269079
 34372/100000: episode: 787, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 29.550, mean reward: 4.925 [4.423, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.563], loss: 0.405375, mae: 0.600886, mean_q: 5.144150
 34381/100000: episode: 788, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 35.265, mean reward: 3.918 [2.859, 6.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.351, 10.466], loss: 0.457824, mae: 0.645191, mean_q: 5.144427
[Info] 3-TH LEVEL FOUND: 10.079248428344727, Considering 10/90 traces
 34392/100000: episode: 789, duration: 4.070s, episode steps: 11, steps per second: 3, episode reward: 69.731, mean reward: 6.339 [4.180, 10.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.564, 10.100], loss: 0.532421, mae: 0.676412, mean_q: 5.446791
 34401/100000: episode: 790, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 40.722, mean reward: 4.525 [3.385, 7.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.346, 10.100], loss: 0.516620, mae: 0.662426, mean_q: 5.402458
 34410/100000: episode: 791, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 56.020, mean reward: 6.224 [3.162, 9.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.321, 10.100], loss: 0.439327, mae: 0.608763, mean_q: 5.330044
 34420/100000: episode: 792, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 85.852, mean reward: 8.585 [4.506, 22.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.376, 10.100], loss: 0.418882, mae: 0.569418, mean_q: 5.258820
 34429/100000: episode: 793, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 42.352, mean reward: 4.706 [3.500, 6.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.767, 10.100], loss: 0.352481, mae: 0.553774, mean_q: 5.447931
 34439/100000: episode: 794, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 60.406, mean reward: 6.041 [4.067, 7.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.399, 10.100], loss: 0.640517, mae: 0.630539, mean_q: 5.284118
 34448/100000: episode: 795, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 41.167, mean reward: 4.574 [3.634, 6.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.044, 10.100], loss: 0.590221, mae: 0.608729, mean_q: 5.206132
 34472/100000: episode: 796, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 77.393, mean reward: 3.225 [2.028, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.379], loss: 1.057217, mae: 0.717010, mean_q: 5.519028
 34487/100000: episode: 797, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 70.256, mean reward: 4.684 [3.477, 7.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.308, 10.627], loss: 52.398201, mae: 1.453720, mean_q: 5.744833
 34496/100000: episode: 798, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 67.136, mean reward: 7.460 [4.664, 13.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.529, 10.100], loss: 1.474590, mae: 1.062027, mean_q: 5.505306
 34520/100000: episode: 799, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 58.725, mean reward: 2.447 [1.673, 4.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.324, 10.260], loss: 0.498732, mae: 0.646031, mean_q: 5.220678
 34529/100000: episode: 800, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 52.259, mean reward: 5.807 [4.027, 9.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.423, 10.100], loss: 0.470395, mae: 0.648205, mean_q: 5.450223
 34539/100000: episode: 801, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 43.971, mean reward: 4.397 [3.238, 6.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.450, 10.100], loss: 0.499773, mae: 0.675731, mean_q: 5.595593
 34554/100000: episode: 802, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 78.437, mean reward: 5.229 [3.179, 7.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.060, 10.591], loss: 0.532189, mae: 0.672354, mean_q: 5.437079
 34564/100000: episode: 803, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 78.618, mean reward: 7.862 [4.762, 18.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.455, 10.100], loss: 0.978943, mae: 0.740234, mean_q: 5.400096
 34574/100000: episode: 804, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 35.083, mean reward: 3.508 [2.662, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.440, 10.100], loss: 0.510892, mae: 0.638507, mean_q: 5.484737
 34584/100000: episode: 805, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 49.255, mean reward: 4.925 [3.642, 10.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.339, 10.100], loss: 0.749596, mae: 0.679535, mean_q: 5.305927
 34608/100000: episode: 806, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 106.883, mean reward: 4.453 [2.678, 8.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.754, 10.467], loss: 0.705761, mae: 0.672895, mean_q: 5.593273
 34618/100000: episode: 807, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 51.398, mean reward: 5.140 [3.578, 8.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.483, 10.100], loss: 0.511247, mae: 0.660538, mean_q: 5.530903
 34628/100000: episode: 808, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 66.292, mean reward: 6.629 [4.755, 11.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.554, 10.100], loss: 0.502241, mae: 0.644951, mean_q: 5.557673
 34637/100000: episode: 809, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 55.511, mean reward: 6.168 [4.742, 8.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.551, 10.100], loss: 0.776532, mae: 0.709253, mean_q: 5.536561
 34646/100000: episode: 810, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 68.160, mean reward: 7.573 [4.596, 13.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.991, 10.100], loss: 0.788581, mae: 0.694296, mean_q: 5.567682
 34656/100000: episode: 811, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 50.585, mean reward: 5.058 [3.811, 6.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.517, 10.100], loss: 0.472569, mae: 0.675052, mean_q: 5.589743
 34666/100000: episode: 812, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 64.005, mean reward: 6.401 [3.420, 12.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.445, 10.100], loss: 0.639865, mae: 0.634128, mean_q: 5.299520
 34690/100000: episode: 813, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 84.618, mean reward: 3.526 [2.209, 5.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.675, 10.458], loss: 0.670681, mae: 0.676991, mean_q: 5.592167
 34700/100000: episode: 814, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 47.143, mean reward: 4.714 [2.970, 6.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.333, 10.100], loss: 0.568319, mae: 0.613417, mean_q: 5.548947
 34710/100000: episode: 815, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 100.081, mean reward: 10.008 [6.802, 14.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.735, 10.100], loss: 0.949431, mae: 0.713542, mean_q: 5.592501
 34719/100000: episode: 816, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 58.679, mean reward: 6.520 [3.333, 11.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.482, 10.100], loss: 0.841137, mae: 0.753189, mean_q: 6.076853
 34729/100000: episode: 817, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 48.459, mean reward: 4.846 [3.674, 6.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.536, 10.100], loss: 0.888652, mae: 0.724823, mean_q: 5.613585
 34739/100000: episode: 818, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 70.319, mean reward: 7.032 [4.719, 8.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.538, 10.100], loss: 76.167160, mae: 1.477491, mean_q: 5.965774
 34754/100000: episode: 819, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 231.231, mean reward: 15.415 [4.298, 84.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-1.524, 10.694], loss: 1.473705, mae: 1.126230, mean_q: 6.200558
 34764/100000: episode: 820, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 54.239, mean reward: 5.424 [3.056, 9.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.483, 10.100], loss: 1.109385, mae: 0.917940, mean_q: 5.771436
 34773/100000: episode: 821, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 34.899, mean reward: 3.878 [2.808, 7.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.253, 10.100], loss: 1.096073, mae: 0.830802, mean_q: 5.602715
 34788/100000: episode: 822, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 75.421, mean reward: 5.028 [3.570, 8.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.817, 10.554], loss: 0.757103, mae: 0.771246, mean_q: 5.753540
 34812/100000: episode: 823, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 162.023, mean reward: 6.751 [3.118, 14.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.580], loss: 1.016387, mae: 0.818183, mean_q: 5.772540
 34822/100000: episode: 824, duration: 0.062s, episode steps: 10, steps per second: 163, episode reward: 75.494, mean reward: 7.549 [4.223, 10.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.439, 10.100], loss: 10.519480, mae: 1.192819, mean_q: 6.189092
 34832/100000: episode: 825, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 56.709, mean reward: 5.671 [4.182, 9.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.540, 10.100], loss: 2.384675, mae: 1.414197, mean_q: 5.863330
 34856/100000: episode: 826, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 121.348, mean reward: 5.056 [3.209, 7.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.904, 10.575], loss: 0.900024, mae: 0.860141, mean_q: 5.953070
 34866/100000: episode: 827, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 57.366, mean reward: 5.737 [4.137, 8.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.412, 10.100], loss: 12.365158, mae: 1.696205, mean_q: 6.657225
 34876/100000: episode: 828, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 53.915, mean reward: 5.392 [4.208, 8.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.625, 10.100], loss: 2.370560, mae: 1.526217, mean_q: 4.786224
 34885/100000: episode: 829, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 48.093, mean reward: 5.344 [3.417, 10.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.360, 10.100], loss: 1.749321, mae: 1.213166, mean_q: 6.228935
 34894/100000: episode: 830, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 48.312, mean reward: 5.368 [3.998, 7.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.175, 10.100], loss: 1.692375, mae: 1.191165, mean_q: 6.059179
 34904/100000: episode: 831, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 54.183, mean reward: 5.418 [4.128, 7.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.266, 10.100], loss: 3.100613, mae: 1.013411, mean_q: 5.679084
 34913/100000: episode: 832, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 50.881, mean reward: 5.653 [3.829, 16.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.356, 10.100], loss: 0.941499, mae: 0.909859, mean_q: 6.049895
 34928/100000: episode: 833, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 93.976, mean reward: 6.265 [4.298, 11.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.554], loss: 7.913943, mae: 1.076716, mean_q: 5.805206
 34938/100000: episode: 834, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 44.349, mean reward: 4.435 [2.942, 6.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.521, 10.100], loss: 86.979713, mae: 2.050594, mean_q: 6.480424
 34953/100000: episode: 835, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 63.430, mean reward: 4.229 [3.392, 6.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.490], loss: 1.596849, mae: 1.121740, mean_q: 6.308181
 34962/100000: episode: 836, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 49.527, mean reward: 5.503 [3.636, 9.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.506, 10.100], loss: 0.890958, mae: 0.852548, mean_q: 5.842483
 34972/100000: episode: 837, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 54.091, mean reward: 5.409 [3.910, 8.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.619, 10.100], loss: 0.904499, mae: 0.821124, mean_q: 6.102758
 34982/100000: episode: 838, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 68.609, mean reward: 6.861 [4.109, 11.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.624, 10.100], loss: 12.535509, mae: 1.486087, mean_q: 6.427735
 34992/100000: episode: 839, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 55.223, mean reward: 5.522 [4.700, 7.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.443, 10.100], loss: 1.195773, mae: 0.921595, mean_q: 6.019347
 35001/100000: episode: 840, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 43.228, mean reward: 4.803 [2.533, 13.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.375, 10.100], loss: 0.666833, mae: 0.722601, mean_q: 5.579813
 35025/100000: episode: 841, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 97.517, mean reward: 4.063 [2.291, 5.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.598, 10.416], loss: 5.247961, mae: 1.098082, mean_q: 6.498611
 35035/100000: episode: 842, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 47.594, mean reward: 4.759 [3.361, 7.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.330, 10.100], loss: 0.857080, mae: 0.801869, mean_q: 5.471959
 35045/100000: episode: 843, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 50.496, mean reward: 5.050 [3.916, 6.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.380, 10.100], loss: 1.432769, mae: 0.979365, mean_q: 6.298153
 35054/100000: episode: 844, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 91.403, mean reward: 10.156 [6.016, 26.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.682, 10.100], loss: 0.918838, mae: 0.799815, mean_q: 6.058570
 35063/100000: episode: 845, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 43.427, mean reward: 4.825 [2.997, 12.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.417, 10.100], loss: 1.503166, mae: 0.905054, mean_q: 6.243977
 35073/100000: episode: 846, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 42.315, mean reward: 4.231 [3.137, 5.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.371, 10.100], loss: 0.964653, mae: 0.789996, mean_q: 6.270141
 35083/100000: episode: 847, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 58.578, mean reward: 5.858 [3.433, 10.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.354, 10.100], loss: 0.813204, mae: 0.795624, mean_q: 6.170174
 35093/100000: episode: 848, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 98.185, mean reward: 9.818 [6.380, 16.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.454, 10.100], loss: 0.639259, mae: 0.712233, mean_q: 5.976910
 35102/100000: episode: 849, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 61.021, mean reward: 6.780 [4.673, 14.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.460, 10.100], loss: 1.377439, mae: 0.901618, mean_q: 6.039003
 35111/100000: episode: 850, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 46.048, mean reward: 5.116 [3.915, 6.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.495, 10.100], loss: 2.168230, mae: 0.965729, mean_q: 6.275156
 35120/100000: episode: 851, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 35.662, mean reward: 3.962 [2.679, 7.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.544, 10.100], loss: 0.969969, mae: 0.820363, mean_q: 6.198599
 35130/100000: episode: 852, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 54.314, mean reward: 5.431 [4.085, 8.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.392, 10.100], loss: 76.377670, mae: 1.654653, mean_q: 6.464822
 35140/100000: episode: 853, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 61.891, mean reward: 6.189 [3.825, 12.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.746, 10.100], loss: 2.037363, mae: 1.163504, mean_q: 6.827418
 35150/100000: episode: 854, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 55.759, mean reward: 5.576 [3.060, 9.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.427, 10.100], loss: 0.951348, mae: 0.837081, mean_q: 5.915828
 35159/100000: episode: 855, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 47.644, mean reward: 5.294 [2.839, 16.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.482, 10.100], loss: 1.131541, mae: 0.936939, mean_q: 6.015966
 35169/100000: episode: 856, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 223.920, mean reward: 22.392 [4.259, 171.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.462, 10.100], loss: 1.357685, mae: 0.930726, mean_q: 6.217361
 35184/100000: episode: 857, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 62.377, mean reward: 4.158 [2.998, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.486], loss: 0.943197, mae: 0.775534, mean_q: 6.021747
 35194/100000: episode: 858, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 50.642, mean reward: 5.064 [4.205, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.505, 10.100], loss: 1.888097, mae: 0.983974, mean_q: 6.188696
 35204/100000: episode: 859, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 85.930, mean reward: 8.593 [5.665, 17.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.532, 10.100], loss: 1.252358, mae: 0.954376, mean_q: 6.628128
 35214/100000: episode: 860, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 103.602, mean reward: 10.360 [6.415, 15.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.384, 10.100], loss: 1.851383, mae: 0.944033, mean_q: 6.547019
 35223/100000: episode: 861, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 55.299, mean reward: 6.144 [4.434, 9.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.540, 10.100], loss: 3.245469, mae: 1.164390, mean_q: 6.726437
 35232/100000: episode: 862, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 50.519, mean reward: 5.613 [4.385, 6.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.379, 10.100], loss: 94.683952, mae: 2.154206, mean_q: 6.692254
 35241/100000: episode: 863, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 49.037, mean reward: 5.449 [3.408, 13.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.341, 10.100], loss: 3.407588, mae: 1.718526, mean_q: 7.664595
 35250/100000: episode: 864, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 38.833, mean reward: 4.315 [3.143, 5.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.342, 10.100], loss: 1.864286, mae: 1.306269, mean_q: 6.310148
 35260/100000: episode: 865, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 112.750, mean reward: 11.275 [5.397, 20.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.414, 10.100], loss: 1.147115, mae: 0.955139, mean_q: 6.084119
 35270/100000: episode: 866, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 53.098, mean reward: 5.310 [4.288, 6.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.850, 10.100], loss: 1.703206, mae: 0.982371, mean_q: 6.597370
 35280/100000: episode: 867, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 99.633, mean reward: 9.963 [6.220, 16.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.435, 10.100], loss: 1.271470, mae: 0.927607, mean_q: 6.442714
 35304/100000: episode: 868, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 114.374, mean reward: 4.766 [2.687, 7.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.642], loss: 32.502026, mae: 1.299041, mean_q: 6.746498
 35314/100000: episode: 869, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 40.551, mean reward: 4.055 [2.989, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.723, 10.100], loss: 10.349522, mae: 1.463102, mean_q: 6.851216
 35324/100000: episode: 870, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 35.983, mean reward: 3.598 [2.610, 5.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.423, 10.100], loss: 2.541942, mae: 1.505863, mean_q: 5.774796
 35334/100000: episode: 871, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 50.687, mean reward: 5.069 [3.222, 10.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.477, 10.100], loss: 1.416983, mae: 1.109615, mean_q: 6.552428
 35358/100000: episode: 872, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 100.670, mean reward: 4.195 [2.868, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.373, 10.551], loss: 1.155527, mae: 0.955748, mean_q: 6.514387
[Info] FALSIFICATION!
 35359/100000: episode: 873, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.171, 9.952], loss: 1.392242, mae: 1.203510, mean_q: 6.461521
 35368/100000: episode: 874, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 41.862, mean reward: 4.651 [3.376, 6.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.399, 10.100], loss: 1.212764, mae: 0.976750, mean_q: 6.927454
 35377/100000: episode: 875, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 43.400, mean reward: 4.822 [3.736, 7.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.462, 10.100], loss: 1.371025, mae: 0.969476, mean_q: 6.597671
 35387/100000: episode: 876, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 60.530, mean reward: 6.053 [4.526, 10.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.369, 10.100], loss: 0.969190, mae: 0.818366, mean_q: 6.492852
 35411/100000: episode: 877, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 86.943, mean reward: 3.623 [2.086, 4.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.107, 10.423], loss: 1.246315, mae: 0.917996, mean_q: 6.652874
 35420/100000: episode: 878, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 62.984, mean reward: 6.998 [4.181, 12.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.551, 10.100], loss: 0.555178, mae: 0.735107, mean_q: 6.297209
[Info] Complete ISplit Iteration
[Info] Levels: [5.623072, 7.552291, 10.079248, 11.577501]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.7]
[Info] Error Prob: 0.0007000000000000001

 35430/100000: episode: 879, duration: 4.306s, episode steps: 10, steps per second: 2, episode reward: 69.603, mean reward: 6.960 [4.535, 15.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.526, 10.100], loss: 0.635205, mae: 0.754680, mean_q: 6.302450
 35530/100000: episode: 880, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.784, mean reward: 1.878 [1.478, 2.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.983, 10.207], loss: 15.459458, mae: 1.439747, mean_q: 6.755047
 35630/100000: episode: 881, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.578, mean reward: 2.026 [1.463, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.794, 10.179], loss: 311.367554, mae: 2.233026, mean_q: 7.235583
 35730/100000: episode: 882, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 206.119, mean reward: 2.061 [1.495, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.435, 10.310], loss: 306.775909, mae: 1.954663, mean_q: 7.264318
 35830/100000: episode: 883, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 191.854, mean reward: 1.919 [1.506, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.305, 10.361], loss: 154.311096, mae: 1.582612, mean_q: 7.075209
 35930/100000: episode: 884, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.979, mean reward: 1.980 [1.472, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.214], loss: 9.908416, mae: 1.227779, mean_q: 6.835455
 36030/100000: episode: 885, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 210.472, mean reward: 2.105 [1.464, 6.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.309, 10.098], loss: 320.593689, mae: 2.050749, mean_q: 7.204059
 36130/100000: episode: 886, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 175.243, mean reward: 1.752 [1.452, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.350, 10.098], loss: 172.244995, mae: 1.954754, mean_q: 7.421048
 36230/100000: episode: 887, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 191.761, mean reward: 1.918 [1.442, 5.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.100, 10.098], loss: 155.614441, mae: 1.582958, mean_q: 6.768141
 36330/100000: episode: 888, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 236.577, mean reward: 2.366 [1.528, 4.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.782, 10.098], loss: 153.989807, mae: 1.621565, mean_q: 7.035410
 36430/100000: episode: 889, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 183.041, mean reward: 1.830 [1.443, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.931, 10.113], loss: 322.239288, mae: 2.002426, mean_q: 6.899155
 36530/100000: episode: 890, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 184.457, mean reward: 1.845 [1.440, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.495, 10.098], loss: 463.652100, mae: 2.994566, mean_q: 7.809671
 36630/100000: episode: 891, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: 188.203, mean reward: 1.882 [1.469, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.777, 10.170], loss: 161.020569, mae: 1.722741, mean_q: 7.066858
 36730/100000: episode: 892, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 219.751, mean reward: 2.198 [1.445, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.359, 10.198], loss: 12.884502, mae: 1.244835, mean_q: 6.837976
 36830/100000: episode: 893, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 186.326, mean reward: 1.863 [1.466, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.470, 10.098], loss: 459.177582, mae: 2.518808, mean_q: 7.294140
 36930/100000: episode: 894, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 197.216, mean reward: 1.972 [1.442, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.982, 10.098], loss: 13.645447, mae: 1.731020, mean_q: 7.157170
 37030/100000: episode: 895, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 196.943, mean reward: 1.969 [1.466, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.098], loss: 15.854154, mae: 1.204367, mean_q: 6.630319
 37130/100000: episode: 896, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 198.800, mean reward: 1.988 [1.468, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.098], loss: 157.504303, mae: 1.626725, mean_q: 6.871950
 37230/100000: episode: 897, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.494, mean reward: 1.825 [1.485, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.008, 10.185], loss: 12.721755, mae: 1.207197, mean_q: 6.599649
 37330/100000: episode: 898, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.725, mean reward: 1.897 [1.480, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.599, 10.126], loss: 169.878342, mae: 1.818406, mean_q: 6.848596
 37430/100000: episode: 899, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.000, mean reward: 1.900 [1.441, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.743, 10.145], loss: 3.372608, mae: 1.168879, mean_q: 6.369053
 37530/100000: episode: 900, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 186.289, mean reward: 1.863 [1.485, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.741, 10.098], loss: 11.647264, mae: 1.362045, mean_q: 6.244490
 37630/100000: episode: 901, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 198.835, mean reward: 1.988 [1.455, 5.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.893, 10.257], loss: 1.717523, mae: 1.044225, mean_q: 6.200856
 37730/100000: episode: 902, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.611, mean reward: 1.896 [1.438, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.592, 10.098], loss: 1.503634, mae: 0.951272, mean_q: 6.094706
 37830/100000: episode: 903, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 183.699, mean reward: 1.837 [1.458, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.666, 10.126], loss: 161.457214, mae: 1.556224, mean_q: 6.442689
 37930/100000: episode: 904, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.666, mean reward: 1.907 [1.440, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.719, 10.098], loss: 158.577225, mae: 1.563393, mean_q: 6.420464
 38030/100000: episode: 905, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 210.813, mean reward: 2.108 [1.483, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.424, 10.098], loss: 15.174905, mae: 1.333506, mean_q: 6.341550
 38130/100000: episode: 906, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.108, mean reward: 1.921 [1.463, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.471, 10.109], loss: 168.829315, mae: 1.730981, mean_q: 6.522389
 38230/100000: episode: 907, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 202.623, mean reward: 2.026 [1.494, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.826, 10.338], loss: 5.689282, mae: 1.048610, mean_q: 6.071336
 38330/100000: episode: 908, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 185.034, mean reward: 1.850 [1.456, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.587, 10.188], loss: 5.443377, mae: 0.986493, mean_q: 5.939444
 38430/100000: episode: 909, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 180.089, mean reward: 1.801 [1.436, 2.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.517, 10.107], loss: 152.948532, mae: 1.171706, mean_q: 5.768732
 38530/100000: episode: 910, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 206.199, mean reward: 2.062 [1.460, 6.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.982, 10.576], loss: 7.479056, mae: 1.345654, mean_q: 6.098157
 38630/100000: episode: 911, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 218.935, mean reward: 2.189 [1.549, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.098], loss: 1.318199, mae: 0.839041, mean_q: 5.752007
 38730/100000: episode: 912, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 191.001, mean reward: 1.910 [1.482, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.118, 10.098], loss: 7.058259, mae: 1.029552, mean_q: 5.808047
 38830/100000: episode: 913, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 189.357, mean reward: 1.894 [1.449, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.578, 10.098], loss: 20.664906, mae: 1.112452, mean_q: 5.756617
 38930/100000: episode: 914, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 210.808, mean reward: 2.108 [1.514, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.866, 10.453], loss: 5.142231, mae: 0.872734, mean_q: 5.663062
 39030/100000: episode: 915, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 193.810, mean reward: 1.938 [1.440, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.457, 10.098], loss: 15.622076, mae: 0.933518, mean_q: 5.585436
 39130/100000: episode: 916, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 208.173, mean reward: 2.082 [1.446, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.677, 10.320], loss: 22.454432, mae: 1.196045, mean_q: 5.600414
 39230/100000: episode: 917, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 210.943, mean reward: 2.109 [1.455, 8.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.686, 10.098], loss: 2.893461, mae: 0.892183, mean_q: 5.499617
 39330/100000: episode: 918, duration: 0.466s, episode steps: 100, steps per second: 214, episode reward: 213.296, mean reward: 2.133 [1.481, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.294, 10.098], loss: 4.946557, mae: 0.794155, mean_q: 5.190936
 39430/100000: episode: 919, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 231.026, mean reward: 2.310 [1.461, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.005, 10.098], loss: 154.597153, mae: 1.184275, mean_q: 5.323794
 39530/100000: episode: 920, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.027, mean reward: 1.920 [1.534, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.953, 10.253], loss: 152.743454, mae: 1.234004, mean_q: 5.384798
 39630/100000: episode: 921, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 193.809, mean reward: 1.938 [1.466, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.235, 10.098], loss: 153.180023, mae: 1.194834, mean_q: 5.349080
 39730/100000: episode: 922, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 181.319, mean reward: 1.813 [1.456, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.509, 10.098], loss: 0.780271, mae: 0.621295, mean_q: 4.890712
 39830/100000: episode: 923, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 200.978, mean reward: 2.010 [1.454, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.377, 10.098], loss: 0.621354, mae: 0.542691, mean_q: 4.633019
 39930/100000: episode: 924, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 186.541, mean reward: 1.865 [1.471, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.165], loss: 0.561816, mae: 0.533538, mean_q: 4.574353
 40030/100000: episode: 925, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 176.230, mean reward: 1.762 [1.459, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.922, 10.098], loss: 0.649791, mae: 0.503435, mean_q: 4.404715
 40130/100000: episode: 926, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.187, mean reward: 1.882 [1.452, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.130, 10.180], loss: 0.340931, mae: 0.419936, mean_q: 4.283550
 40230/100000: episode: 927, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 187.615, mean reward: 1.876 [1.486, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.077, 10.098], loss: 152.527954, mae: 0.833192, mean_q: 4.323111
 40330/100000: episode: 928, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 190.939, mean reward: 1.909 [1.459, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.261, 10.141], loss: 0.357730, mae: 0.452758, mean_q: 4.147476
 40430/100000: episode: 929, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 196.885, mean reward: 1.969 [1.437, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.041, 10.098], loss: 0.184387, mae: 0.366600, mean_q: 3.953024
 40530/100000: episode: 930, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.382, mean reward: 1.964 [1.443, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.264], loss: 0.161288, mae: 0.340795, mean_q: 3.943843
 40630/100000: episode: 931, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.258, mean reward: 1.853 [1.464, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.600, 10.194], loss: 0.128270, mae: 0.326900, mean_q: 3.898403
 40730/100000: episode: 932, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 182.369, mean reward: 1.824 [1.440, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.208], loss: 0.112225, mae: 0.314475, mean_q: 3.881384
 40830/100000: episode: 933, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 187.756, mean reward: 1.878 [1.443, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.098], loss: 0.127136, mae: 0.327095, mean_q: 3.885436
 40930/100000: episode: 934, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.039, mean reward: 1.920 [1.431, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.098], loss: 0.103938, mae: 0.309332, mean_q: 3.855440
 41030/100000: episode: 935, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 188.616, mean reward: 1.886 [1.519, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.049, 10.098], loss: 0.138645, mae: 0.327969, mean_q: 3.870301
 41130/100000: episode: 936, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.580, mean reward: 1.816 [1.481, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.472, 10.230], loss: 0.110542, mae: 0.322263, mean_q: 3.870215
 41230/100000: episode: 937, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 189.879, mean reward: 1.899 [1.462, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.470, 10.098], loss: 0.098374, mae: 0.306553, mean_q: 3.849377
 41330/100000: episode: 938, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.725, mean reward: 1.887 [1.493, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.389, 10.098], loss: 0.109462, mae: 0.310590, mean_q: 3.865385
 41430/100000: episode: 939, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 223.361, mean reward: 2.234 [1.563, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.993, 10.154], loss: 0.105790, mae: 0.310040, mean_q: 3.844780
 41530/100000: episode: 940, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.125, mean reward: 1.961 [1.469, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.250, 10.098], loss: 0.112594, mae: 0.311171, mean_q: 3.859925
 41630/100000: episode: 941, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 192.478, mean reward: 1.925 [1.455, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.395, 10.098], loss: 0.091326, mae: 0.305053, mean_q: 3.868230
 41730/100000: episode: 942, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 190.952, mean reward: 1.910 [1.460, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.717, 10.098], loss: 0.107676, mae: 0.310645, mean_q: 3.851849
 41830/100000: episode: 943, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 191.617, mean reward: 1.916 [1.429, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.001, 10.098], loss: 0.109208, mae: 0.317067, mean_q: 3.865255
 41930/100000: episode: 944, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.702, mean reward: 1.947 [1.456, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.049, 10.245], loss: 0.091417, mae: 0.296168, mean_q: 3.851653
 42030/100000: episode: 945, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 218.071, mean reward: 2.181 [1.478, 6.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.098], loss: 0.116400, mae: 0.321963, mean_q: 3.855090
 42130/100000: episode: 946, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 184.992, mean reward: 1.850 [1.457, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.547, 10.098], loss: 0.115612, mae: 0.321805, mean_q: 3.859943
 42230/100000: episode: 947, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 181.611, mean reward: 1.816 [1.456, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.833, 10.098], loss: 0.101616, mae: 0.309271, mean_q: 3.875139
 42330/100000: episode: 948, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 195.714, mean reward: 1.957 [1.473, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.098], loss: 0.103901, mae: 0.310054, mean_q: 3.867117
 42430/100000: episode: 949, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 195.241, mean reward: 1.952 [1.467, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.593, 10.218], loss: 0.091311, mae: 0.308617, mean_q: 3.848507
 42530/100000: episode: 950, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 191.653, mean reward: 1.917 [1.459, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.264, 10.098], loss: 0.106123, mae: 0.310686, mean_q: 3.885268
 42630/100000: episode: 951, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 236.730, mean reward: 2.367 [1.530, 8.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.100819, mae: 0.307015, mean_q: 3.869091
 42730/100000: episode: 952, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.709, mean reward: 1.967 [1.447, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.877, 10.098], loss: 0.101010, mae: 0.305071, mean_q: 3.880904
 42830/100000: episode: 953, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.891, mean reward: 1.879 [1.463, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.330, 10.280], loss: 0.094329, mae: 0.306164, mean_q: 3.885642
 42930/100000: episode: 954, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 188.752, mean reward: 1.888 [1.449, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.507, 10.098], loss: 0.093092, mae: 0.302226, mean_q: 3.874526
 43030/100000: episode: 955, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 189.041, mean reward: 1.890 [1.454, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.592, 10.098], loss: 0.111887, mae: 0.311030, mean_q: 3.878463
 43130/100000: episode: 956, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 185.950, mean reward: 1.859 [1.450, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.330, 10.098], loss: 0.094105, mae: 0.301004, mean_q: 3.872622
 43230/100000: episode: 957, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 188.472, mean reward: 1.885 [1.449, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.493, 10.282], loss: 0.083282, mae: 0.294750, mean_q: 3.851509
 43330/100000: episode: 958, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 193.730, mean reward: 1.937 [1.441, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.224, 10.098], loss: 0.100116, mae: 0.299366, mean_q: 3.864885
 43430/100000: episode: 959, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 186.418, mean reward: 1.864 [1.506, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.936, 10.231], loss: 0.098867, mae: 0.306714, mean_q: 3.869515
 43530/100000: episode: 960, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 220.141, mean reward: 2.201 [1.432, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.468, 10.098], loss: 0.098753, mae: 0.309340, mean_q: 3.879167
 43630/100000: episode: 961, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 197.413, mean reward: 1.974 [1.439, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.358, 10.266], loss: 0.101198, mae: 0.304274, mean_q: 3.858900
 43730/100000: episode: 962, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 204.170, mean reward: 2.042 [1.455, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.441, 10.098], loss: 0.098500, mae: 0.308010, mean_q: 3.864143
 43830/100000: episode: 963, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 186.275, mean reward: 1.863 [1.463, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.246], loss: 0.084456, mae: 0.293493, mean_q: 3.854216
 43930/100000: episode: 964, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 202.646, mean reward: 2.026 [1.461, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.168], loss: 0.109757, mae: 0.310880, mean_q: 3.866859
 44030/100000: episode: 965, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 196.255, mean reward: 1.963 [1.483, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.509, 10.305], loss: 0.108214, mae: 0.307045, mean_q: 3.862704
 44130/100000: episode: 966, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: 182.251, mean reward: 1.823 [1.454, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.620, 10.098], loss: 0.096380, mae: 0.300280, mean_q: 3.863466
 44230/100000: episode: 967, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 205.016, mean reward: 2.050 [1.467, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.644, 10.098], loss: 0.091033, mae: 0.291283, mean_q: 3.832871
 44330/100000: episode: 968, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 186.519, mean reward: 1.865 [1.458, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.098], loss: 0.092343, mae: 0.297477, mean_q: 3.841929
 44430/100000: episode: 969, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 216.846, mean reward: 2.168 [1.523, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.358], loss: 0.080959, mae: 0.285388, mean_q: 3.837285
 44530/100000: episode: 970, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 190.945, mean reward: 1.909 [1.469, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.370, 10.195], loss: 0.086232, mae: 0.288642, mean_q: 3.834718
 44630/100000: episode: 971, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 257.547, mean reward: 2.575 [1.516, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.832, 10.098], loss: 0.092180, mae: 0.301203, mean_q: 3.837623
 44730/100000: episode: 972, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 207.365, mean reward: 2.074 [1.460, 4.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.395, 10.098], loss: 0.089309, mae: 0.299678, mean_q: 3.879182
 44830/100000: episode: 973, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 230.522, mean reward: 2.305 [1.471, 6.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.437, 10.098], loss: 0.086994, mae: 0.293704, mean_q: 3.886854
 44930/100000: episode: 974, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.743, mean reward: 1.977 [1.466, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.267, 10.098], loss: 0.090336, mae: 0.297918, mean_q: 3.881242
 45030/100000: episode: 975, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.235, mean reward: 1.882 [1.457, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.317, 10.098], loss: 0.101035, mae: 0.305791, mean_q: 3.893345
 45130/100000: episode: 976, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 198.692, mean reward: 1.987 [1.459, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.244, 10.422], loss: 0.107099, mae: 0.310087, mean_q: 3.906698
 45230/100000: episode: 977, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 178.082, mean reward: 1.781 [1.463, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.352, 10.141], loss: 0.090050, mae: 0.301282, mean_q: 3.911809
 45330/100000: episode: 978, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.108, mean reward: 1.951 [1.460, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.699, 10.269], loss: 0.095552, mae: 0.300885, mean_q: 3.891986
[Info] 1-TH LEVEL FOUND: 6.09113073348999, Considering 10/90 traces
 45430/100000: episode: 979, duration: 4.543s, episode steps: 100, steps per second: 22, episode reward: 181.612, mean reward: 1.816 [1.460, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.153, 10.160], loss: 0.097139, mae: 0.305916, mean_q: 3.892654
 45443/100000: episode: 980, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 39.643, mean reward: 3.049 [2.469, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.963, 10.100], loss: 0.074945, mae: 0.275983, mean_q: 3.854159
 45470/100000: episode: 981, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 93.237, mean reward: 3.453 [1.919, 7.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.567, 10.100], loss: 0.100467, mae: 0.293436, mean_q: 3.901953
 45479/100000: episode: 982, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 29.260, mean reward: 3.251 [2.728, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.175, 10.100], loss: 0.060756, mae: 0.266722, mean_q: 3.911345
 45486/100000: episode: 983, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 19.615, mean reward: 2.802 [2.538, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.394, 10.465], loss: 0.087582, mae: 0.283189, mean_q: 3.883817
 45513/100000: episode: 984, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 175.788, mean reward: 6.511 [2.951, 57.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.432, 10.100], loss: 0.096569, mae: 0.315214, mean_q: 3.949137
 45540/100000: episode: 985, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 88.489, mean reward: 3.277 [2.422, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.167, 10.100], loss: 0.116201, mae: 0.307446, mean_q: 3.928187
 45549/100000: episode: 986, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 38.110, mean reward: 4.234 [3.319, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.332, 10.100], loss: 0.092806, mae: 0.291719, mean_q: 3.883008
 45559/100000: episode: 987, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 38.242, mean reward: 3.824 [3.416, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.323, 10.100], loss: 0.067891, mae: 0.270530, mean_q: 3.914228
 45568/100000: episode: 988, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 29.025, mean reward: 3.225 [2.597, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.183, 10.100], loss: 0.099259, mae: 0.315095, mean_q: 3.963166
 45593/100000: episode: 989, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 59.472, mean reward: 2.379 [1.998, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.399], loss: 0.100651, mae: 0.305322, mean_q: 3.927577
 45600/100000: episode: 990, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 20.620, mean reward: 2.946 [2.750, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.407], loss: 0.089658, mae: 0.288731, mean_q: 3.902154
 45625/100000: episode: 991, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 88.766, mean reward: 3.551 [1.886, 5.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.153, 10.288], loss: 0.074955, mae: 0.286787, mean_q: 3.935745
 45634/100000: episode: 992, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 30.627, mean reward: 3.403 [2.551, 6.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.039, 10.100], loss: 0.173433, mae: 0.334890, mean_q: 4.001640
 45641/100000: episode: 993, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 18.713, mean reward: 2.673 [2.384, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.467], loss: 0.117931, mae: 0.343121, mean_q: 3.998553
 45650/100000: episode: 994, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 41.446, mean reward: 4.605 [3.513, 7.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.377, 10.100], loss: 0.186917, mae: 0.351991, mean_q: 4.074729
 45665/100000: episode: 995, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 56.085, mean reward: 3.739 [2.647, 5.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.422, 10.100], loss: 0.120747, mae: 0.330253, mean_q: 3.992195
 45674/100000: episode: 996, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 30.780, mean reward: 3.420 [2.541, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.001, 10.100], loss: 0.086806, mae: 0.293969, mean_q: 3.965764
 45687/100000: episode: 997, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 44.761, mean reward: 3.443 [2.703, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.442, 10.100], loss: 0.079562, mae: 0.303429, mean_q: 4.016067
 45697/100000: episode: 998, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 29.780, mean reward: 2.978 [2.348, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.289, 10.100], loss: 0.159754, mae: 0.367296, mean_q: 4.016083
 45722/100000: episode: 999, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 51.812, mean reward: 2.072 [1.620, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.896, 10.267], loss: 0.142507, mae: 0.321389, mean_q: 4.008480
 45813/100000: episode: 1000, duration: 0.462s, episode steps: 91, steps per second: 197, episode reward: 175.741, mean reward: 1.931 [1.483, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.184, 10.127], loss: 0.660585, mae: 0.378134, mean_q: 4.049452
 45904/100000: episode: 1001, duration: 0.448s, episode steps: 91, steps per second: 203, episode reward: 195.631, mean reward: 2.150 [1.455, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-1.487, 10.100], loss: 0.622713, mae: 0.356915, mean_q: 4.045010
 45913/100000: episode: 1002, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 27.253, mean reward: 3.028 [2.468, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.469, 10.100], loss: 0.158306, mae: 0.368857, mean_q: 4.033915
 45922/100000: episode: 1003, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 27.339, mean reward: 3.038 [2.309, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.456, 10.100], loss: 0.140805, mae: 0.348334, mean_q: 4.122800
 45931/100000: episode: 1004, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 29.514, mean reward: 3.279 [2.830, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.285, 10.100], loss: 0.104865, mae: 0.321983, mean_q: 4.035773
 45940/100000: episode: 1005, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 21.996, mean reward: 2.444 [2.254, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.614, 10.100], loss: 0.117404, mae: 0.338572, mean_q: 4.084424
 45950/100000: episode: 1006, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 26.761, mean reward: 2.676 [2.176, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.246, 10.100], loss: 4.713043, mae: 0.572574, mean_q: 4.178351
 45977/100000: episode: 1007, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 78.876, mean reward: 2.921 [2.158, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.297, 10.100], loss: 1.849681, mae: 0.449824, mean_q: 4.084251
 45987/100000: episode: 1008, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 28.997, mean reward: 2.900 [2.602, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.399, 10.100], loss: 0.126893, mae: 0.387040, mean_q: 4.118502
 46012/100000: episode: 1009, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 59.288, mean reward: 2.372 [2.033, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.303, 10.352], loss: 0.120109, mae: 0.343578, mean_q: 4.013882
 46103/100000: episode: 1010, duration: 0.449s, episode steps: 91, steps per second: 203, episode reward: 173.459, mean reward: 1.906 [1.460, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-0.263, 10.133], loss: 0.665844, mae: 0.401447, mean_q: 4.127485
 46110/100000: episode: 1011, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 17.887, mean reward: 2.555 [2.302, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.537, 10.405], loss: 0.113440, mae: 0.315555, mean_q: 4.085486
 46135/100000: episode: 1012, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 73.059, mean reward: 2.922 [2.240, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.430, 10.491], loss: 0.124665, mae: 0.335177, mean_q: 4.047737
 46162/100000: episode: 1013, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 113.250, mean reward: 4.194 [2.867, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.213, 10.100], loss: 0.163351, mae: 0.338111, mean_q: 4.120031
 46175/100000: episode: 1014, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 42.100, mean reward: 3.238 [2.715, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.553, 10.100], loss: 0.152725, mae: 0.356391, mean_q: 4.169258
 46266/100000: episode: 1015, duration: 0.448s, episode steps: 91, steps per second: 203, episode reward: 159.597, mean reward: 1.754 [1.439, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-1.111, 10.100], loss: 0.148382, mae: 0.352810, mean_q: 4.135021
 46291/100000: episode: 1016, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 58.079, mean reward: 2.323 [1.824, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.411, 10.259], loss: 0.111406, mae: 0.328015, mean_q: 4.068613
 46300/100000: episode: 1017, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 24.948, mean reward: 2.772 [2.271, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.239, 10.100], loss: 0.094714, mae: 0.328558, mean_q: 4.111910
 46309/100000: episode: 1018, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 36.224, mean reward: 4.025 [2.935, 5.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.483, 10.100], loss: 0.115662, mae: 0.322528, mean_q: 4.082480
 46324/100000: episode: 1019, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 47.433, mean reward: 3.162 [2.275, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.314, 10.100], loss: 0.136774, mae: 0.343254, mean_q: 4.145923
 46351/100000: episode: 1020, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 83.699, mean reward: 3.100 [2.157, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.874, 10.100], loss: 0.087189, mae: 0.294313, mean_q: 4.054662
 46364/100000: episode: 1021, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 47.426, mean reward: 3.648 [2.608, 6.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.466, 10.100], loss: 0.123408, mae: 0.341807, mean_q: 4.061483
 46373/100000: episode: 1022, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 24.747, mean reward: 2.750 [2.171, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.287, 10.100], loss: 0.117754, mae: 0.363815, mean_q: 4.202793
 46398/100000: episode: 1023, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 53.077, mean reward: 2.123 [1.640, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.454, 10.244], loss: 0.122681, mae: 0.328682, mean_q: 4.105551
 46411/100000: episode: 1024, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 41.130, mean reward: 3.164 [2.413, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.320, 10.100], loss: 0.106696, mae: 0.326806, mean_q: 4.160587
 46420/100000: episode: 1025, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 24.722, mean reward: 2.747 [2.191, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.251, 10.100], loss: 0.090705, mae: 0.304337, mean_q: 4.142440
 46445/100000: episode: 1026, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 62.834, mean reward: 2.513 [1.966, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.323], loss: 0.148931, mae: 0.354618, mean_q: 4.153753
 46458/100000: episode: 1027, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 37.216, mean reward: 2.863 [2.402, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.280, 10.100], loss: 0.223850, mae: 0.383117, mean_q: 4.205111
 46467/100000: episode: 1028, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 33.512, mean reward: 3.724 [2.390, 8.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.251, 10.100], loss: 0.105904, mae: 0.330920, mean_q: 4.147442
 46477/100000: episode: 1029, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 31.288, mean reward: 3.129 [2.526, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.265, 10.100], loss: 0.144094, mae: 0.367092, mean_q: 4.205539
 46487/100000: episode: 1030, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 39.680, mean reward: 3.968 [3.059, 6.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.216, 10.100], loss: 0.129871, mae: 0.357661, mean_q: 4.218657
 46578/100000: episode: 1031, duration: 0.452s, episode steps: 91, steps per second: 201, episode reward: 174.606, mean reward: 1.919 [1.485, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.879, 10.100], loss: 1.148568, mae: 0.413119, mean_q: 4.211247
 46585/100000: episode: 1032, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 21.933, mean reward: 3.133 [2.524, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.041, 10.502], loss: 0.114744, mae: 0.347821, mean_q: 4.255857
 46595/100000: episode: 1033, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 36.144, mean reward: 3.614 [2.885, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.382, 10.100], loss: 4.673831, mae: 0.551463, mean_q: 4.147508
 46604/100000: episode: 1034, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 26.604, mean reward: 2.956 [2.394, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.310, 10.100], loss: 0.245135, mae: 0.498360, mean_q: 4.439310
 46617/100000: episode: 1035, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 37.411, mean reward: 2.878 [2.475, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.227, 10.100], loss: 0.206459, mae: 0.413554, mean_q: 4.177346
 46626/100000: episode: 1036, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 24.627, mean reward: 2.736 [2.078, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.669, 10.100], loss: 0.141390, mae: 0.340483, mean_q: 4.049751
 46651/100000: episode: 1037, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 68.477, mean reward: 2.739 [2.160, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.443], loss: 0.148994, mae: 0.350070, mean_q: 4.171284
 46658/100000: episode: 1038, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 16.361, mean reward: 2.337 [2.159, 2.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.391], loss: 0.284673, mae: 0.433067, mean_q: 4.265809
 46667/100000: episode: 1039, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 23.186, mean reward: 2.576 [1.995, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.100], loss: 0.149932, mae: 0.356603, mean_q: 4.136867
 46674/100000: episode: 1040, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 20.307, mean reward: 2.901 [2.678, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.973, 10.501], loss: 0.132112, mae: 0.363955, mean_q: 4.201676
 46689/100000: episode: 1041, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 52.559, mean reward: 3.504 [2.356, 5.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.448, 10.100], loss: 3.180455, mae: 0.557023, mean_q: 4.355066
 46698/100000: episode: 1042, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 24.130, mean reward: 2.681 [1.911, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.186, 10.100], loss: 0.144188, mae: 0.364658, mean_q: 4.134603
 46707/100000: episode: 1043, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 27.368, mean reward: 3.041 [2.560, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.542, 10.100], loss: 0.141704, mae: 0.349748, mean_q: 4.234324
 46720/100000: episode: 1044, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 53.905, mean reward: 4.147 [2.921, 6.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.281, 10.100], loss: 0.172058, mae: 0.356138, mean_q: 4.196764
 46727/100000: episode: 1045, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 21.297, mean reward: 3.042 [2.619, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.397, 10.496], loss: 0.154090, mae: 0.355083, mean_q: 4.303871
 46737/100000: episode: 1046, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 47.228, mean reward: 4.723 [3.087, 7.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.449, 10.100], loss: 0.202269, mae: 0.373264, mean_q: 4.199695
 46762/100000: episode: 1047, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 77.129, mean reward: 3.085 [2.176, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.442], loss: 0.125906, mae: 0.332842, mean_q: 4.216237
 46772/100000: episode: 1048, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 31.678, mean reward: 3.168 [2.278, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.480, 10.100], loss: 0.140606, mae: 0.376321, mean_q: 4.287354
 46782/100000: episode: 1049, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 35.049, mean reward: 3.505 [2.953, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.358, 10.100], loss: 0.144573, mae: 0.357629, mean_q: 4.277773
 46873/100000: episode: 1050, duration: 0.460s, episode steps: 91, steps per second: 198, episode reward: 182.037, mean reward: 2.000 [1.442, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.688, 10.120], loss: 0.134745, mae: 0.340936, mean_q: 4.233247
 46883/100000: episode: 1051, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 31.218, mean reward: 3.122 [2.456, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.160, 10.100], loss: 0.190960, mae: 0.396376, mean_q: 4.293531
 46893/100000: episode: 1052, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 29.747, mean reward: 2.975 [2.348, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.250, 10.100], loss: 0.106480, mae: 0.320287, mean_q: 4.268029
 46906/100000: episode: 1053, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 32.571, mean reward: 2.505 [1.969, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.728, 10.100], loss: 0.174620, mae: 0.389265, mean_q: 4.395474
 46931/100000: episode: 1054, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 71.206, mean reward: 2.848 [2.150, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.065, 10.396], loss: 0.126221, mae: 0.347743, mean_q: 4.333857
 46956/100000: episode: 1055, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 58.222, mean reward: 2.329 [1.611, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.041, 10.269], loss: 0.152498, mae: 0.350075, mean_q: 4.335440
 46981/100000: episode: 1056, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 101.012, mean reward: 4.040 [2.028, 7.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.856, 10.593], loss: 0.143956, mae: 0.345631, mean_q: 4.254187
 46991/100000: episode: 1057, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 24.898, mean reward: 2.490 [2.103, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.184, 10.100], loss: 0.127174, mae: 0.337502, mean_q: 4.200233
 47001/100000: episode: 1058, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 37.675, mean reward: 3.767 [2.995, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.564, 10.100], loss: 0.179437, mae: 0.379464, mean_q: 4.273843
 47016/100000: episode: 1059, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 41.571, mean reward: 2.771 [2.272, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.332, 10.100], loss: 0.201883, mae: 0.379550, mean_q: 4.387388
 47107/100000: episode: 1060, duration: 0.452s, episode steps: 91, steps per second: 201, episode reward: 203.013, mean reward: 2.231 [1.450, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-1.698, 10.100], loss: 0.161944, mae: 0.366926, mean_q: 4.312865
 47116/100000: episode: 1061, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 31.768, mean reward: 3.530 [2.927, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.217, 10.100], loss: 0.194230, mae: 0.413986, mean_q: 4.471383
 47131/100000: episode: 1062, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 42.199, mean reward: 2.813 [2.321, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.538, 10.100], loss: 0.159235, mae: 0.387913, mean_q: 4.343807
 47141/100000: episode: 1063, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 29.301, mean reward: 2.930 [2.285, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.305, 10.100], loss: 0.183778, mae: 0.351687, mean_q: 4.277192
 47232/100000: episode: 1064, duration: 0.470s, episode steps: 91, steps per second: 194, episode reward: 192.078, mean reward: 2.111 [1.487, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-1.186, 10.107], loss: 0.147754, mae: 0.362953, mean_q: 4.363770
 47241/100000: episode: 1065, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 25.026, mean reward: 2.781 [2.275, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.396, 10.100], loss: 0.117625, mae: 0.353259, mean_q: 4.443163
 47251/100000: episode: 1066, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 36.839, mean reward: 3.684 [3.019, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.366, 10.100], loss: 0.157024, mae: 0.387180, mean_q: 4.427558
 47260/100000: episode: 1067, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 24.398, mean reward: 2.711 [2.479, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.260, 10.100], loss: 0.218465, mae: 0.399587, mean_q: 4.438963
 47270/100000: episode: 1068, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 32.929, mean reward: 3.293 [2.761, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.672, 10.100], loss: 0.133762, mae: 0.366131, mean_q: 4.483052
[Info] 2-TH LEVEL FOUND: 8.006656646728516, Considering 10/90 traces
 47280/100000: episode: 1069, duration: 4.030s, episode steps: 10, steps per second: 2, episode reward: 35.879, mean reward: 3.588 [2.966, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.437, 10.100], loss: 0.123772, mae: 0.354637, mean_q: 4.362812
 47289/100000: episode: 1070, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 28.626, mean reward: 3.181 [2.875, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.470, 10.100], loss: 0.148220, mae: 0.368334, mean_q: 4.486786
 47310/100000: episode: 1071, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 71.003, mean reward: 3.381 [2.443, 8.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.226, 10.100], loss: 0.185451, mae: 0.363470, mean_q: 4.349193
 47331/100000: episode: 1072, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 76.767, mean reward: 3.656 [2.730, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.508, 10.100], loss: 0.155563, mae: 0.375790, mean_q: 4.403188
 47337/100000: episode: 1073, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 18.541, mean reward: 3.090 [2.699, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.336, 10.100], loss: 0.161595, mae: 0.372583, mean_q: 4.389707
 47349/100000: episode: 1074, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 39.270, mean reward: 3.273 [2.653, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.456, 10.100], loss: 0.134119, mae: 0.373769, mean_q: 4.433082
 47361/100000: episode: 1075, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 52.121, mean reward: 4.343 [2.733, 6.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.408, 10.100], loss: 0.134512, mae: 0.369647, mean_q: 4.409464
 47378/100000: episode: 1076, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 152.462, mean reward: 8.968 [3.830, 21.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.057, 10.100], loss: 0.455900, mae: 0.435996, mean_q: 4.440836
 47385/100000: episode: 1077, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 30.693, mean reward: 4.385 [3.564, 5.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.580, 10.100], loss: 0.170668, mae: 0.399358, mean_q: 4.411999
 47395/100000: episode: 1078, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 32.462, mean reward: 3.246 [2.762, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.387, 10.100], loss: 0.238430, mae: 0.455699, mean_q: 4.512505
 47401/100000: episode: 1079, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 17.458, mean reward: 2.910 [2.703, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.380, 10.100], loss: 0.128430, mae: 0.330736, mean_q: 4.315302
 47407/100000: episode: 1080, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 35.148, mean reward: 5.858 [3.918, 7.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.466, 10.100], loss: 0.125757, mae: 0.364827, mean_q: 4.578119
 47413/100000: episode: 1081, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 23.658, mean reward: 3.943 [3.505, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.334, 10.100], loss: 0.183072, mae: 0.404090, mean_q: 4.443412
 47425/100000: episode: 1082, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 65.328, mean reward: 5.444 [2.985, 7.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.619, 10.100], loss: 0.167800, mae: 0.392112, mean_q: 4.481042
 47431/100000: episode: 1083, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 24.170, mean reward: 4.028 [2.666, 5.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.354, 10.100], loss: 0.268892, mae: 0.460026, mean_q: 4.562265
 47438/100000: episode: 1084, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 22.821, mean reward: 3.260 [2.735, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.611, 10.100], loss: 0.110955, mae: 0.349344, mean_q: 4.412274
 47459/100000: episode: 1085, duration: 0.098s, episode steps: 21, steps per second: 214, episode reward: 98.858, mean reward: 4.708 [2.980, 7.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.500, 10.100], loss: 0.515899, mae: 0.476242, mean_q: 4.501313
 47469/100000: episode: 1086, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 35.398, mean reward: 3.540 [2.677, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.123, 10.100], loss: 5.117315, mae: 0.674728, mean_q: 4.663258
 47475/100000: episode: 1087, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 23.759, mean reward: 3.960 [3.597, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.388, 10.100], loss: 0.228996, mae: 0.472228, mean_q: 4.597492
 47492/100000: episode: 1088, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 63.744, mean reward: 3.750 [3.217, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.469, 10.100], loss: 0.313630, mae: 0.497844, mean_q: 4.493250
 47502/100000: episode: 1089, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 31.866, mean reward: 3.187 [2.515, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.021, 10.100], loss: 0.187526, mae: 0.396277, mean_q: 4.446265
 47511/100000: episode: 1090, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 30.486, mean reward: 3.387 [3.081, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.413, 10.100], loss: 0.306756, mae: 0.433724, mean_q: 4.501375
 47521/100000: episode: 1091, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 27.002, mean reward: 2.700 [2.406, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.360, 10.100], loss: 0.755915, mae: 0.525773, mean_q: 4.650214
 47538/100000: episode: 1092, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 52.711, mean reward: 3.101 [2.321, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.266, 10.100], loss: 0.262491, mae: 0.432734, mean_q: 4.529542
 47550/100000: episode: 1093, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 50.176, mean reward: 4.181 [3.224, 6.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.050, 10.100], loss: 0.188491, mae: 0.419309, mean_q: 4.588227
 47557/100000: episode: 1094, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 24.768, mean reward: 3.538 [3.105, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.342, 10.100], loss: 6.746490, mae: 0.730846, mean_q: 4.748113
 47564/100000: episode: 1095, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 24.506, mean reward: 3.501 [2.483, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.326, 10.100], loss: 0.154874, mae: 0.420459, mean_q: 4.563982
 47574/100000: episode: 1096, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 30.510, mean reward: 3.051 [2.223, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.366, 10.100], loss: 0.186238, mae: 0.439132, mean_q: 4.550236
 47591/100000: episode: 1097, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 108.080, mean reward: 6.358 [4.291, 13.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.665, 10.100], loss: 0.369523, mae: 0.411465, mean_q: 4.547651
 47600/100000: episode: 1098, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 53.111, mean reward: 5.901 [3.550, 8.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.529, 10.100], loss: 0.178964, mae: 0.409370, mean_q: 4.481904
 47607/100000: episode: 1099, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 33.517, mean reward: 4.788 [2.926, 9.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.567, 10.100], loss: 0.126625, mae: 0.367770, mean_q: 4.555919
 47613/100000: episode: 1100, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 23.651, mean reward: 3.942 [3.168, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.428, 10.100], loss: 0.407183, mae: 0.449626, mean_q: 4.769616
 47620/100000: episode: 1101, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 41.926, mean reward: 5.989 [5.166, 7.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.507, 10.100], loss: 0.272017, mae: 0.392440, mean_q: 4.425810
 47626/100000: episode: 1102, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 30.364, mean reward: 5.061 [4.394, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.896, 10.100], loss: 0.724468, mae: 0.408121, mean_q: 4.598821
 47643/100000: episode: 1103, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 74.268, mean reward: 4.369 [2.901, 6.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.352, 10.100], loss: 0.278987, mae: 0.454253, mean_q: 4.748852
 47650/100000: episode: 1104, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 28.527, mean reward: 4.075 [3.252, 5.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.328, 10.100], loss: 0.180762, mae: 0.425286, mean_q: 4.593536
 47671/100000: episode: 1105, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 103.788, mean reward: 4.942 [3.450, 6.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.618, 10.100], loss: 0.322995, mae: 0.490643, mean_q: 4.703893
 47680/100000: episode: 1106, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 36.289, mean reward: 4.032 [3.190, 5.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.412, 10.100], loss: 0.231170, mae: 0.434749, mean_q: 4.710351
 47692/100000: episode: 1107, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 52.136, mean reward: 4.345 [2.807, 5.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.337, 10.100], loss: 0.282596, mae: 0.437638, mean_q: 4.757316
 47698/100000: episode: 1108, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 35.064, mean reward: 5.844 [4.199, 7.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.356, 10.100], loss: 0.974625, mae: 0.509722, mean_q: 4.801306
 47707/100000: episode: 1109, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 41.612, mean reward: 4.624 [3.416, 5.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.312, 10.100], loss: 0.394804, mae: 0.477370, mean_q: 4.752082
 47716/100000: episode: 1110, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 37.697, mean reward: 4.189 [3.500, 5.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.358, 10.100], loss: 0.224657, mae: 0.444315, mean_q: 4.739420
 47728/100000: episode: 1111, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 38.199, mean reward: 3.183 [2.507, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.350, 10.100], loss: 0.227246, mae: 0.420603, mean_q: 4.707604
 47745/100000: episode: 1112, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 60.043, mean reward: 3.532 [2.857, 5.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.356, 10.100], loss: 0.222974, mae: 0.430559, mean_q: 4.886491
 47762/100000: episode: 1113, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 72.414, mean reward: 4.260 [2.727, 7.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.516, 10.100], loss: 2.848284, mae: 0.535692, mean_q: 4.642639
 47778/100000: episode: 1114, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 78.594, mean reward: 4.912 [2.723, 9.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.091, 10.533], loss: 0.359064, mae: 0.541399, mean_q: 4.861032
 47785/100000: episode: 1115, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 25.322, mean reward: 3.617 [2.974, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.526, 10.100], loss: 0.193620, mae: 0.429938, mean_q: 4.645548
 47791/100000: episode: 1116, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 30.669, mean reward: 5.111 [3.723, 7.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.292, 10.100], loss: 0.379088, mae: 0.520722, mean_q: 4.833017
 47800/100000: episode: 1117, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 43.452, mean reward: 4.828 [3.351, 6.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.513, 10.100], loss: 0.644092, mae: 0.493030, mean_q: 4.892950
 47812/100000: episode: 1118, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 40.109, mean reward: 3.342 [2.356, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.774, 10.100], loss: 0.406912, mae: 0.522359, mean_q: 4.867971
 47833/100000: episode: 1119, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 88.986, mean reward: 4.237 [3.069, 6.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.027, 10.100], loss: 0.166833, mae: 0.403309, mean_q: 4.789558
 47842/100000: episode: 1120, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 37.609, mean reward: 4.179 [3.252, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.421, 10.100], loss: 0.160077, mae: 0.389387, mean_q: 4.725684
[Info] FALSIFICATION!
 47851/100000: episode: 1121, duration: 0.212s, episode steps: 9, steps per second: 42, episode reward: 1037.811, mean reward: 115.312 [3.421, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.619, 10.082], loss: 0.276679, mae: 0.486196, mean_q: 4.843894
 47863/100000: episode: 1122, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 49.003, mean reward: 4.084 [2.769, 7.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.467, 10.100], loss: 1279.964478, mae: 3.038132, mean_q: 4.841125
 47869/100000: episode: 1123, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 37.607, mean reward: 6.268 [4.850, 7.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.384, 10.100], loss: 3.944963, mae: 2.168688, mean_q: 7.200037
 47881/100000: episode: 1124, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 258.077, mean reward: 21.506 [2.618, 68.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.541, 10.100], loss: 2.203134, mae: 1.402940, mean_q: 5.334144
 47893/100000: episode: 1125, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 40.991, mean reward: 3.416 [2.624, 5.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.444, 10.100], loss: 1290.251709, mae: 4.292050, mean_q: 5.891075
 47903/100000: episode: 1126, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 29.918, mean reward: 2.992 [2.373, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.468, 10.100], loss: 4.016380, mae: 2.238530, mean_q: 7.248629
 47915/100000: episode: 1127, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 43.940, mean reward: 3.662 [2.966, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.406, 10.100], loss: 1281.001587, mae: 4.361650, mean_q: 6.363496
 47936/100000: episode: 1128, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 193.332, mean reward: 9.206 [3.180, 26.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.747, 10.100], loss: 2.204836, mae: 1.402279, mean_q: 5.851408
 47953/100000: episode: 1129, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 52.503, mean reward: 3.088 [2.523, 5.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.246, 10.100], loss: 4.679946, mae: 1.001456, mean_q: 4.821579
 47965/100000: episode: 1130, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 58.005, mean reward: 4.834 [2.925, 8.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.397, 10.100], loss: 0.908231, mae: 0.704346, mean_q: 5.329849
 47982/100000: episode: 1131, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 72.626, mean reward: 4.272 [3.453, 5.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.478, 10.100], loss: 4.181172, mae: 0.755434, mean_q: 5.281215
 47999/100000: episode: 1132, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 73.397, mean reward: 4.317 [3.161, 7.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.345, 10.100], loss: 0.755160, mae: 0.609412, mean_q: 5.238271
 48006/100000: episode: 1133, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 24.591, mean reward: 3.513 [2.919, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.426, 10.100], loss: 1.198301, mae: 0.622835, mean_q: 5.324670
 48023/100000: episode: 1134, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 100.681, mean reward: 5.922 [3.708, 8.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.409, 10.100], loss: 0.663209, mae: 0.575333, mean_q: 5.345783
 48030/100000: episode: 1135, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 21.659, mean reward: 3.094 [2.656, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.371, 10.100], loss: 1.248281, mae: 0.732220, mean_q: 5.173982
 48039/100000: episode: 1136, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 34.946, mean reward: 3.883 [2.978, 8.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.424, 10.100], loss: 0.487411, mae: 0.585583, mean_q: 5.125072
 48051/100000: episode: 1137, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 57.591, mean reward: 4.799 [2.781, 7.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.437, 10.100], loss: 0.434694, mae: 0.533233, mean_q: 5.132550
 48068/100000: episode: 1138, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 129.162, mean reward: 7.598 [3.183, 42.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.395, 10.100], loss: 0.686065, mae: 0.572030, mean_q: 5.031794
 48080/100000: episode: 1139, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 83.031, mean reward: 6.919 [3.209, 19.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.494, 10.100], loss: 0.754789, mae: 0.675876, mean_q: 5.123873
 48086/100000: episode: 1140, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 20.912, mean reward: 3.485 [2.856, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.469, 10.100], loss: 1.119959, mae: 0.592568, mean_q: 5.088840
 48092/100000: episode: 1141, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 19.887, mean reward: 3.315 [2.814, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.436, 10.100], loss: 3.314724, mae: 0.740962, mean_q: 5.317930
 48104/100000: episode: 1142, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 52.497, mean reward: 4.375 [3.501, 5.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.402, 10.100], loss: 0.948719, mae: 0.718143, mean_q: 5.327164
[Info] FALSIFICATION!
 48116/100000: episode: 1143, duration: 0.224s, episode steps: 12, steps per second: 54, episode reward: 1123.088, mean reward: 93.591 [3.249, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.013, 10.694], loss: 1.137199, mae: 0.723970, mean_q: 5.274293
 48128/100000: episode: 1144, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 33.805, mean reward: 2.817 [2.543, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.391, 10.100], loss: 0.513258, mae: 0.627168, mean_q: 5.373127
 48137/100000: episode: 1145, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 42.959, mean reward: 4.773 [3.233, 6.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.351, 10.100], loss: 0.385319, mae: 0.526553, mean_q: 5.177429
 48149/100000: episode: 1146, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 34.792, mean reward: 2.899 [2.397, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.505, 10.100], loss: 3.905451, mae: 0.789919, mean_q: 5.298759
 48161/100000: episode: 1147, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 43.799, mean reward: 3.650 [2.851, 4.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.342, 10.100], loss: 0.617239, mae: 0.630001, mean_q: 5.277730
 48170/100000: episode: 1148, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 45.715, mean reward: 5.079 [3.625, 7.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.430, 10.100], loss: 7.577360, mae: 0.832387, mean_q: 5.161723
 48177/100000: episode: 1149, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 28.070, mean reward: 4.010 [3.163, 5.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.495, 10.100], loss: 5.637460, mae: 0.944765, mean_q: 5.555027
 48187/100000: episode: 1150, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 37.770, mean reward: 3.777 [2.817, 4.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.376, 10.100], loss: 5.394665, mae: 1.054158, mean_q: 5.696148
 48208/100000: episode: 1151, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 318.645, mean reward: 15.174 [2.889, 213.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.364, 10.100], loss: 2.562547, mae: 0.837288, mean_q: 5.461054
 48220/100000: episode: 1152, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 46.780, mean reward: 3.898 [2.994, 5.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.383, 10.100], loss: 2.888321, mae: 0.850102, mean_q: 5.173497
 48237/100000: episode: 1153, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 98.638, mean reward: 5.802 [3.541, 13.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.183, 10.100], loss: 0.592775, mae: 0.644829, mean_q: 5.251829
 48249/100000: episode: 1154, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 46.862, mean reward: 3.905 [2.853, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.186, 10.100], loss: 1.887192, mae: 0.735043, mean_q: 5.185162
 48261/100000: episode: 1155, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 47.118, mean reward: 3.926 [2.938, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.777, 10.100], loss: 1.604634, mae: 0.691918, mean_q: 5.365622
 48271/100000: episode: 1156, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 26.343, mean reward: 2.634 [2.299, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.506, 10.100], loss: 2.363650, mae: 0.735232, mean_q: 5.339541
 48288/100000: episode: 1157, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 59.332, mean reward: 3.490 [2.823, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.898, 10.100], loss: 904.865784, mae: 3.771431, mean_q: 6.960168
 48294/100000: episode: 1158, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 21.897, mean reward: 3.649 [3.104, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.486, 10.100], loss: 11.562706, mae: 1.865769, mean_q: 6.748789
[Info] Complete ISplit Iteration
[Info] Levels: [6.0911307, 8.006657, 11.898222]
[Info] Cond. Prob: [0.1, 0.1, 0.15]
[Info] Error Prob: 0.0015000000000000002

 48300/100000: episode: 1159, duration: 4.284s, episode steps: 6, steps per second: 1, episode reward: 21.703, mean reward: 3.617 [3.227, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.510, 10.100], loss: 11.406962, mae: 1.573636, mean_q: 5.870607
 48400/100000: episode: 1160, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 200.311, mean reward: 2.003 [1.470, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.547, 10.465], loss: 154.326614, mae: 1.494028, mean_q: 5.661329
 48500/100000: episode: 1161, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 182.751, mean reward: 1.828 [1.474, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.704, 10.098], loss: 167.752426, mae: 1.571800, mean_q: 5.863495
 48600/100000: episode: 1162, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 196.032, mean reward: 1.960 [1.456, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.098], loss: 305.089600, mae: 1.792608, mean_q: 5.652627
 48700/100000: episode: 1163, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 200.315, mean reward: 2.003 [1.443, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.425, 10.146], loss: 21.443264, mae: 1.872705, mean_q: 6.188612
 48800/100000: episode: 1164, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 196.428, mean reward: 1.964 [1.464, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.293, 10.307], loss: 9.901487, mae: 1.050242, mean_q: 5.818428
 48900/100000: episode: 1165, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 192.008, mean reward: 1.920 [1.477, 6.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.580, 10.141], loss: 169.235199, mae: 1.468793, mean_q: 5.928824
 49000/100000: episode: 1166, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.643, mean reward: 1.956 [1.445, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.766, 10.346], loss: 157.264679, mae: 1.426495, mean_q: 5.989516
 49100/100000: episode: 1167, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 223.256, mean reward: 2.233 [1.456, 5.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.267, 10.374], loss: 315.458893, mae: 2.083307, mean_q: 6.082009
 49200/100000: episode: 1168, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 220.881, mean reward: 2.209 [1.527, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.392, 10.098], loss: 155.882446, mae: 1.585463, mean_q: 6.270414
 49300/100000: episode: 1169, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.386, mean reward: 1.934 [1.463, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.339, 10.143], loss: 160.352142, mae: 1.669950, mean_q: 6.342984
 49400/100000: episode: 1170, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 186.556, mean reward: 1.866 [1.443, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.098], loss: 151.883499, mae: 1.535850, mean_q: 6.044321
 49500/100000: episode: 1171, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.158, mean reward: 1.982 [1.482, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.098], loss: 3.518816, mae: 0.971765, mean_q: 5.712217
 49600/100000: episode: 1172, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.732, mean reward: 1.967 [1.455, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.654, 10.261], loss: 2.383179, mae: 0.818457, mean_q: 5.519376
 49700/100000: episode: 1173, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 192.482, mean reward: 1.925 [1.490, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.338, 10.321], loss: 165.151596, mae: 1.666086, mean_q: 6.055269
 49800/100000: episode: 1174, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 187.739, mean reward: 1.877 [1.480, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.098], loss: 149.482452, mae: 1.362189, mean_q: 5.688034
 49900/100000: episode: 1175, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.506, mean reward: 1.955 [1.452, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.857, 10.156], loss: 327.682465, mae: 2.073357, mean_q: 6.124396
 50000/100000: episode: 1176, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 205.472, mean reward: 2.055 [1.496, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.738, 10.397], loss: 152.226059, mae: 1.481620, mean_q: 6.035628
 50100/100000: episode: 1177, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.593, mean reward: 1.866 [1.446, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.313, 10.098], loss: 300.978577, mae: 2.172319, mean_q: 6.546600
 50200/100000: episode: 1178, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 191.169, mean reward: 1.912 [1.453, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.157], loss: 153.755234, mae: 1.679707, mean_q: 6.238541
 50300/100000: episode: 1179, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 183.984, mean reward: 1.840 [1.448, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.389, 10.243], loss: 301.162842, mae: 1.927215, mean_q: 6.217385
 50400/100000: episode: 1180, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 188.694, mean reward: 1.887 [1.484, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.504, 10.098], loss: 304.350861, mae: 2.167431, mean_q: 6.343338
 50500/100000: episode: 1181, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 182.583, mean reward: 1.826 [1.454, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.168], loss: 4.135993, mae: 1.136775, mean_q: 5.961264
 50600/100000: episode: 1182, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 225.075, mean reward: 2.251 [1.504, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.976, 10.098], loss: 315.132721, mae: 2.223257, mean_q: 5.800795
 50700/100000: episode: 1183, duration: 0.461s, episode steps: 100, steps per second: 217, episode reward: 210.616, mean reward: 2.106 [1.500, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.089, 10.132], loss: 159.476242, mae: 1.925747, mean_q: 6.491877
 50800/100000: episode: 1184, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.824, mean reward: 1.858 [1.445, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.331, 10.098], loss: 162.700531, mae: 1.539532, mean_q: 6.028455
 50900/100000: episode: 1185, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 237.159, mean reward: 2.372 [1.462, 18.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.397, 10.165], loss: 2.203381, mae: 0.856305, mean_q: 5.611267
 51000/100000: episode: 1186, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.084, mean reward: 1.921 [1.435, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.958, 10.104], loss: 309.823944, mae: 1.969828, mean_q: 6.111669
 51100/100000: episode: 1187, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 190.803, mean reward: 1.908 [1.468, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.467, 10.098], loss: 10.765558, mae: 1.055353, mean_q: 5.640435
 51200/100000: episode: 1188, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 194.452, mean reward: 1.945 [1.456, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.098], loss: 154.722549, mae: 1.285026, mean_q: 5.789919
 51300/100000: episode: 1189, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.814, mean reward: 1.838 [1.448, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.098], loss: 161.461487, mae: 1.333399, mean_q: 5.756850
 51400/100000: episode: 1190, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.431, mean reward: 1.894 [1.473, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.629, 10.098], loss: 161.524963, mae: 1.310155, mean_q: 5.658262
 51500/100000: episode: 1191, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 193.201, mean reward: 1.932 [1.461, 5.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.475, 10.255], loss: 161.002426, mae: 1.314372, mean_q: 5.746149
 51600/100000: episode: 1192, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.174, mean reward: 1.892 [1.435, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.257, 10.114], loss: 157.348511, mae: 1.518968, mean_q: 5.709062
 51700/100000: episode: 1193, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 196.270, mean reward: 1.963 [1.453, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.236, 10.456], loss: 8.432943, mae: 0.919290, mean_q: 5.400857
 51800/100000: episode: 1194, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 192.266, mean reward: 1.923 [1.442, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.615, 10.116], loss: 154.357773, mae: 1.301187, mean_q: 5.367517
 51900/100000: episode: 1195, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 201.645, mean reward: 2.016 [1.460, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.353, 10.241], loss: 309.944885, mae: 1.799314, mean_q: 5.440146
 52000/100000: episode: 1196, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.925, mean reward: 1.959 [1.487, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.188, 10.159], loss: 181.267563, mae: 1.817354, mean_q: 5.996437
 52100/100000: episode: 1197, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 196.702, mean reward: 1.967 [1.472, 9.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.715, 10.098], loss: 147.229538, mae: 1.395753, mean_q: 5.593977
 52200/100000: episode: 1198, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.389, mean reward: 1.904 [1.474, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.277, 10.155], loss: 311.076172, mae: 1.933833, mean_q: 5.668221
 52300/100000: episode: 1199, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: 216.262, mean reward: 2.163 [1.467, 6.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.078, 10.098], loss: 9.713791, mae: 1.154908, mean_q: 5.605350
 52400/100000: episode: 1200, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 191.721, mean reward: 1.917 [1.464, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.152], loss: 306.611084, mae: 1.576148, mean_q: 5.525145
 52500/100000: episode: 1201, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 200.633, mean reward: 2.006 [1.483, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.210], loss: 1.991543, mae: 0.804684, mean_q: 5.176185
 52600/100000: episode: 1202, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.505, mean reward: 1.805 [1.435, 2.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.635, 10.098], loss: 155.123489, mae: 1.100586, mean_q: 5.047960
 52700/100000: episode: 1203, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 174.581, mean reward: 1.746 [1.443, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.098], loss: 312.280396, mae: 1.827039, mean_q: 5.429146
 52800/100000: episode: 1204, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 198.397, mean reward: 1.984 [1.453, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.714, 10.098], loss: 1.076831, mae: 0.599150, mean_q: 4.602624
 52900/100000: episode: 1205, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 230.240, mean reward: 2.302 [1.496, 16.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.285, 10.098], loss: 153.006302, mae: 1.396359, mean_q: 4.616222
 53000/100000: episode: 1206, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 194.403, mean reward: 1.944 [1.525, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.688, 10.098], loss: 7.421659, mae: 0.600254, mean_q: 4.298842
 53100/100000: episode: 1207, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 191.055, mean reward: 1.911 [1.468, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.272, 10.098], loss: 7.163869, mae: 0.555572, mean_q: 4.158873
 53200/100000: episode: 1208, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 213.988, mean reward: 2.140 [1.492, 6.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.883, 10.147], loss: 0.404856, mae: 0.444701, mean_q: 4.025005
 53300/100000: episode: 1209, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: 213.595, mean reward: 2.136 [1.464, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.098], loss: 0.299720, mae: 0.407028, mean_q: 3.917988
 53400/100000: episode: 1210, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 175.397, mean reward: 1.754 [1.487, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.372, 10.098], loss: 0.171457, mae: 0.377833, mean_q: 3.916389
 53500/100000: episode: 1211, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 210.301, mean reward: 2.103 [1.465, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.440, 10.098], loss: 0.187507, mae: 0.367292, mean_q: 3.915914
 53600/100000: episode: 1212, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 179.587, mean reward: 1.796 [1.488, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.372, 10.115], loss: 0.307803, mae: 0.383806, mean_q: 3.960591
 53700/100000: episode: 1213, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 191.507, mean reward: 1.915 [1.496, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.382], loss: 0.155929, mae: 0.357533, mean_q: 3.932581
 53800/100000: episode: 1214, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 177.352, mean reward: 1.774 [1.431, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.590, 10.098], loss: 0.222093, mae: 0.353546, mean_q: 3.893053
 53900/100000: episode: 1215, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 196.705, mean reward: 1.967 [1.443, 8.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.513, 10.234], loss: 0.200182, mae: 0.362493, mean_q: 3.944501
 54000/100000: episode: 1216, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 191.313, mean reward: 1.913 [1.437, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.098], loss: 0.280276, mae: 0.367671, mean_q: 3.941003
 54100/100000: episode: 1217, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 189.505, mean reward: 1.895 [1.476, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.320, 10.098], loss: 0.217466, mae: 0.348745, mean_q: 3.909592
 54200/100000: episode: 1218, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.709, mean reward: 1.847 [1.439, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.371, 10.098], loss: 0.170775, mae: 0.336976, mean_q: 3.907293
 54300/100000: episode: 1219, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 184.776, mean reward: 1.848 [1.436, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.109], loss: 0.184421, mae: 0.337189, mean_q: 3.873566
 54400/100000: episode: 1220, duration: 0.456s, episode steps: 100, steps per second: 219, episode reward: 192.249, mean reward: 1.922 [1.468, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.178], loss: 0.211517, mae: 0.327164, mean_q: 3.859009
 54500/100000: episode: 1221, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 200.424, mean reward: 2.004 [1.475, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.644, 10.132], loss: 0.163273, mae: 0.336218, mean_q: 3.887625
 54600/100000: episode: 1222, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 207.740, mean reward: 2.077 [1.473, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.025, 10.131], loss: 0.169956, mae: 0.338508, mean_q: 3.877735
 54700/100000: episode: 1223, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 192.281, mean reward: 1.923 [1.466, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.549, 10.098], loss: 0.204984, mae: 0.335880, mean_q: 3.879158
 54800/100000: episode: 1224, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.843, mean reward: 1.888 [1.478, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.977, 10.217], loss: 0.218679, mae: 0.337299, mean_q: 3.893018
 54900/100000: episode: 1225, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.001, mean reward: 1.960 [1.513, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.687, 10.098], loss: 0.119302, mae: 0.327240, mean_q: 3.884815
 55000/100000: episode: 1226, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 195.906, mean reward: 1.959 [1.445, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.347, 10.203], loss: 0.179164, mae: 0.323792, mean_q: 3.878285
 55100/100000: episode: 1227, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 179.865, mean reward: 1.799 [1.484, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.744, 10.314], loss: 0.199439, mae: 0.331250, mean_q: 3.875379
 55200/100000: episode: 1228, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.883, mean reward: 1.809 [1.459, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.373, 10.164], loss: 0.200554, mae: 0.329254, mean_q: 3.868449
 55300/100000: episode: 1229, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 216.956, mean reward: 2.170 [1.480, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.178, 10.235], loss: 0.229118, mae: 0.340537, mean_q: 3.889631
 55400/100000: episode: 1230, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 213.359, mean reward: 2.134 [1.481, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.629, 10.098], loss: 0.231438, mae: 0.334800, mean_q: 3.882640
 55500/100000: episode: 1231, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 246.842, mean reward: 2.468 [1.470, 7.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.311, 10.345], loss: 0.177802, mae: 0.331976, mean_q: 3.915991
 55600/100000: episode: 1232, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.429, mean reward: 1.834 [1.440, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.858, 10.292], loss: 0.237745, mae: 0.341261, mean_q: 3.915148
 55700/100000: episode: 1233, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 186.726, mean reward: 1.867 [1.464, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.086, 10.424], loss: 0.151314, mae: 0.324143, mean_q: 3.889950
 55800/100000: episode: 1234, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.223, mean reward: 1.882 [1.451, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.789, 10.098], loss: 0.238509, mae: 0.344551, mean_q: 3.907298
 55900/100000: episode: 1235, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 234.945, mean reward: 2.349 [1.526, 7.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.883, 10.307], loss: 0.163147, mae: 0.327866, mean_q: 3.893685
 56000/100000: episode: 1236, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.074, mean reward: 2.021 [1.447, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.688, 10.249], loss: 0.149563, mae: 0.319579, mean_q: 3.903614
 56100/100000: episode: 1237, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 187.307, mean reward: 1.873 [1.489, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.841, 10.098], loss: 0.128257, mae: 0.326657, mean_q: 3.903417
 56200/100000: episode: 1238, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 198.753, mean reward: 1.988 [1.461, 8.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.659, 10.161], loss: 0.161811, mae: 0.329103, mean_q: 3.922465
 56300/100000: episode: 1239, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 201.968, mean reward: 2.020 [1.517, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.283], loss: 0.176005, mae: 0.325933, mean_q: 3.890304
 56400/100000: episode: 1240, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 193.207, mean reward: 1.932 [1.489, 5.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.943, 10.098], loss: 0.155529, mae: 0.323617, mean_q: 3.906132
 56500/100000: episode: 1241, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 201.311, mean reward: 2.013 [1.469, 5.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.006, 10.098], loss: 0.129782, mae: 0.319085, mean_q: 3.897367
 56600/100000: episode: 1242, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 204.916, mean reward: 2.049 [1.462, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.984, 10.098], loss: 0.106658, mae: 0.310652, mean_q: 3.880701
 56700/100000: episode: 1243, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 190.474, mean reward: 1.905 [1.522, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.098], loss: 0.139577, mae: 0.325064, mean_q: 3.907579
 56800/100000: episode: 1244, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 191.758, mean reward: 1.918 [1.477, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.935, 10.338], loss: 0.154745, mae: 0.334948, mean_q: 3.922747
 56900/100000: episode: 1245, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 200.445, mean reward: 2.004 [1.481, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.735, 10.098], loss: 0.121482, mae: 0.322152, mean_q: 3.905571
 57000/100000: episode: 1246, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 184.016, mean reward: 1.840 [1.475, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.529, 10.098], loss: 0.109708, mae: 0.308232, mean_q: 3.890531
 57100/100000: episode: 1247, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 188.737, mean reward: 1.887 [1.471, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.098], loss: 0.115194, mae: 0.321190, mean_q: 3.895188
 57200/100000: episode: 1248, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.270, mean reward: 1.893 [1.501, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.277, 10.335], loss: 0.123044, mae: 0.322423, mean_q: 3.911984
 57300/100000: episode: 1249, duration: 0.466s, episode steps: 100, steps per second: 214, episode reward: 197.619, mean reward: 1.976 [1.505, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.512, 10.098], loss: 0.158094, mae: 0.305787, mean_q: 3.874044
 57400/100000: episode: 1250, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 207.822, mean reward: 2.078 [1.492, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.986, 10.239], loss: 0.157294, mae: 0.330228, mean_q: 3.911660
 57500/100000: episode: 1251, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 179.316, mean reward: 1.793 [1.446, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.101], loss: 0.138126, mae: 0.317540, mean_q: 3.899829
 57600/100000: episode: 1252, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 198.454, mean reward: 1.985 [1.480, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.285, 10.245], loss: 0.137752, mae: 0.312300, mean_q: 3.895535
 57700/100000: episode: 1253, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 185.014, mean reward: 1.850 [1.455, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.352, 10.098], loss: 0.117010, mae: 0.320260, mean_q: 3.893052
 57800/100000: episode: 1254, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.685, mean reward: 1.917 [1.449, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.693, 10.147], loss: 0.144379, mae: 0.307054, mean_q: 3.881525
 57900/100000: episode: 1255, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 182.910, mean reward: 1.829 [1.459, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.692, 10.098], loss: 0.124819, mae: 0.314132, mean_q: 3.870479
 58000/100000: episode: 1256, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 216.937, mean reward: 2.169 [1.470, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.297, 10.098], loss: 0.105418, mae: 0.311991, mean_q: 3.853770
 58100/100000: episode: 1257, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 182.118, mean reward: 1.821 [1.445, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.159, 10.280], loss: 0.116057, mae: 0.314815, mean_q: 3.888577
 58200/100000: episode: 1258, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.113, mean reward: 1.851 [1.457, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.913, 10.154], loss: 0.104973, mae: 0.308228, mean_q: 3.872962
[Info] 1-TH LEVEL FOUND: 5.8441267013549805, Considering 10/90 traces
 58300/100000: episode: 1259, duration: 4.481s, episode steps: 100, steps per second: 22, episode reward: 190.778, mean reward: 1.908 [1.448, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.578, 10.119], loss: 0.085464, mae: 0.295198, mean_q: 3.854239
 58326/100000: episode: 1260, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 78.108, mean reward: 3.004 [2.342, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.252, 10.100], loss: 0.086828, mae: 0.288458, mean_q: 3.857456
 58352/100000: episode: 1261, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 66.625, mean reward: 2.562 [2.106, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.424], loss: 0.095755, mae: 0.323376, mean_q: 3.910171
 58379/100000: episode: 1262, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 92.050, mean reward: 3.409 [2.439, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.302, 10.100], loss: 0.100706, mae: 0.322933, mean_q: 3.860289
 58402/100000: episode: 1263, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 63.787, mean reward: 2.773 [2.282, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.439], loss: 0.125931, mae: 0.334003, mean_q: 3.927220
 58407/100000: episode: 1264, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 15.317, mean reward: 3.063 [2.706, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.284, 10.100], loss: 0.115660, mae: 0.344806, mean_q: 3.890160
 58433/100000: episode: 1265, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 83.290, mean reward: 3.203 [2.160, 5.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.566, 10.100], loss: 0.099741, mae: 0.302342, mean_q: 3.873993
 58438/100000: episode: 1266, duration: 0.026s, episode steps: 5, steps per second: 190, episode reward: 13.498, mean reward: 2.700 [2.542, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.250, 10.100], loss: 0.129749, mae: 0.313368, mean_q: 3.892246
 58446/100000: episode: 1267, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 24.805, mean reward: 3.101 [2.274, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.474, 10.100], loss: 0.081565, mae: 0.301276, mean_q: 3.860404
 58454/100000: episode: 1268, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 23.189, mean reward: 2.899 [2.517, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.476], loss: 0.081656, mae: 0.292194, mean_q: 3.906013
 58462/100000: episode: 1269, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 23.043, mean reward: 2.880 [2.114, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.495], loss: 0.088974, mae: 0.292092, mean_q: 3.941403
 58488/100000: episode: 1270, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 60.461, mean reward: 2.325 [1.640, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.085, 10.100], loss: 0.109301, mae: 0.321567, mean_q: 3.903664
 58493/100000: episode: 1271, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 13.525, mean reward: 2.705 [2.360, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.453, 10.100], loss: 0.098918, mae: 0.328404, mean_q: 3.935935
 58505/100000: episode: 1272, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 33.525, mean reward: 2.794 [2.368, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.347, 10.505], loss: 0.085721, mae: 0.305211, mean_q: 3.902022
 58510/100000: episode: 1273, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 12.901, mean reward: 2.580 [2.118, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.339, 10.100], loss: 0.079286, mae: 0.286673, mean_q: 3.919719
 58537/100000: episode: 1274, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 90.086, mean reward: 3.337 [1.630, 8.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.140, 10.100], loss: 0.115269, mae: 0.318319, mean_q: 3.954998
 58545/100000: episode: 1275, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 20.385, mean reward: 2.548 [2.203, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.062, 10.373], loss: 0.089986, mae: 0.270763, mean_q: 3.865448
 58557/100000: episode: 1276, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 28.660, mean reward: 2.388 [2.109, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.384], loss: 0.158239, mae: 0.365161, mean_q: 3.949559
 58565/100000: episode: 1277, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 20.173, mean reward: 2.522 [2.220, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.409], loss: 0.164477, mae: 0.351239, mean_q: 3.912917
 58591/100000: episode: 1278, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 81.244, mean reward: 3.125 [2.072, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.578, 10.100], loss: 0.100641, mae: 0.318350, mean_q: 3.909503
 58618/100000: episode: 1279, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 97.924, mean reward: 3.627 [2.188, 5.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.294, 10.100], loss: 0.129260, mae: 0.332874, mean_q: 3.906542
 58644/100000: episode: 1280, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 83.616, mean reward: 3.216 [2.519, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.135, 10.100], loss: 0.108943, mae: 0.320212, mean_q: 3.947679
 58671/100000: episode: 1281, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 81.250, mean reward: 3.009 [2.229, 5.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.343, 10.100], loss: 0.129859, mae: 0.326106, mean_q: 4.030682
 58687/100000: episode: 1282, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 44.968, mean reward: 2.811 [1.832, 5.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.098, 10.388], loss: 0.105103, mae: 0.325284, mean_q: 4.009146
 58699/100000: episode: 1283, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 31.257, mean reward: 2.605 [1.737, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.440], loss: 0.106189, mae: 0.324701, mean_q: 4.008479
 58722/100000: episode: 1284, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 76.950, mean reward: 3.346 [2.378, 5.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.556], loss: 0.098047, mae: 0.307994, mean_q: 3.996484
 58730/100000: episode: 1285, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 16.733, mean reward: 2.092 [1.951, 2.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.310], loss: 0.097498, mae: 0.289906, mean_q: 3.951160
 58738/100000: episode: 1286, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 21.103, mean reward: 2.638 [2.369, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.117, 10.407], loss: 0.138092, mae: 0.350811, mean_q: 4.088412
 58746/100000: episode: 1287, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 22.140, mean reward: 2.767 [2.218, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.343], loss: 0.151333, mae: 0.329556, mean_q: 4.040513
 58754/100000: episode: 1288, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 28.416, mean reward: 3.552 [2.768, 4.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.858, 10.552], loss: 0.177055, mae: 0.343273, mean_q: 4.044694
 58762/100000: episode: 1289, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 24.508, mean reward: 3.064 [2.349, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-1.343, 10.396], loss: 0.115591, mae: 0.323407, mean_q: 4.009374
 58770/100000: episode: 1290, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 18.977, mean reward: 2.372 [2.137, 2.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.377], loss: 0.136610, mae: 0.346725, mean_q: 4.128942
 58778/100000: episode: 1291, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 19.190, mean reward: 2.399 [2.019, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.413, 10.100], loss: 0.237280, mae: 0.375626, mean_q: 4.124500
 58794/100000: episode: 1292, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 53.311, mean reward: 3.332 [2.490, 5.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.567, 10.438], loss: 0.145897, mae: 0.336573, mean_q: 3.946855
 58802/100000: episode: 1293, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 24.100, mean reward: 3.013 [2.154, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.483], loss: 0.090717, mae: 0.317323, mean_q: 4.067829
 58810/100000: episode: 1294, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 18.563, mean reward: 2.320 [2.042, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.423, 10.403], loss: 0.122201, mae: 0.336276, mean_q: 4.015748
 58836/100000: episode: 1295, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 81.374, mean reward: 3.130 [2.186, 7.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.668, 10.374], loss: 0.155850, mae: 0.364902, mean_q: 4.067204
 58844/100000: episode: 1296, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 22.584, mean reward: 2.823 [2.124, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.505], loss: 0.197361, mae: 0.385480, mean_q: 4.072195
 58852/100000: episode: 1297, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 22.351, mean reward: 2.794 [2.324, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.485], loss: 0.103786, mae: 0.332432, mean_q: 4.140177
 58878/100000: episode: 1298, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 68.939, mean reward: 2.651 [1.935, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.705, 10.100], loss: 0.159459, mae: 0.370370, mean_q: 4.044130
 58883/100000: episode: 1299, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 14.067, mean reward: 2.813 [2.633, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.322, 10.100], loss: 0.084195, mae: 0.312880, mean_q: 3.975005
 58909/100000: episode: 1300, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 91.694, mean reward: 3.527 [2.091, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.368, 10.100], loss: 0.133210, mae: 0.349475, mean_q: 4.058455
 58921/100000: episode: 1301, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 28.716, mean reward: 2.393 [1.912, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.412], loss: 0.108679, mae: 0.343176, mean_q: 4.046697
 58937/100000: episode: 1302, duration: 0.079s, episode steps: 16, steps per second: 204, episode reward: 66.365, mean reward: 4.148 [2.900, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.076, 10.564], loss: 0.100419, mae: 0.323228, mean_q: 4.027832
 58953/100000: episode: 1303, duration: 0.075s, episode steps: 16, steps per second: 212, episode reward: 45.675, mean reward: 2.855 [2.363, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.387], loss: 0.137797, mae: 0.354588, mean_q: 4.170602
 58961/100000: episode: 1304, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 18.606, mean reward: 2.326 [2.142, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.329], loss: 0.150854, mae: 0.384314, mean_q: 4.159922
 58969/100000: episode: 1305, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 22.501, mean reward: 2.813 [2.082, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.657, 10.100], loss: 0.142353, mae: 0.351590, mean_q: 3.987217
 58995/100000: episode: 1306, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 78.561, mean reward: 3.022 [1.863, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.900, 10.100], loss: 0.157971, mae: 0.373573, mean_q: 4.125514
 59022/100000: episode: 1307, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 122.822, mean reward: 4.549 [2.420, 8.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.856, 10.100], loss: 0.176705, mae: 0.383328, mean_q: 4.126433
 59034/100000: episode: 1308, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 38.129, mean reward: 3.177 [2.482, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.544], loss: 0.243515, mae: 0.389490, mean_q: 4.239142
 59039/100000: episode: 1309, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 13.629, mean reward: 2.726 [2.436, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.247, 10.100], loss: 0.228904, mae: 0.453625, mean_q: 4.339717
 59065/100000: episode: 1310, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 61.070, mean reward: 2.349 [1.610, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.241, 10.100], loss: 0.153793, mae: 0.373165, mean_q: 4.176140
 59073/100000: episode: 1311, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 27.131, mean reward: 3.391 [2.311, 6.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.576], loss: 0.155240, mae: 0.369714, mean_q: 4.117020
 59096/100000: episode: 1312, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 67.269, mean reward: 2.925 [2.035, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.429, 10.513], loss: 0.171318, mae: 0.380070, mean_q: 4.172185
 59104/100000: episode: 1313, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 18.289, mean reward: 2.286 [2.081, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.354, 10.100], loss: 0.128812, mae: 0.339356, mean_q: 4.156835
 59131/100000: episode: 1314, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 79.920, mean reward: 2.960 [2.452, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.854, 10.100], loss: 0.133796, mae: 0.351035, mean_q: 4.152197
 59139/100000: episode: 1315, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 22.903, mean reward: 2.863 [2.453, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.425], loss: 0.165013, mae: 0.372650, mean_q: 4.152634
 59147/100000: episode: 1316, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 17.865, mean reward: 2.233 [2.009, 2.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.321, 10.100], loss: 0.133830, mae: 0.373702, mean_q: 4.239825
 59173/100000: episode: 1317, duration: 0.121s, episode steps: 26, steps per second: 214, episode reward: 73.726, mean reward: 2.836 [2.278, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.183, 10.100], loss: 0.109626, mae: 0.328851, mean_q: 4.102536
 59185/100000: episode: 1318, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 29.861, mean reward: 2.488 [1.924, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.378], loss: 0.187865, mae: 0.426937, mean_q: 4.359056
 59197/100000: episode: 1319, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 31.850, mean reward: 2.654 [2.079, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.442], loss: 0.151588, mae: 0.360212, mean_q: 4.137373
 59213/100000: episode: 1320, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 65.925, mean reward: 4.120 [3.052, 5.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.161, 10.594], loss: 0.112806, mae: 0.338610, mean_q: 4.216151
 59221/100000: episode: 1321, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 18.592, mean reward: 2.324 [2.163, 2.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.340, 10.100], loss: 0.177900, mae: 0.403904, mean_q: 4.336518
 59237/100000: episode: 1322, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 48.245, mean reward: 3.015 [2.597, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.356, 10.468], loss: 0.145046, mae: 0.364718, mean_q: 4.281966
 59245/100000: episode: 1323, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 23.035, mean reward: 2.879 [2.416, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.455, 10.430], loss: 0.139701, mae: 0.363932, mean_q: 4.191688
 59257/100000: episode: 1324, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 32.588, mean reward: 2.716 [2.085, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.358, 10.471], loss: 0.124439, mae: 0.338000, mean_q: 4.156574
 59265/100000: episode: 1325, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 19.693, mean reward: 2.462 [2.111, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.129, 10.389], loss: 0.172339, mae: 0.391509, mean_q: 4.346449
 59291/100000: episode: 1326, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 63.302, mean reward: 2.435 [1.578, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.842, 10.197], loss: 0.177387, mae: 0.382009, mean_q: 4.202899
 59317/100000: episode: 1327, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 76.060, mean reward: 2.925 [2.222, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.595, 10.100], loss: 0.142216, mae: 0.357836, mean_q: 4.273361
 59343/100000: episode: 1328, duration: 0.122s, episode steps: 26, steps per second: 213, episode reward: 112.074, mean reward: 4.311 [2.848, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.920, 10.494], loss: 0.144736, mae: 0.370570, mean_q: 4.231034
 59348/100000: episode: 1329, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 14.498, mean reward: 2.900 [2.498, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.378, 10.100], loss: 0.096014, mae: 0.316612, mean_q: 4.306928
 59374/100000: episode: 1330, duration: 0.125s, episode steps: 26, steps per second: 207, episode reward: 78.597, mean reward: 3.023 [2.159, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.501, 10.404], loss: 0.144136, mae: 0.356929, mean_q: 4.215141
 59382/100000: episode: 1331, duration: 0.040s, episode steps: 8, steps per second: 202, episode reward: 21.604, mean reward: 2.701 [1.924, 3.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.495], loss: 0.170002, mae: 0.372786, mean_q: 4.272261
 59408/100000: episode: 1332, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 80.839, mean reward: 3.109 [2.092, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.506, 10.100], loss: 0.146502, mae: 0.366517, mean_q: 4.331383
 59434/100000: episode: 1333, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 103.772, mean reward: 3.991 [2.070, 6.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.078, 10.415], loss: 0.168208, mae: 0.379345, mean_q: 4.261681
 59439/100000: episode: 1334, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 14.212, mean reward: 2.842 [2.653, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.381, 10.100], loss: 0.179965, mae: 0.380344, mean_q: 4.220226
 59462/100000: episode: 1335, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 58.368, mean reward: 2.538 [1.979, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.296], loss: 0.159474, mae: 0.379859, mean_q: 4.285710
 59470/100000: episode: 1336, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 20.770, mean reward: 2.596 [2.099, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.362], loss: 0.151125, mae: 0.353948, mean_q: 4.336971
 59496/100000: episode: 1337, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 69.872, mean reward: 2.687 [2.056, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.059, 10.482], loss: 0.154325, mae: 0.370435, mean_q: 4.265061
 59522/100000: episode: 1338, duration: 0.125s, episode steps: 26, steps per second: 209, episode reward: 81.022, mean reward: 3.116 [2.361, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.490], loss: 0.156929, mae: 0.398064, mean_q: 4.311153
 59530/100000: episode: 1339, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 19.093, mean reward: 2.387 [2.114, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.296, 10.100], loss: 0.168039, mae: 0.391363, mean_q: 4.283776
 59538/100000: episode: 1340, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 25.491, mean reward: 3.186 [2.598, 4.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.450], loss: 0.203298, mae: 0.412413, mean_q: 4.333265
 59564/100000: episode: 1341, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 60.366, mean reward: 2.322 [1.860, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.129, 10.100], loss: 0.205583, mae: 0.413817, mean_q: 4.317968
 59587/100000: episode: 1342, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 52.637, mean reward: 2.289 [1.737, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.821, 10.163], loss: 0.135020, mae: 0.370362, mean_q: 4.277259
 59603/100000: episode: 1343, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 46.192, mean reward: 2.887 [2.500, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.161, 10.396], loss: 0.133504, mae: 0.359528, mean_q: 4.315498
 59615/100000: episode: 1344, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 29.874, mean reward: 2.490 [2.063, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.475], loss: 0.216785, mae: 0.446371, mean_q: 4.408890
 59642/100000: episode: 1345, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 63.559, mean reward: 2.354 [1.769, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.121, 10.100], loss: 0.136932, mae: 0.361905, mean_q: 4.307304
 59650/100000: episode: 1346, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 21.731, mean reward: 2.716 [2.374, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.424, 10.100], loss: 0.136334, mae: 0.340439, mean_q: 4.311417
 59658/100000: episode: 1347, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 24.482, mean reward: 3.060 [2.401, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.970, 10.100], loss: 0.141617, mae: 0.354963, mean_q: 4.305931
 59684/100000: episode: 1348, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 79.397, mean reward: 3.054 [2.160, 5.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.578, 10.100], loss: 0.210291, mae: 0.400886, mean_q: 4.330209
[Info] 2-TH LEVEL FOUND: 7.490804195404053, Considering 10/90 traces
 59711/100000: episode: 1349, duration: 4.125s, episode steps: 27, steps per second: 7, episode reward: 74.492, mean reward: 2.759 [2.235, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.930, 10.100], loss: 0.192448, mae: 0.411790, mean_q: 4.335377
 59729/100000: episode: 1350, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 71.199, mean reward: 3.955 [3.225, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.276, 10.100], loss: 0.220331, mae: 0.443393, mean_q: 4.477672
 59741/100000: episode: 1351, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 48.092, mean reward: 4.008 [2.602, 8.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.093, 10.580], loss: 0.125369, mae: 0.358000, mean_q: 4.464481
 59761/100000: episode: 1352, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 63.035, mean reward: 3.152 [1.903, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.471, 10.100], loss: 0.157244, mae: 0.372527, mean_q: 4.336351
 59771/100000: episode: 1353, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 29.627, mean reward: 2.963 [2.492, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.451, 10.100], loss: 0.198183, mae: 0.422620, mean_q: 4.533220
 59792/100000: episode: 1354, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 71.102, mean reward: 3.386 [2.569, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.466, 10.100], loss: 0.172815, mae: 0.378147, mean_q: 4.376908
 59810/100000: episode: 1355, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 72.037, mean reward: 4.002 [2.703, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.360, 10.100], loss: 0.159324, mae: 0.390430, mean_q: 4.369480
 59828/100000: episode: 1356, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 63.141, mean reward: 3.508 [2.700, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.328, 10.100], loss: 0.187115, mae: 0.388337, mean_q: 4.424120
 59843/100000: episode: 1357, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 53.605, mean reward: 3.574 [2.501, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.674, 10.100], loss: 0.170699, mae: 0.396569, mean_q: 4.431136
 59855/100000: episode: 1358, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 44.333, mean reward: 3.694 [2.733, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.197, 10.522], loss: 0.142811, mae: 0.349910, mean_q: 4.318877
 59863/100000: episode: 1359, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 23.673, mean reward: 2.959 [2.476, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.449], loss: 0.157015, mae: 0.392839, mean_q: 4.445759
 59880/100000: episode: 1360, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 49.401, mean reward: 2.906 [1.933, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.119, 10.336], loss: 0.278678, mae: 0.458736, mean_q: 4.526734
 59892/100000: episode: 1361, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 78.088, mean reward: 6.507 [3.272, 17.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.605, 10.549], loss: 0.146625, mae: 0.373650, mean_q: 4.377809
 59904/100000: episode: 1362, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 82.706, mean reward: 6.892 [2.784, 16.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.581], loss: 0.196337, mae: 0.443420, mean_q: 4.598336
 59922/100000: episode: 1363, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 69.052, mean reward: 3.836 [2.807, 5.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.263, 10.100], loss: 0.152200, mae: 0.375714, mean_q: 4.393298
 59940/100000: episode: 1364, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 64.021, mean reward: 3.557 [2.857, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.461, 10.100], loss: 0.313846, mae: 0.437482, mean_q: 4.503938
 59946/100000: episode: 1365, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 16.613, mean reward: 2.769 [2.477, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.146, 10.433], loss: 0.226039, mae: 0.431387, mean_q: 4.582411
 59958/100000: episode: 1366, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 36.769, mean reward: 3.064 [2.487, 5.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-1.387, 10.455], loss: 0.489624, mae: 0.503458, mean_q: 4.535586
 59973/100000: episode: 1367, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 57.416, mean reward: 3.828 [2.367, 6.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.363, 10.100], loss: 0.207529, mae: 0.425989, mean_q: 4.536685
 59990/100000: episode: 1368, duration: 0.080s, episode steps: 17, steps per second: 212, episode reward: 61.527, mean reward: 3.619 [2.906, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.533, 10.545], loss: 0.248605, mae: 0.449633, mean_q: 4.604666
 60011/100000: episode: 1369, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 163.250, mean reward: 7.774 [2.849, 19.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.581, 10.100], loss: 0.272974, mae: 0.427625, mean_q: 4.558518
 60019/100000: episode: 1370, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 33.721, mean reward: 4.215 [2.485, 6.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.643], loss: 0.201136, mae: 0.415562, mean_q: 4.425590
 60031/100000: episode: 1371, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 46.550, mean reward: 3.879 [3.069, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.525], loss: 0.239352, mae: 0.455027, mean_q: 4.697862
 60052/100000: episode: 1372, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 92.692, mean reward: 4.414 [2.715, 10.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.434, 10.100], loss: 0.227219, mae: 0.443058, mean_q: 4.621102
 60060/100000: episode: 1373, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 26.452, mean reward: 3.306 [2.570, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.556], loss: 0.237572, mae: 0.447698, mean_q: 4.700309
 60068/100000: episode: 1374, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 24.179, mean reward: 3.022 [2.388, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.293, 10.502], loss: 0.208312, mae: 0.413301, mean_q: 4.564837
 60076/100000: episode: 1375, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 36.280, mean reward: 4.535 [2.711, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.635], loss: 0.731581, mae: 0.491248, mean_q: 4.585271
 60086/100000: episode: 1376, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 55.677, mean reward: 5.568 [3.368, 9.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.704, 10.100], loss: 0.284391, mae: 0.550075, mean_q: 4.818301
 60092/100000: episode: 1377, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 17.082, mean reward: 2.847 [2.469, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.419], loss: 0.206313, mae: 0.426026, mean_q: 4.505641
 60112/100000: episode: 1378, duration: 0.094s, episode steps: 20, steps per second: 214, episode reward: 65.403, mean reward: 3.270 [1.757, 8.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.269, 10.100], loss: 0.294124, mae: 0.483214, mean_q: 4.709565
 60133/100000: episode: 1379, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 153.126, mean reward: 7.292 [3.271, 21.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.461, 10.100], loss: 0.429538, mae: 0.497548, mean_q: 4.651730
 60141/100000: episode: 1380, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 21.735, mean reward: 2.717 [2.160, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.474], loss: 0.456078, mae: 0.533232, mean_q: 4.832625
 60149/100000: episode: 1381, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 27.092, mean reward: 3.387 [2.401, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.576], loss: 0.278063, mae: 0.461146, mean_q: 4.568032
 60166/100000: episode: 1382, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 109.862, mean reward: 6.462 [3.857, 11.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.262, 10.551], loss: 0.333355, mae: 0.472847, mean_q: 4.647370
 60183/100000: episode: 1383, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 73.912, mean reward: 4.348 [2.591, 5.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.240, 10.516], loss: 0.284022, mae: 0.484841, mean_q: 4.634450
 60201/100000: episode: 1384, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 123.048, mean reward: 6.836 [2.840, 18.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.512, 10.100], loss: 0.561293, mae: 0.568766, mean_q: 4.843213
 60207/100000: episode: 1385, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 16.873, mean reward: 2.812 [2.492, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.468], loss: 0.266662, mae: 0.462093, mean_q: 4.678190
 60213/100000: episode: 1386, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 18.257, mean reward: 3.043 [2.368, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.219, 10.527], loss: 0.348029, mae: 0.535989, mean_q: 4.855464
 60228/100000: episode: 1387, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 72.976, mean reward: 4.865 [2.726, 8.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.297, 10.100], loss: 0.393646, mae: 0.506907, mean_q: 4.633044
 60236/100000: episode: 1388, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 21.076, mean reward: 2.635 [2.350, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.430], loss: 0.563109, mae: 0.560274, mean_q: 4.907428
 60254/100000: episode: 1389, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 59.651, mean reward: 3.314 [2.615, 5.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.426, 10.100], loss: 0.378101, mae: 0.514505, mean_q: 4.791788
 60275/100000: episode: 1390, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 102.672, mean reward: 4.889 [2.961, 11.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.456, 10.100], loss: 0.493494, mae: 0.547866, mean_q: 4.918014
 60287/100000: episode: 1391, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 47.038, mean reward: 3.920 [3.253, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.531], loss: 0.514550, mae: 0.587108, mean_q: 4.751344
 60297/100000: episode: 1392, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 35.663, mean reward: 3.566 [3.120, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.354, 10.100], loss: 0.215217, mae: 0.480777, mean_q: 4.854183
 60307/100000: episode: 1393, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 50.264, mean reward: 5.026 [4.306, 5.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.371, 10.100], loss: 0.256332, mae: 0.483397, mean_q: 4.742938
 60317/100000: episode: 1394, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 36.793, mean reward: 3.679 [3.235, 5.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.446, 10.100], loss: 0.690980, mae: 0.619129, mean_q: 4.937962
 60334/100000: episode: 1395, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 58.906, mean reward: 3.465 [2.903, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.553], loss: 0.405642, mae: 0.531944, mean_q: 4.860887
 60346/100000: episode: 1396, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 31.040, mean reward: 2.587 [2.168, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.130, 10.409], loss: 0.239072, mae: 0.456657, mean_q: 4.656473
 60355/100000: episode: 1397, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 51.530, mean reward: 5.726 [3.435, 11.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.691], loss: 0.301383, mae: 0.511941, mean_q: 4.956211
 60364/100000: episode: 1398, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 37.614, mean reward: 4.179 [3.564, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.245, 10.481], loss: 0.263341, mae: 0.435588, mean_q: 4.700651
 60384/100000: episode: 1399, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 57.686, mean reward: 2.884 [2.363, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.746, 10.100], loss: 0.544796, mae: 0.586888, mean_q: 4.985084
 60390/100000: episode: 1400, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.778, mean reward: 2.296 [2.016, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.320], loss: 0.777218, mae: 0.561532, mean_q: 4.776363
 60405/100000: episode: 1401, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 56.896, mean reward: 3.793 [2.795, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.299, 10.100], loss: 0.583285, mae: 0.525654, mean_q: 4.912679
 60413/100000: episode: 1402, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 40.630, mean reward: 5.079 [2.686, 9.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.346, 10.597], loss: 0.285754, mae: 0.458076, mean_q: 4.895811
 60428/100000: episode: 1403, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 49.954, mean reward: 3.330 [2.308, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.385, 10.100], loss: 0.548028, mae: 0.558093, mean_q: 4.972326
 60445/100000: episode: 1404, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 79.089, mean reward: 4.652 [3.673, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.887, 10.508], loss: 0.427651, mae: 0.564642, mean_q: 4.758964
 60454/100000: episode: 1405, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 31.369, mean reward: 3.485 [2.613, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.379, 10.477], loss: 0.821026, mae: 0.662338, mean_q: 5.041182
 60474/100000: episode: 1406, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 66.270, mean reward: 3.313 [2.447, 10.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.870, 10.100], loss: 0.671323, mae: 0.691991, mean_q: 4.998158
 60495/100000: episode: 1407, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 89.272, mean reward: 4.251 [2.621, 10.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.463, 10.100], loss: 0.717713, mae: 0.616747, mean_q: 5.034219
 60515/100000: episode: 1408, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 70.295, mean reward: 3.515 [2.396, 6.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.845, 10.100], loss: 0.564368, mae: 0.599392, mean_q: 4.951264
 60521/100000: episode: 1409, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 21.972, mean reward: 3.662 [3.078, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.541], loss: 0.303847, mae: 0.501066, mean_q: 4.659748
 60529/100000: episode: 1410, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 32.421, mean reward: 4.053 [2.641, 6.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.551, 10.422], loss: 0.719539, mae: 0.545878, mean_q: 4.773622
 60538/100000: episode: 1411, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 30.394, mean reward: 3.377 [2.301, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.540, 10.513], loss: 0.236351, mae: 0.478999, mean_q: 4.813014
 60553/100000: episode: 1412, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 57.997, mean reward: 3.866 [3.148, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.457, 10.100], loss: 0.660434, mae: 0.606607, mean_q: 5.003544
 60562/100000: episode: 1413, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 32.148, mean reward: 3.572 [3.203, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.163, 10.527], loss: 0.403755, mae: 0.565660, mean_q: 4.963412
 60579/100000: episode: 1414, duration: 0.083s, episode steps: 17, steps per second: 206, episode reward: 54.387, mean reward: 3.199 [2.474, 4.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.381], loss: 0.398864, mae: 0.552234, mean_q: 5.094301
 60589/100000: episode: 1415, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 53.479, mean reward: 5.348 [3.390, 10.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.366, 10.100], loss: 0.530740, mae: 0.678842, mean_q: 5.001065
 60607/100000: episode: 1416, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 72.526, mean reward: 4.029 [2.814, 11.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.757, 10.100], loss: 0.611043, mae: 0.612285, mean_q: 5.141859
 60619/100000: episode: 1417, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 39.333, mean reward: 3.278 [2.522, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.529], loss: 0.193335, mae: 0.447635, mean_q: 4.912711
 60627/100000: episode: 1418, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 27.009, mean reward: 3.376 [2.423, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.556], loss: 0.462464, mae: 0.546487, mean_q: 4.997604
 60648/100000: episode: 1419, duration: 0.100s, episode steps: 21, steps per second: 209, episode reward: 95.955, mean reward: 4.569 [3.074, 6.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.566, 10.100], loss: 0.573917, mae: 0.536644, mean_q: 5.188572
 60668/100000: episode: 1420, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 79.082, mean reward: 3.954 [2.931, 6.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.524, 10.100], loss: 0.429741, mae: 0.574320, mean_q: 5.041143
 60683/100000: episode: 1421, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 60.936, mean reward: 4.062 [2.464, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.524, 10.100], loss: 0.509126, mae: 0.543915, mean_q: 4.955461
 60698/100000: episode: 1422, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 71.261, mean reward: 4.751 [2.746, 9.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.519, 10.100], loss: 0.381963, mae: 0.549021, mean_q: 5.112801
 60707/100000: episode: 1423, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 30.993, mean reward: 3.444 [2.704, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.466, 10.516], loss: 0.603599, mae: 0.612838, mean_q: 5.210369
 60728/100000: episode: 1424, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 97.049, mean reward: 4.621 [2.543, 6.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.512, 10.100], loss: 0.863877, mae: 0.666278, mean_q: 5.151996
 60740/100000: episode: 1425, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 39.995, mean reward: 3.333 [2.843, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.580], loss: 0.971540, mae: 0.708923, mean_q: 5.259855
 60746/100000: episode: 1426, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 18.130, mean reward: 3.022 [2.745, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.467], loss: 0.606872, mae: 0.673362, mean_q: 5.315305
 60764/100000: episode: 1427, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 74.029, mean reward: 4.113 [2.753, 8.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.291, 10.100], loss: 0.423399, mae: 0.568224, mean_q: 4.981001
 60773/100000: episode: 1428, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 24.998, mean reward: 2.778 [2.366, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.389], loss: 0.587394, mae: 0.720492, mean_q: 5.285877
 60783/100000: episode: 1429, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 39.953, mean reward: 3.995 [3.433, 5.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.237, 10.100], loss: 0.451567, mae: 0.596841, mean_q: 5.047693
 60800/100000: episode: 1430, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 84.363, mean reward: 4.963 [3.765, 8.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.660, 10.558], loss: 0.456491, mae: 0.563648, mean_q: 5.125412
 60817/100000: episode: 1431, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 79.863, mean reward: 4.698 [2.373, 7.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.417], loss: 0.557869, mae: 0.617202, mean_q: 5.280114
 60835/100000: episode: 1432, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 62.522, mean reward: 3.473 [2.868, 5.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.159, 10.100], loss: 0.368640, mae: 0.555061, mean_q: 5.105612
 60852/100000: episode: 1433, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 66.575, mean reward: 3.916 [2.467, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.520], loss: 1.043108, mae: 0.735676, mean_q: 5.300727
 60862/100000: episode: 1434, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 41.742, mean reward: 4.174 [3.397, 5.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.281, 10.100], loss: 0.395404, mae: 0.525183, mean_q: 4.991940
 60880/100000: episode: 1435, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 93.271, mean reward: 5.182 [3.311, 9.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.400, 10.100], loss: 1.053159, mae: 0.726776, mean_q: 5.508677
 60897/100000: episode: 1436, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 59.925, mean reward: 3.525 [2.589, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.544], loss: 0.312253, mae: 0.530089, mean_q: 5.146272
 60912/100000: episode: 1437, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 46.595, mean reward: 3.106 [2.355, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.071, 10.100], loss: 0.539862, mae: 0.606761, mean_q: 5.256477
 60927/100000: episode: 1438, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 67.017, mean reward: 4.468 [3.296, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.304, 10.100], loss: 0.769895, mae: 0.631674, mean_q: 5.280860
[Info] 3-TH LEVEL FOUND: 10.512310028076172, Considering 10/90 traces
 60944/100000: episode: 1439, duration: 4.071s, episode steps: 17, steps per second: 4, episode reward: 56.777, mean reward: 3.340 [2.732, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.824, 10.438], loss: 0.539078, mae: 0.578237, mean_q: 5.178018
 60953/100000: episode: 1440, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 55.004, mean reward: 6.112 [4.218, 10.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.524, 10.100], loss: 0.324810, mae: 0.521517, mean_q: 5.360645
 60957/100000: episode: 1441, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 17.502, mean reward: 4.376 [3.865, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.561], loss: 0.276354, mae: 0.518922, mean_q: 5.420220
 60961/100000: episode: 1442, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 17.903, mean reward: 4.476 [3.803, 5.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.547], loss: 0.293406, mae: 0.502668, mean_q: 5.104984
 60965/100000: episode: 1443, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 20.224, mean reward: 5.056 [4.116, 5.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.054, 10.623], loss: 0.357973, mae: 0.576120, mean_q: 5.126906
 60980/100000: episode: 1444, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 132.669, mean reward: 8.845 [4.922, 16.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.705, 10.100], loss: 0.646194, mae: 0.671270, mean_q: 5.431942
 60995/100000: episode: 1445, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 81.724, mean reward: 5.448 [3.003, 16.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.492, 10.100], loss: 0.751410, mae: 0.660054, mean_q: 5.395267
 61004/100000: episode: 1446, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 77.230, mean reward: 8.581 [5.275, 12.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.383, 10.100], loss: 0.635242, mae: 0.661362, mean_q: 5.458019
 61012/100000: episode: 1447, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 160.544, mean reward: 20.068 [4.853, 88.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.411, 10.100], loss: 0.396196, mae: 0.587301, mean_q: 5.504544
 61015/100000: episode: 1448, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 15.426, mean reward: 5.142 [4.839, 5.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.569], loss: 0.336280, mae: 0.526504, mean_q: 5.393326
 61022/100000: episode: 1449, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 37.282, mean reward: 5.326 [4.570, 6.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.620], loss: 0.372279, mae: 0.553784, mean_q: 5.447396
 61029/100000: episode: 1450, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 39.791, mean reward: 5.684 [3.732, 10.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.581], loss: 0.551458, mae: 0.631274, mean_q: 5.205522
 61032/100000: episode: 1451, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 27.688, mean reward: 9.229 [5.129, 11.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.704], loss: 1.289930, mae: 0.668687, mean_q: 5.401972
 61042/100000: episode: 1452, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 86.157, mean reward: 8.616 [4.846, 14.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.625, 10.100], loss: 11.889410, mae: 0.915133, mean_q: 5.562766
 61051/100000: episode: 1453, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 49.757, mean reward: 5.529 [4.674, 6.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.971, 10.100], loss: 1.034959, mae: 0.893664, mean_q: 5.443207
[Info] FALSIFICATION!
 61064/100000: episode: 1454, duration: 0.228s, episode steps: 13, steps per second: 57, episode reward: 1087.147, mean reward: 83.627 [4.167, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.004, 10.100], loss: 1.020008, mae: 0.734035, mean_q: 5.391436
 61066/100000: episode: 1455, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 7.292, mean reward: 3.646 [3.482, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.035, 10.502], loss: 0.719507, mae: 0.746078, mean_q: 5.649631
 61073/100000: episode: 1456, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 30.056, mean reward: 4.294 [3.814, 5.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.507], loss: 0.947299, mae: 0.754863, mean_q: 5.340765
 61083/100000: episode: 1457, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 49.468, mean reward: 4.947 [3.535, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.391, 10.100], loss: 0.572240, mae: 0.639249, mean_q: 5.407756
 61092/100000: episode: 1458, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 121.668, mean reward: 13.519 [5.893, 50.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.580, 10.100], loss: 0.507709, mae: 0.694720, mean_q: 5.577802
 61099/100000: episode: 1459, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 54.860, mean reward: 7.837 [6.638, 9.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.601], loss: 0.968586, mae: 0.713695, mean_q: 5.408448
 61101/100000: episode: 1460, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 8.500, mean reward: 4.250 [4.195, 4.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.506], loss: 0.358204, mae: 0.579916, mean_q: 5.410299
[Info] FALSIFICATION!
 61107/100000: episode: 1461, duration: 0.296s, episode steps: 6, steps per second: 20, episode reward: 1042.595, mean reward: 173.766 [6.657, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.362, 9.988], loss: 0.334807, mae: 0.589046, mean_q: 5.505972
 61117/100000: episode: 1462, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 60.872, mean reward: 6.087 [4.455, 7.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.792, 10.100], loss: 0.601501, mae: 0.712346, mean_q: 5.860970
 61127/100000: episode: 1463, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 70.736, mean reward: 7.074 [5.061, 11.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.544, 10.100], loss: 0.633246, mae: 0.606244, mean_q: 5.314769
 61135/100000: episode: 1464, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 52.790, mean reward: 6.599 [5.284, 7.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.538, 10.100], loss: 1922.853882, mae: 5.741920, mean_q: 6.827474
 61145/100000: episode: 1465, duration: 0.049s, episode steps: 10, steps per second: 206, episode reward: 42.848, mean reward: 4.285 [3.424, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.433, 10.100], loss: 9.212283, mae: 3.012279, mean_q: 8.320944
 61158/100000: episode: 1466, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 125.407, mean reward: 9.647 [4.169, 19.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.659, 10.100], loss: 7.396438, mae: 1.885660, mean_q: 5.551503
 61162/100000: episode: 1467, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 19.379, mean reward: 4.845 [3.482, 7.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.389], loss: 1.700711, mae: 1.364039, mean_q: 5.494659
 61165/100000: episode: 1468, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 17.795, mean reward: 5.932 [5.662, 6.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.139, 10.554], loss: 5102.579102, mae: 11.525115, mean_q: 6.100137
 61175/100000: episode: 1469, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 51.857, mean reward: 5.186 [4.249, 7.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.466, 10.100], loss: 4.256391, mae: 2.067702, mean_q: 7.486598
[Info] FALSIFICATION!
 61180/100000: episode: 1470, duration: 0.191s, episode steps: 5, steps per second: 26, episode reward: 1045.875, mean reward: 209.175 [6.676, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.258, 9.864], loss: 3.990257, mae: 2.111734, mean_q: 7.307040
 61190/100000: episode: 1471, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 104.293, mean reward: 10.429 [5.269, 24.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.376, 10.100], loss: 2.075281, mae: 1.406670, mean_q: 6.385562
 61200/100000: episode: 1472, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 61.928, mean reward: 6.193 [4.700, 7.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.496, 10.100], loss: 1530.090088, mae: 4.158931, mean_q: 6.061534
[Info] FALSIFICATION!
 61205/100000: episode: 1473, duration: 0.285s, episode steps: 5, steps per second: 18, episode reward: 1026.916, mean reward: 205.383 [5.304, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.355, 9.864], loss: 2.859609, mae: 1.733355, mean_q: 7.334949
 61214/100000: episode: 1474, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 65.728, mean reward: 7.303 [5.403, 11.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.859, 10.100], loss: 2.908490, mae: 1.701277, mean_q: 7.296772
 61216/100000: episode: 1475, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 6.148, mean reward: 3.074 [2.916, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.462], loss: 3.318352, mae: 1.646083, mean_q: 7.383707
 61218/100000: episode: 1476, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 11.092, mean reward: 5.546 [4.000, 7.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.035, 10.655], loss: 2.922473, mae: 1.525354, mean_q: 7.126060
 61227/100000: episode: 1477, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 92.416, mean reward: 10.268 [5.607, 30.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.693, 10.100], loss: 1.293472, mae: 0.952320, mean_q: 6.273161
 61235/100000: episode: 1478, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 131.368, mean reward: 16.421 [10.602, 25.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.497, 10.100], loss: 0.650732, mae: 0.730696, mean_q: 5.819725
 61245/100000: episode: 1479, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 45.826, mean reward: 4.583 [4.034, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.500, 10.100], loss: 0.821302, mae: 0.758578, mean_q: 5.977600
[Info] FALSIFICATION!
 61250/100000: episode: 1480, duration: 0.309s, episode steps: 5, steps per second: 16, episode reward: 1035.710, mean reward: 207.142 [6.122, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.727, 9.952], loss: 8.745508, mae: 1.378285, mean_q: 6.132883
 61253/100000: episode: 1481, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 23.490, mean reward: 7.830 [6.762, 8.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.638], loss: 1.294817, mae: 0.911335, mean_q: 6.199783
 61255/100000: episode: 1482, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 8.559, mean reward: 4.280 [3.609, 4.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.035, 10.538], loss: 4.421138, mae: 1.134396, mean_q: 6.698563
 61270/100000: episode: 1483, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 110.494, mean reward: 7.366 [4.411, 11.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.807, 10.100], loss: 1017.580139, mae: 3.229049, mean_q: 6.511564
 61273/100000: episode: 1484, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 32.929, mean reward: 10.976 [7.665, 14.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.699], loss: 5.007253, mae: 2.353365, mean_q: 7.889638
 61286/100000: episode: 1485, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 212.264, mean reward: 16.328 [3.894, 124.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.776, 10.100], loss: 3.337459, mae: 1.815829, mean_q: 7.460907
 61295/100000: episode: 1486, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 45.334, mean reward: 5.037 [3.830, 6.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.402, 10.100], loss: 1.682479, mae: 1.131739, mean_q: 6.589597
 61299/100000: episode: 1487, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 15.439, mean reward: 3.860 [3.167, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.591], loss: 0.590512, mae: 0.850728, mean_q: 5.881533
 61303/100000: episode: 1488, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 20.122, mean reward: 5.031 [4.218, 5.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.438, 10.614], loss: 1.068287, mae: 1.007038, mean_q: 6.023702
 61307/100000: episode: 1489, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 14.411, mean reward: 3.603 [3.262, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.539], loss: 1.699353, mae: 1.064806, mean_q: 6.505243
[Info] FALSIFICATION!
 61319/100000: episode: 1490, duration: 0.324s, episode steps: 12, steps per second: 37, episode reward: 2158.048, mean reward: 179.837 [6.070, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.644, 10.098], loss: 1.008356, mae: 0.916416, mean_q: 5.810047
 61323/100000: episode: 1491, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 12.589, mean reward: 3.147 [2.881, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.035, 10.491], loss: 3812.430420, mae: 8.715211, mean_q: 6.398917
 61332/100000: episode: 1492, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 47.834, mean reward: 5.315 [3.482, 8.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.445, 10.100], loss: 2.335689, mae: 1.611546, mean_q: 7.485819
 61342/100000: episode: 1493, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 167.629, mean reward: 16.763 [5.141, 77.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.371, 10.100], loss: 3.226134, mae: 1.815918, mean_q: 7.635572
 61349/100000: episode: 1494, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 34.494, mean reward: 4.928 [4.110, 6.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.614], loss: 1.506799, mae: 1.110095, mean_q: 6.679006
 61353/100000: episode: 1495, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 14.447, mean reward: 3.612 [3.284, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.477], loss: 3.948450, mae: 1.262523, mean_q: 6.247143
 61368/100000: episode: 1496, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 144.821, mean reward: 9.655 [3.970, 28.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.610, 10.100], loss: 13.929127, mae: 1.175305, mean_q: 6.049872
 61377/100000: episode: 1497, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 55.058, mean reward: 6.118 [3.897, 10.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.344, 10.100], loss: 1687.870117, mae: 4.408813, mean_q: 6.317638
 61392/100000: episode: 1498, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 213.378, mean reward: 14.225 [6.215, 21.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.170, 10.100], loss: 13.300657, mae: 3.702055, mean_q: 9.442920
 61394/100000: episode: 1499, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 9.493, mean reward: 4.746 [4.180, 5.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.035, 10.576], loss: 12.380344, mae: 3.485786, mean_q: 9.512842
 61402/100000: episode: 1500, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 45.645, mean reward: 5.706 [4.360, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.469, 10.100], loss: 5.102303, mae: 2.004088, mean_q: 7.914461
 61405/100000: episode: 1501, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 16.328, mean reward: 5.443 [5.183, 5.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.585], loss: 2.524825, mae: 1.415064, mean_q: 7.096470
 61420/100000: episode: 1502, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 416.132, mean reward: 27.742 [7.504, 215.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.497, 10.100], loss: 3019.780273, mae: 8.933087, mean_q: 9.429074
 61428/100000: episode: 1503, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 44.576, mean reward: 5.572 [4.384, 7.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.519, 10.100], loss: 1862.149292, mae: 7.619249, mean_q: 10.055910
 61437/100000: episode: 1504, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 41.669, mean reward: 4.630 [3.296, 5.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.494, 10.100], loss: 16.843269, mae: 3.897836, mean_q: 10.511208
 61440/100000: episode: 1505, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 15.863, mean reward: 5.288 [4.434, 6.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.490], loss: 9.921902, mae: 2.798012, mean_q: 8.564532
 61449/100000: episode: 1506, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 45.184, mean reward: 5.020 [4.223, 5.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.507, 10.100], loss: 5.689434, mae: 1.924637, mean_q: 7.578240
 61452/100000: episode: 1507, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 19.268, mean reward: 6.423 [6.048, 6.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.475], loss: 2.608694, mae: 1.514307, mean_q: 6.562653
 61467/100000: episode: 1508, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 189.641, mean reward: 12.643 [7.942, 33.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.557, 10.100], loss: 3.541275, mae: 1.606889, mean_q: 6.242873
 61475/100000: episode: 1509, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 51.180, mean reward: 6.397 [5.396, 8.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.366, 10.100], loss: 2.814656, mae: 1.409045, mean_q: 7.109216
 61485/100000: episode: 1510, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 81.488, mean reward: 8.149 [4.786, 12.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.407, 10.100], loss: 1510.834473, mae: 4.468591, mean_q: 7.364522
 61494/100000: episode: 1511, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 63.234, mean reward: 7.026 [4.825, 9.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.600, 10.100], loss: 1755.617676, mae: 6.966712, mean_q: 9.450853
 61503/100000: episode: 1512, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 45.158, mean reward: 5.018 [3.724, 6.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.477, 10.100], loss: 1671.057251, mae: 6.964014, mean_q: 10.655581
 61512/100000: episode: 1513, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 60.268, mean reward: 6.696 [4.155, 9.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.838, 10.100], loss: 15.167779, mae: 3.635309, mean_q: 10.334364
 61520/100000: episode: 1514, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 99.680, mean reward: 12.460 [6.889, 18.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.489, 10.100], loss: 37.697350, mae: 2.851806, mean_q: 8.465960
 61530/100000: episode: 1515, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 65.496, mean reward: 6.550 [4.887, 13.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.607, 10.100], loss: 5.128390, mae: 1.813972, mean_q: 8.747446
 61532/100000: episode: 1516, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 8.821, mean reward: 4.410 [4.359, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.472], loss: 2.043514, mae: 1.202227, mean_q: 7.191239
 61541/100000: episode: 1517, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 91.400, mean reward: 10.156 [5.649, 23.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.511, 10.100], loss: 7.589701, mae: 1.713597, mean_q: 7.425994
 61551/100000: episode: 1518, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 39.597, mean reward: 3.960 [3.405, 4.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.495, 10.100], loss: 1521.442017, mae: 4.867829, mean_q: 8.125809
 61555/100000: episode: 1519, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 21.398, mean reward: 5.350 [4.654, 6.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.509], loss: 4.875431, mae: 1.883975, mean_q: 9.092142
 61557/100000: episode: 1520, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 10.302, mean reward: 5.151 [4.536, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.408], loss: 8.716381, mae: 2.240421, mean_q: 8.629834
 61566/100000: episode: 1521, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 40.255, mean reward: 4.473 [3.450, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.440, 10.100], loss: 3329.175781, mae: 9.031520, mean_q: 9.701415
 61576/100000: episode: 1522, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 50.854, mean reward: 5.085 [3.671, 7.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.475, 10.100], loss: 23.966389, mae: 4.232348, mean_q: 11.057122
 61586/100000: episode: 1523, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 163.895, mean reward: 16.390 [5.281, 101.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.558, 10.100], loss: 11.817611, mae: 2.982195, mean_q: 9.498128
 61596/100000: episode: 1524, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 121.583, mean reward: 12.158 [6.063, 21.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.518, 10.100], loss: 7.563616, mae: 2.281767, mean_q: 9.444849
[Info] FALSIFICATION!
 61610/100000: episode: 1525, duration: 0.320s, episode steps: 14, steps per second: 44, episode reward: 1197.863, mean reward: 85.562 [5.831, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.574, 10.098], loss: 14.828482, mae: 2.441949, mean_q: 9.366942
[Info] FALSIFICATION!
 61625/100000: episode: 1526, duration: 0.334s, episode steps: 15, steps per second: 45, episode reward: 1805.159, mean reward: 120.344 [5.942, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.621, 10.100], loss: 10.271053, mae: 1.717479, mean_q: 8.288369
 61634/100000: episode: 1527, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 117.532, mean reward: 13.059 [5.123, 36.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.579, 10.100], loss: 3.551047, mae: 1.396930, mean_q: 7.803262
 61644/100000: episode: 1528, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 57.837, mean reward: 5.784 [4.285, 10.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.444, 10.100], loss: 3058.571777, mae: 7.849421, mean_q: 7.940763
[Info] Complete ISplit Iteration
[Info] Levels: [5.8441267, 7.490804, 10.51231, 21.271412]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.63]
[Info] Error Prob: 0.0006300000000000001

 61647/100000: episode: 1529, duration: 4.182s, episode steps: 3, steps per second: 1, episode reward: 16.458, mean reward: 5.486 [5.174, 6.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.454], loss: 11.755371, mae: 3.063154, mean_q: 10.584523
 61747/100000: episode: 1530, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 208.578, mean reward: 2.086 [1.435, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.489, 10.382], loss: 915.810791, mae: 4.797160, mean_q: 10.128607
 61847/100000: episode: 1531, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.463, mean reward: 1.855 [1.443, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.768, 10.179], loss: 907.965210, mae: 4.558522, mean_q: 10.080583
 61947/100000: episode: 1532, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 288.383, mean reward: 2.884 [1.459, 95.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.649, 10.098], loss: 1237.531128, mae: 6.060222, mean_q: 11.001084
 62047/100000: episode: 1533, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 181.762, mean reward: 1.818 [1.439, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.428, 10.203], loss: 934.927551, mae: 5.202948, mean_q: 11.150077
 62147/100000: episode: 1534, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 194.160, mean reward: 1.942 [1.471, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.893, 10.098], loss: 988.794617, mae: 5.284091, mean_q: 10.896937
 62247/100000: episode: 1535, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 199.051, mean reward: 1.991 [1.465, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.098], loss: 949.797729, mae: 5.173165, mean_q: 11.239998
 62347/100000: episode: 1536, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 198.886, mean reward: 1.989 [1.480, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.896, 10.098], loss: 308.026062, mae: 2.457762, mean_q: 9.005140
 62447/100000: episode: 1537, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.958, mean reward: 1.920 [1.472, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.851, 10.335], loss: 901.571533, mae: 4.738201, mean_q: 10.272667
 62547/100000: episode: 1538, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 183.370, mean reward: 1.834 [1.442, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.037, 10.222], loss: 353.086090, mae: 2.845918, mean_q: 9.340739
 62647/100000: episode: 1539, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.234, mean reward: 1.912 [1.475, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.307, 10.098], loss: 1340.539062, mae: 6.577713, mean_q: 11.316797
 62747/100000: episode: 1540, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.103, mean reward: 1.891 [1.502, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.601, 10.147], loss: 902.169128, mae: 4.642853, mean_q: 10.639874
 62847/100000: episode: 1541, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 187.542, mean reward: 1.875 [1.476, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.229, 10.230], loss: 306.834106, mae: 2.540459, mean_q: 8.899250
 62947/100000: episode: 1542, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 196.367, mean reward: 1.964 [1.565, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.837, 10.098], loss: 975.957336, mae: 4.668043, mean_q: 9.738128
 63047/100000: episode: 1543, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 203.649, mean reward: 2.036 [1.512, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.065, 10.098], loss: 605.071960, mae: 4.129404, mean_q: 10.072082
 63147/100000: episode: 1544, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.116, mean reward: 1.841 [1.461, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.356, 10.106], loss: 837.168213, mae: 4.552329, mean_q: 9.998698
 63247/100000: episode: 1545, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 198.659, mean reward: 1.987 [1.531, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.098], loss: 1703.614868, mae: 7.509844, mean_q: 11.459845
 63347/100000: episode: 1546, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.959, mean reward: 2.000 [1.462, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.492, 10.098], loss: 1341.365479, mae: 6.671682, mean_q: 12.187182
 63447/100000: episode: 1547, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 173.634, mean reward: 1.736 [1.454, 2.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.197], loss: 793.010315, mae: 4.611556, mean_q: 11.259174
 63547/100000: episode: 1548, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 220.342, mean reward: 2.203 [1.458, 7.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.301, 10.098], loss: 1187.365967, mae: 5.280563, mean_q: 10.832973
 63647/100000: episode: 1549, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 211.983, mean reward: 2.120 [1.508, 4.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.713, 10.223], loss: 745.840881, mae: 4.305706, mean_q: 10.761201
 63747/100000: episode: 1550, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 198.037, mean reward: 1.980 [1.444, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.681, 10.144], loss: 1071.886841, mae: 5.235834, mean_q: 10.364296
 63847/100000: episode: 1551, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 191.663, mean reward: 1.917 [1.454, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.427, 10.098], loss: 1506.990601, mae: 7.414356, mean_q: 12.503160
 63947/100000: episode: 1552, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 213.103, mean reward: 2.131 [1.484, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.496, 10.222], loss: 1263.397949, mae: 5.743609, mean_q: 10.948983
 64047/100000: episode: 1553, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 196.736, mean reward: 1.967 [1.451, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.485, 10.323], loss: 1095.956299, mae: 5.883802, mean_q: 11.723097
 64147/100000: episode: 1554, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.873, mean reward: 1.929 [1.443, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.809, 10.128], loss: 1371.219238, mae: 6.887026, mean_q: 12.250945
 64247/100000: episode: 1555, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 205.590, mean reward: 2.056 [1.492, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.977, 10.098], loss: 376.827850, mae: 3.189326, mean_q: 9.990069
 64347/100000: episode: 1556, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 197.693, mean reward: 1.977 [1.463, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.361, 10.098], loss: 1082.979004, mae: 5.396814, mean_q: 10.810302
 64447/100000: episode: 1557, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 185.320, mean reward: 1.853 [1.463, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.359, 10.241], loss: 1171.631470, mae: 5.523848, mean_q: 10.663971
 64547/100000: episode: 1558, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 201.662, mean reward: 2.017 [1.450, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.230, 10.101], loss: 690.903564, mae: 4.550902, mean_q: 10.674315
 64647/100000: episode: 1559, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 184.792, mean reward: 1.848 [1.437, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.948, 10.298], loss: 919.688904, mae: 4.487871, mean_q: 9.721051
 64747/100000: episode: 1560, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.857, mean reward: 1.979 [1.472, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.066, 10.098], loss: 617.554932, mae: 3.590188, mean_q: 9.173252
 64847/100000: episode: 1561, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 196.481, mean reward: 1.965 [1.525, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.521, 10.267], loss: 1318.287354, mae: 5.905838, mean_q: 10.109405
 64947/100000: episode: 1562, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.812, mean reward: 1.938 [1.465, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.218], loss: 886.225342, mae: 5.461596, mean_q: 10.992725
 65047/100000: episode: 1563, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 203.752, mean reward: 2.038 [1.518, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.776, 10.098], loss: 585.785767, mae: 3.541480, mean_q: 9.330357
 65147/100000: episode: 1564, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.287, mean reward: 1.973 [1.472, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.917, 10.426], loss: 445.170532, mae: 2.480667, mean_q: 7.865872
 65247/100000: episode: 1565, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.675, mean reward: 1.907 [1.510, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.066, 10.098], loss: 783.156799, mae: 4.316998, mean_q: 8.885900
 65347/100000: episode: 1566, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 207.753, mean reward: 2.078 [1.471, 8.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.227, 10.120], loss: 752.447510, mae: 4.523171, mean_q: 9.615315
 65447/100000: episode: 1567, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 190.506, mean reward: 1.905 [1.467, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.589, 10.296], loss: 610.295837, mae: 3.132456, mean_q: 8.411690
 65547/100000: episode: 1568, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 198.446, mean reward: 1.984 [1.442, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.485, 10.149], loss: 879.145264, mae: 3.935169, mean_q: 8.539350
 65647/100000: episode: 1569, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 185.811, mean reward: 1.858 [1.489, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.411, 10.153], loss: 1027.721924, mae: 5.269083, mean_q: 9.741320
 65747/100000: episode: 1570, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.083, mean reward: 1.971 [1.446, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.291], loss: 595.145874, mae: 3.291239, mean_q: 8.346758
 65847/100000: episode: 1571, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.618, mean reward: 1.916 [1.518, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.671, 10.199], loss: 1194.062988, mae: 5.317371, mean_q: 8.847320
 65947/100000: episode: 1572, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.628, mean reward: 1.886 [1.481, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.582, 10.312], loss: 1015.245300, mae: 5.509011, mean_q: 9.829746
 66047/100000: episode: 1573, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.157, mean reward: 1.932 [1.488, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.942, 10.171], loss: 1041.729126, mae: 4.194064, mean_q: 8.067039
 66147/100000: episode: 1574, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 189.894, mean reward: 1.899 [1.514, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.684, 10.152], loss: 854.106628, mae: 4.236687, mean_q: 8.339865
 66247/100000: episode: 1575, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.026, mean reward: 1.920 [1.447, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.420, 10.098], loss: 298.125092, mae: 2.347421, mean_q: 6.983049
 66347/100000: episode: 1576, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.068, mean reward: 1.891 [1.444, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.161], loss: 321.852661, mae: 1.961150, mean_q: 6.044952
 66447/100000: episode: 1577, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 208.507, mean reward: 2.085 [1.501, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.094, 10.098], loss: 10.901320, mae: 1.051073, mean_q: 5.351700
 66547/100000: episode: 1578, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 208.784, mean reward: 2.088 [1.475, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.425, 10.098], loss: 35.500301, mae: 0.679355, mean_q: 4.409200
 66647/100000: episode: 1579, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 195.173, mean reward: 1.952 [1.457, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.148, 10.305], loss: 0.453938, mae: 0.425596, mean_q: 3.925949
 66747/100000: episode: 1580, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.365, mean reward: 1.844 [1.445, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.428, 10.261], loss: 2.888742, mae: 0.465626, mean_q: 3.880466
 66847/100000: episode: 1581, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 188.845, mean reward: 1.888 [1.463, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.348, 10.098], loss: 0.151015, mae: 0.379944, mean_q: 3.878551
 66947/100000: episode: 1582, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 249.895, mean reward: 2.499 [1.478, 5.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.574, 10.362], loss: 0.156671, mae: 0.386929, mean_q: 3.855881
 67047/100000: episode: 1583, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.040, mean reward: 1.890 [1.476, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.571, 10.182], loss: 0.139300, mae: 0.377752, mean_q: 3.838098
 67147/100000: episode: 1584, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.371, mean reward: 1.934 [1.459, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.172, 10.098], loss: 0.147264, mae: 0.379896, mean_q: 3.852319
 67247/100000: episode: 1585, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 206.630, mean reward: 2.066 [1.497, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.303, 10.098], loss: 0.178403, mae: 0.385970, mean_q: 3.847286
 67347/100000: episode: 1586, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 189.531, mean reward: 1.895 [1.502, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.166], loss: 0.288291, mae: 0.395406, mean_q: 3.888313
 67447/100000: episode: 1587, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 201.325, mean reward: 2.013 [1.458, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.920, 10.349], loss: 0.294544, mae: 0.400479, mean_q: 3.864714
 67547/100000: episode: 1588, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 227.342, mean reward: 2.273 [1.469, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.930, 10.498], loss: 0.207301, mae: 0.395891, mean_q: 3.920619
 67647/100000: episode: 1589, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.695, mean reward: 1.897 [1.453, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.145], loss: 0.250878, mae: 0.382109, mean_q: 3.908008
 67747/100000: episode: 1590, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.201, mean reward: 1.922 [1.447, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.985, 10.377], loss: 0.151084, mae: 0.376993, mean_q: 3.894011
 67847/100000: episode: 1591, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 220.215, mean reward: 2.202 [1.458, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.343, 10.098], loss: 0.304610, mae: 0.382010, mean_q: 3.900056
 67947/100000: episode: 1592, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 191.900, mean reward: 1.919 [1.454, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.098], loss: 0.405941, mae: 0.405441, mean_q: 3.877002
 68047/100000: episode: 1593, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 193.669, mean reward: 1.937 [1.450, 6.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.007, 10.273], loss: 0.243217, mae: 0.394781, mean_q: 3.895139
 68147/100000: episode: 1594, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 199.465, mean reward: 1.995 [1.448, 4.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.805, 10.098], loss: 0.205386, mae: 0.378313, mean_q: 3.905984
 68247/100000: episode: 1595, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 199.378, mean reward: 1.994 [1.510, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.561, 10.098], loss: 0.155465, mae: 0.381646, mean_q: 3.870170
 68347/100000: episode: 1596, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 179.580, mean reward: 1.796 [1.447, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.555, 10.208], loss: 0.125926, mae: 0.359515, mean_q: 3.896138
 68447/100000: episode: 1597, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 177.921, mean reward: 1.779 [1.455, 2.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.794, 10.229], loss: 0.179346, mae: 0.360677, mean_q: 3.878308
 68547/100000: episode: 1598, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 199.265, mean reward: 1.993 [1.445, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.321, 10.260], loss: 0.213998, mae: 0.374453, mean_q: 3.887097
 68647/100000: episode: 1599, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 190.983, mean reward: 1.910 [1.454, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.203, 10.098], loss: 0.161592, mae: 0.361658, mean_q: 3.876462
 68747/100000: episode: 1600, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 182.010, mean reward: 1.820 [1.455, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.710, 10.098], loss: 0.174445, mae: 0.371797, mean_q: 3.880443
 68847/100000: episode: 1601, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 182.354, mean reward: 1.824 [1.440, 4.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.647, 10.152], loss: 0.125270, mae: 0.344239, mean_q: 3.875492
 68947/100000: episode: 1602, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 183.450, mean reward: 1.835 [1.458, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.591, 10.098], loss: 0.208011, mae: 0.365939, mean_q: 3.900192
 69047/100000: episode: 1603, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 182.948, mean reward: 1.829 [1.449, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.804, 10.098], loss: 0.122106, mae: 0.342603, mean_q: 3.850740
 69147/100000: episode: 1604, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 197.922, mean reward: 1.979 [1.498, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.244, 10.219], loss: 0.126845, mae: 0.333946, mean_q: 3.840384
 69247/100000: episode: 1605, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.432, mean reward: 1.864 [1.462, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.976, 10.098], loss: 0.113616, mae: 0.316397, mean_q: 3.837644
 69347/100000: episode: 1606, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.450, mean reward: 1.934 [1.449, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.953, 10.178], loss: 0.115141, mae: 0.328761, mean_q: 3.854008
 69447/100000: episode: 1607, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 177.954, mean reward: 1.780 [1.459, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.313, 10.098], loss: 0.129593, mae: 0.321410, mean_q: 3.839474
 69547/100000: episode: 1608, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 204.316, mean reward: 2.043 [1.438, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.880, 10.311], loss: 0.119019, mae: 0.328139, mean_q: 3.833287
 69647/100000: episode: 1609, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.366, mean reward: 1.834 [1.471, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.048, 10.098], loss: 0.144194, mae: 0.326656, mean_q: 3.841701
 69747/100000: episode: 1610, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.514, mean reward: 2.055 [1.449, 12.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.648, 10.098], loss: 0.136447, mae: 0.319630, mean_q: 3.854029
 69847/100000: episode: 1611, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 200.456, mean reward: 2.005 [1.464, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.601, 10.183], loss: 0.101326, mae: 0.308049, mean_q: 3.833768
 69947/100000: episode: 1612, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.555, mean reward: 1.966 [1.451, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.878, 10.112], loss: 0.160909, mae: 0.336176, mean_q: 3.859139
 70047/100000: episode: 1613, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 211.314, mean reward: 2.113 [1.543, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.280, 10.098], loss: 0.153772, mae: 0.328668, mean_q: 3.846158
 70147/100000: episode: 1614, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.690, mean reward: 1.877 [1.436, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.285, 10.142], loss: 0.117548, mae: 0.308548, mean_q: 3.832699
 70247/100000: episode: 1615, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.696, mean reward: 1.867 [1.434, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.396, 10.152], loss: 0.142781, mae: 0.319992, mean_q: 3.854076
 70347/100000: episode: 1616, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 183.415, mean reward: 1.834 [1.438, 3.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.382, 10.098], loss: 0.098578, mae: 0.299509, mean_q: 3.849600
 70447/100000: episode: 1617, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 202.946, mean reward: 2.029 [1.591, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.487, 10.098], loss: 0.119973, mae: 0.314439, mean_q: 3.851303
 70547/100000: episode: 1618, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 179.939, mean reward: 1.799 [1.459, 2.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.731, 10.244], loss: 0.098635, mae: 0.305848, mean_q: 3.849557
 70647/100000: episode: 1619, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 205.248, mean reward: 2.052 [1.487, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.429, 10.098], loss: 0.129275, mae: 0.307411, mean_q: 3.858086
 70747/100000: episode: 1620, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 221.490, mean reward: 2.215 [1.534, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.530, 10.239], loss: 0.084857, mae: 0.290326, mean_q: 3.838594
 70847/100000: episode: 1621, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 188.255, mean reward: 1.883 [1.472, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.989, 10.098], loss: 0.100336, mae: 0.296851, mean_q: 3.842527
 70947/100000: episode: 1622, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.121, mean reward: 1.911 [1.458, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.508, 10.141], loss: 0.088092, mae: 0.290228, mean_q: 3.853346
 71047/100000: episode: 1623, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 178.889, mean reward: 1.789 [1.437, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.997, 10.231], loss: 0.102304, mae: 0.298496, mean_q: 3.843678
 71147/100000: episode: 1624, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: 202.037, mean reward: 2.020 [1.463, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.470, 10.098], loss: 0.127117, mae: 0.304103, mean_q: 3.863195
 71247/100000: episode: 1625, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 190.312, mean reward: 1.903 [1.449, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.914, 10.311], loss: 0.087967, mae: 0.288112, mean_q: 3.863319
 71347/100000: episode: 1626, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 196.545, mean reward: 1.965 [1.461, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.318, 10.283], loss: 0.115533, mae: 0.298735, mean_q: 3.862736
 71447/100000: episode: 1627, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.198, mean reward: 2.002 [1.463, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.746, 10.191], loss: 0.078240, mae: 0.278766, mean_q: 3.855655
 71547/100000: episode: 1628, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 202.465, mean reward: 2.025 [1.482, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.144, 10.342], loss: 0.095323, mae: 0.293934, mean_q: 3.861886
[Info] 1-TH LEVEL FOUND: 4.851012229919434, Considering 10/90 traces
 71647/100000: episode: 1629, duration: 4.544s, episode steps: 100, steps per second: 22, episode reward: 176.761, mean reward: 1.768 [1.449, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.051, 10.229], loss: 0.106319, mae: 0.291012, mean_q: 3.864311
 71658/100000: episode: 1630, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 22.370, mean reward: 2.034 [1.583, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.184], loss: 0.090825, mae: 0.282775, mean_q: 3.815997
 71677/100000: episode: 1631, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 37.521, mean reward: 1.975 [1.653, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.859, 10.223], loss: 0.064773, mae: 0.261137, mean_q: 3.845569
 71694/100000: episode: 1632, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 81.864, mean reward: 4.816 [1.867, 20.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.696], loss: 0.093759, mae: 0.305093, mean_q: 3.891803
 71730/100000: episode: 1633, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 83.354, mean reward: 2.315 [1.797, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.796, 10.449], loss: 0.082418, mae: 0.277556, mean_q: 3.844543
 71744/100000: episode: 1634, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 29.081, mean reward: 2.077 [1.738, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.074, 10.335], loss: 0.133842, mae: 0.334353, mean_q: 3.908993
 71770/100000: episode: 1635, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 62.126, mean reward: 2.389 [1.738, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.549, 10.209], loss: 0.071730, mae: 0.279199, mean_q: 3.870988
 71869/100000: episode: 1636, duration: 0.497s, episode steps: 99, steps per second: 199, episode reward: 193.838, mean reward: 1.958 [1.441, 3.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.735, 10.254], loss: 0.095715, mae: 0.298605, mean_q: 3.884833
 71968/100000: episode: 1637, duration: 0.488s, episode steps: 99, steps per second: 203, episode reward: 188.910, mean reward: 1.908 [1.450, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.767, 10.117], loss: 0.105696, mae: 0.290318, mean_q: 3.865997
 72067/100000: episode: 1638, duration: 0.490s, episode steps: 99, steps per second: 202, episode reward: 180.650, mean reward: 1.825 [1.438, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.650, 10.100], loss: 0.092824, mae: 0.286004, mean_q: 3.846335
 72093/100000: episode: 1639, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 67.502, mean reward: 2.596 [2.153, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.147, 10.379], loss: 0.105725, mae: 0.301113, mean_q: 3.867687
 72104/100000: episode: 1640, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 44.268, mean reward: 4.024 [2.606, 8.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.433, 10.646], loss: 0.401737, mae: 0.350957, mean_q: 3.900815
 72123/100000: episode: 1641, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 38.324, mean reward: 2.017 [1.712, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.266], loss: 0.093335, mae: 0.291815, mean_q: 3.853519
 72130/100000: episode: 1642, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 24.795, mean reward: 3.542 [2.727, 5.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.446, 10.100], loss: 0.159824, mae: 0.372542, mean_q: 4.061143
 72229/100000: episode: 1643, duration: 0.471s, episode steps: 99, steps per second: 210, episode reward: 194.657, mean reward: 1.966 [1.455, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.930, 10.467], loss: 0.170712, mae: 0.313936, mean_q: 3.897780
 72255/100000: episode: 1644, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 67.500, mean reward: 2.596 [2.108, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.441], loss: 0.089578, mae: 0.281696, mean_q: 3.884543
 72272/100000: episode: 1645, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 34.236, mean reward: 2.014 [1.543, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.202], loss: 0.369635, mae: 0.319254, mean_q: 3.929148
 72297/100000: episode: 1646, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 66.594, mean reward: 2.664 [2.083, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.564, 10.348], loss: 0.082007, mae: 0.277050, mean_q: 3.887599
 72316/100000: episode: 1647, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 38.914, mean reward: 2.048 [1.812, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.279], loss: 0.131412, mae: 0.319713, mean_q: 3.913782
 72327/100000: episode: 1648, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 30.847, mean reward: 2.804 [2.363, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.933, 10.354], loss: 0.081540, mae: 0.296619, mean_q: 3.929768
 72346/100000: episode: 1649, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 43.258, mean reward: 2.277 [2.046, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.334, 10.404], loss: 0.085322, mae: 0.283173, mean_q: 3.912328
 72353/100000: episode: 1650, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 24.079, mean reward: 3.440 [2.992, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.062, 10.100], loss: 0.073100, mae: 0.272774, mean_q: 3.860332
 72389/100000: episode: 1651, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 134.548, mean reward: 3.737 [2.563, 6.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.110, 10.487], loss: 0.227835, mae: 0.316570, mean_q: 3.937480
 72406/100000: episode: 1652, duration: 0.087s, episode steps: 17, steps per second: 194, episode reward: 40.379, mean reward: 2.375 [1.793, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.126, 10.375], loss: 0.097869, mae: 0.302610, mean_q: 3.925558
 72423/100000: episode: 1653, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 49.991, mean reward: 2.941 [1.950, 4.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-1.446, 10.519], loss: 0.086135, mae: 0.293541, mean_q: 3.915040
 72449/100000: episode: 1654, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 62.830, mean reward: 2.417 [1.816, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.613, 10.343], loss: 0.117370, mae: 0.309548, mean_q: 3.926438
 72466/100000: episode: 1655, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 38.060, mean reward: 2.239 [1.875, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.381], loss: 0.115040, mae: 0.309325, mean_q: 3.912602
 72477/100000: episode: 1656, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 27.681, mean reward: 2.516 [2.164, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.555, 10.353], loss: 0.079303, mae: 0.288395, mean_q: 3.964762
 72488/100000: episode: 1657, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 28.486, mean reward: 2.590 [2.099, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.351], loss: 0.090133, mae: 0.306591, mean_q: 3.898072
 72514/100000: episode: 1658, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 55.063, mean reward: 2.118 [1.586, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.183], loss: 0.149730, mae: 0.301599, mean_q: 3.929492
 72613/100000: episode: 1659, duration: 0.470s, episode steps: 99, steps per second: 211, episode reward: 186.952, mean reward: 1.888 [1.477, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.869, 10.100], loss: 0.105011, mae: 0.305294, mean_q: 3.922694
 72649/100000: episode: 1660, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 82.225, mean reward: 2.284 [1.702, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.237, 10.298], loss: 0.092937, mae: 0.298615, mean_q: 3.901892
 72663/100000: episode: 1661, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 25.997, mean reward: 1.857 [1.624, 2.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.297], loss: 0.124504, mae: 0.316813, mean_q: 3.912538
 72699/100000: episode: 1662, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 96.663, mean reward: 2.685 [2.145, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.837, 10.460], loss: 0.217182, mae: 0.335577, mean_q: 3.923658
 72718/100000: episode: 1663, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 47.433, mean reward: 2.496 [2.045, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.070, 10.363], loss: 0.180595, mae: 0.321585, mean_q: 3.970339
 72725/100000: episode: 1664, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 25.294, mean reward: 3.613 [2.471, 5.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.391, 10.100], loss: 0.112419, mae: 0.307665, mean_q: 3.886324
 72824/100000: episode: 1665, duration: 0.503s, episode steps: 99, steps per second: 197, episode reward: 183.566, mean reward: 1.854 [1.463, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.741, 10.100], loss: 0.131683, mae: 0.312537, mean_q: 3.956652
 72838/100000: episode: 1666, duration: 0.066s, episode steps: 14, steps per second: 211, episode reward: 32.561, mean reward: 2.326 [1.951, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.370, 10.426], loss: 0.102141, mae: 0.307885, mean_q: 3.958525
 72852/100000: episode: 1667, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 33.145, mean reward: 2.368 [1.750, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.559, 10.376], loss: 0.123943, mae: 0.315623, mean_q: 3.911706
 72871/100000: episode: 1668, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 43.493, mean reward: 2.289 [1.848, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.173, 10.328], loss: 0.102884, mae: 0.309493, mean_q: 3.915977
 72896/100000: episode: 1669, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 78.138, mean reward: 3.126 [1.917, 6.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.325, 10.548], loss: 0.100894, mae: 0.296957, mean_q: 3.929196
 72913/100000: episode: 1670, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 43.179, mean reward: 2.540 [1.970, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.058, 10.376], loss: 0.402201, mae: 0.362958, mean_q: 3.989993
 72924/100000: episode: 1671, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 31.921, mean reward: 2.902 [2.013, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.358], loss: 0.257557, mae: 0.360633, mean_q: 3.997871
 72941/100000: episode: 1672, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 44.839, mean reward: 2.638 [1.904, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.897, 10.366], loss: 0.197125, mae: 0.334182, mean_q: 3.990906
 72948/100000: episode: 1673, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 19.986, mean reward: 2.855 [2.513, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.347, 10.100], loss: 0.081763, mae: 0.268245, mean_q: 3.866867
 72962/100000: episode: 1674, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 31.561, mean reward: 2.254 [1.949, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.373], loss: 0.114305, mae: 0.318825, mean_q: 3.999418
 72973/100000: episode: 1675, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 32.775, mean reward: 2.980 [2.405, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.472], loss: 0.110256, mae: 0.321817, mean_q: 3.965714
 73072/100000: episode: 1676, duration: 0.466s, episode steps: 99, steps per second: 213, episode reward: 191.623, mean reward: 1.936 [1.450, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.612, 10.100], loss: 0.212772, mae: 0.333743, mean_q: 3.966445
 73089/100000: episode: 1677, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 35.833, mean reward: 2.108 [1.702, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.367], loss: 0.118428, mae: 0.331279, mean_q: 4.012670
 73108/100000: episode: 1678, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 42.087, mean reward: 2.215 [1.821, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.126, 10.290], loss: 0.080777, mae: 0.282579, mean_q: 3.943464
 73207/100000: episode: 1679, duration: 0.478s, episode steps: 99, steps per second: 207, episode reward: 185.997, mean reward: 1.879 [1.459, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.473, 10.194], loss: 0.125051, mae: 0.315135, mean_q: 3.967843
 73221/100000: episode: 1680, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 34.911, mean reward: 2.494 [1.958, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.135, 10.296], loss: 0.455045, mae: 0.372729, mean_q: 3.960333
 73240/100000: episode: 1681, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 40.319, mean reward: 2.122 [1.608, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.316, 10.189], loss: 0.127743, mae: 0.353623, mean_q: 3.940305
 73265/100000: episode: 1682, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 53.315, mean reward: 2.133 [1.729, 2.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.052, 10.292], loss: 0.144012, mae: 0.352494, mean_q: 4.008844
 73272/100000: episode: 1683, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 22.803, mean reward: 3.258 [2.686, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.218, 10.100], loss: 0.098623, mae: 0.317209, mean_q: 4.043540
 73297/100000: episode: 1684, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 83.885, mean reward: 3.355 [2.277, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.859, 10.407], loss: 0.113118, mae: 0.323913, mean_q: 3.951080
 73311/100000: episode: 1685, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 34.788, mean reward: 2.485 [1.907, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-1.112, 10.281], loss: 0.118483, mae: 0.324739, mean_q: 3.952290
 73410/100000: episode: 1686, duration: 0.469s, episode steps: 99, steps per second: 211, episode reward: 183.186, mean reward: 1.850 [1.463, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.791, 10.100], loss: 0.200026, mae: 0.329656, mean_q: 4.011257
 73435/100000: episode: 1687, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 65.888, mean reward: 2.636 [2.148, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.324, 10.457], loss: 0.133063, mae: 0.333733, mean_q: 3.956981
 73534/100000: episode: 1688, duration: 0.490s, episode steps: 99, steps per second: 202, episode reward: 197.008, mean reward: 1.990 [1.488, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-1.238, 10.311], loss: 0.111794, mae: 0.316305, mean_q: 3.986354
 73551/100000: episode: 1689, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 41.326, mean reward: 2.431 [1.855, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.252, 10.471], loss: 0.100236, mae: 0.322931, mean_q: 4.022279
 73562/100000: episode: 1690, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 33.699, mean reward: 3.064 [2.695, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.408], loss: 0.093376, mae: 0.307875, mean_q: 4.001850
 73573/100000: episode: 1691, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 32.674, mean reward: 2.970 [2.614, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.446], loss: 0.114972, mae: 0.332045, mean_q: 3.996125
 73672/100000: episode: 1692, duration: 0.490s, episode steps: 99, steps per second: 202, episode reward: 189.111, mean reward: 1.910 [1.433, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.246, 10.255], loss: 0.155716, mae: 0.318451, mean_q: 3.992382
 73689/100000: episode: 1693, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 45.563, mean reward: 2.680 [2.192, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.118, 10.398], loss: 0.121152, mae: 0.303705, mean_q: 3.982639
 73706/100000: episode: 1694, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 42.880, mean reward: 2.522 [2.005, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.539], loss: 0.128280, mae: 0.332096, mean_q: 4.008263
 73713/100000: episode: 1695, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 18.006, mean reward: 2.572 [2.204, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.346, 10.100], loss: 0.170268, mae: 0.346857, mean_q: 3.846308
 73732/100000: episode: 1696, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 47.765, mean reward: 2.514 [1.791, 5.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.745, 10.412], loss: 0.124466, mae: 0.358173, mean_q: 4.033563
 73757/100000: episode: 1697, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 56.060, mean reward: 2.242 [1.841, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.137, 10.300], loss: 0.110378, mae: 0.313262, mean_q: 3.991741
 73774/100000: episode: 1698, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 33.413, mean reward: 1.965 [1.717, 2.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.279], loss: 0.155943, mae: 0.358867, mean_q: 4.073684
 73810/100000: episode: 1699, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 115.851, mean reward: 3.218 [2.305, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.346, 10.427], loss: 0.124445, mae: 0.330010, mean_q: 4.040983
 73846/100000: episode: 1700, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 116.169, mean reward: 3.227 [2.235, 5.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.669, 10.360], loss: 0.174725, mae: 0.351432, mean_q: 4.025001
 73872/100000: episode: 1701, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 78.254, mean reward: 3.010 [1.832, 6.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.410], loss: 0.160394, mae: 0.368301, mean_q: 4.123075
 73889/100000: episode: 1702, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 43.426, mean reward: 2.554 [2.212, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.421], loss: 0.114723, mae: 0.324832, mean_q: 4.056241
 73988/100000: episode: 1703, duration: 0.506s, episode steps: 99, steps per second: 196, episode reward: 186.587, mean reward: 1.885 [1.475, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.288, 10.100], loss: 0.223766, mae: 0.351669, mean_q: 4.082735
 74002/100000: episode: 1704, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 30.914, mean reward: 2.208 [1.788, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.420, 10.250], loss: 0.165414, mae: 0.386269, mean_q: 4.092581
 74013/100000: episode: 1705, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 31.380, mean reward: 2.853 [2.408, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.462], loss: 0.115848, mae: 0.338669, mean_q: 4.113254
 74030/100000: episode: 1706, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 38.951, mean reward: 2.291 [1.885, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.164, 10.332], loss: 0.096131, mae: 0.319316, mean_q: 4.121527
 74044/100000: episode: 1707, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 32.648, mean reward: 2.332 [1.938, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.846, 10.462], loss: 0.137058, mae: 0.358761, mean_q: 4.085990
 74055/100000: episode: 1708, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 30.690, mean reward: 2.790 [2.234, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.504], loss: 0.105567, mae: 0.318759, mean_q: 4.074967
 74074/100000: episode: 1709, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 49.482, mean reward: 2.604 [2.049, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.112, 10.368], loss: 0.128994, mae: 0.358458, mean_q: 4.142843
 74099/100000: episode: 1710, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 75.006, mean reward: 3.000 [1.941, 6.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.273, 10.370], loss: 0.123528, mae: 0.345933, mean_q: 4.123000
 74116/100000: episode: 1711, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 49.677, mean reward: 2.922 [2.051, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.296, 10.521], loss: 0.097434, mae: 0.306445, mean_q: 4.038367
 74130/100000: episode: 1712, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 37.038, mean reward: 2.646 [1.925, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.425], loss: 0.153491, mae: 0.355799, mean_q: 4.135325
 74141/100000: episode: 1713, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 32.957, mean reward: 2.996 [2.170, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.470], loss: 0.178175, mae: 0.360803, mean_q: 4.056335
 74148/100000: episode: 1714, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 20.215, mean reward: 2.888 [2.490, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.450, 10.100], loss: 0.115901, mae: 0.353250, mean_q: 4.144125
 74173/100000: episode: 1715, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 50.602, mean reward: 2.024 [1.461, 2.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.096, 10.196], loss: 0.115254, mae: 0.336786, mean_q: 4.096848
 74190/100000: episode: 1716, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 53.198, mean reward: 3.129 [2.447, 4.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.247, 10.516], loss: 0.114098, mae: 0.316333, mean_q: 4.141553
 74226/100000: episode: 1717, duration: 0.179s, episode steps: 36, steps per second: 202, episode reward: 134.578, mean reward: 3.738 [2.500, 6.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.323, 10.469], loss: 0.128475, mae: 0.346290, mean_q: 4.133243
 74233/100000: episode: 1718, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 20.520, mean reward: 2.931 [2.589, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.339, 10.100], loss: 0.133813, mae: 0.326721, mean_q: 4.071861
[Info] 2-TH LEVEL FOUND: 6.135797023773193, Considering 10/90 traces
 74269/100000: episode: 1719, duration: 4.155s, episode steps: 36, steps per second: 9, episode reward: 116.635, mean reward: 3.240 [1.674, 8.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.987, 10.179], loss: 0.176653, mae: 0.364460, mean_q: 4.146494
 74273/100000: episode: 1720, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 12.386, mean reward: 3.096 [2.747, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.499, 10.100], loss: 0.133335, mae: 0.330855, mean_q: 4.012594
 74294/100000: episode: 1721, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 66.766, mean reward: 3.179 [2.440, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.306, 10.433], loss: 0.140724, mae: 0.349280, mean_q: 4.176833
 74307/100000: episode: 1722, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 40.359, mean reward: 3.105 [2.359, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.433], loss: 0.138231, mae: 0.347821, mean_q: 4.133332
 74325/100000: episode: 1723, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 77.374, mean reward: 4.299 [3.419, 6.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.847, 10.599], loss: 0.172008, mae: 0.377184, mean_q: 4.194841
 74329/100000: episode: 1724, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 16.493, mean reward: 4.123 [3.658, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.476, 10.100], loss: 0.095439, mae: 0.314488, mean_q: 4.047036
 74333/100000: episode: 1725, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 15.554, mean reward: 3.889 [3.308, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.398, 10.100], loss: 0.271713, mae: 0.447031, mean_q: 4.108347
 74336/100000: episode: 1726, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 8.966, mean reward: 2.989 [2.737, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.485, 10.100], loss: 0.106273, mae: 0.353988, mean_q: 4.309684
 74356/100000: episode: 1727, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 59.846, mean reward: 2.992 [2.176, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.068, 10.445], loss: 0.134874, mae: 0.344975, mean_q: 4.228404
 74363/100000: episode: 1728, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 19.875, mean reward: 2.839 [2.397, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.352], loss: 0.157294, mae: 0.365077, mean_q: 4.128352
 74370/100000: episode: 1729, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 23.277, mean reward: 3.325 [2.961, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-1.507, 10.415], loss: 0.179222, mae: 0.392287, mean_q: 4.426313
 74372/100000: episode: 1730, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 7.504, mean reward: 3.752 [3.695, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.871, 10.100], loss: 2.641322, mae: 0.674035, mean_q: 4.078876
 74379/100000: episode: 1731, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 24.389, mean reward: 3.484 [3.048, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.530], loss: 0.591420, mae: 0.486241, mean_q: 4.386127
 74382/100000: episode: 1732, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 11.056, mean reward: 3.685 [3.316, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.481, 10.100], loss: 0.232368, mae: 0.410275, mean_q: 4.213838
 74403/100000: episode: 1733, duration: 0.099s, episode steps: 21, steps per second: 211, episode reward: 73.063, mean reward: 3.479 [2.546, 5.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.568], loss: 0.163633, mae: 0.370209, mean_q: 4.160957
 74406/100000: episode: 1734, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 10.210, mean reward: 3.403 [2.918, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.470, 10.100], loss: 0.129597, mae: 0.401787, mean_q: 4.277180
 74413/100000: episode: 1735, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 24.184, mean reward: 3.455 [2.859, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.550], loss: 0.225850, mae: 0.425730, mean_q: 4.283731
 74417/100000: episode: 1736, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 12.690, mean reward: 3.173 [2.921, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.435, 10.100], loss: 0.132470, mae: 0.371058, mean_q: 4.312397
 74421/100000: episode: 1737, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 14.718, mean reward: 3.680 [3.559, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.449, 10.100], loss: 0.092792, mae: 0.319622, mean_q: 4.229639
 74423/100000: episode: 1738, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 8.975, mean reward: 4.488 [4.395, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.465, 10.100], loss: 0.361808, mae: 0.430250, mean_q: 4.322262
 74436/100000: episode: 1739, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 39.647, mean reward: 3.050 [2.382, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.801, 10.410], loss: 0.110816, mae: 0.325658, mean_q: 4.244171
 74439/100000: episode: 1740, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 10.972, mean reward: 3.657 [3.608, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.487, 10.100], loss: 1.668197, mae: 0.579261, mean_q: 4.268206
 74459/100000: episode: 1741, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 56.102, mean reward: 2.805 [1.777, 4.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.488, 10.264], loss: 0.391943, mae: 0.427493, mean_q: 4.211125
 74466/100000: episode: 1742, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 22.737, mean reward: 3.248 [3.051, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.470], loss: 0.171169, mae: 0.443789, mean_q: 4.435449
 74468/100000: episode: 1743, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 8.092, mean reward: 4.046 [3.747, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.450, 10.100], loss: 0.095398, mae: 0.322946, mean_q: 4.200491
 74481/100000: episode: 1744, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 51.426, mean reward: 3.956 [3.217, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.343], loss: 0.192570, mae: 0.383200, mean_q: 4.247973
 74502/100000: episode: 1745, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 91.009, mean reward: 4.334 [2.912, 9.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.485], loss: 0.151659, mae: 0.365808, mean_q: 4.258865
 74523/100000: episode: 1746, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 86.454, mean reward: 4.117 [2.877, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.154, 10.457], loss: 0.410802, mae: 0.439723, mean_q: 4.353271
 74544/100000: episode: 1747, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 124.049, mean reward: 5.907 [3.958, 10.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.121, 10.581], loss: 0.155505, mae: 0.369118, mean_q: 4.346333
 74547/100000: episode: 1748, duration: 0.017s, episode steps: 3, steps per second: 172, episode reward: 8.457, mean reward: 2.819 [2.443, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.470, 10.100], loss: 0.402421, mae: 0.450087, mean_q: 4.407161
 74558/100000: episode: 1749, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 32.231, mean reward: 2.930 [2.533, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.411], loss: 0.211903, mae: 0.425856, mean_q: 4.373962
 74576/100000: episode: 1750, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 59.628, mean reward: 3.313 [1.758, 11.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.184, 10.285], loss: 0.233758, mae: 0.419654, mean_q: 4.295547
 74583/100000: episode: 1751, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 22.916, mean reward: 3.274 [2.991, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.445], loss: 0.138387, mae: 0.367012, mean_q: 4.262501
 74586/100000: episode: 1752, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 12.452, mean reward: 4.151 [3.895, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.296, 10.100], loss: 0.134450, mae: 0.363711, mean_q: 4.309990
 74589/100000: episode: 1753, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 12.136, mean reward: 4.045 [3.536, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.431, 10.100], loss: 0.140011, mae: 0.345679, mean_q: 4.219742
 74596/100000: episode: 1754, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 28.610, mean reward: 4.087 [3.111, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.412, 10.480], loss: 0.195175, mae: 0.416939, mean_q: 4.375016
 74600/100000: episode: 1755, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 23.436, mean reward: 5.859 [4.354, 7.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.496, 10.100], loss: 0.295684, mae: 0.460765, mean_q: 4.459399
 74604/100000: episode: 1756, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 10.878, mean reward: 2.719 [2.481, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.470, 10.100], loss: 0.326672, mae: 0.469054, mean_q: 4.434793
 74615/100000: episode: 1757, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 28.789, mean reward: 2.617 [2.213, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.325, 10.392], loss: 0.691074, mae: 0.524734, mean_q: 4.489804
 74618/100000: episode: 1758, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 12.392, mean reward: 4.131 [3.508, 4.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.462, 10.100], loss: 0.199617, mae: 0.447497, mean_q: 4.083829
 74638/100000: episode: 1759, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 59.514, mean reward: 2.976 [2.366, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.641, 10.406], loss: 0.241422, mae: 0.417326, mean_q: 4.389371
 74658/100000: episode: 1760, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 64.616, mean reward: 3.231 [2.458, 6.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.114, 10.424], loss: 0.255127, mae: 0.411855, mean_q: 4.270068
 74661/100000: episode: 1761, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 9.626, mean reward: 3.209 [2.687, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.391, 10.100], loss: 0.198975, mae: 0.426530, mean_q: 4.463085
 74663/100000: episode: 1762, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 7.912, mean reward: 3.956 [3.447, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.479, 10.100], loss: 2.370018, mae: 0.651845, mean_q: 4.578243
 74683/100000: episode: 1763, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 46.933, mean reward: 2.347 [1.835, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.342], loss: 0.151883, mae: 0.394678, mean_q: 4.349787
 74687/100000: episode: 1764, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 11.753, mean reward: 2.938 [2.775, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.423, 10.100], loss: 0.154931, mae: 0.371879, mean_q: 4.452577
 74707/100000: episode: 1765, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 77.485, mean reward: 3.874 [2.987, 5.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.244, 10.466], loss: 0.143381, mae: 0.373737, mean_q: 4.353547
 74714/100000: episode: 1766, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 21.861, mean reward: 3.123 [2.872, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.464], loss: 0.216959, mae: 0.404257, mean_q: 4.437686
 74732/100000: episode: 1767, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 52.909, mean reward: 2.939 [2.077, 7.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.294, 10.424], loss: 0.332273, mae: 0.449824, mean_q: 4.445499
 74752/100000: episode: 1768, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 58.750, mean reward: 2.937 [2.016, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.409], loss: 0.238570, mae: 0.409143, mean_q: 4.464202
 74754/100000: episode: 1769, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 7.539, mean reward: 3.769 [3.485, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.525, 10.100], loss: 0.220820, mae: 0.502762, mean_q: 4.640037
 74756/100000: episode: 1770, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 9.649, mean reward: 4.824 [4.237, 5.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.455, 10.100], loss: 0.158086, mae: 0.400973, mean_q: 4.337481
 74760/100000: episode: 1771, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 11.656, mean reward: 2.914 [2.659, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.435, 10.100], loss: 0.217686, mae: 0.404387, mean_q: 4.399365
 74778/100000: episode: 1772, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 73.104, mean reward: 4.061 [2.080, 10.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.471, 10.385], loss: 0.462802, mae: 0.459730, mean_q: 4.316786
 74799/100000: episode: 1773, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 64.017, mean reward: 3.048 [1.768, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.358], loss: 0.189684, mae: 0.395752, mean_q: 4.386670
 74802/100000: episode: 1774, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 11.940, mean reward: 3.980 [3.629, 4.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.496, 10.100], loss: 0.263137, mae: 0.442395, mean_q: 4.143920
 74805/100000: episode: 1775, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 12.609, mean reward: 4.203 [3.607, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.802, 10.100], loss: 0.128357, mae: 0.367432, mean_q: 4.462454
 74816/100000: episode: 1776, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 44.210, mean reward: 4.019 [2.833, 5.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.542], loss: 0.142272, mae: 0.380470, mean_q: 4.396293
 74823/100000: episode: 1777, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 21.667, mean reward: 3.095 [2.754, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.542], loss: 0.185354, mae: 0.430829, mean_q: 4.396344
 74825/100000: episode: 1778, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 6.359, mean reward: 3.180 [2.912, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.441, 10.100], loss: 0.203296, mae: 0.418224, mean_q: 4.546611
 74845/100000: episode: 1779, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 64.935, mean reward: 3.247 [2.568, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.523], loss: 0.195868, mae: 0.426401, mean_q: 4.439237
 74849/100000: episode: 1780, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 13.191, mean reward: 3.298 [2.729, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.513, 10.100], loss: 0.158849, mae: 0.424576, mean_q: 4.523768
 74856/100000: episode: 1781, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 29.282, mean reward: 4.183 [3.230, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.387], loss: 0.164613, mae: 0.400720, mean_q: 4.365753
 74869/100000: episode: 1782, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 56.249, mean reward: 4.327 [3.139, 6.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.236, 10.404], loss: 0.240387, mae: 0.433679, mean_q: 4.594792
 74873/100000: episode: 1783, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 12.006, mean reward: 3.001 [2.855, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.406, 10.100], loss: 0.179359, mae: 0.395266, mean_q: 4.436999
 74894/100000: episode: 1784, duration: 0.100s, episode steps: 21, steps per second: 211, episode reward: 77.703, mean reward: 3.700 [2.740, 5.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.338, 10.488], loss: 0.286223, mae: 0.474708, mean_q: 4.467681
 74897/100000: episode: 1785, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 9.958, mean reward: 3.319 [3.187, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.475, 10.100], loss: 0.296569, mae: 0.469066, mean_q: 4.379997
 74899/100000: episode: 1786, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 7.917, mean reward: 3.959 [3.478, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.465, 10.100], loss: 0.192812, mae: 0.418279, mean_q: 4.382554
 74917/100000: episode: 1787, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 53.475, mean reward: 2.971 [1.987, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.328], loss: 0.322143, mae: 0.492132, mean_q: 4.592439
 74920/100000: episode: 1788, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 13.536, mean reward: 4.512 [4.377, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.596, 10.100], loss: 0.149220, mae: 0.394078, mean_q: 4.482108
 74923/100000: episode: 1789, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 17.743, mean reward: 5.914 [4.314, 7.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.500, 10.100], loss: 0.218973, mae: 0.422057, mean_q: 4.408631
 74941/100000: episode: 1790, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 105.802, mean reward: 5.878 [3.524, 13.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.159, 10.523], loss: 0.183946, mae: 0.428593, mean_q: 4.393501
 74943/100000: episode: 1791, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 6.334, mean reward: 3.167 [3.096, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.447, 10.100], loss: 0.148228, mae: 0.399392, mean_q: 4.884558
 74947/100000: episode: 1792, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 20.186, mean reward: 5.047 [3.869, 7.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.557, 10.100], loss: 0.260676, mae: 0.468185, mean_q: 4.300432
 74967/100000: episode: 1793, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 89.474, mean reward: 4.474 [2.334, 7.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.472, 10.524], loss: 0.297658, mae: 0.464064, mean_q: 4.569244
 74978/100000: episode: 1794, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 35.074, mean reward: 3.189 [2.598, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.312, 10.517], loss: 0.414751, mae: 0.520168, mean_q: 4.559872
 74980/100000: episode: 1795, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 5.799, mean reward: 2.900 [2.579, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.448, 10.100], loss: 0.430935, mae: 0.555073, mean_q: 4.601134
 74998/100000: episode: 1796, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 66.263, mean reward: 3.681 [2.418, 5.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.420], loss: 0.318420, mae: 0.475919, mean_q: 4.586665
 75002/100000: episode: 1797, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 14.604, mean reward: 3.651 [3.126, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.432, 10.100], loss: 0.437316, mae: 0.559881, mean_q: 4.666383
 75006/100000: episode: 1798, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 13.262, mean reward: 3.316 [2.868, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.447, 10.100], loss: 0.257057, mae: 0.454893, mean_q: 4.545499
 75019/100000: episode: 1799, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 47.891, mean reward: 3.684 [3.183, 4.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.507], loss: 0.276261, mae: 0.478973, mean_q: 4.549933
 75023/100000: episode: 1800, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 14.673, mean reward: 3.668 [3.160, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.478, 10.100], loss: 0.252140, mae: 0.482874, mean_q: 4.721401
 75036/100000: episode: 1801, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 52.194, mean reward: 4.015 [2.940, 8.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.845, 10.638], loss: 0.315513, mae: 0.479969, mean_q: 4.510015
 75054/100000: episode: 1802, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 76.460, mean reward: 4.248 [3.174, 6.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.074, 10.441], loss: 0.285763, mae: 0.488082, mean_q: 4.594004
 75058/100000: episode: 1803, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 14.685, mean reward: 3.671 [3.183, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.388, 10.100], loss: 0.251278, mae: 0.507654, mean_q: 4.806049
 75065/100000: episode: 1804, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 21.215, mean reward: 3.031 [2.605, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.525, 10.370], loss: 0.287980, mae: 0.484129, mean_q: 4.522064
 75076/100000: episode: 1805, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 34.226, mean reward: 3.111 [2.579, 6.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.770, 10.451], loss: 0.278712, mae: 0.504933, mean_q: 4.736269
 75096/100000: episode: 1806, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 68.248, mean reward: 3.412 [2.490, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.249, 10.418], loss: 0.197904, mae: 0.437571, mean_q: 4.482510
 75109/100000: episode: 1807, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 33.211, mean reward: 2.555 [2.000, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.206, 10.220], loss: 0.650685, mae: 0.571518, mean_q: 4.916317
 75113/100000: episode: 1808, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 16.020, mean reward: 4.005 [3.559, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.559, 10.100], loss: 0.427653, mae: 0.544928, mean_q: 4.489329
[Info] 3-TH LEVEL FOUND: 8.004880905151367, Considering 10/90 traces
 75116/100000: episode: 1809, duration: 4.075s, episode steps: 3, steps per second: 1, episode reward: 9.013, mean reward: 3.004 [2.655, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.396, 10.100], loss: 0.286799, mae: 0.538911, mean_q: 4.641856
 75118/100000: episode: 1810, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 8.004, mean reward: 4.002 [3.702, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.766, 10.429], loss: 0.181519, mae: 0.506967, mean_q: 4.648911
 75123/100000: episode: 1811, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 19.687, mean reward: 3.937 [3.350, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.563], loss: 0.204378, mae: 0.424667, mean_q: 4.492399
 75134/100000: episode: 1812, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 112.279, mean reward: 10.207 [4.144, 22.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.756, 10.527], loss: 0.185904, mae: 0.414391, mean_q: 4.587057
 75149/100000: episode: 1813, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 66.414, mean reward: 4.428 [3.293, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.511], loss: 0.417529, mae: 0.516805, mean_q: 4.649587
 75162/100000: episode: 1814, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 56.848, mean reward: 4.373 [2.500, 5.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.392], loss: 0.471917, mae: 0.598044, mean_q: 4.781488
 75167/100000: episode: 1815, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 19.800, mean reward: 3.960 [3.602, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.540], loss: 0.243429, mae: 0.445764, mean_q: 4.512596
 75178/100000: episode: 1816, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 42.232, mean reward: 3.839 [2.901, 6.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.494], loss: 0.216814, mae: 0.457369, mean_q: 4.479505
 75189/100000: episode: 1817, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 37.189, mean reward: 3.381 [2.912, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-1.207, 10.557], loss: 0.222297, mae: 0.434867, mean_q: 4.622763
 75191/100000: episode: 1818, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 6.295, mean reward: 3.148 [3.095, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.457], loss: 0.226471, mae: 0.536248, mean_q: 5.021554
 75202/100000: episode: 1819, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 48.954, mean reward: 4.450 [3.618, 6.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.530], loss: 0.280767, mae: 0.495889, mean_q: 4.554940
 75211/100000: episode: 1820, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 36.095, mean reward: 4.011 [3.539, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.586], loss: 0.314748, mae: 0.533343, mean_q: 4.830881
 75222/100000: episode: 1821, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 54.425, mean reward: 4.948 [3.231, 7.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.052, 10.433], loss: 0.287413, mae: 0.491459, mean_q: 4.522505
 75224/100000: episode: 1822, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 8.070, mean reward: 4.035 [3.699, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.507], loss: 0.173454, mae: 0.468629, mean_q: 4.490926
 75240/100000: episode: 1823, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 86.799, mean reward: 5.425 [2.407, 17.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.442], loss: 0.346190, mae: 0.489604, mean_q: 4.766016
 75251/100000: episode: 1824, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 38.804, mean reward: 3.528 [3.106, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.491], loss: 0.599859, mae: 0.543881, mean_q: 4.667094
 75262/100000: episode: 1825, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 59.880, mean reward: 5.444 [3.896, 7.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.523], loss: 0.260134, mae: 0.496535, mean_q: 4.784794
 75278/100000: episode: 1826, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 43.118, mean reward: 2.695 [1.970, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.262, 10.400], loss: 0.558184, mae: 0.612345, mean_q: 4.900093
 75294/100000: episode: 1827, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 111.591, mean reward: 6.974 [4.439, 14.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.330, 10.688], loss: 0.351398, mae: 0.536316, mean_q: 4.806125
 75300/100000: episode: 1828, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 23.989, mean reward: 3.998 [3.227, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.498], loss: 0.709604, mae: 0.572804, mean_q: 4.824965
 75302/100000: episode: 1829, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 8.451, mean reward: 4.226 [3.771, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.094, 10.389], loss: 0.220487, mae: 0.495642, mean_q: 4.851670
 75304/100000: episode: 1830, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 6.511, mean reward: 3.255 [3.150, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.464], loss: 0.437060, mae: 0.462488, mean_q: 4.352136
 75317/100000: episode: 1831, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 58.646, mean reward: 4.511 [3.511, 6.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.504], loss: 0.352373, mae: 0.530834, mean_q: 4.892217
 75323/100000: episode: 1832, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 23.878, mean reward: 3.980 [3.395, 4.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.489], loss: 0.464208, mae: 0.567800, mean_q: 4.580921
 75329/100000: episode: 1833, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 17.637, mean reward: 2.940 [2.606, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.443], loss: 0.398560, mae: 0.620190, mean_q: 5.012697
 75340/100000: episode: 1834, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 44.556, mean reward: 4.051 [2.658, 5.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.489], loss: 0.355816, mae: 0.539240, mean_q: 4.737939
 75355/100000: episode: 1835, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 60.418, mean reward: 4.028 [3.115, 6.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.149, 10.487], loss: 0.675467, mae: 0.563660, mean_q: 4.830621
 75360/100000: episode: 1836, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 15.263, mean reward: 3.053 [2.785, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.434], loss: 0.368734, mae: 0.600146, mean_q: 5.109725
 75369/100000: episode: 1837, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 29.494, mean reward: 3.277 [2.803, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.402], loss: 0.290648, mae: 0.497881, mean_q: 4.711782
 75378/100000: episode: 1838, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 33.919, mean reward: 3.769 [3.051, 6.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.529], loss: 0.401366, mae: 0.568866, mean_q: 4.962589
 75384/100000: episode: 1839, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 76.024, mean reward: 12.671 [4.225, 40.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.947, 10.356], loss: 0.320423, mae: 0.554612, mean_q: 5.040040
 75399/100000: episode: 1840, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 66.971, mean reward: 4.465 [3.315, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.281, 10.474], loss: 0.915939, mae: 0.693810, mean_q: 5.099727
 75405/100000: episode: 1841, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 26.094, mean reward: 4.349 [3.688, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.607], loss: 0.427405, mae: 0.500257, mean_q: 4.659172
 75420/100000: episode: 1842, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 39.147, mean reward: 2.610 [2.100, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.089, 10.336], loss: 0.526599, mae: 0.611100, mean_q: 5.007711
 75425/100000: episode: 1843, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 23.682, mean reward: 4.736 [4.112, 5.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-1.851, 10.582], loss: 0.467367, mae: 0.561807, mean_q: 4.939327
 75440/100000: episode: 1844, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 64.058, mean reward: 4.271 [3.108, 6.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.623], loss: 0.435478, mae: 0.521427, mean_q: 4.948352
 75455/100000: episode: 1845, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 61.788, mean reward: 4.119 [3.193, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.350, 10.459], loss: 0.366775, mae: 0.533795, mean_q: 4.780149
 75470/100000: episode: 1846, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 85.875, mean reward: 5.725 [4.369, 8.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.648], loss: 0.488108, mae: 0.548247, mean_q: 4.872365
 75476/100000: episode: 1847, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 28.228, mean reward: 4.705 [4.025, 6.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.501], loss: 0.276778, mae: 0.490864, mean_q: 5.029197
 75481/100000: episode: 1848, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 25.021, mean reward: 5.004 [3.191, 9.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.170, 10.349], loss: 0.628382, mae: 0.584722, mean_q: 4.753484
 75486/100000: episode: 1849, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 15.412, mean reward: 3.082 [2.936, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.492], loss: 0.264558, mae: 0.529034, mean_q: 5.028424
 75499/100000: episode: 1850, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 52.917, mean reward: 4.071 [3.345, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.594], loss: 0.302906, mae: 0.511200, mean_q: 5.045575
 75510/100000: episode: 1851, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 54.724, mean reward: 4.975 [4.259, 7.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.236, 10.552], loss: 0.492687, mae: 0.600881, mean_q: 5.130386
 75525/100000: episode: 1852, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 57.143, mean reward: 3.810 [2.805, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.394, 10.526], loss: 1.740091, mae: 0.686917, mean_q: 4.822351
 75536/100000: episode: 1853, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 39.556, mean reward: 3.596 [2.481, 4.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.258, 10.511], loss: 0.921066, mae: 0.715078, mean_q: 5.318294
 75551/100000: episode: 1854, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 48.511, mean reward: 3.234 [2.448, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.351], loss: 0.572886, mae: 0.621211, mean_q: 5.051754
 75566/100000: episode: 1855, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 71.708, mean reward: 4.781 [3.323, 7.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.166, 10.547], loss: 0.334249, mae: 0.537556, mean_q: 4.948137
 75582/100000: episode: 1856, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 107.768, mean reward: 6.735 [3.259, 15.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.508, 10.552], loss: 0.353961, mae: 0.521891, mean_q: 4.997703
 75598/100000: episode: 1857, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 65.081, mean reward: 4.068 [3.263, 5.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.614], loss: 0.581155, mae: 0.625394, mean_q: 5.178587
 75604/100000: episode: 1858, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 20.403, mean reward: 3.401 [2.849, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.509], loss: 0.553727, mae: 0.682291, mean_q: 5.174002
 75619/100000: episode: 1859, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 75.040, mean reward: 5.003 [3.431, 7.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.930, 10.520], loss: 0.394721, mae: 0.541291, mean_q: 5.039727
 75628/100000: episode: 1860, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 36.418, mean reward: 4.046 [3.212, 4.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.585], loss: 0.429114, mae: 0.543196, mean_q: 5.134761
 75634/100000: episode: 1861, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 21.651, mean reward: 3.608 [3.059, 4.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.379], loss: 0.227029, mae: 0.483612, mean_q: 5.035738
 75643/100000: episode: 1862, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 40.398, mean reward: 4.489 [3.413, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.559], loss: 0.565595, mae: 0.594363, mean_q: 5.126976
 75648/100000: episode: 1863, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 16.527, mean reward: 3.305 [3.141, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.546, 10.480], loss: 0.353904, mae: 0.610251, mean_q: 5.238775
 75657/100000: episode: 1864, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 41.858, mean reward: 4.651 [4.279, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.540], loss: 0.313230, mae: 0.567509, mean_q: 5.061039
 75662/100000: episode: 1865, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 20.756, mean reward: 4.151 [3.603, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.517], loss: 4.294345, mae: 0.772396, mean_q: 5.029819
 75667/100000: episode: 1866, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 18.350, mean reward: 3.670 [3.046, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.573], loss: 0.417505, mae: 0.675281, mean_q: 5.368111
 75680/100000: episode: 1867, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 52.446, mean reward: 4.034 [2.902, 5.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.261, 10.463], loss: 0.707920, mae: 0.690817, mean_q: 5.154524
 75693/100000: episode: 1868, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 61.948, mean reward: 4.765 [3.602, 6.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-1.160, 10.574], loss: 2.162633, mae: 0.770174, mean_q: 5.208480
 75698/100000: episode: 1869, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 18.511, mean reward: 3.702 [3.517, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.526], loss: 1.360234, mae: 0.754293, mean_q: 5.316776
 75713/100000: episode: 1870, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 65.309, mean reward: 4.354 [3.444, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.253, 10.517], loss: 0.461475, mae: 0.613125, mean_q: 5.073443
 75715/100000: episode: 1871, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 6.824, mean reward: 3.412 [3.069, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.505], loss: 0.522429, mae: 0.667830, mean_q: 5.177648
 75724/100000: episode: 1872, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 34.196, mean reward: 3.800 [3.090, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.407, 10.558], loss: 0.618522, mae: 0.716164, mean_q: 5.355846
 75729/100000: episode: 1873, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 17.610, mean reward: 3.522 [2.815, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.580], loss: 0.245020, mae: 0.503635, mean_q: 5.135443
 75740/100000: episode: 1874, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 56.955, mean reward: 5.178 [3.829, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.589], loss: 0.449840, mae: 0.599120, mean_q: 5.201005
 75751/100000: episode: 1875, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 70.858, mean reward: 6.442 [4.152, 10.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.199, 10.615], loss: 0.344922, mae: 0.548092, mean_q: 5.155242
 75760/100000: episode: 1876, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 32.468, mean reward: 3.608 [2.523, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.230, 10.433], loss: 0.491134, mae: 0.593403, mean_q: 5.298579
 75775/100000: episode: 1877, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 67.454, mean reward: 4.497 [3.340, 6.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.636], loss: 0.686252, mae: 0.630794, mean_q: 5.239389
 75784/100000: episode: 1878, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 38.141, mean reward: 4.238 [3.632, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.508], loss: 0.286909, mae: 0.538927, mean_q: 5.232944
 75795/100000: episode: 1879, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 45.433, mean reward: 4.130 [3.011, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.568], loss: 0.975459, mae: 0.694440, mean_q: 5.481944
 75811/100000: episode: 1880, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 55.161, mean reward: 3.448 [2.553, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.489], loss: 0.452135, mae: 0.593800, mean_q: 5.192492
 75824/100000: episode: 1881, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 45.754, mean reward: 3.520 [2.785, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.379, 10.358], loss: 0.447612, mae: 0.653569, mean_q: 5.320217
 75839/100000: episode: 1882, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 70.218, mean reward: 4.681 [3.423, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.253, 10.472], loss: 0.669178, mae: 0.632254, mean_q: 5.171674
 75852/100000: episode: 1883, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 54.766, mean reward: 4.213 [2.829, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.481], loss: 0.311849, mae: 0.528264, mean_q: 5.225715
 75867/100000: episode: 1884, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 71.436, mean reward: 4.762 [3.175, 8.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.618], loss: 0.419917, mae: 0.561201, mean_q: 5.192490
 75882/100000: episode: 1885, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 50.114, mean reward: 3.341 [2.103, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.397], loss: 0.933049, mae: 0.730424, mean_q: 5.588362
 75884/100000: episode: 1886, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 7.314, mean reward: 3.657 [3.464, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.035, 10.539], loss: 0.377212, mae: 0.591552, mean_q: 5.324594
 75895/100000: episode: 1887, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 46.011, mean reward: 4.183 [3.080, 7.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.630], loss: 0.468022, mae: 0.624996, mean_q: 5.208906
 75897/100000: episode: 1888, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 7.353, mean reward: 3.676 [3.545, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.507], loss: 0.241665, mae: 0.501149, mean_q: 5.335493
 75912/100000: episode: 1889, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 76.284, mean reward: 5.086 [3.251, 9.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.526], loss: 0.858841, mae: 0.673919, mean_q: 5.463463
 75923/100000: episode: 1890, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 36.950, mean reward: 3.359 [2.681, 5.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.469, 10.329], loss: 0.573513, mae: 0.645980, mean_q: 5.403930
 75932/100000: episode: 1891, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 42.691, mean reward: 4.743 [3.511, 6.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.279, 10.635], loss: 0.678342, mae: 0.697842, mean_q: 5.382981
 75947/100000: episode: 1892, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 49.767, mean reward: 3.318 [2.660, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.309, 10.455], loss: 0.563479, mae: 0.626160, mean_q: 5.380302
 75953/100000: episode: 1893, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 28.738, mean reward: 4.790 [3.446, 7.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.411], loss: 0.525150, mae: 0.782215, mean_q: 5.839859
 75969/100000: episode: 1894, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 106.673, mean reward: 6.667 [3.525, 13.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.262, 10.572], loss: 0.795120, mae: 0.680095, mean_q: 5.409311
 75971/100000: episode: 1895, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 8.948, mean reward: 4.474 [3.420, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.405], loss: 0.417719, mae: 0.629543, mean_q: 5.521039
 75987/100000: episode: 1896, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 97.271, mean reward: 6.079 [4.211, 12.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.344, 10.709], loss: 2.104526, mae: 0.752554, mean_q: 5.605389
 75996/100000: episode: 1897, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 31.875, mean reward: 3.542 [2.626, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.490], loss: 0.443888, mae: 0.603197, mean_q: 5.384519
 76011/100000: episode: 1898, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 61.421, mean reward: 4.095 [3.202, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.134, 10.596], loss: 0.372529, mae: 0.584047, mean_q: 5.473903
[Info] 4-TH LEVEL FOUND: 9.428594589233398, Considering 10/90 traces
 76017/100000: episode: 1899, duration: 4.111s, episode steps: 6, steps per second: 1, episode reward: 40.777, mean reward: 6.796 [3.901, 11.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.651], loss: 0.615421, mae: 0.635703, mean_q: 5.359391
 76025/100000: episode: 1900, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 29.942, mean reward: 3.743 [2.948, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.472], loss: 3.759352, mae: 0.961261, mean_q: 5.955935
[Info] FALSIFICATION!
 76032/100000: episode: 1901, duration: 0.213s, episode steps: 7, steps per second: 33, episode reward: 1112.509, mean reward: 158.930 [3.793, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.015, 10.764], loss: 1.104649, mae: 0.726208, mean_q: 5.618102
 76035/100000: episode: 1902, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 14.578, mean reward: 4.859 [4.161, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.401], loss: 0.522440, mae: 0.642153, mean_q: 5.297556
[Info] FALSIFICATION!
 76043/100000: episode: 1903, duration: 0.218s, episode steps: 8, steps per second: 37, episode reward: 1337.370, mean reward: 167.171 [7.270, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.071, 10.770], loss: 0.637644, mae: 0.767000, mean_q: 5.844286
 76051/100000: episode: 1904, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 38.383, mean reward: 4.798 [3.905, 5.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.468], loss: 0.502272, mae: 0.648000, mean_q: 5.733239
 76064/100000: episode: 1905, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 139.374, mean reward: 10.721 [6.656, 25.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.322, 10.739], loss: 8.325067, mae: 1.267623, mean_q: 6.187112
 76077/100000: episode: 1906, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 83.942, mean reward: 6.457 [3.725, 14.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.584, 10.496], loss: 1185.515259, mae: 5.017689, mean_q: 7.614273
 76080/100000: episode: 1907, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 14.253, mean reward: 4.751 [4.346, 5.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.571], loss: 8.388009, mae: 2.989938, mean_q: 8.603378
 76093/100000: episode: 1908, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 60.543, mean reward: 4.657 [3.263, 6.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.520], loss: 3.798772, mae: 1.782049, mean_q: 4.019688
[Info] FALSIFICATION!
 76097/100000: episode: 1909, duration: 0.299s, episode steps: 4, steps per second: 13, episode reward: 1099.354, mean reward: 274.838 [11.473, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.015, 10.733], loss: 1.947753, mae: 1.394059, mean_q: 4.320916
 76107/100000: episode: 1910, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 40.968, mean reward: 4.097 [3.723, 5.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.582], loss: 2.247551, mae: 1.085699, mean_q: 4.988564
 76110/100000: episode: 1911, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 11.558, mean reward: 3.853 [3.663, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.337], loss: 0.961409, mae: 0.972133, mean_q: 5.609886
 76115/100000: episode: 1912, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 23.132, mean reward: 4.626 [3.727, 5.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.497], loss: 0.960807, mae: 0.926500, mean_q: 5.765102
 76118/100000: episode: 1913, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 13.330, mean reward: 4.443 [4.396, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.120, 10.555], loss: 0.870195, mae: 0.860250, mean_q: 5.998815
 76123/100000: episode: 1914, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 19.528, mean reward: 3.906 [3.167, 4.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.590, 10.449], loss: 19.360825, mae: 1.656565, mean_q: 6.198888
 76128/100000: episode: 1915, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 17.073, mean reward: 3.415 [3.256, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.468], loss: 1.177459, mae: 1.201044, mean_q: 6.301607
 76131/100000: episode: 1916, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 18.618, mean reward: 6.206 [5.536, 7.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.608], loss: 1.208918, mae: 1.073408, mean_q: 5.926261
 76144/100000: episode: 1917, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 137.517, mean reward: 10.578 [4.562, 30.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.219, 10.365], loss: 0.430837, mae: 0.712041, mean_q: 5.528887
 76146/100000: episode: 1918, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 12.158, mean reward: 6.079 [5.441, 6.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.418], loss: 0.697997, mae: 0.851846, mean_q: 5.240173
 76149/100000: episode: 1919, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 52.581, mean reward: 17.527 [9.723, 26.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.035, 10.755], loss: 5153.004395, mae: 11.164210, mean_q: 5.483809
 76157/100000: episode: 1920, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 43.203, mean reward: 5.400 [3.318, 9.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.605], loss: 13.417356, mae: 2.529732, mean_q: 7.443478
 76162/100000: episode: 1921, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 18.597, mean reward: 3.719 [3.020, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.390], loss: 7.673421, mae: 3.168673, mean_q: 8.601969
 76170/100000: episode: 1922, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 36.799, mean reward: 4.600 [3.498, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.402, 10.572], loss: 73.257057, mae: 2.691308, mean_q: 7.377278
 76178/100000: episode: 1923, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 35.733, mean reward: 4.467 [3.610, 6.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.740, 10.468], loss: 1910.051758, mae: 4.966571, mean_q: 6.231391
 76186/100000: episode: 1924, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 32.115, mean reward: 4.014 [3.706, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.110, 10.515], loss: 1905.386230, mae: 5.749059, mean_q: 7.564082
 76197/100000: episode: 1925, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 47.489, mean reward: 4.317 [3.325, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.565], loss: 6.524463, mae: 3.057483, mean_q: 8.963140
 76202/100000: episode: 1926, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 24.334, mean reward: 4.867 [3.591, 7.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.671], loss: 1.838207, mae: 1.501592, mean_q: 7.226865
 76207/100000: episode: 1927, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 23.751, mean reward: 4.750 [4.210, 4.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.837, 10.539], loss: 14.346698, mae: 1.200184, mean_q: 6.205265
 76217/100000: episode: 1928, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 59.038, mean reward: 5.904 [3.667, 14.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-1.286, 10.519], loss: 11.728492, mae: 1.158838, mean_q: 5.968615
 76220/100000: episode: 1929, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 10.727, mean reward: 3.576 [3.305, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.483], loss: 0.390105, mae: 0.575013, mean_q: 6.216767
 76223/100000: episode: 1930, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 12.746, mean reward: 4.249 [4.032, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.497], loss: 0.415649, mae: 0.656166, mean_q: 5.858578
 76225/100000: episode: 1931, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 11.989, mean reward: 5.995 [5.387, 6.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.506, 10.355], loss: 1.983020, mae: 1.126720, mean_q: 6.083515
 76233/100000: episode: 1932, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 88.018, mean reward: 11.002 [4.186, 23.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.756], loss: 1.488767, mae: 0.866785, mean_q: 5.951608
 76236/100000: episode: 1933, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 15.648, mean reward: 5.216 [4.761, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.048, 10.429], loss: 0.541994, mae: 0.741427, mean_q: 6.301471
 76249/100000: episode: 1934, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 49.916, mean reward: 3.840 [2.130, 5.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.058, 10.559], loss: 2.137357, mae: 0.972681, mean_q: 6.194221
 76260/100000: episode: 1935, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 77.820, mean reward: 7.075 [4.416, 11.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.348, 10.586], loss: 1.356871, mae: 0.927902, mean_q: 6.379901
 76263/100000: episode: 1936, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 11.490, mean reward: 3.830 [3.554, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.466], loss: 1.174589, mae: 0.847583, mean_q: 6.034990
 76273/100000: episode: 1937, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 62.939, mean reward: 6.294 [3.764, 13.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.481], loss: 8.762062, mae: 1.122000, mean_q: 6.233279
 76283/100000: episode: 1938, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 83.430, mean reward: 8.343 [4.621, 19.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.891, 10.559], loss: 0.925272, mae: 0.761471, mean_q: 5.870933
 76286/100000: episode: 1939, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 16.688, mean reward: 5.563 [4.744, 6.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-1.614, 10.646], loss: 1.422732, mae: 0.960931, mean_q: 6.472292
 76294/100000: episode: 1940, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 42.367, mean reward: 5.296 [3.848, 7.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.080, 10.567], loss: 0.916143, mae: 0.839465, mean_q: 6.146224
 76302/100000: episode: 1941, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 103.483, mean reward: 12.935 [6.242, 37.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.755], loss: 1.348698, mae: 0.822412, mean_q: 6.053175
 76307/100000: episode: 1942, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 29.961, mean reward: 5.992 [4.767, 7.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.663], loss: 1.093578, mae: 0.897208, mean_q: 6.495817
 76312/100000: episode: 1943, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 19.850, mean reward: 3.970 [3.413, 5.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.125, 10.580], loss: 119.448875, mae: 2.505827, mean_q: 6.728523
 76315/100000: episode: 1944, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 14.984, mean reward: 4.995 [4.561, 5.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.504], loss: 1.576796, mae: 1.383364, mean_q: 6.753554
 76320/100000: episode: 1945, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 19.758, mean reward: 3.952 [3.421, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.950, 10.470], loss: 1.126468, mae: 1.167848, mean_q: 6.918426
 76323/100000: episode: 1946, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 14.216, mean reward: 4.739 [4.299, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.499], loss: 1.274997, mae: 1.097979, mean_q: 6.933554
 76325/100000: episode: 1947, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 12.492, mean reward: 6.246 [6.106, 6.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.473], loss: 0.611776, mae: 0.774284, mean_q: 6.276619
 76335/100000: episode: 1948, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 64.657, mean reward: 6.466 [4.230, 8.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.154, 10.595], loss: 1530.969727, mae: 4.829506, mean_q: 7.567837
 76340/100000: episode: 1949, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 38.141, mean reward: 7.628 [5.672, 8.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.637], loss: 15.870264, mae: 2.732587, mean_q: 8.105401
 76348/100000: episode: 1950, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 65.739, mean reward: 8.217 [5.930, 10.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.609], loss: 12.853037, mae: 2.134025, mean_q: 7.944273
 76351/100000: episode: 1951, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 9.828, mean reward: 3.276 [3.195, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.417], loss: 1.047569, mae: 1.047557, mean_q: 6.993783
 76362/100000: episode: 1952, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 65.281, mean reward: 5.935 [4.524, 9.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.773, 10.471], loss: 1393.281372, mae: 3.634110, mean_q: 6.235782
 76367/100000: episode: 1953, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 28.632, mean reward: 5.726 [4.223, 7.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.447, 10.569], loss: 2.310653, mae: 1.510335, mean_q: 7.335117
 76372/100000: episode: 1954, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 28.214, mean reward: 5.643 [3.827, 9.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.577], loss: 3.368360, mae: 2.109738, mean_q: 8.510061
 76375/100000: episode: 1955, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 18.166, mean reward: 6.055 [4.009, 7.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.609], loss: 2.836994, mae: 1.847187, mean_q: 7.876578
 76388/100000: episode: 1956, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 75.028, mean reward: 5.771 [3.235, 9.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.531], loss: 1189.517090, mae: 4.652261, mean_q: 8.195344
 76398/100000: episode: 1957, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 177.869, mean reward: 17.787 [4.271, 53.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.721], loss: 3.067493, mae: 1.796775, mean_q: 8.137403
 76406/100000: episode: 1958, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 30.091, mean reward: 3.761 [3.095, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.492], loss: 2.147588, mae: 1.110183, mean_q: 6.902356
 76411/100000: episode: 1959, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 23.297, mean reward: 4.659 [3.413, 6.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.518], loss: 1.958134, mae: 1.102640, mean_q: 6.443655
 76422/100000: episode: 1960, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 226.869, mean reward: 20.624 [3.954, 152.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.312, 10.779], loss: 1395.921021, mae: 3.978057, mean_q: 6.792217
 76435/100000: episode: 1961, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 300.329, mean reward: 23.102 [6.313, 196.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.245, 10.669], loss: 3.795951, mae: 2.061300, mean_q: 8.449892
 76438/100000: episode: 1962, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 14.880, mean reward: 4.960 [4.270, 5.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.535], loss: 1.938012, mae: 1.393285, mean_q: 7.835098
 76449/100000: episode: 1963, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 46.543, mean reward: 4.231 [3.493, 4.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.594], loss: 8.867240, mae: 1.293947, mean_q: 6.822997
 76462/100000: episode: 1964, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 126.414, mean reward: 9.724 [4.890, 14.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.056, 10.644], loss: 1.347387, mae: 0.994176, mean_q: 6.592239
 76467/100000: episode: 1965, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 42.193, mean reward: 8.439 [6.590, 11.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.146, 10.396], loss: 1.244451, mae: 0.880695, mean_q: 6.696227
 76470/100000: episode: 1966, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 12.548, mean reward: 4.183 [3.648, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.422], loss: 1.889841, mae: 1.126455, mean_q: 6.593378
 76475/100000: episode: 1967, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 72.162, mean reward: 14.432 [5.422, 40.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.479], loss: 2.596519, mae: 1.105268, mean_q: 6.575607
 76480/100000: episode: 1968, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 20.312, mean reward: 4.062 [3.076, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.591], loss: 0.808551, mae: 0.688009, mean_q: 6.305635
 76483/100000: episode: 1969, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 14.463, mean reward: 4.821 [3.869, 6.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.583], loss: 0.797588, mae: 0.780275, mean_q: 6.195276
 76496/100000: episode: 1970, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 92.345, mean reward: 7.103 [3.200, 12.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.902, 10.581], loss: 1178.407349, mae: 3.953340, mean_q: 7.436055
 76506/100000: episode: 1971, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 39.168, mean reward: 3.917 [3.470, 5.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.725, 10.588], loss: 62.985870, mae: 3.063085, mean_q: 8.838301
 76509/100000: episode: 1972, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 18.898, mean reward: 6.299 [5.842, 6.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.618], loss: 4.156142, mae: 1.716089, mean_q: 8.213811
 76514/100000: episode: 1973, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 23.044, mean reward: 4.609 [3.526, 5.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.143, 10.490], loss: 62.279987, mae: 2.374444, mean_q: 8.094888
 76517/100000: episode: 1974, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 18.832, mean reward: 6.277 [5.939, 6.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.616], loss: 28.963661, mae: 1.777623, mean_q: 7.331861
 76520/100000: episode: 1975, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 13.282, mean reward: 4.427 [3.934, 5.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.441], loss: 1.022986, mae: 1.024142, mean_q: 7.567972
 76523/100000: episode: 1976, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 10.353, mean reward: 3.451 [3.267, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.035, 10.521], loss: 13.432427, mae: 1.590999, mean_q: 7.286167
 76525/100000: episode: 1977, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: 8.089, mean reward: 4.044 [3.765, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.035, 10.522], loss: 2.447524, mae: 1.150755, mean_q: 7.243072
 76536/100000: episode: 1978, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 53.854, mean reward: 4.896 [3.956, 7.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.542], loss: 4.489692, mae: 1.044048, mean_q: 6.817565
 76549/100000: episode: 1979, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 75.042, mean reward: 5.772 [3.825, 10.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.516], loss: 1.305961, mae: 0.879295, mean_q: 6.957100
 76560/100000: episode: 1980, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 49.826, mean reward: 4.530 [3.546, 6.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.478], loss: 54.102810, mae: 1.567244, mean_q: 7.027983
 76563/100000: episode: 1981, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 10.768, mean reward: 3.589 [3.473, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.493], loss: 100.832741, mae: 2.427197, mean_q: 6.764669
 76566/100000: episode: 1982, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 11.733, mean reward: 3.911 [3.523, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.435], loss: 2.394531, mae: 1.265823, mean_q: 7.244031
 76571/100000: episode: 1983, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 38.200, mean reward: 7.640 [3.837, 11.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.684], loss: 60.316162, mae: 2.003569, mean_q: 7.611797
 76582/100000: episode: 1984, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 69.173, mean reward: 6.288 [3.955, 16.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.172, 10.638], loss: 1393.399780, mae: 3.902274, mean_q: 7.383786
 76587/100000: episode: 1985, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 31.972, mean reward: 6.394 [4.429, 9.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.682], loss: 231.244781, mae: 5.519317, mean_q: 10.094054
 76595/100000: episode: 1986, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 223.665, mean reward: 27.958 [9.584, 89.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.747], loss: 13.430736, mae: 4.296420, mean_q: 11.310826
 76608/100000: episode: 1987, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 129.921, mean reward: 9.994 [5.663, 14.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.092, 10.670], loss: 3.798426, mae: 1.519962, mean_q: 7.560662
 76621/100000: episode: 1988, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 71.879, mean reward: 5.529 [4.078, 7.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.577], loss: 3.209179, mae: 1.491616, mean_q: 6.306064
[Info] Complete ISplit Iteration
[Info] Levels: [4.851012, 6.135797, 8.004881, 9.428595, 13.434928]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.74]
[Info] Error Prob: 7.400000000000002e-05

 76629/100000: episode: 1989, duration: 4.293s, episode steps: 8, steps per second: 2, episode reward: 38.416, mean reward: 4.802 [4.404, 5.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.449, 10.545], loss: 76.996185, mae: 1.888108, mean_q: 6.774881
 76729/100000: episode: 1990, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 189.293, mean reward: 1.893 [1.476, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.222, 10.098], loss: 482.842896, mae: 3.622273, mean_q: 8.996866
 76829/100000: episode: 1991, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 189.829, mean reward: 1.898 [1.477, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.760, 10.163], loss: 317.031067, mae: 2.159564, mean_q: 8.201830
 76929/100000: episode: 1992, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 178.206, mean reward: 1.782 [1.438, 2.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.810, 10.098], loss: 324.521027, mae: 2.587038, mean_q: 8.461137
 77029/100000: episode: 1993, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 191.139, mean reward: 1.911 [1.444, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.196, 10.098], loss: 329.385223, mae: 2.668410, mean_q: 8.497385
 77129/100000: episode: 1994, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 232.993, mean reward: 2.330 [1.464, 6.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.694, 10.635], loss: 175.292404, mae: 2.105645, mean_q: 8.021817
 77229/100000: episode: 1995, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.418, mean reward: 1.904 [1.486, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.766, 10.098], loss: 773.364136, mae: 3.876786, mean_q: 9.016369
 77329/100000: episode: 1996, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.201, mean reward: 2.002 [1.471, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.814, 10.098], loss: 173.945358, mae: 2.120605, mean_q: 8.155693
 77429/100000: episode: 1997, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 189.923, mean reward: 1.899 [1.465, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.938, 10.133], loss: 172.145691, mae: 1.810869, mean_q: 7.499761
 77529/100000: episode: 1998, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 186.352, mean reward: 1.864 [1.458, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.519, 10.272], loss: 158.214432, mae: 1.983709, mean_q: 7.655786
 77629/100000: episode: 1999, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 209.803, mean reward: 2.098 [1.507, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.716, 10.211], loss: 177.427719, mae: 1.935333, mean_q: 7.179749
 77729/100000: episode: 2000, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: 200.922, mean reward: 2.009 [1.444, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.971, 10.137], loss: 621.475403, mae: 3.327798, mean_q: 8.369641
 77829/100000: episode: 2001, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 190.715, mean reward: 1.907 [1.473, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.252, 10.232], loss: 315.199005, mae: 2.393699, mean_q: 7.950004
 77929/100000: episode: 2002, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 202.622, mean reward: 2.026 [1.458, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.538, 10.098], loss: 335.944061, mae: 2.601759, mean_q: 7.975632
 78029/100000: episode: 2003, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 201.576, mean reward: 2.016 [1.473, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.281, 10.098], loss: 161.695984, mae: 1.740163, mean_q: 7.226480
 78129/100000: episode: 2004, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 200.159, mean reward: 2.002 [1.479, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.499, 10.098], loss: 794.593018, mae: 4.038878, mean_q: 8.514346
 78229/100000: episode: 2005, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.826, mean reward: 1.878 [1.448, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.905, 10.285], loss: 12.758955, mae: 1.756216, mean_q: 7.795795
 78329/100000: episode: 2006, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 193.487, mean reward: 1.935 [1.552, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.000, 10.098], loss: 310.496643, mae: 2.262767, mean_q: 7.688849
 78429/100000: episode: 2007, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 210.995, mean reward: 2.110 [1.466, 8.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.290, 10.098], loss: 470.621124, mae: 2.686228, mean_q: 7.756186
 78529/100000: episode: 2008, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 209.965, mean reward: 2.100 [1.473, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.744, 10.098], loss: 618.335571, mae: 3.663700, mean_q: 8.879609
 78629/100000: episode: 2009, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.951, mean reward: 1.920 [1.493, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.986, 10.098], loss: 168.031448, mae: 1.780958, mean_q: 7.552293
 78729/100000: episode: 2010, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 186.355, mean reward: 1.864 [1.471, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.391, 10.398], loss: 164.985428, mae: 1.782252, mean_q: 7.180202
 78829/100000: episode: 2011, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 183.912, mean reward: 1.839 [1.471, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.569, 10.098], loss: 337.493042, mae: 2.190452, mean_q: 7.084269
 78929/100000: episode: 2012, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.676, mean reward: 1.867 [1.477, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.747, 10.140], loss: 15.364128, mae: 1.794061, mean_q: 7.530802
 79029/100000: episode: 2013, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 207.198, mean reward: 2.072 [1.451, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.398, 10.228], loss: 320.636139, mae: 2.441430, mean_q: 7.487437
 79129/100000: episode: 2014, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 193.248, mean reward: 1.932 [1.514, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.326], loss: 336.564545, mae: 2.593479, mean_q: 7.560996
 79229/100000: episode: 2015, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 188.977, mean reward: 1.890 [1.474, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.490, 10.142], loss: 467.968719, mae: 2.665750, mean_q: 7.641864
 79329/100000: episode: 2016, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.737, mean reward: 1.917 [1.490, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.241, 10.113], loss: 474.253021, mae: 2.772897, mean_q: 7.769186
 79429/100000: episode: 2017, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.051, mean reward: 1.881 [1.461, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.406, 10.231], loss: 177.594574, mae: 2.038368, mean_q: 7.219306
 79529/100000: episode: 2018, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.940, mean reward: 1.939 [1.449, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.810, 10.098], loss: 466.696381, mae: 2.412230, mean_q: 7.282347
 79629/100000: episode: 2019, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 197.413, mean reward: 1.974 [1.475, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.235], loss: 314.544281, mae: 2.381340, mean_q: 7.478679
 79729/100000: episode: 2020, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.045, mean reward: 1.970 [1.438, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.273, 10.297], loss: 611.732178, mae: 3.215794, mean_q: 7.612938
 79829/100000: episode: 2021, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.774, mean reward: 1.988 [1.444, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.460, 10.098], loss: 171.090973, mae: 1.853999, mean_q: 6.944984
 79929/100000: episode: 2022, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 186.702, mean reward: 1.867 [1.492, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.366, 10.098], loss: 317.086365, mae: 2.260819, mean_q: 7.083997
 80029/100000: episode: 2023, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 200.690, mean reward: 2.007 [1.454, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.098], loss: 769.918335, mae: 3.666427, mean_q: 7.857925
 80129/100000: episode: 2024, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.019, mean reward: 1.840 [1.433, 2.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.278, 10.179], loss: 21.991863, mae: 1.294773, mean_q: 6.483446
 80229/100000: episode: 2025, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 189.230, mean reward: 1.892 [1.505, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.387, 10.098], loss: 470.476044, mae: 2.566299, mean_q: 6.846957
 80329/100000: episode: 2026, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.577, mean reward: 1.966 [1.465, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.793, 10.098], loss: 469.243195, mae: 2.638541, mean_q: 7.103889
 80429/100000: episode: 2027, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 187.837, mean reward: 1.878 [1.449, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.678, 10.098], loss: 312.426300, mae: 2.199558, mean_q: 7.032736
 80529/100000: episode: 2028, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 179.552, mean reward: 1.796 [1.457, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.192], loss: 607.033264, mae: 2.578345, mean_q: 6.738920
 80629/100000: episode: 2029, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 209.690, mean reward: 2.097 [1.455, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.061, 10.149], loss: 167.777496, mae: 1.615631, mean_q: 6.395656
 80729/100000: episode: 2030, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 180.788, mean reward: 1.808 [1.465, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.507, 10.098], loss: 768.555420, mae: 3.403655, mean_q: 6.862614
 80829/100000: episode: 2031, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 183.136, mean reward: 1.831 [1.454, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.098], loss: 469.773621, mae: 2.639352, mean_q: 6.731718
 80929/100000: episode: 2032, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 296.914, mean reward: 2.969 [1.498, 7.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.156, 10.098], loss: 611.730469, mae: 3.107833, mean_q: 7.176782
 81029/100000: episode: 2033, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 181.954, mean reward: 1.820 [1.470, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.298, 10.389], loss: 13.083169, mae: 1.203112, mean_q: 5.928598
 81129/100000: episode: 2034, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.777, mean reward: 1.888 [1.438, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.471, 10.098], loss: 1.455017, mae: 0.632776, mean_q: 5.038300
 81229/100000: episode: 2035, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.417, mean reward: 1.854 [1.480, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.343, 10.160], loss: 3.677504, mae: 0.543748, mean_q: 4.645504
 81329/100000: episode: 2036, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 202.384, mean reward: 2.024 [1.541, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.149, 10.098], loss: 6.194373, mae: 0.543421, mean_q: 4.422997
 81429/100000: episode: 2037, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 174.129, mean reward: 1.741 [1.449, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.958, 10.102], loss: 2.568882, mae: 0.449355, mean_q: 4.216733
 81529/100000: episode: 2038, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 248.353, mean reward: 2.484 [1.654, 7.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.345, 10.341], loss: 0.450460, mae: 0.377841, mean_q: 4.080735
 81629/100000: episode: 2039, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.506, mean reward: 1.885 [1.467, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.678, 10.191], loss: 0.101791, mae: 0.306775, mean_q: 3.885670
 81729/100000: episode: 2040, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 198.783, mean reward: 1.988 [1.449, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.553, 10.098], loss: 0.132781, mae: 0.315629, mean_q: 3.884663
 81829/100000: episode: 2041, duration: 0.464s, episode steps: 100, steps per second: 216, episode reward: 189.821, mean reward: 1.898 [1.475, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.220], loss: 0.114125, mae: 0.315760, mean_q: 3.890357
 81929/100000: episode: 2042, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 189.362, mean reward: 1.894 [1.502, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.318, 10.098], loss: 0.110149, mae: 0.299244, mean_q: 3.869842
 82029/100000: episode: 2043, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.038, mean reward: 1.970 [1.495, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.200, 10.098], loss: 0.124799, mae: 0.309094, mean_q: 3.888767
 82129/100000: episode: 2044, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 190.852, mean reward: 1.909 [1.473, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.285, 10.098], loss: 0.124451, mae: 0.319408, mean_q: 3.889911
 82229/100000: episode: 2045, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.137, mean reward: 2.051 [1.472, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.667, 10.098], loss: 0.106981, mae: 0.302631, mean_q: 3.888255
 82329/100000: episode: 2046, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 193.081, mean reward: 1.931 [1.469, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.000, 10.142], loss: 0.102306, mae: 0.300421, mean_q: 3.890979
 82429/100000: episode: 2047, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 193.381, mean reward: 1.934 [1.442, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.571, 10.224], loss: 0.098167, mae: 0.298893, mean_q: 3.889353
 82529/100000: episode: 2048, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.853, mean reward: 1.889 [1.445, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.959, 10.174], loss: 0.094761, mae: 0.289536, mean_q: 3.863381
 82629/100000: episode: 2049, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 194.787, mean reward: 1.948 [1.478, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.063, 10.098], loss: 0.095319, mae: 0.294357, mean_q: 3.858182
 82729/100000: episode: 2050, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 195.419, mean reward: 1.954 [1.456, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.472, 10.098], loss: 0.093302, mae: 0.296150, mean_q: 3.884733
 82829/100000: episode: 2051, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 190.777, mean reward: 1.908 [1.465, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.171], loss: 0.111964, mae: 0.302516, mean_q: 3.897679
 82929/100000: episode: 2052, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 193.283, mean reward: 1.933 [1.450, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.515, 10.098], loss: 0.111106, mae: 0.310096, mean_q: 3.891000
 83029/100000: episode: 2053, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.103, mean reward: 1.821 [1.446, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.721, 10.098], loss: 0.104531, mae: 0.294355, mean_q: 3.872102
 83129/100000: episode: 2054, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 214.575, mean reward: 2.146 [1.518, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.586, 10.098], loss: 0.109445, mae: 0.307914, mean_q: 3.888374
 83229/100000: episode: 2055, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: 215.933, mean reward: 2.159 [1.475, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.106, 10.098], loss: 0.120584, mae: 0.312941, mean_q: 3.900018
 83329/100000: episode: 2056, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.414, mean reward: 1.904 [1.458, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.284, 10.138], loss: 0.094861, mae: 0.294218, mean_q: 3.886441
 83429/100000: episode: 2057, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 210.136, mean reward: 2.101 [1.502, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.757, 10.301], loss: 0.084279, mae: 0.293160, mean_q: 3.885904
 83529/100000: episode: 2058, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.805, mean reward: 1.998 [1.446, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.507, 10.098], loss: 0.081986, mae: 0.285404, mean_q: 3.877642
 83629/100000: episode: 2059, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 213.376, mean reward: 2.134 [1.483, 6.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.465, 10.402], loss: 0.083916, mae: 0.291236, mean_q: 3.889876
 83729/100000: episode: 2060, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 189.780, mean reward: 1.898 [1.475, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.321, 10.187], loss: 0.084079, mae: 0.284490, mean_q: 3.880005
 83829/100000: episode: 2061, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 194.441, mean reward: 1.944 [1.459, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.109], loss: 0.100694, mae: 0.307926, mean_q: 3.909513
 83929/100000: episode: 2062, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 199.512, mean reward: 1.995 [1.462, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.742, 10.098], loss: 0.096944, mae: 0.300640, mean_q: 3.910439
 84029/100000: episode: 2063, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 179.684, mean reward: 1.797 [1.447, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.711, 10.132], loss: 0.088626, mae: 0.297498, mean_q: 3.894323
 84129/100000: episode: 2064, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 202.899, mean reward: 2.029 [1.458, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.821, 10.098], loss: 0.090482, mae: 0.290883, mean_q: 3.907000
 84229/100000: episode: 2065, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: 212.886, mean reward: 2.129 [1.448, 4.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.129, 10.098], loss: 0.096734, mae: 0.296012, mean_q: 3.903487
 84329/100000: episode: 2066, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: 192.314, mean reward: 1.923 [1.438, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.874, 10.141], loss: 0.098303, mae: 0.298246, mean_q: 3.896041
 84429/100000: episode: 2067, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.906, mean reward: 1.919 [1.456, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.098], loss: 0.094985, mae: 0.293109, mean_q: 3.890348
 84529/100000: episode: 2068, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 189.375, mean reward: 1.894 [1.436, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.844, 10.098], loss: 0.077077, mae: 0.278505, mean_q: 3.886421
 84629/100000: episode: 2069, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 193.860, mean reward: 1.939 [1.483, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.628, 10.206], loss: 0.087615, mae: 0.289409, mean_q: 3.901733
 84729/100000: episode: 2070, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 183.356, mean reward: 1.834 [1.462, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.110], loss: 0.088787, mae: 0.293656, mean_q: 3.894487
 84829/100000: episode: 2071, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 207.935, mean reward: 2.079 [1.454, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.924, 10.155], loss: 0.088141, mae: 0.293728, mean_q: 3.897686
 84929/100000: episode: 2072, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 203.632, mean reward: 2.036 [1.431, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.671, 10.098], loss: 0.091104, mae: 0.297173, mean_q: 3.892790
 85029/100000: episode: 2073, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.728, mean reward: 1.967 [1.452, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.964, 10.098], loss: 0.088087, mae: 0.291454, mean_q: 3.911036
 85129/100000: episode: 2074, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: 239.526, mean reward: 2.395 [1.553, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.640, 10.098], loss: 0.084964, mae: 0.295123, mean_q: 3.916418
 85229/100000: episode: 2075, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 184.294, mean reward: 1.843 [1.435, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.989, 10.098], loss: 0.090270, mae: 0.296548, mean_q: 3.918345
 85329/100000: episode: 2076, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 177.028, mean reward: 1.770 [1.464, 2.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.192, 10.109], loss: 0.096336, mae: 0.300491, mean_q: 3.921122
 85429/100000: episode: 2077, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: 199.192, mean reward: 1.992 [1.456, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.972, 10.098], loss: 0.097701, mae: 0.302951, mean_q: 3.959916
 85529/100000: episode: 2078, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 215.528, mean reward: 2.155 [1.483, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.754, 10.334], loss: 0.092133, mae: 0.301927, mean_q: 3.935086
 85629/100000: episode: 2079, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.415, mean reward: 1.824 [1.459, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.160], loss: 0.089871, mae: 0.302751, mean_q: 3.949276
 85729/100000: episode: 2080, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 195.247, mean reward: 1.952 [1.464, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.714, 10.098], loss: 0.099541, mae: 0.301437, mean_q: 3.948800
 85829/100000: episode: 2081, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.730, mean reward: 1.887 [1.466, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.174], loss: 0.093748, mae: 0.304217, mean_q: 3.954891
 85929/100000: episode: 2082, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.436, mean reward: 1.914 [1.456, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.651, 10.255], loss: 0.078765, mae: 0.282825, mean_q: 3.903077
 86029/100000: episode: 2083, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 185.683, mean reward: 1.857 [1.468, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.877, 10.101], loss: 0.087794, mae: 0.292457, mean_q: 3.900448
 86129/100000: episode: 2084, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.541, mean reward: 1.855 [1.460, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.807, 10.098], loss: 0.091419, mae: 0.294180, mean_q: 3.910498
 86229/100000: episode: 2085, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 189.599, mean reward: 1.896 [1.456, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.417, 10.099], loss: 0.090460, mae: 0.289423, mean_q: 3.887393
 86329/100000: episode: 2086, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 186.022, mean reward: 1.860 [1.471, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.098], loss: 0.088368, mae: 0.293905, mean_q: 3.891708
 86429/100000: episode: 2087, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.264, mean reward: 1.923 [1.442, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.098], loss: 0.085166, mae: 0.294829, mean_q: 3.900462
 86529/100000: episode: 2088, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 191.939, mean reward: 1.919 [1.436, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.582, 10.329], loss: 0.073141, mae: 0.277994, mean_q: 3.865933
[Info] 1-TH LEVEL FOUND: 5.128327369689941, Considering 10/90 traces
 86629/100000: episode: 2089, duration: 4.590s, episode steps: 100, steps per second: 22, episode reward: 189.065, mean reward: 1.891 [1.478, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.927, 10.098], loss: 0.086207, mae: 0.292652, mean_q: 3.872155
 86680/100000: episode: 2090, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 140.830, mean reward: 2.761 [2.076, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.716, 10.100], loss: 0.090128, mae: 0.296427, mean_q: 3.888330
 86723/100000: episode: 2091, duration: 0.211s, episode steps: 43, steps per second: 203, episode reward: 195.181, mean reward: 4.539 [2.324, 13.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.335, 10.100], loss: 0.110473, mae: 0.303438, mean_q: 3.943626
 86817/100000: episode: 2092, duration: 0.473s, episode steps: 94, steps per second: 199, episode reward: 168.843, mean reward: 1.796 [1.475, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-1.030, 10.126], loss: 0.134194, mae: 0.305584, mean_q: 3.918502
 86860/100000: episode: 2093, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 107.357, mean reward: 2.497 [1.905, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.419, 10.100], loss: 0.091081, mae: 0.306057, mean_q: 3.915213
 86880/100000: episode: 2094, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 53.387, mean reward: 2.669 [2.144, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.373, 10.100], loss: 0.093807, mae: 0.314313, mean_q: 3.933600
 86932/100000: episode: 2095, duration: 0.276s, episode steps: 52, steps per second: 189, episode reward: 106.116, mean reward: 2.041 [1.443, 2.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.356, 10.100], loss: 0.112875, mae: 0.304640, mean_q: 3.936175
 86978/100000: episode: 2096, duration: 0.222s, episode steps: 46, steps per second: 207, episode reward: 110.080, mean reward: 2.393 [1.554, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.898, 10.163], loss: 0.130223, mae: 0.323054, mean_q: 3.927069
 87029/100000: episode: 2097, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 153.550, mean reward: 3.011 [1.754, 5.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.416, 10.100], loss: 0.133871, mae: 0.324993, mean_q: 3.960574
 87081/100000: episode: 2098, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 136.828, mean reward: 2.631 [2.021, 6.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.332, 10.100], loss: 0.121759, mae: 0.313441, mean_q: 3.967844
 87132/100000: episode: 2099, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 98.575, mean reward: 1.933 [1.469, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.121, 10.149], loss: 0.101727, mae: 0.309088, mean_q: 3.983813
 87228/100000: episode: 2100, duration: 0.471s, episode steps: 96, steps per second: 204, episode reward: 195.346, mean reward: 2.035 [1.522, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.803, 10.346], loss: 0.111847, mae: 0.314404, mean_q: 3.958771
 87274/100000: episode: 2101, duration: 0.232s, episode steps: 46, steps per second: 198, episode reward: 110.363, mean reward: 2.399 [1.447, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.883, 10.256], loss: 0.134515, mae: 0.329574, mean_q: 4.004909
 87294/100000: episode: 2102, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 45.459, mean reward: 2.273 [2.036, 2.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.386, 10.100], loss: 0.098303, mae: 0.306616, mean_q: 3.989422
 87337/100000: episode: 2103, duration: 0.209s, episode steps: 43, steps per second: 206, episode reward: 111.611, mean reward: 2.596 [1.937, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.367, 10.100], loss: 0.125551, mae: 0.320448, mean_q: 3.989622
 87388/100000: episode: 2104, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 100.121, mean reward: 1.963 [1.465, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.168, 10.186], loss: 0.103636, mae: 0.316322, mean_q: 3.985503
 87482/100000: episode: 2105, duration: 0.466s, episode steps: 94, steps per second: 202, episode reward: 180.890, mean reward: 1.924 [1.510, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.573, 10.100], loss: 0.126694, mae: 0.326872, mean_q: 4.016179
 87533/100000: episode: 2106, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 114.571, mean reward: 2.246 [1.780, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.646, 10.100], loss: 0.084783, mae: 0.294117, mean_q: 3.974398
 87579/100000: episode: 2107, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 132.432, mean reward: 2.879 [1.811, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.607, 10.100], loss: 0.152707, mae: 0.331084, mean_q: 4.035785
 87625/100000: episode: 2108, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 104.988, mean reward: 2.282 [1.443, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.615, 10.100], loss: 0.112236, mae: 0.319055, mean_q: 4.030416
 87636/100000: episode: 2109, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 24.160, mean reward: 2.196 [1.937, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.374], loss: 0.116285, mae: 0.318680, mean_q: 3.957264
 87679/100000: episode: 2110, duration: 0.205s, episode steps: 43, steps per second: 210, episode reward: 82.887, mean reward: 1.928 [1.466, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.407, 10.100], loss: 0.111113, mae: 0.316256, mean_q: 4.019238
 87727/100000: episode: 2111, duration: 0.221s, episode steps: 48, steps per second: 217, episode reward: 122.288, mean reward: 2.548 [1.531, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.862, 10.210], loss: 0.112162, mae: 0.327067, mean_q: 4.018041
 87747/100000: episode: 2112, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 46.859, mean reward: 2.343 [1.672, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.268, 10.100], loss: 0.116423, mae: 0.335257, mean_q: 4.060964
 87758/100000: episode: 2113, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 29.298, mean reward: 2.663 [1.820, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.385, 10.472], loss: 0.238706, mae: 0.339337, mean_q: 4.004658
 87778/100000: episode: 2114, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 50.443, mean reward: 2.522 [1.986, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.556, 10.100], loss: 0.148206, mae: 0.332657, mean_q: 4.080443
 87791/100000: episode: 2115, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 34.133, mean reward: 2.626 [1.971, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.107, 10.365], loss: 0.142807, mae: 0.352916, mean_q: 4.096500
 87839/100000: episode: 2116, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 142.771, mean reward: 2.974 [2.072, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.306, 10.393], loss: 0.124720, mae: 0.340015, mean_q: 4.056577
 87850/100000: episode: 2117, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 28.301, mean reward: 2.573 [2.125, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.504, 10.468], loss: 0.318266, mae: 0.383192, mean_q: 4.056889
 87944/100000: episode: 2118, duration: 0.478s, episode steps: 94, steps per second: 197, episode reward: 174.305, mean reward: 1.854 [1.449, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.611, 10.166], loss: 0.151843, mae: 0.354841, mean_q: 4.063065
 88040/100000: episode: 2119, duration: 0.466s, episode steps: 96, steps per second: 206, episode reward: 185.220, mean reward: 1.929 [1.451, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.786, 10.100], loss: 0.117191, mae: 0.334685, mean_q: 4.091936
 88092/100000: episode: 2120, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 117.057, mean reward: 2.251 [1.485, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.651, 10.100], loss: 0.194835, mae: 0.365407, mean_q: 4.100859
 88188/100000: episode: 2121, duration: 0.478s, episode steps: 96, steps per second: 201, episode reward: 177.548, mean reward: 1.849 [1.439, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.590, 10.166], loss: 0.127124, mae: 0.335581, mean_q: 4.079367
 88284/100000: episode: 2122, duration: 0.484s, episode steps: 96, steps per second: 199, episode reward: 193.787, mean reward: 2.019 [1.480, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.906, 10.297], loss: 0.184986, mae: 0.377444, mean_q: 4.116992
 88380/100000: episode: 2123, duration: 0.487s, episode steps: 96, steps per second: 197, episode reward: 196.422, mean reward: 2.046 [1.493, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-1.243, 10.100], loss: 0.123173, mae: 0.333314, mean_q: 4.074774
 88476/100000: episode: 2124, duration: 0.451s, episode steps: 96, steps per second: 213, episode reward: 197.359, mean reward: 2.056 [1.489, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-2.263, 10.100], loss: 0.142651, mae: 0.330468, mean_q: 4.061053
 88524/100000: episode: 2125, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 140.717, mean reward: 2.932 [1.872, 10.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.927, 10.335], loss: 0.172752, mae: 0.346360, mean_q: 4.080279
 88570/100000: episode: 2126, duration: 0.220s, episode steps: 46, steps per second: 210, episode reward: 132.264, mean reward: 2.875 [1.491, 10.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.254, 10.100], loss: 0.121280, mae: 0.337703, mean_q: 4.070256
 88664/100000: episode: 2127, duration: 0.444s, episode steps: 94, steps per second: 212, episode reward: 194.473, mean reward: 2.069 [1.464, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.447, 10.100], loss: 0.161707, mae: 0.355702, mean_q: 4.100888
 88684/100000: episode: 2128, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 57.099, mean reward: 2.855 [2.040, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.267, 10.100], loss: 0.143581, mae: 0.329381, mean_q: 4.107106
 88778/100000: episode: 2129, duration: 0.467s, episode steps: 94, steps per second: 201, episode reward: 170.564, mean reward: 1.815 [1.461, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-1.340, 10.150], loss: 0.129890, mae: 0.342315, mean_q: 4.087227
 88789/100000: episode: 2130, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 22.531, mean reward: 2.048 [1.800, 2.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.291], loss: 0.186148, mae: 0.407790, mean_q: 4.193010
 88841/100000: episode: 2131, duration: 0.265s, episode steps: 52, steps per second: 196, episode reward: 171.181, mean reward: 3.292 [2.387, 6.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.925, 10.100], loss: 0.135731, mae: 0.344911, mean_q: 4.129612
 88887/100000: episode: 2132, duration: 0.218s, episode steps: 46, steps per second: 211, episode reward: 120.090, mean reward: 2.611 [1.456, 5.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.087, 10.126], loss: 0.174112, mae: 0.359807, mean_q: 4.139417
 88938/100000: episode: 2133, duration: 0.240s, episode steps: 51, steps per second: 212, episode reward: 251.140, mean reward: 4.924 [2.046, 35.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.844, 10.100], loss: 0.475808, mae: 0.395146, mean_q: 4.209623
 88981/100000: episode: 2134, duration: 0.210s, episode steps: 43, steps per second: 204, episode reward: 97.134, mean reward: 2.259 [1.585, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.432, 10.100], loss: 0.196952, mae: 0.404003, mean_q: 4.169825
 89024/100000: episode: 2135, duration: 0.251s, episode steps: 43, steps per second: 171, episode reward: 120.021, mean reward: 2.791 [1.902, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.811, 10.100], loss: 0.426708, mae: 0.375827, mean_q: 4.199620
 89120/100000: episode: 2136, duration: 0.460s, episode steps: 96, steps per second: 209, episode reward: 183.743, mean reward: 1.914 [1.447, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.220, 10.100], loss: 0.228658, mae: 0.407833, mean_q: 4.217789
 89216/100000: episode: 2137, duration: 0.494s, episode steps: 96, steps per second: 194, episode reward: 179.153, mean reward: 1.866 [1.481, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.605, 10.100], loss: 0.233360, mae: 0.386457, mean_q: 4.233137
 89268/100000: episode: 2138, duration: 0.263s, episode steps: 52, steps per second: 197, episode reward: 143.628, mean reward: 2.762 [1.947, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-2.116, 10.100], loss: 0.401301, mae: 0.408392, mean_q: 4.276325
 89314/100000: episode: 2139, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 302.306, mean reward: 6.572 [3.646, 16.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.394, 10.100], loss: 0.262902, mae: 0.419572, mean_q: 4.287416
 89325/100000: episode: 2140, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 20.798, mean reward: 1.891 [1.575, 2.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.177], loss: 1.050098, mae: 0.510817, mean_q: 4.377620
 89371/100000: episode: 2141, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 113.985, mean reward: 2.478 [1.455, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.593, 10.100], loss: 0.276576, mae: 0.441817, mean_q: 4.344375
 89391/100000: episode: 2142, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 40.758, mean reward: 2.038 [1.669, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.595, 10.100], loss: 0.251348, mae: 0.413255, mean_q: 4.369150
 89443/100000: episode: 2143, duration: 0.248s, episode steps: 52, steps per second: 210, episode reward: 115.237, mean reward: 2.216 [1.702, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.773, 10.100], loss: 0.261359, mae: 0.408952, mean_q: 4.336476
 89489/100000: episode: 2144, duration: 0.227s, episode steps: 46, steps per second: 202, episode reward: 130.274, mean reward: 2.832 [1.747, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.162, 10.100], loss: 0.238066, mae: 0.411193, mean_q: 4.334048
 89540/100000: episode: 2145, duration: 0.240s, episode steps: 51, steps per second: 213, episode reward: 111.672, mean reward: 2.190 [1.483, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.984, 10.128], loss: 0.193859, mae: 0.392505, mean_q: 4.329749
 89560/100000: episode: 2146, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 44.844, mean reward: 2.242 [1.677, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.772, 10.100], loss: 0.205724, mae: 0.412010, mean_q: 4.343599
 89606/100000: episode: 2147, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 106.979, mean reward: 2.326 [1.636, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.712, 10.100], loss: 0.235215, mae: 0.383323, mean_q: 4.290739
 89626/100000: episode: 2148, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 53.704, mean reward: 2.685 [2.055, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.351, 10.100], loss: 0.191382, mae: 0.398750, mean_q: 4.406764
 89669/100000: episode: 2149, duration: 0.211s, episode steps: 43, steps per second: 204, episode reward: 101.940, mean reward: 2.371 [1.602, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.351, 10.100], loss: 0.285739, mae: 0.410418, mean_q: 4.354536
 89765/100000: episode: 2150, duration: 0.487s, episode steps: 96, steps per second: 197, episode reward: 188.097, mean reward: 1.959 [1.502, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-1.611, 10.100], loss: 0.212177, mae: 0.399819, mean_q: 4.387010
 89816/100000: episode: 2151, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 107.539, mean reward: 2.109 [1.689, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.037, 10.100], loss: 0.202859, mae: 0.372860, mean_q: 4.344271
 89862/100000: episode: 2152, duration: 0.223s, episode steps: 46, steps per second: 207, episode reward: 263.593, mean reward: 5.730 [2.855, 17.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.507, 10.100], loss: 0.232922, mae: 0.391694, mean_q: 4.338159
 89910/100000: episode: 2153, duration: 0.237s, episode steps: 48, steps per second: 202, episode reward: 127.809, mean reward: 2.663 [1.927, 8.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.913, 10.368], loss: 0.328657, mae: 0.448458, mean_q: 4.441029
 90006/100000: episode: 2154, duration: 0.509s, episode steps: 96, steps per second: 188, episode reward: 189.134, mean reward: 1.970 [1.439, 5.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.572, 10.122], loss: 0.307337, mae: 0.442870, mean_q: 4.412005
 90049/100000: episode: 2155, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 105.145, mean reward: 2.445 [1.666, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.291, 10.250], loss: 0.287861, mae: 0.440362, mean_q: 4.476229
 90092/100000: episode: 2156, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 104.090, mean reward: 2.421 [1.515, 7.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.178, 10.138], loss: 0.248487, mae: 0.408032, mean_q: 4.461332
 90103/100000: episode: 2157, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 24.998, mean reward: 2.273 [1.909, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.419], loss: 0.321020, mae: 0.421550, mean_q: 4.403978
 90155/100000: episode: 2158, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 171.877, mean reward: 3.305 [2.127, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.440, 10.100], loss: 0.239798, mae: 0.425538, mean_q: 4.410136
 90198/100000: episode: 2159, duration: 0.226s, episode steps: 43, steps per second: 191, episode reward: 119.729, mean reward: 2.784 [1.956, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.428, 10.100], loss: 0.459233, mae: 0.515510, mean_q: 4.548776
 90292/100000: episode: 2160, duration: 0.465s, episode steps: 94, steps per second: 202, episode reward: 177.321, mean reward: 1.886 [1.456, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.844, 10.234], loss: 0.295676, mae: 0.454392, mean_q: 4.513873
 90338/100000: episode: 2161, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 133.254, mean reward: 2.897 [1.830, 6.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.571, 10.100], loss: 0.195336, mae: 0.397704, mean_q: 4.450815
 90349/100000: episode: 2162, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 27.741, mean reward: 2.522 [1.921, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.172, 10.438], loss: 0.406467, mae: 0.507964, mean_q: 4.548586
 90401/100000: episode: 2163, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 106.788, mean reward: 2.054 [1.484, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.367, 10.100], loss: 0.251957, mae: 0.433288, mean_q: 4.504783
 90421/100000: episode: 2164, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 61.711, mean reward: 3.086 [2.135, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.383, 10.100], loss: 0.162589, mae: 0.394408, mean_q: 4.447769
 90517/100000: episode: 2165, duration: 0.466s, episode steps: 96, steps per second: 206, episode reward: 184.021, mean reward: 1.917 [1.476, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-1.496, 10.267], loss: 0.291576, mae: 0.437802, mean_q: 4.522580
 90569/100000: episode: 2166, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 122.259, mean reward: 2.351 [1.465, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.550, 10.100], loss: 0.295149, mae: 0.430468, mean_q: 4.552894
 90663/100000: episode: 2167, duration: 0.480s, episode steps: 94, steps per second: 196, episode reward: 175.477, mean reward: 1.867 [1.441, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.643, 10.139], loss: 0.364278, mae: 0.458907, mean_q: 4.511489
 90711/100000: episode: 2168, duration: 0.242s, episode steps: 48, steps per second: 199, episode reward: 102.881, mean reward: 2.143 [1.513, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.861, 10.210], loss: 0.650356, mae: 0.506308, mean_q: 4.546412
 90763/100000: episode: 2169, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 136.121, mean reward: 2.618 [1.682, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.775, 10.100], loss: 0.250645, mae: 0.436295, mean_q: 4.527168
 90811/100000: episode: 2170, duration: 0.227s, episode steps: 48, steps per second: 211, episode reward: 112.500, mean reward: 2.344 [1.734, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.672, 10.291], loss: 0.251477, mae: 0.429959, mean_q: 4.561450
 90824/100000: episode: 2171, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 31.407, mean reward: 2.416 [2.040, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.446], loss: 0.637674, mae: 0.536403, mean_q: 4.817414
 90844/100000: episode: 2172, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 47.324, mean reward: 2.366 [1.759, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.980, 10.100], loss: 0.331808, mae: 0.505628, mean_q: 4.457245
 90892/100000: episode: 2173, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 133.021, mean reward: 2.771 [1.861, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.867, 10.491], loss: 0.254426, mae: 0.438412, mean_q: 4.552414
 90905/100000: episode: 2174, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 34.064, mean reward: 2.620 [1.968, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.340], loss: 0.344468, mae: 0.467879, mean_q: 4.538211
 90957/100000: episode: 2175, duration: 0.268s, episode steps: 52, steps per second: 194, episode reward: 119.907, mean reward: 2.306 [1.709, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.251, 10.100], loss: 0.225521, mae: 0.415221, mean_q: 4.558074
 91053/100000: episode: 2176, duration: 0.477s, episode steps: 96, steps per second: 201, episode reward: 178.720, mean reward: 1.862 [1.452, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.381, 10.268], loss: 0.317241, mae: 0.461909, mean_q: 4.614143
 91104/100000: episode: 2177, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 118.952, mean reward: 2.332 [1.521, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.771, 10.152], loss: 0.270635, mae: 0.442658, mean_q: 4.576488
 91115/100000: episode: 2178, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 25.313, mean reward: 2.301 [1.905, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.618, 10.334], loss: 0.185642, mae: 0.428047, mean_q: 4.606030
[Info] 2-TH LEVEL FOUND: 9.144354820251465, Considering 10/90 traces
 91166/100000: episode: 2179, duration: 4.284s, episode steps: 51, steps per second: 12, episode reward: 128.971, mean reward: 2.529 [1.715, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.387, 10.100], loss: 0.262781, mae: 0.428907, mean_q: 4.614725
 91202/100000: episode: 2180, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 101.789, mean reward: 2.827 [2.117, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.797, 10.100], loss: 0.302147, mae: 0.441588, mean_q: 4.625672
 91238/100000: episode: 2181, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 131.876, mean reward: 3.663 [2.305, 5.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.539, 10.100], loss: 0.232980, mae: 0.426377, mean_q: 4.574196
 91273/100000: episode: 2182, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 175.861, mean reward: 5.025 [2.943, 18.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.405, 10.100], loss: 0.353306, mae: 0.480424, mean_q: 4.711148
 91305/100000: episode: 2183, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 118.699, mean reward: 3.709 [2.549, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.798, 10.100], loss: 0.712622, mae: 0.573899, mean_q: 4.771256
 91329/100000: episode: 2184, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 69.586, mean reward: 2.899 [2.239, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.158, 10.100], loss: 0.502751, mae: 0.581889, mean_q: 4.881767
 91365/100000: episode: 2185, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 107.393, mean reward: 2.983 [1.776, 5.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.767, 10.100], loss: 0.866735, mae: 0.575497, mean_q: 4.829693
 91400/100000: episode: 2186, duration: 0.168s, episode steps: 35, steps per second: 208, episode reward: 118.595, mean reward: 3.388 [2.487, 5.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.379, 10.100], loss: 0.352455, mae: 0.511681, mean_q: 4.691519
 91436/100000: episode: 2187, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 140.303, mean reward: 3.897 [2.467, 6.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.562, 10.100], loss: 0.302482, mae: 0.472770, mean_q: 4.786437
 91468/100000: episode: 2188, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 131.894, mean reward: 4.122 [2.086, 8.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.463, 10.100], loss: 0.301593, mae: 0.480330, mean_q: 4.773946
 91491/100000: episode: 2189, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 70.579, mean reward: 3.069 [2.339, 5.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.289, 10.100], loss: 0.368227, mae: 0.478936, mean_q: 4.807324
 91527/100000: episode: 2190, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 118.634, mean reward: 3.295 [1.557, 7.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.684, 10.100], loss: 0.301426, mae: 0.472239, mean_q: 4.871592
 91563/100000: episode: 2191, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 157.224, mean reward: 4.367 [2.959, 15.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.862, 10.100], loss: 0.411965, mae: 0.529924, mean_q: 4.878498
 91587/100000: episode: 2192, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 89.118, mean reward: 3.713 [2.636, 5.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.398, 10.100], loss: 0.294609, mae: 0.470754, mean_q: 4.819698
 91623/100000: episode: 2193, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 137.687, mean reward: 3.825 [2.282, 7.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.866, 10.100], loss: 0.267943, mae: 0.474067, mean_q: 4.862829
 91655/100000: episode: 2194, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 132.219, mean reward: 4.132 [2.580, 8.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.520, 10.100], loss: 0.246244, mae: 0.430816, mean_q: 4.823546
 91679/100000: episode: 2195, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 110.019, mean reward: 4.584 [2.653, 12.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.466, 10.100], loss: 0.298052, mae: 0.460742, mean_q: 4.915941
 91702/100000: episode: 2196, duration: 0.112s, episode steps: 23, steps per second: 206, episode reward: 77.783, mean reward: 3.382 [2.301, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.386, 10.100], loss: 0.585066, mae: 0.506517, mean_q: 4.885026
 91737/100000: episode: 2197, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 95.702, mean reward: 2.734 [1.955, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.998, 10.100], loss: 0.365944, mae: 0.505739, mean_q: 4.990715
 91772/100000: episode: 2198, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 231.075, mean reward: 6.602 [2.255, 29.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.373, 10.100], loss: 0.295534, mae: 0.475319, mean_q: 4.879664
 91804/100000: episode: 2199, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 126.653, mean reward: 3.958 [1.962, 7.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.366, 10.100], loss: 0.365301, mae: 0.501631, mean_q: 4.901844
 91827/100000: episode: 2200, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 81.528, mean reward: 3.545 [2.580, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.005, 10.100], loss: 0.629187, mae: 0.555882, mean_q: 4.974747
 91863/100000: episode: 2201, duration: 0.185s, episode steps: 36, steps per second: 194, episode reward: 95.923, mean reward: 2.665 [1.793, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.599, 10.100], loss: 0.633451, mae: 0.556609, mean_q: 5.066055
 91887/100000: episode: 2202, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 98.097, mean reward: 4.087 [2.793, 7.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.823, 10.100], loss: 0.350153, mae: 0.507745, mean_q: 4.927705
 91894/100000: episode: 2203, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 26.642, mean reward: 3.806 [3.220, 4.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.477], loss: 0.395689, mae: 0.518557, mean_q: 5.010942
 91930/100000: episode: 2204, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 98.643, mean reward: 2.740 [1.436, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.514, 10.100], loss: 0.460210, mae: 0.508949, mean_q: 4.929598
 91966/100000: episode: 2205, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 191.064, mean reward: 5.307 [3.788, 9.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.884, 10.100], loss: 0.932399, mae: 0.615209, mean_q: 4.958281
 92002/100000: episode: 2206, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 117.335, mean reward: 3.259 [2.503, 5.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.408, 10.100], loss: 0.347014, mae: 0.540055, mean_q: 5.052856
 92037/100000: episode: 2207, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 102.661, mean reward: 2.933 [2.141, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.791, 10.100], loss: 0.453444, mae: 0.548204, mean_q: 5.109800
 92067/100000: episode: 2208, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 74.374, mean reward: 2.479 [1.548, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.205, 10.100], loss: 0.854373, mae: 0.604697, mean_q: 5.125412
 92102/100000: episode: 2209, duration: 0.165s, episode steps: 35, steps per second: 212, episode reward: 133.696, mean reward: 3.820 [1.955, 7.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.209, 10.100], loss: 0.553287, mae: 0.575265, mean_q: 5.224221
 92137/100000: episode: 2210, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 121.455, mean reward: 3.470 [1.599, 11.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.273, 10.100], loss: 0.671655, mae: 0.568831, mean_q: 5.120269
 92172/100000: episode: 2211, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 175.648, mean reward: 5.019 [2.875, 17.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.114, 10.100], loss: 0.385645, mae: 0.526093, mean_q: 5.127791
 92207/100000: episode: 2212, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 294.845, mean reward: 8.424 [2.395, 25.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.511, 10.100], loss: 0.763982, mae: 0.590854, mean_q: 5.247272
 92243/100000: episode: 2213, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 132.559, mean reward: 3.682 [2.510, 6.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.782, 10.100], loss: 1.021559, mae: 0.687626, mean_q: 5.382204
 92273/100000: episode: 2214, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 143.845, mean reward: 4.795 [2.965, 7.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.482, 10.100], loss: 0.730515, mae: 0.601470, mean_q: 5.253135
 92305/100000: episode: 2215, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 91.008, mean reward: 2.844 [1.996, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.483, 10.100], loss: 0.775703, mae: 0.575318, mean_q: 5.260091
 92340/100000: episode: 2216, duration: 0.167s, episode steps: 35, steps per second: 209, episode reward: 161.896, mean reward: 4.626 [2.985, 16.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.944, 10.100], loss: 1.481657, mae: 0.758436, mean_q: 5.495135
 92370/100000: episode: 2217, duration: 0.153s, episode steps: 30, steps per second: 197, episode reward: 81.584, mean reward: 2.719 [1.786, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.304, 10.100], loss: 1.056908, mae: 0.743295, mean_q: 5.188876
 92393/100000: episode: 2218, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 78.184, mean reward: 3.399 [2.679, 5.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.265, 10.100], loss: 0.710605, mae: 0.619850, mean_q: 5.294897
 92429/100000: episode: 2219, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 135.357, mean reward: 3.760 [2.738, 8.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.385, 10.100], loss: 0.490941, mae: 0.586502, mean_q: 5.386588
 92464/100000: episode: 2220, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 138.242, mean reward: 3.950 [2.544, 6.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.464, 10.100], loss: 0.979096, mae: 0.669188, mean_q: 5.389784
 92500/100000: episode: 2221, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 114.748, mean reward: 3.187 [2.248, 5.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.200, 10.100], loss: 0.697048, mae: 0.572230, mean_q: 5.248801
 92507/100000: episode: 2222, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 28.118, mean reward: 4.017 [3.049, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.577], loss: 0.888996, mae: 0.639869, mean_q: 5.404544
 92514/100000: episode: 2223, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 29.653, mean reward: 4.236 [3.760, 4.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.502], loss: 0.831429, mae: 0.733505, mean_q: 5.698478
 92549/100000: episode: 2224, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 117.356, mean reward: 3.353 [1.981, 6.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.837, 10.100], loss: 0.984002, mae: 0.659941, mean_q: 5.501711
 92585/100000: episode: 2225, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 136.273, mean reward: 3.785 [2.523, 6.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.463, 10.100], loss: 0.854910, mae: 0.642078, mean_q: 5.446096
 92621/100000: episode: 2226, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 169.938, mean reward: 4.721 [2.894, 8.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.619, 10.100], loss: 0.695743, mae: 0.635998, mean_q: 5.428460
 92656/100000: episode: 2227, duration: 0.164s, episode steps: 35, steps per second: 213, episode reward: 84.591, mean reward: 2.417 [1.635, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.115, 10.100], loss: 1.120103, mae: 0.709701, mean_q: 5.609490
 92692/100000: episode: 2228, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 152.010, mean reward: 4.223 [3.069, 6.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.416, 10.100], loss: 1.222059, mae: 0.754095, mean_q: 5.672559
 92728/100000: episode: 2229, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 118.071, mean reward: 3.280 [1.693, 6.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.834, 10.100], loss: 0.985644, mae: 0.681899, mean_q: 5.574492
 92758/100000: episode: 2230, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 77.540, mean reward: 2.585 [1.708, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.183, 10.100], loss: 0.673717, mae: 0.660194, mean_q: 5.543787
 92790/100000: episode: 2231, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 141.127, mean reward: 4.410 [2.426, 11.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.331, 10.100], loss: 0.569183, mae: 0.616781, mean_q: 5.506630
 92813/100000: episode: 2232, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 97.092, mean reward: 4.221 [2.210, 13.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.414, 10.100], loss: 0.615815, mae: 0.633492, mean_q: 5.511596
 92849/100000: episode: 2233, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 106.088, mean reward: 2.947 [1.726, 10.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.612, 10.100], loss: 0.525197, mae: 0.608569, mean_q: 5.639711
 92885/100000: episode: 2234, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 140.106, mean reward: 3.892 [2.438, 5.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.245, 10.100], loss: 1.118745, mae: 0.739529, mean_q: 5.473559
 92909/100000: episode: 2235, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 70.574, mean reward: 2.941 [2.267, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.333, 10.100], loss: 0.838775, mae: 0.682588, mean_q: 5.515902
 92941/100000: episode: 2236, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 76.758, mean reward: 2.399 [1.716, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.274, 10.100], loss: 0.940374, mae: 0.646693, mean_q: 5.572847
 92964/100000: episode: 2237, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 68.978, mean reward: 2.999 [2.009, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.112, 10.100], loss: 0.811132, mae: 0.696244, mean_q: 5.664549
 92999/100000: episode: 2238, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 110.528, mean reward: 3.158 [2.084, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.302, 10.100], loss: 1.055018, mae: 0.693365, mean_q: 5.639179
 93034/100000: episode: 2239, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 140.799, mean reward: 4.023 [2.093, 11.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.567, 10.100], loss: 0.919147, mae: 0.640779, mean_q: 5.600882
 93057/100000: episode: 2240, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 94.879, mean reward: 4.125 [2.985, 5.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.276, 10.100], loss: 0.824468, mae: 0.637548, mean_q: 5.622266
 93093/100000: episode: 2241, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 129.765, mean reward: 3.605 [2.048, 18.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.921, 10.100], loss: 0.861068, mae: 0.725578, mean_q: 5.719594
 93117/100000: episode: 2242, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 67.893, mean reward: 2.829 [1.809, 5.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.419, 10.100], loss: 0.840794, mae: 0.686011, mean_q: 5.702101
 93140/100000: episode: 2243, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 96.449, mean reward: 4.193 [2.902, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.397, 10.100], loss: 1.113407, mae: 0.736517, mean_q: 5.782897
 93147/100000: episode: 2244, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 25.603, mean reward: 3.658 [3.431, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.473], loss: 0.653988, mae: 0.736950, mean_q: 6.197937
 93171/100000: episode: 2245, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 65.051, mean reward: 2.710 [2.206, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.202, 10.100], loss: 1.301621, mae: 0.722395, mean_q: 5.600240
 93207/100000: episode: 2246, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 362.464, mean reward: 10.068 [3.066, 166.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.566, 10.100], loss: 1.199398, mae: 0.745738, mean_q: 5.797595
 93231/100000: episode: 2247, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 56.877, mean reward: 2.370 [1.597, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.613, 10.100], loss: 0.598743, mae: 0.639380, mean_q: 5.806486
 93266/100000: episode: 2248, duration: 0.189s, episode steps: 35, steps per second: 186, episode reward: 124.097, mean reward: 3.546 [2.243, 5.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.234, 10.100], loss: 0.716741, mae: 0.659967, mean_q: 5.802088
 93302/100000: episode: 2249, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 109.253, mean reward: 3.035 [1.458, 6.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.280, 10.100], loss: 12.787877, mae: 1.034428, mean_q: 6.086762
 93337/100000: episode: 2250, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 162.897, mean reward: 4.654 [2.741, 9.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.998, 10.100], loss: 1.136379, mae: 0.798896, mean_q: 5.820878
 93344/100000: episode: 2251, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 26.022, mean reward: 3.717 [3.338, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.552], loss: 1.304094, mae: 0.822979, mean_q: 5.880653
 93379/100000: episode: 2252, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 130.249, mean reward: 3.721 [2.276, 6.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.437, 10.100], loss: 1.001203, mae: 0.723396, mean_q: 5.955574
 93403/100000: episode: 2253, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 100.607, mean reward: 4.192 [2.753, 8.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.382, 10.100], loss: 1.783473, mae: 0.849408, mean_q: 6.003637
 93435/100000: episode: 2254, duration: 0.156s, episode steps: 32, steps per second: 206, episode reward: 102.167, mean reward: 3.193 [2.148, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.927, 10.100], loss: 1.169470, mae: 0.750584, mean_q: 5.996684
 93470/100000: episode: 2255, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 189.819, mean reward: 5.423 [2.762, 23.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.597, 10.100], loss: 1.479719, mae: 0.877612, mean_q: 6.017612
 93506/100000: episode: 2256, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 121.167, mean reward: 3.366 [1.821, 7.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.178, 10.100], loss: 12.191224, mae: 0.907442, mean_q: 5.993936
 93542/100000: episode: 2257, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 161.228, mean reward: 4.479 [2.927, 8.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.781, 10.100], loss: 1.497779, mae: 0.857038, mean_q: 6.070525
 93566/100000: episode: 2258, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 82.322, mean reward: 3.430 [2.294, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.083, 10.100], loss: 0.963712, mae: 0.711073, mean_q: 6.111701
 93573/100000: episode: 2259, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 43.360, mean reward: 6.194 [3.815, 12.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.635, 10.646], loss: 0.805201, mae: 0.732675, mean_q: 5.913997
 93605/100000: episode: 2260, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: 121.112, mean reward: 3.785 [2.480, 6.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.317, 10.100], loss: 0.794834, mae: 0.699375, mean_q: 6.072433
 93641/100000: episode: 2261, duration: 0.196s, episode steps: 36, steps per second: 183, episode reward: 136.145, mean reward: 3.782 [2.273, 6.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.438, 10.100], loss: 0.702009, mae: 0.690118, mean_q: 5.992130
 93673/100000: episode: 2262, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 171.738, mean reward: 5.367 [2.755, 9.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.795, 10.100], loss: 1.097372, mae: 0.757884, mean_q: 6.028522
 93705/100000: episode: 2263, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 116.462, mean reward: 3.639 [2.365, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.333, 10.100], loss: 1.574081, mae: 0.809748, mean_q: 6.188967
 93737/100000: episode: 2264, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 127.279, mean reward: 3.977 [2.913, 9.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.475, 10.100], loss: 1.190947, mae: 0.751086, mean_q: 6.109351
 93773/100000: episode: 2265, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 139.834, mean reward: 3.884 [2.593, 5.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.415, 10.100], loss: 1.010375, mae: 0.729994, mean_q: 6.208478
 93797/100000: episode: 2266, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 72.422, mean reward: 3.018 [2.212, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.566, 10.100], loss: 0.797465, mae: 0.665582, mean_q: 6.017447
 93832/100000: episode: 2267, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 112.132, mean reward: 3.204 [1.690, 6.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.128, 10.100], loss: 0.934399, mae: 0.732471, mean_q: 6.080000
 93867/100000: episode: 2268, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 132.968, mean reward: 3.799 [2.266, 6.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.491, 10.100], loss: 12.842044, mae: 1.105436, mean_q: 6.292147
[Info] 3-TH LEVEL FOUND: 12.033529281616211, Considering 10/90 traces
 93890/100000: episode: 2269, duration: 4.165s, episode steps: 23, steps per second: 6, episode reward: 81.184, mean reward: 3.530 [2.158, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.783, 10.100], loss: 0.553625, mae: 0.678475, mean_q: 6.199956
 93914/100000: episode: 2270, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 143.949, mean reward: 5.998 [3.175, 29.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.235, 10.100], loss: 18.162992, mae: 1.167389, mean_q: 6.424162
 93936/100000: episode: 2271, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 89.718, mean reward: 4.078 [2.137, 7.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.545, 10.100], loss: 19.413506, mae: 1.017532, mean_q: 6.436621
 93960/100000: episode: 2272, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 215.678, mean reward: 8.987 [5.029, 21.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.592, 10.100], loss: 1.659525, mae: 0.964417, mean_q: 6.391243
 93982/100000: episode: 2273, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 96.045, mean reward: 4.366 [3.331, 8.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.393, 10.100], loss: 0.910048, mae: 0.784257, mean_q: 6.365435
 94004/100000: episode: 2274, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 121.320, mean reward: 5.515 [3.056, 9.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.142, 10.100], loss: 1.357527, mae: 0.835102, mean_q: 6.496687
[Info] FALSIFICATION!
 94009/100000: episode: 2275, duration: 0.287s, episode steps: 5, steps per second: 17, episode reward: 2049.391, mean reward: 409.878 [10.870, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.335, 9.756], loss: 0.717491, mae: 0.776668, mean_q: 6.399258
 94037/100000: episode: 2276, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 120.265, mean reward: 4.295 [3.299, 6.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.358, 10.100], loss: 1.142619, mae: 0.794217, mean_q: 6.379976
 94061/100000: episode: 2277, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 140.384, mean reward: 5.849 [3.755, 17.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.419, 10.100], loss: 1.114904, mae: 0.812634, mean_q: 6.497555
 94083/100000: episode: 2278, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 95.309, mean reward: 4.332 [3.415, 8.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.607, 10.100], loss: 1.355345, mae: 0.826098, mean_q: 6.462147
 94111/100000: episode: 2279, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 123.698, mean reward: 4.418 [2.191, 9.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.538, 10.100], loss: 547.744141, mae: 2.979496, mean_q: 7.553460
 94134/100000: episode: 2280, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 143.021, mean reward: 6.218 [3.082, 12.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-2.464, 10.100], loss: 689.416565, mae: 2.721304, mean_q: 6.793574
 94158/100000: episode: 2281, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 109.265, mean reward: 4.553 [1.986, 13.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.408, 10.100], loss: 6.241104, mae: 2.588956, mean_q: 8.641734
 94180/100000: episode: 2282, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 83.133, mean reward: 3.779 [2.651, 5.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.546, 10.100], loss: 695.478943, mae: 3.190891, mean_q: 7.607697
 94204/100000: episode: 2283, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 145.412, mean reward: 6.059 [3.177, 17.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.406, 10.100], loss: 2.678342, mae: 1.354782, mean_q: 6.856245
 94226/100000: episode: 2284, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 176.194, mean reward: 8.009 [3.019, 29.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.696, 10.100], loss: 1.964388, mae: 0.969817, mean_q: 6.794613
 94248/100000: episode: 2285, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 117.420, mean reward: 5.337 [2.902, 17.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.700, 10.100], loss: 1.141862, mae: 0.875706, mean_q: 6.811905
 94265/100000: episode: 2286, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 77.350, mean reward: 4.550 [2.558, 8.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.704, 10.100], loss: 896.383118, mae: 3.896765, mean_q: 8.516896
 94287/100000: episode: 2287, duration: 0.103s, episode steps: 22, steps per second: 215, episode reward: 88.799, mean reward: 4.036 [2.845, 6.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.573, 10.100], loss: 1.641210, mae: 1.241127, mean_q: 6.973671
 94310/100000: episode: 2288, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 143.170, mean reward: 6.225 [2.440, 26.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.770, 10.100], loss: 674.107056, mae: 3.260062, mean_q: 8.333549
 94333/100000: episode: 2289, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 234.789, mean reward: 10.208 [3.182, 72.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.370, 10.100], loss: 5.095993, mae: 1.245829, mean_q: 6.974601
 94350/100000: episode: 2290, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 67.174, mean reward: 3.951 [2.603, 9.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.300, 10.100], loss: 25.379484, mae: 1.313423, mean_q: 7.339373
 94372/100000: episode: 2291, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 167.363, mean reward: 7.607 [3.632, 24.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.484, 10.100], loss: 20.583002, mae: 1.364733, mean_q: 7.372920
 94394/100000: episode: 2292, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 72.459, mean reward: 3.294 [2.324, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.343, 10.100], loss: 1.152698, mae: 1.004604, mean_q: 7.398796
 94416/100000: episode: 2293, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 85.665, mean reward: 3.894 [2.638, 9.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.369, 10.100], loss: 1.022202, mae: 0.859191, mean_q: 6.818125
 94438/100000: episode: 2294, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 79.454, mean reward: 3.612 [2.450, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.420, 10.100], loss: 1.509945, mae: 0.846078, mean_q: 6.839798
 94460/100000: episode: 2295, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 112.174, mean reward: 5.099 [3.760, 9.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.534, 10.100], loss: 1.554239, mae: 0.905996, mean_q: 6.977211
 94482/100000: episode: 2296, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 116.488, mean reward: 5.295 [3.262, 7.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.324, 10.100], loss: 1.496966, mae: 0.937443, mean_q: 7.130123
 94504/100000: episode: 2297, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 153.884, mean reward: 6.995 [3.029, 42.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.396, 10.100], loss: 2.156184, mae: 1.032630, mean_q: 7.007837
[Info] FALSIFICATION!
 94515/100000: episode: 2298, duration: 0.314s, episode steps: 11, steps per second: 35, episode reward: 1114.160, mean reward: 101.287 [7.008, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.448, 9.695], loss: 2.070803, mae: 1.010845, mean_q: 7.262234
[Info] FALSIFICATION!
 94521/100000: episode: 2299, duration: 0.289s, episode steps: 6, steps per second: 21, episode reward: 1036.857, mean reward: 172.810 [3.957, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.004, 9.320], loss: 0.626137, mae: 0.757520, mean_q: 6.618774
 94544/100000: episode: 2300, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 102.198, mean reward: 4.443 [2.742, 7.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-2.011, 10.100], loss: 1.478974, mae: 0.893937, mean_q: 7.020963
 94566/100000: episode: 2301, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 78.791, mean reward: 3.581 [2.014, 6.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.665, 10.100], loss: 712.052856, mae: 2.924098, mean_q: 7.404425
 94590/100000: episode: 2302, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 144.703, mean reward: 6.029 [3.282, 13.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.186, 10.100], loss: 23.841585, mae: 2.420017, mean_q: 8.674174
 94614/100000: episode: 2303, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 243.534, mean reward: 10.147 [4.115, 38.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.610, 10.100], loss: 1.819687, mae: 1.043278, mean_q: 6.994315
 94637/100000: episode: 2304, duration: 0.109s, episode steps: 23, steps per second: 210, episode reward: 95.562, mean reward: 4.155 [2.755, 11.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.630, 10.100], loss: 1316.209351, mae: 5.278412, mean_q: 9.740064
 94660/100000: episode: 2305, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 141.155, mean reward: 6.137 [2.843, 15.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.818, 10.100], loss: 3.682039, mae: 1.613238, mean_q: 6.901725
 94684/100000: episode: 2306, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 108.509, mean reward: 4.521 [3.423, 6.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.418, 10.100], loss: 629.950928, mae: 4.231622, mean_q: 10.207215
 94708/100000: episode: 2307, duration: 0.116s, episode steps: 24, steps per second: 206, episode reward: 115.644, mean reward: 4.818 [2.925, 12.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.473, 10.100], loss: 3.637074, mae: 1.544899, mean_q: 7.243168
 94732/100000: episode: 2308, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 157.769, mean reward: 6.574 [2.163, 13.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.211, 10.100], loss: 1270.444946, mae: 4.446187, mean_q: 8.560172
 94756/100000: episode: 2309, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 112.099, mean reward: 4.671 [2.736, 12.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.570, 10.100], loss: 5.670939, mae: 2.442128, mean_q: 9.334725
 94780/100000: episode: 2310, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 123.630, mean reward: 5.151 [3.915, 6.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.339, 10.100], loss: 6.443147, mae: 1.454851, mean_q: 7.194719
 94797/100000: episode: 2311, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 72.970, mean reward: 4.292 [2.268, 12.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.288, 10.100], loss: 2.715526, mae: 1.297138, mean_q: 8.199440
 94814/100000: episode: 2312, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 172.372, mean reward: 10.140 [5.424, 20.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.578, 10.100], loss: 3.559933, mae: 1.276536, mean_q: 7.936583
[Info] FALSIFICATION!
 94829/100000: episode: 2313, duration: 0.341s, episode steps: 15, steps per second: 44, episode reward: 1138.387, mean reward: 75.892 [4.651, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.526, 10.093], loss: 1028.399658, mae: 3.877323, mean_q: 8.735467
 94851/100000: episode: 2314, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 97.274, mean reward: 4.422 [2.663, 7.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.199, 10.100], loss: 3377.310303, mae: 10.379587, mean_q: 11.054897
 94875/100000: episode: 2315, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 145.251, mean reward: 6.052 [3.774, 9.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.504, 10.100], loss: 643.288025, mae: 7.349521, mean_q: 13.667539
 94899/100000: episode: 2316, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 120.584, mean reward: 5.024 [2.706, 10.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.309, 10.100], loss: 5.978438, mae: 2.482398, mean_q: 8.044075
 94921/100000: episode: 2317, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 74.733, mean reward: 3.397 [2.836, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.359, 10.100], loss: 3.529060, mae: 1.826289, mean_q: 7.743759
 94945/100000: episode: 2318, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 94.354, mean reward: 3.931 [2.307, 7.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.667, 10.100], loss: 3.113533, mae: 1.707628, mean_q: 8.564408
 94969/100000: episode: 2319, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 131.041, mean reward: 5.460 [3.098, 19.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.338, 10.100], loss: 2.875507, mae: 1.475486, mean_q: 8.134063
 94991/100000: episode: 2320, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 98.355, mean reward: 4.471 [3.107, 7.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.313, 10.100], loss: 700.907959, mae: 3.026992, mean_q: 8.671386
 95008/100000: episode: 2321, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 118.073, mean reward: 6.945 [4.649, 10.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.458, 10.100], loss: 4.075683, mae: 1.789598, mean_q: 8.625802
 95030/100000: episode: 2322, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 75.924, mean reward: 3.451 [2.148, 8.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.049, 10.100], loss: 1361.309448, mae: 4.676770, mean_q: 8.891418
 95052/100000: episode: 2323, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 114.647, mean reward: 5.211 [3.766, 8.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.292, 10.100], loss: 32.245964, mae: 4.123622, mean_q: 11.529132
 95075/100000: episode: 2324, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 133.081, mean reward: 5.786 [2.202, 15.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.708, 10.100], loss: 667.496887, mae: 3.099337, mean_q: 8.482533
 95098/100000: episode: 2325, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 121.380, mean reward: 5.277 [3.851, 11.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.491, 10.100], loss: 657.457275, mae: 4.002506, mean_q: 10.461284
 95120/100000: episode: 2326, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 101.839, mean reward: 4.629 [2.896, 10.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.441, 10.100], loss: 20.895239, mae: 1.784678, mean_q: 8.098827
 95143/100000: episode: 2327, duration: 0.110s, episode steps: 23, steps per second: 209, episode reward: 100.240, mean reward: 4.358 [3.117, 6.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.511, 10.100], loss: 6.632775, mae: 1.554757, mean_q: 8.444280
 95167/100000: episode: 2328, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 168.052, mean reward: 7.002 [3.637, 16.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.307, 10.100], loss: 1.874046, mae: 1.314413, mean_q: 8.172793
 95191/100000: episode: 2329, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 131.812, mean reward: 5.492 [3.627, 10.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.238, 10.100], loss: 634.520569, mae: 2.891966, mean_q: 8.528620
 95215/100000: episode: 2330, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 187.563, mean reward: 7.815 [4.311, 22.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.243, 10.100], loss: 24.119902, mae: 2.894359, mean_q: 10.307305
[Info] FALSIFICATION!
 95230/100000: episode: 2331, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 1100.230, mean reward: 73.349 [3.125, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.455, 9.911], loss: 4.364447, mae: 1.859856, mean_q: 8.447038
 95254/100000: episode: 2332, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 223.816, mean reward: 9.326 [3.466, 69.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.652, 10.100], loss: 3.397778, mae: 1.552110, mean_q: 7.572535
 95277/100000: episode: 2333, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 282.112, mean reward: 12.266 [3.785, 68.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.464, 10.100], loss: 1313.369751, mae: 5.315210, mean_q: 9.988006
 95300/100000: episode: 2334, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 149.329, mean reward: 6.493 [4.253, 17.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.471, 10.100], loss: 657.015869, mae: 5.076864, mean_q: 11.698558
[Info] FALSIFICATION!
 95307/100000: episode: 2335, duration: 0.198s, episode steps: 7, steps per second: 35, episode reward: 1040.936, mean reward: 148.705 [3.324, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.162, 9.042], loss: 4.802614, mae: 2.025199, mean_q: 9.160553
 95329/100000: episode: 2336, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 81.330, mean reward: 3.697 [2.351, 8.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.570, 10.100], loss: 1363.064453, mae: 5.354279, mean_q: 9.924984
 95353/100000: episode: 2337, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 86.622, mean reward: 3.609 [2.371, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.340, 10.100], loss: 637.076233, mae: 3.910452, mean_q: 10.524812
 95375/100000: episode: 2338, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 92.837, mean reward: 4.220 [3.104, 7.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.461, 10.100], loss: 695.661377, mae: 3.138568, mean_q: 9.466754
 95397/100000: episode: 2339, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 74.435, mean reward: 3.383 [2.370, 7.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.473, 10.100], loss: 9.405692, mae: 1.785480, mean_q: 9.278747
 95419/100000: episode: 2340, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 148.012, mean reward: 6.728 [3.915, 20.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.357, 10.100], loss: 1382.468018, mae: 5.268384, mean_q: 9.975183
 95443/100000: episode: 2341, duration: 0.112s, episode steps: 24, steps per second: 213, episode reward: 140.804, mean reward: 5.867 [3.659, 12.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.382, 10.100], loss: 641.701416, mae: 4.611477, mean_q: 11.302512
[Info] FALSIFICATION!
 95445/100000: episode: 2342, duration: 0.265s, episode steps: 2, steps per second: 8, episode reward: 1018.513, mean reward: 509.256 [18.513, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.004, 9.042], loss: 6.070401, mae: 2.430575, mean_q: 8.919931
 95469/100000: episode: 2343, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 77.815, mean reward: 3.242 [2.351, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.373, 10.100], loss: 1269.315552, mae: 6.830336, mean_q: 12.645270
 95491/100000: episode: 2344, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 1396.763, mean reward: 63.489 [3.673, 745.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.933, 10.100], loss: 27.420343, mae: 2.701237, mean_q: 8.624783
 95515/100000: episode: 2345, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 97.180, mean reward: 4.049 [1.721, 9.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.414, 10.100], loss: 633.883179, mae: 3.000850, mean_q: 9.367244
 95538/100000: episode: 2346, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 159.585, mean reward: 6.938 [2.795, 17.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.649, 10.100], loss: 1972.799927, mae: 7.803296, mean_q: 12.591825
 95561/100000: episode: 2347, duration: 0.109s, episode steps: 23, steps per second: 211, episode reward: 107.972, mean reward: 4.694 [2.851, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.775, 10.100], loss: 657.007629, mae: 5.140917, mean_q: 12.393384
 95585/100000: episode: 2348, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 100.683, mean reward: 4.195 [2.652, 8.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.711, 10.100], loss: 8.377779, mae: 1.948351, mean_q: 9.669640
 95607/100000: episode: 2349, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 85.175, mean reward: 3.872 [2.565, 9.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.510, 10.100], loss: 1066.075562, mae: 4.839655, mean_q: 11.001685
 95629/100000: episode: 2350, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 120.863, mean reward: 5.494 [3.336, 15.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.505, 10.100], loss: 393.746124, mae: 3.779093, mean_q: 11.379861
 95651/100000: episode: 2351, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 400.775, mean reward: 18.217 [3.375, 168.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.431, 10.100], loss: 1080.244629, mae: 4.277817, mean_q: 9.915092
 95673/100000: episode: 2352, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 117.416, mean reward: 5.337 [2.907, 9.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.377, 10.100], loss: 685.505676, mae: 5.433918, mean_q: 13.308010
 95696/100000: episode: 2353, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 307.841, mean reward: 13.384 [4.777, 126.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.857, 10.100], loss: 674.161438, mae: 3.884465, mean_q: 11.589922
 95720/100000: episode: 2354, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 180.120, mean reward: 7.505 [4.566, 29.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.425, 10.100], loss: 651.063599, mae: 3.332099, mean_q: 10.326164
 95743/100000: episode: 2355, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 112.450, mean reward: 4.889 [3.695, 7.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-2.095, 10.100], loss: 639.304993, mae: 3.366092, mean_q: 10.849716
[Info] FALSIFICATION!
 95754/100000: episode: 2356, duration: 0.226s, episode steps: 11, steps per second: 49, episode reward: 1104.262, mean reward: 100.387 [3.445, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.126, 9.404], loss: 9.716369, mae: 3.231313, mean_q: 12.187286
[Info] FALSIFICATION!
 95770/100000: episode: 2357, duration: 0.333s, episode steps: 16, steps per second: 48, episode reward: 1119.470, mean reward: 69.967 [3.208, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.291, 10.020], loss: 957.820740, mae: 4.512972, mean_q: 11.319760
 95794/100000: episode: 2358, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 84.631, mean reward: 3.526 [1.801, 6.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.225, 10.100], loss: 621.816711, mae: 4.616033, mean_q: 12.826678
[Info] Complete ISplit Iteration
[Info] Levels: [5.1283274, 9.144355, 12.033529, 25.160236]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.62]
[Info] Error Prob: 0.0006200000000000001

 95822/100000: episode: 2359, duration: 4.404s, episode steps: 28, steps per second: 6, episode reward: 182.890, mean reward: 6.532 [3.368, 14.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.915, 10.100], loss: 559.616516, mae: 3.225157, mean_q: 10.007916
 95922/100000: episode: 2360, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 218.664, mean reward: 2.187 [1.538, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.146, 10.522], loss: 1210.226685, mae: 6.055716, mean_q: 12.705132
 96022/100000: episode: 2361, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 211.725, mean reward: 2.117 [1.470, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.856, 10.098], loss: 1052.123047, mae: 5.235021, mean_q: 12.059093
 96122/100000: episode: 2362, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 191.516, mean reward: 1.915 [1.454, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.833, 10.170], loss: 1311.011841, mae: 7.338878, mean_q: 14.127697
 96222/100000: episode: 2363, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.434, mean reward: 1.804 [1.452, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.463, 10.234], loss: 899.254395, mae: 5.036388, mean_q: 12.360427
 96322/100000: episode: 2364, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.556, mean reward: 1.906 [1.473, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.899, 10.245], loss: 761.467651, mae: 4.407448, mean_q: 12.066562
 96422/100000: episode: 2365, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.505, mean reward: 1.835 [1.455, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.475, 10.098], loss: 699.881714, mae: 4.124014, mean_q: 11.557683
 96522/100000: episode: 2366, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 191.992, mean reward: 1.920 [1.453, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.138, 10.166], loss: 757.404663, mae: 4.359759, mean_q: 11.389396
 96622/100000: episode: 2367, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.525, mean reward: 1.815 [1.437, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.511, 10.121], loss: 916.742004, mae: 5.264740, mean_q: 12.048363
 96722/100000: episode: 2368, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 205.058, mean reward: 2.051 [1.482, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.572, 10.098], loss: 321.189331, mae: 2.944739, mean_q: 10.231827
 96822/100000: episode: 2369, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.009, mean reward: 1.810 [1.438, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.544, 10.107], loss: 1523.239746, mae: 7.497210, mean_q: 13.059482
 96922/100000: episode: 2370, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 219.173, mean reward: 2.192 [1.571, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.567, 10.098], loss: 1747.209229, mae: 7.859991, mean_q: 13.872425
 97022/100000: episode: 2371, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.506, mean reward: 1.945 [1.443, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.445, 10.108], loss: 1206.093384, mae: 6.472540, mean_q: 13.466932
 97122/100000: episode: 2372, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 226.582, mean reward: 2.266 [1.430, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.441, 10.098], loss: 1278.185913, mae: 6.508067, mean_q: 13.112427
 97222/100000: episode: 2373, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 186.485, mean reward: 1.865 [1.454, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.082, 10.098], loss: 753.543823, mae: 4.607523, mean_q: 11.991217
 97322/100000: episode: 2374, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.977, mean reward: 1.890 [1.460, 3.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.374, 10.149], loss: 1248.281128, mae: 6.167037, mean_q: 12.361187
 97422/100000: episode: 2375, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.037, mean reward: 1.900 [1.442, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.968, 10.098], loss: 822.112976, mae: 5.256746, mean_q: 12.102085
 97522/100000: episode: 2376, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 206.226, mean reward: 2.062 [1.500, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.434, 10.098], loss: 1006.281860, mae: 5.373875, mean_q: 11.717499
 97622/100000: episode: 2377, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 191.039, mean reward: 1.910 [1.474, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.098], loss: 754.072998, mae: 4.073357, mean_q: 10.784911
 97722/100000: episode: 2378, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 193.536, mean reward: 1.935 [1.460, 5.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.628, 10.098], loss: 763.723267, mae: 4.524876, mean_q: 10.968159
 97822/100000: episode: 2379, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 182.020, mean reward: 1.820 [1.440, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.667, 10.146], loss: 1356.492065, mae: 6.382702, mean_q: 11.666803
 97922/100000: episode: 2380, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 178.518, mean reward: 1.785 [1.441, 2.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.575, 10.117], loss: 1132.877319, mae: 6.133589, mean_q: 12.254189
 98022/100000: episode: 2381, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 187.783, mean reward: 1.878 [1.461, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.904, 10.098], loss: 462.621399, mae: 3.254482, mean_q: 10.140635
 98122/100000: episode: 2382, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.870, mean reward: 1.959 [1.439, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.699, 10.209], loss: 1577.085938, mae: 6.651613, mean_q: 11.379198
 98222/100000: episode: 2383, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 198.517, mean reward: 1.985 [1.444, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.157], loss: 601.605591, mae: 4.682152, mean_q: 11.453933
 98322/100000: episode: 2384, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.897, mean reward: 2.009 [1.522, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.098], loss: 893.610962, mae: 4.594429, mean_q: 10.603985
 98422/100000: episode: 2385, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 220.640, mean reward: 2.206 [1.498, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.369, 10.098], loss: 552.223022, mae: 3.610330, mean_q: 9.826743
 98522/100000: episode: 2386, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 181.712, mean reward: 1.817 [1.441, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.229, 10.098], loss: 758.463684, mae: 4.465584, mean_q: 9.885780
 98622/100000: episode: 2387, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.944, mean reward: 1.919 [1.472, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.818, 10.260], loss: 688.566650, mae: 4.073609, mean_q: 9.924785
 98722/100000: episode: 2388, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 189.910, mean reward: 1.899 [1.445, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.824, 10.250], loss: 841.770386, mae: 3.981637, mean_q: 9.110245
 98822/100000: episode: 2389, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 197.972, mean reward: 1.980 [1.452, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.039, 10.098], loss: 765.319824, mae: 4.589414, mean_q: 10.132758
 98922/100000: episode: 2390, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 176.812, mean reward: 1.768 [1.474, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.117, 10.098], loss: 1488.126831, mae: 7.251341, mean_q: 11.790523
 99022/100000: episode: 2391, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 218.578, mean reward: 2.186 [1.490, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.067, 10.098], loss: 585.095764, mae: 3.573820, mean_q: 9.061175
 99122/100000: episode: 2392, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.603, mean reward: 1.816 [1.453, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.784, 10.136], loss: 241.675797, mae: 2.487843, mean_q: 8.404459
 99222/100000: episode: 2393, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.399, mean reward: 1.914 [1.449, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.098], loss: 796.722900, mae: 4.226303, mean_q: 8.831575
 99322/100000: episode: 2394, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.617, mean reward: 1.936 [1.458, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.867, 10.265], loss: 1264.892578, mae: 5.930509, mean_q: 9.901536
 99422/100000: episode: 2395, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.701, mean reward: 1.947 [1.504, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.895, 10.098], loss: 796.168823, mae: 4.101880, mean_q: 9.235003
 99522/100000: episode: 2396, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 187.392, mean reward: 1.874 [1.458, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.070, 10.164], loss: 738.213074, mae: 3.919070, mean_q: 9.286836
 99622/100000: episode: 2397, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 196.558, mean reward: 1.966 [1.489, 5.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.957, 10.169], loss: 673.103333, mae: 3.660318, mean_q: 8.826300
 99722/100000: episode: 2398, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 214.130, mean reward: 2.141 [1.457, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.734, 10.152], loss: 317.305145, mae: 2.638514, mean_q: 7.906583
 99822/100000: episode: 2399, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 197.703, mean reward: 1.977 [1.448, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.334, 10.098], loss: 227.621658, mae: 1.810744, mean_q: 6.565955
 99922/100000: episode: 2400, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 192.976, mean reward: 1.930 [1.502, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.123, 10.098], loss: 312.054626, mae: 2.429195, mean_q: 6.901169
done, took 615.781 seconds
[Info] End Importance Splitting. Falsification occurred 29 times.
