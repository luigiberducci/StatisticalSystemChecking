Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 574, episode reward: 195.189, mean reward: 1.952 [1.439, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.558, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 191.334, mean reward: 1.913 [1.447, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.519, 10.119], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 191.683, mean reward: 1.917 [1.481, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.617, 10.146], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 193.872, mean reward: 1.939 [1.464, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.857, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.072s, episode steps: 100, steps per second: 1383, episode reward: 185.751, mean reward: 1.858 [1.468, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 194.016, mean reward: 1.940 [1.439, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.433, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 198.080, mean reward: 1.981 [1.441, 5.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.144], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 190.939, mean reward: 1.909 [1.453, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.798, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.073s, episode steps: 100, steps per second: 1378, episode reward: 203.640, mean reward: 2.036 [1.457, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.284, 10.299], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.066s, episode steps: 100, steps per second: 1516, episode reward: 219.721, mean reward: 2.197 [1.487, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.261, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 190.096, mean reward: 1.901 [1.439, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 186.421, mean reward: 1.864 [1.452, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.123, 10.173], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 201.901, mean reward: 2.019 [1.434, 6.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.134], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.079s, episode steps: 100, steps per second: 1262, episode reward: 182.923, mean reward: 1.829 [1.445, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.326, 10.133], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.065s, episode steps: 100, steps per second: 1533, episode reward: 192.903, mean reward: 1.929 [1.464, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.071s, episode steps: 100, steps per second: 1415, episode reward: 230.140, mean reward: 2.301 [1.450, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.307], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 210.585, mean reward: 2.106 [1.449, 6.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.336, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 191.219, mean reward: 1.912 [1.453, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.148, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.068s, episode steps: 100, steps per second: 1472, episode reward: 181.891, mean reward: 1.819 [1.462, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.628, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.069s, episode steps: 100, steps per second: 1448, episode reward: 183.661, mean reward: 1.837 [1.438, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.267, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.070s, episode steps: 100, steps per second: 1434, episode reward: 231.397, mean reward: 2.314 [1.475, 8.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.044, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 230.333, mean reward: 2.303 [1.440, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.967, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 197.439, mean reward: 1.974 [1.475, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.386, 10.217], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.065s, episode steps: 100, steps per second: 1532, episode reward: 181.618, mean reward: 1.816 [1.495, 2.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.397, 10.180], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 176.329, mean reward: 1.763 [1.432, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.995, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 197.617, mean reward: 1.976 [1.486, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.341, 10.285], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.069s, episode steps: 100, steps per second: 1455, episode reward: 184.146, mean reward: 1.841 [1.451, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.737, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 190.378, mean reward: 1.904 [1.475, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.439, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.070s, episode steps: 100, steps per second: 1429, episode reward: 188.403, mean reward: 1.884 [1.451, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 188.257, mean reward: 1.883 [1.457, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.331, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 213.747, mean reward: 2.137 [1.482, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.305, 10.362], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.065s, episode steps: 100, steps per second: 1540, episode reward: 188.192, mean reward: 1.882 [1.454, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.394, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 191.418, mean reward: 1.914 [1.447, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.740, 10.154], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 181.665, mean reward: 1.817 [1.478, 2.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.768, 10.185], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.079s, episode steps: 100, steps per second: 1264, episode reward: 194.841, mean reward: 1.948 [1.463, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.944, 10.297], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.067s, episode steps: 100, steps per second: 1494, episode reward: 186.791, mean reward: 1.868 [1.449, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.430, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 189.376, mean reward: 1.894 [1.433, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.554, 10.267], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 206.900, mean reward: 2.069 [1.476, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.717, 10.199], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 198.308, mean reward: 1.983 [1.472, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.182, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.071s, episode steps: 100, steps per second: 1409, episode reward: 189.189, mean reward: 1.892 [1.443, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.189, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.070s, episode steps: 100, steps per second: 1437, episode reward: 191.307, mean reward: 1.913 [1.456, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.696, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.070s, episode steps: 100, steps per second: 1423, episode reward: 215.611, mean reward: 2.156 [1.536, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.085, 10.399], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 199.236, mean reward: 1.992 [1.437, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.403, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.069s, episode steps: 100, steps per second: 1453, episode reward: 190.306, mean reward: 1.903 [1.472, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.271, 10.114], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 202.677, mean reward: 2.027 [1.452, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 197.171, mean reward: 1.972 [1.432, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.014, 10.120], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 194.677, mean reward: 1.947 [1.443, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.299, 10.370], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 206.085, mean reward: 2.061 [1.466, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.299, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 192.873, mean reward: 1.929 [1.466, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.085s, episode steps: 100, steps per second: 1171, episode reward: 184.006, mean reward: 1.840 [1.482, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.835, 10.231], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 184.709, mean reward: 1.847 [1.445, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.169], loss: 0.163156, mae: 0.392853, mean_q: 2.975366
  5200/100000: episode: 52, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 191.542, mean reward: 1.915 [1.458, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.714, 10.098], loss: 0.108450, mae: 0.321160, mean_q: 3.314938
  5300/100000: episode: 53, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.432, mean reward: 1.854 [1.448, 5.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.761, 10.237], loss: 0.113717, mae: 0.328174, mean_q: 3.526434
  5400/100000: episode: 54, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.330, mean reward: 1.923 [1.499, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.405, 10.098], loss: 0.114992, mae: 0.316619, mean_q: 3.631312
  5500/100000: episode: 55, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.593, mean reward: 1.836 [1.456, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.119, 10.175], loss: 0.113457, mae: 0.323050, mean_q: 3.717754
  5600/100000: episode: 56, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 192.010, mean reward: 1.920 [1.443, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.232], loss: 0.121047, mae: 0.323795, mean_q: 3.761680
  5700/100000: episode: 57, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 213.107, mean reward: 2.131 [1.555, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.678, 10.334], loss: 0.126661, mae: 0.330192, mean_q: 3.800844
  5800/100000: episode: 58, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 187.834, mean reward: 1.878 [1.438, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.938, 10.310], loss: 0.115612, mae: 0.322339, mean_q: 3.824900
  5900/100000: episode: 59, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 215.314, mean reward: 2.153 [1.461, 5.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.005, 10.098], loss: 0.119242, mae: 0.320477, mean_q: 3.843727
  6000/100000: episode: 60, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 184.210, mean reward: 1.842 [1.436, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.223, 10.144], loss: 0.102867, mae: 0.315323, mean_q: 3.841636
  6100/100000: episode: 61, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 203.264, mean reward: 2.033 [1.462, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.223, 10.098], loss: 0.125793, mae: 0.325182, mean_q: 3.847888
  6200/100000: episode: 62, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 190.716, mean reward: 1.907 [1.465, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.341], loss: 0.126544, mae: 0.333314, mean_q: 3.862178
  6300/100000: episode: 63, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.881, mean reward: 1.869 [1.445, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.550, 10.254], loss: 0.120395, mae: 0.328223, mean_q: 3.843734
  6400/100000: episode: 64, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.130, mean reward: 1.861 [1.440, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.098], loss: 0.127353, mae: 0.335257, mean_q: 3.868986
  6500/100000: episode: 65, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 209.106, mean reward: 2.091 [1.436, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.532, 10.098], loss: 0.127143, mae: 0.330214, mean_q: 3.862456
  6600/100000: episode: 66, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.415, mean reward: 1.934 [1.502, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.651, 10.175], loss: 0.118922, mae: 0.324606, mean_q: 3.872191
  6700/100000: episode: 67, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.316, mean reward: 1.933 [1.524, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.776, 10.098], loss: 0.109143, mae: 0.317323, mean_q: 3.863153
  6800/100000: episode: 68, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 205.612, mean reward: 2.056 [1.509, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.233], loss: 0.095496, mae: 0.300639, mean_q: 3.831260
  6900/100000: episode: 69, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.303, mean reward: 1.923 [1.467, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.779, 10.098], loss: 0.100055, mae: 0.309774, mean_q: 3.844690
  7000/100000: episode: 70, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 203.375, mean reward: 2.034 [1.484, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.889, 10.098], loss: 0.105262, mae: 0.311157, mean_q: 3.849804
  7100/100000: episode: 71, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 184.893, mean reward: 1.849 [1.472, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.119], loss: 0.109386, mae: 0.320595, mean_q: 3.844713
  7200/100000: episode: 72, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 203.083, mean reward: 2.031 [1.441, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.800, 10.326], loss: 0.103411, mae: 0.309904, mean_q: 3.839879
  7300/100000: episode: 73, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 211.351, mean reward: 2.114 [1.450, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.428], loss: 0.092612, mae: 0.301087, mean_q: 3.843151
  7400/100000: episode: 74, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.299, mean reward: 1.893 [1.488, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.993, 10.279], loss: 0.101382, mae: 0.313151, mean_q: 3.868118
  7500/100000: episode: 75, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 193.897, mean reward: 1.939 [1.456, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.240, 10.292], loss: 0.085969, mae: 0.293709, mean_q: 3.842523
  7600/100000: episode: 76, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 199.448, mean reward: 1.994 [1.483, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.411, 10.098], loss: 0.094817, mae: 0.304691, mean_q: 3.860557
  7700/100000: episode: 77, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 212.964, mean reward: 2.130 [1.489, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.055, 10.124], loss: 0.091896, mae: 0.300303, mean_q: 3.851961
  7800/100000: episode: 78, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 178.546, mean reward: 1.785 [1.439, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.296], loss: 0.094790, mae: 0.310557, mean_q: 3.859828
  7900/100000: episode: 79, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 194.117, mean reward: 1.941 [1.435, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.188, 10.098], loss: 0.096023, mae: 0.303909, mean_q: 3.862155
  8000/100000: episode: 80, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 232.373, mean reward: 2.324 [1.520, 6.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.156, 10.098], loss: 0.102649, mae: 0.317562, mean_q: 3.860310
  8100/100000: episode: 81, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.669, mean reward: 1.967 [1.496, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.569, 10.300], loss: 0.100478, mae: 0.307894, mean_q: 3.853477
  8200/100000: episode: 82, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 194.849, mean reward: 1.948 [1.471, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.224, 10.126], loss: 0.125456, mae: 0.335143, mean_q: 3.869406
  8300/100000: episode: 83, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 221.276, mean reward: 2.213 [1.522, 6.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.010, 10.098], loss: 0.095968, mae: 0.305995, mean_q: 3.868970
  8400/100000: episode: 84, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 211.913, mean reward: 2.119 [1.480, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.972, 10.298], loss: 0.108766, mae: 0.323869, mean_q: 3.888140
  8500/100000: episode: 85, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 186.562, mean reward: 1.866 [1.457, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.361, 10.098], loss: 0.095203, mae: 0.304338, mean_q: 3.864987
  8600/100000: episode: 86, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 220.564, mean reward: 2.206 [1.518, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.921, 10.098], loss: 0.113048, mae: 0.328249, mean_q: 3.898357
  8700/100000: episode: 87, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 177.289, mean reward: 1.773 [1.462, 2.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.438, 10.098], loss: 0.104446, mae: 0.311284, mean_q: 3.890075
  8800/100000: episode: 88, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 202.166, mean reward: 2.022 [1.478, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.102, 10.098], loss: 0.113550, mae: 0.327964, mean_q: 3.881271
  8900/100000: episode: 89, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 180.293, mean reward: 1.803 [1.495, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.554, 10.143], loss: 0.100204, mae: 0.312582, mean_q: 3.882969
  9000/100000: episode: 90, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 185.678, mean reward: 1.857 [1.471, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.114], loss: 0.101699, mae: 0.313151, mean_q: 3.894672
  9100/100000: episode: 91, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 177.049, mean reward: 1.770 [1.460, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.905, 10.141], loss: 0.102649, mae: 0.313552, mean_q: 3.894390
  9200/100000: episode: 92, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.736, mean reward: 1.877 [1.453, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.759, 10.098], loss: 0.096230, mae: 0.308681, mean_q: 3.885326
  9300/100000: episode: 93, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 202.772, mean reward: 2.028 [1.502, 4.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.478, 10.152], loss: 0.099717, mae: 0.314454, mean_q: 3.874318
  9400/100000: episode: 94, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.777, mean reward: 1.908 [1.471, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.489, 10.098], loss: 0.098310, mae: 0.306275, mean_q: 3.869099
  9500/100000: episode: 95, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 184.302, mean reward: 1.843 [1.450, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.395, 10.098], loss: 0.098360, mae: 0.305459, mean_q: 3.872770
  9600/100000: episode: 96, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.966, mean reward: 1.800 [1.450, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.132, 10.098], loss: 0.089179, mae: 0.302639, mean_q: 3.861964
  9700/100000: episode: 97, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 180.363, mean reward: 1.804 [1.456, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.213, 10.098], loss: 0.097800, mae: 0.303217, mean_q: 3.860968
  9800/100000: episode: 98, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 203.041, mean reward: 2.030 [1.436, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.664, 10.098], loss: 0.096268, mae: 0.302342, mean_q: 3.845647
  9900/100000: episode: 99, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 196.540, mean reward: 1.965 [1.475, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.006, 10.410], loss: 0.112399, mae: 0.318784, mean_q: 3.854048
 10000/100000: episode: 100, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.193, mean reward: 1.972 [1.476, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.209], loss: 0.094816, mae: 0.306020, mean_q: 3.860857
 10100/100000: episode: 101, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 188.614, mean reward: 1.886 [1.466, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.179, 10.098], loss: 0.103953, mae: 0.306837, mean_q: 3.871173
 10200/100000: episode: 102, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 185.860, mean reward: 1.859 [1.470, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.291], loss: 0.101872, mae: 0.316562, mean_q: 3.884537
 10300/100000: episode: 103, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 206.962, mean reward: 2.070 [1.482, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.120, 10.098], loss: 0.094366, mae: 0.307012, mean_q: 3.874675
 10400/100000: episode: 104, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.626, mean reward: 1.936 [1.530, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.098], loss: 0.106608, mae: 0.313965, mean_q: 3.875203
 10500/100000: episode: 105, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.830, mean reward: 1.898 [1.438, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.205], loss: 0.089133, mae: 0.295854, mean_q: 3.879176
 10600/100000: episode: 106, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 182.778, mean reward: 1.828 [1.474, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.188, 10.101], loss: 0.094193, mae: 0.305568, mean_q: 3.891003
 10700/100000: episode: 107, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 177.914, mean reward: 1.779 [1.449, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.534, 10.098], loss: 0.095432, mae: 0.302205, mean_q: 3.884219
 10800/100000: episode: 108, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 182.897, mean reward: 1.829 [1.456, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.843, 10.217], loss: 0.088645, mae: 0.296969, mean_q: 3.846484
 10900/100000: episode: 109, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 191.683, mean reward: 1.917 [1.479, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.838, 10.098], loss: 0.087591, mae: 0.298818, mean_q: 3.846090
 11000/100000: episode: 110, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 198.773, mean reward: 1.988 [1.510, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.711, 10.098], loss: 0.087494, mae: 0.303018, mean_q: 3.871337
 11100/100000: episode: 111, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 252.033, mean reward: 2.520 [1.496, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.545, 10.098], loss: 0.093035, mae: 0.306994, mean_q: 3.875729
 11200/100000: episode: 112, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 195.445, mean reward: 1.954 [1.439, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.655, 10.209], loss: 0.102675, mae: 0.319227, mean_q: 3.868865
 11300/100000: episode: 113, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 212.420, mean reward: 2.124 [1.524, 4.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.499, 10.098], loss: 0.101126, mae: 0.316941, mean_q: 3.886423
 11400/100000: episode: 114, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 179.152, mean reward: 1.792 [1.442, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.138], loss: 0.100282, mae: 0.311964, mean_q: 3.882462
 11500/100000: episode: 115, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.719, mean reward: 1.937 [1.440, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.098], loss: 0.092077, mae: 0.312078, mean_q: 3.890363
 11600/100000: episode: 116, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 207.931, mean reward: 2.079 [1.475, 4.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.102], loss: 0.094639, mae: 0.310927, mean_q: 3.891323
 11700/100000: episode: 117, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.352, mean reward: 1.844 [1.454, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.570, 10.212], loss: 0.091434, mae: 0.301354, mean_q: 3.885798
 11800/100000: episode: 118, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 183.641, mean reward: 1.836 [1.457, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.111, 10.101], loss: 0.100718, mae: 0.314122, mean_q: 3.872462
 11900/100000: episode: 119, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.226, mean reward: 1.852 [1.460, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.899, 10.098], loss: 0.096970, mae: 0.308615, mean_q: 3.871302
 12000/100000: episode: 120, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.491, mean reward: 1.905 [1.493, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.017, 10.098], loss: 0.088012, mae: 0.306590, mean_q: 3.878371
 12100/100000: episode: 121, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 198.710, mean reward: 1.987 [1.483, 5.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.453, 10.098], loss: 0.088456, mae: 0.296767, mean_q: 3.876434
 12200/100000: episode: 122, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.302, mean reward: 1.883 [1.497, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.832, 10.212], loss: 0.095240, mae: 0.304767, mean_q: 3.874453
 12300/100000: episode: 123, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 195.363, mean reward: 1.954 [1.441, 5.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.976, 10.171], loss: 0.080119, mae: 0.284713, mean_q: 3.839944
 12400/100000: episode: 124, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 195.767, mean reward: 1.958 [1.463, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.919, 10.098], loss: 0.091920, mae: 0.310350, mean_q: 3.857470
 12500/100000: episode: 125, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.893, mean reward: 2.049 [1.494, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.722, 10.098], loss: 0.104708, mae: 0.322312, mean_q: 3.887893
 12600/100000: episode: 126, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 207.542, mean reward: 2.075 [1.435, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.063, 10.098], loss: 0.095323, mae: 0.307716, mean_q: 3.870885
 12700/100000: episode: 127, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 199.966, mean reward: 2.000 [1.510, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.805, 10.113], loss: 0.101455, mae: 0.312655, mean_q: 3.864138
 12800/100000: episode: 128, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 203.447, mean reward: 2.034 [1.474, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.324], loss: 0.102849, mae: 0.309750, mean_q: 3.852849
 12900/100000: episode: 129, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 177.826, mean reward: 1.778 [1.445, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.445, 10.098], loss: 0.093123, mae: 0.305935, mean_q: 3.868384
 13000/100000: episode: 130, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.240, mean reward: 1.962 [1.487, 5.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.753, 10.367], loss: 0.085623, mae: 0.299358, mean_q: 3.851611
 13100/100000: episode: 131, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.786, mean reward: 1.868 [1.470, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.622, 10.205], loss: 0.091055, mae: 0.306736, mean_q: 3.834294
 13200/100000: episode: 132, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 196.862, mean reward: 1.969 [1.464, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.374, 10.098], loss: 0.097068, mae: 0.310565, mean_q: 3.851470
 13300/100000: episode: 133, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.883, mean reward: 1.929 [1.441, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.753, 10.098], loss: 0.090302, mae: 0.303504, mean_q: 3.829841
 13400/100000: episode: 134, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 209.438, mean reward: 2.094 [1.483, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.766, 10.226], loss: 0.087154, mae: 0.289667, mean_q: 3.828561
 13500/100000: episode: 135, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 175.020, mean reward: 1.750 [1.433, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.496, 10.306], loss: 0.081827, mae: 0.289147, mean_q: 3.842948
 13600/100000: episode: 136, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.588, mean reward: 1.856 [1.451, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.956, 10.136], loss: 0.096948, mae: 0.304598, mean_q: 3.831859
 13700/100000: episode: 137, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 224.191, mean reward: 2.242 [1.534, 8.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.895, 10.222], loss: 0.094043, mae: 0.298302, mean_q: 3.827451
 13800/100000: episode: 138, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 173.586, mean reward: 1.736 [1.453, 2.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.633, 10.219], loss: 0.089395, mae: 0.294858, mean_q: 3.826020
 13900/100000: episode: 139, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 184.869, mean reward: 1.849 [1.487, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.006, 10.098], loss: 0.090934, mae: 0.303409, mean_q: 3.841075
 14000/100000: episode: 140, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 203.813, mean reward: 2.038 [1.479, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.528, 10.098], loss: 0.096057, mae: 0.301925, mean_q: 3.836805
 14100/100000: episode: 141, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 202.369, mean reward: 2.024 [1.447, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.028, 10.165], loss: 0.096721, mae: 0.311247, mean_q: 3.835205
 14200/100000: episode: 142, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 188.692, mean reward: 1.887 [1.433, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.011, 10.138], loss: 0.085559, mae: 0.286578, mean_q: 3.817365
 14300/100000: episode: 143, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 202.943, mean reward: 2.029 [1.466, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.098], loss: 0.090710, mae: 0.296475, mean_q: 3.819486
 14400/100000: episode: 144, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 207.836, mean reward: 2.078 [1.466, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.024, 10.294], loss: 0.101411, mae: 0.312481, mean_q: 3.836527
 14500/100000: episode: 145, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.188, mean reward: 1.832 [1.457, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.587, 10.098], loss: 0.101083, mae: 0.311923, mean_q: 3.829370
 14600/100000: episode: 146, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 193.770, mean reward: 1.938 [1.461, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.258], loss: 0.095210, mae: 0.302923, mean_q: 3.830775
 14700/100000: episode: 147, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 200.959, mean reward: 2.010 [1.466, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.287], loss: 0.099938, mae: 0.315766, mean_q: 3.842547
 14800/100000: episode: 148, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.018, mean reward: 1.940 [1.473, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.528, 10.253], loss: 0.089441, mae: 0.295772, mean_q: 3.836519
 14900/100000: episode: 149, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.611, mean reward: 1.896 [1.465, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.720, 10.098], loss: 0.096152, mae: 0.313087, mean_q: 3.880359
[Info] 1-TH LEVEL FOUND: 4.542059421539307, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.978s, episode steps: 100, steps per second: 20, episode reward: 205.667, mean reward: 2.057 [1.557, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.783, 10.248], loss: 0.094914, mae: 0.302471, mean_q: 3.836056
 15045/100000: episode: 151, duration: 0.264s, episode steps: 45, steps per second: 170, episode reward: 119.981, mean reward: 2.666 [1.751, 5.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.728, 10.494], loss: 0.080913, mae: 0.290018, mean_q: 3.867973
 15090/100000: episode: 152, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 98.885, mean reward: 2.197 [1.475, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.327, 10.167], loss: 0.085912, mae: 0.296983, mean_q: 3.855015
 15100/100000: episode: 153, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 31.646, mean reward: 3.165 [2.365, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.536, 10.100], loss: 0.093599, mae: 0.309562, mean_q: 3.846163
 15110/100000: episode: 154, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 23.797, mean reward: 2.380 [2.017, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.409, 10.100], loss: 0.109150, mae: 0.345329, mean_q: 3.921677
 15120/100000: episode: 155, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 25.709, mean reward: 2.571 [2.016, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.364, 10.100], loss: 0.085223, mae: 0.288607, mean_q: 3.827980
 15130/100000: episode: 156, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 24.995, mean reward: 2.500 [2.080, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.349, 10.100], loss: 0.081564, mae: 0.307888, mean_q: 3.892143
 15153/100000: episode: 157, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 55.913, mean reward: 2.431 [1.792, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.386, 10.100], loss: 0.088770, mae: 0.300680, mean_q: 3.882886
 15198/100000: episode: 158, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 89.236, mean reward: 1.983 [1.496, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.573, 10.114], loss: 0.113058, mae: 0.327513, mean_q: 3.905735
 15208/100000: episode: 159, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 26.018, mean reward: 2.602 [2.097, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.335, 10.100], loss: 0.091883, mae: 0.306326, mean_q: 3.869455
 15255/100000: episode: 160, duration: 0.252s, episode steps: 47, steps per second: 187, episode reward: 129.631, mean reward: 2.758 [1.647, 5.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.676, 10.194], loss: 0.100975, mae: 0.314867, mean_q: 3.898195
 15265/100000: episode: 161, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 27.323, mean reward: 2.732 [2.318, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.927, 10.100], loss: 0.093666, mae: 0.308477, mean_q: 3.900985
 15311/100000: episode: 162, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 103.109, mean reward: 2.242 [1.686, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.218, 10.247], loss: 0.108844, mae: 0.314683, mean_q: 3.878046
 15335/100000: episode: 163, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 59.945, mean reward: 2.498 [1.778, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.186, 10.100], loss: 0.103954, mae: 0.313179, mean_q: 3.892796
 15345/100000: episode: 164, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 24.933, mean reward: 2.493 [2.132, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.158, 10.100], loss: 0.116386, mae: 0.334179, mean_q: 3.970769
 15391/100000: episode: 165, duration: 0.240s, episode steps: 46, steps per second: 192, episode reward: 116.295, mean reward: 2.528 [1.515, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-1.794, 10.218], loss: 0.116106, mae: 0.340730, mean_q: 3.899495
 15436/100000: episode: 166, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 104.219, mean reward: 2.316 [1.583, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.019, 10.464], loss: 0.117257, mae: 0.340472, mean_q: 3.943052
 15446/100000: episode: 167, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 25.266, mean reward: 2.527 [2.164, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.272, 10.100], loss: 0.103101, mae: 0.326308, mean_q: 3.855697
 15493/100000: episode: 168, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 102.966, mean reward: 2.191 [1.566, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.606, 10.326], loss: 0.100209, mae: 0.308712, mean_q: 3.933128
 15517/100000: episode: 169, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 58.841, mean reward: 2.452 [1.573, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.295, 10.100], loss: 0.088299, mae: 0.308035, mean_q: 3.908660
 15562/100000: episode: 170, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 106.679, mean reward: 2.371 [1.712, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.158, 10.336], loss: 0.101892, mae: 0.331190, mean_q: 3.944327
 15572/100000: episode: 171, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 24.225, mean reward: 2.422 [2.035, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.262, 10.100], loss: 0.108518, mae: 0.334587, mean_q: 3.939702
 15617/100000: episode: 172, duration: 0.223s, episode steps: 45, steps per second: 201, episode reward: 86.685, mean reward: 1.926 [1.469, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.754, 10.126], loss: 0.094404, mae: 0.317660, mean_q: 3.979495
 15640/100000: episode: 173, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 70.669, mean reward: 3.073 [2.058, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.543, 10.100], loss: 0.134382, mae: 0.341976, mean_q: 3.998220
 15664/100000: episode: 174, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 64.598, mean reward: 2.692 [2.100, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.458, 10.100], loss: 0.100514, mae: 0.324536, mean_q: 3.922173
 15708/100000: episode: 175, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 102.655, mean reward: 2.333 [1.683, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.508, 10.293], loss: 0.132829, mae: 0.350631, mean_q: 3.994184
 15754/100000: episode: 176, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 121.171, mean reward: 2.634 [1.934, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.645, 10.326], loss: 0.095602, mae: 0.319173, mean_q: 3.975325
 15799/100000: episode: 177, duration: 0.216s, episode steps: 45, steps per second: 208, episode reward: 97.183, mean reward: 2.160 [1.488, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.251, 10.160], loss: 0.096298, mae: 0.310209, mean_q: 3.935159
 15822/100000: episode: 178, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 54.114, mean reward: 2.353 [1.682, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.617, 10.100], loss: 0.110564, mae: 0.338208, mean_q: 4.005783
 15845/100000: episode: 179, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 62.670, mean reward: 2.725 [2.057, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.564, 10.100], loss: 0.104448, mae: 0.326691, mean_q: 3.983032
 15855/100000: episode: 180, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 24.206, mean reward: 2.421 [1.949, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.262, 10.100], loss: 0.113379, mae: 0.325396, mean_q: 4.005916
 15900/100000: episode: 181, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 100.421, mean reward: 2.232 [1.764, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.336, 10.340], loss: 0.092536, mae: 0.312835, mean_q: 4.011795
 15910/100000: episode: 182, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 29.311, mean reward: 2.931 [2.064, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.203, 10.100], loss: 0.122874, mae: 0.335598, mean_q: 3.992910
 15954/100000: episode: 183, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 86.290, mean reward: 1.961 [1.443, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.744, 10.100], loss: 0.119217, mae: 0.336012, mean_q: 4.000794
 16001/100000: episode: 184, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 111.705, mean reward: 2.377 [1.676, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.236, 10.219], loss: 0.096125, mae: 0.318931, mean_q: 4.003403
 16011/100000: episode: 185, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 27.506, mean reward: 2.751 [2.122, 5.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.397, 10.100], loss: 0.075205, mae: 0.293722, mean_q: 4.020302
 16059/100000: episode: 186, duration: 0.253s, episode steps: 48, steps per second: 190, episode reward: 115.831, mean reward: 2.413 [1.526, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.574, 10.167], loss: 0.091759, mae: 0.311002, mean_q: 3.998595
 16069/100000: episode: 187, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 23.350, mean reward: 2.335 [1.984, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.498, 10.100], loss: 0.146875, mae: 0.371352, mean_q: 4.048938
 16115/100000: episode: 188, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 94.585, mean reward: 2.056 [1.495, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.035, 10.184], loss: 0.115910, mae: 0.328798, mean_q: 4.019242
 16163/100000: episode: 189, duration: 0.253s, episode steps: 48, steps per second: 190, episode reward: 115.857, mean reward: 2.414 [1.905, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.644, 10.453], loss: 0.128391, mae: 0.350162, mean_q: 4.007170
 16211/100000: episode: 190, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 118.780, mean reward: 2.475 [1.850, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.837, 10.438], loss: 0.102162, mae: 0.322600, mean_q: 4.018496
 16256/100000: episode: 191, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 104.037, mean reward: 2.312 [1.712, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.646, 10.291], loss: 0.115533, mae: 0.341333, mean_q: 4.043082
 16301/100000: episode: 192, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 98.623, mean reward: 2.192 [1.489, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.852, 10.422], loss: 0.098488, mae: 0.318081, mean_q: 4.031665
 16311/100000: episode: 193, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 22.405, mean reward: 2.240 [1.962, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.375, 10.100], loss: 0.098204, mae: 0.309559, mean_q: 4.019927
 16358/100000: episode: 194, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 101.662, mean reward: 2.163 [1.585, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.683, 10.100], loss: 0.108212, mae: 0.317330, mean_q: 4.045426
 16403/100000: episode: 195, duration: 0.248s, episode steps: 45, steps per second: 181, episode reward: 113.670, mean reward: 2.526 [1.845, 4.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.390, 10.329], loss: 0.097280, mae: 0.317149, mean_q: 4.021531
 16447/100000: episode: 196, duration: 0.217s, episode steps: 44, steps per second: 203, episode reward: 97.496, mean reward: 2.216 [1.498, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.431, 10.100], loss: 0.118386, mae: 0.338332, mean_q: 4.039468
 16495/100000: episode: 197, duration: 0.242s, episode steps: 48, steps per second: 199, episode reward: 120.047, mean reward: 2.501 [1.835, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.249, 10.487], loss: 0.098898, mae: 0.317789, mean_q: 4.044532
 16542/100000: episode: 198, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 111.562, mean reward: 2.374 [1.870, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.326, 10.382], loss: 0.111549, mae: 0.333270, mean_q: 4.083455
 16566/100000: episode: 199, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 53.652, mean reward: 2.235 [1.631, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.999, 10.100], loss: 0.131123, mae: 0.339346, mean_q: 4.103538
 16576/100000: episode: 200, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 28.640, mean reward: 2.864 [2.293, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.861, 10.100], loss: 0.126262, mae: 0.368017, mean_q: 4.108703
 16600/100000: episode: 201, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 69.697, mean reward: 2.904 [2.073, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.338, 10.100], loss: 0.090191, mae: 0.318395, mean_q: 4.077466
 16610/100000: episode: 202, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 32.255, mean reward: 3.226 [2.121, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.311, 10.100], loss: 0.086104, mae: 0.308135, mean_q: 4.115234
 16655/100000: episode: 203, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 95.385, mean reward: 2.120 [1.445, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.777, 10.256], loss: 0.108224, mae: 0.337999, mean_q: 4.115240
 16678/100000: episode: 204, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 95.804, mean reward: 4.165 [2.517, 12.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.230, 10.100], loss: 0.089966, mae: 0.305694, mean_q: 4.051273
 16723/100000: episode: 205, duration: 0.236s, episode steps: 45, steps per second: 191, episode reward: 104.125, mean reward: 2.314 [1.520, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.373, 10.146], loss: 0.138982, mae: 0.336457, mean_q: 4.123352
 16747/100000: episode: 206, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 60.800, mean reward: 2.533 [2.002, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.344, 10.100], loss: 0.179014, mae: 0.371112, mean_q: 4.215262
 16757/100000: episode: 207, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 23.254, mean reward: 2.325 [2.093, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.836, 10.100], loss: 0.085250, mae: 0.295881, mean_q: 4.086562
 16801/100000: episode: 208, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 87.120, mean reward: 1.980 [1.452, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.134, 10.291], loss: 0.123690, mae: 0.344869, mean_q: 4.125187
 16845/100000: episode: 209, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 90.970, mean reward: 2.068 [1.501, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.431, 10.103], loss: 0.110850, mae: 0.334227, mean_q: 4.141560
 16855/100000: episode: 210, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 25.510, mean reward: 2.551 [2.038, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.276, 10.100], loss: 0.135339, mae: 0.353505, mean_q: 4.201947
 16879/100000: episode: 211, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 74.222, mean reward: 3.093 [2.093, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.229, 10.100], loss: 0.188871, mae: 0.379220, mean_q: 4.204754
 16903/100000: episode: 212, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 64.422, mean reward: 2.684 [1.968, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.818, 10.100], loss: 0.097404, mae: 0.316674, mean_q: 4.108819
 16927/100000: episode: 213, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 56.192, mean reward: 2.341 [1.935, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.482, 10.100], loss: 0.102164, mae: 0.330919, mean_q: 4.157965
 16974/100000: episode: 214, duration: 0.224s, episode steps: 47, steps per second: 209, episode reward: 101.333, mean reward: 2.156 [1.807, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.505, 10.246], loss: 0.138921, mae: 0.360173, mean_q: 4.164048
 17019/100000: episode: 215, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 133.217, mean reward: 2.960 [1.843, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.464, 10.305], loss: 0.116191, mae: 0.351153, mean_q: 4.181292
 17042/100000: episode: 216, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 69.549, mean reward: 3.024 [2.381, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.168, 10.100], loss: 0.098938, mae: 0.317834, mean_q: 4.178012
 17065/100000: episode: 217, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 62.198, mean reward: 2.704 [1.769, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.428, 10.100], loss: 0.091731, mae: 0.318804, mean_q: 4.175871
 17075/100000: episode: 218, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 32.057, mean reward: 3.206 [2.387, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.409, 10.100], loss: 0.120368, mae: 0.365509, mean_q: 4.265805
 17085/100000: episode: 219, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 23.524, mean reward: 2.352 [2.028, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.764, 10.100], loss: 0.105275, mae: 0.312014, mean_q: 4.125302
 17132/100000: episode: 220, duration: 0.230s, episode steps: 47, steps per second: 205, episode reward: 146.132, mean reward: 3.109 [1.912, 5.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.085, 10.426], loss: 0.128296, mae: 0.344139, mean_q: 4.163641
 17180/100000: episode: 221, duration: 0.249s, episode steps: 48, steps per second: 192, episode reward: 120.379, mean reward: 2.508 [1.959, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.665, 10.450], loss: 0.159700, mae: 0.373748, mean_q: 4.197713
 17190/100000: episode: 222, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 28.900, mean reward: 2.890 [1.945, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.361, 10.100], loss: 0.112307, mae: 0.359098, mean_q: 4.133110
 17237/100000: episode: 223, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 122.840, mean reward: 2.614 [1.760, 5.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.252, 10.525], loss: 0.128450, mae: 0.367337, mean_q: 4.251057
 17284/100000: episode: 224, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 94.746, mean reward: 2.016 [1.481, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.429, 10.206], loss: 0.130037, mae: 0.355250, mean_q: 4.238550
 17294/100000: episode: 225, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 24.846, mean reward: 2.485 [2.223, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.284, 10.100], loss: 0.119520, mae: 0.350269, mean_q: 4.254243
 17304/100000: episode: 226, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 29.083, mean reward: 2.908 [2.110, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.312, 10.100], loss: 0.115573, mae: 0.345618, mean_q: 4.229191
 17351/100000: episode: 227, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 102.965, mean reward: 2.191 [1.741, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.337, 10.303], loss: 0.129885, mae: 0.363982, mean_q: 4.263283
 17361/100000: episode: 228, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 27.650, mean reward: 2.765 [1.854, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.319, 10.100], loss: 0.126253, mae: 0.370009, mean_q: 4.371849
 17408/100000: episode: 229, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 97.238, mean reward: 2.069 [1.496, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.490, 10.100], loss: 0.164044, mae: 0.373492, mean_q: 4.302169
 17431/100000: episode: 230, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 88.838, mean reward: 3.863 [2.546, 7.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.705, 10.100], loss: 0.112890, mae: 0.353979, mean_q: 4.338974
 17441/100000: episode: 231, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 26.590, mean reward: 2.659 [1.974, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.226, 10.100], loss: 0.146300, mae: 0.364035, mean_q: 4.305823
 17486/100000: episode: 232, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 124.025, mean reward: 2.756 [2.037, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.432, 10.428], loss: 0.136593, mae: 0.363099, mean_q: 4.284869
 17534/100000: episode: 233, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 121.451, mean reward: 2.530 [1.552, 5.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.304, 10.180], loss: 0.135809, mae: 0.378301, mean_q: 4.335711
 17544/100000: episode: 234, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 18.401, mean reward: 1.840 [1.514, 2.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.196, 10.100], loss: 0.185202, mae: 0.437621, mean_q: 4.425996
 17567/100000: episode: 235, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 69.421, mean reward: 3.018 [2.632, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.557, 10.100], loss: 0.109541, mae: 0.342808, mean_q: 4.285674
 17590/100000: episode: 236, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 60.506, mean reward: 2.631 [1.793, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.124, 10.100], loss: 0.154754, mae: 0.360407, mean_q: 4.331328
 17600/100000: episode: 237, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 29.324, mean reward: 2.932 [2.454, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.332, 10.100], loss: 0.135736, mae: 0.381455, mean_q: 4.360117
 17624/100000: episode: 238, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 61.744, mean reward: 2.573 [1.908, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.619, 10.100], loss: 0.126360, mae: 0.359922, mean_q: 4.281836
 17670/100000: episode: 239, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 112.334, mean reward: 2.442 [1.911, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.936, 10.356], loss: 0.191756, mae: 0.379043, mean_q: 4.351392
[Info] 2-TH LEVEL FOUND: 5.965838432312012, Considering 10/90 traces
 17718/100000: episode: 240, duration: 4.240s, episode steps: 48, steps per second: 11, episode reward: 128.977, mean reward: 2.687 [1.998, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.547, 10.319], loss: 0.128859, mae: 0.364956, mean_q: 4.349827
 17725/100000: episode: 241, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 20.853, mean reward: 2.979 [2.674, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.394, 10.100], loss: 0.099126, mae: 0.327297, mean_q: 4.349788
 17743/100000: episode: 242, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 56.436, mean reward: 3.135 [2.651, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.276, 10.100], loss: 0.144675, mae: 0.381835, mean_q: 4.343388
 17760/100000: episode: 243, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 90.881, mean reward: 5.346 [3.183, 9.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.412, 10.100], loss: 0.253016, mae: 0.451920, mean_q: 4.329788
 17780/100000: episode: 244, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 84.231, mean reward: 4.212 [2.658, 6.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.869, 10.100], loss: 0.167399, mae: 0.419676, mean_q: 4.421674
 17787/100000: episode: 245, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 24.308, mean reward: 3.473 [2.797, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.321, 10.100], loss: 0.103737, mae: 0.332408, mean_q: 4.194393
 17806/100000: episode: 246, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 61.706, mean reward: 3.248 [1.964, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.818, 10.100], loss: 0.182260, mae: 0.401457, mean_q: 4.465361
 17824/100000: episode: 247, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 99.372, mean reward: 5.521 [3.148, 8.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.417, 10.100], loss: 0.166041, mae: 0.400177, mean_q: 4.397637
 17831/100000: episode: 248, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 15.705, mean reward: 2.244 [1.964, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.386, 10.100], loss: 0.158385, mae: 0.401083, mean_q: 4.303837
 17850/100000: episode: 249, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 53.460, mean reward: 2.814 [1.575, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.585, 10.100], loss: 0.183394, mae: 0.419744, mean_q: 4.379150
 17867/100000: episode: 250, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 55.803, mean reward: 3.283 [2.093, 5.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.158, 10.100], loss: 0.213813, mae: 0.424809, mean_q: 4.429989
 17872/100000: episode: 251, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 19.020, mean reward: 3.804 [3.545, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.436, 10.100], loss: 0.152316, mae: 0.413929, mean_q: 4.470773
 17887/100000: episode: 252, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 54.662, mean reward: 3.644 [2.525, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.370, 10.100], loss: 0.169191, mae: 0.401285, mean_q: 4.392570
 17904/100000: episode: 253, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 63.188, mean reward: 3.717 [2.847, 5.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.440, 10.100], loss: 0.139599, mae: 0.380537, mean_q: 4.394515
 17924/100000: episode: 254, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 85.574, mean reward: 4.279 [2.283, 8.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.378, 10.100], loss: 0.154774, mae: 0.377797, mean_q: 4.446996
 17941/100000: episode: 255, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 52.952, mean reward: 3.115 [2.428, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.359, 10.100], loss: 0.139572, mae: 0.374456, mean_q: 4.450111
 17946/100000: episode: 256, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 17.286, mean reward: 3.457 [2.851, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.413, 10.100], loss: 0.288982, mae: 0.450688, mean_q: 4.407064
 17965/100000: episode: 257, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 54.725, mean reward: 2.880 [2.219, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.253, 10.100], loss: 0.202746, mae: 0.429400, mean_q: 4.530064
 17972/100000: episode: 258, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 24.377, mean reward: 3.482 [2.971, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.477, 10.100], loss: 0.116876, mae: 0.342333, mean_q: 4.392039
 17977/100000: episode: 259, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 18.192, mean reward: 3.638 [3.342, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.581, 10.100], loss: 0.294377, mae: 0.419291, mean_q: 4.437937
 17984/100000: episode: 260, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 19.524, mean reward: 2.789 [2.198, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.349, 10.100], loss: 0.175345, mae: 0.420703, mean_q: 4.532472
 17990/100000: episode: 261, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 16.103, mean reward: 2.684 [2.199, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.271, 10.100], loss: 0.141986, mae: 0.379144, mean_q: 4.484075
 17995/100000: episode: 262, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 19.499, mean reward: 3.900 [3.223, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.362, 10.100], loss: 0.132606, mae: 0.376355, mean_q: 4.476975
 18014/100000: episode: 263, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 106.779, mean reward: 5.620 [2.899, 10.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.416, 10.100], loss: 0.226635, mae: 0.407377, mean_q: 4.490391
 18019/100000: episode: 264, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 16.514, mean reward: 3.303 [3.191, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.375, 10.100], loss: 0.227899, mae: 0.500415, mean_q: 4.753112
 18026/100000: episode: 265, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 27.621, mean reward: 3.946 [2.186, 7.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.429, 10.100], loss: 0.481781, mae: 0.524129, mean_q: 4.573145
 18033/100000: episode: 266, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 19.502, mean reward: 2.786 [2.497, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.339, 10.100], loss: 0.139653, mae: 0.358131, mean_q: 4.422400
 18040/100000: episode: 267, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 18.880, mean reward: 2.697 [2.452, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.450, 10.100], loss: 0.172094, mae: 0.433712, mean_q: 4.628428
 18058/100000: episode: 268, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 84.185, mean reward: 4.677 [2.600, 7.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.479, 10.100], loss: 0.207223, mae: 0.436413, mean_q: 4.603477
 18077/100000: episode: 269, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 62.618, mean reward: 3.296 [2.910, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.152, 10.100], loss: 0.210103, mae: 0.427290, mean_q: 4.572244
 18095/100000: episode: 270, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 79.923, mean reward: 4.440 [2.638, 9.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.449, 10.100], loss: 0.237270, mae: 0.439638, mean_q: 4.568219
 18100/100000: episode: 271, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 14.503, mean reward: 2.901 [2.646, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.588, 10.100], loss: 0.322750, mae: 0.476543, mean_q: 4.723429
 18118/100000: episode: 272, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 69.033, mean reward: 3.835 [2.756, 5.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.214, 10.100], loss: 0.204103, mae: 0.437804, mean_q: 4.577971
 18123/100000: episode: 273, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 19.442, mean reward: 3.888 [3.427, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.356, 10.100], loss: 0.181567, mae: 0.400979, mean_q: 4.604179
 18142/100000: episode: 274, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 54.973, mean reward: 2.893 [2.394, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.327, 10.100], loss: 0.232136, mae: 0.448180, mean_q: 4.592455
 18160/100000: episode: 275, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 70.087, mean reward: 3.894 [2.918, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.682, 10.100], loss: 0.266368, mae: 0.449365, mean_q: 4.553350
 18166/100000: episode: 276, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 17.843, mean reward: 2.974 [2.338, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.369, 10.100], loss: 0.463926, mae: 0.517653, mean_q: 4.695065
 18186/100000: episode: 277, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 68.802, mean reward: 3.440 [2.150, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.627, 10.100], loss: 0.263393, mae: 0.511869, mean_q: 4.704841
 18193/100000: episode: 278, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 20.008, mean reward: 2.858 [2.046, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.524, 10.100], loss: 0.185188, mae: 0.434306, mean_q: 4.538898
 18208/100000: episode: 279, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 53.376, mean reward: 3.558 [1.875, 4.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.191, 10.100], loss: 0.271759, mae: 0.457944, mean_q: 4.642719
 18214/100000: episode: 280, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 16.714, mean reward: 2.786 [2.298, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.388, 10.100], loss: 0.172691, mae: 0.432534, mean_q: 4.706826
 18220/100000: episode: 281, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 19.016, mean reward: 3.169 [2.316, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.345, 10.100], loss: 0.267916, mae: 0.490228, mean_q: 4.793546
 18237/100000: episode: 282, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 67.546, mean reward: 3.973 [2.744, 6.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.279, 10.100], loss: 0.262729, mae: 0.465542, mean_q: 4.781767
 18252/100000: episode: 283, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 55.955, mean reward: 3.730 [2.639, 5.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.326, 10.100], loss: 0.189505, mae: 0.427035, mean_q: 4.625676
 18270/100000: episode: 284, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 61.277, mean reward: 3.404 [2.616, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.410, 10.100], loss: 0.234999, mae: 0.445824, mean_q: 4.684439
 18290/100000: episode: 285, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 59.740, mean reward: 2.987 [2.234, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.349, 10.100], loss: 0.195342, mae: 0.421284, mean_q: 4.663467
 18307/100000: episode: 286, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 61.294, mean reward: 3.606 [2.810, 5.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.395, 10.100], loss: 0.202089, mae: 0.446124, mean_q: 4.728972
 18327/100000: episode: 287, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 75.790, mean reward: 3.790 [2.458, 5.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.395, 10.100], loss: 0.213642, mae: 0.456732, mean_q: 4.671671
 18345/100000: episode: 288, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 104.418, mean reward: 5.801 [3.278, 10.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.435, 10.100], loss: 0.199730, mae: 0.438851, mean_q: 4.741642
 18360/100000: episode: 289, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 60.482, mean reward: 4.032 [2.733, 9.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.583, 10.100], loss: 0.217311, mae: 0.436518, mean_q: 4.639620
 18365/100000: episode: 290, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 17.663, mean reward: 3.533 [3.173, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.495, 10.100], loss: 0.249914, mae: 0.434740, mean_q: 4.806317
 18371/100000: episode: 291, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 15.116, mean reward: 2.519 [2.184, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.398, 10.100], loss: 0.239965, mae: 0.495518, mean_q: 4.848506
 18377/100000: episode: 292, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 16.515, mean reward: 2.753 [2.314, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.363, 10.100], loss: 0.423623, mae: 0.508115, mean_q: 4.786911
 18394/100000: episode: 293, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 78.935, mean reward: 4.643 [3.630, 7.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.347, 10.100], loss: 0.285731, mae: 0.490185, mean_q: 4.736909
 18409/100000: episode: 294, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 51.345, mean reward: 3.423 [2.885, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.232, 10.100], loss: 0.344634, mae: 0.504570, mean_q: 4.886212
 18416/100000: episode: 295, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 19.125, mean reward: 2.732 [2.234, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.424, 10.100], loss: 0.264568, mae: 0.440517, mean_q: 4.671034
 18436/100000: episode: 296, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 57.940, mean reward: 2.897 [2.303, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.138, 10.100], loss: 0.320963, mae: 0.511030, mean_q: 4.737245
 18453/100000: episode: 297, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 54.915, mean reward: 3.230 [2.484, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.458, 10.100], loss: 0.331663, mae: 0.537833, mean_q: 4.898615
 18470/100000: episode: 298, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 86.120, mean reward: 5.066 [3.557, 8.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.486, 10.100], loss: 0.202340, mae: 0.444860, mean_q: 4.757304
 18477/100000: episode: 299, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 19.752, mean reward: 2.822 [2.071, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.396, 10.100], loss: 0.369643, mae: 0.511895, mean_q: 4.664170
 18482/100000: episode: 300, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 26.642, mean reward: 5.328 [4.694, 6.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.361, 10.100], loss: 0.256806, mae: 0.509960, mean_q: 4.878952
 18501/100000: episode: 301, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 107.689, mean reward: 5.668 [2.475, 18.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.911, 10.100], loss: 0.319896, mae: 0.489426, mean_q: 4.833904
 18508/100000: episode: 302, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 16.486, mean reward: 2.355 [1.925, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.362, 10.100], loss: 0.190404, mae: 0.423963, mean_q: 4.740008
 18513/100000: episode: 303, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 15.830, mean reward: 3.166 [2.721, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.356, 10.100], loss: 0.325413, mae: 0.558325, mean_q: 4.845416
 18531/100000: episode: 304, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 62.824, mean reward: 3.490 [2.534, 6.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.304, 10.100], loss: 0.284089, mae: 0.500839, mean_q: 4.933753
 18546/100000: episode: 305, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 67.602, mean reward: 4.507 [3.573, 5.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.448, 10.100], loss: 0.219713, mae: 0.469339, mean_q: 4.763406
 18553/100000: episode: 306, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 22.268, mean reward: 3.181 [2.689, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.451, 10.100], loss: 0.271710, mae: 0.445714, mean_q: 4.724748
 18559/100000: episode: 307, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 16.506, mean reward: 2.751 [2.612, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.380, 10.100], loss: 0.206689, mae: 0.466773, mean_q: 4.833807
 18579/100000: episode: 308, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 63.401, mean reward: 3.170 [2.150, 5.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.688, 10.100], loss: 0.285888, mae: 0.509487, mean_q: 4.878966
 18598/100000: episode: 309, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 85.268, mean reward: 4.488 [2.639, 6.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.745, 10.100], loss: 0.238045, mae: 0.485801, mean_q: 4.896187
 18616/100000: episode: 310, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 63.796, mean reward: 3.544 [2.051, 5.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.511, 10.100], loss: 0.280486, mae: 0.499263, mean_q: 4.819060
 18621/100000: episode: 311, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 17.982, mean reward: 3.596 [2.963, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.432, 10.100], loss: 0.199707, mae: 0.461390, mean_q: 4.876276
 18638/100000: episode: 312, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 46.711, mean reward: 2.748 [2.112, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.459, 10.100], loss: 0.305946, mae: 0.520459, mean_q: 4.877275
 18644/100000: episode: 313, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 18.413, mean reward: 3.069 [2.523, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.460, 10.100], loss: 0.264412, mae: 0.512009, mean_q: 4.976612
 18664/100000: episode: 314, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 47.760, mean reward: 2.388 [1.988, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.896, 10.100], loss: 0.292202, mae: 0.507620, mean_q: 4.994858
 18681/100000: episode: 315, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 61.677, mean reward: 3.628 [2.712, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.883, 10.100], loss: 0.347831, mae: 0.509523, mean_q: 4.902852
 18688/100000: episode: 316, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 24.492, mean reward: 3.499 [2.541, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.341, 10.100], loss: 0.232332, mae: 0.487749, mean_q: 4.980472
 18707/100000: episode: 317, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 83.245, mean reward: 4.381 [2.523, 6.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.835, 10.100], loss: 0.257454, mae: 0.488429, mean_q: 4.995632
 18713/100000: episode: 318, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 15.721, mean reward: 2.620 [2.471, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.376, 10.100], loss: 0.257157, mae: 0.494482, mean_q: 4.991074
 18733/100000: episode: 319, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 53.912, mean reward: 2.696 [2.272, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.177, 10.100], loss: 0.303149, mae: 0.488194, mean_q: 4.962749
 18751/100000: episode: 320, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 59.110, mean reward: 3.284 [2.591, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.145, 10.100], loss: 0.229305, mae: 0.456845, mean_q: 5.018522
 18769/100000: episode: 321, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 62.489, mean reward: 3.472 [2.691, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.420, 10.100], loss: 0.293163, mae: 0.491475, mean_q: 4.959497
 18787/100000: episode: 322, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 55.072, mean reward: 3.060 [1.825, 5.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.308, 10.100], loss: 0.260187, mae: 0.492094, mean_q: 4.945928
 18806/100000: episode: 323, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 62.700, mean reward: 3.300 [2.227, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.300, 10.100], loss: 0.354725, mae: 0.543659, mean_q: 5.055320
 18826/100000: episode: 324, duration: 0.137s, episode steps: 20, steps per second: 146, episode reward: 63.760, mean reward: 3.188 [2.380, 5.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.189, 10.100], loss: 0.307429, mae: 0.528957, mean_q: 5.020755
 18833/100000: episode: 325, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 23.092, mean reward: 3.299 [2.724, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.330, 10.100], loss: 0.387913, mae: 0.542459, mean_q: 5.056164
 18840/100000: episode: 326, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 16.584, mean reward: 2.369 [2.067, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.333, 10.100], loss: 0.236866, mae: 0.461928, mean_q: 4.821252
 18847/100000: episode: 327, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 19.063, mean reward: 2.723 [2.298, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.387, 10.100], loss: 0.196976, mae: 0.479772, mean_q: 4.939106
 18864/100000: episode: 328, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 54.362, mean reward: 3.198 [2.283, 5.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.358, 10.100], loss: 0.247766, mae: 0.508514, mean_q: 5.090244
 18882/100000: episode: 329, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 66.223, mean reward: 3.679 [2.850, 6.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.485, 10.100], loss: 0.273142, mae: 0.484601, mean_q: 5.055857
[Info] 3-TH LEVEL FOUND: 8.611381530761719, Considering 10/90 traces
 18887/100000: episode: 330, duration: 4.006s, episode steps: 5, steps per second: 1, episode reward: 19.332, mean reward: 3.866 [3.481, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.437, 10.100], loss: 0.414043, mae: 0.452049, mean_q: 4.957860
[Info] FALSIFICATION!
 18892/100000: episode: 331, duration: 0.388s, episode steps: 5, steps per second: 13, episode reward: 1175.884, mean reward: 235.177 [6.200, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.412, 10.082], loss: 0.565084, mae: 0.644843, mean_q: 5.348050
 18903/100000: episode: 332, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 67.357, mean reward: 6.123 [4.295, 8.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.464, 10.100], loss: 0.283365, mae: 0.502472, mean_q: 5.111143
 18914/100000: episode: 333, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 73.076, mean reward: 6.643 [4.146, 15.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.443, 10.100], loss: 0.245459, mae: 0.506280, mean_q: 5.215873
 18925/100000: episode: 334, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 46.736, mean reward: 4.249 [3.199, 6.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.517, 10.100], loss: 0.255920, mae: 0.477419, mean_q: 5.077365
 18929/100000: episode: 335, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 28.783, mean reward: 7.196 [5.046, 11.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.501, 10.100], loss: 0.233160, mae: 0.506325, mean_q: 4.863611
 18940/100000: episode: 336, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 37.206, mean reward: 3.382 [2.742, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.497, 10.100], loss: 0.355970, mae: 0.542522, mean_q: 5.071121
 18947/100000: episode: 337, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 48.334, mean reward: 6.905 [4.435, 14.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.674, 10.100], loss: 0.340990, mae: 0.540840, mean_q: 5.223342
 18958/100000: episode: 338, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 61.635, mean reward: 5.603 [4.080, 7.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.399, 10.100], loss: 0.262870, mae: 0.437510, mean_q: 5.017682
 18965/100000: episode: 339, duration: 0.054s, episode steps: 7, steps per second: 129, episode reward: 30.346, mean reward: 4.335 [3.949, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.565, 10.100], loss: 0.270655, mae: 0.494619, mean_q: 5.226303
 18969/100000: episode: 340, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 25.674, mean reward: 6.419 [5.043, 8.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.515, 10.100], loss: 0.333401, mae: 0.562606, mean_q: 5.173301
 18976/100000: episode: 341, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 59.219, mean reward: 8.460 [4.678, 13.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.409, 10.100], loss: 0.321598, mae: 0.560279, mean_q: 5.202867
 18984/100000: episode: 342, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 76.870, mean reward: 9.609 [6.119, 17.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.523, 10.100], loss: 0.790602, mae: 0.677738, mean_q: 5.164703
 18991/100000: episode: 343, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 23.356, mean reward: 3.337 [2.855, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.491, 10.100], loss: 0.554098, mae: 0.656334, mean_q: 5.320931
 18995/100000: episode: 344, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 28.086, mean reward: 7.021 [4.872, 9.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.562, 10.100], loss: 0.290579, mae: 0.560797, mean_q: 4.854129
 19006/100000: episode: 345, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 59.730, mean reward: 5.430 [3.632, 10.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.546, 10.100], loss: 0.405473, mae: 0.545801, mean_q: 5.017817
 19017/100000: episode: 346, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 63.322, mean reward: 5.757 [4.666, 8.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.367, 10.100], loss: 0.315108, mae: 0.552217, mean_q: 5.161423
 19021/100000: episode: 347, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 20.385, mean reward: 5.096 [3.823, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.440, 10.100], loss: 0.163620, mae: 0.452677, mean_q: 5.206860
 19028/100000: episode: 348, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 58.035, mean reward: 8.291 [7.033, 11.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.559, 10.100], loss: 2194.833740, mae: 6.042167, mean_q: 6.499310
 19039/100000: episode: 349, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 66.052, mean reward: 6.005 [3.769, 13.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.042, 10.100], loss: 3.174057, mae: 1.805234, mean_q: 6.014646
 19043/100000: episode: 350, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 18.502, mean reward: 4.626 [4.216, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.541, 10.100], loss: 0.843854, mae: 0.948392, mean_q: 4.635244
 19054/100000: episode: 351, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 150.815, mean reward: 13.710 [5.686, 49.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.442, 10.100], loss: 1398.337036, mae: 4.180152, mean_q: 6.458832
 19065/100000: episode: 352, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 61.853, mean reward: 5.623 [4.357, 7.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.363, 10.100], loss: 6.240889, mae: 2.751224, mean_q: 7.773581
 19076/100000: episode: 353, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 55.101, mean reward: 5.009 [3.327, 9.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.134, 10.100], loss: 1.511304, mae: 1.336847, mean_q: 4.947168
 19087/100000: episode: 354, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 67.698, mean reward: 6.154 [3.349, 10.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.248, 10.100], loss: 1.005952, mae: 0.887706, mean_q: 5.879425
 19097/100000: episode: 355, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 39.698, mean reward: 3.970 [3.624, 4.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.423, 10.100], loss: 3.169389, mae: 0.800903, mean_q: 5.960416
[Info] FALSIFICATION!
 19100/100000: episode: 356, duration: 0.284s, episode steps: 3, steps per second: 11, episode reward: 1271.178, mean reward: 423.726 [34.215, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.651, 10.067], loss: 0.737341, mae: 0.746415, mean_q: 6.060658
 19104/100000: episode: 357, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 22.951, mean reward: 5.738 [3.777, 7.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.548, 10.100], loss: 0.872130, mae: 0.801133, mean_q: 5.897340
 19111/100000: episode: 358, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 32.361, mean reward: 4.623 [3.270, 7.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.392, 10.100], loss: 0.427584, mae: 0.597352, mean_q: 5.794807
 19122/100000: episode: 359, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 51.787, mean reward: 4.708 [3.405, 5.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.427, 10.100], loss: 0.609477, mae: 0.652956, mean_q: 5.732101
 19132/100000: episode: 360, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 34.468, mean reward: 3.447 [3.054, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.470, 10.100], loss: 0.570987, mae: 0.629653, mean_q: 5.661979
 19140/100000: episode: 361, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 62.081, mean reward: 7.760 [5.415, 10.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.340, 10.100], loss: 2.266920, mae: 0.721372, mean_q: 5.535878
 19151/100000: episode: 362, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 71.728, mean reward: 6.521 [3.707, 13.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.499, 10.100], loss: 1.316753, mae: 0.825508, mean_q: 5.950176
 19159/100000: episode: 363, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 56.026, mean reward: 7.003 [4.470, 12.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.416, 10.100], loss: 1.481894, mae: 0.868869, mean_q: 5.814721
 19169/100000: episode: 364, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 47.841, mean reward: 4.784 [3.742, 7.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.473, 10.100], loss: 1.068988, mae: 0.780744, mean_q: 5.913125
 19176/100000: episode: 365, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 27.674, mean reward: 3.953 [3.512, 4.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.529, 10.100], loss: 121.623840, mae: 1.867112, mean_q: 6.032659
 19180/100000: episode: 366, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 21.846, mean reward: 5.462 [4.580, 7.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.122, 10.100], loss: 0.828509, mae: 0.949635, mean_q: 6.503545
 19191/100000: episode: 367, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 42.471, mean reward: 3.861 [2.953, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.350, 10.100], loss: 1396.447144, mae: 4.059290, mean_q: 6.547780
 19198/100000: episode: 368, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 23.611, mean reward: 3.373 [3.004, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.476, 10.100], loss: 6.816651, mae: 2.554667, mean_q: 8.127917
 19202/100000: episode: 369, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 21.188, mean reward: 5.297 [4.645, 6.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.498, 10.100], loss: 3.618549, mae: 1.871889, mean_q: 7.312550
 19209/100000: episode: 370, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 28.552, mean reward: 4.079 [3.669, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.362, 10.100], loss: 1.410737, mae: 1.099771, mean_q: 6.156292
 19220/100000: episode: 371, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 50.555, mean reward: 4.596 [3.967, 5.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.682, 10.100], loss: 1.038615, mae: 0.912165, mean_q: 5.481041
 19224/100000: episode: 372, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 29.234, mean reward: 7.308 [5.872, 8.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.496, 10.100], loss: 1.469152, mae: 0.896564, mean_q: 5.383991
 19235/100000: episode: 373, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 50.457, mean reward: 4.587 [3.384, 6.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.468, 10.100], loss: 1397.287109, mae: 4.289883, mean_q: 6.890502
 19245/100000: episode: 374, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 42.811, mean reward: 4.281 [3.273, 6.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.468, 10.100], loss: 4.733800, mae: 2.137787, mean_q: 7.642975
 19252/100000: episode: 375, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 65.873, mean reward: 9.410 [5.748, 16.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.431, 10.100], loss: 2.013287, mae: 1.276154, mean_q: 6.413083
[Info] FALSIFICATION!
 19259/100000: episode: 376, duration: 0.268s, episode steps: 7, steps per second: 26, episode reward: 1041.576, mean reward: 148.797 [5.114, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.723, 10.067], loss: 1.558313, mae: 0.956078, mean_q: 5.774243
 19269/100000: episode: 377, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 72.617, mean reward: 7.262 [3.950, 15.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.492, 10.100], loss: 33.985294, mae: 1.449340, mean_q: 5.745427
 19280/100000: episode: 378, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 103.015, mean reward: 9.365 [3.510, 27.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.659, 10.100], loss: 0.994302, mae: 0.865872, mean_q: 6.245338
 19290/100000: episode: 379, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 41.348, mean reward: 4.135 [3.189, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.368, 10.100], loss: 1535.322754, mae: 5.662948, mean_q: 8.569763
 19301/100000: episode: 380, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 42.148, mean reward: 3.832 [2.891, 6.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.533, 10.100], loss: 1604.520630, mae: 8.695220, mean_q: 10.160084
 19308/100000: episode: 381, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 55.724, mean reward: 7.961 [3.216, 12.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.061, 10.100], loss: 15.369257, mae: 4.386808, mean_q: 10.671922
 19315/100000: episode: 382, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 25.672, mean reward: 3.667 [2.922, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.502, 10.100], loss: 3.500012, mae: 1.743778, mean_q: 7.505460
 19323/100000: episode: 383, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 43.419, mean reward: 5.427 [4.545, 6.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.461, 10.100], loss: 1.527057, mae: 1.105214, mean_q: 6.681403
 19334/100000: episode: 384, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 95.088, mean reward: 8.644 [4.505, 11.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.765, 10.100], loss: 2.069557, mae: 1.120846, mean_q: 6.673475
 19345/100000: episode: 385, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 47.782, mean reward: 4.344 [2.825, 8.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.635, 10.100], loss: 1.361642, mae: 1.055696, mean_q: 6.117136
 19356/100000: episode: 386, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 73.598, mean reward: 6.691 [3.511, 19.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.517, 10.100], loss: 80.196175, mae: 1.738654, mean_q: 6.321133
 19363/100000: episode: 387, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 30.962, mean reward: 4.423 [3.272, 5.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.549, 10.100], loss: 1.458428, mae: 1.076970, mean_q: 7.032651
 19367/100000: episode: 388, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 24.302, mean reward: 6.075 [4.311, 7.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.524, 10.100], loss: 1.595436, mae: 1.118830, mean_q: 7.202702
 19377/100000: episode: 389, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 52.998, mean reward: 5.300 [4.049, 9.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.655, 10.100], loss: 1.470457, mae: 0.887231, mean_q: 6.522958
 19381/100000: episode: 390, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 26.976, mean reward: 6.744 [5.548, 7.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.491, 10.100], loss: 1.584111, mae: 1.020271, mean_q: 6.128465
 19392/100000: episode: 391, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 64.423, mean reward: 5.857 [4.448, 9.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.595, 10.100], loss: 1466.910889, mae: 4.759146, mean_q: 6.999696
 19396/100000: episode: 392, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 20.897, mean reward: 5.224 [4.045, 6.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.463, 10.100], loss: 210.264069, mae: 4.821061, mean_q: 9.425113
 19403/100000: episode: 393, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 23.433, mean reward: 3.348 [2.896, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.474, 10.100], loss: 8.642123, mae: 3.035084, mean_q: 9.222565
 19410/100000: episode: 394, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 77.510, mean reward: 11.073 [4.722, 22.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.437, 10.100], loss: 2177.048096, mae: 6.113811, mean_q: 8.112062
 19417/100000: episode: 395, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 25.813, mean reward: 3.688 [2.749, 5.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.494, 10.100], loss: 48.128136, mae: 2.557994, mean_q: 8.214346
 19428/100000: episode: 396, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 40.004, mean reward: 3.637 [2.914, 6.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.458, 10.100], loss: 30.739202, mae: 1.817043, mean_q: 7.614236
 19436/100000: episode: 397, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 44.252, mean reward: 5.532 [4.785, 6.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.397, 10.100], loss: 1908.146240, mae: 5.567948, mean_q: 8.106323
 19443/100000: episode: 398, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 151.184, mean reward: 21.598 [11.099, 44.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.513, 10.100], loss: 5.510704, mae: 2.187668, mean_q: 8.750533
 19450/100000: episode: 399, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 27.084, mean reward: 3.869 [3.371, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.330, 10.100], loss: 3.568915, mae: 1.607819, mean_q: 7.719349
 19457/100000: episode: 400, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 35.209, mean reward: 5.030 [3.511, 8.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.572, 10.100], loss: 1.823126, mae: 1.206670, mean_q: 6.728620
 19468/100000: episode: 401, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 49.744, mean reward: 4.522 [3.781, 5.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.570, 10.100], loss: 5.300060, mae: 1.455480, mean_q: 6.420969
 19475/100000: episode: 402, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 32.815, mean reward: 4.688 [4.138, 5.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.542, 10.100], loss: 2177.861084, mae: 5.487291, mean_q: 6.793079
 19486/100000: episode: 403, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 141.553, mean reward: 12.868 [5.008, 50.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.492, 10.100], loss: 9.999350, mae: 3.033511, mean_q: 9.316568
 19490/100000: episode: 404, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 16.374, mean reward: 4.094 [3.697, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.489, 10.100], loss: 6.894474, mae: 2.669124, mean_q: 9.129749
 19501/100000: episode: 405, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 84.541, mean reward: 7.686 [4.394, 11.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.454, 10.100], loss: 4.208286, mae: 1.750478, mean_q: 8.176001
 19512/100000: episode: 406, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 70.904, mean reward: 6.446 [4.035, 8.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.415, 10.100], loss: 1.254165, mae: 1.001006, mean_q: 6.480416
 19523/100000: episode: 407, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 42.721, mean reward: 3.884 [2.859, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.502, 10.100], loss: 2781.469971, mae: 6.843895, mean_q: 6.718582
 19531/100000: episode: 408, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 154.326, mean reward: 19.291 [5.606, 55.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.711, 10.100], loss: 12.751191, mae: 3.639880, mean_q: 9.978126
 19542/100000: episode: 409, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 65.805, mean reward: 5.982 [4.465, 11.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.502, 10.100], loss: 14.400928, mae: 4.148363, mean_q: 10.850936
 19553/100000: episode: 410, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 86.157, mean reward: 7.832 [3.112, 19.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.584, 10.100], loss: 7.472606, mae: 2.183594, mean_q: 8.889638
 19561/100000: episode: 411, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 44.186, mean reward: 5.523 [4.198, 8.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.001, 10.100], loss: 1.947640, mae: 1.150890, mean_q: 7.492402
 19572/100000: episode: 412, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 170.462, mean reward: 15.497 [4.637, 58.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.443, 10.100], loss: 4.555797, mae: 1.070398, mean_q: 6.734636
 19576/100000: episode: 413, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 32.637, mean reward: 8.159 [5.048, 9.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.518, 10.100], loss: 9.885302, mae: 1.299312, mean_q: 6.561789
 19587/100000: episode: 414, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 75.747, mean reward: 6.886 [3.316, 11.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.604, 10.100], loss: 5.350001, mae: 1.151381, mean_q: 6.784986
 19591/100000: episode: 415, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 26.602, mean reward: 6.650 [6.196, 7.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.489, 10.100], loss: 12.230217, mae: 1.234632, mean_q: 7.234747
 19595/100000: episode: 416, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 25.982, mean reward: 6.496 [4.958, 8.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.500, 10.100], loss: 0.987371, mae: 0.863376, mean_q: 7.205691
 19606/100000: episode: 417, duration: 0.077s, episode steps: 11, steps per second: 143, episode reward: 69.205, mean reward: 6.291 [3.488, 10.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.464, 10.100], loss: 5.497325, mae: 1.119682, mean_q: 7.378066
 19613/100000: episode: 418, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 101.952, mean reward: 14.565 [6.825, 28.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.507, 10.100], loss: 5.997790, mae: 1.253582, mean_q: 7.297523
 19620/100000: episode: 419, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 80.359, mean reward: 11.480 [9.170, 14.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.548, 10.100], loss: 1.141525, mae: 0.917418, mean_q: 6.920806
[Info] Complete ISplit Iteration
[Info] Levels: [4.5420594, 5.9658384, 8.611382, 10.820585]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.91]
[Info] Error Prob: 0.0009100000000000002

 19627/100000: episode: 420, duration: 4.323s, episode steps: 7, steps per second: 2, episode reward: 31.573, mean reward: 4.510 [3.680, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.520, 10.100], loss: 1.281512, mae: 0.941407, mean_q: 7.063392
 19727/100000: episode: 421, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 205.571, mean reward: 2.056 [1.481, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.395, 10.191], loss: 156.596207, mae: 1.526542, mean_q: 7.212536
 19827/100000: episode: 422, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 197.482, mean reward: 1.975 [1.530, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.777, 10.098], loss: 312.361206, mae: 2.001988, mean_q: 7.368964
 19927/100000: episode: 423, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 216.395, mean reward: 2.164 [1.471, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.848, 10.098], loss: 465.321808, mae: 2.449292, mean_q: 7.603335
 20027/100000: episode: 424, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.277, mean reward: 1.893 [1.482, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.303, 10.264], loss: 156.720352, mae: 1.773215, mean_q: 7.473980
 20127/100000: episode: 425, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 195.841, mean reward: 1.958 [1.492, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.103, 10.098], loss: 169.221130, mae: 1.838439, mean_q: 7.373320
 20227/100000: episode: 426, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.590, mean reward: 1.946 [1.453, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.180, 10.098], loss: 466.782623, mae: 2.671140, mean_q: 7.711544
 20327/100000: episode: 427, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.324, mean reward: 1.953 [1.471, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.000, 10.203], loss: 613.428345, mae: 2.917146, mean_q: 7.714654
 20427/100000: episode: 428, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 175.489, mean reward: 1.755 [1.451, 2.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.197], loss: 461.731323, mae: 2.583279, mean_q: 8.006526
 20527/100000: episode: 429, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 183.543, mean reward: 1.835 [1.452, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.098], loss: 155.411743, mae: 1.638626, mean_q: 7.394937
 20627/100000: episode: 430, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.598, mean reward: 1.826 [1.457, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.323, 10.221], loss: 167.734009, mae: 1.746029, mean_q: 7.256391
 20727/100000: episode: 431, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.220, mean reward: 1.972 [1.509, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.917, 10.228], loss: 615.599792, mae: 2.952767, mean_q: 7.780127
 20827/100000: episode: 432, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 208.131, mean reward: 2.081 [1.438, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.546, 10.391], loss: 619.206787, mae: 3.228883, mean_q: 8.101242
 20927/100000: episode: 433, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 206.493, mean reward: 2.065 [1.489, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.635, 10.271], loss: 165.698120, mae: 1.705328, mean_q: 7.297122
 21027/100000: episode: 434, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.489, mean reward: 1.845 [1.486, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.182, 10.293], loss: 308.695984, mae: 2.384254, mean_q: 7.796980
 21127/100000: episode: 435, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.164, mean reward: 1.822 [1.455, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.372, 10.098], loss: 163.255112, mae: 1.574651, mean_q: 7.041569
 21227/100000: episode: 436, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.474, mean reward: 1.915 [1.486, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.217], loss: 612.162903, mae: 3.021649, mean_q: 7.686680
 21327/100000: episode: 437, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 193.759, mean reward: 1.938 [1.445, 5.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.109, 10.098], loss: 310.260040, mae: 2.137076, mean_q: 7.549256
 21427/100000: episode: 438, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.836, mean reward: 1.958 [1.455, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.981, 10.108], loss: 15.160305, mae: 1.241099, mean_q: 6.645532
 21527/100000: episode: 439, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.439, mean reward: 1.984 [1.456, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.782, 10.200], loss: 5.143187, mae: 0.940658, mean_q: 6.383059
 21627/100000: episode: 440, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 174.698, mean reward: 1.747 [1.442, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.392, 10.098], loss: 327.336487, mae: 2.261552, mean_q: 7.041815
 21727/100000: episode: 441, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.820, mean reward: 1.908 [1.482, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.252], loss: 314.399048, mae: 2.201308, mean_q: 7.099673
 21827/100000: episode: 442, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 205.206, mean reward: 2.052 [1.498, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.089, 10.502], loss: 156.405563, mae: 1.495282, mean_q: 6.759241
 21927/100000: episode: 443, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 201.951, mean reward: 2.020 [1.489, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.389, 10.098], loss: 460.098511, mae: 2.502017, mean_q: 7.288126
 22027/100000: episode: 444, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 196.783, mean reward: 1.968 [1.455, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.458, 10.146], loss: 769.924011, mae: 3.494529, mean_q: 7.713667
 22127/100000: episode: 445, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 182.807, mean reward: 1.828 [1.476, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: 155.871216, mae: 1.764319, mean_q: 7.116935
 22227/100000: episode: 446, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 206.585, mean reward: 2.066 [1.445, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.623, 10.098], loss: 319.003662, mae: 2.155642, mean_q: 7.141011
 22327/100000: episode: 447, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 190.570, mean reward: 1.906 [1.496, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.515, 10.186], loss: 155.998550, mae: 1.518008, mean_q: 6.704905
 22427/100000: episode: 448, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.504, mean reward: 1.835 [1.504, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.095, 10.125], loss: 155.449860, mae: 1.494196, mean_q: 6.632575
 22527/100000: episode: 449, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 175.516, mean reward: 1.755 [1.468, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.788, 10.112], loss: 183.408905, mae: 1.711447, mean_q: 6.563056
 22627/100000: episode: 450, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.740, mean reward: 1.857 [1.440, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.002, 10.297], loss: 766.461243, mae: 3.688160, mean_q: 7.571246
 22727/100000: episode: 451, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.142, mean reward: 1.861 [1.483, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.911, 10.109], loss: 468.782776, mae: 2.983332, mean_q: 7.651945
 22827/100000: episode: 452, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 198.383, mean reward: 1.984 [1.451, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.479, 10.389], loss: 164.678589, mae: 1.580322, mean_q: 6.655912
 22927/100000: episode: 453, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.130, mean reward: 1.901 [1.485, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.610, 10.098], loss: 155.822128, mae: 1.471253, mean_q: 6.404196
 23027/100000: episode: 454, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.234, mean reward: 1.872 [1.483, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.508, 10.134], loss: 17.548056, mae: 1.082700, mean_q: 5.828368
 23127/100000: episode: 455, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.755, mean reward: 1.858 [1.446, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.140, 10.098], loss: 164.253296, mae: 1.564855, mean_q: 6.110334
 23227/100000: episode: 456, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.582, mean reward: 1.836 [1.449, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.111], loss: 164.590607, mae: 1.556561, mean_q: 6.015779
 23327/100000: episode: 457, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 194.020, mean reward: 1.940 [1.494, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.345, 10.273], loss: 317.435303, mae: 1.889454, mean_q: 6.212522
 23427/100000: episode: 458, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 200.500, mean reward: 2.005 [1.551, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.640, 10.098], loss: 609.262634, mae: 2.739870, mean_q: 6.656727
 23527/100000: episode: 459, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 207.602, mean reward: 2.076 [1.470, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.604, 10.098], loss: 166.060455, mae: 1.559186, mean_q: 6.083383
 23627/100000: episode: 460, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.180, mean reward: 1.852 [1.473, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.520, 10.247], loss: 306.718170, mae: 1.881544, mean_q: 6.370801
 23727/100000: episode: 461, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.640, mean reward: 1.866 [1.488, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.323, 10.098], loss: 154.820007, mae: 1.301650, mean_q: 5.853860
 23827/100000: episode: 462, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.256, mean reward: 1.863 [1.453, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.056, 10.142], loss: 10.309076, mae: 0.753990, mean_q: 5.270793
 23927/100000: episode: 463, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.844, mean reward: 1.848 [1.452, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.516, 10.098], loss: 18.114788, mae: 0.807342, mean_q: 5.127666
 24027/100000: episode: 464, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 184.841, mean reward: 1.848 [1.468, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.671, 10.098], loss: 154.368744, mae: 1.190973, mean_q: 5.151799
 24127/100000: episode: 465, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 231.675, mean reward: 2.317 [1.502, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.945, 10.098], loss: 2.017091, mae: 0.587200, mean_q: 4.673627
 24227/100000: episode: 466, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 195.778, mean reward: 1.958 [1.465, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.287, 10.118], loss: 1.328198, mae: 0.530500, mean_q: 4.492089
 24327/100000: episode: 467, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 229.551, mean reward: 2.296 [1.471, 9.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.645, 10.523], loss: 0.742884, mae: 0.453765, mean_q: 4.302906
 24427/100000: episode: 468, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 210.863, mean reward: 2.109 [1.453, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.027, 10.375], loss: 0.485694, mae: 0.408098, mean_q: 4.190047
 24527/100000: episode: 469, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 217.720, mean reward: 2.177 [1.437, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.602, 10.098], loss: 0.301652, mae: 0.359929, mean_q: 3.989318
 24627/100000: episode: 470, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 216.176, mean reward: 2.162 [1.522, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.563, 10.118], loss: 0.124655, mae: 0.323346, mean_q: 3.869364
 24727/100000: episode: 471, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 201.847, mean reward: 2.018 [1.467, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.261, 10.148], loss: 0.105082, mae: 0.322162, mean_q: 3.868510
 24827/100000: episode: 472, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 185.229, mean reward: 1.852 [1.457, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.845, 10.098], loss: 0.108092, mae: 0.316470, mean_q: 3.856867
 24927/100000: episode: 473, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.014, mean reward: 1.950 [1.464, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.917, 10.098], loss: 0.095589, mae: 0.305133, mean_q: 3.847295
 25027/100000: episode: 474, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.556, mean reward: 1.966 [1.471, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.281, 10.256], loss: 0.097485, mae: 0.311020, mean_q: 3.846747
 25127/100000: episode: 475, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 201.908, mean reward: 2.019 [1.597, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.464, 10.365], loss: 0.112081, mae: 0.313601, mean_q: 3.855820
 25227/100000: episode: 476, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 195.723, mean reward: 1.957 [1.475, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.837, 10.098], loss: 0.100520, mae: 0.308505, mean_q: 3.855658
 25327/100000: episode: 477, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 233.745, mean reward: 2.337 [1.467, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.782, 10.391], loss: 0.083076, mae: 0.292441, mean_q: 3.835834
 25427/100000: episode: 478, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 203.197, mean reward: 2.032 [1.464, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.272], loss: 0.090837, mae: 0.304666, mean_q: 3.856950
 25527/100000: episode: 479, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 190.284, mean reward: 1.903 [1.461, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.383, 10.216], loss: 0.101642, mae: 0.312341, mean_q: 3.873490
 25627/100000: episode: 480, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 193.494, mean reward: 1.935 [1.472, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.900, 10.098], loss: 0.103922, mae: 0.299031, mean_q: 3.864127
 25727/100000: episode: 481, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 187.280, mean reward: 1.873 [1.460, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.213], loss: 0.102753, mae: 0.313425, mean_q: 3.882417
 25827/100000: episode: 482, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.829, mean reward: 1.918 [1.461, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.729, 10.143], loss: 0.106014, mae: 0.304813, mean_q: 3.881449
 25927/100000: episode: 483, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.273, mean reward: 1.823 [1.448, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.543, 10.098], loss: 0.113071, mae: 0.303641, mean_q: 3.872591
 26027/100000: episode: 484, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 201.290, mean reward: 2.013 [1.437, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.491, 10.098], loss: 0.086597, mae: 0.295312, mean_q: 3.866685
 26127/100000: episode: 485, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.336, mean reward: 1.933 [1.451, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.041, 10.098], loss: 0.083503, mae: 0.292984, mean_q: 3.877044
 26227/100000: episode: 486, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.039, mean reward: 1.960 [1.468, 5.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.203, 10.344], loss: 0.092063, mae: 0.293657, mean_q: 3.867727
 26327/100000: episode: 487, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 187.528, mean reward: 1.875 [1.460, 5.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.715, 10.188], loss: 0.094389, mae: 0.294778, mean_q: 3.857726
 26427/100000: episode: 488, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 222.839, mean reward: 2.228 [1.514, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.973, 10.448], loss: 0.097973, mae: 0.304736, mean_q: 3.885052
 26527/100000: episode: 489, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 211.119, mean reward: 2.111 [1.432, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.306, 10.261], loss: 0.091747, mae: 0.297507, mean_q: 3.898367
 26627/100000: episode: 490, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.535, mean reward: 1.855 [1.455, 2.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.497, 10.098], loss: 0.089335, mae: 0.303525, mean_q: 3.913172
 26727/100000: episode: 491, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 192.327, mean reward: 1.923 [1.485, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.949, 10.098], loss: 0.094500, mae: 0.301051, mean_q: 3.897024
 26827/100000: episode: 492, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.075, mean reward: 1.901 [1.464, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.560, 10.098], loss: 0.086639, mae: 0.286057, mean_q: 3.883435
 26927/100000: episode: 493, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 207.442, mean reward: 2.074 [1.454, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.350, 10.257], loss: 0.087951, mae: 0.300569, mean_q: 3.904585
 27027/100000: episode: 494, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 200.783, mean reward: 2.008 [1.486, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.774, 10.098], loss: 0.096512, mae: 0.297694, mean_q: 3.896240
 27127/100000: episode: 495, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 177.765, mean reward: 1.778 [1.443, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.570, 10.098], loss: 0.092809, mae: 0.290974, mean_q: 3.899748
 27227/100000: episode: 496, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.133, mean reward: 1.871 [1.452, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.958, 10.123], loss: 0.093973, mae: 0.299105, mean_q: 3.889385
 27327/100000: episode: 497, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 234.635, mean reward: 2.346 [1.486, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.928, 10.127], loss: 0.086360, mae: 0.299375, mean_q: 3.902084
 27427/100000: episode: 498, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.559, mean reward: 1.956 [1.472, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.522, 10.335], loss: 0.092584, mae: 0.301090, mean_q: 3.898175
 27527/100000: episode: 499, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 195.795, mean reward: 1.958 [1.439, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.147, 10.289], loss: 0.082855, mae: 0.293969, mean_q: 3.904646
 27627/100000: episode: 500, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 211.086, mean reward: 2.111 [1.465, 6.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.618, 10.112], loss: 0.094942, mae: 0.305913, mean_q: 3.918570
 27727/100000: episode: 501, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.450, mean reward: 1.954 [1.458, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.747, 10.227], loss: 0.096251, mae: 0.303502, mean_q: 3.919470
 27827/100000: episode: 502, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 203.010, mean reward: 2.030 [1.496, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.582, 10.098], loss: 0.094852, mae: 0.309805, mean_q: 3.922542
 27927/100000: episode: 503, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 204.272, mean reward: 2.043 [1.457, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.393, 10.243], loss: 0.106130, mae: 0.309534, mean_q: 3.929422
 28027/100000: episode: 504, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.147, mean reward: 1.911 [1.478, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.098], loss: 0.103544, mae: 0.313930, mean_q: 3.940432
 28127/100000: episode: 505, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 199.034, mean reward: 1.990 [1.485, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.071, 10.098], loss: 0.094886, mae: 0.301874, mean_q: 3.938028
 28227/100000: episode: 506, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 217.376, mean reward: 2.174 [1.485, 5.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.098], loss: 0.095258, mae: 0.298280, mean_q: 3.927873
 28327/100000: episode: 507, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.540, mean reward: 1.935 [1.457, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.098], loss: 0.097981, mae: 0.311797, mean_q: 3.950414
 28427/100000: episode: 508, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.934, mean reward: 1.929 [1.550, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.804, 10.104], loss: 0.096925, mae: 0.308259, mean_q: 3.944254
 28527/100000: episode: 509, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 199.475, mean reward: 1.995 [1.469, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.862, 10.253], loss: 0.098276, mae: 0.309120, mean_q: 3.943843
 28627/100000: episode: 510, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 198.528, mean reward: 1.985 [1.442, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.145, 10.559], loss: 0.087011, mae: 0.296804, mean_q: 3.932050
 28727/100000: episode: 511, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 207.965, mean reward: 2.080 [1.543, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.905, 10.098], loss: 0.122254, mae: 0.323416, mean_q: 3.968406
 28827/100000: episode: 512, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.217, mean reward: 1.832 [1.452, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.519, 10.158], loss: 0.102170, mae: 0.315147, mean_q: 3.954833
 28927/100000: episode: 513, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.728, mean reward: 1.917 [1.438, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.482, 10.098], loss: 0.105370, mae: 0.322108, mean_q: 3.973091
 29027/100000: episode: 514, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.624, mean reward: 1.836 [1.474, 2.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.070, 10.142], loss: 0.105790, mae: 0.313905, mean_q: 3.968812
 29127/100000: episode: 515, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.915, mean reward: 2.019 [1.494, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.673, 10.098], loss: 0.090055, mae: 0.298253, mean_q: 3.956150
 29227/100000: episode: 516, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 230.862, mean reward: 2.309 [1.511, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.098], loss: 0.096906, mae: 0.301433, mean_q: 3.945969
 29327/100000: episode: 517, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.706, mean reward: 1.907 [1.431, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.098], loss: 0.101597, mae: 0.319643, mean_q: 3.964854
 29427/100000: episode: 518, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.547, mean reward: 1.845 [1.483, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.688, 10.098], loss: 0.092442, mae: 0.300363, mean_q: 3.953392
 29527/100000: episode: 519, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 183.158, mean reward: 1.832 [1.447, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.542, 10.150], loss: 0.093098, mae: 0.306992, mean_q: 3.942902
[Info] 1-TH LEVEL FOUND: 5.122609615325928, Considering 10/90 traces
 29627/100000: episode: 520, duration: 4.628s, episode steps: 100, steps per second: 22, episode reward: 201.023, mean reward: 2.010 [1.485, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.992, 10.098], loss: 0.091836, mae: 0.305318, mean_q: 3.922035
 29668/100000: episode: 521, duration: 0.240s, episode steps: 41, steps per second: 171, episode reward: 141.636, mean reward: 3.455 [1.643, 6.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.507, 10.100], loss: 0.118513, mae: 0.319784, mean_q: 3.954476
 29692/100000: episode: 522, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 57.705, mean reward: 2.404 [1.721, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.313], loss: 0.107720, mae: 0.331856, mean_q: 3.983171
 29697/100000: episode: 523, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 16.665, mean reward: 3.333 [3.053, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.485, 10.480], loss: 0.085948, mae: 0.289441, mean_q: 3.925570
 29750/100000: episode: 524, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 103.371, mean reward: 1.950 [1.470, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.041, 10.139], loss: 0.097302, mae: 0.306660, mean_q: 3.932520
 29763/100000: episode: 525, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 44.158, mean reward: 3.397 [2.773, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.416, 10.100], loss: 0.117433, mae: 0.324074, mean_q: 3.953151
 29816/100000: episode: 526, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 140.310, mean reward: 2.647 [1.482, 5.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.336, 10.118], loss: 0.101133, mae: 0.308653, mean_q: 3.936658
 29853/100000: episode: 527, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 85.376, mean reward: 2.307 [1.717, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.421, 10.100], loss: 0.096996, mae: 0.303447, mean_q: 3.943996
 29858/100000: episode: 528, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 18.490, mean reward: 3.698 [2.887, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.534], loss: 0.083353, mae: 0.306419, mean_q: 3.910834
 29880/100000: episode: 529, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 57.316, mean reward: 2.605 [1.858, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.299], loss: 0.135164, mae: 0.343155, mean_q: 3.988952
 29893/100000: episode: 530, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 40.914, mean reward: 3.147 [2.396, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.306, 10.100], loss: 0.145494, mae: 0.355790, mean_q: 4.020226
 29917/100000: episode: 531, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 75.273, mean reward: 3.136 [2.422, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.537, 10.397], loss: 0.091533, mae: 0.314575, mean_q: 3.995842
 29922/100000: episode: 532, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 18.615, mean reward: 3.723 [2.788, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.514], loss: 0.162721, mae: 0.351027, mean_q: 3.894114
 29963/100000: episode: 533, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 95.502, mean reward: 2.329 [1.716, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.243, 10.100], loss: 0.099271, mae: 0.316615, mean_q: 3.974128
 30000/100000: episode: 534, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 96.347, mean reward: 2.604 [1.614, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.213, 10.111], loss: 0.109166, mae: 0.321923, mean_q: 3.994000
 30037/100000: episode: 535, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 140.643, mean reward: 3.801 [2.349, 7.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.663, 10.100], loss: 0.118873, mae: 0.325135, mean_q: 4.022084
 30090/100000: episode: 536, duration: 0.284s, episode steps: 53, steps per second: 186, episode reward: 208.160, mean reward: 3.928 [1.736, 46.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-1.196, 10.100], loss: 0.119868, mae: 0.335308, mean_q: 4.024900
 30120/100000: episode: 537, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 82.570, mean reward: 2.752 [1.843, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.174, 10.100], loss: 0.112750, mae: 0.331924, mean_q: 4.053973
 30150/100000: episode: 538, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 95.691, mean reward: 3.190 [2.278, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.597, 10.100], loss: 0.092358, mae: 0.313064, mean_q: 4.044900
 30180/100000: episode: 539, duration: 0.147s, episode steps: 30, steps per second: 203, episode reward: 98.838, mean reward: 3.295 [2.273, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.548, 10.100], loss: 0.110329, mae: 0.323822, mean_q: 4.038764
 30221/100000: episode: 540, duration: 0.232s, episode steps: 41, steps per second: 176, episode reward: 81.015, mean reward: 1.976 [1.462, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.045, 10.100], loss: 0.104783, mae: 0.332055, mean_q: 4.111997
 30243/100000: episode: 541, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 71.630, mean reward: 3.256 [2.472, 5.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.454], loss: 0.115946, mae: 0.335299, mean_q: 4.044475
 30289/100000: episode: 542, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 131.855, mean reward: 2.866 [1.985, 7.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.646, 10.100], loss: 0.817394, mae: 0.399854, mean_q: 4.167875
 30335/100000: episode: 543, duration: 0.242s, episode steps: 46, steps per second: 190, episode reward: 114.451, mean reward: 2.488 [1.720, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.004, 10.100], loss: 0.132284, mae: 0.368810, mean_q: 4.130924
 30359/100000: episode: 544, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 75.339, mean reward: 3.139 [2.436, 5.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.471], loss: 0.168826, mae: 0.370513, mean_q: 4.174361
 30400/100000: episode: 545, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 102.670, mean reward: 2.504 [1.529, 5.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.057, 10.100], loss: 0.121835, mae: 0.347541, mean_q: 4.099186
 30405/100000: episode: 546, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 17.532, mean reward: 3.506 [3.258, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.630, 10.521], loss: 0.102997, mae: 0.288356, mean_q: 3.975189
 30418/100000: episode: 547, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 34.924, mean reward: 2.686 [2.133, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.067, 10.100], loss: 0.161089, mae: 0.360181, mean_q: 4.130133
 30423/100000: episode: 548, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 18.026, mean reward: 3.605 [2.824, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.453], loss: 0.112468, mae: 0.317088, mean_q: 4.058295
 30453/100000: episode: 549, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 70.527, mean reward: 2.351 [1.826, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.166, 10.100], loss: 0.144267, mae: 0.365035, mean_q: 4.176725
 30475/100000: episode: 550, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 49.868, mean reward: 2.267 [1.775, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.886, 10.288], loss: 0.129299, mae: 0.367034, mean_q: 4.175853
 30488/100000: episode: 551, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 28.145, mean reward: 2.165 [1.718, 2.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.036, 10.100], loss: 0.141006, mae: 0.354684, mean_q: 4.108565
 30509/100000: episode: 552, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 43.597, mean reward: 2.076 [1.683, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.964, 10.100], loss: 0.129503, mae: 0.354157, mean_q: 4.091123
 30531/100000: episode: 553, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 61.303, mean reward: 2.787 [1.856, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.744, 10.304], loss: 0.150522, mae: 0.358265, mean_q: 4.191359
 30536/100000: episode: 554, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 16.684, mean reward: 3.337 [3.109, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.490], loss: 0.121357, mae: 0.330154, mean_q: 4.103458
 30558/100000: episode: 555, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 65.620, mean reward: 2.983 [1.961, 6.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.436, 10.353], loss: 0.124820, mae: 0.344938, mean_q: 4.206711
 30579/100000: episode: 556, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 46.939, mean reward: 2.235 [1.836, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.830, 10.100], loss: 0.152675, mae: 0.354136, mean_q: 4.167416
 30609/100000: episode: 557, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 82.392, mean reward: 2.746 [1.985, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.412, 10.100], loss: 0.157464, mae: 0.378355, mean_q: 4.187574
 30662/100000: episode: 558, duration: 0.278s, episode steps: 53, steps per second: 190, episode reward: 174.671, mean reward: 3.296 [1.461, 6.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-1.285, 10.162], loss: 0.145919, mae: 0.380036, mean_q: 4.206293
 30675/100000: episode: 559, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 64.973, mean reward: 4.998 [2.130, 20.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.762, 10.100], loss: 0.156553, mae: 0.373739, mean_q: 4.173260
 30688/100000: episode: 560, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 33.268, mean reward: 2.559 [2.035, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.366, 10.100], loss: 0.146530, mae: 0.385342, mean_q: 4.276548
 30701/100000: episode: 561, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 34.309, mean reward: 2.639 [2.266, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.312, 10.100], loss: 0.127908, mae: 0.346345, mean_q: 4.270301
 30714/100000: episode: 562, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 42.631, mean reward: 3.279 [2.222, 6.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.362, 10.100], loss: 0.155710, mae: 0.366754, mean_q: 4.236752
 30735/100000: episode: 563, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 49.418, mean reward: 2.353 [1.924, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.229, 10.100], loss: 0.395533, mae: 0.418008, mean_q: 4.222889
 30757/100000: episode: 564, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 59.530, mean reward: 2.706 [1.935, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.129, 10.327], loss: 0.169427, mae: 0.386225, mean_q: 4.191858
 30798/100000: episode: 565, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 104.697, mean reward: 2.554 [1.688, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.236, 10.100], loss: 0.261991, mae: 0.400313, mean_q: 4.275222
 30811/100000: episode: 566, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 38.471, mean reward: 2.959 [2.226, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.168, 10.100], loss: 0.255572, mae: 0.443971, mean_q: 4.286083
 30841/100000: episode: 567, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 93.215, mean reward: 3.107 [2.282, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.423, 10.100], loss: 0.158982, mae: 0.385471, mean_q: 4.250985
 30894/100000: episode: 568, duration: 0.275s, episode steps: 53, steps per second: 193, episode reward: 133.009, mean reward: 2.510 [1.491, 5.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.103, 10.120], loss: 0.150688, mae: 0.372597, mean_q: 4.242504
 30918/100000: episode: 569, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 55.131, mean reward: 2.297 [1.439, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.259, 10.200], loss: 0.135745, mae: 0.363631, mean_q: 4.250646
 30971/100000: episode: 570, duration: 0.273s, episode steps: 53, steps per second: 194, episode reward: 146.436, mean reward: 2.763 [1.527, 6.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.582, 10.100], loss: 0.715723, mae: 0.404056, mean_q: 4.256664
 30995/100000: episode: 571, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 84.014, mean reward: 3.501 [2.257, 6.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.370], loss: 0.268624, mae: 0.531183, mean_q: 4.361805
 31016/100000: episode: 572, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 52.713, mean reward: 2.510 [1.921, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.565, 10.100], loss: 0.496269, mae: 0.504081, mean_q: 4.299420
 31057/100000: episode: 573, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 90.187, mean reward: 2.200 [1.713, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.376, 10.100], loss: 0.281642, mae: 0.424794, mean_q: 4.344548
 31081/100000: episode: 574, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 53.204, mean reward: 2.217 [1.510, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.125, 10.115], loss: 0.140941, mae: 0.366073, mean_q: 4.236883
 31134/100000: episode: 575, duration: 0.268s, episode steps: 53, steps per second: 198, episode reward: 136.979, mean reward: 2.585 [1.490, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.722, 10.185], loss: 0.724888, mae: 0.409961, mean_q: 4.367611
 31187/100000: episode: 576, duration: 0.259s, episode steps: 53, steps per second: 204, episode reward: 170.737, mean reward: 3.221 [2.431, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.257, 10.100], loss: 1.371952, mae: 0.538297, mean_q: 4.432820
 31233/100000: episode: 577, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 106.987, mean reward: 2.326 [1.551, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-1.368, 10.100], loss: 0.833749, mae: 0.458448, mean_q: 4.411299
 31257/100000: episode: 578, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 62.245, mean reward: 2.594 [1.451, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.595, 10.176], loss: 0.382639, mae: 0.454518, mean_q: 4.381262
 31262/100000: episode: 579, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 17.116, mean reward: 3.423 [3.000, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.498], loss: 0.122472, mae: 0.372243, mean_q: 4.438649
 31299/100000: episode: 580, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 115.579, mean reward: 3.124 [1.929, 4.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.779, 10.100], loss: 0.970382, mae: 0.411630, mean_q: 4.331315
 31329/100000: episode: 581, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 89.295, mean reward: 2.977 [1.963, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.571, 10.100], loss: 0.233950, mae: 0.469366, mean_q: 4.474024
 31382/100000: episode: 582, duration: 0.279s, episode steps: 53, steps per second: 190, episode reward: 144.423, mean reward: 2.725 [1.955, 6.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.279, 10.100], loss: 0.166832, mae: 0.402881, mean_q: 4.407632
 31435/100000: episode: 583, duration: 0.262s, episode steps: 53, steps per second: 202, episode reward: 231.471, mean reward: 4.367 [2.076, 11.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.888, 10.100], loss: 0.867700, mae: 0.484758, mean_q: 4.490879
 31457/100000: episode: 584, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 100.109, mean reward: 4.550 [3.061, 7.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.391, 10.402], loss: 0.170450, mae: 0.385361, mean_q: 4.419634
 31510/100000: episode: 585, duration: 0.263s, episode steps: 53, steps per second: 202, episode reward: 121.598, mean reward: 2.294 [1.566, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.399, 10.100], loss: 0.188185, mae: 0.405810, mean_q: 4.456315
 31551/100000: episode: 586, duration: 0.220s, episode steps: 41, steps per second: 187, episode reward: 87.441, mean reward: 2.133 [1.467, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.293, 10.100], loss: 0.206775, mae: 0.453227, mean_q: 4.549057
 31572/100000: episode: 587, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 53.152, mean reward: 2.531 [2.056, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.078, 10.100], loss: 0.205204, mae: 0.433211, mean_q: 4.497035
 31585/100000: episode: 588, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 45.263, mean reward: 3.482 [2.550, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.566, 10.100], loss: 0.234613, mae: 0.485756, mean_q: 4.634963
 31615/100000: episode: 589, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 89.139, mean reward: 2.971 [1.810, 5.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.163, 10.100], loss: 0.222234, mae: 0.436206, mean_q: 4.519147
 31620/100000: episode: 590, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 17.124, mean reward: 3.425 [2.944, 5.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.464], loss: 0.116995, mae: 0.379101, mean_q: 4.461633
 31657/100000: episode: 591, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 121.824, mean reward: 3.293 [2.068, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.620, 10.100], loss: 0.170344, mae: 0.416989, mean_q: 4.555144
 31687/100000: episode: 592, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 89.938, mean reward: 2.998 [2.222, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.441, 10.100], loss: 0.410775, mae: 0.482224, mean_q: 4.600466
 31709/100000: episode: 593, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 83.838, mean reward: 3.811 [2.704, 9.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.185, 10.465], loss: 0.223993, mae: 0.472802, mean_q: 4.675401
 31730/100000: episode: 594, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 52.485, mean reward: 2.499 [1.843, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.339, 10.100], loss: 0.311136, mae: 0.474051, mean_q: 4.577112
 31760/100000: episode: 595, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 107.737, mean reward: 3.591 [2.701, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.497, 10.100], loss: 1.224125, mae: 0.489881, mean_q: 4.678563
 31782/100000: episode: 596, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 94.022, mean reward: 4.274 [2.983, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.634, 10.528], loss: 0.216428, mae: 0.441066, mean_q: 4.562232
 31803/100000: episode: 597, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 61.892, mean reward: 2.947 [1.932, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.209, 10.100], loss: 0.203467, mae: 0.444119, mean_q: 4.607146
 31808/100000: episode: 598, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 22.556, mean reward: 4.511 [3.890, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.567], loss: 0.161424, mae: 0.391186, mean_q: 4.560251
 31813/100000: episode: 599, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 16.311, mean reward: 3.262 [2.758, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.457], loss: 0.278981, mae: 0.474924, mean_q: 4.696349
 31818/100000: episode: 600, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 18.330, mean reward: 3.666 [3.536, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.514], loss: 0.222643, mae: 0.445622, mean_q: 4.461639
 31840/100000: episode: 601, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 84.877, mean reward: 3.858 [2.893, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.566, 10.488], loss: 1.786364, mae: 0.591590, mean_q: 4.732409
 31853/100000: episode: 602, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 32.950, mean reward: 2.535 [2.175, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.307, 10.100], loss: 0.323195, mae: 0.575202, mean_q: 4.705178
 31894/100000: episode: 603, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 95.789, mean reward: 2.336 [1.720, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.207, 10.100], loss: 0.257229, mae: 0.473040, mean_q: 4.612963
 31907/100000: episode: 604, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 29.175, mean reward: 2.244 [1.730, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.308, 10.100], loss: 0.212440, mae: 0.429739, mean_q: 4.716334
 31931/100000: episode: 605, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 122.671, mean reward: 5.111 [2.911, 7.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.615], loss: 0.182331, mae: 0.410277, mean_q: 4.667676
 31984/100000: episode: 606, duration: 0.285s, episode steps: 53, steps per second: 186, episode reward: 121.365, mean reward: 2.290 [1.550, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.635, 10.103], loss: 0.199759, mae: 0.432939, mean_q: 4.647397
 32030/100000: episode: 607, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 97.005, mean reward: 2.109 [1.479, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.150, 10.100], loss: 0.173099, mae: 0.410309, mean_q: 4.706099
 32060/100000: episode: 608, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 101.211, mean reward: 3.374 [2.403, 5.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.291, 10.100], loss: 0.230521, mae: 0.446753, mean_q: 4.695064
 32097/100000: episode: 609, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 96.808, mean reward: 2.616 [1.653, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.127, 10.107], loss: 0.201565, mae: 0.420991, mean_q: 4.668995
[Info] 2-TH LEVEL FOUND: 7.921803951263428, Considering 10/90 traces
 32127/100000: episode: 610, duration: 4.220s, episode steps: 30, steps per second: 7, episode reward: 81.724, mean reward: 2.724 [2.074, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.264, 10.100], loss: 0.211266, mae: 0.435139, mean_q: 4.712479
 32172/100000: episode: 611, duration: 0.248s, episode steps: 45, steps per second: 181, episode reward: 168.371, mean reward: 3.742 [2.795, 11.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.353, 10.100], loss: 0.188907, mae: 0.420484, mean_q: 4.665285
 32180/100000: episode: 612, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 29.567, mean reward: 3.696 [3.152, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.325, 10.100], loss: 0.254201, mae: 0.472101, mean_q: 4.724710
 32225/100000: episode: 613, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 171.654, mean reward: 3.815 [2.359, 9.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.761, 10.100], loss: 0.347106, mae: 0.458154, mean_q: 4.753532
 32242/100000: episode: 614, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 59.834, mean reward: 3.520 [2.533, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.405], loss: 2.203815, mae: 0.566758, mean_q: 4.768694
 32248/100000: episode: 615, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 25.579, mean reward: 4.263 [3.202, 5.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.539], loss: 0.420488, mae: 0.620960, mean_q: 5.010218
 32254/100000: episode: 616, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 27.324, mean reward: 4.554 [3.686, 5.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.517], loss: 0.364602, mae: 0.595755, mean_q: 4.593063
 32295/100000: episode: 617, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 229.990, mean reward: 5.610 [2.254, 11.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.059, 10.100], loss: 0.261466, mae: 0.485426, mean_q: 4.861302
 32336/100000: episode: 618, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 137.406, mean reward: 3.351 [1.585, 9.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.159, 10.100], loss: 0.335557, mae: 0.509348, mean_q: 4.882349
 32353/100000: episode: 619, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 76.403, mean reward: 4.494 [3.400, 6.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.564], loss: 0.225992, mae: 0.473580, mean_q: 4.911694
 32361/100000: episode: 620, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 30.275, mean reward: 3.784 [3.250, 4.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.336, 10.100], loss: 0.322227, mae: 0.496205, mean_q: 4.844873
 32374/100000: episode: 621, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 47.565, mean reward: 3.659 [2.748, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.349, 10.100], loss: 0.234984, mae: 0.447410, mean_q: 4.826434
 32382/100000: episode: 622, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 25.111, mean reward: 3.139 [2.854, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.377, 10.100], loss: 0.378941, mae: 0.539683, mean_q: 4.898960
 32390/100000: episode: 623, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 30.942, mean reward: 3.868 [3.035, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.346, 10.100], loss: 0.259630, mae: 0.478592, mean_q: 4.866802
 32403/100000: episode: 624, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 51.187, mean reward: 3.937 [2.697, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.226, 10.100], loss: 0.320587, mae: 0.482846, mean_q: 4.858071
 32422/100000: episode: 625, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 74.021, mean reward: 3.896 [2.809, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.379, 10.100], loss: 0.210870, mae: 0.447037, mean_q: 4.921838
 32439/100000: episode: 626, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 66.416, mean reward: 3.907 [2.970, 5.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.063, 10.496], loss: 0.274878, mae: 0.469156, mean_q: 4.940444
[Info] FALSIFICATION!
 32450/100000: episode: 627, duration: 0.225s, episode steps: 11, steps per second: 49, episode reward: 1061.810, mean reward: 96.528 [4.753, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.018, 8.112], loss: 0.239363, mae: 0.479519, mean_q: 4.987796
 32467/100000: episode: 628, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 109.157, mean reward: 6.421 [3.492, 14.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.659, 10.549], loss: 910.102356, mae: 2.377729, mean_q: 5.066842
 32512/100000: episode: 629, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 192.832, mean reward: 4.285 [2.477, 6.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.878, 10.100], loss: 2.115898, mae: 1.458347, mean_q: 5.386803
 32518/100000: episode: 630, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 41.421, mean reward: 6.904 [5.141, 8.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.599], loss: 0.467738, mae: 0.687514, mean_q: 5.449636
 32537/100000: episode: 631, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 89.846, mean reward: 4.729 [3.497, 7.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.841, 10.100], loss: 0.561857, mae: 0.669530, mean_q: 5.421292
 32543/100000: episode: 632, duration: 0.046s, episode steps: 6, steps per second: 130, episode reward: 26.558, mean reward: 4.426 [3.466, 5.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.204, 10.476], loss: 0.503371, mae: 0.609296, mean_q: 5.528761
 32556/100000: episode: 633, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 45.155, mean reward: 3.473 [2.491, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.523, 10.100], loss: 0.434425, mae: 0.573814, mean_q: 5.361095
 32569/100000: episode: 634, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 59.393, mean reward: 4.569 [3.676, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.335, 10.100], loss: 0.321079, mae: 0.553249, mean_q: 5.373744
 32575/100000: episode: 635, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 34.546, mean reward: 5.758 [4.312, 6.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.365, 10.473], loss: 0.358893, mae: 0.491619, mean_q: 5.081768
 32583/100000: episode: 636, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 33.177, mean reward: 4.147 [3.305, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.329, 10.100], loss: 0.383152, mae: 0.595300, mean_q: 5.356138
 32622/100000: episode: 637, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 120.878, mean reward: 3.099 [2.117, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.868, 10.100], loss: 0.607079, mae: 0.607005, mean_q: 5.331697
 32635/100000: episode: 638, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 54.746, mean reward: 4.211 [3.080, 6.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.885, 10.100], loss: 1188.622314, mae: 2.952614, mean_q: 5.351469
 32652/100000: episode: 639, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 91.513, mean reward: 5.383 [3.624, 8.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.502], loss: 2.017091, mae: 1.715670, mean_q: 6.458679
 32660/100000: episode: 640, duration: 0.061s, episode steps: 8, steps per second: 132, episode reward: 33.375, mean reward: 4.172 [3.166, 6.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.323, 10.100], loss: 0.664205, mae: 0.917053, mean_q: 5.580585
 32705/100000: episode: 641, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 155.321, mean reward: 3.452 [1.475, 7.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.914, 10.100], loss: 0.566127, mae: 0.705844, mean_q: 5.281451
 32711/100000: episode: 642, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 31.790, mean reward: 5.298 [4.045, 8.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.681], loss: 0.616009, mae: 0.647828, mean_q: 5.315988
 32750/100000: episode: 643, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 130.445, mean reward: 3.345 [1.845, 6.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.538, 10.100], loss: 2.072455, mae: 0.748479, mean_q: 5.377828
 32791/100000: episode: 644, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 119.953, mean reward: 2.926 [1.596, 6.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.610, 10.100], loss: 0.394359, mae: 0.612296, mean_q: 5.233023
 32804/100000: episode: 645, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 56.326, mean reward: 4.333 [3.105, 6.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.268, 10.100], loss: 0.415320, mae: 0.616575, mean_q: 5.311673
 32845/100000: episode: 646, duration: 0.211s, episode steps: 41, steps per second: 195, episode reward: 141.674, mean reward: 3.455 [1.952, 7.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.294, 10.100], loss: 1.845041, mae: 0.681395, mean_q: 5.301370
 32858/100000: episode: 647, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 76.752, mean reward: 5.904 [4.428, 7.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.657, 10.100], loss: 1187.223511, mae: 3.237080, mean_q: 5.591866
 32883/100000: episode: 648, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 84.394, mean reward: 3.376 [2.610, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.955, 10.100], loss: 618.097717, mae: 2.934557, mean_q: 6.380478
 32889/100000: episode: 649, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 29.573, mean reward: 4.929 [4.572, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.580], loss: 1.890809, mae: 1.620547, mean_q: 7.090996
 32928/100000: episode: 650, duration: 0.206s, episode steps: 39, steps per second: 190, episode reward: 103.397, mean reward: 2.651 [1.745, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.722, 10.100], loss: 1.694271, mae: 1.071439, mean_q: 5.846698
 32934/100000: episode: 651, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 41.126, mean reward: 6.854 [5.685, 8.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.468, 10.650], loss: 1.176888, mae: 0.931640, mean_q: 5.863049
 32953/100000: episode: 652, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 92.676, mean reward: 4.878 [2.608, 9.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.306, 10.100], loss: 0.818722, mae: 0.770137, mean_q: 5.664914
 32961/100000: episode: 653, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 31.346, mean reward: 3.918 [3.324, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.450, 10.100], loss: 0.325831, mae: 0.619226, mean_q: 5.497742
 32986/100000: episode: 654, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 101.949, mean reward: 4.078 [2.620, 6.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.435, 10.100], loss: 617.718872, mae: 2.658083, mean_q: 6.496593
 33027/100000: episode: 655, duration: 0.211s, episode steps: 41, steps per second: 195, episode reward: 183.425, mean reward: 4.474 [1.485, 12.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.581, 10.115], loss: 0.916868, mae: 0.877346, mean_q: 5.560866
 33052/100000: episode: 656, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 107.317, mean reward: 4.293 [3.120, 8.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.251, 10.100], loss: 0.670521, mae: 0.767988, mean_q: 5.723350
 33091/100000: episode: 657, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 86.227, mean reward: 2.211 [1.656, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.254, 10.100], loss: 0.596806, mae: 0.711241, mean_q: 5.661358
[Info] FALSIFICATION!
 33097/100000: episode: 658, duration: 0.289s, episode steps: 6, steps per second: 21, episode reward: 1161.736, mean reward: 193.623 [4.905, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.668 [-0.016, 7.336], loss: 0.449671, mae: 0.662642, mean_q: 5.578155
 33105/100000: episode: 659, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 23.451, mean reward: 2.931 [2.549, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.282, 10.100], loss: 1929.162842, mae: 5.082872, mean_q: 6.166642
 33150/100000: episode: 660, duration: 0.261s, episode steps: 45, steps per second: 173, episode reward: 163.298, mean reward: 3.629 [1.952, 13.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.375, 10.100], loss: 1.992595, mae: 1.240704, mean_q: 6.254662
 33163/100000: episode: 661, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 40.281, mean reward: 3.099 [2.535, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.256, 10.100], loss: 6.341164, mae: 0.932001, mean_q: 5.781520
 33169/100000: episode: 662, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 39.449, mean reward: 6.575 [4.803, 8.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.669], loss: 0.428017, mae: 0.718476, mean_q: 5.981040
 33182/100000: episode: 663, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 42.471, mean reward: 3.267 [2.262, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.278, 10.100], loss: 0.704486, mae: 0.782720, mean_q: 5.972453
 33201/100000: episode: 664, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 426.841, mean reward: 22.465 [4.297, 316.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.497, 10.100], loss: 0.552407, mae: 0.724325, mean_q: 5.804275
 33226/100000: episode: 665, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 83.821, mean reward: 3.353 [2.756, 5.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.026, 10.100], loss: 617.741028, mae: 2.244771, mean_q: 6.104395
 33245/100000: episode: 666, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 75.577, mean reward: 3.978 [2.875, 6.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.360, 10.100], loss: 2.178296, mae: 1.752855, mean_q: 7.079725
 33253/100000: episode: 667, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 29.889, mean reward: 3.736 [2.941, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.405, 10.100], loss: 10.122927, mae: 1.310328, mean_q: 6.096082
 33259/100000: episode: 668, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 25.232, mean reward: 4.205 [3.729, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.573], loss: 0.720816, mae: 0.854265, mean_q: 5.897706
 33304/100000: episode: 669, duration: 0.245s, episode steps: 45, steps per second: 184, episode reward: 150.357, mean reward: 3.341 [1.841, 8.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.615, 10.100], loss: 343.448090, mae: 1.908281, mean_q: 6.599468
 33317/100000: episode: 670, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 50.862, mean reward: 3.912 [3.148, 5.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.173, 10.100], loss: 0.526864, mae: 0.727816, mean_q: 6.017581
 33356/100000: episode: 671, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 141.367, mean reward: 3.625 [2.045, 5.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.454, 10.100], loss: 395.469513, mae: 2.186003, mean_q: 6.757748
 33375/100000: episode: 672, duration: 0.113s, episode steps: 19, steps per second: 167, episode reward: 69.740, mean reward: 3.671 [2.671, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.289, 10.100], loss: 810.305481, mae: 2.965594, mean_q: 7.029379
 33388/100000: episode: 673, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 44.960, mean reward: 3.458 [2.387, 5.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.366, 10.100], loss: 1180.076660, mae: 3.531217, mean_q: 6.765517
 33396/100000: episode: 674, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 26.360, mean reward: 3.295 [2.845, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.430, 10.100], loss: 7.414322, mae: 1.825680, mean_q: 7.502643
 33441/100000: episode: 675, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 129.062, mean reward: 2.868 [1.492, 6.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.932, 10.131], loss: 1.011019, mae: 1.053153, mean_q: 6.547641
 33447/100000: episode: 676, duration: 0.044s, episode steps: 6, steps per second: 137, episode reward: 28.622, mean reward: 4.770 [3.903, 5.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.548], loss: 0.414841, mae: 0.707026, mean_q: 5.996832
 33464/100000: episode: 677, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 118.860, mean reward: 6.992 [3.009, 30.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.619], loss: 2.179165, mae: 0.777206, mean_q: 5.954197
 33505/100000: episode: 678, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 146.170, mean reward: 3.565 [2.339, 6.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.257, 10.100], loss: 376.604950, mae: 2.130028, mean_q: 6.829402
 33518/100000: episode: 679, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 63.104, mean reward: 4.854 [3.474, 6.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.242, 10.100], loss: 4.383382, mae: 0.952658, mean_q: 6.236495
 33537/100000: episode: 680, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 69.680, mean reward: 3.667 [2.619, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.295, 10.100], loss: 0.742862, mae: 0.774284, mean_q: 6.140168
 33543/100000: episode: 681, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 39.215, mean reward: 6.536 [5.275, 8.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.644], loss: 2.504582, mae: 0.935217, mean_q: 6.267004
 33551/100000: episode: 682, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 30.607, mean reward: 3.826 [2.945, 4.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.266, 10.100], loss: 10.081117, mae: 1.131186, mean_q: 6.367433
 33590/100000: episode: 683, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 95.747, mean reward: 2.455 [1.464, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.035, 10.280], loss: 435.526703, mae: 2.533861, mean_q: 7.066697
 33596/100000: episode: 684, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 51.322, mean reward: 8.554 [6.281, 13.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.571], loss: 2556.445068, mae: 6.392397, mean_q: 7.412912
 33602/100000: episode: 685, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 32.718, mean reward: 5.453 [4.669, 6.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.603], loss: 2551.370850, mae: 7.451415, mean_q: 8.471130
 33647/100000: episode: 686, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 140.740, mean reward: 3.128 [1.896, 6.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.504, 10.100], loss: 37.770718, mae: 1.849794, mean_q: 7.412195
 33692/100000: episode: 687, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 108.900, mean reward: 2.420 [1.486, 6.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.292, 10.100], loss: 684.967163, mae: 2.852560, mean_q: 7.273603
 33731/100000: episode: 688, duration: 0.210s, episode steps: 39, steps per second: 185, episode reward: 190.629, mean reward: 4.888 [2.696, 10.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-1.540, 10.100], loss: 395.569000, mae: 2.882810, mean_q: 7.867105
 33756/100000: episode: 689, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 165.380, mean reward: 6.615 [2.593, 22.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.672, 10.100], loss: 673.892212, mae: 3.330502, mean_q: 7.653082
 33795/100000: episode: 690, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 142.355, mean reward: 3.650 [2.902, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.510, 10.100], loss: 788.911133, mae: 3.043037, mean_q: 7.369880
 33834/100000: episode: 691, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 144.977, mean reward: 3.717 [1.708, 6.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.325, 10.100], loss: 2.928301, mae: 1.873139, mean_q: 7.555188
 33847/100000: episode: 692, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 51.579, mean reward: 3.968 [2.864, 5.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.323, 10.100], loss: 1180.051147, mae: 4.000823, mean_q: 7.616574
 33853/100000: episode: 693, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 24.876, mean reward: 4.146 [3.862, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.428], loss: 10.182767, mae: 2.300442, mean_q: 8.068928
 33892/100000: episode: 694, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 210.888, mean reward: 5.407 [2.806, 9.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.687, 10.100], loss: 1.305143, mae: 1.244006, mean_q: 7.163854
 33917/100000: episode: 695, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 74.140, mean reward: 2.966 [2.177, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.474, 10.100], loss: 614.143555, mae: 2.533968, mean_q: 7.452585
 33925/100000: episode: 696, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 27.240, mean reward: 3.405 [3.003, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.423, 10.100], loss: 1.227094, mae: 1.225988, mean_q: 7.205517
 33942/100000: episode: 697, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 121.715, mean reward: 7.160 [3.011, 14.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.114, 10.587], loss: 0.907028, mae: 1.007316, mean_q: 6.596655
 33983/100000: episode: 698, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 99.473, mean reward: 2.426 [1.566, 6.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.707, 10.100], loss: 38.668312, mae: 1.223978, mean_q: 6.775027
 34002/100000: episode: 699, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 81.282, mean reward: 4.278 [3.291, 7.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.529, 10.100], loss: 5.041978, mae: 1.263388, mean_q: 7.171457
[Info] Complete ISplit Iteration
[Info] Levels: [5.1226096, 7.921804, 9.996494]
[Info] Cond. Prob: [0.1, 0.1, 0.56]
[Info] Error Prob: 0.005600000000000002

 34015/100000: episode: 700, duration: 4.341s, episode steps: 13, steps per second: 3, episode reward: 61.600, mean reward: 4.738 [2.568, 7.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.351, 10.100], loss: 0.969591, mae: 0.942732, mean_q: 6.610769
 34115/100000: episode: 701, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 196.811, mean reward: 1.968 [1.454, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.706, 10.098], loss: 156.808044, mae: 1.460281, mean_q: 6.784321
 34215/100000: episode: 702, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.383, mean reward: 1.914 [1.448, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.889, 10.098], loss: 171.114899, mae: 1.518415, mean_q: 6.704740
 34315/100000: episode: 703, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.678, mean reward: 1.877 [1.456, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.392, 10.098], loss: 155.102692, mae: 1.595948, mean_q: 6.849319
 34415/100000: episode: 704, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 205.384, mean reward: 2.054 [1.498, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.836, 10.098], loss: 170.144714, mae: 1.605649, mean_q: 6.931456
 34515/100000: episode: 705, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 208.990, mean reward: 2.090 [1.492, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.172], loss: 324.152710, mae: 1.985772, mean_q: 7.003429
 34615/100000: episode: 706, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 178.568, mean reward: 1.786 [1.493, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.925, 10.098], loss: 1.404503, mae: 1.076738, mean_q: 6.700528
 34715/100000: episode: 707, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 227.153, mean reward: 2.272 [1.504, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.428, 10.429], loss: 308.324036, mae: 1.788681, mean_q: 6.827424
 34815/100000: episode: 708, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 205.054, mean reward: 2.051 [1.496, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.668, 10.183], loss: 307.045654, mae: 1.838719, mean_q: 6.901194
 34915/100000: episode: 709, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 217.958, mean reward: 2.180 [1.457, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.098], loss: 155.904633, mae: 1.435529, mean_q: 6.601036
 35015/100000: episode: 710, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.944, mean reward: 1.829 [1.463, 5.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.920, 10.098], loss: 323.377136, mae: 2.087309, mean_q: 6.990455
 35115/100000: episode: 711, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 206.073, mean reward: 2.061 [1.509, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.519, 10.098], loss: 1.452053, mae: 0.967411, mean_q: 6.410611
 35215/100000: episode: 712, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 204.823, mean reward: 2.048 [1.505, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.012, 10.098], loss: 352.884735, mae: 2.422101, mean_q: 7.130619
 35315/100000: episode: 713, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.233, mean reward: 1.892 [1.434, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.566, 10.109], loss: 16.023752, mae: 0.939788, mean_q: 6.279395
 35415/100000: episode: 714, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 198.206, mean reward: 1.982 [1.470, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.656, 10.141], loss: 1.547908, mae: 0.761017, mean_q: 6.073830
 35515/100000: episode: 715, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 196.598, mean reward: 1.966 [1.454, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.545, 10.098], loss: 0.649243, mae: 0.692990, mean_q: 5.946005
 35615/100000: episode: 716, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.306, mean reward: 1.953 [1.456, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.311, 10.233], loss: 1.081447, mae: 0.690685, mean_q: 5.848505
 35715/100000: episode: 717, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 189.994, mean reward: 1.900 [1.504, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.044, 10.273], loss: 308.707886, mae: 1.681035, mean_q: 6.297414
 35815/100000: episode: 718, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 212.490, mean reward: 2.125 [1.454, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.142, 10.098], loss: 323.642700, mae: 2.282716, mean_q: 6.781605
 35915/100000: episode: 719, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 179.138, mean reward: 1.791 [1.460, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.525, 10.098], loss: 459.660004, mae: 2.423430, mean_q: 7.099518
 36015/100000: episode: 720, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.057, mean reward: 1.851 [1.448, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.809, 10.215], loss: 18.035433, mae: 1.057265, mean_q: 6.265278
 36115/100000: episode: 721, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.249, mean reward: 1.852 [1.481, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.188], loss: 308.926300, mae: 1.956189, mean_q: 6.483035
 36215/100000: episode: 722, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 194.760, mean reward: 1.948 [1.505, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.567, 10.165], loss: 2.222656, mae: 0.869033, mean_q: 5.939315
 36315/100000: episode: 723, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.835, mean reward: 1.958 [1.454, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.995, 10.194], loss: 154.987289, mae: 1.088539, mean_q: 5.877117
 36415/100000: episode: 724, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.747, mean reward: 2.027 [1.489, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.265, 10.125], loss: 308.644775, mae: 2.128654, mean_q: 6.648775
 36515/100000: episode: 725, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 206.677, mean reward: 2.067 [1.523, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.793, 10.098], loss: 155.369720, mae: 1.457849, mean_q: 6.226569
 36615/100000: episode: 726, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.276, mean reward: 1.843 [1.439, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.217], loss: 624.404846, mae: 2.582619, mean_q: 6.504775
 36715/100000: episode: 727, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.527, mean reward: 1.975 [1.457, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.517, 10.098], loss: 306.680481, mae: 2.287615, mean_q: 6.727914
 36815/100000: episode: 728, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 196.916, mean reward: 1.969 [1.457, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.576, 10.173], loss: 154.038788, mae: 1.373987, mean_q: 6.012163
 36915/100000: episode: 729, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 205.842, mean reward: 2.058 [1.488, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.469, 10.277], loss: 305.703430, mae: 1.861137, mean_q: 6.366485
 37015/100000: episode: 730, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.001, mean reward: 2.000 [1.444, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.334, 10.428], loss: 154.752579, mae: 1.346037, mean_q: 6.078890
 37115/100000: episode: 731, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 239.359, mean reward: 2.394 [1.437, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.608, 10.098], loss: 306.035736, mae: 1.839783, mean_q: 6.243611
 37215/100000: episode: 732, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 190.773, mean reward: 1.908 [1.457, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.320, 10.098], loss: 318.439545, mae: 1.720933, mean_q: 5.956865
 37315/100000: episode: 733, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.797, mean reward: 1.978 [1.449, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.399, 10.169], loss: 169.820816, mae: 1.482452, mean_q: 5.946024
 37415/100000: episode: 734, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.717, mean reward: 1.887 [1.439, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.625, 10.200], loss: 167.816635, mae: 1.382504, mean_q: 5.890010
 37515/100000: episode: 735, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.408, mean reward: 1.874 [1.441, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.109, 10.192], loss: 1.936034, mae: 0.698877, mean_q: 5.278132
 37615/100000: episode: 736, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 196.211, mean reward: 1.962 [1.440, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.098], loss: 199.358826, mae: 1.669367, mean_q: 5.707872
 37715/100000: episode: 737, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.138, mean reward: 1.851 [1.459, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.390], loss: 153.143707, mae: 1.264877, mean_q: 5.266909
 37815/100000: episode: 738, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.093, mean reward: 2.011 [1.476, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.098], loss: 1.938181, mae: 0.667321, mean_q: 4.921213
 37915/100000: episode: 739, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 213.776, mean reward: 2.138 [1.443, 4.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.223, 10.220], loss: 0.437554, mae: 0.550192, mean_q: 4.813086
 38015/100000: episode: 740, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.492, mean reward: 2.005 [1.510, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.877, 10.098], loss: 31.025684, mae: 0.839511, mean_q: 4.960101
 38115/100000: episode: 741, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 192.677, mean reward: 1.927 [1.449, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.828, 10.098], loss: 15.569921, mae: 0.677729, mean_q: 4.737356
 38215/100000: episode: 742, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 213.628, mean reward: 2.136 [1.486, 5.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.098], loss: 0.382084, mae: 0.484304, mean_q: 4.521822
 38315/100000: episode: 743, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 201.099, mean reward: 2.011 [1.475, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.637, 10.098], loss: 0.291659, mae: 0.441195, mean_q: 4.396475
 38415/100000: episode: 744, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.561, mean reward: 1.846 [1.463, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.098], loss: 0.411776, mae: 0.459472, mean_q: 4.358099
 38515/100000: episode: 745, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 224.563, mean reward: 2.246 [1.473, 4.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.631, 10.358], loss: 0.254264, mae: 0.435277, mean_q: 4.333796
 38615/100000: episode: 746, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 191.945, mean reward: 1.919 [1.450, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.966, 10.297], loss: 0.251650, mae: 0.432664, mean_q: 4.320192
 38715/100000: episode: 747, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.536, mean reward: 1.835 [1.484, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.406, 10.098], loss: 0.215294, mae: 0.412920, mean_q: 4.238026
 38815/100000: episode: 748, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 212.301, mean reward: 2.123 [1.500, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.098], loss: 0.162630, mae: 0.379951, mean_q: 4.136996
 38915/100000: episode: 749, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.273, mean reward: 1.983 [1.477, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.692, 10.284], loss: 0.130024, mae: 0.356910, mean_q: 4.029887
 39015/100000: episode: 750, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 208.750, mean reward: 2.088 [1.480, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.963, 10.411], loss: 0.118272, mae: 0.347729, mean_q: 3.957305
 39115/100000: episode: 751, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 188.836, mean reward: 1.888 [1.442, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.019, 10.098], loss: 0.116372, mae: 0.350477, mean_q: 3.949653
 39215/100000: episode: 752, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 206.118, mean reward: 2.061 [1.470, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.892, 10.171], loss: 0.115970, mae: 0.350555, mean_q: 3.957262
 39315/100000: episode: 753, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.068, mean reward: 1.871 [1.442, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.707, 10.098], loss: 0.107703, mae: 0.332236, mean_q: 3.933786
 39415/100000: episode: 754, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.731, mean reward: 1.897 [1.476, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.204], loss: 0.103235, mae: 0.324970, mean_q: 3.951343
 39515/100000: episode: 755, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 185.386, mean reward: 1.854 [1.438, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.923, 10.227], loss: 0.101200, mae: 0.329157, mean_q: 3.947809
 39615/100000: episode: 756, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 276.627, mean reward: 2.766 [1.509, 8.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.385, 10.186], loss: 0.117857, mae: 0.342025, mean_q: 3.943659
 39715/100000: episode: 757, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 182.942, mean reward: 1.829 [1.456, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.208], loss: 0.125149, mae: 0.348106, mean_q: 3.953363
 39815/100000: episode: 758, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 194.696, mean reward: 1.947 [1.484, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.269, 10.141], loss: 0.111210, mae: 0.334145, mean_q: 3.946406
 39915/100000: episode: 759, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 206.007, mean reward: 2.060 [1.465, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.644, 10.496], loss: 0.118088, mae: 0.335576, mean_q: 3.901482
 40015/100000: episode: 760, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.904, mean reward: 1.849 [1.442, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.378, 10.265], loss: 0.124155, mae: 0.339125, mean_q: 3.943079
 40115/100000: episode: 761, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.989, mean reward: 1.970 [1.468, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.133, 10.098], loss: 0.116337, mae: 0.336089, mean_q: 3.927795
 40215/100000: episode: 762, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.754, mean reward: 1.898 [1.452, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.649, 10.130], loss: 0.131384, mae: 0.340471, mean_q: 3.936082
 40315/100000: episode: 763, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 229.549, mean reward: 2.295 [1.494, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.811, 10.332], loss: 0.108256, mae: 0.321655, mean_q: 3.911511
 40415/100000: episode: 764, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.886, mean reward: 1.939 [1.467, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.715, 10.250], loss: 0.120471, mae: 0.328197, mean_q: 3.910100
 40515/100000: episode: 765, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.921, mean reward: 1.859 [1.450, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.601, 10.098], loss: 0.108015, mae: 0.327359, mean_q: 3.909637
 40615/100000: episode: 766, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 196.695, mean reward: 1.967 [1.463, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.506, 10.187], loss: 0.102743, mae: 0.325494, mean_q: 3.926265
 40715/100000: episode: 767, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 219.370, mean reward: 2.194 [1.473, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.610, 10.211], loss: 0.112608, mae: 0.327844, mean_q: 3.943593
 40815/100000: episode: 768, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.455, mean reward: 1.855 [1.498, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.761, 10.098], loss: 0.106052, mae: 0.322333, mean_q: 3.943813
 40915/100000: episode: 769, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 207.473, mean reward: 2.075 [1.446, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.983, 10.116], loss: 0.115938, mae: 0.327717, mean_q: 3.937176
 41015/100000: episode: 770, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.919, mean reward: 1.939 [1.471, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.116], loss: 0.106771, mae: 0.316092, mean_q: 3.937846
 41115/100000: episode: 771, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.930, mean reward: 1.889 [1.479, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.580, 10.098], loss: 0.116875, mae: 0.335882, mean_q: 3.952985
 41215/100000: episode: 772, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.945, mean reward: 1.869 [1.488, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.645, 10.098], loss: 0.118125, mae: 0.320327, mean_q: 3.947559
 41315/100000: episode: 773, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.818, mean reward: 1.898 [1.461, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.040, 10.098], loss: 0.118454, mae: 0.330531, mean_q: 3.945770
 41415/100000: episode: 774, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 218.976, mean reward: 2.190 [1.492, 9.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.463, 10.098], loss: 0.089663, mae: 0.301035, mean_q: 3.942258
 41515/100000: episode: 775, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.033, mean reward: 1.930 [1.435, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.284, 10.184], loss: 0.105304, mae: 0.312695, mean_q: 3.933326
 41615/100000: episode: 776, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.012, mean reward: 1.960 [1.475, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.352, 10.149], loss: 0.100354, mae: 0.313129, mean_q: 3.941952
 41715/100000: episode: 777, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 184.590, mean reward: 1.846 [1.475, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.221, 10.098], loss: 0.113202, mae: 0.324169, mean_q: 3.972281
 41815/100000: episode: 778, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 181.656, mean reward: 1.817 [1.437, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.592, 10.098], loss: 0.109259, mae: 0.316859, mean_q: 3.939319
 41915/100000: episode: 779, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.176, mean reward: 1.902 [1.480, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.098], loss: 0.116706, mae: 0.318877, mean_q: 3.935016
 42015/100000: episode: 780, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 178.820, mean reward: 1.788 [1.452, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.900, 10.174], loss: 0.111339, mae: 0.313966, mean_q: 3.903302
 42115/100000: episode: 781, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 176.479, mean reward: 1.765 [1.438, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.098], loss: 0.111803, mae: 0.316629, mean_q: 3.914751
 42215/100000: episode: 782, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 201.009, mean reward: 2.010 [1.491, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.771, 10.098], loss: 0.107825, mae: 0.310456, mean_q: 3.888446
 42315/100000: episode: 783, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 199.974, mean reward: 2.000 [1.442, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.690, 10.184], loss: 0.099680, mae: 0.303467, mean_q: 3.911297
 42415/100000: episode: 784, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 209.235, mean reward: 2.092 [1.481, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.332, 10.121], loss: 0.111240, mae: 0.315197, mean_q: 3.902275
 42515/100000: episode: 785, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 213.437, mean reward: 2.134 [1.503, 5.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.756, 10.098], loss: 0.093696, mae: 0.303618, mean_q: 3.913931
 42615/100000: episode: 786, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 192.609, mean reward: 1.926 [1.441, 6.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.601, 10.232], loss: 0.096902, mae: 0.307809, mean_q: 3.900726
 42715/100000: episode: 787, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 191.451, mean reward: 1.915 [1.480, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.506, 10.098], loss: 0.104996, mae: 0.302835, mean_q: 3.905755
 42815/100000: episode: 788, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 210.131, mean reward: 2.101 [1.448, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.874, 10.173], loss: 0.113025, mae: 0.317298, mean_q: 3.898911
 42915/100000: episode: 789, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 189.464, mean reward: 1.895 [1.460, 2.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.971, 10.280], loss: 0.105955, mae: 0.308512, mean_q: 3.898111
 43015/100000: episode: 790, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 216.739, mean reward: 2.167 [1.470, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.205, 10.098], loss: 0.092579, mae: 0.308445, mean_q: 3.882276
 43115/100000: episode: 791, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 208.344, mean reward: 2.083 [1.484, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.890, 10.098], loss: 0.104920, mae: 0.312884, mean_q: 3.904407
 43215/100000: episode: 792, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 192.480, mean reward: 1.925 [1.463, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.172, 10.098], loss: 0.111789, mae: 0.317102, mean_q: 3.896400
 43315/100000: episode: 793, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 213.395, mean reward: 2.134 [1.484, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.324, 10.311], loss: 0.100431, mae: 0.310964, mean_q: 3.908601
 43415/100000: episode: 794, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.195, mean reward: 1.942 [1.500, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.053, 10.165], loss: 0.102788, mae: 0.307933, mean_q: 3.929578
 43515/100000: episode: 795, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 201.881, mean reward: 2.019 [1.447, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.289], loss: 0.099260, mae: 0.308540, mean_q: 3.917957
 43615/100000: episode: 796, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 206.529, mean reward: 2.065 [1.450, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.108, 10.263], loss: 0.107984, mae: 0.312420, mean_q: 3.919149
 43715/100000: episode: 797, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.682, mean reward: 1.907 [1.437, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.292, 10.188], loss: 0.121212, mae: 0.325603, mean_q: 3.947586
 43815/100000: episode: 798, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.536, mean reward: 1.855 [1.454, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.738, 10.098], loss: 0.098424, mae: 0.308663, mean_q: 3.906875
 43915/100000: episode: 799, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.724, mean reward: 2.017 [1.467, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.416, 10.181], loss: 0.097358, mae: 0.299406, mean_q: 3.885724
[Info] 1-TH LEVEL FOUND: 5.863308906555176, Considering 10/90 traces
 44015/100000: episode: 800, duration: 4.595s, episode steps: 100, steps per second: 22, episode reward: 254.885, mean reward: 2.549 [1.547, 8.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.098], loss: 0.114952, mae: 0.320675, mean_q: 3.912215
 44048/100000: episode: 801, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 141.419, mean reward: 4.285 [2.635, 10.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.612, 10.100], loss: 0.126331, mae: 0.317531, mean_q: 3.949489
 44063/100000: episode: 802, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 35.533, mean reward: 2.369 [1.780, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.408], loss: 0.156681, mae: 0.341305, mean_q: 3.939506
 44088/100000: episode: 803, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 62.539, mean reward: 2.502 [1.937, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.452], loss: 0.146614, mae: 0.341466, mean_q: 3.979372
 44095/100000: episode: 804, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 21.021, mean reward: 3.003 [2.677, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.461], loss: 0.199220, mae: 0.333829, mean_q: 3.890669
 44137/100000: episode: 805, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 98.984, mean reward: 2.357 [1.637, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.196, 10.235], loss: 0.106244, mae: 0.323513, mean_q: 3.965320
 44173/100000: episode: 806, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 108.589, mean reward: 3.016 [2.506, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.487], loss: 0.151324, mae: 0.353558, mean_q: 3.996137
 44195/100000: episode: 807, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 64.443, mean reward: 2.929 [1.951, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.050, 10.529], loss: 0.121527, mae: 0.325215, mean_q: 3.953600
 44241/100000: episode: 808, duration: 0.232s, episode steps: 46, steps per second: 198, episode reward: 107.978, mean reward: 2.347 [1.480, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.249, 10.100], loss: 0.144034, mae: 0.349882, mean_q: 3.991740
 44287/100000: episode: 809, duration: 0.242s, episode steps: 46, steps per second: 190, episode reward: 108.838, mean reward: 2.366 [1.645, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.500, 10.264], loss: 0.167321, mae: 0.354345, mean_q: 4.006685
 44294/100000: episode: 810, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 19.376, mean reward: 2.768 [2.292, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.445], loss: 0.098974, mae: 0.320526, mean_q: 3.932610
 44340/100000: episode: 811, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 96.500, mean reward: 2.098 [1.444, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.591, 10.100], loss: 0.145953, mae: 0.363956, mean_q: 4.046717
 44347/100000: episode: 812, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 20.741, mean reward: 2.963 [2.746, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.476], loss: 0.111103, mae: 0.330270, mean_q: 3.987627
 44393/100000: episode: 813, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 120.409, mean reward: 2.618 [1.711, 13.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.778, 10.262], loss: 0.174457, mae: 0.370928, mean_q: 4.051494
 44400/100000: episode: 814, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 25.780, mean reward: 3.683 [3.399, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.542], loss: 0.117541, mae: 0.340714, mean_q: 4.043621
 44446/100000: episode: 815, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 113.369, mean reward: 2.465 [1.522, 4.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-2.087, 10.100], loss: 0.132750, mae: 0.350153, mean_q: 4.035427
 44492/100000: episode: 816, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 104.119, mean reward: 2.263 [1.598, 5.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.784, 10.209], loss: 0.130885, mae: 0.349326, mean_q: 4.067036
 44527/100000: episode: 817, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 87.737, mean reward: 2.507 [2.169, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.129, 10.359], loss: 0.128125, mae: 0.346668, mean_q: 4.064944
 44544/100000: episode: 818, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 38.313, mean reward: 2.254 [2.018, 2.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.337], loss: 0.092602, mae: 0.315619, mean_q: 3.997643
 44577/100000: episode: 819, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 158.666, mean reward: 4.808 [2.445, 20.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.345, 10.100], loss: 0.099456, mae: 0.320079, mean_q: 4.045037
 44612/100000: episode: 820, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 85.748, mean reward: 2.450 [1.649, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.473, 10.303], loss: 0.117897, mae: 0.338156, mean_q: 4.072187
 44637/100000: episode: 821, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 62.029, mean reward: 2.481 [2.072, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.528, 10.312], loss: 0.386309, mae: 0.404696, mean_q: 4.105375
 44652/100000: episode: 822, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 39.736, mean reward: 2.649 [2.182, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.377], loss: 0.353795, mae: 0.389653, mean_q: 4.109653
 44674/100000: episode: 823, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 43.675, mean reward: 1.985 [1.709, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.093, 10.344], loss: 0.152502, mae: 0.378642, mean_q: 4.111460
 44716/100000: episode: 824, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 98.774, mean reward: 2.352 [1.701, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.177, 10.286], loss: 0.148767, mae: 0.352564, mean_q: 4.077029
 44741/100000: episode: 825, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 59.651, mean reward: 2.386 [2.015, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.338, 10.414], loss: 0.206478, mae: 0.375074, mean_q: 4.097674
 44774/100000: episode: 826, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 106.561, mean reward: 3.229 [2.285, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.365, 10.100], loss: 0.244773, mae: 0.356530, mean_q: 4.070434
 44820/100000: episode: 827, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 115.138, mean reward: 2.503 [1.662, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.447, 10.249], loss: 0.293945, mae: 0.390083, mean_q: 4.176086
 44837/100000: episode: 828, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 45.566, mean reward: 2.680 [1.985, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.079, 10.399], loss: 0.196954, mae: 0.345154, mean_q: 4.123671
 44854/100000: episode: 829, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 39.742, mean reward: 2.338 [1.946, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.353], loss: 0.139003, mae: 0.351681, mean_q: 4.099041
 44889/100000: episode: 830, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 87.251, mean reward: 2.493 [1.749, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.286, 10.247], loss: 0.131292, mae: 0.344316, mean_q: 4.086309
 44904/100000: episode: 831, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 41.514, mean reward: 2.768 [2.359, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.377, 10.446], loss: 0.270891, mae: 0.379053, mean_q: 4.131835
 44929/100000: episode: 832, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 64.229, mean reward: 2.569 [2.004, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.327], loss: 0.143105, mae: 0.356839, mean_q: 4.180704
 44951/100000: episode: 833, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 75.922, mean reward: 3.451 [2.397, 7.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.656], loss: 0.135633, mae: 0.354360, mean_q: 4.117039
 44997/100000: episode: 834, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 104.771, mean reward: 2.278 [1.604, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.250, 10.262], loss: 0.189822, mae: 0.367493, mean_q: 4.111159
 45022/100000: episode: 835, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 64.985, mean reward: 2.599 [1.829, 7.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.737, 10.266], loss: 0.283873, mae: 0.410226, mean_q: 4.159716
 45037/100000: episode: 836, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 50.832, mean reward: 3.389 [2.511, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.513], loss: 0.121915, mae: 0.365435, mean_q: 4.239207
 45062/100000: episode: 837, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 89.817, mean reward: 3.593 [2.348, 5.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.562], loss: 0.278683, mae: 0.390629, mean_q: 4.245865
 45098/100000: episode: 838, duration: 0.215s, episode steps: 36, steps per second: 167, episode reward: 76.374, mean reward: 2.121 [1.594, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.228], loss: 0.167683, mae: 0.380627, mean_q: 4.163005
 45123/100000: episode: 839, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 64.296, mean reward: 2.572 [1.675, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.730, 10.100], loss: 0.223238, mae: 0.384262, mean_q: 4.165517
 45169/100000: episode: 840, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 156.067, mean reward: 3.393 [1.892, 5.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.779, 10.396], loss: 0.210725, mae: 0.397364, mean_q: 4.240506
 45205/100000: episode: 841, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 96.656, mean reward: 2.685 [1.790, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.421, 10.303], loss: 0.303772, mae: 0.404372, mean_q: 4.221557
 45230/100000: episode: 842, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 59.871, mean reward: 2.395 [1.938, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.410], loss: 0.192798, mae: 0.385345, mean_q: 4.273188
 45272/100000: episode: 843, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 108.171, mean reward: 2.575 [1.817, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.243, 10.438], loss: 0.159288, mae: 0.384863, mean_q: 4.250494
 45305/100000: episode: 844, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 112.954, mean reward: 3.423 [2.111, 5.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.417, 10.100], loss: 0.257401, mae: 0.395624, mean_q: 4.244203
 45338/100000: episode: 845, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 129.649, mean reward: 3.929 [2.702, 5.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.316, 10.100], loss: 0.223579, mae: 0.416101, mean_q: 4.315729
 45363/100000: episode: 846, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 56.821, mean reward: 2.273 [1.867, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.358], loss: 0.163010, mae: 0.385320, mean_q: 4.256668
 45380/100000: episode: 847, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 51.132, mean reward: 3.008 [2.319, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.292, 10.476], loss: 0.165040, mae: 0.385169, mean_q: 4.361392
 45402/100000: episode: 848, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 47.016, mean reward: 2.137 [1.672, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.213, 10.231], loss: 0.126702, mae: 0.355471, mean_q: 4.281921
 45435/100000: episode: 849, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 96.039, mean reward: 2.910 [2.026, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.100], loss: 0.262448, mae: 0.399234, mean_q: 4.320994
 45452/100000: episode: 850, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 44.599, mean reward: 2.623 [2.145, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.254, 10.418], loss: 0.255845, mae: 0.372825, mean_q: 4.330351
 45477/100000: episode: 851, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 60.960, mean reward: 2.438 [2.086, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.875, 10.377], loss: 0.153884, mae: 0.377894, mean_q: 4.240788
 45484/100000: episode: 852, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 23.074, mean reward: 3.296 [2.775, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.126, 10.506], loss: 0.195566, mae: 0.377707, mean_q: 4.169824
 45501/100000: episode: 853, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 38.971, mean reward: 2.292 [1.582, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.200, 10.307], loss: 0.142744, mae: 0.398947, mean_q: 4.377499
 45534/100000: episode: 854, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 126.030, mean reward: 3.819 [2.617, 9.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.582, 10.100], loss: 0.407544, mae: 0.437595, mean_q: 4.400063
 45549/100000: episode: 855, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 43.521, mean reward: 2.901 [2.274, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.765, 10.509], loss: 0.256576, mae: 0.449553, mean_q: 4.516263
 45556/100000: episode: 856, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 19.566, mean reward: 2.795 [2.495, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.467], loss: 0.172696, mae: 0.407424, mean_q: 4.172201
 45573/100000: episode: 857, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 47.584, mean reward: 2.799 [2.031, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.731, 10.474], loss: 0.125847, mae: 0.375293, mean_q: 4.291852
 45580/100000: episode: 858, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 20.145, mean reward: 2.878 [2.553, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.455], loss: 0.131096, mae: 0.360740, mean_q: 4.383404
 45602/100000: episode: 859, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 45.760, mean reward: 2.080 [1.664, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.470, 10.299], loss: 0.316271, mae: 0.448386, mean_q: 4.454407
 45624/100000: episode: 860, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 68.683, mean reward: 3.122 [1.728, 5.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.593], loss: 0.236184, mae: 0.421495, mean_q: 4.479863
 45659/100000: episode: 861, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 87.636, mean reward: 2.504 [2.081, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.503, 10.344], loss: 0.214517, mae: 0.420152, mean_q: 4.321511
 45695/100000: episode: 862, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 85.917, mean reward: 2.387 [1.511, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.902, 10.171], loss: 0.198376, mae: 0.399286, mean_q: 4.380037
 45728/100000: episode: 863, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 139.766, mean reward: 4.235 [2.979, 11.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.291, 10.100], loss: 0.237550, mae: 0.409657, mean_q: 4.370999
 45764/100000: episode: 864, duration: 0.185s, episode steps: 36, steps per second: 194, episode reward: 80.598, mean reward: 2.239 [1.663, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.035, 10.339], loss: 0.165534, mae: 0.392360, mean_q: 4.358741
 45810/100000: episode: 865, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 101.699, mean reward: 2.211 [1.488, 5.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.498, 10.100], loss: 0.192053, mae: 0.394186, mean_q: 4.395587
 45832/100000: episode: 866, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 56.950, mean reward: 2.589 [1.617, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.181, 10.445], loss: 0.361501, mae: 0.442567, mean_q: 4.503517
 45867/100000: episode: 867, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 69.339, mean reward: 1.981 [1.544, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.142, 10.281], loss: 0.180160, mae: 0.393443, mean_q: 4.442696
 45884/100000: episode: 868, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 52.040, mean reward: 3.061 [2.380, 5.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.427, 10.403], loss: 0.184461, mae: 0.403112, mean_q: 4.404076
 45891/100000: episode: 869, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 28.041, mean reward: 4.006 [3.441, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.578], loss: 0.184159, mae: 0.410005, mean_q: 4.315853
 45916/100000: episode: 870, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 75.997, mean reward: 3.040 [2.232, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.604, 10.462], loss: 0.196374, mae: 0.400258, mean_q: 4.460799
 45923/100000: episode: 871, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 18.418, mean reward: 2.631 [2.145, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.367], loss: 0.683727, mae: 0.520609, mean_q: 4.548563
 45930/100000: episode: 872, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 18.273, mean reward: 2.610 [2.069, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.416], loss: 0.225649, mae: 0.404057, mean_q: 4.394935
 45955/100000: episode: 873, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 60.744, mean reward: 2.430 [1.845, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.470, 10.278], loss: 0.181185, mae: 0.414138, mean_q: 4.407628
 45997/100000: episode: 874, duration: 0.229s, episode steps: 42, steps per second: 184, episode reward: 98.307, mean reward: 2.341 [1.646, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.374, 10.249], loss: 0.167323, mae: 0.396050, mean_q: 4.392132
 46014/100000: episode: 875, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 40.350, mean reward: 2.374 [2.048, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.431], loss: 0.216226, mae: 0.430154, mean_q: 4.478600
 46047/100000: episode: 876, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 79.854, mean reward: 2.420 [1.740, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-1.359, 10.100], loss: 0.181690, mae: 0.401102, mean_q: 4.433222
 46054/100000: episode: 877, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 20.879, mean reward: 2.983 [2.226, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.378, 10.330], loss: 0.396126, mae: 0.549276, mean_q: 4.786067
 46087/100000: episode: 878, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 104.854, mean reward: 3.177 [2.023, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.362, 10.100], loss: 0.145663, mae: 0.371404, mean_q: 4.349021
 46120/100000: episode: 879, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 102.496, mean reward: 3.106 [1.972, 5.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.597, 10.100], loss: 0.209137, mae: 0.424647, mean_q: 4.566259
 46145/100000: episode: 880, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 73.509, mean reward: 2.940 [2.211, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.579, 10.524], loss: 0.174311, mae: 0.397933, mean_q: 4.460248
 46178/100000: episode: 881, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 121.603, mean reward: 3.685 [2.262, 6.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.590, 10.100], loss: 0.184695, mae: 0.413703, mean_q: 4.574181
 46211/100000: episode: 882, duration: 0.202s, episode steps: 33, steps per second: 163, episode reward: 188.517, mean reward: 5.713 [2.795, 19.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.458, 10.100], loss: 0.237739, mae: 0.424381, mean_q: 4.571331
 46247/100000: episode: 883, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 75.510, mean reward: 2.098 [1.446, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.039, 10.228], loss: 0.296995, mae: 0.424897, mean_q: 4.546619
 46269/100000: episode: 884, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 45.494, mean reward: 2.068 [1.726, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.228], loss: 0.275793, mae: 0.456573, mean_q: 4.561007
 46311/100000: episode: 885, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 109.972, mean reward: 2.618 [1.882, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.190, 10.408], loss: 0.345166, mae: 0.439180, mean_q: 4.541034
 46328/100000: episode: 886, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 50.238, mean reward: 2.955 [2.240, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.257, 10.491], loss: 0.257796, mae: 0.483479, mean_q: 4.628218
 46343/100000: episode: 887, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 42.315, mean reward: 2.821 [2.048, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.603, 10.526], loss: 0.259570, mae: 0.453742, mean_q: 4.604551
 46350/100000: episode: 888, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 21.114, mean reward: 3.016 [2.544, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.541], loss: 0.514697, mae: 0.516424, mean_q: 4.701928
 46375/100000: episode: 889, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 64.605, mean reward: 2.584 [1.787, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.346], loss: 0.437939, mae: 0.497177, mean_q: 4.674038
[Info] 2-TH LEVEL FOUND: 8.146985054016113, Considering 10/90 traces
 46400/100000: episode: 890, duration: 4.200s, episode steps: 25, steps per second: 6, episode reward: 65.371, mean reward: 2.615 [2.005, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.117, 10.428], loss: 0.321906, mae: 0.448182, mean_q: 4.628879
 46428/100000: episode: 891, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 112.305, mean reward: 4.011 [1.881, 6.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.230, 10.100], loss: 0.423515, mae: 0.505812, mean_q: 4.675270
 46459/100000: episode: 892, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 104.195, mean reward: 3.361 [1.876, 12.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.537, 10.100], loss: 0.303976, mae: 0.473021, mean_q: 4.694541
 46490/100000: episode: 893, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 170.782, mean reward: 5.509 [2.857, 12.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.493, 10.100], loss: 0.234417, mae: 0.441615, mean_q: 4.671576
 46518/100000: episode: 894, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 80.480, mean reward: 2.874 [2.153, 4.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.387, 10.100], loss: 0.449211, mae: 0.529503, mean_q: 4.773744
 46546/100000: episode: 895, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 121.681, mean reward: 4.346 [2.655, 8.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.387, 10.100], loss: 0.329969, mae: 0.463783, mean_q: 4.757395
 46574/100000: episode: 896, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 114.703, mean reward: 4.097 [2.695, 6.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.285, 10.100], loss: 0.524742, mae: 0.519344, mean_q: 4.740138
 46602/100000: episode: 897, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 124.460, mean reward: 4.445 [2.722, 8.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.547, 10.100], loss: 0.202206, mae: 0.429379, mean_q: 4.739340
 46630/100000: episode: 898, duration: 0.160s, episode steps: 28, steps per second: 176, episode reward: 109.305, mean reward: 3.904 [2.825, 6.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.761, 10.100], loss: 0.273072, mae: 0.473368, mean_q: 4.750024
 46660/100000: episode: 899, duration: 0.180s, episode steps: 30, steps per second: 167, episode reward: 107.799, mean reward: 3.593 [2.031, 7.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.935, 10.100], loss: 0.339185, mae: 0.463678, mean_q: 4.794247
 46688/100000: episode: 900, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 137.774, mean reward: 4.921 [2.846, 8.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.145, 10.100], loss: 0.281086, mae: 0.484671, mean_q: 4.768291
 46716/100000: episode: 901, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 115.585, mean reward: 4.128 [2.097, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.378, 10.100], loss: 0.394685, mae: 0.508153, mean_q: 4.869978
 46747/100000: episode: 902, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 97.062, mean reward: 3.131 [2.273, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.299, 10.100], loss: 0.433803, mae: 0.520079, mean_q: 4.885110
 46778/100000: episode: 903, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 177.905, mean reward: 5.739 [3.552, 11.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.481, 10.100], loss: 0.314054, mae: 0.497141, mean_q: 4.783916
 46809/100000: episode: 904, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 222.816, mean reward: 7.188 [2.932, 20.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.624, 10.100], loss: 0.290858, mae: 0.501836, mean_q: 4.923960
[Info] FALSIFICATION!
 46820/100000: episode: 905, duration: 0.379s, episode steps: 11, steps per second: 29, episode reward: 1096.190, mean reward: 99.654 [4.053, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.007, 9.231], loss: 0.269863, mae: 0.497591, mean_q: 4.883840
 46848/100000: episode: 906, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 144.406, mean reward: 5.157 [3.061, 23.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.307, 10.100], loss: 0.458450, mae: 0.545840, mean_q: 5.044458
 46878/100000: episode: 907, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 139.320, mean reward: 4.644 [2.945, 6.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.324, 10.100], loss: 0.636447, mae: 0.578500, mean_q: 5.023596
 46906/100000: episode: 908, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 402.981, mean reward: 14.392 [3.118, 276.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.912, 10.100], loss: 42.885414, mae: 1.334660, mean_q: 5.648512
 46934/100000: episode: 909, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 98.716, mean reward: 3.526 [3.097, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.278, 10.100], loss: 84.362305, mae: 1.840053, mean_q: 5.545350
 46962/100000: episode: 910, duration: 0.166s, episode steps: 28, steps per second: 169, episode reward: 96.071, mean reward: 3.431 [1.979, 6.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.475, 10.100], loss: 1.083272, mae: 0.877787, mean_q: 5.062099
 46993/100000: episode: 911, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 198.118, mean reward: 6.391 [2.762, 21.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.988, 10.100], loss: 38.106503, mae: 1.174842, mean_q: 5.762814
 47024/100000: episode: 912, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 91.472, mean reward: 2.951 [1.997, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.947, 10.100], loss: 38.548237, mae: 1.151086, mean_q: 5.561293
 47052/100000: episode: 913, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 186.102, mean reward: 6.647 [3.054, 10.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.403, 10.100], loss: 0.748805, mae: 0.688806, mean_q: 5.173448
 47083/100000: episode: 914, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 100.425, mean reward: 3.240 [2.126, 6.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.376, 10.100], loss: 495.029144, mae: 2.385730, mean_q: 6.470608
 47113/100000: episode: 915, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 107.832, mean reward: 3.594 [2.524, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.457, 10.100], loss: 1.053613, mae: 0.820447, mean_q: 5.148558
 47143/100000: episode: 916, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 76.377, mean reward: 2.546 [1.872, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.397, 10.100], loss: 0.812303, mae: 0.685954, mean_q: 5.635706
 47171/100000: episode: 917, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 104.547, mean reward: 3.734 [2.698, 6.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.337, 10.100], loss: 0.408601, mae: 0.560388, mean_q: 5.523609
 47202/100000: episode: 918, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 165.725, mean reward: 5.346 [2.717, 36.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.318, 10.100], loss: 0.762449, mae: 0.644323, mean_q: 5.587075
 47232/100000: episode: 919, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 171.154, mean reward: 5.705 [3.606, 10.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.342, 10.100], loss: 0.627811, mae: 0.610241, mean_q: 5.503565
 47263/100000: episode: 920, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 117.736, mean reward: 3.798 [2.279, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.716, 10.100], loss: 528.372742, mae: 2.297544, mean_q: 5.912164
 47291/100000: episode: 921, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 91.995, mean reward: 3.286 [2.220, 6.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.417, 10.100], loss: 5.036122, mae: 1.678746, mean_q: 6.554241
[Info] FALSIFICATION!
 47299/100000: episode: 922, duration: 0.200s, episode steps: 8, steps per second: 40, episode reward: 1044.314, mean reward: 130.539 [3.113, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.008, 9.139], loss: 0.702723, mae: 0.748270, mean_q: 5.364598
 47317/100000: episode: 923, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 59.587, mean reward: 3.310 [2.320, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.282, 10.100], loss: 846.435486, mae: 2.468185, mean_q: 5.981148
 47362/100000: episode: 924, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 119.911, mean reward: 2.665 [1.572, 11.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.199, 10.212], loss: 367.898560, mae: 3.068331, mean_q: 7.102458
 47390/100000: episode: 925, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 104.055, mean reward: 3.716 [2.418, 5.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.606, 10.100], loss: 547.543823, mae: 2.447332, mean_q: 6.737556
 47418/100000: episode: 926, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 105.105, mean reward: 3.754 [2.379, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.646, 10.100], loss: 1.455038, mae: 0.975759, mean_q: 6.131485
 47449/100000: episode: 927, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 84.436, mean reward: 2.724 [1.980, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.259, 10.100], loss: 980.521057, mae: 3.240332, mean_q: 6.660937
 47477/100000: episode: 928, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 333.900, mean reward: 11.925 [2.934, 103.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.605, 10.100], loss: 6.634944, mae: 2.122089, mean_q: 7.047684
 47495/100000: episode: 929, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 155.199, mean reward: 8.622 [3.114, 30.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.828, 10.100], loss: 855.158203, mae: 3.635580, mean_q: 7.591547
 47540/100000: episode: 930, duration: 0.229s, episode steps: 45, steps per second: 196, episode reward: 130.612, mean reward: 2.902 [1.725, 7.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.181, 10.213], loss: 704.965210, mae: 3.615840, mean_q: 8.047917
 47571/100000: episode: 931, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 116.319, mean reward: 3.752 [2.796, 5.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.400, 10.100], loss: 492.266449, mae: 2.609733, mean_q: 7.639436
 47616/100000: episode: 932, duration: 0.256s, episode steps: 45, steps per second: 176, episode reward: 122.881, mean reward: 2.731 [1.704, 15.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.924, 10.332], loss: 1.358005, mae: 0.968391, mean_q: 6.667961
 47644/100000: episode: 933, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 89.498, mean reward: 3.196 [2.321, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.236, 10.100], loss: 0.862619, mae: 0.836598, mean_q: 6.416884
 47675/100000: episode: 934, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 187.947, mean reward: 6.063 [3.480, 15.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.800, 10.100], loss: 979.460388, mae: 2.943778, mean_q: 6.542819
 47703/100000: episode: 935, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 102.040, mean reward: 3.644 [2.617, 6.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.274, 10.100], loss: 8.649134, mae: 2.300405, mean_q: 8.424234
 47734/100000: episode: 936, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 93.761, mean reward: 3.025 [1.944, 5.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.699, 10.100], loss: 570.531616, mae: 3.280383, mean_q: 7.582856
 47752/100000: episode: 937, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 69.916, mean reward: 3.884 [2.512, 10.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.730, 10.100], loss: 2.632593, mae: 1.446025, mean_q: 7.744410
 47780/100000: episode: 938, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 113.771, mean reward: 4.063 [2.799, 8.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.459, 10.100], loss: 1.266539, mae: 0.841931, mean_q: 6.576148
 47811/100000: episode: 939, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 162.092, mean reward: 5.229 [3.169, 17.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.452, 10.100], loss: 9.210100, mae: 0.914396, mean_q: 6.504799
 47842/100000: episode: 940, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 137.436, mean reward: 4.433 [3.272, 7.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.168, 10.100], loss: 37.522320, mae: 1.095233, mean_q: 6.715734
 47873/100000: episode: 941, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 135.031, mean reward: 4.356 [2.667, 9.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.774, 10.100], loss: 5.108716, mae: 0.888563, mean_q: 6.488088
 47904/100000: episode: 942, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 98.562, mean reward: 3.179 [2.348, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.429, 10.100], loss: 1.107709, mae: 0.807819, mean_q: 6.559950
 47935/100000: episode: 943, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 157.122, mean reward: 5.068 [3.094, 10.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.862, 10.100], loss: 495.942047, mae: 2.305088, mean_q: 6.966035
 47953/100000: episode: 944, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 78.420, mean reward: 4.357 [2.969, 6.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.399, 10.100], loss: 1.706931, mae: 1.242876, mean_q: 7.208189
 47984/100000: episode: 945, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 169.027, mean reward: 5.452 [3.497, 9.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.484, 10.100], loss: 38.163780, mae: 1.147908, mean_q: 6.486948
 48015/100000: episode: 946, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 81.583, mean reward: 2.632 [2.148, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.425, 10.100], loss: 5.609227, mae: 0.969141, mean_q: 6.489019
 48043/100000: episode: 947, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 204.274, mean reward: 7.296 [2.599, 38.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.097, 10.100], loss: 1.117820, mae: 0.829212, mean_q: 6.444041
 48073/100000: episode: 948, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 102.182, mean reward: 3.406 [2.534, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.184, 10.100], loss: 1053.475464, mae: 5.246573, mean_q: 9.155116
 48104/100000: episode: 949, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 158.953, mean reward: 5.128 [3.057, 8.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.345, 10.100], loss: 1.925392, mae: 1.282509, mean_q: 7.716401
 48132/100000: episode: 950, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 100.510, mean reward: 3.590 [2.180, 6.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.252, 10.100], loss: 545.340820, mae: 2.257444, mean_q: 7.127236
[Info] FALSIFICATION!
 48158/100000: episode: 951, duration: 0.321s, episode steps: 26, steps per second: 81, episode reward: 1165.790, mean reward: 44.838 [2.963, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.502, 10.093], loss: 3.450348, mae: 1.612156, mean_q: 7.916561
 48189/100000: episode: 952, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 113.576, mean reward: 3.664 [2.386, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.127, 10.100], loss: 1.047883, mae: 0.911985, mean_q: 6.744042
 48220/100000: episode: 953, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 170.504, mean reward: 5.500 [3.488, 14.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.224, 10.100], loss: 3.120418, mae: 0.959478, mean_q: 6.920125
 48251/100000: episode: 954, duration: 0.153s, episode steps: 31, steps per second: 202, episode reward: 201.075, mean reward: 6.486 [3.240, 55.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.275, 10.100], loss: 983.304260, mae: 3.651680, mean_q: 7.831367
 48282/100000: episode: 955, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 250.085, mean reward: 8.067 [3.228, 37.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.426, 10.100], loss: 43.295395, mae: 1.929833, mean_q: 8.203681
 48300/100000: episode: 956, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 68.092, mean reward: 3.783 [2.773, 5.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.353, 10.100], loss: 1.258595, mae: 0.987329, mean_q: 7.278035
 48331/100000: episode: 957, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 148.929, mean reward: 4.804 [2.554, 7.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.280, 10.100], loss: 494.172791, mae: 2.174866, mean_q: 7.276577
 48362/100000: episode: 958, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 135.549, mean reward: 4.373 [2.973, 6.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.333, 10.100], loss: 490.777161, mae: 2.996338, mean_q: 8.731342
 48393/100000: episode: 959, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 193.198, mean reward: 6.232 [3.178, 24.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.963, 10.100], loss: 4.993353, mae: 1.065201, mean_q: 7.195791
 48411/100000: episode: 960, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 56.978, mean reward: 3.165 [2.189, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.265, 10.100], loss: 1.201667, mae: 0.896176, mean_q: 7.104300
 48442/100000: episode: 961, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 107.853, mean reward: 3.479 [1.745, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.573, 10.100], loss: 1.918259, mae: 0.962623, mean_q: 7.264669
[Info] FALSIFICATION!
 48466/100000: episode: 962, duration: 0.339s, episode steps: 24, steps per second: 71, episode reward: 1144.221, mean reward: 47.676 [2.976, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.243, 9.988], loss: 634.692993, mae: 2.460779, mean_q: 7.380622
 48494/100000: episode: 963, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 84.202, mean reward: 3.007 [2.001, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.977, 10.100], loss: 548.034607, mae: 2.989164, mean_q: 8.545575
 48539/100000: episode: 964, duration: 0.240s, episode steps: 45, steps per second: 188, episode reward: 104.185, mean reward: 2.315 [1.485, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.088, 10.185], loss: 27.635696, mae: 1.318784, mean_q: 7.447485
[Info] FALSIFICATION!
 48544/100000: episode: 965, duration: 0.186s, episode steps: 5, steps per second: 27, episode reward: 1017.728, mean reward: 203.546 [3.051, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.003, 9.695], loss: 3041.719971, mae: 7.143992, mean_q: 7.473696
 48572/100000: episode: 966, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 128.658, mean reward: 4.595 [2.782, 7.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.607, 10.100], loss: 587.515686, mae: 3.710396, mean_q: 9.129299
 48603/100000: episode: 967, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 106.338, mean reward: 3.430 [2.278, 5.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.529, 10.100], loss: 982.969055, mae: 4.456999, mean_q: 9.676948
 48634/100000: episode: 968, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 90.288, mean reward: 2.913 [1.836, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.577, 10.100], loss: 6.083714, mae: 1.315704, mean_q: 7.502294
 48665/100000: episode: 969, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 519.983, mean reward: 16.774 [2.956, 132.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.685, 10.100], loss: 565.048828, mae: 3.133182, mean_q: 8.116326
 48683/100000: episode: 970, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 64.925, mean reward: 3.607 [2.499, 5.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.225, 10.100], loss: 13.032497, mae: 2.065371, mean_q: 8.816102
 48711/100000: episode: 971, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 110.188, mean reward: 3.935 [2.810, 5.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.373, 10.100], loss: 1098.787964, mae: 4.558761, mean_q: 9.288253
 48739/100000: episode: 972, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 86.384, mean reward: 3.085 [2.038, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.228, 10.100], loss: 1087.174683, mae: 5.775966, mean_q: 11.004066
 48770/100000: episode: 973, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 115.811, mean reward: 3.736 [2.249, 6.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.366, 10.100], loss: 503.980347, mae: 2.724178, mean_q: 8.907088
 48801/100000: episode: 974, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 87.490, mean reward: 2.822 [1.865, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.587, 10.100], loss: 1018.496399, mae: 3.964032, mean_q: 8.838011
 48819/100000: episode: 975, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 62.332, mean reward: 3.463 [2.297, 9.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.373, 10.100], loss: 24.885355, mae: 3.832434, mean_q: 11.370872
 48864/100000: episode: 976, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 96.709, mean reward: 2.149 [1.468, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.199, 10.164], loss: 350.845001, mae: 2.316921, mean_q: 8.786533
 48909/100000: episode: 977, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 105.136, mean reward: 2.336 [1.451, 5.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.287, 10.191], loss: 352.937622, mae: 2.366581, mean_q: 8.686311
 48937/100000: episode: 978, duration: 0.138s, episode steps: 28, steps per second: 202, episode reward: 155.816, mean reward: 5.565 [2.708, 9.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.192, 10.100], loss: 585.297791, mae: 3.254099, mean_q: 9.240415
 48965/100000: episode: 979, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 108.461, mean reward: 3.874 [2.569, 11.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.277, 10.100], loss: 3.181334, mae: 1.406785, mean_q: 8.408956
[Info] Complete ISplit Iteration
[Info] Levels: [5.863309, 8.146985, 12.844744]
[Info] Cond. Prob: [0.1, 0.1, 0.63]
[Info] Error Prob: 0.006300000000000001

 48983/100000: episode: 980, duration: 4.334s, episode steps: 18, steps per second: 4, episode reward: 55.876, mean reward: 3.104 [2.046, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.619, 10.100], loss: 12.593230, mae: 1.157198, mean_q: 7.510668
 49083/100000: episode: 981, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.156, mean reward: 1.942 [1.493, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.606, 10.278], loss: 324.081573, mae: 2.365550, mean_q: 8.331074
 49183/100000: episode: 982, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.883, mean reward: 1.819 [1.450, 2.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.484, 10.367], loss: 157.923431, mae: 1.778682, mean_q: 8.230222
 49283/100000: episode: 983, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.748, mean reward: 1.817 [1.439, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.016, 10.098], loss: 765.546631, mae: 3.646056, mean_q: 8.808497
 49383/100000: episode: 984, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 199.423, mean reward: 1.994 [1.443, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.992, 10.276], loss: 162.942993, mae: 1.965074, mean_q: 8.215645
 49483/100000: episode: 985, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 174.092, mean reward: 1.741 [1.458, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.324, 10.098], loss: 772.537720, mae: 4.012646, mean_q: 9.364419
 49583/100000: episode: 986, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.385, mean reward: 1.854 [1.450, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.098], loss: 624.598938, mae: 3.612287, mean_q: 9.102600
 49683/100000: episode: 987, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 199.033, mean reward: 1.990 [1.453, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.086, 10.277], loss: 160.592880, mae: 1.839586, mean_q: 8.282585
 49783/100000: episode: 988, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 211.429, mean reward: 2.114 [1.439, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.734, 10.098], loss: 323.779175, mae: 2.470123, mean_q: 8.678702
 49883/100000: episode: 989, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.669, mean reward: 1.977 [1.493, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.397, 10.212], loss: 461.718933, mae: 2.479417, mean_q: 8.167000
 49983/100000: episode: 990, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.866, mean reward: 1.869 [1.474, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.459, 10.162], loss: 622.353333, mae: 3.801449, mean_q: 9.101412
 50083/100000: episode: 991, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.961, mean reward: 1.840 [1.454, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.143, 10.305], loss: 473.655884, mae: 2.914161, mean_q: 8.946451
 50183/100000: episode: 992, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.506, mean reward: 1.895 [1.442, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.027, 10.197], loss: 474.950806, mae: 2.991690, mean_q: 8.644495
 50283/100000: episode: 993, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.010, mean reward: 1.830 [1.482, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.449, 10.119], loss: 939.379822, mae: 4.942137, mean_q: 10.126471
 50383/100000: episode: 994, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.054, mean reward: 1.901 [1.468, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.096, 10.098], loss: 756.999207, mae: 3.584575, mean_q: 9.031463
 50483/100000: episode: 995, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 182.570, mean reward: 1.826 [1.480, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.134], loss: 484.759094, mae: 3.165601, mean_q: 9.274393
 50583/100000: episode: 996, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 209.407, mean reward: 2.094 [1.455, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.245, 10.217], loss: 625.499512, mae: 3.472220, mean_q: 9.208789
 50683/100000: episode: 997, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.708, mean reward: 2.037 [1.442, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.254], loss: 788.380554, mae: 3.790797, mean_q: 9.075303
 50783/100000: episode: 998, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.923, mean reward: 1.899 [1.457, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.858, 10.098], loss: 629.322449, mae: 3.758552, mean_q: 9.727527
 50883/100000: episode: 999, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 194.194, mean reward: 1.942 [1.447, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.740, 10.268], loss: 155.368576, mae: 1.547843, mean_q: 7.717214
 50983/100000: episode: 1000, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.901, mean reward: 1.859 [1.468, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.919, 10.098], loss: 459.607727, mae: 2.444900, mean_q: 8.097536
 51083/100000: episode: 1001, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.478, mean reward: 1.975 [1.472, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.220, 10.158], loss: 605.404053, mae: 3.329222, mean_q: 8.945413
 51183/100000: episode: 1002, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.645, mean reward: 1.906 [1.466, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.597, 10.098], loss: 488.623474, mae: 2.878365, mean_q: 8.324177
 51283/100000: episode: 1003, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.170, mean reward: 1.832 [1.436, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.236, 10.098], loss: 466.395905, mae: 2.969619, mean_q: 8.725500
 51383/100000: episode: 1004, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.032, mean reward: 1.920 [1.493, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.632, 10.098], loss: 781.094360, mae: 3.662900, mean_q: 8.628376
 51483/100000: episode: 1005, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 184.014, mean reward: 1.840 [1.442, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.124, 10.098], loss: 622.815674, mae: 3.244096, mean_q: 8.417682
 51583/100000: episode: 1006, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.296, mean reward: 1.863 [1.461, 2.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.033, 10.098], loss: 454.868958, mae: 3.002037, mean_q: 8.559937
 51683/100000: episode: 1007, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.436, mean reward: 2.024 [1.507, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.294], loss: 168.300140, mae: 1.744955, mean_q: 7.230463
 51783/100000: episode: 1008, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.817, mean reward: 1.848 [1.446, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.244], loss: 765.498047, mae: 3.744554, mean_q: 8.317785
 51883/100000: episode: 1009, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.055, mean reward: 2.001 [1.481, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.717, 10.098], loss: 312.118378, mae: 2.172992, mean_q: 7.377610
 51983/100000: episode: 1010, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.181, mean reward: 1.992 [1.455, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.776, 10.137], loss: 156.513397, mae: 1.476016, mean_q: 6.683512
 52083/100000: episode: 1011, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.332, mean reward: 1.873 [1.452, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.680, 10.287], loss: 158.299179, mae: 1.303906, mean_q: 6.269739
 52183/100000: episode: 1012, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.307, mean reward: 1.903 [1.447, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.174, 10.120], loss: 311.154022, mae: 1.938735, mean_q: 6.437620
 52283/100000: episode: 1013, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.946, mean reward: 2.009 [1.463, 5.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.314, 10.098], loss: 153.491531, mae: 1.502725, mean_q: 6.446465
 52383/100000: episode: 1014, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.609, mean reward: 1.916 [1.450, 6.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.860, 10.098], loss: 457.732178, mae: 2.305408, mean_q: 6.583294
 52483/100000: episode: 1015, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 211.093, mean reward: 2.111 [1.453, 6.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.017, 10.221], loss: 154.504486, mae: 1.350531, mean_q: 6.102552
 52583/100000: episode: 1016, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 186.742, mean reward: 1.867 [1.490, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.042, 10.098], loss: 457.685028, mae: 2.384872, mean_q: 6.576527
 52683/100000: episode: 1017, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.961, mean reward: 1.920 [1.483, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.511, 10.213], loss: 599.750549, mae: 2.821658, mean_q: 6.601818
 52783/100000: episode: 1018, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 180.869, mean reward: 1.809 [1.440, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.490, 10.159], loss: 152.937592, mae: 1.379893, mean_q: 6.043867
 52883/100000: episode: 1019, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.445, mean reward: 1.904 [1.466, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.511, 10.110], loss: 304.781036, mae: 1.893649, mean_q: 5.977157
 52983/100000: episode: 1020, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 180.977, mean reward: 1.810 [1.452, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.142, 10.098], loss: 601.650330, mae: 2.862033, mean_q: 6.681095
 53083/100000: episode: 1021, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.230, mean reward: 1.992 [1.499, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.224, 10.098], loss: 442.518463, mae: 2.316341, mean_q: 6.210778
 53183/100000: episode: 1022, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.854, mean reward: 1.909 [1.464, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.673, 10.098], loss: 4.967036, mae: 0.802893, mean_q: 5.024936
 53283/100000: episode: 1023, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 232.510, mean reward: 2.325 [1.461, 13.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.098], loss: 4.383200, mae: 0.627030, mean_q: 4.589100
 53383/100000: episode: 1024, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.067, mean reward: 1.871 [1.491, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.098], loss: 445.102356, mae: 2.048537, mean_q: 5.249414
 53483/100000: episode: 1025, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.410, mean reward: 1.884 [1.495, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.098], loss: 294.524597, mae: 1.863570, mean_q: 5.248101
 53583/100000: episode: 1026, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.590, mean reward: 1.936 [1.478, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.054, 10.309], loss: 6.311067, mae: 0.653269, mean_q: 4.349054
 53683/100000: episode: 1027, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 178.395, mean reward: 1.784 [1.468, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.098], loss: 0.399341, mae: 0.412685, mean_q: 4.077477
 53783/100000: episode: 1028, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 192.618, mean reward: 1.926 [1.440, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.467, 10.121], loss: 0.233054, mae: 0.358103, mean_q: 3.901043
 53883/100000: episode: 1029, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.302, mean reward: 1.813 [1.480, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.148], loss: 0.273974, mae: 0.377348, mean_q: 3.907025
 53983/100000: episode: 1030, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 183.539, mean reward: 1.835 [1.449, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.302], loss: 0.104307, mae: 0.308967, mean_q: 3.780643
 54083/100000: episode: 1031, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 197.194, mean reward: 1.972 [1.490, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.942, 10.098], loss: 0.154956, mae: 0.319433, mean_q: 3.787300
 54183/100000: episode: 1032, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.069, mean reward: 1.971 [1.483, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.098], loss: 0.133586, mae: 0.318034, mean_q: 3.778482
 54283/100000: episode: 1033, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 252.799, mean reward: 2.528 [1.481, 4.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.066, 10.098], loss: 0.150358, mae: 0.324055, mean_q: 3.788445
 54383/100000: episode: 1034, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 184.186, mean reward: 1.842 [1.457, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.255, 10.098], loss: 0.185306, mae: 0.342815, mean_q: 3.819066
 54483/100000: episode: 1035, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 220.819, mean reward: 2.208 [1.571, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.270, 10.231], loss: 0.114367, mae: 0.320174, mean_q: 3.806430
 54583/100000: episode: 1036, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.489, mean reward: 1.995 [1.464, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.459, 10.098], loss: 0.130828, mae: 0.332917, mean_q: 3.826446
 54683/100000: episode: 1037, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 187.818, mean reward: 1.878 [1.447, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.098], loss: 0.132838, mae: 0.329401, mean_q: 3.838282
 54783/100000: episode: 1038, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.171, mean reward: 1.802 [1.444, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.442, 10.098], loss: 0.163000, mae: 0.326884, mean_q: 3.831801
 54883/100000: episode: 1039, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.346, mean reward: 1.973 [1.484, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.787, 10.098], loss: 0.133729, mae: 0.320777, mean_q: 3.816172
 54983/100000: episode: 1040, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 212.038, mean reward: 2.120 [1.471, 5.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.351, 10.357], loss: 0.121380, mae: 0.316022, mean_q: 3.803341
 55083/100000: episode: 1041, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.630, mean reward: 1.876 [1.446, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.903, 10.098], loss: 0.103297, mae: 0.310753, mean_q: 3.810099
 55183/100000: episode: 1042, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 211.255, mean reward: 2.113 [1.453, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.386, 10.342], loss: 0.139898, mae: 0.328726, mean_q: 3.831760
 55283/100000: episode: 1043, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.921, mean reward: 1.899 [1.446, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.876, 10.282], loss: 0.123037, mae: 0.331961, mean_q: 3.841605
 55383/100000: episode: 1044, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.084, mean reward: 1.821 [1.456, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.649, 10.197], loss: 0.115744, mae: 0.328562, mean_q: 3.850553
 55483/100000: episode: 1045, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 202.863, mean reward: 2.029 [1.483, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.051, 10.098], loss: 0.124139, mae: 0.328594, mean_q: 3.836169
 55583/100000: episode: 1046, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 205.747, mean reward: 2.057 [1.474, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.951, 10.098], loss: 0.134108, mae: 0.318323, mean_q: 3.833596
 55683/100000: episode: 1047, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.004, mean reward: 1.960 [1.477, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.702, 10.098], loss: 0.100281, mae: 0.310142, mean_q: 3.825045
 55783/100000: episode: 1048, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.408, mean reward: 1.894 [1.463, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.098], loss: 0.114444, mae: 0.320972, mean_q: 3.838137
 55883/100000: episode: 1049, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.163, mean reward: 1.882 [1.444, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.989, 10.278], loss: 0.129072, mae: 0.320995, mean_q: 3.839012
 55983/100000: episode: 1050, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.938, mean reward: 1.819 [1.459, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.035, 10.098], loss: 0.125545, mae: 0.315234, mean_q: 3.836599
 56083/100000: episode: 1051, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 237.315, mean reward: 2.373 [1.467, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.430, 10.098], loss: 0.139331, mae: 0.332651, mean_q: 3.848059
 56183/100000: episode: 1052, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 231.822, mean reward: 2.318 [1.463, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.307, 10.098], loss: 0.119133, mae: 0.321217, mean_q: 3.857970
 56283/100000: episode: 1053, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 182.090, mean reward: 1.821 [1.439, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.098], loss: 0.119005, mae: 0.330802, mean_q: 3.877838
 56383/100000: episode: 1054, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.116, mean reward: 1.941 [1.469, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.602, 10.115], loss: 0.143274, mae: 0.339880, mean_q: 3.899069
 56483/100000: episode: 1055, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.706, mean reward: 1.927 [1.459, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.368, 10.136], loss: 0.140410, mae: 0.323607, mean_q: 3.878140
 56583/100000: episode: 1056, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 213.537, mean reward: 2.135 [1.483, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.120, 10.098], loss: 0.154993, mae: 0.341854, mean_q: 3.888891
 56683/100000: episode: 1057, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.612, mean reward: 1.976 [1.453, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.107, 10.098], loss: 0.148107, mae: 0.346733, mean_q: 3.904391
 56783/100000: episode: 1058, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 188.181, mean reward: 1.882 [1.445, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.307, 10.098], loss: 0.136919, mae: 0.332282, mean_q: 3.884030
 56883/100000: episode: 1059, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.248, mean reward: 1.912 [1.459, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.736, 10.166], loss: 0.139120, mae: 0.327677, mean_q: 3.883706
 56983/100000: episode: 1060, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.120, mean reward: 1.811 [1.478, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.303, 10.098], loss: 0.113321, mae: 0.322985, mean_q: 3.864049
 57083/100000: episode: 1061, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.247, mean reward: 1.882 [1.445, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.423, 10.098], loss: 0.112574, mae: 0.325889, mean_q: 3.875056
 57183/100000: episode: 1062, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 198.637, mean reward: 1.986 [1.484, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.119, 10.291], loss: 0.166233, mae: 0.337126, mean_q: 3.890480
 57283/100000: episode: 1063, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 220.803, mean reward: 2.208 [1.460, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.287, 10.233], loss: 0.148394, mae: 0.324639, mean_q: 3.878114
 57383/100000: episode: 1064, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 204.553, mean reward: 2.046 [1.548, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.331, 10.100], loss: 0.110016, mae: 0.316067, mean_q: 3.899261
 57483/100000: episode: 1065, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 216.940, mean reward: 2.169 [1.552, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.842, 10.098], loss: 0.141206, mae: 0.320159, mean_q: 3.905098
 57583/100000: episode: 1066, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.005, mean reward: 2.030 [1.460, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.852, 10.098], loss: 0.126255, mae: 0.319360, mean_q: 3.912491
 57683/100000: episode: 1067, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.098, mean reward: 1.961 [1.483, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.496, 10.235], loss: 0.125793, mae: 0.324659, mean_q: 3.899651
 57783/100000: episode: 1068, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 235.174, mean reward: 2.352 [1.481, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.155, 10.388], loss: 0.125535, mae: 0.327122, mean_q: 3.937472
 57883/100000: episode: 1069, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.324, mean reward: 1.933 [1.452, 5.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.416, 10.310], loss: 0.122462, mae: 0.329508, mean_q: 3.921482
 57983/100000: episode: 1070, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.361, mean reward: 2.084 [1.524, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.736, 10.215], loss: 0.114688, mae: 0.334537, mean_q: 3.930056
 58083/100000: episode: 1071, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.873, mean reward: 1.819 [1.433, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.378, 10.098], loss: 0.153122, mae: 0.339430, mean_q: 3.946954
 58183/100000: episode: 1072, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.466, mean reward: 1.825 [1.446, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.098], loss: 0.108868, mae: 0.318951, mean_q: 3.923544
 58283/100000: episode: 1073, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 182.702, mean reward: 1.827 [1.436, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.204, 10.182], loss: 0.094648, mae: 0.311739, mean_q: 3.925210
 58383/100000: episode: 1074, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 226.302, mean reward: 2.263 [1.472, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.864, 10.292], loss: 0.095026, mae: 0.307462, mean_q: 3.916545
 58483/100000: episode: 1075, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 174.342, mean reward: 1.743 [1.453, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.939, 10.158], loss: 0.096075, mae: 0.305579, mean_q: 3.896468
 58583/100000: episode: 1076, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 186.547, mean reward: 1.865 [1.435, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.098], loss: 0.108928, mae: 0.318077, mean_q: 3.905928
 58683/100000: episode: 1077, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 216.682, mean reward: 2.167 [1.509, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.533, 10.354], loss: 0.106036, mae: 0.319893, mean_q: 3.933340
 58783/100000: episode: 1078, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 190.074, mean reward: 1.901 [1.443, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.530, 10.276], loss: 0.108936, mae: 0.318082, mean_q: 3.911932
 58883/100000: episode: 1079, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.612, mean reward: 1.856 [1.441, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.090, 10.098], loss: 0.106089, mae: 0.322466, mean_q: 3.945383
[Info] 1-TH LEVEL FOUND: 5.5035624504089355, Considering 10/90 traces
 58983/100000: episode: 1080, duration: 4.624s, episode steps: 100, steps per second: 22, episode reward: 203.734, mean reward: 2.037 [1.465, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.617, 10.450], loss: 0.106887, mae: 0.320974, mean_q: 3.941689
 59019/100000: episode: 1081, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 108.829, mean reward: 3.023 [2.291, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.720, 10.507], loss: 0.103679, mae: 0.328660, mean_q: 3.998494
 59040/100000: episode: 1082, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 41.102, mean reward: 1.957 [1.695, 2.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.121, 10.100], loss: 0.107487, mae: 0.326555, mean_q: 3.971201
 59084/100000: episode: 1083, duration: 0.232s, episode steps: 44, steps per second: 190, episode reward: 101.072, mean reward: 2.297 [1.564, 5.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.361, 10.172], loss: 0.101624, mae: 0.321207, mean_q: 3.955074
 59107/100000: episode: 1084, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 60.803, mean reward: 2.644 [1.999, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.685, 10.100], loss: 0.090321, mae: 0.289255, mean_q: 3.965614
 59151/100000: episode: 1085, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 120.293, mean reward: 2.734 [1.686, 5.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.779, 10.293], loss: 0.100094, mae: 0.323437, mean_q: 3.993284
 59172/100000: episode: 1086, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 45.590, mean reward: 2.171 [1.472, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.278], loss: 0.115223, mae: 0.329568, mean_q: 3.992601
 59190/100000: episode: 1087, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 62.217, mean reward: 3.456 [3.066, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.248, 10.100], loss: 0.101239, mae: 0.321872, mean_q: 3.987718
 59213/100000: episode: 1088, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 54.196, mean reward: 2.356 [1.782, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.615, 10.157], loss: 0.106676, mae: 0.321908, mean_q: 3.995682
 59231/100000: episode: 1089, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 59.395, mean reward: 3.300 [2.560, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.267, 10.100], loss: 0.091302, mae: 0.306073, mean_q: 3.983876
 59246/100000: episode: 1090, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 44.445, mean reward: 2.963 [2.279, 3.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.443], loss: 0.105532, mae: 0.329926, mean_q: 3.983674
 59272/100000: episode: 1091, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 91.211, mean reward: 3.508 [2.406, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.927, 10.100], loss: 0.122043, mae: 0.335325, mean_q: 4.034922
 59310/100000: episode: 1092, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 89.071, mean reward: 2.344 [1.591, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.068, 10.238], loss: 0.094832, mae: 0.310497, mean_q: 4.023643
 59348/100000: episode: 1093, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 104.507, mean reward: 2.750 [1.518, 5.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.095, 10.100], loss: 0.106086, mae: 0.325537, mean_q: 4.044650
 59373/100000: episode: 1094, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 57.518, mean reward: 2.301 [1.640, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.296], loss: 0.097341, mae: 0.310091, mean_q: 4.016130
 59411/100000: episode: 1095, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 107.923, mean reward: 2.840 [1.932, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.350, 10.454], loss: 0.105954, mae: 0.326628, mean_q: 4.029957
 59432/100000: episode: 1096, duration: 0.123s, episode steps: 21, steps per second: 170, episode reward: 49.771, mean reward: 2.370 [1.828, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.282, 10.100], loss: 0.122181, mae: 0.330328, mean_q: 4.001734
 59453/100000: episode: 1097, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 55.315, mean reward: 2.634 [1.915, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.836, 10.100], loss: 0.108624, mae: 0.314658, mean_q: 4.039419
 59478/100000: episode: 1098, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 61.476, mean reward: 2.459 [2.012, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.641, 10.319], loss: 0.127105, mae: 0.344036, mean_q: 4.095351
 59501/100000: episode: 1099, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 61.353, mean reward: 2.668 [1.854, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.976, 10.100], loss: 0.108325, mae: 0.332042, mean_q: 4.057769
 59522/100000: episode: 1100, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 44.618, mean reward: 2.125 [1.770, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.262, 10.100], loss: 0.129472, mae: 0.353247, mean_q: 4.040376
 59566/100000: episode: 1101, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 132.448, mean reward: 3.010 [1.869, 9.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.790, 10.316], loss: 0.099981, mae: 0.326295, mean_q: 4.040637
 59604/100000: episode: 1102, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 89.534, mean reward: 2.356 [1.843, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.583, 10.479], loss: 0.129264, mae: 0.352294, mean_q: 4.054610
 59640/100000: episode: 1103, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 97.351, mean reward: 2.704 [1.893, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.516], loss: 0.099079, mae: 0.324689, mean_q: 4.065357
 59678/100000: episode: 1104, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 97.789, mean reward: 2.573 [1.698, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.611, 10.278], loss: 0.110643, mae: 0.321833, mean_q: 4.080778
 59701/100000: episode: 1105, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 68.328, mean reward: 2.971 [2.088, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.114, 10.100], loss: 0.113508, mae: 0.314149, mean_q: 4.070296
 59716/100000: episode: 1106, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 37.485, mean reward: 2.499 [1.942, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.412], loss: 0.126700, mae: 0.364779, mean_q: 4.195953
 59731/100000: episode: 1107, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 37.772, mean reward: 2.518 [2.003, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.421, 10.380], loss: 0.116805, mae: 0.336523, mean_q: 4.114573
 59754/100000: episode: 1108, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 60.496, mean reward: 2.630 [1.917, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.796, 10.100], loss: 0.133450, mae: 0.360760, mean_q: 4.160390
 59775/100000: episode: 1109, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 80.749, mean reward: 3.845 [2.024, 7.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.540, 10.100], loss: 0.112778, mae: 0.337295, mean_q: 4.154641
 59796/100000: episode: 1110, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 52.308, mean reward: 2.491 [1.785, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.044, 10.277], loss: 0.154159, mae: 0.352028, mean_q: 4.097964
 59811/100000: episode: 1111, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 36.779, mean reward: 2.452 [1.993, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.329], loss: 0.093798, mae: 0.312333, mean_q: 4.073056
 59849/100000: episode: 1112, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 99.374, mean reward: 2.615 [1.876, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.631, 10.324], loss: 0.116063, mae: 0.329839, mean_q: 4.144876
 59893/100000: episode: 1113, duration: 0.232s, episode steps: 44, steps per second: 190, episode reward: 133.661, mean reward: 3.038 [1.912, 7.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.460, 10.556], loss: 0.169203, mae: 0.380024, mean_q: 4.167121
 59937/100000: episode: 1114, duration: 0.218s, episode steps: 44, steps per second: 201, episode reward: 115.382, mean reward: 2.622 [1.722, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.482, 10.136], loss: 0.128094, mae: 0.351057, mean_q: 4.195172
 59963/100000: episode: 1115, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 90.524, mean reward: 3.482 [2.538, 6.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.771, 10.100], loss: 0.112800, mae: 0.341225, mean_q: 4.158078
 59986/100000: episode: 1116, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 67.376, mean reward: 2.929 [1.938, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.299, 10.100], loss: 0.151222, mae: 0.354088, mean_q: 4.192016
 60001/100000: episode: 1117, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 38.476, mean reward: 2.565 [1.910, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.466, 10.336], loss: 0.095628, mae: 0.309981, mean_q: 4.120395
 60026/100000: episode: 1118, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 69.362, mean reward: 2.774 [2.343, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.876, 10.427], loss: 0.091727, mae: 0.312054, mean_q: 4.156566
 60051/100000: episode: 1119, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 69.734, mean reward: 2.789 [1.827, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.311, 10.302], loss: 0.117533, mae: 0.342482, mean_q: 4.213431
 60095/100000: episode: 1120, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 117.210, mean reward: 2.664 [1.533, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.241, 10.258], loss: 0.126902, mae: 0.361553, mean_q: 4.245790
 60139/100000: episode: 1121, duration: 0.247s, episode steps: 44, steps per second: 178, episode reward: 91.835, mean reward: 2.087 [1.499, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.234, 10.153], loss: 0.138174, mae: 0.351018, mean_q: 4.204794
 60157/100000: episode: 1122, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 51.060, mean reward: 2.837 [1.951, 7.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.056, 10.100], loss: 0.140941, mae: 0.363071, mean_q: 4.272199
 60180/100000: episode: 1123, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 58.121, mean reward: 2.527 [1.752, 7.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.532, 10.100], loss: 0.119645, mae: 0.353938, mean_q: 4.226210
 60218/100000: episode: 1124, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 102.458, mean reward: 2.696 [1.858, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.674, 10.415], loss: 0.140266, mae: 0.348945, mean_q: 4.271334
 60243/100000: episode: 1125, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 84.419, mean reward: 3.377 [2.037, 6.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.685, 10.303], loss: 0.121912, mae: 0.351961, mean_q: 4.261370
 60269/100000: episode: 1126, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 65.324, mean reward: 2.512 [1.799, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.013, 10.100], loss: 0.128707, mae: 0.349021, mean_q: 4.228940
 60292/100000: episode: 1127, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 60.120, mean reward: 2.614 [2.124, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.340, 10.100], loss: 0.112763, mae: 0.336870, mean_q: 4.225948
 60313/100000: episode: 1128, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 46.698, mean reward: 2.224 [1.837, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.295, 10.276], loss: 0.137382, mae: 0.350168, mean_q: 4.248938
 60334/100000: episode: 1129, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 55.619, mean reward: 2.649 [2.017, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.801, 10.100], loss: 0.104016, mae: 0.330566, mean_q: 4.264237
 60370/100000: episode: 1130, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 78.729, mean reward: 2.187 [1.456, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.892, 10.100], loss: 0.124538, mae: 0.356842, mean_q: 4.256399
 60391/100000: episode: 1131, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 52.323, mean reward: 2.492 [1.995, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.091, 10.302], loss: 0.142385, mae: 0.364747, mean_q: 4.294160
 60414/100000: episode: 1132, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 59.943, mean reward: 2.606 [1.992, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.384, 10.100], loss: 0.145308, mae: 0.360045, mean_q: 4.312579
 60439/100000: episode: 1133, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 96.580, mean reward: 3.863 [2.455, 7.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.505, 10.560], loss: 0.129306, mae: 0.363690, mean_q: 4.342438
 60465/100000: episode: 1134, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 62.234, mean reward: 2.394 [1.840, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.349, 10.100], loss: 0.124276, mae: 0.343643, mean_q: 4.298518
 60486/100000: episode: 1135, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 47.673, mean reward: 2.270 [1.818, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.239, 10.100], loss: 0.139308, mae: 0.356490, mean_q: 4.381076
 60504/100000: episode: 1136, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 63.409, mean reward: 3.523 [1.905, 15.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.139, 10.100], loss: 0.127788, mae: 0.350726, mean_q: 4.281133
 60527/100000: episode: 1137, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 56.280, mean reward: 2.447 [1.841, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.068, 10.100], loss: 0.119273, mae: 0.340325, mean_q: 4.302054
 60548/100000: episode: 1138, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 53.762, mean reward: 2.560 [1.965, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.491, 10.403], loss: 0.121263, mae: 0.348659, mean_q: 4.352408
 60573/100000: episode: 1139, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 59.571, mean reward: 2.383 [1.501, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.177, 10.187], loss: 0.129527, mae: 0.360310, mean_q: 4.365711
 60609/100000: episode: 1140, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 117.884, mean reward: 3.275 [2.097, 5.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.417, 10.594], loss: 0.132225, mae: 0.353765, mean_q: 4.271288
 60653/100000: episode: 1141, duration: 0.238s, episode steps: 44, steps per second: 185, episode reward: 93.457, mean reward: 2.124 [1.598, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.234, 10.203], loss: 0.204107, mae: 0.370070, mean_q: 4.319403
 60679/100000: episode: 1142, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 73.467, mean reward: 2.826 [2.098, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.215, 10.100], loss: 0.208383, mae: 0.383462, mean_q: 4.386014
 60700/100000: episode: 1143, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 63.056, mean reward: 3.003 [2.445, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.541], loss: 0.154121, mae: 0.363344, mean_q: 4.316312
 60744/100000: episode: 1144, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 131.804, mean reward: 2.996 [1.497, 5.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.544, 10.161], loss: 0.205419, mae: 0.390800, mean_q: 4.362159
 60788/100000: episode: 1145, duration: 0.244s, episode steps: 44, steps per second: 180, episode reward: 178.856, mean reward: 4.065 [2.101, 6.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.569, 10.374], loss: 0.135993, mae: 0.366694, mean_q: 4.376432
 60803/100000: episode: 1146, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 43.070, mean reward: 2.871 [2.113, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.043, 10.391], loss: 0.168407, mae: 0.392580, mean_q: 4.486295
 60847/100000: episode: 1147, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 106.534, mean reward: 2.421 [1.515, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.265, 10.127], loss: 0.152194, mae: 0.369526, mean_q: 4.441740
 60873/100000: episode: 1148, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 65.337, mean reward: 2.513 [1.938, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.218, 10.100], loss: 0.148014, mae: 0.369852, mean_q: 4.359214
 60899/100000: episode: 1149, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 74.429, mean reward: 2.863 [1.521, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.572, 10.100], loss: 0.154749, mae: 0.377065, mean_q: 4.420789
 60920/100000: episode: 1150, duration: 0.125s, episode steps: 21, steps per second: 167, episode reward: 47.707, mean reward: 2.272 [1.874, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.449, 10.335], loss: 0.145925, mae: 0.366771, mean_q: 4.436236
 60941/100000: episode: 1151, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 53.788, mean reward: 2.561 [2.087, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.396, 10.100], loss: 0.193275, mae: 0.410753, mean_q: 4.465054
 60959/100000: episode: 1152, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 54.617, mean reward: 3.034 [1.908, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.629, 10.100], loss: 0.180445, mae: 0.376493, mean_q: 4.512663
 60985/100000: episode: 1153, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 73.597, mean reward: 2.831 [2.014, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.471, 10.100], loss: 0.119824, mae: 0.354825, mean_q: 4.341169
 61006/100000: episode: 1154, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 52.153, mean reward: 2.483 [1.742, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.064, 10.252], loss: 0.181745, mae: 0.395390, mean_q: 4.490715
 61050/100000: episode: 1155, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 196.074, mean reward: 4.456 [1.878, 13.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.241, 10.562], loss: 0.160669, mae: 0.386940, mean_q: 4.486523
 61086/100000: episode: 1156, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 183.389, mean reward: 5.094 [2.698, 9.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.941, 10.680], loss: 0.270962, mae: 0.431507, mean_q: 4.437436
 61101/100000: episode: 1157, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 37.542, mean reward: 2.503 [2.135, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.644, 10.342], loss: 0.193859, mae: 0.402127, mean_q: 4.475443
 61127/100000: episode: 1158, duration: 0.154s, episode steps: 26, steps per second: 168, episode reward: 56.529, mean reward: 2.174 [1.463, 3.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.898, 10.105], loss: 0.302528, mae: 0.448677, mean_q: 4.590827
 61165/100000: episode: 1159, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 89.686, mean reward: 2.360 [1.727, 7.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.869, 10.242], loss: 0.150684, mae: 0.380863, mean_q: 4.514416
 61188/100000: episode: 1160, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 61.377, mean reward: 2.669 [2.127, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.366, 10.100], loss: 0.340696, mae: 0.432196, mean_q: 4.582070
 61203/100000: episode: 1161, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 36.519, mean reward: 2.435 [1.914, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.092, 10.323], loss: 0.251998, mae: 0.453200, mean_q: 4.736694
 61228/100000: episode: 1162, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 71.714, mean reward: 2.869 [1.674, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.953, 10.308], loss: 0.359260, mae: 0.428274, mean_q: 4.498304
 61249/100000: episode: 1163, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 57.489, mean reward: 2.738 [2.063, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.203, 10.100], loss: 0.348103, mae: 0.454685, mean_q: 4.493592
 61264/100000: episode: 1164, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 37.139, mean reward: 2.476 [1.853, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.384], loss: 0.250289, mae: 0.397229, mean_q: 4.473023
 61285/100000: episode: 1165, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 48.614, mean reward: 2.315 [1.760, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.122, 10.100], loss: 0.447559, mae: 0.498143, mean_q: 4.619714
 61300/100000: episode: 1166, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 44.697, mean reward: 2.980 [2.486, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.381, 10.524], loss: 0.257546, mae: 0.437283, mean_q: 4.584504
 61321/100000: episode: 1167, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 49.081, mean reward: 2.337 [1.901, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.195, 10.100], loss: 0.218929, mae: 0.427764, mean_q: 4.616441
 61365/100000: episode: 1168, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 122.287, mean reward: 2.779 [1.829, 4.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.493, 10.258], loss: 0.204617, mae: 0.418098, mean_q: 4.593475
 61391/100000: episode: 1169, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 102.260, mean reward: 3.933 [2.790, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.539, 10.100], loss: 0.278806, mae: 0.440231, mean_q: 4.631108
[Info] 2-TH LEVEL FOUND: 7.612504005432129, Considering 10/90 traces
 61435/100000: episode: 1170, duration: 4.338s, episode steps: 44, steps per second: 10, episode reward: 112.696, mean reward: 2.561 [1.951, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.444, 10.386], loss: 0.224213, mae: 0.423502, mean_q: 4.604794
 61472/100000: episode: 1171, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 118.856, mean reward: 3.212 [1.695, 5.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.456, 10.281], loss: 0.176309, mae: 0.404643, mean_q: 4.593591
 61484/100000: episode: 1172, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 47.506, mean reward: 3.959 [2.475, 4.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.485, 10.100], loss: 0.153524, mae: 0.390222, mean_q: 4.636860
 61504/100000: episode: 1173, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 66.370, mean reward: 3.318 [2.649, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.947, 10.508], loss: 0.199958, mae: 0.420471, mean_q: 4.599525
 61524/100000: episode: 1174, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 64.850, mean reward: 3.242 [2.321, 6.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.630], loss: 0.179215, mae: 0.403522, mean_q: 4.619042
 61544/100000: episode: 1175, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 64.242, mean reward: 3.212 [2.479, 4.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.641, 10.450], loss: 0.363035, mae: 0.486671, mean_q: 4.780462
 61582/100000: episode: 1176, duration: 0.213s, episode steps: 38, steps per second: 179, episode reward: 127.889, mean reward: 3.366 [1.811, 6.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.264, 10.314], loss: 0.243973, mae: 0.446729, mean_q: 4.691423
 61620/100000: episode: 1177, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 109.920, mean reward: 2.893 [1.638, 7.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.160, 10.447], loss: 0.241424, mae: 0.443630, mean_q: 4.762084
 61658/100000: episode: 1178, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 97.503, mean reward: 2.566 [1.836, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.702, 10.294], loss: 0.251892, mae: 0.457714, mean_q: 4.751291
 61695/100000: episode: 1179, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 102.275, mean reward: 2.764 [1.845, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.346, 10.370], loss: 0.297416, mae: 0.445297, mean_q: 4.726033
 61734/100000: episode: 1180, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 155.690, mean reward: 3.992 [2.070, 15.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.414, 10.500], loss: 0.215944, mae: 0.438734, mean_q: 4.769617
 61773/100000: episode: 1181, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 431.651, mean reward: 11.068 [2.395, 100.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.201, 10.638], loss: 0.323580, mae: 0.469769, mean_q: 4.695204
 61780/100000: episode: 1182, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 29.465, mean reward: 4.209 [3.858, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.463, 10.100], loss: 0.490896, mae: 0.475247, mean_q: 4.832471
 61817/100000: episode: 1183, duration: 0.203s, episode steps: 37, steps per second: 183, episode reward: 118.129, mean reward: 3.193 [2.015, 4.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.047, 10.523], loss: 0.591540, mae: 0.484141, mean_q: 4.813729
 61856/100000: episode: 1184, duration: 0.193s, episode steps: 39, steps per second: 202, episode reward: 104.384, mean reward: 2.677 [1.667, 5.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.210, 10.246], loss: 7.638704, mae: 0.717098, mean_q: 5.040593
 61868/100000: episode: 1185, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 37.051, mean reward: 3.088 [2.497, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.565, 10.100], loss: 0.440041, mae: 0.567309, mean_q: 4.770519
 61878/100000: episode: 1186, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 41.601, mean reward: 4.160 [3.297, 5.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.973, 10.100], loss: 0.323552, mae: 0.560545, mean_q: 4.869126
 61915/100000: episode: 1187, duration: 0.204s, episode steps: 37, steps per second: 182, episode reward: 86.106, mean reward: 2.327 [1.504, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.435, 10.185], loss: 5.779376, mae: 0.872632, mean_q: 5.101857
 61953/100000: episode: 1188, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 99.701, mean reward: 2.624 [2.012, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.501, 10.351], loss: 0.358008, mae: 0.515260, mean_q: 4.931476
 61973/100000: episode: 1189, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 63.976, mean reward: 3.199 [2.654, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.446, 10.401], loss: 3.164585, mae: 0.622556, mean_q: 5.008204
 61995/100000: episode: 1190, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 64.361, mean reward: 2.925 [2.272, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.684, 10.416], loss: 0.447119, mae: 0.630844, mean_q: 5.060936
 62002/100000: episode: 1191, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 26.539, mean reward: 3.791 [3.398, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.479, 10.100], loss: 0.322583, mae: 0.549379, mean_q: 4.738679
 62014/100000: episode: 1192, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 41.001, mean reward: 3.417 [2.646, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.344, 10.100], loss: 1.420609, mae: 0.710040, mean_q: 5.296052
 62021/100000: episode: 1193, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 36.843, mean reward: 5.263 [3.851, 6.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.413, 10.100], loss: 0.237920, mae: 0.481047, mean_q: 4.841353
[Info] FALSIFICATION!
 62037/100000: episode: 1194, duration: 0.349s, episode steps: 16, steps per second: 46, episode reward: 1072.710, mean reward: 67.044 [2.497, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.014, 10.729], loss: 0.222962, mae: 0.438773, mean_q: 4.858288
[Info] FALSIFICATION!
 62073/100000: episode: 1195, duration: 0.365s, episode steps: 36, steps per second: 99, episode reward: 1153.554, mean reward: 32.043 [2.992, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.242, 10.396], loss: 4.586390, mae: 0.808192, mean_q: 5.091505
 62110/100000: episode: 1196, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 100.022, mean reward: 2.703 [1.965, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.311, 10.455], loss: 1.021114, mae: 0.652749, mean_q: 5.122391
 62142/100000: episode: 1197, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 127.581, mean reward: 3.987 [2.356, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.487], loss: 965.606262, mae: 4.230705, mean_q: 7.170668
 62162/100000: episode: 1198, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 93.996, mean reward: 4.700 [3.177, 8.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.613, 10.523], loss: 4.688213, mae: 1.258041, mean_q: 6.124180
 62201/100000: episode: 1199, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 122.909, mean reward: 3.152 [2.491, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.283, 10.507], loss: 396.265961, mae: 2.037751, mean_q: 6.435428
 62240/100000: episode: 1200, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 195.775, mean reward: 5.020 [2.443, 10.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.335, 10.465], loss: 785.501953, mae: 2.459270, mean_q: 6.021183
 62278/100000: episode: 1201, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 116.253, mean reward: 3.059 [2.232, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.092, 10.408], loss: 406.282745, mae: 3.215628, mean_q: 8.022259
 62288/100000: episode: 1202, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 42.230, mean reward: 4.223 [3.094, 5.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.469, 10.100], loss: 1.454671, mae: 1.157006, mean_q: 6.686682
 62320/100000: episode: 1203, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 112.946, mean reward: 3.530 [2.506, 5.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.035, 10.455], loss: 479.414795, mae: 2.191554, mean_q: 6.557918
 62327/100000: episode: 1204, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 23.504, mean reward: 3.358 [2.805, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.388, 10.100], loss: 1.340747, mae: 1.173356, mean_q: 6.710830
 62359/100000: episode: 1205, duration: 0.181s, episode steps: 32, steps per second: 176, episode reward: 145.417, mean reward: 4.544 [2.145, 9.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.342, 10.377], loss: 962.365051, mae: 3.425212, mean_q: 7.011973
 62366/100000: episode: 1206, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 44.251, mean reward: 6.322 [3.756, 11.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.330, 10.100], loss: 3.112446, mae: 1.825645, mean_q: 7.485620
 62388/100000: episode: 1207, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 64.854, mean reward: 2.948 [2.317, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.504], loss: 7.991594, mae: 1.511265, mean_q: 6.992117
 62408/100000: episode: 1208, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 54.624, mean reward: 2.731 [2.053, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.674, 10.368], loss: 1.296169, mae: 0.938921, mean_q: 6.113561
 62446/100000: episode: 1209, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 178.748, mean reward: 4.704 [2.833, 14.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-1.618, 10.572], loss: 2.419340, mae: 0.887851, mean_q: 5.940126
 62466/100000: episode: 1210, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 119.009, mean reward: 5.950 [2.971, 13.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.251, 10.428], loss: 0.538853, mae: 0.713964, mean_q: 6.003324
 62488/100000: episode: 1211, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 78.778, mean reward: 3.581 [2.300, 5.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.544, 10.372], loss: 3.495222, mae: 0.843271, mean_q: 5.892797
 62495/100000: episode: 1212, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 52.850, mean reward: 7.550 [4.258, 12.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.435, 10.100], loss: 0.575486, mae: 0.681844, mean_q: 6.024665
 62502/100000: episode: 1213, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 31.840, mean reward: 4.549 [3.754, 5.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.290, 10.100], loss: 2183.496826, mae: 5.673518, mean_q: 6.513538
 62512/100000: episode: 1214, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 44.895, mean reward: 4.489 [3.313, 9.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.686, 10.100], loss: 1532.866943, mae: 5.513450, mean_q: 7.793439
 62544/100000: episode: 1215, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 125.694, mean reward: 3.928 [2.791, 6.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.488], loss: 3.314625, mae: 1.813302, mean_q: 7.318942
 62560/100000: episode: 1216, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 91.662, mean reward: 5.729 [3.997, 9.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.123, 10.549], loss: 959.608887, mae: 3.131895, mean_q: 6.523695
 62576/100000: episode: 1217, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 55.202, mean reward: 3.450 [2.629, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.451], loss: 1.922248, mae: 1.365815, mean_q: 6.978216
 62592/100000: episode: 1218, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 72.688, mean reward: 4.543 [3.082, 6.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.681, 10.589], loss: 0.930663, mae: 0.918122, mean_q: 6.076618
 62624/100000: episode: 1219, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 110.924, mean reward: 3.466 [1.914, 6.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.159, 10.322], loss: 955.848450, mae: 3.315759, mean_q: 6.722315
 62646/100000: episode: 1220, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 70.476, mean reward: 3.203 [2.501, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.104, 10.412], loss: 3.743946, mae: 2.072027, mean_q: 7.716209
 62684/100000: episode: 1221, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 96.928, mean reward: 2.551 [1.696, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.946, 10.209], loss: 2.563412, mae: 0.954759, mean_q: 6.394232
 62704/100000: episode: 1222, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 65.995, mean reward: 3.300 [2.427, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.682, 10.428], loss: 0.811459, mae: 0.765994, mean_q: 6.191786
 62736/100000: episode: 1223, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 148.627, mean reward: 4.645 [3.102, 7.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.892, 10.469], loss: 0.694673, mae: 0.702121, mean_q: 6.181929
 62746/100000: episode: 1224, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 44.395, mean reward: 4.440 [3.301, 5.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.449, 10.100], loss: 0.659413, mae: 0.704850, mean_q: 5.961342
 62756/100000: episode: 1225, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 43.414, mean reward: 4.341 [3.555, 5.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.532, 10.100], loss: 1.186365, mae: 0.717054, mean_q: 5.869725
 62772/100000: episode: 1226, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 53.178, mean reward: 3.324 [2.473, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.475], loss: 0.423846, mae: 0.648256, mean_q: 5.987509
 62809/100000: episode: 1227, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 112.004, mean reward: 3.027 [1.830, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.101, 10.252], loss: 831.839600, mae: 3.400064, mean_q: 7.014257
 62825/100000: episode: 1228, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 58.430, mean reward: 3.652 [2.767, 8.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.828, 10.474], loss: 2.728433, mae: 1.378278, mean_q: 6.897030
 62862/100000: episode: 1229, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 120.693, mean reward: 3.262 [2.129, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.055, 10.387], loss: 0.955646, mae: 0.888181, mean_q: 6.012400
 62884/100000: episode: 1230, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 120.360, mean reward: 5.471 [2.654, 9.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.646], loss: 0.672531, mae: 0.783435, mean_q: 6.227099
 62894/100000: episode: 1231, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 48.384, mean reward: 4.838 [3.003, 8.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.396, 10.100], loss: 1.476219, mae: 0.789350, mean_q: 6.028459
 62910/100000: episode: 1232, duration: 0.084s, episode steps: 16, steps per second: 192, episode reward: 82.784, mean reward: 5.174 [3.288, 12.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.047, 10.636], loss: 1.230538, mae: 0.776262, mean_q: 6.065605
 62930/100000: episode: 1233, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 71.358, mean reward: 3.568 [2.732, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.456], loss: 0.670715, mae: 0.747585, mean_q: 6.098538
 62962/100000: episode: 1234, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 104.715, mean reward: 3.272 [2.114, 7.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.684, 10.403], loss: 0.911764, mae: 0.747117, mean_q: 6.077918
 62972/100000: episode: 1235, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 45.296, mean reward: 4.530 [3.506, 6.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.502, 10.100], loss: 0.592008, mae: 0.666505, mean_q: 5.832261
 62982/100000: episode: 1236, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 37.808, mean reward: 3.781 [3.381, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.335, 10.100], loss: 1529.301880, mae: 3.950469, mean_q: 6.257220
 62994/100000: episode: 1237, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 70.629, mean reward: 5.886 [2.857, 10.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.675, 10.100], loss: 2.939873, mae: 1.978455, mean_q: 7.248237
 63014/100000: episode: 1238, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 56.167, mean reward: 2.808 [1.649, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.117, 10.341], loss: 1.266002, mae: 1.042216, mean_q: 6.169932
 63026/100000: episode: 1239, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 28.956, mean reward: 2.413 [2.144, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.439, 10.100], loss: 2552.355713, mae: 6.454600, mean_q: 6.834875
 63048/100000: episode: 1240, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 88.772, mean reward: 4.035 [2.550, 9.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.755, 10.480], loss: 8.165211, mae: 2.726555, mean_q: 7.968756
 63055/100000: episode: 1241, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 25.881, mean reward: 3.697 [3.060, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.403, 10.100], loss: 10.280792, mae: 1.516064, mean_q: 6.530670
 63065/100000: episode: 1242, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 38.813, mean reward: 3.881 [3.110, 4.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.372, 10.100], loss: 1.238505, mae: 0.980943, mean_q: 6.049791
 63075/100000: episode: 1243, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 62.795, mean reward: 6.279 [4.568, 8.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.797, 10.100], loss: 0.767670, mae: 0.772322, mean_q: 5.961873
 63085/100000: episode: 1244, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 50.554, mean reward: 5.055 [3.537, 7.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.383, 10.100], loss: 1.124984, mae: 0.862018, mean_q: 6.262499
 63101/100000: episode: 1245, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 41.646, mean reward: 2.603 [2.161, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.421], loss: 0.668286, mae: 0.770091, mean_q: 6.193238
 63121/100000: episode: 1246, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 105.254, mean reward: 5.263 [3.865, 6.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.317, 10.652], loss: 0.592711, mae: 0.677277, mean_q: 6.194287
 63160/100000: episode: 1247, duration: 0.186s, episode steps: 39, steps per second: 210, episode reward: 127.099, mean reward: 3.259 [1.685, 10.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.765, 10.373], loss: 785.890259, mae: 3.089337, mean_q: 7.104491
 63182/100000: episode: 1248, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 94.619, mean reward: 4.301 [2.812, 10.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.687], loss: 3.021187, mae: 1.738048, mean_q: 7.531483
 63202/100000: episode: 1249, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 65.429, mean reward: 3.271 [2.369, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.544, 10.432], loss: 0.853105, mae: 0.880802, mean_q: 6.135366
 63234/100000: episode: 1250, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 140.292, mean reward: 4.384 [2.308, 7.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.362, 10.403], loss: 477.935089, mae: 2.155100, mean_q: 6.852225
 63244/100000: episode: 1251, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 50.646, mean reward: 5.065 [4.086, 7.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.439, 10.100], loss: 2.406787, mae: 1.511840, mean_q: 7.233388
 63254/100000: episode: 1252, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 39.498, mean reward: 3.950 [2.993, 6.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.327, 10.100], loss: 1.290419, mae: 1.067883, mean_q: 6.750321
 63286/100000: episode: 1253, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 102.550, mean reward: 3.205 [2.462, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.315, 10.397], loss: 2.532349, mae: 0.901460, mean_q: 6.526412
 63302/100000: episode: 1254, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 72.492, mean reward: 4.531 [3.310, 5.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.587], loss: 0.676313, mae: 0.711725, mean_q: 6.441053
 63324/100000: episode: 1255, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 67.142, mean reward: 3.052 [2.244, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.397, 10.344], loss: 0.662359, mae: 0.766519, mean_q: 6.341547
 63331/100000: episode: 1256, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 26.139, mean reward: 3.734 [3.276, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.477, 10.100], loss: 1.000940, mae: 0.784953, mean_q: 6.536538
 63369/100000: episode: 1257, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 121.909, mean reward: 3.208 [2.409, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.690, 10.450], loss: 809.757263, mae: 2.929486, mean_q: 6.820441
 63379/100000: episode: 1258, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 47.168, mean reward: 4.717 [3.879, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.464, 10.100], loss: 8.643240, mae: 3.332197, mean_q: 8.781667
 63416/100000: episode: 1259, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 94.116, mean reward: 2.544 [1.853, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.504, 10.315], loss: 1.774549, mae: 1.249554, mean_q: 6.672091
[Info] Complete ISplit Iteration
[Info] Levels: [5.5035625, 7.612504, 10.934727]
[Info] Cond. Prob: [0.1, 0.1, 0.22]
[Info] Error Prob: 0.0022000000000000006

 63454/100000: episode: 1260, duration: 4.416s, episode steps: 38, steps per second: 9, episode reward: 115.586, mean reward: 3.042 [1.888, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.477, 10.350], loss: 2.338121, mae: 0.851535, mean_q: 6.409442
 63554/100000: episode: 1261, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 201.283, mean reward: 2.013 [1.459, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.399, 10.098], loss: 153.806213, mae: 1.318567, mean_q: 6.661503
 63654/100000: episode: 1262, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 181.481, mean reward: 1.815 [1.451, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.132], loss: 153.423950, mae: 1.252734, mean_q: 6.517150
 63754/100000: episode: 1263, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.691, mean reward: 1.807 [1.449, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.060, 10.098], loss: 154.629807, mae: 1.391052, mean_q: 6.541389
 63854/100000: episode: 1264, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 203.257, mean reward: 2.033 [1.489, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.664, 10.098], loss: 309.693939, mae: 2.114774, mean_q: 6.959697
 63954/100000: episode: 1265, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 214.609, mean reward: 2.146 [1.491, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.098], loss: 309.213135, mae: 1.757923, mean_q: 6.670820
 64054/100000: episode: 1266, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.687, mean reward: 1.967 [1.475, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.048, 10.098], loss: 310.444183, mae: 2.250636, mean_q: 7.470942
 64154/100000: episode: 1267, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 207.577, mean reward: 2.076 [1.478, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.320, 10.098], loss: 305.202026, mae: 1.593106, mean_q: 6.677153
 64254/100000: episode: 1268, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.528, mean reward: 1.965 [1.453, 5.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.119, 10.156], loss: 155.437271, mae: 1.631084, mean_q: 6.984365
 64354/100000: episode: 1269, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.995, mean reward: 1.900 [1.492, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.122, 10.098], loss: 152.854965, mae: 1.075079, mean_q: 6.254913
 64454/100000: episode: 1270, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 208.480, mean reward: 2.085 [1.467, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.730, 10.098], loss: 310.138489, mae: 2.269038, mean_q: 7.130842
 64554/100000: episode: 1271, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.057, mean reward: 1.911 [1.469, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.547, 10.140], loss: 2.503638, mae: 0.854544, mean_q: 6.275027
 64654/100000: episode: 1272, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.482, mean reward: 1.905 [1.470, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.013, 10.155], loss: 1.322867, mae: 0.730106, mean_q: 6.053360
 64754/100000: episode: 1273, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.715, mean reward: 1.807 [1.452, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.098], loss: 0.824818, mae: 0.739692, mean_q: 5.903486
 64854/100000: episode: 1274, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.573, mean reward: 1.926 [1.478, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.246, 10.098], loss: 309.180695, mae: 1.922019, mean_q: 6.321135
 64954/100000: episode: 1275, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 213.765, mean reward: 2.138 [1.440, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.058, 10.321], loss: 460.617554, mae: 2.509761, mean_q: 7.065220
 65054/100000: episode: 1276, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.275, mean reward: 2.033 [1.452, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.336, 10.098], loss: 306.994873, mae: 1.660589, mean_q: 6.580989
 65154/100000: episode: 1277, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 212.644, mean reward: 2.126 [1.487, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.293, 10.098], loss: 306.096283, mae: 2.159100, mean_q: 6.984180
 65254/100000: episode: 1278, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 195.388, mean reward: 1.954 [1.463, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.652, 10.098], loss: 308.856903, mae: 1.731349, mean_q: 6.527800
 65354/100000: episode: 1279, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 179.598, mean reward: 1.796 [1.447, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.098], loss: 310.016083, mae: 1.866790, mean_q: 6.518510
 65454/100000: episode: 1280, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 182.126, mean reward: 1.821 [1.453, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.127], loss: 156.198593, mae: 1.792560, mean_q: 6.874155
 65554/100000: episode: 1281, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.437, mean reward: 1.994 [1.503, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.135, 10.293], loss: 306.650421, mae: 1.802674, mean_q: 6.641976
 65654/100000: episode: 1282, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 189.965, mean reward: 1.900 [1.448, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.316, 10.174], loss: 2.300634, mae: 0.755310, mean_q: 6.001927
 65754/100000: episode: 1283, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 206.478, mean reward: 2.065 [1.459, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.481, 10.489], loss: 306.842529, mae: 1.603685, mean_q: 6.179527
 65854/100000: episode: 1284, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.852, mean reward: 1.999 [1.438, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.676, 10.251], loss: 1.539819, mae: 0.968079, mean_q: 6.073384
 65954/100000: episode: 1285, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 202.130, mean reward: 2.021 [1.538, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.622, 10.254], loss: 154.388794, mae: 1.104922, mean_q: 5.895617
 66054/100000: episode: 1286, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 222.139, mean reward: 2.221 [1.477, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.241, 10.098], loss: 0.755134, mae: 0.682690, mean_q: 5.654277
 66154/100000: episode: 1287, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 180.433, mean reward: 1.804 [1.456, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.678, 10.098], loss: 157.692444, mae: 1.305242, mean_q: 5.902991
 66254/100000: episode: 1288, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.302, mean reward: 1.853 [1.468, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.551, 10.161], loss: 155.141830, mae: 1.154097, mean_q: 5.775941
 66354/100000: episode: 1289, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 193.853, mean reward: 1.939 [1.466, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.420, 10.098], loss: 0.910313, mae: 0.694842, mean_q: 5.417109
 66454/100000: episode: 1290, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 232.494, mean reward: 2.325 [1.483, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.906, 10.098], loss: 156.545944, mae: 1.081605, mean_q: 5.555459
 66554/100000: episode: 1291, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 193.884, mean reward: 1.939 [1.447, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.337], loss: 306.492584, mae: 1.626828, mean_q: 5.761961
 66654/100000: episode: 1292, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 198.975, mean reward: 1.990 [1.519, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.188], loss: 1.099370, mae: 0.815692, mean_q: 5.658388
 66754/100000: episode: 1293, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 206.905, mean reward: 2.069 [1.469, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.375, 10.278], loss: 152.889023, mae: 0.942023, mean_q: 5.342407
 66854/100000: episode: 1294, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 179.733, mean reward: 1.797 [1.436, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.338, 10.098], loss: 153.089554, mae: 1.105690, mean_q: 5.414829
 66954/100000: episode: 1295, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 192.132, mean reward: 1.921 [1.451, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.724, 10.221], loss: 0.591351, mae: 0.625190, mean_q: 5.234807
 67054/100000: episode: 1296, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 189.213, mean reward: 1.892 [1.459, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.830, 10.213], loss: 0.361597, mae: 0.518309, mean_q: 4.964731
 67154/100000: episode: 1297, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 208.298, mean reward: 2.083 [1.450, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.549, 10.324], loss: 0.328741, mae: 0.486884, mean_q: 4.815788
 67254/100000: episode: 1298, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.569, mean reward: 1.866 [1.451, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.293, 10.098], loss: 0.296393, mae: 0.470080, mean_q: 4.732340
 67354/100000: episode: 1299, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.599, mean reward: 1.846 [1.482, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.005, 10.098], loss: 0.281648, mae: 0.461716, mean_q: 4.629998
 67454/100000: episode: 1300, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 179.554, mean reward: 1.796 [1.461, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.487, 10.189], loss: 0.288613, mae: 0.451196, mean_q: 4.593408
 67554/100000: episode: 1301, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.174, mean reward: 1.892 [1.455, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.202, 10.098], loss: 0.227971, mae: 0.416379, mean_q: 4.458827
 67654/100000: episode: 1302, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 196.161, mean reward: 1.962 [1.508, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.932, 10.098], loss: 0.186231, mae: 0.389910, mean_q: 4.402803
 67754/100000: episode: 1303, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.823, mean reward: 1.878 [1.511, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.137], loss: 0.187478, mae: 0.379214, mean_q: 4.234824
 67854/100000: episode: 1304, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 217.461, mean reward: 2.175 [1.442, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.593, 10.148], loss: 0.171823, mae: 0.373517, mean_q: 4.237763
 67954/100000: episode: 1305, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.640, mean reward: 1.836 [1.435, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.697, 10.098], loss: 0.172161, mae: 0.361945, mean_q: 4.156372
 68054/100000: episode: 1306, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.108, mean reward: 1.851 [1.502, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.197], loss: 0.141563, mae: 0.341611, mean_q: 4.102780
 68154/100000: episode: 1307, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 199.092, mean reward: 1.991 [1.461, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.190, 10.098], loss: 0.106977, mae: 0.324156, mean_q: 4.025996
 68254/100000: episode: 1308, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 197.926, mean reward: 1.979 [1.444, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.074, 10.266], loss: 0.099880, mae: 0.317699, mean_q: 3.984095
 68354/100000: episode: 1309, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 316.983, mean reward: 3.170 [1.450, 13.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.409 [-1.307, 10.098], loss: 0.105834, mae: 0.317935, mean_q: 3.933778
 68454/100000: episode: 1310, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 182.275, mean reward: 1.823 [1.432, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.146], loss: 0.121863, mae: 0.325702, mean_q: 3.926286
 68554/100000: episode: 1311, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.233, mean reward: 1.902 [1.482, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.551, 10.262], loss: 0.102305, mae: 0.315040, mean_q: 3.931059
 68654/100000: episode: 1312, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 187.207, mean reward: 1.872 [1.460, 4.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.388, 10.098], loss: 0.098785, mae: 0.310957, mean_q: 3.915682
 68754/100000: episode: 1313, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.340, mean reward: 2.033 [1.445, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.547, 10.098], loss: 0.109618, mae: 0.304820, mean_q: 3.904474
 68854/100000: episode: 1314, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 187.715, mean reward: 1.877 [1.429, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.481, 10.205], loss: 0.098571, mae: 0.300384, mean_q: 3.896568
 68954/100000: episode: 1315, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.015, mean reward: 1.990 [1.448, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.870, 10.178], loss: 0.094315, mae: 0.307111, mean_q: 3.903350
 69054/100000: episode: 1316, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.352, mean reward: 1.804 [1.466, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.482, 10.131], loss: 0.112782, mae: 0.306695, mean_q: 3.893174
 69154/100000: episode: 1317, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 202.485, mean reward: 2.025 [1.466, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.482, 10.098], loss: 0.096569, mae: 0.302449, mean_q: 3.891408
 69254/100000: episode: 1318, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.729, mean reward: 1.907 [1.487, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.822, 10.173], loss: 0.090981, mae: 0.298766, mean_q: 3.888462
 69354/100000: episode: 1319, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 185.557, mean reward: 1.856 [1.455, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.289, 10.187], loss: 0.096076, mae: 0.295468, mean_q: 3.877772
 69454/100000: episode: 1320, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.890, mean reward: 1.919 [1.460, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.801, 10.098], loss: 0.083434, mae: 0.293949, mean_q: 3.867247
 69554/100000: episode: 1321, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.134, mean reward: 1.851 [1.482, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.199, 10.196], loss: 0.123527, mae: 0.302623, mean_q: 3.889286
 69654/100000: episode: 1322, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 187.821, mean reward: 1.878 [1.459, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.372, 10.098], loss: 0.099186, mae: 0.306730, mean_q: 3.896850
 69754/100000: episode: 1323, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 194.352, mean reward: 1.944 [1.443, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.098], loss: 0.095301, mae: 0.297426, mean_q: 3.906215
 69854/100000: episode: 1324, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 207.312, mean reward: 2.073 [1.480, 6.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.875, 10.278], loss: 0.107554, mae: 0.301871, mean_q: 3.894433
 69954/100000: episode: 1325, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 236.740, mean reward: 2.367 [1.446, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.551], loss: 0.089575, mae: 0.289772, mean_q: 3.872224
 70054/100000: episode: 1326, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.712, mean reward: 1.887 [1.448, 5.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.945, 10.098], loss: 0.113417, mae: 0.315249, mean_q: 3.928848
 70154/100000: episode: 1327, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.468, mean reward: 1.995 [1.493, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.840, 10.098], loss: 0.103705, mae: 0.295088, mean_q: 3.904657
 70254/100000: episode: 1328, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.771, mean reward: 1.918 [1.484, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.834, 10.170], loss: 0.095052, mae: 0.290086, mean_q: 3.890156
 70354/100000: episode: 1329, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.079, mean reward: 1.871 [1.481, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.222, 10.206], loss: 0.095297, mae: 0.299250, mean_q: 3.905860
 70454/100000: episode: 1330, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.227, mean reward: 1.902 [1.453, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.098], loss: 0.102299, mae: 0.294542, mean_q: 3.928430
 70554/100000: episode: 1331, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 200.670, mean reward: 2.007 [1.456, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.160, 10.098], loss: 0.094148, mae: 0.296667, mean_q: 3.915412
 70654/100000: episode: 1332, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.627, mean reward: 2.016 [1.526, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.191], loss: 0.094744, mae: 0.296402, mean_q: 3.914907
 70754/100000: episode: 1333, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.596, mean reward: 1.886 [1.481, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.466, 10.098], loss: 0.089155, mae: 0.294664, mean_q: 3.918010
 70854/100000: episode: 1334, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.194, mean reward: 2.052 [1.499, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.291, 10.098], loss: 0.104426, mae: 0.297055, mean_q: 3.910175
 70954/100000: episode: 1335, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.231, mean reward: 1.792 [1.462, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.098], loss: 0.082066, mae: 0.277952, mean_q: 3.905596
 71054/100000: episode: 1336, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 201.111, mean reward: 2.011 [1.467, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.175, 10.098], loss: 0.099687, mae: 0.294777, mean_q: 3.913563
 71154/100000: episode: 1337, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 180.361, mean reward: 1.804 [1.464, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.193], loss: 0.088831, mae: 0.282724, mean_q: 3.888944
 71254/100000: episode: 1338, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 197.437, mean reward: 1.974 [1.477, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.670, 10.192], loss: 0.097600, mae: 0.282653, mean_q: 3.893279
 71354/100000: episode: 1339, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 177.752, mean reward: 1.778 [1.448, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.210, 10.098], loss: 0.090131, mae: 0.290645, mean_q: 3.873765
 71454/100000: episode: 1340, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 221.613, mean reward: 2.216 [1.488, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.690, 10.098], loss: 0.090528, mae: 0.284821, mean_q: 3.873349
 71554/100000: episode: 1341, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.063, mean reward: 1.951 [1.454, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.829, 10.311], loss: 0.102393, mae: 0.299965, mean_q: 3.888631
 71654/100000: episode: 1342, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 199.642, mean reward: 1.996 [1.483, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.418, 10.098], loss: 0.104964, mae: 0.299109, mean_q: 3.901318
 71754/100000: episode: 1343, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 190.832, mean reward: 1.908 [1.482, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.401, 10.098], loss: 0.090277, mae: 0.287161, mean_q: 3.879828
 71854/100000: episode: 1344, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 209.545, mean reward: 2.095 [1.533, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.753, 10.098], loss: 0.074433, mae: 0.279205, mean_q: 3.870633
 71954/100000: episode: 1345, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.637, mean reward: 1.966 [1.456, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.897, 10.098], loss: 0.100015, mae: 0.294083, mean_q: 3.880907
 72054/100000: episode: 1346, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 206.029, mean reward: 2.060 [1.479, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.127], loss: 0.082577, mae: 0.288161, mean_q: 3.890558
 72154/100000: episode: 1347, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.517, mean reward: 1.835 [1.439, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.381, 10.224], loss: 0.089416, mae: 0.290233, mean_q: 3.898367
 72254/100000: episode: 1348, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 204.391, mean reward: 2.044 [1.480, 6.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.895, 10.098], loss: 0.094933, mae: 0.289612, mean_q: 3.878095
 72354/100000: episode: 1349, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.172, mean reward: 1.962 [1.458, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.098], loss: 0.103655, mae: 0.306177, mean_q: 3.908631
 72454/100000: episode: 1350, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 195.200, mean reward: 1.952 [1.468, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.339, 10.282], loss: 0.100168, mae: 0.300770, mean_q: 3.891739
 72554/100000: episode: 1351, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 191.513, mean reward: 1.915 [1.540, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.727, 10.194], loss: 0.097915, mae: 0.296616, mean_q: 3.899035
 72654/100000: episode: 1352, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.357, mean reward: 1.834 [1.499, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.133], loss: 0.082678, mae: 0.290923, mean_q: 3.909835
 72754/100000: episode: 1353, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 209.955, mean reward: 2.100 [1.496, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.136, 10.098], loss: 0.100423, mae: 0.302972, mean_q: 3.938437
 72854/100000: episode: 1354, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.516, mean reward: 1.885 [1.456, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.460, 10.283], loss: 0.096895, mae: 0.302562, mean_q: 3.914334
 72954/100000: episode: 1355, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.494, mean reward: 1.885 [1.445, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.909, 10.098], loss: 0.093176, mae: 0.298018, mean_q: 3.879022
 73054/100000: episode: 1356, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.508, mean reward: 1.915 [1.463, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.206, 10.213], loss: 0.098246, mae: 0.302187, mean_q: 3.888406
 73154/100000: episode: 1357, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.946, mean reward: 1.889 [1.466, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.102], loss: 0.086835, mae: 0.292682, mean_q: 3.892860
 73254/100000: episode: 1358, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.525, mean reward: 1.875 [1.445, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.098], loss: 0.101354, mae: 0.305402, mean_q: 3.909507
 73354/100000: episode: 1359, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.661, mean reward: 1.857 [1.464, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.678, 10.161], loss: 0.081476, mae: 0.287205, mean_q: 3.850110
[Info] 1-TH LEVEL FOUND: 5.44801664352417, Considering 10/90 traces
 73454/100000: episode: 1360, duration: 4.642s, episode steps: 100, steps per second: 22, episode reward: 180.710, mean reward: 1.807 [1.463, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.110, 10.098], loss: 0.084593, mae: 0.294611, mean_q: 3.851135
 73484/100000: episode: 1361, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 81.510, mean reward: 2.717 [2.147, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.239, 10.432], loss: 0.087448, mae: 0.289492, mean_q: 3.819878
 73509/100000: episode: 1362, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 77.832, mean reward: 3.113 [2.082, 7.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.994, 10.100], loss: 0.072346, mae: 0.273829, mean_q: 3.815575
 73555/100000: episode: 1363, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 192.204, mean reward: 4.178 [2.149, 8.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.416, 10.100], loss: 0.084197, mae: 0.295715, mean_q: 3.869096
 73585/100000: episode: 1364, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 86.771, mean reward: 2.892 [1.768, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.302, 10.225], loss: 0.101683, mae: 0.311958, mean_q: 3.945781
 73625/100000: episode: 1365, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 94.375, mean reward: 2.359 [1.740, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.139, 10.100], loss: 0.094080, mae: 0.304564, mean_q: 3.916619
 73631/100000: episode: 1366, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 12.777, mean reward: 2.130 [1.870, 2.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.311, 10.100], loss: 0.116149, mae: 0.340206, mean_q: 3.952861
 73637/100000: episode: 1367, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 12.999, mean reward: 2.166 [1.877, 2.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.935, 10.100], loss: 0.063178, mae: 0.259714, mean_q: 3.924330
 73647/100000: episode: 1368, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 26.382, mean reward: 2.638 [2.050, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.261, 10.100], loss: 0.097770, mae: 0.313653, mean_q: 3.891179
 73677/100000: episode: 1369, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 85.805, mean reward: 2.860 [2.012, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.343], loss: 0.103402, mae: 0.323895, mean_q: 3.968352
 73696/100000: episode: 1370, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 43.372, mean reward: 2.283 [1.680, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.232, 10.100], loss: 0.105836, mae: 0.298638, mean_q: 3.897616
 73706/100000: episode: 1371, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 21.945, mean reward: 2.194 [1.771, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.412, 10.100], loss: 0.081923, mae: 0.296139, mean_q: 3.936268
 73752/100000: episode: 1372, duration: 0.232s, episode steps: 46, steps per second: 199, episode reward: 152.514, mean reward: 3.316 [1.740, 8.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.010, 10.100], loss: 0.089006, mae: 0.298605, mean_q: 3.935791
 73792/100000: episode: 1373, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 120.360, mean reward: 3.009 [1.886, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.788, 10.100], loss: 0.098647, mae: 0.313172, mean_q: 3.951694
 73802/100000: episode: 1374, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 25.670, mean reward: 2.567 [1.895, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.353, 10.100], loss: 0.083203, mae: 0.271869, mean_q: 3.878068
 73843/100000: episode: 1375, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 111.742, mean reward: 2.725 [1.873, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.914, 10.100], loss: 0.127108, mae: 0.335316, mean_q: 3.980679
 73853/100000: episode: 1376, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 20.018, mean reward: 2.002 [1.811, 2.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.584, 10.100], loss: 0.088595, mae: 0.296221, mean_q: 3.934046
 73893/100000: episode: 1377, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 94.524, mean reward: 2.363 [1.566, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.153, 10.100], loss: 0.101434, mae: 0.323000, mean_q: 4.007528
 73936/100000: episode: 1378, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 147.566, mean reward: 3.432 [2.030, 5.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.279, 10.508], loss: 0.102032, mae: 0.314887, mean_q: 3.989367
 73952/100000: episode: 1379, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 50.524, mean reward: 3.158 [2.331, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.791, 10.100], loss: 0.092412, mae: 0.289572, mean_q: 3.965516
 73962/100000: episode: 1380, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 26.053, mean reward: 2.605 [2.139, 2.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.351, 10.100], loss: 0.117858, mae: 0.322526, mean_q: 3.992282
 74003/100000: episode: 1381, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 127.986, mean reward: 3.122 [1.861, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.553, 10.100], loss: 0.109912, mae: 0.333674, mean_q: 4.064942
 74019/100000: episode: 1382, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 53.772, mean reward: 3.361 [2.407, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.320, 10.100], loss: 0.098600, mae: 0.296536, mean_q: 3.994343
 74044/100000: episode: 1383, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 70.503, mean reward: 2.820 [1.678, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.200, 10.100], loss: 0.171305, mae: 0.374103, mean_q: 4.105680
 74074/100000: episode: 1384, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 89.250, mean reward: 2.975 [2.056, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.024, 10.550], loss: 0.156737, mae: 0.350274, mean_q: 4.024318
 74084/100000: episode: 1385, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 26.596, mean reward: 2.660 [2.099, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.337, 10.100], loss: 0.130309, mae: 0.349431, mean_q: 4.061160
 74127/100000: episode: 1386, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 101.367, mean reward: 2.357 [1.613, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.150, 10.202], loss: 0.125841, mae: 0.343205, mean_q: 4.102449
 74157/100000: episode: 1387, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 75.161, mean reward: 2.505 [1.645, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.870, 10.174], loss: 0.136848, mae: 0.342187, mean_q: 4.105048
 74203/100000: episode: 1388, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 180.365, mean reward: 3.921 [2.497, 5.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.102, 10.100], loss: 0.152424, mae: 0.364776, mean_q: 4.117622
 74213/100000: episode: 1389, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 24.218, mean reward: 2.422 [2.081, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.293, 10.100], loss: 0.137649, mae: 0.360246, mean_q: 4.208686
 74219/100000: episode: 1390, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 15.099, mean reward: 2.517 [2.061, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.465, 10.100], loss: 0.093885, mae: 0.303476, mean_q: 4.053520
 74262/100000: episode: 1391, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 144.935, mean reward: 3.371 [2.442, 5.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.384, 10.367], loss: 0.122830, mae: 0.343636, mean_q: 4.148918
 74272/100000: episode: 1392, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 21.398, mean reward: 2.140 [1.903, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.373, 10.100], loss: 0.137130, mae: 0.355788, mean_q: 4.175197
 74278/100000: episode: 1393, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 14.650, mean reward: 2.442 [1.706, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.309, 10.100], loss: 0.141876, mae: 0.382442, mean_q: 4.169260
 74319/100000: episode: 1394, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 108.229, mean reward: 2.640 [1.900, 5.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.597, 10.100], loss: 0.135831, mae: 0.355089, mean_q: 4.167816
 74359/100000: episode: 1395, duration: 0.224s, episode steps: 40, steps per second: 179, episode reward: 97.592, mean reward: 2.440 [1.654, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-1.095, 10.100], loss: 0.131580, mae: 0.366230, mean_q: 4.201652
 74365/100000: episode: 1396, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 12.887, mean reward: 2.148 [1.822, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.271, 10.100], loss: 0.105742, mae: 0.333194, mean_q: 4.272698
 74390/100000: episode: 1397, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 63.605, mean reward: 2.544 [1.909, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.296, 10.100], loss: 0.140397, mae: 0.347673, mean_q: 4.115526
 74436/100000: episode: 1398, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 132.229, mean reward: 2.875 [2.025, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.431, 10.100], loss: 0.144409, mae: 0.367845, mean_q: 4.203124
 74455/100000: episode: 1399, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 42.796, mean reward: 2.252 [1.910, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.243, 10.100], loss: 0.132031, mae: 0.352943, mean_q: 4.190011
 74474/100000: episode: 1400, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 54.912, mean reward: 2.890 [2.191, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.179, 10.100], loss: 0.157382, mae: 0.371598, mean_q: 4.170467
 74514/100000: episode: 1401, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 104.951, mean reward: 2.624 [1.899, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.150, 10.100], loss: 0.172234, mae: 0.390445, mean_q: 4.238185
 74554/100000: episode: 1402, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 91.289, mean reward: 2.282 [1.732, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.591, 10.100], loss: 0.147169, mae: 0.376116, mean_q: 4.271882
 74595/100000: episode: 1403, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 93.883, mean reward: 2.290 [1.750, 7.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.090, 10.100], loss: 0.150103, mae: 0.373602, mean_q: 4.269722
 74625/100000: episode: 1404, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 71.179, mean reward: 2.373 [1.881, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.092, 10.334], loss: 0.138410, mae: 0.356586, mean_q: 4.225459
 74650/100000: episode: 1405, duration: 0.158s, episode steps: 25, steps per second: 158, episode reward: 65.134, mean reward: 2.605 [2.111, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.257, 10.100], loss: 0.133041, mae: 0.356021, mean_q: 4.222744
 74656/100000: episode: 1406, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 12.676, mean reward: 2.113 [1.744, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.275, 10.100], loss: 0.146913, mae: 0.369737, mean_q: 4.447424
 74686/100000: episode: 1407, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 79.578, mean reward: 2.653 [1.558, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.413, 10.359], loss: 0.139864, mae: 0.372252, mean_q: 4.246745
 74711/100000: episode: 1408, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 72.818, mean reward: 2.913 [2.039, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.729, 10.100], loss: 0.133216, mae: 0.369016, mean_q: 4.284369
 74730/100000: episode: 1409, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 45.827, mean reward: 2.412 [1.915, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.126, 10.100], loss: 0.157451, mae: 0.385790, mean_q: 4.322526
 74740/100000: episode: 1410, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 27.874, mean reward: 2.787 [2.058, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.376, 10.100], loss: 0.148950, mae: 0.388043, mean_q: 4.211862
 74783/100000: episode: 1411, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 128.907, mean reward: 2.998 [1.891, 6.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.451, 10.523], loss: 0.138309, mae: 0.370074, mean_q: 4.299112
 74789/100000: episode: 1412, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 11.996, mean reward: 1.999 [1.837, 2.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.364, 10.100], loss: 0.114758, mae: 0.337953, mean_q: 4.368502
 74829/100000: episode: 1413, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 87.401, mean reward: 2.185 [1.478, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.788, 10.259], loss: 0.127996, mae: 0.362656, mean_q: 4.297191
 74854/100000: episode: 1414, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 70.343, mean reward: 2.814 [1.914, 4.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.436, 10.100], loss: 0.147737, mae: 0.380516, mean_q: 4.338694
 74879/100000: episode: 1415, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 61.091, mean reward: 2.444 [2.083, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.236, 10.100], loss: 0.252976, mae: 0.434199, mean_q: 4.355375
 74885/100000: episode: 1416, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 15.099, mean reward: 2.517 [1.676, 5.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.631, 10.100], loss: 0.113072, mae: 0.369640, mean_q: 4.442284
 74891/100000: episode: 1417, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 12.139, mean reward: 2.023 [1.783, 2.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.361, 10.100], loss: 0.146179, mae: 0.363208, mean_q: 4.212137
 74910/100000: episode: 1418, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 52.363, mean reward: 2.756 [1.980, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.314, 10.100], loss: 0.138360, mae: 0.362341, mean_q: 4.249301
 74953/100000: episode: 1419, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 92.342, mean reward: 2.147 [1.478, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.619, 10.283], loss: 0.142096, mae: 0.368974, mean_q: 4.286418
 74963/100000: episode: 1420, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 27.736, mean reward: 2.774 [1.986, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.359, 10.100], loss: 0.143068, mae: 0.371154, mean_q: 4.323090
 75004/100000: episode: 1421, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 105.253, mean reward: 2.567 [1.792, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.711, 10.100], loss: 0.140643, mae: 0.372021, mean_q: 4.266758
 75023/100000: episode: 1422, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 52.007, mean reward: 2.737 [2.075, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.461, 10.100], loss: 0.229444, mae: 0.429640, mean_q: 4.437819
 75033/100000: episode: 1423, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 24.412, mean reward: 2.441 [2.016, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.309, 10.100], loss: 0.132820, mae: 0.340185, mean_q: 4.270815
 75079/100000: episode: 1424, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 115.687, mean reward: 2.515 [1.513, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.420, 10.100], loss: 0.158581, mae: 0.366454, mean_q: 4.267341
 75098/100000: episode: 1425, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 57.308, mean reward: 3.016 [2.147, 4.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.686, 10.100], loss: 0.148085, mae: 0.377973, mean_q: 4.338966
 75117/100000: episode: 1426, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 54.124, mean reward: 2.849 [2.045, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.279, 10.100], loss: 0.148973, mae: 0.383957, mean_q: 4.405817
 75142/100000: episode: 1427, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 58.912, mean reward: 2.356 [1.923, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.215, 10.100], loss: 0.144619, mae: 0.367353, mean_q: 4.300414
 75167/100000: episode: 1428, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 62.524, mean reward: 2.501 [2.052, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.171, 10.100], loss: 0.140188, mae: 0.384651, mean_q: 4.344950
 75197/100000: episode: 1429, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 77.074, mean reward: 2.569 [1.794, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.353], loss: 0.181338, mae: 0.412154, mean_q: 4.375831
 75222/100000: episode: 1430, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 58.935, mean reward: 2.357 [1.864, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.441, 10.100], loss: 0.149753, mae: 0.384678, mean_q: 4.361745
 75263/100000: episode: 1431, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 103.385, mean reward: 2.522 [2.101, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.014, 10.100], loss: 0.138194, mae: 0.368646, mean_q: 4.355080
 75269/100000: episode: 1432, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 11.334, mean reward: 1.889 [1.751, 2.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.189, 10.100], loss: 0.148636, mae: 0.376146, mean_q: 4.132675
 75299/100000: episode: 1433, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 78.921, mean reward: 2.631 [1.836, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.186, 10.213], loss: 0.182109, mae: 0.385890, mean_q: 4.382311
 75305/100000: episode: 1434, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 14.399, mean reward: 2.400 [1.938, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.400, 10.100], loss: 0.138138, mae: 0.375457, mean_q: 4.390044
 75315/100000: episode: 1435, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 28.863, mean reward: 2.886 [1.810, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.200, 10.100], loss: 0.132605, mae: 0.378195, mean_q: 4.475291
 75334/100000: episode: 1436, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 42.598, mean reward: 2.242 [1.774, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.477, 10.100], loss: 0.135463, mae: 0.384069, mean_q: 4.349735
 75380/100000: episode: 1437, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 535.114, mean reward: 11.633 [2.368, 244.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.847, 10.100], loss: 19.692413, mae: 0.575519, mean_q: 4.448779
 75405/100000: episode: 1438, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 74.886, mean reward: 2.995 [2.083, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.650, 10.100], loss: 35.277054, mae: 0.989388, mean_q: 4.608817
 75435/100000: episode: 1439, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 81.908, mean reward: 2.730 [1.692, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.035, 10.274], loss: 0.754892, mae: 0.814707, mean_q: 4.573105
 75441/100000: episode: 1440, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 13.806, mean reward: 2.301 [1.726, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.381, 10.100], loss: 0.347313, mae: 0.575867, mean_q: 4.315983
 75457/100000: episode: 1441, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 39.228, mean reward: 2.452 [1.706, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.684, 10.100], loss: 0.280653, mae: 0.516694, mean_q: 4.574119
 75498/100000: episode: 1442, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 90.158, mean reward: 2.199 [1.535, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.371, 10.100], loss: 0.750460, mae: 0.519052, mean_q: 4.550570
 75538/100000: episode: 1443, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 98.531, mean reward: 2.463 [1.902, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.447, 10.100], loss: 0.265769, mae: 0.471116, mean_q: 4.535404
 75581/100000: episode: 1444, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 113.311, mean reward: 2.635 [1.897, 5.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.710, 10.363], loss: 21.285891, mae: 0.872460, mean_q: 4.644966
 75600/100000: episode: 1445, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 41.324, mean reward: 2.175 [1.839, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.834, 10.100], loss: 47.417675, mae: 0.993160, mean_q: 4.538056
 75606/100000: episode: 1446, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 13.977, mean reward: 2.329 [1.679, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.395, 10.100], loss: 1.912092, mae: 1.314930, mean_q: 5.522660
 75647/100000: episode: 1447, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 109.980, mean reward: 2.682 [1.463, 6.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.064, 10.254], loss: 1.099000, mae: 0.730053, mean_q: 4.536141
 75687/100000: episode: 1448, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 125.888, mean reward: 3.147 [2.196, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.271, 10.100], loss: 0.270362, mae: 0.508943, mean_q: 4.534120
 75693/100000: episode: 1449, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 12.282, mean reward: 2.047 [1.723, 2.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.294, 10.100], loss: 0.252428, mae: 0.510740, mean_q: 4.524146
[Info] 2-TH LEVEL FOUND: 10.236475944519043, Considering 10/90 traces
 75736/100000: episode: 1450, duration: 4.304s, episode steps: 43, steps per second: 10, episode reward: 144.400, mean reward: 3.358 [1.929, 8.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.415, 10.372], loss: 1.148662, mae: 0.547711, mean_q: 4.545378
 75749/100000: episode: 1451, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 41.532, mean reward: 3.195 [2.419, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.239, 10.100], loss: 0.273967, mae: 0.489541, mean_q: 4.588893
 75768/100000: episode: 1452, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 58.989, mean reward: 3.105 [2.297, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.507, 10.100], loss: 0.331097, mae: 0.522304, mean_q: 4.616565
 75781/100000: episode: 1453, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 39.564, mean reward: 3.043 [2.625, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.304, 10.100], loss: 0.251153, mae: 0.489452, mean_q: 4.625645
 75787/100000: episode: 1454, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 18.312, mean reward: 3.052 [2.727, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.464, 10.100], loss: 0.347851, mae: 0.478000, mean_q: 4.522577
 75793/100000: episode: 1455, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 19.053, mean reward: 3.176 [2.744, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.386, 10.100], loss: 0.202372, mae: 0.434291, mean_q: 4.504855
 75818/100000: episode: 1456, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 112.586, mean reward: 4.503 [2.352, 7.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.408, 10.100], loss: 0.219163, mae: 0.470254, mean_q: 4.584700
 75831/100000: episode: 1457, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 34.972, mean reward: 2.690 [2.056, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.710, 10.100], loss: 0.244950, mae: 0.465351, mean_q: 4.530156
 75850/100000: episode: 1458, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 92.968, mean reward: 4.893 [3.602, 7.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.468, 10.100], loss: 0.275415, mae: 0.488263, mean_q: 4.563889
 75869/100000: episode: 1459, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 88.402, mean reward: 4.653 [2.994, 6.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.644, 10.100], loss: 0.351085, mae: 0.458608, mean_q: 4.735205
 75875/100000: episode: 1460, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 19.408, mean reward: 3.235 [2.934, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.403, 10.100], loss: 0.232129, mae: 0.448176, mean_q: 4.567410
[Info] FALSIFICATION!
 75900/100000: episode: 1461, duration: 0.291s, episode steps: 25, steps per second: 86, episode reward: 1159.168, mean reward: 46.367 [3.244, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.090, 9.988], loss: 0.254315, mae: 0.482274, mean_q: 4.695329
 75932/100000: episode: 1462, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 136.886, mean reward: 4.278 [1.752, 8.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.049, 10.100], loss: 0.231027, mae: 0.445891, mean_q: 4.607627
 75936/100000: episode: 1463, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 35.958, mean reward: 8.990 [5.574, 16.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.603], loss: 0.789470, mae: 0.600493, mean_q: 4.967582
 75946/100000: episode: 1464, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 27.937, mean reward: 2.794 [2.339, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.320, 10.100], loss: 0.312152, mae: 0.507822, mean_q: 4.636421
 75959/100000: episode: 1465, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 37.234, mean reward: 2.864 [2.289, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.470, 10.100], loss: 0.299001, mae: 0.488869, mean_q: 4.759903
 75965/100000: episode: 1466, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 19.253, mean reward: 3.209 [2.763, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.536, 10.100], loss: 0.163672, mae: 0.408676, mean_q: 4.586315
[Info] FALSIFICATION!
 75969/100000: episode: 1467, duration: 0.276s, episode steps: 4, steps per second: 15, episode reward: 1014.641, mean reward: 253.660 [4.452, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.011, 8.262], loss: 0.358796, mae: 0.524908, mean_q: 4.509321
 75975/100000: episode: 1468, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 16.503, mean reward: 2.751 [2.431, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.411, 10.100], loss: 0.206121, mae: 0.409593, mean_q: 4.685168
 75992/100000: episode: 1469, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 63.966, mean reward: 3.763 [2.920, 5.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.329, 10.100], loss: 898.912720, mae: 2.496042, mean_q: 4.961936
 76002/100000: episode: 1470, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 37.195, mean reward: 3.719 [3.216, 5.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.341, 10.100], loss: 3.873161, mae: 1.919896, mean_q: 5.980900
 76017/100000: episode: 1471, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 53.007, mean reward: 3.534 [2.035, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.259, 10.100], loss: 0.718079, mae: 0.851539, mean_q: 4.327100
 76030/100000: episode: 1472, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 56.320, mean reward: 4.332 [2.928, 7.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.427, 10.100], loss: 0.798697, mae: 0.790954, mean_q: 5.099450
 76036/100000: episode: 1473, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 18.000, mean reward: 3.000 [2.804, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.363, 10.100], loss: 0.565214, mae: 0.658999, mean_q: 4.925762
 76046/100000: episode: 1474, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 38.299, mean reward: 3.830 [2.635, 5.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.541, 10.100], loss: 0.362490, mae: 0.574174, mean_q: 4.854677
 76059/100000: episode: 1475, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 43.542, mean reward: 3.349 [2.446, 5.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.294, 10.100], loss: 0.364856, mae: 0.587841, mean_q: 4.722249
 76074/100000: episode: 1476, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 73.117, mean reward: 4.874 [3.105, 7.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.314, 10.100], loss: 0.306429, mae: 0.552940, mean_q: 4.903695
 76091/100000: episode: 1477, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 50.373, mean reward: 2.963 [2.498, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.263, 10.100], loss: 53.400604, mae: 1.147615, mean_q: 5.069788
 76101/100000: episode: 1478, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 39.164, mean reward: 3.916 [2.778, 5.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.367, 10.100], loss: 0.596489, mae: 0.714271, mean_q: 5.014141
 76111/100000: episode: 1479, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 38.989, mean reward: 3.899 [2.818, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.325, 10.100], loss: 0.391982, mae: 0.595751, mean_q: 4.520148
 76117/100000: episode: 1480, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 17.529, mean reward: 2.921 [2.760, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.407, 10.100], loss: 149.327408, mae: 1.777881, mean_q: 4.705262
 76142/100000: episode: 1481, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 116.066, mean reward: 4.643 [1.909, 8.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.281, 10.100], loss: 0.611349, mae: 0.748299, mean_q: 5.163372
 76152/100000: episode: 1482, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 42.407, mean reward: 4.241 [2.843, 7.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.316, 10.100], loss: 1525.695557, mae: 4.198019, mean_q: 5.420433
 76172/100000: episode: 1483, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 58.898, mean reward: 2.945 [2.263, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.212, 10.100], loss: 3.312748, mae: 1.772590, mean_q: 6.089431
 76191/100000: episode: 1484, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 63.746, mean reward: 3.355 [2.637, 4.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.263, 10.100], loss: 0.972816, mae: 0.929978, mean_q: 4.849238
[Info] FALSIFICATION!
 76195/100000: episode: 1485, duration: 0.275s, episode steps: 4, steps per second: 15, episode reward: 1028.558, mean reward: 257.139 [5.231, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.035, 10.836], loss: 0.461499, mae: 0.695868, mean_q: 5.264982
 76199/100000: episode: 1486, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 17.076, mean reward: 4.269 [3.966, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.512], loss: 0.233013, mae: 0.543098, mean_q: 5.018574
 76209/100000: episode: 1487, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 29.517, mean reward: 2.952 [2.494, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.412, 10.100], loss: 0.578271, mae: 0.664388, mean_q: 5.224181
 76226/100000: episode: 1488, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 66.838, mean reward: 3.932 [2.810, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.320, 10.100], loss: 0.530237, mae: 0.656516, mean_q: 5.076468
 76246/100000: episode: 1489, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 52.983, mean reward: 2.649 [1.569, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.591, 10.110], loss: 762.703735, mae: 3.097733, mean_q: 6.258748
 76259/100000: episode: 1490, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 52.198, mean reward: 4.015 [2.813, 7.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.504, 10.100], loss: 2339.842773, mae: 7.325879, mean_q: 7.441819
 76269/100000: episode: 1491, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 34.298, mean reward: 3.430 [2.404, 6.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.539, 10.100], loss: 7.939410, mae: 2.937137, mean_q: 7.231415
 76286/100000: episode: 1492, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 48.066, mean reward: 2.827 [2.314, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.735, 10.100], loss: 1.200998, mae: 0.961517, mean_q: 4.982278
 76292/100000: episode: 1493, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 15.817, mean reward: 2.636 [2.389, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.450, 10.100], loss: 0.581180, mae: 0.771702, mean_q: 5.316568
 76305/100000: episode: 1494, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 37.444, mean reward: 2.880 [2.364, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.617, 10.100], loss: 0.667570, mae: 0.842394, mean_q: 5.440687
 76311/100000: episode: 1495, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 18.738, mean reward: 3.123 [2.671, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.464, 10.100], loss: 0.656432, mae: 0.747427, mean_q: 5.596513
 76321/100000: episode: 1496, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 33.918, mean reward: 3.392 [2.497, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.554, 10.100], loss: 0.396314, mae: 0.659872, mean_q: 5.439103
 76340/100000: episode: 1497, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 51.019, mean reward: 2.685 [2.007, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.438, 10.100], loss: 796.881042, mae: 2.300637, mean_q: 5.522070
 76344/100000: episode: 1498, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 27.481, mean reward: 6.870 [4.427, 11.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.555], loss: 2.155778, mae: 1.583845, mean_q: 6.454317
 76376/100000: episode: 1499, duration: 0.177s, episode steps: 32, steps per second: 180, episode reward: 149.442, mean reward: 4.670 [2.106, 13.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.678, 10.100], loss: 2.338147, mae: 1.467659, mean_q: 6.386565
 76396/100000: episode: 1500, duration: 0.106s, episode steps: 20, steps per second: 190, episode reward: 78.129, mean reward: 3.906 [1.960, 7.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.149, 10.100], loss: 0.564005, mae: 0.710505, mean_q: 5.386054
 76409/100000: episode: 1501, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 35.577, mean reward: 2.737 [2.241, 4.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.160, 10.100], loss: 0.714201, mae: 0.679216, mean_q: 5.448145
 76422/100000: episode: 1502, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 49.175, mean reward: 3.783 [2.453, 6.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.387, 10.100], loss: 0.552562, mae: 0.673630, mean_q: 5.551708
 76428/100000: episode: 1503, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 22.848, mean reward: 3.808 [2.682, 6.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.395, 10.100], loss: 0.543378, mae: 0.675694, mean_q: 5.800986
 76445/100000: episode: 1504, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 42.625, mean reward: 2.507 [2.197, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.360, 10.100], loss: 893.186768, mae: 3.616027, mean_q: 6.723574
 76470/100000: episode: 1505, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 147.508, mean reward: 5.900 [2.556, 17.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.240, 10.100], loss: 620.276978, mae: 2.294043, mean_q: 5.930257
 76490/100000: episode: 1506, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 66.250, mean reward: 3.313 [2.368, 6.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.518, 10.100], loss: 2.424325, mae: 1.193323, mean_q: 6.296522
 76509/100000: episode: 1507, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 57.982, mean reward: 3.052 [2.628, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.399, 10.100], loss: 0.666842, mae: 0.770310, mean_q: 5.805685
 76534/100000: episode: 1508, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 132.202, mean reward: 5.288 [2.303, 12.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.132, 10.100], loss: 1233.124634, mae: 3.396310, mean_q: 5.918010
 76547/100000: episode: 1509, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 53.275, mean reward: 4.098 [2.802, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.503, 10.100], loss: 3.797937, mae: 2.224577, mean_q: 7.626386
 76562/100000: episode: 1510, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 52.080, mean reward: 3.472 [2.943, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.289, 10.100], loss: 2047.711060, mae: 7.029388, mean_q: 8.435811
 76579/100000: episode: 1511, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 55.354, mean reward: 3.256 [2.399, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.202, 10.100], loss: 3.915103, mae: 2.079794, mean_q: 7.615400
 76596/100000: episode: 1512, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 65.538, mean reward: 3.855 [2.546, 8.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.511, 10.100], loss: 1.005897, mae: 0.906665, mean_q: 6.083680
 76628/100000: episode: 1513, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 143.780, mean reward: 4.493 [2.656, 16.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.513, 10.100], loss: 478.837189, mae: 1.720907, mean_q: 5.504333
 76638/100000: episode: 1514, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 38.249, mean reward: 3.825 [3.009, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.543, 10.100], loss: 94.262222, mae: 2.290354, mean_q: 6.632028
 76657/100000: episode: 1515, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 99.809, mean reward: 5.253 [2.762, 10.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.452, 10.100], loss: 848.325439, mae: 3.357641, mean_q: 6.401517
 76674/100000: episode: 1516, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 69.384, mean reward: 4.081 [2.839, 7.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.407, 10.100], loss: 4.562352, mae: 2.183268, mean_q: 7.745689
 76691/100000: episode: 1517, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 52.474, mean reward: 3.087 [2.064, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.345, 10.100], loss: 1.627124, mae: 1.249902, mean_q: 6.186376
 76706/100000: episode: 1518, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 84.765, mean reward: 5.651 [4.501, 7.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.499, 10.100], loss: 0.856414, mae: 0.938109, mean_q: 5.862535
 76731/100000: episode: 1519, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 89.842, mean reward: 3.594 [2.437, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.697, 10.100], loss: 616.145691, mae: 2.111857, mean_q: 6.329867
 76748/100000: episode: 1520, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 55.049, mean reward: 3.238 [2.617, 5.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.247, 10.100], loss: 2.575618, mae: 1.780167, mean_q: 7.791653
 76780/100000: episode: 1521, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 135.826, mean reward: 4.245 [2.359, 11.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.844, 10.100], loss: 0.967466, mae: 0.849430, mean_q: 6.411833
 76800/100000: episode: 1522, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 84.206, mean reward: 4.210 [3.208, 6.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.177, 10.100], loss: 764.719604, mae: 2.470068, mean_q: 6.081385
 76819/100000: episode: 1523, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 65.534, mean reward: 3.449 [2.792, 5.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.261, 10.100], loss: 1.748568, mae: 1.151158, mean_q: 6.445672
 76829/100000: episode: 1524, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 26.800, mean reward: 2.680 [2.356, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.297, 10.100], loss: 2.128641, mae: 1.487831, mean_q: 7.132878
 76839/100000: episode: 1525, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 31.965, mean reward: 3.196 [2.556, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.380, 10.100], loss: 5.025430, mae: 2.560675, mean_q: 8.238150
 76845/100000: episode: 1526, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 23.970, mean reward: 3.995 [2.476, 6.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.513, 10.100], loss: 5.831167, mae: 2.705523, mean_q: 8.636417
 76855/100000: episode: 1527, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 50.756, mean reward: 5.076 [2.902, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.679, 10.100], loss: 2.458133, mae: 1.597977, mean_q: 7.215547
[Info] FALSIFICATION!
 76869/100000: episode: 1528, duration: 0.291s, episode steps: 14, steps per second: 48, episode reward: 1083.347, mean reward: 77.382 [3.400, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.010, 9.320], loss: 0.828274, mae: 0.849002, mean_q: 6.119784
 76888/100000: episode: 1529, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 64.022, mean reward: 3.370 [2.173, 5.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.318, 10.100], loss: 1.061129, mae: 0.829241, mean_q: 5.869470
 76898/100000: episode: 1530, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 28.686, mean reward: 2.869 [2.330, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.441, 10.100], loss: 91.711983, mae: 1.517357, mean_q: 6.136806
 76911/100000: episode: 1531, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 42.815, mean reward: 3.293 [2.822, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.354, 10.100], loss: 1.483121, mae: 1.021992, mean_q: 6.472343
 76921/100000: episode: 1532, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 43.635, mean reward: 4.364 [2.661, 6.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.508, 10.100], loss: 0.755711, mae: 0.774984, mean_q: 6.104335
 76953/100000: episode: 1533, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 174.239, mean reward: 5.445 [2.755, 10.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.221, 10.100], loss: 0.562826, mae: 0.681811, mean_q: 5.954364
 76978/100000: episode: 1534, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 105.219, mean reward: 4.209 [3.047, 6.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.725, 10.100], loss: 1230.506348, mae: 3.482880, mean_q: 6.397587
 76997/100000: episode: 1535, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 86.949, mean reward: 4.576 [2.924, 10.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.332, 10.100], loss: 7.141859, mae: 2.802275, mean_q: 8.530223
 77003/100000: episode: 1536, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 19.593, mean reward: 3.266 [2.811, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.539, 10.100], loss: 1.141105, mae: 1.071495, mean_q: 6.635996
 77028/100000: episode: 1537, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 105.665, mean reward: 4.227 [2.612, 5.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.617, 10.100], loss: 616.417236, mae: 2.198451, mean_q: 6.221830
 77047/100000: episode: 1538, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 70.157, mean reward: 3.692 [2.901, 6.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.481, 10.100], loss: 803.983704, mae: 3.508067, mean_q: 7.569683
 77072/100000: episode: 1539, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 150.394, mean reward: 6.016 [3.458, 18.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.345, 10.100], loss: 3.723985, mae: 1.922930, mean_q: 7.744146
[Info] Complete ISplit Iteration
[Info] Levels: [5.4480166, 10.236476, 11.984702]
[Info] Cond. Prob: [0.1, 0.1, 0.84]
[Info] Error Prob: 0.008400000000000001

 77078/100000: episode: 1540, duration: 4.234s, episode steps: 6, steps per second: 1, episode reward: 21.484, mean reward: 3.581 [3.087, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.397, 10.100], loss: 2524.229248, mae: 6.127898, mean_q: 6.444677
 77178/100000: episode: 1541, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.512, mean reward: 1.835 [1.448, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.073, 10.199], loss: 307.849396, mae: 1.675211, mean_q: 6.609127
 77278/100000: episode: 1542, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.532, mean reward: 1.895 [1.454, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.811, 10.161], loss: 153.669540, mae: 1.427936, mean_q: 6.557182
 77378/100000: episode: 1543, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.228, mean reward: 1.932 [1.449, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.743, 10.098], loss: 307.910461, mae: 1.994040, mean_q: 6.860399
 77478/100000: episode: 1544, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 203.921, mean reward: 2.039 [1.480, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.338], loss: 162.122192, mae: 1.456390, mean_q: 6.270856
 77578/100000: episode: 1545, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 204.594, mean reward: 2.046 [1.478, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.098], loss: 768.594910, mae: 3.230658, mean_q: 6.996856
 77678/100000: episode: 1546, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 207.870, mean reward: 2.079 [1.487, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.847, 10.352], loss: 461.732697, mae: 2.483219, mean_q: 7.543591
 77778/100000: episode: 1547, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.141, mean reward: 1.901 [1.472, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.113, 10.098], loss: 619.481628, mae: 3.037502, mean_q: 7.642291
 77878/100000: episode: 1548, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 204.641, mean reward: 2.046 [1.437, 6.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.286, 10.098], loss: 925.057739, mae: 4.232103, mean_q: 8.371799
 77978/100000: episode: 1549, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 196.448, mean reward: 1.964 [1.444, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.098], loss: 305.882202, mae: 1.927848, mean_q: 7.318337
 78078/100000: episode: 1550, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 187.894, mean reward: 1.879 [1.476, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.205, 10.098], loss: 313.863220, mae: 1.833362, mean_q: 7.029395
 78178/100000: episode: 1551, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 178.657, mean reward: 1.787 [1.461, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.139], loss: 469.958099, mae: 2.936104, mean_q: 7.815038
 78278/100000: episode: 1552, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 251.610, mean reward: 2.516 [1.489, 5.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.753, 10.207], loss: 313.782074, mae: 1.825361, mean_q: 6.804636
 78378/100000: episode: 1553, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.387, mean reward: 1.924 [1.463, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.259], loss: 316.518158, mae: 1.762104, mean_q: 6.744078
 78478/100000: episode: 1554, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.830, mean reward: 1.968 [1.482, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.263], loss: 763.498291, mae: 3.241031, mean_q: 7.470850
 78578/100000: episode: 1555, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.051, mean reward: 1.941 [1.456, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.138, 10.179], loss: 306.522675, mae: 2.254656, mean_q: 7.512473
 78678/100000: episode: 1556, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 208.115, mean reward: 2.081 [1.464, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.284, 10.136], loss: 315.343048, mae: 1.891387, mean_q: 6.882647
 78778/100000: episode: 1557, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 184.502, mean reward: 1.845 [1.445, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.523, 10.311], loss: 610.877197, mae: 2.751256, mean_q: 7.277806
 78878/100000: episode: 1558, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 183.245, mean reward: 1.832 [1.475, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.869, 10.119], loss: 304.868652, mae: 2.090277, mean_q: 6.904736
 78978/100000: episode: 1559, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.705, mean reward: 1.817 [1.441, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.152], loss: 620.536072, mae: 3.284972, mean_q: 7.725809
 79078/100000: episode: 1560, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.291, mean reward: 1.853 [1.474, 2.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.852, 10.098], loss: 457.732849, mae: 2.582743, mean_q: 7.279984
 79178/100000: episode: 1561, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 194.553, mean reward: 1.946 [1.450, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.257, 10.229], loss: 318.218048, mae: 1.809751, mean_q: 6.460273
 79278/100000: episode: 1562, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 190.800, mean reward: 1.908 [1.452, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.743, 10.209], loss: 321.230103, mae: 1.969083, mean_q: 6.672309
 79378/100000: episode: 1563, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.676, mean reward: 1.967 [1.460, 5.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.140, 10.122], loss: 304.230408, mae: 1.833215, mean_q: 6.400658
 79478/100000: episode: 1564, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 193.003, mean reward: 1.930 [1.488, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.686, 10.098], loss: 19.533989, mae: 1.177844, mean_q: 6.096091
 79578/100000: episode: 1565, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 218.337, mean reward: 2.183 [1.508, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.098], loss: 304.120911, mae: 1.609601, mean_q: 5.925855
 79678/100000: episode: 1566, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.966, mean reward: 1.990 [1.495, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.460, 10.098], loss: 306.237122, mae: 2.185742, mean_q: 6.612448
 79778/100000: episode: 1567, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 182.336, mean reward: 1.823 [1.453, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.152, 10.220], loss: 601.593384, mae: 2.803687, mean_q: 6.932036
 79878/100000: episode: 1568, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.786, mean reward: 1.888 [1.465, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.890, 10.098], loss: 169.533676, mae: 1.508556, mean_q: 6.146410
 79978/100000: episode: 1569, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.206, mean reward: 1.892 [1.459, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.343, 10.098], loss: 608.210876, mae: 2.985430, mean_q: 7.475686
 80078/100000: episode: 1570, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.373, mean reward: 1.984 [1.504, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.016, 10.226], loss: 778.932861, mae: 3.102852, mean_q: 6.952886
 80178/100000: episode: 1571, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 242.367, mean reward: 2.424 [1.460, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.753, 10.098], loss: 894.067017, mae: 4.005971, mean_q: 7.916824
 80278/100000: episode: 1572, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 183.347, mean reward: 1.833 [1.456, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.453, 10.298], loss: 608.978210, mae: 2.635985, mean_q: 6.988820
 80378/100000: episode: 1573, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.347, mean reward: 1.933 [1.475, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.098], loss: 304.868561, mae: 1.949354, mean_q: 6.812845
 80478/100000: episode: 1574, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 204.317, mean reward: 2.043 [1.445, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.127], loss: 151.779556, mae: 1.464089, mean_q: 6.495706
 80578/100000: episode: 1575, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 202.099, mean reward: 2.021 [1.466, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.071, 10.098], loss: 153.053894, mae: 1.281396, mean_q: 5.968424
 80678/100000: episode: 1576, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.189, mean reward: 1.962 [1.463, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.891, 10.154], loss: 299.466095, mae: 1.504440, mean_q: 5.721695
 80778/100000: episode: 1577, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.689, mean reward: 1.817 [1.467, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.929, 10.221], loss: 450.970245, mae: 2.159275, mean_q: 6.096931
 80878/100000: episode: 1578, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 193.280, mean reward: 1.933 [1.450, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.057, 10.098], loss: 153.442429, mae: 1.380971, mean_q: 5.918385
 80978/100000: episode: 1579, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.064, mean reward: 1.871 [1.512, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.934, 10.276], loss: 0.898347, mae: 0.717290, mean_q: 5.202807
 81078/100000: episode: 1580, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.733, mean reward: 1.997 [1.489, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.528, 10.098], loss: 148.505112, mae: 1.244177, mean_q: 5.499906
 81178/100000: episode: 1581, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 187.217, mean reward: 1.872 [1.455, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.201, 10.098], loss: 0.849037, mae: 0.693026, mean_q: 5.184464
 81278/100000: episode: 1582, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.333, mean reward: 1.923 [1.437, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.065, 10.098], loss: 0.569021, mae: 0.588207, mean_q: 4.884385
 81378/100000: episode: 1583, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 199.286, mean reward: 1.993 [1.483, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.171, 10.098], loss: 0.595053, mae: 0.588821, mean_q: 4.752322
 81478/100000: episode: 1584, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.640, mean reward: 1.846 [1.465, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.341, 10.098], loss: 153.381195, mae: 0.908016, mean_q: 4.705648
 81578/100000: episode: 1585, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 184.801, mean reward: 1.848 [1.477, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.257], loss: 152.942581, mae: 1.108363, mean_q: 4.962813
 81678/100000: episode: 1586, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 208.422, mean reward: 2.084 [1.473, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.663, 10.098], loss: 303.574097, mae: 1.558232, mean_q: 5.110260
 81778/100000: episode: 1587, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 201.656, mean reward: 2.017 [1.487, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.215, 10.343], loss: 301.251007, mae: 1.337764, mean_q: 4.795533
 81878/100000: episode: 1588, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 212.135, mean reward: 2.121 [1.488, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.868, 10.098], loss: 1.192470, mae: 0.700707, mean_q: 4.663661
 81978/100000: episode: 1589, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.938, mean reward: 1.819 [1.450, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.804, 10.171], loss: 0.254399, mae: 0.415061, mean_q: 4.132107
 82078/100000: episode: 1590, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.313, mean reward: 1.963 [1.469, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.348, 10.416], loss: 0.155266, mae: 0.372582, mean_q: 3.996786
 82178/100000: episode: 1591, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 189.597, mean reward: 1.896 [1.470, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.310, 10.168], loss: 0.154665, mae: 0.371902, mean_q: 3.954156
 82278/100000: episode: 1592, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.125, mean reward: 1.901 [1.443, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.318, 10.214], loss: 0.141609, mae: 0.365871, mean_q: 3.941422
 82378/100000: episode: 1593, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 199.576, mean reward: 1.996 [1.459, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.228], loss: 0.129611, mae: 0.349355, mean_q: 3.921583
 82478/100000: episode: 1594, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.453, mean reward: 1.925 [1.492, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.659, 10.262], loss: 0.127174, mae: 0.342456, mean_q: 3.897974
 82578/100000: episode: 1595, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 186.679, mean reward: 1.867 [1.450, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.828, 10.098], loss: 0.124988, mae: 0.342504, mean_q: 3.871786
 82678/100000: episode: 1596, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.359, mean reward: 1.854 [1.465, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.933, 10.128], loss: 0.127373, mae: 0.346085, mean_q: 3.890243
 82778/100000: episode: 1597, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 208.252, mean reward: 2.083 [1.499, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.164, 10.329], loss: 0.124152, mae: 0.344292, mean_q: 3.872649
 82878/100000: episode: 1598, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.260, mean reward: 1.893 [1.439, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.957, 10.098], loss: 0.111357, mae: 0.327338, mean_q: 3.876232
 82978/100000: episode: 1599, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.663, mean reward: 1.977 [1.522, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.936, 10.098], loss: 0.112218, mae: 0.333585, mean_q: 3.903074
 83078/100000: episode: 1600, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 176.857, mean reward: 1.769 [1.434, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.129], loss: 0.109332, mae: 0.329593, mean_q: 3.885070
 83178/100000: episode: 1601, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.097, mean reward: 1.981 [1.477, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.525, 10.179], loss: 0.121315, mae: 0.341513, mean_q: 3.878755
 83278/100000: episode: 1602, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 210.735, mean reward: 2.107 [1.461, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.222, 10.169], loss: 0.101807, mae: 0.316365, mean_q: 3.864997
 83378/100000: episode: 1603, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 210.898, mean reward: 2.109 [1.439, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.517], loss: 0.104516, mae: 0.321247, mean_q: 3.856961
 83478/100000: episode: 1604, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 211.994, mean reward: 2.120 [1.466, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.738, 10.098], loss: 0.107144, mae: 0.325448, mean_q: 3.851259
 83578/100000: episode: 1605, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 185.418, mean reward: 1.854 [1.448, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.931, 10.281], loss: 0.102514, mae: 0.319934, mean_q: 3.859101
 83678/100000: episode: 1606, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 180.721, mean reward: 1.807 [1.512, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.499, 10.099], loss: 0.113778, mae: 0.326732, mean_q: 3.870157
 83778/100000: episode: 1607, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.580, mean reward: 1.976 [1.446, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.762, 10.186], loss: 0.104835, mae: 0.317428, mean_q: 3.859859
 83878/100000: episode: 1608, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.429, mean reward: 1.884 [1.439, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.659, 10.284], loss: 0.109661, mae: 0.321241, mean_q: 3.867877
 83978/100000: episode: 1609, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.903, mean reward: 1.869 [1.456, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.401, 10.253], loss: 0.105957, mae: 0.316295, mean_q: 3.853178
 84078/100000: episode: 1610, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 178.308, mean reward: 1.783 [1.439, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.253, 10.146], loss: 0.111672, mae: 0.325480, mean_q: 3.851801
 84178/100000: episode: 1611, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.467, mean reward: 1.965 [1.503, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.420, 10.098], loss: 0.102667, mae: 0.316465, mean_q: 3.867970
 84278/100000: episode: 1612, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 193.860, mean reward: 1.939 [1.469, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.089, 10.126], loss: 0.097756, mae: 0.312576, mean_q: 3.851075
 84378/100000: episode: 1613, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 208.151, mean reward: 2.082 [1.441, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.321, 10.098], loss: 0.101160, mae: 0.313068, mean_q: 3.857411
 84478/100000: episode: 1614, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.301, mean reward: 1.913 [1.449, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.162, 10.180], loss: 0.109441, mae: 0.317504, mean_q: 3.863089
 84578/100000: episode: 1615, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.157, mean reward: 1.972 [1.441, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.450, 10.130], loss: 0.100733, mae: 0.306556, mean_q: 3.861633
 84678/100000: episode: 1616, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 203.672, mean reward: 2.037 [1.446, 7.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.058, 10.098], loss: 0.106130, mae: 0.316992, mean_q: 3.861448
 84778/100000: episode: 1617, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.521, mean reward: 1.805 [1.490, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.017, 10.176], loss: 0.117768, mae: 0.318858, mean_q: 3.859981
 84878/100000: episode: 1618, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 200.204, mean reward: 2.002 [1.486, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.022, 10.098], loss: 0.110552, mae: 0.321876, mean_q: 3.867682
 84978/100000: episode: 1619, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.205, mean reward: 2.032 [1.471, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.907, 10.098], loss: 0.101679, mae: 0.305987, mean_q: 3.866623
 85078/100000: episode: 1620, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 199.814, mean reward: 1.998 [1.529, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.369, 10.366], loss: 0.106791, mae: 0.325268, mean_q: 3.860294
 85178/100000: episode: 1621, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.558, mean reward: 1.916 [1.453, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.097112, mae: 0.308463, mean_q: 3.848343
 85278/100000: episode: 1622, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 189.006, mean reward: 1.890 [1.446, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.923, 10.098], loss: 0.098387, mae: 0.304021, mean_q: 3.852613
 85378/100000: episode: 1623, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 199.069, mean reward: 1.991 [1.449, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.354, 10.098], loss: 0.098184, mae: 0.313470, mean_q: 3.861931
 85478/100000: episode: 1624, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 203.802, mean reward: 2.038 [1.467, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.834, 10.098], loss: 0.107864, mae: 0.311654, mean_q: 3.870511
 85578/100000: episode: 1625, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.745, mean reward: 1.957 [1.466, 5.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.412, 10.206], loss: 0.099872, mae: 0.305819, mean_q: 3.849798
 85678/100000: episode: 1626, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.127, mean reward: 1.871 [1.442, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.707, 10.098], loss: 0.098061, mae: 0.309855, mean_q: 3.854540
 85778/100000: episode: 1627, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.361, mean reward: 1.854 [1.445, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.658, 10.098], loss: 0.101760, mae: 0.307158, mean_q: 3.859932
 85878/100000: episode: 1628, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 184.276, mean reward: 1.843 [1.449, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.899, 10.234], loss: 0.103548, mae: 0.301898, mean_q: 3.852182
 85978/100000: episode: 1629, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 191.142, mean reward: 1.911 [1.468, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.355, 10.291], loss: 0.104923, mae: 0.312992, mean_q: 3.864200
 86078/100000: episode: 1630, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.645, mean reward: 1.886 [1.439, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.183], loss: 0.088851, mae: 0.298562, mean_q: 3.850239
 86178/100000: episode: 1631, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.753, mean reward: 1.968 [1.485, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.328, 10.315], loss: 0.081707, mae: 0.291921, mean_q: 3.844540
 86278/100000: episode: 1632, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.365, mean reward: 1.874 [1.464, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.029, 10.197], loss: 0.091840, mae: 0.299728, mean_q: 3.839117
 86378/100000: episode: 1633, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 226.000, mean reward: 2.260 [1.464, 15.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.972, 10.270], loss: 0.103952, mae: 0.303675, mean_q: 3.855870
 86478/100000: episode: 1634, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 201.463, mean reward: 2.015 [1.490, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.116, 10.274], loss: 0.127698, mae: 0.312572, mean_q: 3.861820
 86578/100000: episode: 1635, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 201.566, mean reward: 2.016 [1.466, 4.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.402, 10.098], loss: 0.123624, mae: 0.307887, mean_q: 3.863708
 86678/100000: episode: 1636, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.544, mean reward: 2.035 [1.493, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.203, 10.323], loss: 0.094639, mae: 0.302078, mean_q: 3.849137
 86778/100000: episode: 1637, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 201.548, mean reward: 2.015 [1.439, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.438, 10.254], loss: 0.234977, mae: 0.333715, mean_q: 3.900164
 86878/100000: episode: 1638, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.164, mean reward: 1.842 [1.458, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.123], loss: 0.097793, mae: 0.293354, mean_q: 3.836537
 86978/100000: episode: 1639, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 213.340, mean reward: 2.133 [1.483, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.080, 10.565], loss: 0.092049, mae: 0.293133, mean_q: 3.849507
[Info] 1-TH LEVEL FOUND: 5.492096424102783, Considering 10/90 traces
 87078/100000: episode: 1640, duration: 4.784s, episode steps: 100, steps per second: 21, episode reward: 221.325, mean reward: 2.213 [1.445, 11.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.207, 10.098], loss: 0.087142, mae: 0.292366, mean_q: 3.846168
 87081/100000: episode: 1641, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 7.763, mean reward: 2.588 [2.292, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.415], loss: 0.089972, mae: 0.301092, mean_q: 3.815264
 87096/100000: episode: 1642, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 32.268, mean reward: 2.151 [1.760, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.474, 10.275], loss: 0.342741, mae: 0.384399, mean_q: 3.921625
 87112/100000: episode: 1643, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 29.683, mean reward: 1.855 [1.491, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.326, 10.208], loss: 0.119520, mae: 0.329627, mean_q: 3.914810
 87127/100000: episode: 1644, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 35.684, mean reward: 2.379 [2.019, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.216, 10.368], loss: 0.115537, mae: 0.309033, mean_q: 3.884670
 87142/100000: episode: 1645, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 55.948, mean reward: 3.730 [2.446, 5.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.522], loss: 0.157770, mae: 0.323186, mean_q: 3.916089
 87169/100000: episode: 1646, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 71.771, mean reward: 2.658 [1.719, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.265, 10.100], loss: 0.138942, mae: 0.329017, mean_q: 3.887583
 87184/100000: episode: 1647, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 38.550, mean reward: 2.570 [2.136, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.278, 10.483], loss: 0.119624, mae: 0.334531, mean_q: 3.952338
 87199/100000: episode: 1648, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 42.112, mean reward: 2.807 [2.195, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.044, 10.473], loss: 0.110242, mae: 0.327851, mean_q: 3.857698
 87232/100000: episode: 1649, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 84.415, mean reward: 2.558 [1.558, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.777, 10.182], loss: 0.106413, mae: 0.307213, mean_q: 3.898312
 87235/100000: episode: 1650, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 8.014, mean reward: 2.671 [2.388, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.463], loss: 0.117381, mae: 0.346360, mean_q: 3.986945
 87250/100000: episode: 1651, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 35.541, mean reward: 2.369 [1.855, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.354], loss: 0.135502, mae: 0.328583, mean_q: 3.910444
 87253/100000: episode: 1652, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 8.174, mean reward: 2.725 [2.349, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.474], loss: 0.061197, mae: 0.266726, mean_q: 3.895260
 87269/100000: episode: 1653, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 45.975, mean reward: 2.873 [2.389, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.066, 10.338], loss: 0.190301, mae: 0.328150, mean_q: 3.893462
 87302/100000: episode: 1654, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 120.766, mean reward: 3.660 [2.290, 5.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.457], loss: 0.130262, mae: 0.331564, mean_q: 3.926445
 87329/100000: episode: 1655, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 60.742, mean reward: 2.250 [1.626, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.201, 10.100], loss: 0.249255, mae: 0.378237, mean_q: 3.950765
 87362/100000: episode: 1656, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 88.752, mean reward: 2.689 [1.832, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.635, 10.315], loss: 0.201490, mae: 0.342029, mean_q: 3.961667
 87377/100000: episode: 1657, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 41.867, mean reward: 2.791 [2.025, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.054, 10.351], loss: 0.146691, mae: 0.350468, mean_q: 3.984464
 87392/100000: episode: 1658, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 33.043, mean reward: 2.203 [1.902, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.305], loss: 0.135184, mae: 0.346220, mean_q: 3.938987
 87408/100000: episode: 1659, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 43.616, mean reward: 2.726 [2.146, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.899, 10.100], loss: 0.152097, mae: 0.370483, mean_q: 3.997410
 87423/100000: episode: 1660, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 36.648, mean reward: 2.443 [1.970, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.438, 10.336], loss: 0.141973, mae: 0.359612, mean_q: 3.960111
 87450/100000: episode: 1661, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 72.522, mean reward: 2.686 [1.842, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.037, 10.100], loss: 0.130937, mae: 0.358813, mean_q: 3.939329
 87477/100000: episode: 1662, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 71.663, mean reward: 2.654 [2.161, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.355, 10.100], loss: 0.116444, mae: 0.342743, mean_q: 3.970828
 87492/100000: episode: 1663, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 34.950, mean reward: 2.330 [1.873, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.676, 10.329], loss: 0.153752, mae: 0.337832, mean_q: 3.947980
 87507/100000: episode: 1664, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 42.599, mean reward: 2.840 [2.164, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.449, 10.399], loss: 0.108902, mae: 0.326704, mean_q: 3.936125
 87522/100000: episode: 1665, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 34.951, mean reward: 2.330 [1.968, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.372], loss: 0.170015, mae: 0.364804, mean_q: 4.018762
 87538/100000: episode: 1666, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 31.559, mean reward: 1.972 [1.497, 2.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.775, 10.100], loss: 0.118659, mae: 0.333167, mean_q: 3.993178
 87553/100000: episode: 1667, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 40.027, mean reward: 2.668 [2.091, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.392], loss: 0.141300, mae: 0.337517, mean_q: 3.962948
 87568/100000: episode: 1668, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 38.737, mean reward: 2.582 [2.002, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.329, 10.489], loss: 0.129723, mae: 0.361806, mean_q: 4.014253
 87583/100000: episode: 1669, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 32.205, mean reward: 2.147 [1.599, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.812, 10.295], loss: 0.147239, mae: 0.373158, mean_q: 4.025282
 87598/100000: episode: 1670, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 37.370, mean reward: 2.491 [2.052, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.390], loss: 0.344138, mae: 0.381592, mean_q: 4.049888
 87625/100000: episode: 1671, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 66.665, mean reward: 2.469 [1.702, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.415, 10.100], loss: 0.245806, mae: 0.399945, mean_q: 4.050402
 87640/100000: episode: 1672, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 33.620, mean reward: 2.241 [1.771, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.352], loss: 0.115166, mae: 0.322380, mean_q: 3.973496
 87656/100000: episode: 1673, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 35.701, mean reward: 2.231 [1.864, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.361], loss: 0.161661, mae: 0.367790, mean_q: 4.026333
 87672/100000: episode: 1674, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 39.436, mean reward: 2.465 [1.997, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.378], loss: 0.114968, mae: 0.326021, mean_q: 3.996670
 87675/100000: episode: 1675, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 7.361, mean reward: 2.454 [2.322, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.385], loss: 0.086007, mae: 0.305123, mean_q: 3.952029
 87678/100000: episode: 1676, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 8.382, mean reward: 2.794 [2.536, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.359, 10.342], loss: 0.173079, mae: 0.364487, mean_q: 3.881938
 87711/100000: episode: 1677, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 105.104, mean reward: 3.185 [2.119, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.310, 10.453], loss: 0.170277, mae: 0.354601, mean_q: 4.010134
 87726/100000: episode: 1678, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 39.358, mean reward: 2.624 [2.217, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.451], loss: 0.127492, mae: 0.346647, mean_q: 3.986526
 87741/100000: episode: 1679, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 34.740, mean reward: 2.316 [1.893, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.376], loss: 0.123360, mae: 0.338082, mean_q: 4.000823
 87744/100000: episode: 1680, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 6.928, mean reward: 2.309 [2.280, 2.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.352], loss: 0.123120, mae: 0.369769, mean_q: 4.013307
 87777/100000: episode: 1681, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 136.432, mean reward: 4.134 [2.717, 7.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.095, 10.529], loss: 0.137888, mae: 0.359251, mean_q: 4.032188
 87780/100000: episode: 1682, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 8.701, mean reward: 2.900 [2.840, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.459], loss: 0.129652, mae: 0.365659, mean_q: 4.067050
 87795/100000: episode: 1683, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 36.984, mean reward: 2.466 [1.979, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.074, 10.389], loss: 0.203894, mae: 0.359595, mean_q: 4.029373
 87810/100000: episode: 1684, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 45.276, mean reward: 3.018 [2.184, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.191, 10.488], loss: 0.190832, mae: 0.399707, mean_q: 4.058578
 87826/100000: episode: 1685, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 39.302, mean reward: 2.456 [2.004, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.086, 10.408], loss: 0.218073, mae: 0.365963, mean_q: 3.969395
 87829/100000: episode: 1686, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 7.926, mean reward: 2.642 [2.433, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.446], loss: 0.120478, mae: 0.336444, mean_q: 4.104938
 87862/100000: episode: 1687, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 95.987, mean reward: 2.909 [2.091, 6.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.737, 10.291], loss: 0.152161, mae: 0.375541, mean_q: 4.057714
 87877/100000: episode: 1688, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 36.426, mean reward: 2.428 [1.895, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.267, 10.342], loss: 0.128017, mae: 0.378835, mean_q: 4.158105
 87893/100000: episode: 1689, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 45.718, mean reward: 2.857 [2.318, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.457, 10.100], loss: 0.140670, mae: 0.358966, mean_q: 4.006318
 87920/100000: episode: 1690, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 68.568, mean reward: 2.540 [1.810, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.180, 10.100], loss: 0.141448, mae: 0.363090, mean_q: 4.064534
 87935/100000: episode: 1691, duration: 0.087s, episode steps: 15, steps per second: 171, episode reward: 38.032, mean reward: 2.535 [2.005, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.369], loss: 0.141572, mae: 0.371213, mean_q: 4.122436
 87951/100000: episode: 1692, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 44.746, mean reward: 2.797 [1.859, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.151, 10.478], loss: 0.122325, mae: 0.333994, mean_q: 4.066778
 87954/100000: episode: 1693, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 7.139, mean reward: 2.380 [2.277, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.293, 10.363], loss: 0.099921, mae: 0.348292, mean_q: 4.093456
 87969/100000: episode: 1694, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 42.998, mean reward: 2.867 [2.137, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.459], loss: 0.170795, mae: 0.390983, mean_q: 4.078126
 87984/100000: episode: 1695, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 48.344, mean reward: 3.223 [2.625, 5.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.757, 10.440], loss: 0.141609, mae: 0.355534, mean_q: 4.078127
 88017/100000: episode: 1696, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 107.275, mean reward: 3.251 [2.127, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.403, 10.382], loss: 0.162069, mae: 0.384966, mean_q: 4.154042
 88020/100000: episode: 1697, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 8.301, mean reward: 2.767 [2.521, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.478], loss: 0.149702, mae: 0.366607, mean_q: 4.093773
 88036/100000: episode: 1698, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 38.240, mean reward: 2.390 [2.078, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.338], loss: 0.159856, mae: 0.365704, mean_q: 4.119695
 88051/100000: episode: 1699, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 63.058, mean reward: 4.204 [2.255, 7.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.269, 10.597], loss: 0.124678, mae: 0.349194, mean_q: 4.123233
 88066/100000: episode: 1700, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 41.569, mean reward: 2.771 [2.286, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.440], loss: 0.155271, mae: 0.388243, mean_q: 4.126831
 88081/100000: episode: 1701, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 35.308, mean reward: 2.354 [1.885, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.299], loss: 0.145873, mae: 0.377841, mean_q: 4.155354
 88096/100000: episode: 1702, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 39.341, mean reward: 2.623 [1.920, 6.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.584, 10.345], loss: 0.230896, mae: 0.389350, mean_q: 4.152068
 88111/100000: episode: 1703, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 33.578, mean reward: 2.239 [1.654, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.245], loss: 0.126063, mae: 0.363807, mean_q: 4.156657
 88126/100000: episode: 1704, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 35.610, mean reward: 2.374 [2.011, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.125, 10.366], loss: 0.177980, mae: 0.376241, mean_q: 4.090749
 88141/100000: episode: 1705, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 38.865, mean reward: 2.591 [1.909, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.215, 10.276], loss: 0.132315, mae: 0.342119, mean_q: 4.038184
 88156/100000: episode: 1706, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 36.786, mean reward: 2.452 [2.106, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.367], loss: 0.150017, mae: 0.370171, mean_q: 4.211020
 88171/100000: episode: 1707, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 28.575, mean reward: 1.905 [1.666, 2.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.280], loss: 0.162333, mae: 0.386544, mean_q: 4.111270
 88204/100000: episode: 1708, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 104.253, mean reward: 3.159 [1.865, 6.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.545, 10.285], loss: 0.168974, mae: 0.383003, mean_q: 4.126245
 88207/100000: episode: 1709, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 6.936, mean reward: 2.312 [2.239, 2.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.381], loss: 0.167562, mae: 0.390747, mean_q: 4.207675
 88222/100000: episode: 1710, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 36.919, mean reward: 2.461 [2.049, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.398], loss: 0.141428, mae: 0.355378, mean_q: 4.112675
 88225/100000: episode: 1711, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 7.515, mean reward: 2.505 [2.388, 2.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.386], loss: 0.092171, mae: 0.341887, mean_q: 4.164819
 88258/100000: episode: 1712, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 73.702, mean reward: 2.233 [1.562, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.071, 10.201], loss: 0.201403, mae: 0.394870, mean_q: 4.172997
 88273/100000: episode: 1713, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 44.338, mean reward: 2.956 [2.202, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.152, 10.342], loss: 0.181643, mae: 0.392627, mean_q: 4.203043
 88289/100000: episode: 1714, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 39.440, mean reward: 2.465 [1.932, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.261, 10.100], loss: 0.148823, mae: 0.372959, mean_q: 4.124777
 88304/100000: episode: 1715, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 45.721, mean reward: 3.048 [2.293, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.496, 10.407], loss: 0.366300, mae: 0.426374, mean_q: 4.209704
 88319/100000: episode: 1716, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 32.232, mean reward: 2.149 [1.859, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.353], loss: 0.147107, mae: 0.370272, mean_q: 4.086460
 88352/100000: episode: 1717, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 85.008, mean reward: 2.576 [1.691, 5.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.490, 10.293], loss: 0.306550, mae: 0.428261, mean_q: 4.172456
 88379/100000: episode: 1718, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 80.052, mean reward: 2.965 [2.103, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.272, 10.100], loss: 0.175441, mae: 0.390050, mean_q: 4.221698
 88394/100000: episode: 1719, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 45.440, mean reward: 3.029 [2.190, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.277, 10.598], loss: 0.263170, mae: 0.411095, mean_q: 4.183611
 88409/100000: episode: 1720, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 31.841, mean reward: 2.123 [1.823, 2.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.300], loss: 0.168439, mae: 0.383434, mean_q: 4.176700
 88424/100000: episode: 1721, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 39.767, mean reward: 2.651 [2.106, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.203, 10.480], loss: 0.125302, mae: 0.358138, mean_q: 4.199345
 88439/100000: episode: 1722, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 49.299, mean reward: 3.287 [2.125, 5.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.403, 10.528], loss: 0.431562, mae: 0.457601, mean_q: 4.239567
 88454/100000: episode: 1723, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 42.429, mean reward: 2.829 [2.375, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.650, 10.469], loss: 0.147405, mae: 0.385581, mean_q: 4.196678
 88469/100000: episode: 1724, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 30.054, mean reward: 2.004 [1.668, 2.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.315], loss: 0.331553, mae: 0.401539, mean_q: 4.219902
 88484/100000: episode: 1725, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 30.360, mean reward: 2.024 [1.846, 2.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.290], loss: 0.150028, mae: 0.356399, mean_q: 4.172906
 88499/100000: episode: 1726, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 36.591, mean reward: 2.439 [1.976, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.636, 10.376], loss: 0.158843, mae: 0.375579, mean_q: 4.173703
 88502/100000: episode: 1727, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 8.153, mean reward: 2.718 [2.310, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.479], loss: 0.350502, mae: 0.416223, mean_q: 4.158998
 88505/100000: episode: 1728, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 7.277, mean reward: 2.426 [2.154, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.365], loss: 0.120129, mae: 0.365036, mean_q: 4.206889
 88520/100000: episode: 1729, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 34.757, mean reward: 2.317 [1.889, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.360], loss: 0.283771, mae: 0.425340, mean_q: 4.211556
[Info] 2-TH LEVEL FOUND: 8.739754676818848, Considering 11/89 traces
 88523/100000: episode: 1730, duration: 4.053s, episode steps: 3, steps per second: 1, episode reward: 10.543, mean reward: 3.514 [2.510, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.515], loss: 0.139478, mae: 0.363100, mean_q: 4.292448
 88556/100000: episode: 1731, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 96.244, mean reward: 2.916 [1.678, 6.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.402, 10.294], loss: 0.168789, mae: 0.382315, mean_q: 4.242104
 88589/100000: episode: 1732, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 125.891, mean reward: 3.815 [1.925, 7.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.032, 10.261], loss: 0.242692, mae: 0.379610, mean_q: 4.246000
 88622/100000: episode: 1733, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 97.511, mean reward: 2.955 [2.088, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.218, 10.480], loss: 0.204596, mae: 0.394215, mean_q: 4.252208
 88655/100000: episode: 1734, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 129.767, mean reward: 3.932 [2.648, 5.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.221, 10.491], loss: 0.289277, mae: 0.432556, mean_q: 4.288956
 88688/100000: episode: 1735, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 107.289, mean reward: 3.251 [2.256, 10.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.818, 10.437], loss: 0.222245, mae: 0.435939, mean_q: 4.336235
 88721/100000: episode: 1736, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 108.481, mean reward: 3.287 [2.053, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.362, 10.326], loss: 0.213160, mae: 0.413056, mean_q: 4.330406
 88754/100000: episode: 1737, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 96.858, mean reward: 2.935 [2.063, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.338, 10.370], loss: 0.285393, mae: 0.426223, mean_q: 4.361488
 88787/100000: episode: 1738, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 106.727, mean reward: 3.234 [2.184, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.890, 10.415], loss: 0.203598, mae: 0.405695, mean_q: 4.327747
 88820/100000: episode: 1739, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 167.042, mean reward: 5.062 [2.985, 14.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.160, 10.509], loss: 0.338165, mae: 0.431685, mean_q: 4.323545
 88853/100000: episode: 1740, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 96.741, mean reward: 2.932 [1.999, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.353], loss: 0.275560, mae: 0.451380, mean_q: 4.380182
 88886/100000: episode: 1741, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 127.279, mean reward: 3.857 [2.594, 7.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.579, 10.518], loss: 0.230078, mae: 0.422169, mean_q: 4.370719
 88919/100000: episode: 1742, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 87.308, mean reward: 2.646 [1.731, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.550, 10.258], loss: 0.268069, mae: 0.445293, mean_q: 4.397973
 88952/100000: episode: 1743, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 100.093, mean reward: 3.033 [2.281, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.390, 10.468], loss: 0.275823, mae: 0.450941, mean_q: 4.481701
 88985/100000: episode: 1744, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 140.339, mean reward: 4.253 [3.055, 5.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.731, 10.480], loss: 0.293851, mae: 0.471011, mean_q: 4.485491
 89018/100000: episode: 1745, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 108.367, mean reward: 3.284 [2.270, 6.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.192, 10.383], loss: 0.254265, mae: 0.437088, mean_q: 4.440087
 89051/100000: episode: 1746, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 123.707, mean reward: 3.749 [2.615, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.483], loss: 0.290271, mae: 0.466569, mean_q: 4.560378
 89084/100000: episode: 1747, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 98.106, mean reward: 2.973 [1.774, 5.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.583, 10.269], loss: 0.279191, mae: 0.472321, mean_q: 4.539671
 89117/100000: episode: 1748, duration: 0.171s, episode steps: 33, steps per second: 192, episode reward: 112.907, mean reward: 3.421 [2.146, 7.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.275, 10.382], loss: 0.373287, mae: 0.483898, mean_q: 4.617552
 89150/100000: episode: 1749, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 113.003, mean reward: 3.424 [2.614, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.482], loss: 0.349454, mae: 0.456389, mean_q: 4.487944
 89183/100000: episode: 1750, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 95.830, mean reward: 2.904 [1.879, 4.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.116, 10.358], loss: 0.286301, mae: 0.468471, mean_q: 4.588509
 89216/100000: episode: 1751, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 101.958, mean reward: 3.090 [2.087, 5.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.353, 10.336], loss: 0.292090, mae: 0.478419, mean_q: 4.640310
 89249/100000: episode: 1752, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 102.549, mean reward: 3.108 [1.724, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.563, 10.281], loss: 0.287819, mae: 0.460509, mean_q: 4.625181
 89282/100000: episode: 1753, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 74.666, mean reward: 2.263 [1.559, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.106, 10.100], loss: 0.239192, mae: 0.445832, mean_q: 4.699984
 89315/100000: episode: 1754, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 96.163, mean reward: 2.914 [1.508, 7.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.500, 10.171], loss: 0.290050, mae: 0.465530, mean_q: 4.625888
 89348/100000: episode: 1755, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 112.046, mean reward: 3.395 [2.642, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.676, 10.457], loss: 0.284889, mae: 0.467753, mean_q: 4.601548
 89381/100000: episode: 1756, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 115.683, mean reward: 3.506 [2.270, 7.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.481, 10.531], loss: 0.345150, mae: 0.479633, mean_q: 4.652332
 89414/100000: episode: 1757, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 77.439, mean reward: 2.347 [1.524, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.507, 10.100], loss: 0.248522, mae: 0.456814, mean_q: 4.645753
 89447/100000: episode: 1758, duration: 0.191s, episode steps: 33, steps per second: 172, episode reward: 90.790, mean reward: 2.751 [2.217, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.441, 10.302], loss: 0.373096, mae: 0.517842, mean_q: 4.752152
 89480/100000: episode: 1759, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 114.234, mean reward: 3.462 [1.942, 6.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.927, 10.405], loss: 0.492297, mae: 0.526160, mean_q: 4.663382
 89513/100000: episode: 1760, duration: 0.162s, episode steps: 33, steps per second: 203, episode reward: 89.965, mean reward: 2.726 [2.258, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.909, 10.405], loss: 0.236390, mae: 0.474382, mean_q: 4.808634
 89546/100000: episode: 1761, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 106.078, mean reward: 3.214 [2.395, 5.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.408], loss: 0.290608, mae: 0.480755, mean_q: 4.725832
 89579/100000: episode: 1762, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 135.601, mean reward: 4.109 [2.895, 6.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.596], loss: 0.345056, mae: 0.489267, mean_q: 4.729123
 89612/100000: episode: 1763, duration: 0.205s, episode steps: 33, steps per second: 161, episode reward: 159.545, mean reward: 4.835 [2.767, 8.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.890, 10.487], loss: 0.358134, mae: 0.502416, mean_q: 4.756937
 89645/100000: episode: 1764, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 113.657, mean reward: 3.444 [2.219, 7.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.025, 10.447], loss: 0.322780, mae: 0.484789, mean_q: 4.817411
 89678/100000: episode: 1765, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 110.623, mean reward: 3.352 [1.990, 10.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.881, 10.367], loss: 0.515978, mae: 0.543819, mean_q: 4.889644
 89711/100000: episode: 1766, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 81.365, mean reward: 2.466 [1.530, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.068, 10.100], loss: 0.305420, mae: 0.476249, mean_q: 4.821333
 89744/100000: episode: 1767, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 78.531, mean reward: 2.380 [1.859, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.332], loss: 0.262618, mae: 0.496207, mean_q: 4.892331
 89777/100000: episode: 1768, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 103.859, mean reward: 3.147 [2.172, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.404], loss: 0.278247, mae: 0.489481, mean_q: 4.854638
 89810/100000: episode: 1769, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 83.879, mean reward: 2.542 [1.489, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.337, 10.100], loss: 0.350398, mae: 0.542800, mean_q: 4.925424
 89843/100000: episode: 1770, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 106.664, mean reward: 3.232 [2.520, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.408], loss: 0.331838, mae: 0.506940, mean_q: 4.840095
 89876/100000: episode: 1771, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 86.248, mean reward: 2.614 [1.932, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.120, 10.310], loss: 0.244205, mae: 0.465310, mean_q: 4.833737
 89909/100000: episode: 1772, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 118.452, mean reward: 3.589 [2.376, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.715, 10.380], loss: 0.305558, mae: 0.501398, mean_q: 4.865130
 89942/100000: episode: 1773, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 80.308, mean reward: 2.434 [1.657, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.777, 10.267], loss: 0.391565, mae: 0.531598, mean_q: 4.987633
 89975/100000: episode: 1774, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 94.327, mean reward: 2.858 [1.478, 5.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.244], loss: 0.298471, mae: 0.496767, mean_q: 4.999274
 90008/100000: episode: 1775, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 104.570, mean reward: 3.169 [1.816, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.650, 10.510], loss: 0.371662, mae: 0.545278, mean_q: 4.943120
 90041/100000: episode: 1776, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 119.280, mean reward: 3.615 [2.709, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.800, 10.464], loss: 0.352192, mae: 0.556914, mean_q: 4.983853
 90074/100000: episode: 1777, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 143.635, mean reward: 4.353 [2.643, 8.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.688, 10.474], loss: 0.301183, mae: 0.506603, mean_q: 4.986005
 90107/100000: episode: 1778, duration: 0.155s, episode steps: 33, steps per second: 213, episode reward: 110.911, mean reward: 3.361 [2.368, 6.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.178, 10.391], loss: 0.331975, mae: 0.519170, mean_q: 4.981694
 90140/100000: episode: 1779, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 99.785, mean reward: 3.024 [2.303, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.402], loss: 0.372693, mae: 0.523816, mean_q: 4.910211
 90173/100000: episode: 1780, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 84.193, mean reward: 2.551 [1.624, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.131, 10.239], loss: 0.474490, mae: 0.579548, mean_q: 5.005622
 90206/100000: episode: 1781, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 82.518, mean reward: 2.501 [1.765, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.330], loss: 0.283843, mae: 0.515475, mean_q: 5.067829
 90239/100000: episode: 1782, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 115.836, mean reward: 3.510 [2.092, 5.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.299], loss: 0.298982, mae: 0.512221, mean_q: 5.030769
 90272/100000: episode: 1783, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 82.856, mean reward: 2.511 [1.536, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.148, 10.100], loss: 0.246063, mae: 0.473395, mean_q: 4.976628
 90305/100000: episode: 1784, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 95.933, mean reward: 2.907 [1.884, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.405, 10.332], loss: 0.354948, mae: 0.526919, mean_q: 5.053486
 90338/100000: episode: 1785, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 110.882, mean reward: 3.360 [2.448, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.797, 10.549], loss: 0.271015, mae: 0.494227, mean_q: 5.008067
[Info] FALSIFICATION!
 90347/100000: episode: 1786, duration: 0.211s, episode steps: 9, steps per second: 43, episode reward: 1035.205, mean reward: 115.023 [2.980, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.596, 8.729], loss: 0.360772, mae: 0.542937, mean_q: 5.185676
 90380/100000: episode: 1787, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 86.289, mean reward: 2.615 [1.725, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.329, 10.303], loss: 0.363920, mae: 0.539278, mean_q: 5.023734
 90413/100000: episode: 1788, duration: 0.164s, episode steps: 33, steps per second: 202, episode reward: 95.788, mean reward: 2.903 [1.520, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.157, 10.171], loss: 936.961060, mae: 3.725007, mean_q: 6.705684
 90446/100000: episode: 1789, duration: 0.170s, episode steps: 33, steps per second: 195, episode reward: 83.219, mean reward: 2.522 [1.497, 6.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.287, 10.108], loss: 1.001395, mae: 0.973031, mean_q: 5.204738
 90479/100000: episode: 1790, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 89.855, mean reward: 2.723 [1.764, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.737, 10.294], loss: 0.532726, mae: 0.697413, mean_q: 5.237236
 90512/100000: episode: 1791, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 88.700, mean reward: 2.688 [1.905, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.859, 10.279], loss: 0.540993, mae: 0.622109, mean_q: 5.243121
 90545/100000: episode: 1792, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 77.728, mean reward: 2.355 [1.591, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.116, 10.256], loss: 0.389019, mae: 0.585994, mean_q: 5.248558
 90578/100000: episode: 1793, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 88.853, mean reward: 2.693 [1.736, 5.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.537, 10.474], loss: 467.974701, mae: 2.216795, mean_q: 6.422480
 90611/100000: episode: 1794, duration: 0.164s, episode steps: 33, steps per second: 202, episode reward: 113.565, mean reward: 3.441 [2.257, 7.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.716, 10.400], loss: 0.432190, mae: 0.674206, mean_q: 5.126023
 90644/100000: episode: 1795, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 80.441, mean reward: 2.438 [1.768, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.314], loss: 467.417786, mae: 2.183523, mean_q: 6.417288
 90677/100000: episode: 1796, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 118.712, mean reward: 3.597 [2.041, 8.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.388, 10.342], loss: 0.582434, mae: 0.740786, mean_q: 5.192609
 90710/100000: episode: 1797, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 120.172, mean reward: 3.642 [2.518, 4.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.536, 10.529], loss: 0.400597, mae: 0.618233, mean_q: 5.424637
 90743/100000: episode: 1798, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 85.155, mean reward: 2.580 [1.842, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.693, 10.303], loss: 0.356511, mae: 0.555240, mean_q: 5.390124
 90776/100000: episode: 1799, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 97.553, mean reward: 2.956 [1.838, 8.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.112, 10.283], loss: 0.343291, mae: 0.573572, mean_q: 5.354358
 90809/100000: episode: 1800, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 79.075, mean reward: 2.396 [1.655, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.247], loss: 467.183502, mae: 1.906776, mean_q: 5.908834
 90842/100000: episode: 1801, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 90.993, mean reward: 2.757 [2.020, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.471, 10.418], loss: 0.787900, mae: 0.847415, mean_q: 5.608510
 90875/100000: episode: 1802, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 159.592, mean reward: 4.836 [2.940, 7.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.830, 10.560], loss: 467.179901, mae: 2.156149, mean_q: 6.419719
 90908/100000: episode: 1803, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 152.108, mean reward: 4.609 [2.756, 10.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.035, 10.459], loss: 0.461263, mae: 0.705222, mean_q: 5.432343
 90941/100000: episode: 1804, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 156.828, mean reward: 4.752 [2.448, 8.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.704, 10.401], loss: 0.445771, mae: 0.626104, mean_q: 5.539296
 90974/100000: episode: 1805, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 78.898, mean reward: 2.391 [1.805, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.695, 10.347], loss: 0.381730, mae: 0.579113, mean_q: 5.548701
 91007/100000: episode: 1806, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 87.171, mean reward: 2.642 [1.581, 5.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.315, 10.237], loss: 466.500885, mae: 1.625394, mean_q: 5.662541
 91040/100000: episode: 1807, duration: 0.170s, episode steps: 33, steps per second: 195, episode reward: 81.650, mean reward: 2.474 [1.723, 5.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.131, 10.241], loss: 466.399200, mae: 2.292397, mean_q: 6.665888
 91073/100000: episode: 1808, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 79.547, mean reward: 2.411 [1.679, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.289], loss: 0.678046, mae: 0.892660, mean_q: 6.002689
 91106/100000: episode: 1809, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 98.751, mean reward: 2.992 [2.473, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.095, 10.399], loss: 0.441976, mae: 0.636547, mean_q: 5.527493
 91139/100000: episode: 1810, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 110.252, mean reward: 3.341 [1.841, 5.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-2.003, 10.282], loss: 0.405155, mae: 0.592305, mean_q: 5.467856
 91172/100000: episode: 1811, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 90.719, mean reward: 2.749 [1.860, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.220, 10.353], loss: 0.335253, mae: 0.571665, mean_q: 5.622188
 91205/100000: episode: 1812, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 108.159, mean reward: 3.278 [2.326, 4.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.670, 10.401], loss: 0.368457, mae: 0.597166, mean_q: 5.612566
 91238/100000: episode: 1813, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 113.047, mean reward: 3.426 [1.706, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.339, 10.361], loss: 0.348731, mae: 0.570698, mean_q: 5.619771
 91271/100000: episode: 1814, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 76.868, mean reward: 2.329 [1.618, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.789, 10.224], loss: 0.351983, mae: 0.585770, mean_q: 5.590043
 91304/100000: episode: 1815, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 94.346, mean reward: 2.859 [1.617, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.803, 10.237], loss: 0.375136, mae: 0.593052, mean_q: 5.528488
 91337/100000: episode: 1816, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 113.053, mean reward: 3.426 [2.514, 5.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.562, 10.419], loss: 0.303702, mae: 0.553196, mean_q: 5.550224
 91370/100000: episode: 1817, duration: 0.160s, episode steps: 33, steps per second: 207, episode reward: 83.760, mean reward: 2.538 [1.554, 5.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.014, 10.173], loss: 0.397895, mae: 0.568881, mean_q: 5.569766
 91403/100000: episode: 1818, duration: 0.171s, episode steps: 33, steps per second: 192, episode reward: 98.770, mean reward: 2.993 [2.140, 4.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.085, 10.340], loss: 466.741211, mae: 1.545374, mean_q: 5.695072
[Info] Complete ISplit Iteration
[Info] Levels: [5.4920964, 8.739755, 10.627953]
[Info] Cond. Prob: [0.1, 0.11, 1.0]
[Info] Error Prob: 0.011000000000000001

 91436/100000: episode: 1819, duration: 4.328s, episode steps: 33, steps per second: 8, episode reward: 90.752, mean reward: 2.750 [2.134, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.142, 10.373], loss: 466.049896, mae: 2.384367, mean_q: 6.990079
 91536/100000: episode: 1820, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 195.984, mean reward: 1.960 [1.448, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.592, 10.098], loss: 154.344879, mae: 1.120821, mean_q: 5.959052
 91636/100000: episode: 1821, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.286, mean reward: 1.943 [1.482, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.932, 10.098], loss: 307.746338, mae: 1.873729, mean_q: 6.732500
 91736/100000: episode: 1822, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.015, mean reward: 1.880 [1.446, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.764, 10.325], loss: 0.497745, mae: 0.722548, mean_q: 6.086918
 91836/100000: episode: 1823, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.639, mean reward: 1.816 [1.435, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.411, 10.199], loss: 154.361404, mae: 1.130128, mean_q: 6.140457
 91936/100000: episode: 1824, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.692, mean reward: 1.957 [1.454, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.796, 10.098], loss: 0.399039, mae: 0.646524, mean_q: 5.890814
 92036/100000: episode: 1825, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 182.840, mean reward: 1.828 [1.435, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.320, 10.098], loss: 307.841827, mae: 1.558074, mean_q: 6.187585
 92136/100000: episode: 1826, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 199.914, mean reward: 1.999 [1.459, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.452, 10.098], loss: 154.136887, mae: 1.229375, mean_q: 6.154190
 92236/100000: episode: 1827, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 178.025, mean reward: 1.780 [1.448, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.086, 10.191], loss: 0.457287, mae: 0.634514, mean_q: 5.704327
 92336/100000: episode: 1828, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 197.218, mean reward: 1.972 [1.433, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.386], loss: 0.413279, mae: 0.606233, mean_q: 5.564420
 92436/100000: episode: 1829, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 181.147, mean reward: 1.811 [1.451, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.913, 10.184], loss: 0.344011, mae: 0.574207, mean_q: 5.533772
 92536/100000: episode: 1830, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 218.412, mean reward: 2.184 [1.509, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.875, 10.131], loss: 0.409640, mae: 0.601911, mean_q: 5.497088
 92636/100000: episode: 1831, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.757, mean reward: 2.028 [1.475, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.387], loss: 0.309274, mae: 0.545527, mean_q: 5.362334
 92736/100000: episode: 1832, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 184.286, mean reward: 1.843 [1.468, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.151, 10.098], loss: 0.356847, mae: 0.553365, mean_q: 5.345742
 92836/100000: episode: 1833, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 180.682, mean reward: 1.807 [1.455, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.020, 10.197], loss: 0.333595, mae: 0.533671, mean_q: 5.350547
 92936/100000: episode: 1834, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.574, mean reward: 1.876 [1.447, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.112, 10.317], loss: 0.289306, mae: 0.513464, mean_q: 5.273098
 93036/100000: episode: 1835, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 178.372, mean reward: 1.784 [1.441, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.098], loss: 0.330527, mae: 0.527615, mean_q: 5.293697
 93136/100000: episode: 1836, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 191.711, mean reward: 1.917 [1.442, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.915, 10.098], loss: 0.320616, mae: 0.522120, mean_q: 5.216034
 93236/100000: episode: 1837, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.170, mean reward: 1.912 [1.475, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.292], loss: 0.265974, mae: 0.488062, mean_q: 5.123409
 93336/100000: episode: 1838, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.109, mean reward: 1.861 [1.448, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.654, 10.209], loss: 154.514297, mae: 1.138714, mean_q: 5.489933
 93436/100000: episode: 1839, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.615, mean reward: 1.976 [1.452, 6.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.133], loss: 308.068970, mae: 1.728546, mean_q: 5.987010
 93536/100000: episode: 1840, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.922, mean reward: 1.879 [1.457, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.852, 10.252], loss: 460.669037, mae: 1.956576, mean_q: 6.176508
 93636/100000: episode: 1841, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 190.565, mean reward: 1.906 [1.453, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.666, 10.098], loss: 0.337668, mae: 0.585354, mean_q: 5.315717
 93736/100000: episode: 1842, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.432, mean reward: 1.994 [1.512, 5.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.505, 10.098], loss: 154.177643, mae: 1.227744, mean_q: 5.691101
 93836/100000: episode: 1843, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 204.570, mean reward: 2.046 [1.445, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.687, 10.098], loss: 0.268170, mae: 0.517484, mean_q: 5.132058
 93936/100000: episode: 1844, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 194.865, mean reward: 1.949 [1.491, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.788, 10.100], loss: 461.604919, mae: 2.071908, mean_q: 5.946982
 94036/100000: episode: 1845, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 192.369, mean reward: 1.924 [1.477, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.524, 10.112], loss: 153.747116, mae: 0.949114, mean_q: 5.249810
 94136/100000: episode: 1846, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.432, mean reward: 1.914 [1.443, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.098], loss: 153.628555, mae: 1.108074, mean_q: 5.449151
 94236/100000: episode: 1847, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 183.745, mean reward: 1.837 [1.507, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.260, 10.265], loss: 153.623993, mae: 0.808384, mean_q: 4.943621
 94336/100000: episode: 1848, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.793, mean reward: 1.998 [1.463, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.335], loss: 0.464376, mae: 0.633238, mean_q: 5.075952
 94436/100000: episode: 1849, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.141, mean reward: 1.961 [1.441, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.869, 10.112], loss: 0.237149, mae: 0.450868, mean_q: 4.786555
 94536/100000: episode: 1850, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 188.401, mean reward: 1.884 [1.479, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.920, 10.122], loss: 306.768768, mae: 1.409719, mean_q: 5.057293
 94636/100000: episode: 1851, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 189.371, mean reward: 1.894 [1.495, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.231, 10.098], loss: 0.211924, mae: 0.433090, mean_q: 4.636952
 94736/100000: episode: 1852, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 197.299, mean reward: 1.973 [1.462, 5.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.960, 10.098], loss: 306.744507, mae: 1.640986, mean_q: 5.326096
 94836/100000: episode: 1853, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 201.295, mean reward: 2.013 [1.463, 5.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.668, 10.098], loss: 0.204636, mae: 0.432626, mean_q: 4.632061
 94936/100000: episode: 1854, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 208.368, mean reward: 2.084 [1.455, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.673, 10.187], loss: 0.175415, mae: 0.390586, mean_q: 4.528907
 95036/100000: episode: 1855, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 215.173, mean reward: 2.152 [1.463, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.582, 10.191], loss: 307.009552, mae: 1.499258, mean_q: 5.073257
 95136/100000: episode: 1856, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.861, mean reward: 1.849 [1.474, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.422, 10.230], loss: 0.362844, mae: 0.533250, mean_q: 4.677977
 95236/100000: episode: 1857, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 206.680, mean reward: 2.067 [1.473, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.269], loss: 153.499969, mae: 0.920277, mean_q: 4.656096
 95336/100000: episode: 1858, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.688, mean reward: 1.957 [1.455, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.098], loss: 0.354324, mae: 0.531165, mean_q: 4.507880
 95436/100000: episode: 1859, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 216.479, mean reward: 2.165 [1.442, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.401, 10.103], loss: 0.192687, mae: 0.395286, mean_q: 4.332022
 95536/100000: episode: 1860, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.823, mean reward: 1.868 [1.478, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.113, 10.207], loss: 0.185092, mae: 0.390516, mean_q: 4.279385
 95636/100000: episode: 1861, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.811, mean reward: 1.838 [1.444, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.102], loss: 0.175271, mae: 0.377952, mean_q: 4.220452
 95736/100000: episode: 1862, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.624, mean reward: 1.916 [1.471, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.039, 10.360], loss: 0.118512, mae: 0.330957, mean_q: 4.113969
 95836/100000: episode: 1863, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 188.008, mean reward: 1.880 [1.456, 5.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.823, 10.122], loss: 0.139261, mae: 0.343427, mean_q: 4.070748
 95936/100000: episode: 1864, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 209.306, mean reward: 2.093 [1.503, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.813, 10.098], loss: 0.116984, mae: 0.323590, mean_q: 4.001231
 96036/100000: episode: 1865, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 200.111, mean reward: 2.001 [1.461, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.300, 10.188], loss: 0.113635, mae: 0.324727, mean_q: 3.971094
 96136/100000: episode: 1866, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 190.571, mean reward: 1.906 [1.472, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.474, 10.098], loss: 0.098982, mae: 0.307785, mean_q: 3.948616
 96236/100000: episode: 1867, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: 203.403, mean reward: 2.034 [1.480, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.997, 10.435], loss: 0.099271, mae: 0.301055, mean_q: 3.915854
 96336/100000: episode: 1868, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 209.526, mean reward: 2.095 [1.516, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.098], loss: 0.087485, mae: 0.299501, mean_q: 3.885268
 96436/100000: episode: 1869, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 179.489, mean reward: 1.795 [1.455, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.027, 10.098], loss: 0.091574, mae: 0.294761, mean_q: 3.839349
 96536/100000: episode: 1870, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 191.937, mean reward: 1.919 [1.474, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.849, 10.098], loss: 0.090287, mae: 0.296432, mean_q: 3.845523
 96636/100000: episode: 1871, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 214.832, mean reward: 2.148 [1.498, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.301], loss: 0.102047, mae: 0.306395, mean_q: 3.860623
 96736/100000: episode: 1872, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.576, mean reward: 1.886 [1.464, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.724, 10.098], loss: 0.098148, mae: 0.303292, mean_q: 3.863627
 96836/100000: episode: 1873, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 199.052, mean reward: 1.991 [1.452, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.606, 10.457], loss: 0.113852, mae: 0.316896, mean_q: 3.870644
 96936/100000: episode: 1874, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 191.790, mean reward: 1.918 [1.471, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.510, 10.098], loss: 0.092541, mae: 0.298640, mean_q: 3.868634
 97036/100000: episode: 1875, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 242.320, mean reward: 2.423 [1.514, 6.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.424], loss: 0.091922, mae: 0.293163, mean_q: 3.856326
 97136/100000: episode: 1876, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.778, mean reward: 1.948 [1.433, 7.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.233], loss: 0.095511, mae: 0.303111, mean_q: 3.888831
 97236/100000: episode: 1877, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.706, mean reward: 1.847 [1.475, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.912, 10.199], loss: 0.097922, mae: 0.297034, mean_q: 3.863823
 97336/100000: episode: 1878, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 213.374, mean reward: 2.134 [1.470, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.392, 10.150], loss: 0.106546, mae: 0.305250, mean_q: 3.873090
 97436/100000: episode: 1879, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 199.157, mean reward: 1.992 [1.441, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.559, 10.237], loss: 0.095146, mae: 0.297862, mean_q: 3.886405
 97536/100000: episode: 1880, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 202.866, mean reward: 2.029 [1.443, 5.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.526, 10.283], loss: 0.086849, mae: 0.292612, mean_q: 3.869588
 97636/100000: episode: 1881, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 200.902, mean reward: 2.009 [1.477, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.523, 10.098], loss: 0.090501, mae: 0.293539, mean_q: 3.868935
 97736/100000: episode: 1882, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: 182.860, mean reward: 1.829 [1.439, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.322, 10.291], loss: 0.091165, mae: 0.298508, mean_q: 3.892985
 97836/100000: episode: 1883, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 202.134, mean reward: 2.021 [1.465, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.626, 10.397], loss: 0.110210, mae: 0.307838, mean_q: 3.880741
 97936/100000: episode: 1884, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.853, mean reward: 1.949 [1.459, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.771, 10.098], loss: 0.109402, mae: 0.301500, mean_q: 3.892832
 98036/100000: episode: 1885, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 203.080, mean reward: 2.031 [1.468, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.316, 10.447], loss: 0.098938, mae: 0.300906, mean_q: 3.896668
 98136/100000: episode: 1886, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 178.246, mean reward: 1.782 [1.433, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.199, 10.151], loss: 0.114724, mae: 0.315620, mean_q: 3.913752
 98236/100000: episode: 1887, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 183.671, mean reward: 1.837 [1.442, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.980, 10.098], loss: 0.100307, mae: 0.309107, mean_q: 3.905000
 98336/100000: episode: 1888, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.722, mean reward: 1.847 [1.442, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.677, 10.275], loss: 0.102201, mae: 0.301379, mean_q: 3.909844
 98436/100000: episode: 1889, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 239.913, mean reward: 2.399 [1.480, 11.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.637, 10.098], loss: 0.127454, mae: 0.307905, mean_q: 3.902853
 98536/100000: episode: 1890, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 199.487, mean reward: 1.995 [1.460, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.373, 10.175], loss: 0.111801, mae: 0.304924, mean_q: 3.907383
 98636/100000: episode: 1891, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 237.464, mean reward: 2.375 [1.448, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.631, 10.227], loss: 0.109468, mae: 0.318745, mean_q: 3.931115
 98736/100000: episode: 1892, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.155, mean reward: 1.912 [1.468, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.477, 10.098], loss: 0.096952, mae: 0.304154, mean_q: 3.927993
 98836/100000: episode: 1893, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.523, mean reward: 1.845 [1.466, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.180, 10.295], loss: 0.127690, mae: 0.320377, mean_q: 3.941076
 98936/100000: episode: 1894, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.280, mean reward: 1.923 [1.472, 5.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.676, 10.260], loss: 0.115071, mae: 0.307251, mean_q: 3.943652
 99036/100000: episode: 1895, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.870, mean reward: 1.889 [1.470, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.280, 10.098], loss: 0.097155, mae: 0.302770, mean_q: 3.924645
 99136/100000: episode: 1896, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 221.459, mean reward: 2.215 [1.486, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.496], loss: 0.100230, mae: 0.305046, mean_q: 3.920501
 99236/100000: episode: 1897, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 224.378, mean reward: 2.244 [1.514, 4.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.843, 10.372], loss: 0.122422, mae: 0.318449, mean_q: 3.935457
 99336/100000: episode: 1898, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.320, mean reward: 1.893 [1.459, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.375, 10.161], loss: 0.133089, mae: 0.333394, mean_q: 3.956448
 99436/100000: episode: 1899, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 180.509, mean reward: 1.805 [1.453, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.621, 10.183], loss: 0.124288, mae: 0.318058, mean_q: 3.950245
 99536/100000: episode: 1900, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.319, mean reward: 1.863 [1.518, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.197], loss: 0.115570, mae: 0.309228, mean_q: 3.946597
 99636/100000: episode: 1901, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.337, mean reward: 1.833 [1.449, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.669, 10.218], loss: 0.112066, mae: 0.315928, mean_q: 3.966233
 99736/100000: episode: 1902, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 213.710, mean reward: 2.137 [1.466, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.793, 10.098], loss: 0.117177, mae: 0.318098, mean_q: 3.947843
 99836/100000: episode: 1903, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 199.279, mean reward: 1.993 [1.440, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.374, 10.098], loss: 0.144766, mae: 0.326798, mean_q: 3.951392
 99936/100000: episode: 1904, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.232, mean reward: 1.832 [1.448, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.570, 10.098], loss: 0.118214, mae: 0.316492, mean_q: 3.948674
done, took 590.496 seconds
[Info] End Importance Splitting. Falsification occurred 17 times.
