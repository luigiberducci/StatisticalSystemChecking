Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 574, episode reward: 182.021, mean reward: 1.820 [1.435, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.201], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 206.667, mean reward: 2.067 [1.481, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.780, 10.459], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.062s, episode steps: 100, steps per second: 1613, episode reward: 193.203, mean reward: 1.932 [1.506, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.197, 10.360], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 216.144, mean reward: 2.161 [1.467, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.747, 10.451], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1604, episode reward: 206.591, mean reward: 2.066 [1.538, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.100, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.062s, episode steps: 100, steps per second: 1612, episode reward: 209.514, mean reward: 2.095 [1.442, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.739, 10.265], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.068s, episode steps: 100, steps per second: 1470, episode reward: 201.371, mean reward: 2.014 [1.442, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.474, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 180.678, mean reward: 1.807 [1.440, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.509, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 192.440, mean reward: 1.924 [1.450, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.548, 10.149], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 193.061, mean reward: 1.931 [1.476, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.474, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.062s, episode steps: 100, steps per second: 1616, episode reward: 183.711, mean reward: 1.837 [1.457, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.396, 10.248], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 193.274, mean reward: 1.933 [1.504, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.066, 10.174], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.066s, episode steps: 100, steps per second: 1522, episode reward: 211.981, mean reward: 2.120 [1.534, 5.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.297], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 182.896, mean reward: 1.829 [1.438, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 193.902, mean reward: 1.939 [1.456, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.006, 10.143], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.067s, episode steps: 100, steps per second: 1493, episode reward: 215.922, mean reward: 2.159 [1.510, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.318, 10.376], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 193.329, mean reward: 1.933 [1.444, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.236, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.068s, episode steps: 100, steps per second: 1475, episode reward: 189.832, mean reward: 1.898 [1.477, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.624, 10.420], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.062s, episode steps: 100, steps per second: 1604, episode reward: 191.745, mean reward: 1.917 [1.457, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.031, 10.117], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 189.241, mean reward: 1.892 [1.459, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: 187.359, mean reward: 1.874 [1.454, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.365, 10.295], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 197.488, mean reward: 1.975 [1.493, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 175.987, mean reward: 1.760 [1.441, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.171], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 186.276, mean reward: 1.863 [1.460, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.690, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.073s, episode steps: 100, steps per second: 1367, episode reward: 226.958, mean reward: 2.270 [1.464, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.619, 10.271], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 196.046, mean reward: 1.960 [1.435, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.017, 10.399], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.068s, episode steps: 100, steps per second: 1461, episode reward: 189.572, mean reward: 1.896 [1.455, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.832, 10.165], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: 217.008, mean reward: 2.170 [1.498, 5.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.762, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 187.956, mean reward: 1.880 [1.437, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.935, 10.223], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 187.870, mean reward: 1.879 [1.464, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.020, 10.165], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 194.593, mean reward: 1.946 [1.480, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.403, 10.464], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.062s, episode steps: 100, steps per second: 1613, episode reward: 184.030, mean reward: 1.840 [1.452, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.055, 10.355], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: 195.715, mean reward: 1.957 [1.467, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.669, 10.251], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: 191.052, mean reward: 1.911 [1.473, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.299, 10.346], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.062s, episode steps: 100, steps per second: 1604, episode reward: 197.750, mean reward: 1.978 [1.455, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.542, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.075s, episode steps: 100, steps per second: 1338, episode reward: 194.724, mean reward: 1.947 [1.465, 5.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.618, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.067s, episode steps: 100, steps per second: 1486, episode reward: 187.158, mean reward: 1.872 [1.444, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.556, 10.143], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 186.914, mean reward: 1.869 [1.462, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.494, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 195.080, mean reward: 1.951 [1.489, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.212, 10.139], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 207.608, mean reward: 2.076 [1.493, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 197.300, mean reward: 1.973 [1.464, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.328, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 199.955, mean reward: 2.000 [1.482, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.457, 10.228], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 186.734, mean reward: 1.867 [1.467, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.224, 10.162], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 195.565, mean reward: 1.956 [1.446, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.853, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.068s, episode steps: 100, steps per second: 1481, episode reward: 280.241, mean reward: 2.802 [1.521, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.086, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.068s, episode steps: 100, steps per second: 1467, episode reward: 183.033, mean reward: 1.830 [1.464, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.492, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 197.651, mean reward: 1.977 [1.448, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.944, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.077s, episode steps: 100, steps per second: 1299, episode reward: 191.011, mean reward: 1.910 [1.479, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.116, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.068s, episode steps: 100, steps per second: 1470, episode reward: 207.153, mean reward: 2.072 [1.464, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.105, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.067s, episode steps: 100, steps per second: 1483, episode reward: 183.686, mean reward: 1.837 [1.436, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.155, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 187.072, mean reward: 1.871 [1.445, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.029, 10.198], loss: 0.246916, mae: 0.524624, mean_q: 2.296995
  5200/100000: episode: 52, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.113, mean reward: 1.881 [1.504, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.255, 10.098], loss: 0.120612, mae: 0.347322, mean_q: 3.030890
  5300/100000: episode: 53, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 191.211, mean reward: 1.912 [1.481, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.328, 10.098], loss: 0.102578, mae: 0.320101, mean_q: 3.330641
  5400/100000: episode: 54, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 196.592, mean reward: 1.966 [1.453, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.989, 10.335], loss: 0.099155, mae: 0.310176, mean_q: 3.530750
  5500/100000: episode: 55, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 223.641, mean reward: 2.236 [1.510, 8.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.280, 10.098], loss: 0.104593, mae: 0.315891, mean_q: 3.664916
  5600/100000: episode: 56, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 204.355, mean reward: 2.044 [1.493, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.208, 10.118], loss: 0.124126, mae: 0.330152, mean_q: 3.744791
  5700/100000: episode: 57, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 201.979, mean reward: 2.020 [1.459, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.649, 10.098], loss: 0.099726, mae: 0.317786, mean_q: 3.788710
  5800/100000: episode: 58, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 205.412, mean reward: 2.054 [1.452, 5.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.829, 10.098], loss: 0.109354, mae: 0.325501, mean_q: 3.832804
  5900/100000: episode: 59, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 261.497, mean reward: 2.615 [1.552, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.400, 10.400], loss: 0.114985, mae: 0.324496, mean_q: 3.861472
  6000/100000: episode: 60, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 177.071, mean reward: 1.771 [1.472, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.911, 10.193], loss: 0.125262, mae: 0.345840, mean_q: 3.895022
  6100/100000: episode: 61, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 196.823, mean reward: 1.968 [1.528, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.371, 10.217], loss: 0.112353, mae: 0.323590, mean_q: 3.881552
  6200/100000: episode: 62, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.703, mean reward: 1.907 [1.463, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.128, 10.098], loss: 0.132211, mae: 0.346389, mean_q: 3.894720
  6300/100000: episode: 63, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 189.566, mean reward: 1.896 [1.442, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.044, 10.208], loss: 0.124659, mae: 0.340427, mean_q: 3.894680
  6400/100000: episode: 64, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 192.098, mean reward: 1.921 [1.461, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.252, 10.273], loss: 0.128262, mae: 0.344823, mean_q: 3.897188
  6500/100000: episode: 65, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 194.845, mean reward: 1.948 [1.442, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.714, 10.098], loss: 0.130922, mae: 0.349855, mean_q: 3.925712
  6600/100000: episode: 66, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 192.732, mean reward: 1.927 [1.449, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.252], loss: 0.108965, mae: 0.324038, mean_q: 3.894039
  6700/100000: episode: 67, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 205.760, mean reward: 2.058 [1.479, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.825, 10.098], loss: 0.127143, mae: 0.339360, mean_q: 3.902322
  6800/100000: episode: 68, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.237, mean reward: 1.952 [1.471, 4.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.123, 10.098], loss: 0.114695, mae: 0.330027, mean_q: 3.895596
  6900/100000: episode: 69, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 176.752, mean reward: 1.768 [1.464, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.792, 10.188], loss: 0.118978, mae: 0.337814, mean_q: 3.912728
  7000/100000: episode: 70, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 189.860, mean reward: 1.899 [1.452, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.098], loss: 0.115454, mae: 0.338568, mean_q: 3.897274
  7100/100000: episode: 71, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 179.059, mean reward: 1.791 [1.482, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.782, 10.255], loss: 0.120589, mae: 0.341695, mean_q: 3.891729
  7200/100000: episode: 72, duration: 1.257s, episode steps: 100, steps per second: 80, episode reward: 198.393, mean reward: 1.984 [1.454, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.723, 10.098], loss: 0.118337, mae: 0.332291, mean_q: 3.898875
  7300/100000: episode: 73, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 200.588, mean reward: 2.006 [1.455, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.163, 10.098], loss: 0.116242, mae: 0.335085, mean_q: 3.915356
  7400/100000: episode: 74, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 194.871, mean reward: 1.949 [1.522, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.441, 10.098], loss: 0.124164, mae: 0.337324, mean_q: 3.918279
  7500/100000: episode: 75, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 193.584, mean reward: 1.936 [1.442, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.098], loss: 0.108063, mae: 0.327227, mean_q: 3.916848
  7600/100000: episode: 76, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 197.791, mean reward: 1.978 [1.450, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.150, 10.112], loss: 0.129309, mae: 0.343527, mean_q: 3.919425
  7700/100000: episode: 77, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 211.655, mean reward: 2.117 [1.525, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.801, 10.182], loss: 0.116358, mae: 0.324988, mean_q: 3.902848
  7800/100000: episode: 78, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 214.421, mean reward: 2.144 [1.463, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.472, 10.098], loss: 0.123468, mae: 0.328639, mean_q: 3.910245
  7900/100000: episode: 79, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 210.367, mean reward: 2.104 [1.477, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.730, 10.399], loss: 0.122308, mae: 0.343837, mean_q: 3.911674
  8000/100000: episode: 80, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 192.161, mean reward: 1.922 [1.434, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.007, 10.098], loss: 0.142136, mae: 0.346958, mean_q: 3.911833
  8100/100000: episode: 81, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 182.750, mean reward: 1.828 [1.442, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.774, 10.098], loss: 0.114151, mae: 0.332513, mean_q: 3.920495
  8200/100000: episode: 82, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 194.856, mean reward: 1.949 [1.466, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.449, 10.098], loss: 0.118126, mae: 0.327390, mean_q: 3.921902
  8300/100000: episode: 83, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 184.856, mean reward: 1.849 [1.457, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.168, 10.116], loss: 0.121000, mae: 0.333232, mean_q: 3.913936
  8400/100000: episode: 84, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 196.762, mean reward: 1.968 [1.434, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.437, 10.098], loss: 0.109350, mae: 0.318098, mean_q: 3.905208
  8500/100000: episode: 85, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.316, mean reward: 1.913 [1.451, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.028, 10.222], loss: 0.120839, mae: 0.331507, mean_q: 3.907570
  8600/100000: episode: 86, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 202.342, mean reward: 2.023 [1.455, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.659, 10.206], loss: 0.128352, mae: 0.338758, mean_q: 3.915089
  8700/100000: episode: 87, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 194.356, mean reward: 1.944 [1.495, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.042, 10.244], loss: 0.117056, mae: 0.332981, mean_q: 3.919330
  8800/100000: episode: 88, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 197.314, mean reward: 1.973 [1.481, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.795, 10.098], loss: 0.118540, mae: 0.333079, mean_q: 3.925227
  8900/100000: episode: 89, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 196.164, mean reward: 1.962 [1.496, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.819, 10.098], loss: 0.138598, mae: 0.350883, mean_q: 3.928632
  9000/100000: episode: 90, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 184.372, mean reward: 1.844 [1.444, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.738, 10.153], loss: 0.122529, mae: 0.327353, mean_q: 3.919474
  9100/100000: episode: 91, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 199.943, mean reward: 1.999 [1.513, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.883, 10.098], loss: 0.120147, mae: 0.324936, mean_q: 3.903678
  9200/100000: episode: 92, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 189.582, mean reward: 1.896 [1.465, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.515, 10.300], loss: 0.110640, mae: 0.323408, mean_q: 3.908804
  9300/100000: episode: 93, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 200.047, mean reward: 2.000 [1.481, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.471, 10.382], loss: 0.106893, mae: 0.320541, mean_q: 3.898351
  9400/100000: episode: 94, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 183.264, mean reward: 1.833 [1.467, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.739, 10.230], loss: 0.108733, mae: 0.315344, mean_q: 3.910031
  9500/100000: episode: 95, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 197.327, mean reward: 1.973 [1.540, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.583, 10.235], loss: 0.111991, mae: 0.319315, mean_q: 3.907597
  9600/100000: episode: 96, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 181.304, mean reward: 1.813 [1.449, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.537, 10.197], loss: 0.115342, mae: 0.325913, mean_q: 3.898607
  9700/100000: episode: 97, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 199.083, mean reward: 1.991 [1.510, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.400, 10.168], loss: 0.103383, mae: 0.305675, mean_q: 3.871701
  9800/100000: episode: 98, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.682, mean reward: 1.937 [1.474, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.391, 10.310], loss: 0.103880, mae: 0.313713, mean_q: 3.890627
  9900/100000: episode: 99, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 195.794, mean reward: 1.958 [1.489, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.104], loss: 0.095257, mae: 0.304735, mean_q: 3.868690
 10000/100000: episode: 100, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 204.423, mean reward: 2.044 [1.443, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.098], loss: 0.096102, mae: 0.305878, mean_q: 3.893451
 10100/100000: episode: 101, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.488, mean reward: 2.005 [1.448, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.340, 10.122], loss: 0.101505, mae: 0.307935, mean_q: 3.864936
 10200/100000: episode: 102, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 199.951, mean reward: 2.000 [1.476, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.829, 10.098], loss: 0.094817, mae: 0.304722, mean_q: 3.878273
 10300/100000: episode: 103, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 196.985, mean reward: 1.970 [1.465, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.738, 10.178], loss: 0.095208, mae: 0.305100, mean_q: 3.877839
 10400/100000: episode: 104, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 188.461, mean reward: 1.885 [1.438, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.591, 10.177], loss: 0.104519, mae: 0.316820, mean_q: 3.902493
 10500/100000: episode: 105, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 195.329, mean reward: 1.953 [1.487, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.521, 10.300], loss: 0.105065, mae: 0.322031, mean_q: 3.900793
 10600/100000: episode: 106, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 191.879, mean reward: 1.919 [1.467, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.214], loss: 0.098231, mae: 0.310326, mean_q: 3.890243
 10700/100000: episode: 107, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 217.052, mean reward: 2.171 [1.469, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.578, 10.098], loss: 0.096665, mae: 0.308137, mean_q: 3.875973
 10800/100000: episode: 108, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 214.388, mean reward: 2.144 [1.528, 6.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.491, 10.318], loss: 0.093195, mae: 0.307741, mean_q: 3.866137
 10900/100000: episode: 109, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 186.515, mean reward: 1.865 [1.448, 4.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.435, 10.098], loss: 0.096532, mae: 0.302066, mean_q: 3.876163
 11000/100000: episode: 110, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 194.451, mean reward: 1.945 [1.462, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.055, 10.277], loss: 0.100632, mae: 0.309406, mean_q: 3.868411
 11100/100000: episode: 111, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 182.446, mean reward: 1.824 [1.466, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.729, 10.309], loss: 0.101780, mae: 0.308883, mean_q: 3.866076
 11200/100000: episode: 112, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 199.530, mean reward: 1.995 [1.497, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.621, 10.098], loss: 0.104750, mae: 0.313249, mean_q: 3.881934
 11300/100000: episode: 113, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 187.250, mean reward: 1.873 [1.505, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.781, 10.261], loss: 0.090917, mae: 0.301644, mean_q: 3.878887
 11400/100000: episode: 114, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 200.763, mean reward: 2.008 [1.489, 6.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.302, 10.477], loss: 0.101453, mae: 0.306609, mean_q: 3.874235
 11500/100000: episode: 115, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 191.188, mean reward: 1.912 [1.499, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.756, 10.098], loss: 0.092388, mae: 0.298471, mean_q: 3.887946
 11600/100000: episode: 116, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 202.401, mean reward: 2.024 [1.541, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.273, 10.222], loss: 0.090195, mae: 0.294051, mean_q: 3.863201
 11700/100000: episode: 117, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 203.081, mean reward: 2.031 [1.479, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.394, 10.166], loss: 0.087400, mae: 0.286043, mean_q: 3.857683
 11800/100000: episode: 118, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 195.345, mean reward: 1.953 [1.505, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.450, 10.309], loss: 0.091499, mae: 0.292751, mean_q: 3.872833
 11900/100000: episode: 119, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 195.644, mean reward: 1.956 [1.468, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.165], loss: 0.097904, mae: 0.302730, mean_q: 3.861173
 12000/100000: episode: 120, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 178.008, mean reward: 1.780 [1.445, 2.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.656, 10.098], loss: 0.102115, mae: 0.315136, mean_q: 3.876072
 12100/100000: episode: 121, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 199.481, mean reward: 1.995 [1.438, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.525, 10.171], loss: 0.098821, mae: 0.304346, mean_q: 3.873620
 12200/100000: episode: 122, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 188.082, mean reward: 1.881 [1.481, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.368, 10.098], loss: 0.086257, mae: 0.297721, mean_q: 3.862943
 12300/100000: episode: 123, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 177.059, mean reward: 1.771 [1.468, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.341, 10.258], loss: 0.094154, mae: 0.301804, mean_q: 3.872410
 12400/100000: episode: 124, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 204.598, mean reward: 2.046 [1.486, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.271], loss: 0.082169, mae: 0.283036, mean_q: 3.858703
 12500/100000: episode: 125, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 189.030, mean reward: 1.890 [1.467, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.747, 10.313], loss: 0.093551, mae: 0.302063, mean_q: 3.878316
 12600/100000: episode: 126, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 197.788, mean reward: 1.978 [1.456, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.342, 10.098], loss: 0.090685, mae: 0.292367, mean_q: 3.867982
 12700/100000: episode: 127, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 197.598, mean reward: 1.976 [1.483, 4.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.919, 10.214], loss: 0.096967, mae: 0.293236, mean_q: 3.863081
 12800/100000: episode: 128, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 191.929, mean reward: 1.919 [1.495, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.602, 10.098], loss: 0.081802, mae: 0.285535, mean_q: 3.849582
 12900/100000: episode: 129, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 192.005, mean reward: 1.920 [1.443, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.862, 10.098], loss: 0.091448, mae: 0.295247, mean_q: 3.867183
 13000/100000: episode: 130, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 194.273, mean reward: 1.943 [1.442, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.953, 10.166], loss: 0.084202, mae: 0.279903, mean_q: 3.843839
 13100/100000: episode: 131, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 194.164, mean reward: 1.942 [1.486, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.184, 10.322], loss: 0.080137, mae: 0.280411, mean_q: 3.849486
 13200/100000: episode: 132, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 187.090, mean reward: 1.871 [1.512, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.198, 10.243], loss: 0.086956, mae: 0.285841, mean_q: 3.841196
 13300/100000: episode: 133, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 190.779, mean reward: 1.908 [1.480, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.225, 10.133], loss: 0.083060, mae: 0.282839, mean_q: 3.843187
 13400/100000: episode: 134, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 204.555, mean reward: 2.046 [1.465, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.098], loss: 0.086531, mae: 0.289946, mean_q: 3.866802
 13500/100000: episode: 135, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 208.724, mean reward: 2.087 [1.468, 5.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.696, 10.204], loss: 0.084685, mae: 0.288563, mean_q: 3.871572
 13600/100000: episode: 136, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 190.538, mean reward: 1.905 [1.488, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.262, 10.264], loss: 0.089559, mae: 0.289486, mean_q: 3.846946
 13700/100000: episode: 137, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 210.064, mean reward: 2.101 [1.447, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.284], loss: 0.087066, mae: 0.283938, mean_q: 3.860633
 13800/100000: episode: 138, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 192.327, mean reward: 1.923 [1.478, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.945, 10.098], loss: 0.086685, mae: 0.288907, mean_q: 3.864346
 13900/100000: episode: 139, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 187.098, mean reward: 1.871 [1.442, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.931, 10.098], loss: 0.086554, mae: 0.285380, mean_q: 3.864371
 14000/100000: episode: 140, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 189.208, mean reward: 1.892 [1.497, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.224, 10.098], loss: 0.087860, mae: 0.278367, mean_q: 3.863688
 14100/100000: episode: 141, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 194.032, mean reward: 1.940 [1.457, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.169, 10.098], loss: 0.077427, mae: 0.278747, mean_q: 3.867025
 14200/100000: episode: 142, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 187.424, mean reward: 1.874 [1.452, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.537, 10.098], loss: 0.085466, mae: 0.290555, mean_q: 3.861892
 14300/100000: episode: 143, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 204.588, mean reward: 2.046 [1.469, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.052, 10.098], loss: 0.087600, mae: 0.286846, mean_q: 3.848336
 14400/100000: episode: 144, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 196.443, mean reward: 1.964 [1.471, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.122, 10.098], loss: 0.082953, mae: 0.289166, mean_q: 3.871124
 14500/100000: episode: 145, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 188.822, mean reward: 1.888 [1.458, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.284], loss: 0.079460, mae: 0.278913, mean_q: 3.843747
 14600/100000: episode: 146, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 196.393, mean reward: 1.964 [1.487, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.843, 10.098], loss: 0.082010, mae: 0.283567, mean_q: 3.853321
 14700/100000: episode: 147, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 190.981, mean reward: 1.910 [1.459, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.564, 10.161], loss: 0.079958, mae: 0.284745, mean_q: 3.860738
 14800/100000: episode: 148, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 187.945, mean reward: 1.879 [1.491, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.755, 10.191], loss: 0.070152, mae: 0.268274, mean_q: 3.842356
 14900/100000: episode: 149, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 185.353, mean reward: 1.854 [1.452, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.162, 10.156], loss: 0.083273, mae: 0.289300, mean_q: 3.842263
[Info] 1-TH LEVEL FOUND: 4.766300201416016, Considering 10/90 traces
 15000/100000: episode: 150, duration: 7.176s, episode steps: 100, steps per second: 14, episode reward: 191.363, mean reward: 1.914 [1.446, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.038, 10.296], loss: 0.087226, mae: 0.286178, mean_q: 3.843799
 15033/100000: episode: 151, duration: 0.256s, episode steps: 33, steps per second: 129, episode reward: 78.448, mean reward: 2.377 [1.886, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.042, 10.100], loss: 0.092620, mae: 0.287724, mean_q: 3.870480
 15083/100000: episode: 152, duration: 0.393s, episode steps: 50, steps per second: 127, episode reward: 105.688, mean reward: 2.114 [1.628, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.582, 10.238], loss: 0.078687, mae: 0.283410, mean_q: 3.871240
 15116/100000: episode: 153, duration: 0.271s, episode steps: 33, steps per second: 122, episode reward: 106.272, mean reward: 3.220 [2.236, 5.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.589, 10.100], loss: 0.070949, mae: 0.270139, mean_q: 3.867402
 15163/100000: episode: 154, duration: 0.361s, episode steps: 47, steps per second: 130, episode reward: 104.065, mean reward: 2.214 [1.472, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.244, 10.309], loss: 0.080507, mae: 0.279507, mean_q: 3.828342
 15212/100000: episode: 155, duration: 0.373s, episode steps: 49, steps per second: 131, episode reward: 96.462, mean reward: 1.969 [1.472, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.317, 10.100], loss: 0.083724, mae: 0.286391, mean_q: 3.862521
 15262/100000: episode: 156, duration: 0.399s, episode steps: 50, steps per second: 125, episode reward: 101.033, mean reward: 2.021 [1.577, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.848, 10.100], loss: 0.084135, mae: 0.291290, mean_q: 3.866975
 15277/100000: episode: 157, duration: 0.120s, episode steps: 15, steps per second: 125, episode reward: 49.180, mean reward: 3.279 [2.669, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.472, 10.100], loss: 0.079427, mae: 0.278988, mean_q: 3.845252
 15310/100000: episode: 158, duration: 0.259s, episode steps: 33, steps per second: 128, episode reward: 77.170, mean reward: 2.338 [1.814, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.591, 10.100], loss: 0.106020, mae: 0.310513, mean_q: 3.887839
 15333/100000: episode: 159, duration: 0.172s, episode steps: 23, steps per second: 134, episode reward: 49.033, mean reward: 2.132 [1.637, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.103, 10.100], loss: 0.105404, mae: 0.301190, mean_q: 3.873636
 15383/100000: episode: 160, duration: 0.392s, episode steps: 50, steps per second: 127, episode reward: 101.520, mean reward: 2.030 [1.503, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.117, 10.100], loss: 0.093717, mae: 0.301693, mean_q: 3.893295
 15435/100000: episode: 161, duration: 0.405s, episode steps: 52, steps per second: 128, episode reward: 92.469, mean reward: 1.778 [1.463, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.343, 10.100], loss: 0.118073, mae: 0.316860, mean_q: 3.916706
 15468/100000: episode: 162, duration: 0.255s, episode steps: 33, steps per second: 129, episode reward: 89.415, mean reward: 2.710 [2.095, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.149, 10.100], loss: 0.092010, mae: 0.309595, mean_q: 3.875355
 15518/100000: episode: 163, duration: 0.403s, episode steps: 50, steps per second: 124, episode reward: 98.335, mean reward: 1.967 [1.462, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.278, 10.212], loss: 0.087847, mae: 0.289527, mean_q: 3.867114
 15551/100000: episode: 164, duration: 0.264s, episode steps: 33, steps per second: 125, episode reward: 75.901, mean reward: 2.300 [1.505, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.253, 10.100], loss: 0.088138, mae: 0.287386, mean_q: 3.891939
 15576/100000: episode: 165, duration: 0.200s, episode steps: 25, steps per second: 125, episode reward: 69.441, mean reward: 2.778 [2.207, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.533, 10.100], loss: 0.094440, mae: 0.290852, mean_q: 3.869285
 15601/100000: episode: 166, duration: 0.200s, episode steps: 25, steps per second: 125, episode reward: 67.188, mean reward: 2.688 [2.066, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.280, 10.100], loss: 0.070326, mae: 0.272067, mean_q: 3.887320
 15648/100000: episode: 167, duration: 0.371s, episode steps: 47, steps per second: 127, episode reward: 93.084, mean reward: 1.981 [1.572, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.485, 10.305], loss: 0.105716, mae: 0.299972, mean_q: 3.909616
 15671/100000: episode: 168, duration: 0.187s, episode steps: 23, steps per second: 123, episode reward: 61.791, mean reward: 2.687 [1.801, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.406, 10.100], loss: 0.104154, mae: 0.303156, mean_q: 3.860841
 15704/100000: episode: 169, duration: 0.269s, episode steps: 33, steps per second: 123, episode reward: 84.455, mean reward: 2.559 [2.014, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.241, 10.100], loss: 0.079082, mae: 0.282852, mean_q: 3.917736
 15729/100000: episode: 170, duration: 0.201s, episode steps: 25, steps per second: 124, episode reward: 77.986, mean reward: 3.119 [2.396, 5.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.262, 10.100], loss: 0.105392, mae: 0.321302, mean_q: 3.945064
 15776/100000: episode: 171, duration: 0.361s, episode steps: 47, steps per second: 130, episode reward: 97.951, mean reward: 2.084 [1.446, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.578, 10.235], loss: 0.097580, mae: 0.301923, mean_q: 3.889307
 15809/100000: episode: 172, duration: 0.263s, episode steps: 33, steps per second: 125, episode reward: 84.255, mean reward: 2.553 [1.472, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.177, 10.100], loss: 0.094254, mae: 0.301426, mean_q: 3.934272
 15856/100000: episode: 173, duration: 0.391s, episode steps: 47, steps per second: 120, episode reward: 98.287, mean reward: 2.091 [1.500, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.232, 10.108], loss: 0.094097, mae: 0.296300, mean_q: 3.924990
 15906/100000: episode: 174, duration: 0.384s, episode steps: 50, steps per second: 130, episode reward: 95.168, mean reward: 1.903 [1.475, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.253, 10.201], loss: 0.097704, mae: 0.301591, mean_q: 3.911256
 15953/100000: episode: 175, duration: 0.365s, episode steps: 47, steps per second: 129, episode reward: 92.506, mean reward: 1.968 [1.540, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-1.562, 10.100], loss: 0.079108, mae: 0.286690, mean_q: 3.902980
 16000/100000: episode: 176, duration: 0.340s, episode steps: 47, steps per second: 138, episode reward: 95.268, mean reward: 2.027 [1.605, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.479, 10.356], loss: 0.088128, mae: 0.287882, mean_q: 3.936495
 16025/100000: episode: 177, duration: 0.207s, episode steps: 25, steps per second: 121, episode reward: 72.676, mean reward: 2.907 [2.476, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.232, 10.100], loss: 0.087191, mae: 0.273859, mean_q: 3.897771
 16048/100000: episode: 178, duration: 0.193s, episode steps: 23, steps per second: 119, episode reward: 62.117, mean reward: 2.701 [1.816, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.344, 10.100], loss: 0.091363, mae: 0.319358, mean_q: 3.975579
 16063/100000: episode: 179, duration: 0.119s, episode steps: 15, steps per second: 126, episode reward: 44.106, mean reward: 2.940 [2.407, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.336, 10.100], loss: 0.086762, mae: 0.297939, mean_q: 3.971033
 16088/100000: episode: 180, duration: 0.190s, episode steps: 25, steps per second: 131, episode reward: 66.936, mean reward: 2.677 [2.306, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.662, 10.100], loss: 0.097204, mae: 0.304790, mean_q: 3.925025
 16137/100000: episode: 181, duration: 0.371s, episode steps: 49, steps per second: 132, episode reward: 105.783, mean reward: 2.159 [1.498, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.535, 10.211], loss: 0.078247, mae: 0.278100, mean_q: 3.953887
 16170/100000: episode: 182, duration: 0.270s, episode steps: 33, steps per second: 122, episode reward: 114.219, mean reward: 3.461 [2.184, 8.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.369, 10.100], loss: 0.087499, mae: 0.290426, mean_q: 3.949910
 16222/100000: episode: 183, duration: 0.366s, episode steps: 52, steps per second: 142, episode reward: 138.818, mean reward: 2.670 [1.558, 6.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.012, 10.105], loss: 0.105678, mae: 0.300788, mean_q: 3.970024
 16271/100000: episode: 184, duration: 0.267s, episode steps: 49, steps per second: 183, episode reward: 114.811, mean reward: 2.343 [1.527, 10.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.209, 10.100], loss: 0.133678, mae: 0.320444, mean_q: 3.979405
 16296/100000: episode: 185, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 73.119, mean reward: 2.925 [1.997, 5.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.359, 10.100], loss: 0.107952, mae: 0.317891, mean_q: 4.006818
 16319/100000: episode: 186, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 48.463, mean reward: 2.107 [1.895, 2.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.204, 10.100], loss: 0.128192, mae: 0.307603, mean_q: 3.978690
 16342/100000: episode: 187, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 52.441, mean reward: 2.280 [1.559, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.864, 10.100], loss: 0.104936, mae: 0.315441, mean_q: 3.967986
 16365/100000: episode: 188, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 64.055, mean reward: 2.785 [2.193, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.373, 10.100], loss: 0.108752, mae: 0.322918, mean_q: 4.034969
 16412/100000: episode: 189, duration: 0.283s, episode steps: 47, steps per second: 166, episode reward: 105.778, mean reward: 2.251 [1.627, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.193, 10.202], loss: 0.096599, mae: 0.312135, mean_q: 4.013334
 16464/100000: episode: 190, duration: 0.331s, episode steps: 52, steps per second: 157, episode reward: 177.667, mean reward: 3.417 [2.129, 9.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.812, 10.378], loss: 0.123009, mae: 0.317281, mean_q: 4.000440
 16513/100000: episode: 191, duration: 0.353s, episode steps: 49, steps per second: 139, episode reward: 109.358, mean reward: 2.232 [1.595, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.336, 10.176], loss: 0.149627, mae: 0.345620, mean_q: 4.065482
 16546/100000: episode: 192, duration: 0.254s, episode steps: 33, steps per second: 130, episode reward: 98.416, mean reward: 2.982 [2.206, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.400, 10.100], loss: 0.116950, mae: 0.330305, mean_q: 4.053502
 16569/100000: episode: 193, duration: 0.190s, episode steps: 23, steps per second: 121, episode reward: 57.314, mean reward: 2.492 [1.884, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.567, 10.100], loss: 0.110557, mae: 0.309999, mean_q: 4.093034
 16594/100000: episode: 194, duration: 0.194s, episode steps: 25, steps per second: 129, episode reward: 86.132, mean reward: 3.445 [2.332, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.843, 10.100], loss: 0.156739, mae: 0.346332, mean_q: 4.068254
 16627/100000: episode: 195, duration: 0.266s, episode steps: 33, steps per second: 124, episode reward: 91.782, mean reward: 2.781 [1.776, 8.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.262, 10.100], loss: 0.098074, mae: 0.318786, mean_q: 4.071426
 16676/100000: episode: 196, duration: 0.363s, episode steps: 49, steps per second: 135, episode reward: 110.427, mean reward: 2.254 [1.566, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.329, 10.100], loss: 0.093773, mae: 0.306344, mean_q: 4.051548
 16723/100000: episode: 197, duration: 0.367s, episode steps: 47, steps per second: 128, episode reward: 111.598, mean reward: 2.374 [1.701, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.684, 10.392], loss: 0.127128, mae: 0.332185, mean_q: 4.093222
 16756/100000: episode: 198, duration: 0.245s, episode steps: 33, steps per second: 135, episode reward: 83.088, mean reward: 2.518 [1.734, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.203, 10.100], loss: 0.182604, mae: 0.372938, mean_q: 4.139677
 16789/100000: episode: 199, duration: 0.261s, episode steps: 33, steps per second: 126, episode reward: 96.873, mean reward: 2.936 [1.980, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.604, 10.100], loss: 0.127571, mae: 0.347956, mean_q: 4.130583
 16836/100000: episode: 200, duration: 0.383s, episode steps: 47, steps per second: 123, episode reward: 111.408, mean reward: 2.370 [1.559, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.515, 10.425], loss: 0.114815, mae: 0.341193, mean_q: 4.143173
 16886/100000: episode: 201, duration: 0.371s, episode steps: 50, steps per second: 135, episode reward: 113.960, mean reward: 2.279 [1.624, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.276, 10.202], loss: 0.143360, mae: 0.343463, mean_q: 4.162456
 16901/100000: episode: 202, duration: 0.112s, episode steps: 15, steps per second: 134, episode reward: 50.640, mean reward: 3.376 [2.764, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.127, 10.100], loss: 0.214988, mae: 0.368920, mean_q: 4.136563
 16948/100000: episode: 203, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 100.626, mean reward: 2.141 [1.626, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.741, 10.293], loss: 0.128362, mae: 0.332543, mean_q: 4.189130
 17000/100000: episode: 204, duration: 0.327s, episode steps: 52, steps per second: 159, episode reward: 135.230, mean reward: 2.601 [2.004, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.635, 10.290], loss: 0.131121, mae: 0.339559, mean_q: 4.157500
 17033/100000: episode: 205, duration: 0.244s, episode steps: 33, steps per second: 135, episode reward: 99.323, mean reward: 3.010 [2.249, 5.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.475, 10.100], loss: 0.122636, mae: 0.339566, mean_q: 4.178875
 17083/100000: episode: 206, duration: 0.376s, episode steps: 50, steps per second: 133, episode reward: 126.478, mean reward: 2.530 [1.467, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.863, 10.166], loss: 0.151511, mae: 0.343642, mean_q: 4.171076
 17106/100000: episode: 207, duration: 0.191s, episode steps: 23, steps per second: 120, episode reward: 54.763, mean reward: 2.381 [1.925, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.617, 10.100], loss: 0.155386, mae: 0.371782, mean_q: 4.213011
 17139/100000: episode: 208, duration: 0.254s, episode steps: 33, steps per second: 130, episode reward: 89.224, mean reward: 2.704 [1.892, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.119, 10.100], loss: 0.104991, mae: 0.323719, mean_q: 4.188910
 17154/100000: episode: 209, duration: 0.113s, episode steps: 15, steps per second: 132, episode reward: 53.361, mean reward: 3.557 [2.698, 6.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.698, 10.100], loss: 0.104020, mae: 0.321303, mean_q: 4.149130
 17169/100000: episode: 210, duration: 0.123s, episode steps: 15, steps per second: 122, episode reward: 48.544, mean reward: 3.236 [2.492, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.676, 10.100], loss: 0.106618, mae: 0.323264, mean_q: 4.262203
 17184/100000: episode: 211, duration: 0.128s, episode steps: 15, steps per second: 117, episode reward: 45.141, mean reward: 3.009 [2.492, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.370, 10.100], loss: 0.098900, mae: 0.325240, mean_q: 4.279974
 17234/100000: episode: 212, duration: 0.373s, episode steps: 50, steps per second: 134, episode reward: 96.648, mean reward: 1.933 [1.523, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.253, 10.221], loss: 0.115974, mae: 0.337773, mean_q: 4.239815
 17284/100000: episode: 213, duration: 0.399s, episode steps: 50, steps per second: 125, episode reward: 122.765, mean reward: 2.455 [1.680, 6.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.879, 10.268], loss: 0.119676, mae: 0.331651, mean_q: 4.202089
 17334/100000: episode: 214, duration: 0.383s, episode steps: 50, steps per second: 131, episode reward: 108.849, mean reward: 2.177 [1.523, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.943, 10.142], loss: 0.154008, mae: 0.353009, mean_q: 4.208647
 17383/100000: episode: 215, duration: 0.393s, episode steps: 49, steps per second: 125, episode reward: 121.457, mean reward: 2.479 [1.603, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.079, 10.207], loss: 0.119725, mae: 0.353842, mean_q: 4.237265
 17433/100000: episode: 216, duration: 0.392s, episode steps: 50, steps per second: 128, episode reward: 111.004, mean reward: 2.220 [1.556, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.439, 10.464], loss: 0.134037, mae: 0.352801, mean_q: 4.246892
 17458/100000: episode: 217, duration: 0.184s, episode steps: 25, steps per second: 136, episode reward: 65.341, mean reward: 2.614 [2.002, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.080, 10.100], loss: 0.110422, mae: 0.341695, mean_q: 4.278133
 17483/100000: episode: 218, duration: 0.203s, episode steps: 25, steps per second: 123, episode reward: 79.151, mean reward: 3.166 [2.004, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.185, 10.100], loss: 0.197585, mae: 0.392439, mean_q: 4.303968
 17516/100000: episode: 219, duration: 0.248s, episode steps: 33, steps per second: 133, episode reward: 120.730, mean reward: 3.658 [2.217, 6.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.191, 10.100], loss: 0.150038, mae: 0.374192, mean_q: 4.299780
 17549/100000: episode: 220, duration: 0.247s, episode steps: 33, steps per second: 134, episode reward: 81.547, mean reward: 2.471 [1.858, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.336, 10.100], loss: 0.130176, mae: 0.350402, mean_q: 4.294806
 17599/100000: episode: 221, duration: 0.402s, episode steps: 50, steps per second: 124, episode reward: 102.942, mean reward: 2.059 [1.457, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.541, 10.174], loss: 0.145476, mae: 0.363973, mean_q: 4.294019
 17632/100000: episode: 222, duration: 0.249s, episode steps: 33, steps per second: 133, episode reward: 95.804, mean reward: 2.903 [1.992, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.100], loss: 0.149647, mae: 0.373087, mean_q: 4.249608
[Info] FALSIFICATION!
 17633/100000: episode: 223, duration: 0.494s, episode steps: 1, steps per second: 2, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.556 [-0.258, 5.792], loss: 0.116965, mae: 0.387762, mean_q: 4.328309
 17680/100000: episode: 224, duration: 0.392s, episode steps: 47, steps per second: 120, episode reward: 104.443, mean reward: 2.222 [1.577, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.245, 10.303], loss: 0.198307, mae: 0.394217, mean_q: 4.290038
 17695/100000: episode: 225, duration: 0.124s, episode steps: 15, steps per second: 121, episode reward: 42.749, mean reward: 2.850 [2.493, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.200, 10.100], loss: 1030.976929, mae: 2.825300, mean_q: 4.751297
 17720/100000: episode: 226, duration: 0.204s, episode steps: 25, steps per second: 122, episode reward: 75.390, mean reward: 3.016 [2.389, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.226, 10.100], loss: 617.697754, mae: 4.090450, mean_q: 4.176487
 17735/100000: episode: 227, duration: 0.129s, episode steps: 15, steps per second: 116, episode reward: 56.230, mean reward: 3.749 [2.451, 6.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.207, 10.100], loss: 2.349390, mae: 1.658884, mean_q: 4.832621
 17768/100000: episode: 228, duration: 0.241s, episode steps: 33, steps per second: 137, episode reward: 92.635, mean reward: 2.807 [2.259, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.638, 10.100], loss: 0.507784, mae: 0.729226, mean_q: 4.531859
 17815/100000: episode: 229, duration: 0.293s, episode steps: 47, steps per second: 161, episode reward: 86.354, mean reward: 1.837 [1.463, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.795, 10.114], loss: 0.251363, mae: 0.510642, mean_q: 4.519533
 17840/100000: episode: 230, duration: 0.189s, episode steps: 25, steps per second: 132, episode reward: 92.875, mean reward: 3.715 [2.627, 6.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.455, 10.100], loss: 0.201257, mae: 0.468491, mean_q: 4.365680
 17855/100000: episode: 231, duration: 0.119s, episode steps: 15, steps per second: 126, episode reward: 43.904, mean reward: 2.927 [2.413, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.964, 10.100], loss: 0.231148, mae: 0.494757, mean_q: 4.227711
 17907/100000: episode: 232, duration: 0.385s, episode steps: 52, steps per second: 135, episode reward: 126.137, mean reward: 2.426 [1.984, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.343, 10.306], loss: 0.252366, mae: 0.507747, mean_q: 4.324406
 17940/100000: episode: 233, duration: 0.266s, episode steps: 33, steps per second: 124, episode reward: 77.116, mean reward: 2.337 [1.501, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.180, 10.234], loss: 0.264550, mae: 0.527912, mean_q: 4.400780
 17955/100000: episode: 234, duration: 0.127s, episode steps: 15, steps per second: 118, episode reward: 47.056, mean reward: 3.137 [2.369, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.275, 10.100], loss: 0.189375, mae: 0.459042, mean_q: 4.313469
 17978/100000: episode: 235, duration: 0.175s, episode steps: 23, steps per second: 132, episode reward: 53.472, mean reward: 2.325 [1.957, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.751, 10.100], loss: 673.326538, mae: 2.887247, mean_q: 5.745147
 18028/100000: episode: 236, duration: 0.386s, episode steps: 50, steps per second: 130, episode reward: 143.199, mean reward: 2.864 [1.745, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.739, 10.400], loss: 615.545532, mae: 2.734524, mean_q: 5.597975
 18061/100000: episode: 237, duration: 0.248s, episode steps: 33, steps per second: 133, episode reward: 75.279, mean reward: 2.281 [1.896, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.050, 10.100], loss: 0.795207, mae: 0.944509, mean_q: 4.343792
 18113/100000: episode: 238, duration: 0.402s, episode steps: 52, steps per second: 129, episode reward: 114.355, mean reward: 2.199 [1.535, 5.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.586, 10.197], loss: 0.462611, mae: 0.697066, mean_q: 4.604793
 18128/100000: episode: 239, duration: 0.120s, episode steps: 15, steps per second: 125, episode reward: 38.606, mean reward: 2.574 [2.113, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.595, 10.100], loss: 0.349082, mae: 0.636491, mean_q: 4.554022
[Info] Complete ISplit Iteration
[Info] Levels: [4.7663, 8.494232]
[Info] Cond. Prob: [0.1, 0.06]
[Info] Error Prob: 0.006

 18151/100000: episode: 240, duration: 5.273s, episode steps: 23, steps per second: 4, episode reward: 57.715, mean reward: 2.509 [1.817, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.599, 10.100], loss: 0.308205, mae: 0.558466, mean_q: 4.598794
 18251/100000: episode: 241, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 183.897, mean reward: 1.839 [1.460, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.940, 10.260], loss: 308.875397, mae: 1.828071, mean_q: 5.000687
 18351/100000: episode: 242, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 186.085, mean reward: 1.861 [1.469, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.554, 10.098], loss: 307.760132, mae: 1.750567, mean_q: 5.204537
 18451/100000: episode: 243, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 216.518, mean reward: 2.165 [1.503, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.617, 10.098], loss: 153.887253, mae: 1.155003, mean_q: 4.930502
 18551/100000: episode: 244, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 193.749, mean reward: 1.937 [1.463, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.098], loss: 0.296504, mae: 0.530700, mean_q: 4.678653
 18651/100000: episode: 245, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 212.492, mean reward: 2.125 [1.474, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.749, 10.098], loss: 0.283441, mae: 0.510694, mean_q: 4.577662
 18751/100000: episode: 246, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 199.279, mean reward: 1.993 [1.499, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.382, 10.257], loss: 154.718033, mae: 0.917544, mean_q: 4.694489
 18851/100000: episode: 247, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 196.291, mean reward: 1.963 [1.453, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.036, 10.192], loss: 0.359302, mae: 0.597789, mean_q: 4.600677
 18951/100000: episode: 248, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 196.427, mean reward: 1.964 [1.481, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.285], loss: 154.603500, mae: 0.929347, mean_q: 4.705045
 19051/100000: episode: 249, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 191.053, mean reward: 1.911 [1.503, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.580, 10.098], loss: 308.662476, mae: 1.831957, mean_q: 5.340352
 19151/100000: episode: 250, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 216.790, mean reward: 2.168 [1.489, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.411, 10.156], loss: 307.616028, mae: 1.588497, mean_q: 5.229405
 19251/100000: episode: 251, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 210.172, mean reward: 2.102 [1.463, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.551, 10.098], loss: 0.461301, mae: 0.644866, mean_q: 4.777461
 19351/100000: episode: 252, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 195.603, mean reward: 1.956 [1.492, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.095, 10.098], loss: 0.298120, mae: 0.533357, mean_q: 4.627148
 19451/100000: episode: 253, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 198.651, mean reward: 1.987 [1.475, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.339, 10.098], loss: 0.293240, mae: 0.523387, mean_q: 4.591956
 19551/100000: episode: 254, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 214.890, mean reward: 2.149 [1.508, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.092, 10.098], loss: 154.665161, mae: 1.032250, mean_q: 4.727087
 19651/100000: episode: 255, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 205.893, mean reward: 2.059 [1.466, 6.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.402, 10.098], loss: 154.104752, mae: 0.868918, mean_q: 4.644053
 19751/100000: episode: 256, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 194.219, mean reward: 1.942 [1.511, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.457, 10.402], loss: 0.718638, mae: 0.759699, mean_q: 4.852867
 19851/100000: episode: 257, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 184.085, mean reward: 1.841 [1.435, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.721, 10.098], loss: 0.284361, mae: 0.508351, mean_q: 4.572652
 19951/100000: episode: 258, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 191.299, mean reward: 1.913 [1.465, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.931, 10.129], loss: 0.283944, mae: 0.516991, mean_q: 4.564623
 20051/100000: episode: 259, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 187.363, mean reward: 1.874 [1.443, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.586, 10.166], loss: 154.740585, mae: 1.083278, mean_q: 4.796485
 20151/100000: episode: 260, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 247.365, mean reward: 2.474 [1.481, 38.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.913, 10.532], loss: 0.304926, mae: 0.534226, mean_q: 4.532797
 20251/100000: episode: 261, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 192.752, mean reward: 1.928 [1.486, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.208, 10.274], loss: 0.695222, mae: 0.536749, mean_q: 4.526351
 20351/100000: episode: 262, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 202.043, mean reward: 2.020 [1.453, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.115], loss: 0.293476, mae: 0.517170, mean_q: 4.463747
 20451/100000: episode: 263, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 197.593, mean reward: 1.976 [1.452, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.708, 10.228], loss: 154.367188, mae: 0.872627, mean_q: 4.534439
 20551/100000: episode: 264, duration: 0.778s, episode steps: 100, steps per second: 128, episode reward: 190.348, mean reward: 1.903 [1.471, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.504, 10.098], loss: 154.271683, mae: 1.225951, mean_q: 4.885374
 20651/100000: episode: 265, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 184.871, mean reward: 1.849 [1.450, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.858, 10.098], loss: 154.292694, mae: 1.025694, mean_q: 4.723667
 20751/100000: episode: 266, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 208.781, mean reward: 2.088 [1.513, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.951, 10.424], loss: 0.606543, mae: 0.599043, mean_q: 4.571323
 20851/100000: episode: 267, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 177.438, mean reward: 1.774 [1.452, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.431, 10.098], loss: 0.318775, mae: 0.521305, mean_q: 4.495898
 20951/100000: episode: 268, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 184.148, mean reward: 1.841 [1.485, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.104], loss: 154.222580, mae: 0.943100, mean_q: 4.610687
 21051/100000: episode: 269, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 180.976, mean reward: 1.810 [1.447, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.605, 10.098], loss: 0.385978, mae: 0.566174, mean_q: 4.537951
 21151/100000: episode: 270, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 241.049, mean reward: 2.410 [1.472, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.862, 10.284], loss: 306.937012, mae: 1.309200, mean_q: 4.606297
 21251/100000: episode: 271, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 200.167, mean reward: 2.002 [1.528, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.884, 10.193], loss: 1.747568, mae: 0.852917, mean_q: 4.839356
 21351/100000: episode: 272, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 187.638, mean reward: 1.876 [1.485, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.745, 10.098], loss: 0.485481, mae: 0.526205, mean_q: 4.458671
 21451/100000: episode: 273, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 184.706, mean reward: 1.847 [1.451, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.180], loss: 307.201721, mae: 1.496142, mean_q: 4.862926
 21551/100000: episode: 274, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 208.691, mean reward: 2.087 [1.510, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.900, 10.098], loss: 0.734815, mae: 0.623997, mean_q: 4.527755
 21651/100000: episode: 275, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.216, mean reward: 1.882 [1.459, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.101, 10.156], loss: 0.268325, mae: 0.479133, mean_q: 4.359219
 21751/100000: episode: 276, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.007, mean reward: 1.870 [1.464, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.763, 10.098], loss: 154.473190, mae: 0.986777, mean_q: 4.532532
 21851/100000: episode: 277, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 199.863, mean reward: 1.999 [1.501, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.211, 10.098], loss: 153.892609, mae: 0.892436, mean_q: 4.507754
 21951/100000: episode: 278, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 186.910, mean reward: 1.869 [1.459, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.141, 10.098], loss: 153.955566, mae: 1.140371, mean_q: 4.690267
 22051/100000: episode: 279, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 191.066, mean reward: 1.911 [1.486, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.844, 10.098], loss: 0.351192, mae: 0.511167, mean_q: 4.384489
 22151/100000: episode: 280, duration: 0.766s, episode steps: 100, steps per second: 130, episode reward: 193.879, mean reward: 1.939 [1.452, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.103], loss: 0.212518, mae: 0.420332, mean_q: 4.214079
 22251/100000: episode: 281, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 190.659, mean reward: 1.907 [1.482, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.267, 10.169], loss: 308.563232, mae: 1.608738, mean_q: 4.713266
 22351/100000: episode: 282, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 193.631, mean reward: 1.936 [1.470, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.098], loss: 0.953534, mae: 0.649090, mean_q: 4.452812
 22451/100000: episode: 283, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 180.498, mean reward: 1.805 [1.440, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.365, 10.098], loss: 0.468528, mae: 0.482467, mean_q: 4.274865
 22551/100000: episode: 284, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 180.782, mean reward: 1.808 [1.488, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.216], loss: 153.533447, mae: 0.871896, mean_q: 4.349756
 22651/100000: episode: 285, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 198.945, mean reward: 1.989 [1.508, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.205, 10.102], loss: 0.460344, mae: 0.456288, mean_q: 4.174356
 22751/100000: episode: 286, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 175.847, mean reward: 1.758 [1.464, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.080, 10.109], loss: 0.384110, mae: 0.411596, mean_q: 4.082027
 22851/100000: episode: 287, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 181.760, mean reward: 1.818 [1.498, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.622, 10.157], loss: 0.384853, mae: 0.405739, mean_q: 4.050104
 22951/100000: episode: 288, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 196.492, mean reward: 1.965 [1.451, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.275, 10.098], loss: 0.167791, mae: 0.372057, mean_q: 3.968055
 23051/100000: episode: 289, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 193.787, mean reward: 1.938 [1.471, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.359, 10.261], loss: 0.536491, mae: 0.379065, mean_q: 3.940646
 23151/100000: episode: 290, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 188.424, mean reward: 1.884 [1.455, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.013, 10.135], loss: 0.142395, mae: 0.343498, mean_q: 3.887586
 23251/100000: episode: 291, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 206.716, mean reward: 2.067 [1.442, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.261, 10.420], loss: 0.129930, mae: 0.340074, mean_q: 3.894127
 23351/100000: episode: 292, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 190.427, mean reward: 1.904 [1.514, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.200], loss: 0.321117, mae: 0.346285, mean_q: 3.875793
 23451/100000: episode: 293, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 192.350, mean reward: 1.923 [1.456, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.071, 10.182], loss: 0.313510, mae: 0.341887, mean_q: 3.875739
 23551/100000: episode: 294, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 234.884, mean reward: 2.349 [1.464, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.858, 10.125], loss: 0.317181, mae: 0.348709, mean_q: 3.885089
 23651/100000: episode: 295, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 192.877, mean reward: 1.929 [1.479, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.983, 10.132], loss: 0.319733, mae: 0.353054, mean_q: 3.895552
 23751/100000: episode: 296, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 183.260, mean reward: 1.833 [1.436, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.056, 10.098], loss: 0.512368, mae: 0.356872, mean_q: 3.898847
 23851/100000: episode: 297, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 203.131, mean reward: 2.031 [1.455, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.410], loss: 0.131571, mae: 0.342027, mean_q: 3.882043
 23951/100000: episode: 298, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 191.651, mean reward: 1.917 [1.494, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.644, 10.098], loss: 0.126366, mae: 0.334104, mean_q: 3.879828
 24051/100000: episode: 299, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 182.922, mean reward: 1.829 [1.441, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.197], loss: 0.325632, mae: 0.340585, mean_q: 3.889993
 24151/100000: episode: 300, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 207.380, mean reward: 2.074 [1.460, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.108], loss: 0.123336, mae: 0.332210, mean_q: 3.871007
 24251/100000: episode: 301, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 184.289, mean reward: 1.843 [1.465, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.201, 10.136], loss: 0.109850, mae: 0.317798, mean_q: 3.866153
 24351/100000: episode: 302, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 181.594, mean reward: 1.816 [1.441, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.890, 10.142], loss: 0.317271, mae: 0.335681, mean_q: 3.868861
 24451/100000: episode: 303, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 191.974, mean reward: 1.920 [1.443, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.031, 10.253], loss: 0.123977, mae: 0.330954, mean_q: 3.863800
 24551/100000: episode: 304, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 189.655, mean reward: 1.897 [1.467, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.967, 10.131], loss: 0.102781, mae: 0.307455, mean_q: 3.844537
 24651/100000: episode: 305, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 206.676, mean reward: 2.067 [1.434, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.350, 10.221], loss: 0.101662, mae: 0.301147, mean_q: 3.832759
 24751/100000: episode: 306, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 188.938, mean reward: 1.889 [1.454, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.241], loss: 0.102043, mae: 0.308869, mean_q: 3.833275
 24851/100000: episode: 307, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 209.058, mean reward: 2.091 [1.492, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.538, 10.098], loss: 0.302825, mae: 0.321239, mean_q: 3.846143
 24951/100000: episode: 308, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.755, mean reward: 1.868 [1.482, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.173, 10.222], loss: 0.307093, mae: 0.323085, mean_q: 3.841037
 25051/100000: episode: 309, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 177.923, mean reward: 1.779 [1.447, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.081, 10.098], loss: 0.300629, mae: 0.318478, mean_q: 3.846695
 25151/100000: episode: 310, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 225.824, mean reward: 2.258 [1.531, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.499, 10.098], loss: 0.121026, mae: 0.332756, mean_q: 3.863723
 25251/100000: episode: 311, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 209.694, mean reward: 2.097 [1.459, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.446, 10.098], loss: 0.091041, mae: 0.292039, mean_q: 3.840321
 25351/100000: episode: 312, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 184.979, mean reward: 1.850 [1.448, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.274, 10.245], loss: 0.097389, mae: 0.301139, mean_q: 3.832696
 25451/100000: episode: 313, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 183.398, mean reward: 1.834 [1.460, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.013, 10.098], loss: 0.096767, mae: 0.301896, mean_q: 3.844507
 25551/100000: episode: 314, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 203.274, mean reward: 2.033 [1.440, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.455, 10.098], loss: 0.095086, mae: 0.295638, mean_q: 3.834771
 25651/100000: episode: 315, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 187.492, mean reward: 1.875 [1.458, 2.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.629, 10.098], loss: 0.089679, mae: 0.297391, mean_q: 3.826507
 25751/100000: episode: 316, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 189.107, mean reward: 1.891 [1.490, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.669, 10.098], loss: 0.083565, mae: 0.283974, mean_q: 3.812015
 25851/100000: episode: 317, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.489, mean reward: 2.015 [1.509, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.157], loss: 0.090985, mae: 0.297660, mean_q: 3.833413
 25951/100000: episode: 318, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 188.679, mean reward: 1.887 [1.447, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.186, 10.114], loss: 0.092202, mae: 0.300435, mean_q: 3.846491
 26051/100000: episode: 319, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 208.659, mean reward: 2.087 [1.573, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.098], loss: 0.096252, mae: 0.305420, mean_q: 3.853654
 26151/100000: episode: 320, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 201.799, mean reward: 2.018 [1.467, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.014, 10.296], loss: 0.085970, mae: 0.295921, mean_q: 3.852046
 26251/100000: episode: 321, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 194.907, mean reward: 1.949 [1.443, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.806, 10.098], loss: 0.085458, mae: 0.292886, mean_q: 3.862902
 26351/100000: episode: 322, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 214.704, mean reward: 2.147 [1.480, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.372, 10.098], loss: 0.083943, mae: 0.293743, mean_q: 3.852038
 26451/100000: episode: 323, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 180.557, mean reward: 1.806 [1.434, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.573, 10.173], loss: 0.088305, mae: 0.295457, mean_q: 3.850577
 26551/100000: episode: 324, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 186.086, mean reward: 1.861 [1.468, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.511, 10.099], loss: 0.088610, mae: 0.299612, mean_q: 3.845387
 26651/100000: episode: 325, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 193.671, mean reward: 1.937 [1.447, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.908, 10.276], loss: 0.080760, mae: 0.288416, mean_q: 3.848249
 26751/100000: episode: 326, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.291, mean reward: 1.883 [1.448, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.000, 10.260], loss: 0.087409, mae: 0.295389, mean_q: 3.853798
 26851/100000: episode: 327, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.850, mean reward: 1.918 [1.497, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.288, 10.098], loss: 0.087324, mae: 0.295197, mean_q: 3.845608
 26951/100000: episode: 328, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 193.881, mean reward: 1.939 [1.447, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.862, 10.098], loss: 0.079506, mae: 0.291173, mean_q: 3.845168
 27051/100000: episode: 329, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 209.921, mean reward: 2.099 [1.482, 6.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.180, 10.098], loss: 0.096215, mae: 0.300505, mean_q: 3.849455
 27151/100000: episode: 330, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 197.781, mean reward: 1.978 [1.449, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.562, 10.098], loss: 0.086175, mae: 0.290846, mean_q: 3.856494
 27251/100000: episode: 331, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.281, mean reward: 1.843 [1.448, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.214], loss: 0.093641, mae: 0.299291, mean_q: 3.855786
 27351/100000: episode: 332, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 219.106, mean reward: 2.191 [1.455, 9.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.313, 10.464], loss: 0.092249, mae: 0.290842, mean_q: 3.838911
 27451/100000: episode: 333, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 188.871, mean reward: 1.889 [1.476, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.123], loss: 0.105074, mae: 0.300136, mean_q: 3.859104
 27551/100000: episode: 334, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.383, mean reward: 1.894 [1.503, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.654, 10.198], loss: 0.118155, mae: 0.306907, mean_q: 3.871434
 27651/100000: episode: 335, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 189.552, mean reward: 1.896 [1.463, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.787, 10.150], loss: 0.099221, mae: 0.301143, mean_q: 3.859189
 27751/100000: episode: 336, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 187.570, mean reward: 1.876 [1.463, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.230, 10.137], loss: 0.107148, mae: 0.299589, mean_q: 3.859748
 27851/100000: episode: 337, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 214.025, mean reward: 2.140 [1.594, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.531, 10.313], loss: 0.107626, mae: 0.306140, mean_q: 3.876478
 27951/100000: episode: 338, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 206.960, mean reward: 2.070 [1.462, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.068, 10.353], loss: 0.103976, mae: 0.298296, mean_q: 3.873868
 28051/100000: episode: 339, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 198.163, mean reward: 1.982 [1.464, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.075, 10.098], loss: 0.111031, mae: 0.297612, mean_q: 3.872665
[Info] 1-TH LEVEL FOUND: 4.797276496887207, Considering 10/90 traces
 28151/100000: episode: 340, duration: 4.734s, episode steps: 100, steps per second: 21, episode reward: 185.500, mean reward: 1.855 [1.469, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.880, 10.239], loss: 0.103013, mae: 0.305892, mean_q: 3.899763
 28172/100000: episode: 341, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 53.826, mean reward: 2.563 [1.717, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.079, 10.100], loss: 0.083767, mae: 0.291437, mean_q: 3.883071
 28224/100000: episode: 342, duration: 0.267s, episode steps: 52, steps per second: 194, episode reward: 190.601, mean reward: 3.665 [2.373, 7.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.394, 10.437], loss: 0.113457, mae: 0.302619, mean_q: 3.885824
 28266/100000: episode: 343, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 128.141, mean reward: 3.051 [2.290, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.401, 10.100], loss: 0.123434, mae: 0.322047, mean_q: 3.923827
 28318/100000: episode: 344, duration: 0.271s, episode steps: 52, steps per second: 192, episode reward: 109.017, mean reward: 2.096 [1.532, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.241, 10.245], loss: 0.108849, mae: 0.321270, mean_q: 3.929114
 28339/100000: episode: 345, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 66.490, mean reward: 3.166 [2.545, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.298, 10.100], loss: 0.189242, mae: 0.362287, mean_q: 3.966979
 28365/100000: episode: 346, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 73.241, mean reward: 2.817 [1.917, 5.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.424, 10.100], loss: 0.159009, mae: 0.317981, mean_q: 3.909221
 28417/100000: episode: 347, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 98.725, mean reward: 1.899 [1.493, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.420, 10.154], loss: 0.101512, mae: 0.312728, mean_q: 3.932070
 28441/100000: episode: 348, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 59.243, mean reward: 2.468 [1.941, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.297, 10.100], loss: 0.127670, mae: 0.332957, mean_q: 3.958434
 28465/100000: episode: 349, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 64.485, mean reward: 2.687 [2.169, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.176, 10.100], loss: 0.104021, mae: 0.315316, mean_q: 3.957920
 28491/100000: episode: 350, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 51.193, mean reward: 1.969 [1.663, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.514, 10.100], loss: 0.122622, mae: 0.332664, mean_q: 3.982282
 28533/100000: episode: 351, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 90.332, mean reward: 2.151 [1.454, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.017, 10.100], loss: 0.117965, mae: 0.327240, mean_q: 3.951918
 28585/100000: episode: 352, duration: 0.278s, episode steps: 52, steps per second: 187, episode reward: 132.131, mean reward: 2.541 [1.735, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.373, 10.384], loss: 0.118069, mae: 0.333217, mean_q: 3.982831
 28618/100000: episode: 353, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 62.948, mean reward: 1.908 [1.576, 2.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.989, 10.248], loss: 0.143751, mae: 0.336020, mean_q: 3.949451
 28658/100000: episode: 354, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 101.072, mean reward: 2.527 [1.479, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.920, 10.100], loss: 0.113531, mae: 0.315306, mean_q: 3.952986
 28691/100000: episode: 355, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 64.276, mean reward: 1.948 [1.533, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.986, 10.164], loss: 0.119343, mae: 0.316554, mean_q: 3.966563
 28725/100000: episode: 356, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 116.751, mean reward: 3.434 [2.234, 6.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.100, 10.100], loss: 0.151789, mae: 0.347737, mean_q: 4.009910
 28751/100000: episode: 357, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 50.666, mean reward: 1.949 [1.538, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.329], loss: 0.162159, mae: 0.342266, mean_q: 3.945729
 28784/100000: episode: 358, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 82.668, mean reward: 2.505 [2.074, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.676, 10.100], loss: 0.126833, mae: 0.339191, mean_q: 4.012542
 28818/100000: episode: 359, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 63.632, mean reward: 1.872 [1.616, 2.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.636, 10.100], loss: 0.137596, mae: 0.343834, mean_q: 3.980309
 28844/100000: episode: 360, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 60.249, mean reward: 2.317 [1.915, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.152, 10.100], loss: 0.115409, mae: 0.329218, mean_q: 4.020271
 28884/100000: episode: 361, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 89.738, mean reward: 2.243 [1.597, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.755, 10.100], loss: 0.137922, mae: 0.339024, mean_q: 4.003985
 28926/100000: episode: 362, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 114.712, mean reward: 2.731 [2.020, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.307, 10.100], loss: 0.152304, mae: 0.344046, mean_q: 4.008318
 28952/100000: episode: 363, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 50.689, mean reward: 1.950 [1.734, 2.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.089, 10.100], loss: 0.108182, mae: 0.314648, mean_q: 3.994922
 28985/100000: episode: 364, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 67.855, mean reward: 2.056 [1.475, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.356, 10.151], loss: 0.138100, mae: 0.328599, mean_q: 4.010288
 29009/100000: episode: 365, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 54.372, mean reward: 2.265 [1.650, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.156, 10.100], loss: 0.158219, mae: 0.350823, mean_q: 4.038302
 29042/100000: episode: 366, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 72.692, mean reward: 2.203 [1.729, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.235, 10.100], loss: 0.141017, mae: 0.357090, mean_q: 4.016748
 29074/100000: episode: 367, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 73.641, mean reward: 2.301 [1.860, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.372, 10.100], loss: 0.122719, mae: 0.328442, mean_q: 4.009067
 29107/100000: episode: 368, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 67.173, mean reward: 2.036 [1.779, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.243, 10.100], loss: 0.143361, mae: 0.338572, mean_q: 4.048613
 29159/100000: episode: 369, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 108.297, mean reward: 2.083 [1.587, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.103, 10.357], loss: 0.143399, mae: 0.342837, mean_q: 4.029538
 29180/100000: episode: 370, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 58.598, mean reward: 2.790 [2.208, 6.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.264, 10.100], loss: 0.104240, mae: 0.317203, mean_q: 4.014414
 29201/100000: episode: 371, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 61.375, mean reward: 2.923 [2.106, 5.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.233, 10.100], loss: 0.141075, mae: 0.342945, mean_q: 4.018953
 29233/100000: episode: 372, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 74.296, mean reward: 2.322 [1.801, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.456, 10.100], loss: 0.171997, mae: 0.359551, mean_q: 4.051619
 29254/100000: episode: 373, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 59.813, mean reward: 2.848 [2.039, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.452, 10.100], loss: 0.107191, mae: 0.323920, mean_q: 4.019587
 29287/100000: episode: 374, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 91.719, mean reward: 2.779 [2.076, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.751, 10.100], loss: 0.128276, mae: 0.338024, mean_q: 4.078603
 29319/100000: episode: 375, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 71.174, mean reward: 2.224 [1.635, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.685, 10.100], loss: 0.141969, mae: 0.326146, mean_q: 4.026786
 29371/100000: episode: 376, duration: 0.281s, episode steps: 52, steps per second: 185, episode reward: 142.341, mean reward: 2.737 [2.168, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.383, 10.296], loss: 0.137209, mae: 0.350575, mean_q: 4.095245
 29423/100000: episode: 377, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 143.378, mean reward: 2.757 [1.849, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.644, 10.304], loss: 0.142879, mae: 0.363192, mean_q: 4.082127
 29457/100000: episode: 378, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 101.173, mean reward: 2.976 [2.162, 4.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.860, 10.100], loss: 0.139355, mae: 0.358747, mean_q: 4.124264
 29481/100000: episode: 379, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 50.846, mean reward: 2.119 [1.686, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.844, 10.100], loss: 0.100708, mae: 0.311820, mean_q: 4.081083
 29521/100000: episode: 380, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 141.632, mean reward: 3.541 [2.130, 14.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.432, 10.100], loss: 0.128342, mae: 0.331484, mean_q: 4.071191
 29547/100000: episode: 381, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 52.310, mean reward: 2.012 [1.687, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.389, 10.100], loss: 0.139634, mae: 0.345107, mean_q: 4.154174
 29589/100000: episode: 382, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 82.770, mean reward: 1.971 [1.496, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.216, 10.100], loss: 0.131284, mae: 0.343900, mean_q: 4.124287
 29629/100000: episode: 383, duration: 0.211s, episode steps: 40, steps per second: 189, episode reward: 89.045, mean reward: 2.226 [1.467, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.294, 10.100], loss: 0.204008, mae: 0.378775, mean_q: 4.157408
 29653/100000: episode: 384, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 56.342, mean reward: 2.348 [1.711, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.174, 10.100], loss: 0.121835, mae: 0.339708, mean_q: 4.109390
 29674/100000: episode: 385, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 52.711, mean reward: 2.510 [1.899, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.649, 10.100], loss: 0.145123, mae: 0.344743, mean_q: 4.160999
 29700/100000: episode: 386, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 58.290, mean reward: 2.242 [1.846, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.365, 10.100], loss: 0.143321, mae: 0.369127, mean_q: 4.140133
 29726/100000: episode: 387, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 55.175, mean reward: 2.122 [1.783, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.331, 10.100], loss: 0.168648, mae: 0.397576, mean_q: 4.163589
 29750/100000: episode: 388, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 72.140, mean reward: 3.006 [2.345, 7.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.311, 10.100], loss: 0.127755, mae: 0.336789, mean_q: 4.122635
 29790/100000: episode: 389, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 78.063, mean reward: 1.952 [1.532, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.316, 10.100], loss: 0.144920, mae: 0.366349, mean_q: 4.207965
 29816/100000: episode: 390, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 64.030, mean reward: 2.463 [2.028, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.604, 10.100], loss: 0.115704, mae: 0.338565, mean_q: 4.153268
 29856/100000: episode: 391, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 93.628, mean reward: 2.341 [1.792, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.179, 10.100], loss: 0.111953, mae: 0.322563, mean_q: 4.169263
 29880/100000: episode: 392, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 58.647, mean reward: 2.444 [1.872, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.300, 10.100], loss: 0.140267, mae: 0.369194, mean_q: 4.180866
 29932/100000: episode: 393, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 114.220, mean reward: 2.197 [1.453, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.545, 10.263], loss: 0.136939, mae: 0.353625, mean_q: 4.188664
 29953/100000: episode: 394, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 61.770, mean reward: 2.941 [2.144, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.402, 10.100], loss: 0.162621, mae: 0.393546, mean_q: 4.199317
 30005/100000: episode: 395, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 107.128, mean reward: 2.060 [1.640, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.372, 10.327], loss: 0.193958, mae: 0.379162, mean_q: 4.185954
 30037/100000: episode: 396, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 60.973, mean reward: 1.905 [1.454, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.502, 10.138], loss: 0.155957, mae: 0.381969, mean_q: 4.236496
 30089/100000: episode: 397, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 117.690, mean reward: 2.263 [1.573, 5.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.149, 10.152], loss: 0.145942, mae: 0.366947, mean_q: 4.187664
 30129/100000: episode: 398, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 142.309, mean reward: 3.558 [1.976, 10.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.171, 10.100], loss: 0.143278, mae: 0.370811, mean_q: 4.214749
 30171/100000: episode: 399, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 121.205, mean reward: 2.886 [1.963, 5.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.995, 10.100], loss: 0.132700, mae: 0.346116, mean_q: 4.236669
 30213/100000: episode: 400, duration: 0.219s, episode steps: 42, steps per second: 191, episode reward: 121.938, mean reward: 2.903 [2.093, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.613, 10.100], loss: 0.168615, mae: 0.376215, mean_q: 4.192695
 30245/100000: episode: 401, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 67.114, mean reward: 2.097 [1.664, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.399, 10.100], loss: 0.149043, mae: 0.364552, mean_q: 4.221116
 30271/100000: episode: 402, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 55.947, mean reward: 2.152 [1.825, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.127, 10.100], loss: 0.116431, mae: 0.348738, mean_q: 4.224905
 30292/100000: episode: 403, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 59.898, mean reward: 2.852 [2.139, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.158, 10.100], loss: 0.141016, mae: 0.356210, mean_q: 4.210693
 30324/100000: episode: 404, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 91.415, mean reward: 2.857 [1.756, 5.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.425, 10.100], loss: 0.140786, mae: 0.359642, mean_q: 4.226183
 30358/100000: episode: 405, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 95.038, mean reward: 2.795 [2.057, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.923, 10.100], loss: 0.145278, mae: 0.366086, mean_q: 4.279944
 30379/100000: episode: 406, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 54.982, mean reward: 2.618 [2.195, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.848, 10.100], loss: 0.263749, mae: 0.407387, mean_q: 4.315458
 30405/100000: episode: 407, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 60.977, mean reward: 2.345 [1.832, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.200, 10.100], loss: 0.165405, mae: 0.364452, mean_q: 4.284276
 30445/100000: episode: 408, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 135.468, mean reward: 3.387 [2.150, 8.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-1.079, 10.100], loss: 0.191905, mae: 0.399217, mean_q: 4.263811
 30479/100000: episode: 409, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 88.309, mean reward: 2.597 [1.902, 7.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.260, 10.100], loss: 0.149788, mae: 0.372063, mean_q: 4.299966
 30511/100000: episode: 410, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 72.653, mean reward: 2.270 [1.748, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.326, 10.100], loss: 0.179634, mae: 0.397509, mean_q: 4.290371
 30551/100000: episode: 411, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 90.833, mean reward: 2.271 [1.612, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.432, 10.127], loss: 0.182566, mae: 0.396567, mean_q: 4.280686
 30603/100000: episode: 412, duration: 0.254s, episode steps: 52, steps per second: 205, episode reward: 111.681, mean reward: 2.148 [1.461, 5.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.377, 10.319], loss: 0.148868, mae: 0.374014, mean_q: 4.338151
 30637/100000: episode: 413, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 85.377, mean reward: 2.511 [1.706, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.174, 10.100], loss: 0.257556, mae: 0.420988, mean_q: 4.374889
 30663/100000: episode: 414, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 52.549, mean reward: 2.021 [1.803, 2.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.483, 10.100], loss: 0.158656, mae: 0.359086, mean_q: 4.296500
 30697/100000: episode: 415, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 81.426, mean reward: 2.395 [1.577, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.229, 10.100], loss: 0.151775, mae: 0.377564, mean_q: 4.332823
 30737/100000: episode: 416, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 87.627, mean reward: 2.191 [1.537, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.687, 10.100], loss: 0.168361, mae: 0.375605, mean_q: 4.356471
 30763/100000: episode: 417, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 54.023, mean reward: 2.078 [1.571, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.327, 10.100], loss: 0.157910, mae: 0.364100, mean_q: 4.348745
 30789/100000: episode: 418, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 57.945, mean reward: 2.229 [1.744, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.702, 10.100], loss: 0.188444, mae: 0.389125, mean_q: 4.384501
 30815/100000: episode: 419, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 55.438, mean reward: 2.132 [1.836, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.215, 10.100], loss: 0.169561, mae: 0.366872, mean_q: 4.397347
 30867/100000: episode: 420, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 98.595, mean reward: 1.896 [1.463, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.391, 10.257], loss: 0.172903, mae: 0.392376, mean_q: 4.360423
 30891/100000: episode: 421, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 55.380, mean reward: 2.307 [1.670, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.551, 10.100], loss: 0.185190, mae: 0.394334, mean_q: 4.344857
 30917/100000: episode: 422, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 49.477, mean reward: 1.903 [1.555, 2.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.201, 10.194], loss: 0.171733, mae: 0.374832, mean_q: 4.298825
 30951/100000: episode: 423, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 87.890, mean reward: 2.585 [1.952, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.747, 10.100], loss: 0.176459, mae: 0.384726, mean_q: 4.333671
 31003/100000: episode: 424, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 111.510, mean reward: 2.144 [1.498, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.161, 10.100], loss: 0.161419, mae: 0.379648, mean_q: 4.359441
 31055/100000: episode: 425, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 128.657, mean reward: 2.474 [1.846, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.387, 10.277], loss: 0.158311, mae: 0.379406, mean_q: 4.364713
 31079/100000: episode: 426, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 66.711, mean reward: 2.780 [2.084, 4.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.276, 10.100], loss: 0.192025, mae: 0.372933, mean_q: 4.347543
 31113/100000: episode: 427, duration: 0.170s, episode steps: 34, steps per second: 201, episode reward: 91.529, mean reward: 2.692 [1.713, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.387, 10.100], loss: 0.266896, mae: 0.424530, mean_q: 4.425575
 31145/100000: episode: 428, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 63.860, mean reward: 1.996 [1.445, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.257, 10.162], loss: 0.159804, mae: 0.367380, mean_q: 4.327539
 31197/100000: episode: 429, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 151.351, mean reward: 2.911 [2.162, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.398, 10.450], loss: 0.153460, mae: 0.381258, mean_q: 4.398360
[Info] 2-TH LEVEL FOUND: 5.920750141143799, Considering 10/90 traces
 31249/100000: episode: 430, duration: 4.446s, episode steps: 52, steps per second: 12, episode reward: 129.586, mean reward: 2.492 [1.887, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.866, 10.350], loss: 0.199270, mae: 0.389200, mean_q: 4.371182
 31299/100000: episode: 431, duration: 0.277s, episode steps: 50, steps per second: 181, episode reward: 173.794, mean reward: 3.476 [2.090, 7.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.372, 10.360], loss: 0.161101, mae: 0.385938, mean_q: 4.393855
 31324/100000: episode: 432, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 94.276, mean reward: 3.771 [2.296, 5.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.672, 10.100], loss: 0.182243, mae: 0.427295, mean_q: 4.391678
 31332/100000: episode: 433, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 38.080, mean reward: 4.760 [3.930, 5.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.441, 10.100], loss: 0.152822, mae: 0.407777, mean_q: 4.519319
 31382/100000: episode: 434, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 129.585, mean reward: 2.592 [1.843, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.828, 10.318], loss: 0.155301, mae: 0.386541, mean_q: 4.448393
 31432/100000: episode: 435, duration: 0.246s, episode steps: 50, steps per second: 203, episode reward: 116.565, mean reward: 2.331 [1.659, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.454, 10.313], loss: 0.188313, mae: 0.417026, mean_q: 4.428080
 31482/100000: episode: 436, duration: 0.278s, episode steps: 50, steps per second: 180, episode reward: 119.555, mean reward: 2.391 [1.590, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.392, 10.249], loss: 0.242966, mae: 0.434835, mean_q: 4.514549
 31532/100000: episode: 437, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 183.997, mean reward: 3.680 [2.114, 8.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.395, 10.386], loss: 0.188312, mae: 0.399155, mean_q: 4.486701
 31582/100000: episode: 438, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 98.773, mean reward: 1.975 [1.493, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.679, 10.168], loss: 0.250730, mae: 0.456849, mean_q: 4.495358
 31632/100000: episode: 439, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 202.512, mean reward: 4.050 [2.076, 16.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.386, 10.325], loss: 0.219721, mae: 0.430397, mean_q: 4.544070
 31682/100000: episode: 440, duration: 0.250s, episode steps: 50, steps per second: 200, episode reward: 183.553, mean reward: 3.671 [2.428, 6.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.764, 10.411], loss: 0.270783, mae: 0.438720, mean_q: 4.573172
 31732/100000: episode: 441, duration: 0.264s, episode steps: 50, steps per second: 189, episode reward: 113.850, mean reward: 2.277 [1.502, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.369, 10.111], loss: 0.280255, mae: 0.454233, mean_q: 4.610805
 31782/100000: episode: 442, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 179.109, mean reward: 3.582 [2.538, 5.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.445, 10.542], loss: 0.308596, mae: 0.443256, mean_q: 4.643014
 31798/100000: episode: 443, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 69.720, mean reward: 4.358 [2.114, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.414, 10.100], loss: 0.262309, mae: 0.447011, mean_q: 4.662103
 31806/100000: episode: 444, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 76.254, mean reward: 9.532 [6.025, 17.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.500, 10.100], loss: 0.203035, mae: 0.435141, mean_q: 4.588781
 31831/100000: episode: 445, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 90.446, mean reward: 3.618 [2.893, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.298, 10.100], loss: 0.251003, mae: 0.433789, mean_q: 4.680693
 31881/100000: episode: 446, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 140.696, mean reward: 2.814 [2.086, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.406, 10.411], loss: 0.289429, mae: 0.496234, mean_q: 4.709653
 31889/100000: episode: 447, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 41.058, mean reward: 5.132 [4.127, 7.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.352, 10.100], loss: 0.535962, mae: 0.530557, mean_q: 4.851756
 31939/100000: episode: 448, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 120.728, mean reward: 2.415 [1.477, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.372, 10.244], loss: 0.261477, mae: 0.438718, mean_q: 4.714552
 31989/100000: episode: 449, duration: 0.244s, episode steps: 50, steps per second: 205, episode reward: 123.983, mean reward: 2.480 [1.459, 5.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.371, 10.111], loss: 0.224333, mae: 0.447729, mean_q: 4.726048
 32039/100000: episode: 450, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 118.008, mean reward: 2.360 [1.808, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.766, 10.377], loss: 0.285335, mae: 0.461473, mean_q: 4.714620
 32058/100000: episode: 451, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 78.236, mean reward: 4.118 [3.159, 6.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.584, 10.100], loss: 0.421373, mae: 0.471626, mean_q: 4.783984
 32084/100000: episode: 452, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 75.741, mean reward: 2.913 [1.908, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.508, 10.100], loss: 0.222050, mae: 0.436075, mean_q: 4.810165
 32134/100000: episode: 453, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 179.134, mean reward: 3.583 [1.921, 15.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.978, 10.288], loss: 0.528798, mae: 0.542292, mean_q: 4.872452
 32153/100000: episode: 454, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 57.857, mean reward: 3.045 [2.298, 5.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.118, 10.100], loss: 0.236462, mae: 0.475401, mean_q: 4.779095
 32203/100000: episode: 455, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 122.048, mean reward: 2.441 [1.742, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.375, 10.332], loss: 0.357586, mae: 0.483482, mean_q: 4.786716
 32228/100000: episode: 456, duration: 0.153s, episode steps: 25, steps per second: 164, episode reward: 88.484, mean reward: 3.539 [2.260, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.130, 10.100], loss: 0.228155, mae: 0.426326, mean_q: 4.725213
 32244/100000: episode: 457, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 44.221, mean reward: 2.764 [2.169, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.409, 10.100], loss: 0.595804, mae: 0.495409, mean_q: 4.859171
 32252/100000: episode: 458, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 30.866, mean reward: 3.858 [2.768, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.399, 10.100], loss: 0.206284, mae: 0.491847, mean_q: 4.668058
 32302/100000: episode: 459, duration: 0.280s, episode steps: 50, steps per second: 179, episode reward: 126.202, mean reward: 2.524 [1.890, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.724, 10.404], loss: 0.319952, mae: 0.521441, mean_q: 4.880821
 32327/100000: episode: 460, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 121.605, mean reward: 4.864 [3.177, 11.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.617, 10.100], loss: 0.407622, mae: 0.523237, mean_q: 4.828597
 32377/100000: episode: 461, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 105.403, mean reward: 2.108 [1.491, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.519, 10.100], loss: 0.221711, mae: 0.452247, mean_q: 4.853393
 32396/100000: episode: 462, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 60.965, mean reward: 3.209 [2.456, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.932, 10.100], loss: 0.295449, mae: 0.484147, mean_q: 4.923157
 32446/100000: episode: 463, duration: 0.248s, episode steps: 50, steps per second: 201, episode reward: 133.444, mean reward: 2.669 [1.724, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.382, 10.253], loss: 0.430466, mae: 0.516398, mean_q: 4.940738
 32496/100000: episode: 464, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 121.963, mean reward: 2.439 [1.546, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.825, 10.277], loss: 0.349149, mae: 0.475549, mean_q: 4.901542
 32512/100000: episode: 465, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 56.088, mean reward: 3.506 [2.114, 6.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.487, 10.100], loss: 0.468076, mae: 0.523120, mean_q: 5.011371
 32537/100000: episode: 466, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 82.663, mean reward: 3.307 [2.007, 4.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.238, 10.100], loss: 0.397899, mae: 0.517174, mean_q: 5.000368
 32556/100000: episode: 467, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 65.994, mean reward: 3.473 [2.702, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.424, 10.100], loss: 0.419792, mae: 0.506211, mean_q: 5.006264
 32582/100000: episode: 468, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 92.695, mean reward: 3.565 [2.555, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.423, 10.100], loss: 0.180190, mae: 0.417685, mean_q: 4.903589
[Info] FALSIFICATION!
 32595/100000: episode: 469, duration: 0.227s, episode steps: 13, steps per second: 57, episode reward: 1049.521, mean reward: 80.732 [2.783, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.186, 9.756], loss: 0.233618, mae: 0.466879, mean_q: 4.960515
 32621/100000: episode: 470, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 57.795, mean reward: 2.223 [1.610, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.479, 10.120], loss: 592.839539, mae: 2.791896, mean_q: 6.277723
 32671/100000: episode: 471, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 111.205, mean reward: 2.224 [1.455, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.372, 10.127], loss: 0.878678, mae: 0.928079, mean_q: 4.837427
 32721/100000: episode: 472, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 170.590, mean reward: 3.412 [1.900, 6.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.480, 10.357], loss: 0.348339, mae: 0.521826, mean_q: 4.883196
 32746/100000: episode: 473, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 88.099, mean reward: 3.524 [2.681, 5.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.710, 10.100], loss: 614.193115, mae: 1.912461, mean_q: 5.193787
 32796/100000: episode: 474, duration: 0.257s, episode steps: 50, steps per second: 194, episode reward: 119.159, mean reward: 2.383 [1.460, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.907, 10.148], loss: 1.747259, mae: 1.319893, mean_q: 5.542411
 32846/100000: episode: 475, duration: 0.279s, episode steps: 50, steps per second: 179, episode reward: 149.530, mean reward: 2.991 [1.445, 6.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.623, 10.163], loss: 0.404708, mae: 0.590637, mean_q: 5.177689
 32896/100000: episode: 476, duration: 0.249s, episode steps: 50, steps per second: 201, episode reward: 114.005, mean reward: 2.280 [1.535, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.352, 10.100], loss: 0.488071, mae: 0.563307, mean_q: 5.110155
 32946/100000: episode: 477, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 208.943, mean reward: 4.179 [2.441, 10.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.810, 10.457], loss: 308.114166, mae: 1.601042, mean_q: 5.618114
 32996/100000: episode: 478, duration: 0.280s, episode steps: 50, steps per second: 179, episode reward: 220.177, mean reward: 4.404 [2.023, 16.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.380, 10.681], loss: 308.057465, mae: 1.825216, mean_q: 5.959938
 33021/100000: episode: 479, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 117.454, mean reward: 4.698 [3.010, 5.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.223, 10.100], loss: 0.784916, mae: 0.728422, mean_q: 5.212306
 33071/100000: episode: 480, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 161.813, mean reward: 3.236 [1.930, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.864, 10.412], loss: 0.514132, mae: 0.628533, mean_q: 5.298363
 33121/100000: episode: 481, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 142.724, mean reward: 2.854 [1.841, 5.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.831, 10.228], loss: 0.510850, mae: 0.600466, mean_q: 5.299595
 33137/100000: episode: 482, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 45.259, mean reward: 2.829 [2.102, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.244, 10.100], loss: 0.508995, mae: 0.579689, mean_q: 5.299551
 33153/100000: episode: 483, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 54.922, mean reward: 3.433 [2.448, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.342, 10.100], loss: 960.348022, mae: 3.816029, mean_q: 6.969522
 33203/100000: episode: 484, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 114.736, mean reward: 2.295 [1.498, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.592, 10.215], loss: 613.707520, mae: 2.579174, mean_q: 5.920331
 33253/100000: episode: 485, duration: 0.247s, episode steps: 50, steps per second: 203, episode reward: 117.750, mean reward: 2.355 [1.744, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.479, 10.295], loss: 1.572415, mae: 1.140020, mean_q: 5.760021
 33303/100000: episode: 486, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 122.071, mean reward: 2.441 [1.541, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.236, 10.173], loss: 611.876038, mae: 2.623256, mean_q: 6.583451
 33329/100000: episode: 487, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 84.771, mean reward: 3.260 [2.545, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.703, 10.100], loss: 1.087964, mae: 0.995497, mean_q: 5.388241
 33355/100000: episode: 488, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 98.558, mean reward: 3.791 [2.717, 6.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.325, 10.100], loss: 0.506759, mae: 0.691059, mean_q: 5.619433
 33371/100000: episode: 489, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 61.951, mean reward: 3.872 [2.258, 8.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.358, 10.100], loss: 954.171814, mae: 2.655255, mean_q: 5.556438
 33421/100000: episode: 490, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 134.275, mean reward: 2.686 [1.546, 11.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.354, 10.177], loss: 1.822343, mae: 1.261796, mean_q: 5.977751
 33471/100000: episode: 491, duration: 0.268s, episode steps: 50, steps per second: 187, episode reward: 108.182, mean reward: 2.164 [1.604, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.357, 10.100], loss: 0.576962, mae: 0.679272, mean_q: 5.526577
 33521/100000: episode: 492, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 113.394, mean reward: 2.268 [1.461, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.768, 10.166], loss: 0.551267, mae: 0.668960, mean_q: 5.441453
 33546/100000: episode: 493, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 87.403, mean reward: 3.496 [2.365, 5.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.674, 10.100], loss: 0.473806, mae: 0.629693, mean_q: 5.399031
 33565/100000: episode: 494, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 69.908, mean reward: 3.679 [2.673, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.447, 10.100], loss: 0.569690, mae: 0.621009, mean_q: 5.365731
 33591/100000: episode: 495, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 83.001, mean reward: 3.192 [2.154, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.158, 10.100], loss: 0.588798, mae: 0.651811, mean_q: 5.405655
 33641/100000: episode: 496, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 123.015, mean reward: 2.460 [1.584, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.560, 10.100], loss: 0.428384, mae: 0.607098, mean_q: 5.276281
 33660/100000: episode: 497, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 70.625, mean reward: 3.717 [2.554, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.531, 10.100], loss: 0.488166, mae: 0.629626, mean_q: 5.403400
 33668/100000: episode: 498, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 32.377, mean reward: 4.047 [2.724, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.053, 10.100], loss: 0.454507, mae: 0.620783, mean_q: 5.194526
 33718/100000: episode: 499, duration: 0.242s, episode steps: 50, steps per second: 207, episode reward: 145.571, mean reward: 2.911 [2.095, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.360, 10.445], loss: 0.528517, mae: 0.618251, mean_q: 5.360651
 33768/100000: episode: 500, duration: 0.277s, episode steps: 50, steps per second: 180, episode reward: 146.070, mean reward: 2.921 [1.904, 6.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.871, 10.406], loss: 0.465429, mae: 0.592881, mean_q: 5.366150
 33776/100000: episode: 501, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 49.649, mean reward: 6.206 [3.910, 9.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.371, 10.100], loss: 0.512180, mae: 0.651393, mean_q: 5.417441
 33826/100000: episode: 502, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 142.414, mean reward: 2.848 [1.890, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.039, 10.302], loss: 307.336700, mae: 1.854892, mean_q: 5.822221
 33876/100000: episode: 503, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 113.926, mean reward: 2.279 [1.486, 5.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.438, 10.100], loss: 0.671166, mae: 0.672093, mean_q: 5.473232
 33895/100000: episode: 504, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 60.041, mean reward: 3.160 [2.057, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.293, 10.100], loss: 806.625549, mae: 3.615375, mean_q: 7.307230
 33945/100000: episode: 505, duration: 0.242s, episode steps: 50, steps per second: 206, episode reward: 108.742, mean reward: 2.175 [1.504, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.380, 10.127], loss: 0.890278, mae: 0.813839, mean_q: 5.476576
[Info] FALSIFICATION!
 33958/100000: episode: 506, duration: 0.330s, episode steps: 13, steps per second: 39, episode reward: 1137.270, mean reward: 87.482 [2.843, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.195, 7.052], loss: 0.773860, mae: 0.720599, mean_q: 5.574893
 33984/100000: episode: 507, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 126.165, mean reward: 4.852 [2.689, 10.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.114, 10.100], loss: 0.517055, mae: 0.653329, mean_q: 5.506807
 34034/100000: episode: 508, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 123.781, mean reward: 2.476 [1.836, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.381, 10.167], loss: 2.012648, mae: 0.754872, mean_q: 5.643195
 34059/100000: episode: 509, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 116.229, mean reward: 4.649 [2.281, 9.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.684, 10.100], loss: 616.882385, mae: 2.302263, mean_q: 6.101307
 34078/100000: episode: 510, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 76.460, mean reward: 4.024 [2.782, 6.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.702, 10.100], loss: 805.907166, mae: 2.888802, mean_q: 6.570767
 34128/100000: episode: 511, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 136.095, mean reward: 2.722 [1.919, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.397, 10.360], loss: 2.266988, mae: 1.204909, mean_q: 6.082690
 34178/100000: episode: 512, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 213.895, mean reward: 4.278 [2.390, 9.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.952, 10.451], loss: 1.283659, mae: 0.723058, mean_q: 5.775891
 34203/100000: episode: 513, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 91.380, mean reward: 3.655 [2.759, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.331, 10.100], loss: 3.006103, mae: 0.794187, mean_q: 5.749678
 34229/100000: episode: 514, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 88.284, mean reward: 3.396 [2.256, 8.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.450, 10.100], loss: 0.660110, mae: 0.674176, mean_q: 5.680010
 34279/100000: episode: 515, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 106.593, mean reward: 2.132 [1.522, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.685, 10.409], loss: 308.744080, mae: 1.848522, mean_q: 6.408608
 34329/100000: episode: 516, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 114.013, mean reward: 2.280 [1.562, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.855, 10.127], loss: 613.460205, mae: 2.358303, mean_q: 6.161827
 34348/100000: episode: 517, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 78.120, mean reward: 4.112 [2.223, 8.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.163, 10.100], loss: 3.237033, mae: 1.887854, mean_q: 7.339187
 34374/100000: episode: 518, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 89.265, mean reward: 3.433 [2.453, 5.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.912, 10.100], loss: 1.429119, mae: 0.913646, mean_q: 5.548524
 34390/100000: episode: 519, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 58.163, mean reward: 3.635 [2.447, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.486, 10.100], loss: 959.842285, mae: 3.104173, mean_q: 6.612696
[Info] Complete ISplit Iteration
[Info] Levels: [4.7972765, 5.92075, 9.692599]
[Info] Cond. Prob: [0.1, 0.1, 0.15]
[Info] Error Prob: 0.0015000000000000002

 34440/100000: episode: 520, duration: 4.790s, episode steps: 50, steps per second: 10, episode reward: 131.916, mean reward: 2.638 [1.482, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.128, 10.100], loss: 0.861767, mae: 0.923707, mean_q: 6.040497
 34540/100000: episode: 521, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.250, mean reward: 1.893 [1.439, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.325, 10.140], loss: 459.846252, mae: 2.110861, mean_q: 6.411501
 34640/100000: episode: 522, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 183.966, mean reward: 1.840 [1.458, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.424, 10.098], loss: 1.404699, mae: 0.867369, mean_q: 6.005311
 34740/100000: episode: 523, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.495, mean reward: 1.915 [1.467, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.098], loss: 459.789917, mae: 2.325810, mean_q: 6.605868
 34840/100000: episode: 524, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 214.742, mean reward: 2.147 [1.442, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.319], loss: 153.938339, mae: 1.531588, mean_q: 6.326775
 34940/100000: episode: 525, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.245, mean reward: 1.882 [1.448, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.463, 10.098], loss: 305.390594, mae: 1.891973, mean_q: 6.329601
 35040/100000: episode: 526, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 184.021, mean reward: 1.840 [1.435, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.112, 10.127], loss: 0.667817, mae: 0.731419, mean_q: 5.768526
 35140/100000: episode: 527, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 223.597, mean reward: 2.236 [1.590, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.498, 10.386], loss: 153.728394, mae: 1.299782, mean_q: 5.926864
 35240/100000: episode: 528, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.931, mean reward: 1.899 [1.457, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.030, 10.109], loss: 0.696984, mae: 0.721002, mean_q: 5.537955
 35340/100000: episode: 529, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.538, mean reward: 1.955 [1.491, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.333, 10.161], loss: 154.734604, mae: 1.237105, mean_q: 5.837293
 35440/100000: episode: 530, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 198.792, mean reward: 1.988 [1.447, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.778, 10.368], loss: 610.356812, mae: 2.603565, mean_q: 6.452235
 35540/100000: episode: 531, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 195.528, mean reward: 1.955 [1.481, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.897, 10.179], loss: 156.399612, mae: 1.640668, mean_q: 6.333679
 35640/100000: episode: 532, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 195.550, mean reward: 1.956 [1.473, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.268, 10.098], loss: 153.630798, mae: 1.278814, mean_q: 5.893613
 35740/100000: episode: 533, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 190.608, mean reward: 1.906 [1.452, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.359, 10.098], loss: 0.928630, mae: 0.730189, mean_q: 5.558724
 35840/100000: episode: 534, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 193.198, mean reward: 1.932 [1.437, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.891, 10.098], loss: 153.420670, mae: 1.050176, mean_q: 5.530997
 35940/100000: episode: 535, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 217.668, mean reward: 2.177 [1.456, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.916, 10.374], loss: 155.444138, mae: 1.580570, mean_q: 6.057132
 36040/100000: episode: 536, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 178.953, mean reward: 1.790 [1.453, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.343, 10.098], loss: 0.986430, mae: 0.710709, mean_q: 5.415741
 36140/100000: episode: 537, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 198.633, mean reward: 1.986 [1.467, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.881, 10.098], loss: 0.669916, mae: 0.683627, mean_q: 5.322401
 36240/100000: episode: 538, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.529, mean reward: 1.935 [1.469, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.962, 10.158], loss: 307.304016, mae: 1.744555, mean_q: 5.871918
 36340/100000: episode: 539, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 198.596, mean reward: 1.986 [1.452, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.344, 10.256], loss: 457.226562, mae: 2.532283, mean_q: 6.182362
 36440/100000: episode: 540, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 183.782, mean reward: 1.838 [1.476, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.759, 10.098], loss: 456.911041, mae: 2.314848, mean_q: 6.169395
 36540/100000: episode: 541, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 194.902, mean reward: 1.949 [1.451, 5.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.098], loss: 304.680756, mae: 1.925072, mean_q: 5.940859
 36640/100000: episode: 542, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 201.746, mean reward: 2.017 [1.456, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.149, 10.222], loss: 154.289322, mae: 1.708377, mean_q: 6.220395
 36740/100000: episode: 543, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 209.730, mean reward: 2.097 [1.447, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.132, 10.231], loss: 305.241394, mae: 1.733999, mean_q: 5.898825
 36840/100000: episode: 544, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 184.464, mean reward: 1.845 [1.469, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.160], loss: 152.768524, mae: 1.221290, mean_q: 5.440477
 36940/100000: episode: 545, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 199.473, mean reward: 1.995 [1.471, 5.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.045, 10.098], loss: 153.353958, mae: 1.271487, mean_q: 5.541686
 37040/100000: episode: 546, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 187.690, mean reward: 1.877 [1.443, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.103, 10.170], loss: 305.223053, mae: 1.605889, mean_q: 5.419022
 37140/100000: episode: 547, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.251, mean reward: 1.933 [1.443, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.539, 10.098], loss: 152.600479, mae: 1.450458, mean_q: 5.684124
 37240/100000: episode: 548, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.826, mean reward: 1.978 [1.471, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.745, 10.098], loss: 152.614182, mae: 1.251395, mean_q: 5.391391
 37340/100000: episode: 549, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 220.638, mean reward: 2.206 [1.488, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.051, 10.242], loss: 305.035736, mae: 1.793591, mean_q: 5.676651
 37440/100000: episode: 550, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.217, mean reward: 1.902 [1.491, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.415, 10.157], loss: 0.701937, mae: 0.713645, mean_q: 5.001271
 37540/100000: episode: 551, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.682, mean reward: 1.927 [1.481, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.098], loss: 152.855164, mae: 1.250670, mean_q: 5.205036
 37640/100000: episode: 552, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.169, mean reward: 1.862 [1.445, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.998, 10.098], loss: 451.236511, mae: 2.215765, mean_q: 5.587099
 37740/100000: episode: 553, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.551, mean reward: 1.876 [1.456, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.060, 10.098], loss: 152.265427, mae: 1.415341, mean_q: 5.128112
 37840/100000: episode: 554, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 192.402, mean reward: 1.924 [1.445, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.098], loss: 151.191010, mae: 1.333170, mean_q: 5.189931
 37940/100000: episode: 555, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 189.506, mean reward: 1.895 [1.450, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.580, 10.098], loss: 0.527255, mae: 0.621613, mean_q: 4.642921
 38040/100000: episode: 556, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 214.638, mean reward: 2.146 [1.522, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.192, 10.107], loss: 151.914566, mae: 1.191910, mean_q: 4.922481
 38140/100000: episode: 557, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.585, mean reward: 1.986 [1.441, 6.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.895, 10.098], loss: 299.600037, mae: 1.388145, mean_q: 4.784898
 38240/100000: episode: 558, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.216, mean reward: 1.902 [1.448, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.098], loss: 150.665268, mae: 1.598208, mean_q: 5.076788
 38340/100000: episode: 559, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.438, mean reward: 1.964 [1.453, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.547, 10.150], loss: 444.206085, mae: 1.790598, mean_q: 4.906203
 38440/100000: episode: 560, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.672, mean reward: 1.857 [1.447, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.807, 10.107], loss: 148.933121, mae: 1.750655, mean_q: 5.021721
 38540/100000: episode: 561, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.510, mean reward: 2.005 [1.457, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.438, 10.098], loss: 298.647278, mae: 1.756447, mean_q: 5.137372
 38640/100000: episode: 562, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 196.609, mean reward: 1.966 [1.460, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.413, 10.098], loss: 1.475452, mae: 0.778495, mean_q: 4.503679
 38740/100000: episode: 563, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.773, mean reward: 1.888 [1.447, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.098], loss: 150.253082, mae: 1.086544, mean_q: 4.655130
 38840/100000: episode: 564, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 185.392, mean reward: 1.854 [1.445, 2.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.187], loss: 148.950562, mae: 1.182296, mean_q: 4.647542
 38940/100000: episode: 565, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 266.018, mean reward: 2.660 [1.541, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.356, 10.098], loss: 149.075287, mae: 1.114110, mean_q: 4.644157
 39040/100000: episode: 566, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.857, mean reward: 1.879 [1.469, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.028, 10.212], loss: 0.692156, mae: 0.564371, mean_q: 4.180218
 39140/100000: episode: 567, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 197.071, mean reward: 1.971 [1.479, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.292], loss: 0.416860, mae: 0.460656, mean_q: 4.099451
 39240/100000: episode: 568, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 201.487, mean reward: 2.015 [1.478, 4.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.796, 10.326], loss: 0.201132, mae: 0.399884, mean_q: 4.015399
 39340/100000: episode: 569, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 190.748, mean reward: 1.907 [1.469, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.688, 10.098], loss: 0.197664, mae: 0.374455, mean_q: 3.958684
 39440/100000: episode: 570, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 195.155, mean reward: 1.952 [1.441, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.626, 10.098], loss: 0.142833, mae: 0.349662, mean_q: 3.918761
 39540/100000: episode: 571, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 178.274, mean reward: 1.783 [1.504, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.948, 10.174], loss: 0.123007, mae: 0.336178, mean_q: 3.891680
 39640/100000: episode: 572, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 194.098, mean reward: 1.941 [1.468, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.319, 10.098], loss: 0.132430, mae: 0.348001, mean_q: 3.891880
 39740/100000: episode: 573, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 221.577, mean reward: 2.216 [1.554, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.470, 10.354], loss: 0.115678, mae: 0.336983, mean_q: 3.896734
 39840/100000: episode: 574, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 184.841, mean reward: 1.848 [1.457, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.137], loss: 0.117175, mae: 0.334721, mean_q: 3.903752
 39940/100000: episode: 575, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 184.665, mean reward: 1.847 [1.445, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.098], loss: 0.136897, mae: 0.349280, mean_q: 3.923400
 40040/100000: episode: 576, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 182.162, mean reward: 1.822 [1.463, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.281, 10.245], loss: 0.140236, mae: 0.350955, mean_q: 3.898479
 40140/100000: episode: 577, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 212.134, mean reward: 2.121 [1.490, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.441, 10.098], loss: 0.109694, mae: 0.326160, mean_q: 3.906740
 40240/100000: episode: 578, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 182.067, mean reward: 1.821 [1.443, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.208], loss: 0.109209, mae: 0.321968, mean_q: 3.897562
 40340/100000: episode: 579, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 234.287, mean reward: 2.343 [1.521, 19.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.735, 10.098], loss: 0.115762, mae: 0.335712, mean_q: 3.893868
 40440/100000: episode: 580, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.997, mean reward: 1.910 [1.448, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.776, 10.153], loss: 0.169872, mae: 0.344804, mean_q: 3.909954
 40540/100000: episode: 581, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.295, mean reward: 1.953 [1.446, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.605, 10.275], loss: 0.168653, mae: 0.331370, mean_q: 3.891486
 40640/100000: episode: 582, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.668, mean reward: 2.007 [1.452, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.402, 10.127], loss: 0.111938, mae: 0.329370, mean_q: 3.897959
 40740/100000: episode: 583, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.734, mean reward: 1.867 [1.489, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.148, 10.147], loss: 0.110606, mae: 0.323937, mean_q: 3.889081
 40840/100000: episode: 584, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.183, mean reward: 1.962 [1.460, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.012, 10.098], loss: 0.215031, mae: 0.341082, mean_q: 3.896626
 40940/100000: episode: 585, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.595, mean reward: 1.946 [1.468, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.567, 10.098], loss: 0.109608, mae: 0.326727, mean_q: 3.908133
 41040/100000: episode: 586, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 180.504, mean reward: 1.805 [1.435, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.581, 10.098], loss: 0.205667, mae: 0.339350, mean_q: 3.906749
 41140/100000: episode: 587, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 188.912, mean reward: 1.889 [1.513, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.353, 10.098], loss: 0.208422, mae: 0.344443, mean_q: 3.907190
 41240/100000: episode: 588, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 206.329, mean reward: 2.063 [1.475, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.728, 10.098], loss: 0.249816, mae: 0.340974, mean_q: 3.898315
 41340/100000: episode: 589, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 221.902, mean reward: 2.219 [1.438, 10.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.292, 10.098], loss: 0.104790, mae: 0.320950, mean_q: 3.894183
 41440/100000: episode: 590, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 175.771, mean reward: 1.758 [1.435, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.489, 10.133], loss: 0.271936, mae: 0.349458, mean_q: 3.913964
 41540/100000: episode: 591, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.194, mean reward: 1.832 [1.456, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.098], loss: 0.146498, mae: 0.339426, mean_q: 3.917255
 41640/100000: episode: 592, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 181.135, mean reward: 1.811 [1.469, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.898, 10.212], loss: 0.151398, mae: 0.320094, mean_q: 3.891693
 41740/100000: episode: 593, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 220.568, mean reward: 2.206 [1.464, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.621, 10.098], loss: 0.113863, mae: 0.327211, mean_q: 3.887871
 41840/100000: episode: 594, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 183.602, mean reward: 1.836 [1.460, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.098], loss: 0.120250, mae: 0.333586, mean_q: 3.896243
 41940/100000: episode: 595, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.674, mean reward: 2.007 [1.483, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.412, 10.098], loss: 0.195745, mae: 0.321632, mean_q: 3.876050
 42040/100000: episode: 596, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 176.510, mean reward: 1.765 [1.450, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.374, 10.156], loss: 0.159271, mae: 0.325903, mean_q: 3.912356
 42140/100000: episode: 597, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.665, mean reward: 1.907 [1.466, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.098], loss: 0.114405, mae: 0.316684, mean_q: 3.887541
 42240/100000: episode: 598, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.559, mean reward: 1.996 [1.467, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.236, 10.098], loss: 0.301990, mae: 0.347936, mean_q: 3.928708
 42340/100000: episode: 599, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 191.771, mean reward: 1.918 [1.476, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.151, 10.098], loss: 0.150736, mae: 0.310972, mean_q: 3.883934
 42440/100000: episode: 600, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 186.672, mean reward: 1.867 [1.473, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.020, 10.221], loss: 0.195771, mae: 0.319614, mean_q: 3.882429
 42540/100000: episode: 601, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 193.710, mean reward: 1.937 [1.469, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.403, 10.098], loss: 0.148282, mae: 0.314061, mean_q: 3.878557
 42640/100000: episode: 602, duration: 1.416s, episode steps: 100, steps per second: 71, episode reward: 190.121, mean reward: 1.901 [1.455, 4.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.473, 10.232], loss: 0.153147, mae: 0.320194, mean_q: 3.873629
 42740/100000: episode: 603, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 183.175, mean reward: 1.832 [1.436, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.400, 10.098], loss: 0.161904, mae: 0.322481, mean_q: 3.886858
 42840/100000: episode: 604, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 178.093, mean reward: 1.781 [1.441, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.114, 10.098], loss: 0.150076, mae: 0.321431, mean_q: 3.868381
 42940/100000: episode: 605, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 181.474, mean reward: 1.815 [1.446, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.731, 10.378], loss: 0.110513, mae: 0.304391, mean_q: 3.856915
 43040/100000: episode: 606, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 208.106, mean reward: 2.081 [1.450, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.383, 10.176], loss: 0.161075, mae: 0.325873, mean_q: 3.854834
 43140/100000: episode: 607, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.569, mean reward: 1.846 [1.448, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.326, 10.098], loss: 0.190895, mae: 0.311081, mean_q: 3.846607
 43240/100000: episode: 608, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.173, mean reward: 1.932 [1.469, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.092, 10.232], loss: 0.104879, mae: 0.314801, mean_q: 3.862758
 43340/100000: episode: 609, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 203.507, mean reward: 2.035 [1.522, 8.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.657, 10.098], loss: 0.099964, mae: 0.301256, mean_q: 3.832155
 43440/100000: episode: 610, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 180.614, mean reward: 1.806 [1.473, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.250, 10.098], loss: 0.168516, mae: 0.318930, mean_q: 3.876527
 43540/100000: episode: 611, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 184.811, mean reward: 1.848 [1.464, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.541, 10.136], loss: 0.099228, mae: 0.306267, mean_q: 3.835715
 43640/100000: episode: 612, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 211.286, mean reward: 2.113 [1.446, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.602, 10.098], loss: 0.229900, mae: 0.338152, mean_q: 3.886119
 43740/100000: episode: 613, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 190.813, mean reward: 1.908 [1.472, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.453, 10.179], loss: 0.157179, mae: 0.318004, mean_q: 3.865310
 43840/100000: episode: 614, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 189.967, mean reward: 1.900 [1.445, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.218, 10.098], loss: 0.162988, mae: 0.325091, mean_q: 3.879565
 43940/100000: episode: 615, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.602, mean reward: 2.056 [1.519, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.861, 10.098], loss: 0.139779, mae: 0.323655, mean_q: 3.840676
 44040/100000: episode: 616, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 189.285, mean reward: 1.893 [1.470, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.158], loss: 0.160299, mae: 0.321698, mean_q: 3.822927
 44140/100000: episode: 617, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.561, mean reward: 1.836 [1.467, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.863, 10.115], loss: 0.173967, mae: 0.325825, mean_q: 3.836838
 44240/100000: episode: 618, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 224.813, mean reward: 2.248 [1.498, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.167, 10.505], loss: 0.212773, mae: 0.326523, mean_q: 3.837924
 44340/100000: episode: 619, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 202.166, mean reward: 2.022 [1.477, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.269], loss: 0.147799, mae: 0.310964, mean_q: 3.831173
[Info] 1-TH LEVEL FOUND: 5.260514736175537, Considering 10/90 traces
 44440/100000: episode: 620, duration: 5.263s, episode steps: 100, steps per second: 19, episode reward: 192.082, mean reward: 1.921 [1.483, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.325, 10.118], loss: 0.148304, mae: 0.316871, mean_q: 3.859652
 44459/100000: episode: 621, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 35.505, mean reward: 1.869 [1.576, 2.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.513, 10.100], loss: 0.156968, mae: 0.315180, mean_q: 3.840365
 44478/100000: episode: 622, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 47.640, mean reward: 2.507 [1.921, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.962, 10.100], loss: 0.366567, mae: 0.353573, mean_q: 3.871813
 44535/100000: episode: 623, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 134.817, mean reward: 2.365 [1.642, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.258, 10.100], loss: 0.111351, mae: 0.315528, mean_q: 3.850809
 44572/100000: episode: 624, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 122.814, mean reward: 3.319 [2.197, 9.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.475, 10.100], loss: 0.272092, mae: 0.347002, mean_q: 3.884915
 44595/100000: episode: 625, duration: 0.110s, episode steps: 23, steps per second: 208, episode reward: 56.769, mean reward: 2.468 [1.768, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.563, 10.100], loss: 0.083899, mae: 0.294990, mean_q: 3.847751
 44614/100000: episode: 626, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 46.711, mean reward: 2.458 [1.769, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.383, 10.100], loss: 0.093960, mae: 0.312963, mean_q: 3.907650
 44664/100000: episode: 627, duration: 0.278s, episode steps: 50, steps per second: 180, episode reward: 127.561, mean reward: 2.551 [1.810, 5.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.386, 10.344], loss: 0.291633, mae: 0.334971, mean_q: 3.889025
 44691/100000: episode: 628, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 64.134, mean reward: 2.375 [1.687, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.332, 10.100], loss: 0.109498, mae: 0.317458, mean_q: 3.920595
 44720/100000: episode: 629, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 62.692, mean reward: 2.162 [1.606, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.790, 10.100], loss: 0.141175, mae: 0.329062, mean_q: 3.914395
 44739/100000: episode: 630, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 50.312, mean reward: 2.648 [1.923, 4.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.825, 10.100], loss: 0.156747, mae: 0.351539, mean_q: 3.964630
 44791/100000: episode: 631, duration: 0.264s, episode steps: 52, steps per second: 197, episode reward: 221.213, mean reward: 4.254 [2.430, 10.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-1.378, 10.100], loss: 0.131123, mae: 0.322112, mean_q: 3.933843
 44816/100000: episode: 632, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 64.204, mean reward: 2.568 [1.884, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.036, 10.100], loss: 0.138173, mae: 0.324890, mean_q: 3.915794
 44866/100000: episode: 633, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 137.298, mean reward: 2.746 [2.038, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.839, 10.415], loss: 0.122788, mae: 0.323798, mean_q: 3.923627
 44895/100000: episode: 634, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 68.296, mean reward: 2.355 [1.637, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.704, 10.100], loss: 0.138081, mae: 0.319223, mean_q: 3.964496
 44924/100000: episode: 635, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 76.046, mean reward: 2.622 [1.917, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.927, 10.100], loss: 0.114544, mae: 0.333288, mean_q: 3.984644
 44949/100000: episode: 636, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 63.409, mean reward: 2.536 [1.984, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.097, 10.100], loss: 0.144205, mae: 0.338463, mean_q: 3.965658
 44972/100000: episode: 637, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 57.727, mean reward: 2.510 [2.052, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.285, 10.100], loss: 0.114366, mae: 0.334921, mean_q: 3.946837
 45024/100000: episode: 638, duration: 0.262s, episode steps: 52, steps per second: 199, episode reward: 160.605, mean reward: 3.089 [1.928, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.443, 10.100], loss: 0.113572, mae: 0.326470, mean_q: 3.997312
 45051/100000: episode: 639, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 69.791, mean reward: 2.585 [1.968, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.264, 10.100], loss: 0.130117, mae: 0.353897, mean_q: 3.994076
 45076/100000: episode: 640, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 58.868, mean reward: 2.355 [1.838, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.410, 10.100], loss: 0.368751, mae: 0.374041, mean_q: 4.019101
 45105/100000: episode: 641, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 67.172, mean reward: 2.316 [1.495, 4.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.169, 10.100], loss: 0.161237, mae: 0.362287, mean_q: 4.013519
 45128/100000: episode: 642, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 59.926, mean reward: 2.605 [1.854, 6.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.677, 10.100], loss: 0.200530, mae: 0.389358, mean_q: 4.077369
 45178/100000: episode: 643, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 114.024, mean reward: 2.280 [1.497, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.604, 10.100], loss: 0.185239, mae: 0.384048, mean_q: 4.018317
 45203/100000: episode: 644, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 79.269, mean reward: 3.171 [2.450, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.045, 10.100], loss: 0.136522, mae: 0.361144, mean_q: 4.026056
 45240/100000: episode: 645, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 94.692, mean reward: 2.559 [1.920, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.309, 10.100], loss: 0.171499, mae: 0.363862, mean_q: 4.073984
 45269/100000: episode: 646, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 78.157, mean reward: 2.695 [1.944, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.272, 10.100], loss: 0.135112, mae: 0.347599, mean_q: 4.065925
 45296/100000: episode: 647, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 63.011, mean reward: 2.334 [1.844, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-2.035, 10.100], loss: 0.144298, mae: 0.372179, mean_q: 4.078621
 45333/100000: episode: 648, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 82.725, mean reward: 2.236 [1.448, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.383, 10.286], loss: 0.201680, mae: 0.386478, mean_q: 4.089668
 45356/100000: episode: 649, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 49.620, mean reward: 2.157 [1.862, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.148, 10.100], loss: 0.177613, mae: 0.373398, mean_q: 4.059620
 45375/100000: episode: 650, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 47.474, mean reward: 2.499 [2.059, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.693, 10.100], loss: 0.147187, mae: 0.360578, mean_q: 4.118043
 45427/100000: episode: 651, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 129.991, mean reward: 2.500 [1.514, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.529, 10.100], loss: 0.139648, mae: 0.359864, mean_q: 4.047142
 45456/100000: episode: 652, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 72.147, mean reward: 2.488 [1.704, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.209, 10.100], loss: 0.131331, mae: 0.349115, mean_q: 4.097216
 45506/100000: episode: 653, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 208.670, mean reward: 4.173 [2.020, 11.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.856, 10.502], loss: 0.192657, mae: 0.385577, mean_q: 4.103909
 45556/100000: episode: 654, duration: 0.277s, episode steps: 50, steps per second: 181, episode reward: 114.472, mean reward: 2.289 [1.518, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.735, 10.174], loss: 0.232538, mae: 0.414025, mean_q: 4.071530
 45606/100000: episode: 655, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 187.142, mean reward: 3.743 [2.272, 7.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.049, 10.514], loss: 0.212349, mae: 0.406212, mean_q: 4.128270
 45625/100000: episode: 656, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 37.197, mean reward: 1.958 [1.749, 2.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.164, 10.100], loss: 0.224890, mae: 0.425233, mean_q: 4.240444
 45662/100000: episode: 657, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 120.251, mean reward: 3.250 [2.027, 7.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.723, 10.100], loss: 0.193647, mae: 0.401286, mean_q: 4.149817
 45691/100000: episode: 658, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 69.463, mean reward: 2.395 [1.763, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.100], loss: 0.181437, mae: 0.386929, mean_q: 4.135420
 45716/100000: episode: 659, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 63.721, mean reward: 2.549 [1.731, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.257, 10.100], loss: 0.222485, mae: 0.416641, mean_q: 4.189275
 45743/100000: episode: 660, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 58.396, mean reward: 2.163 [1.745, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.778, 10.100], loss: 0.178031, mae: 0.377182, mean_q: 4.180363
 45766/100000: episode: 661, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 44.173, mean reward: 1.921 [1.560, 2.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.069, 10.100], loss: 0.214654, mae: 0.399583, mean_q: 4.166830
 45783/100000: episode: 662, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 48.488, mean reward: 2.852 [2.145, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.607, 10.100], loss: 0.205348, mae: 0.406013, mean_q: 4.173066
 45800/100000: episode: 663, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 38.820, mean reward: 2.284 [1.801, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.851, 10.100], loss: 0.188440, mae: 0.403854, mean_q: 4.169034
 45825/100000: episode: 664, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 61.771, mean reward: 2.471 [1.830, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.570, 10.100], loss: 0.259892, mae: 0.417556, mean_q: 4.244095
 45875/100000: episode: 665, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 113.590, mean reward: 2.272 [1.611, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.067, 10.178], loss: 0.197042, mae: 0.398440, mean_q: 4.188365
 45925/100000: episode: 666, duration: 0.252s, episode steps: 50, steps per second: 199, episode reward: 107.213, mean reward: 2.144 [1.589, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.375, 10.250], loss: 0.220756, mae: 0.419308, mean_q: 4.192272
 45942/100000: episode: 667, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 58.244, mean reward: 3.426 [2.077, 6.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.323, 10.100], loss: 0.164831, mae: 0.380752, mean_q: 4.270271
 45959/100000: episode: 668, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 69.310, mean reward: 4.077 [1.967, 11.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.545, 10.100], loss: 0.191688, mae: 0.376311, mean_q: 4.160662
 45988/100000: episode: 669, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 64.618, mean reward: 2.228 [1.553, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.142, 10.100], loss: 0.252253, mae: 0.414230, mean_q: 4.277262
 46005/100000: episode: 670, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 40.350, mean reward: 2.374 [1.975, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.244, 10.100], loss: 0.172091, mae: 0.410375, mean_q: 4.154843
 46055/100000: episode: 671, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 117.216, mean reward: 2.344 [1.595, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.380, 10.237], loss: 0.205842, mae: 0.426604, mean_q: 4.252428
 46107/100000: episode: 672, duration: 0.266s, episode steps: 52, steps per second: 196, episode reward: 115.063, mean reward: 2.213 [1.586, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.355, 10.262], loss: 0.231939, mae: 0.424265, mean_q: 4.256186
 46132/100000: episode: 673, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 81.136, mean reward: 3.245 [2.346, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.366, 10.100], loss: 0.219894, mae: 0.428289, mean_q: 4.325441
 46161/100000: episode: 674, duration: 0.139s, episode steps: 29, steps per second: 209, episode reward: 59.603, mean reward: 2.055 [1.726, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.303, 10.100], loss: 0.185246, mae: 0.392890, mean_q: 4.235928
 46213/100000: episode: 675, duration: 0.262s, episode steps: 52, steps per second: 198, episode reward: 142.419, mean reward: 2.739 [1.870, 6.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.391, 10.100], loss: 0.195080, mae: 0.411626, mean_q: 4.252874
 46242/100000: episode: 676, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 73.623, mean reward: 2.539 [1.814, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.523, 10.100], loss: 0.230159, mae: 0.445025, mean_q: 4.304598
 46265/100000: episode: 677, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 51.698, mean reward: 2.248 [1.722, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.283, 10.100], loss: 0.176458, mae: 0.416697, mean_q: 4.307096
 46288/100000: episode: 678, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 63.447, mean reward: 2.759 [2.205, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.323, 10.100], loss: 0.169822, mae: 0.395703, mean_q: 4.255708
 46340/100000: episode: 679, duration: 0.262s, episode steps: 52, steps per second: 198, episode reward: 113.923, mean reward: 2.191 [1.670, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.497, 10.100], loss: 0.214486, mae: 0.426349, mean_q: 4.353544
 46365/100000: episode: 680, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 75.999, mean reward: 3.040 [2.031, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.601, 10.100], loss: 0.260707, mae: 0.457728, mean_q: 4.348227
 46402/100000: episode: 681, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 135.783, mean reward: 3.670 [2.287, 28.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.736, 10.100], loss: 0.232772, mae: 0.431677, mean_q: 4.302766
 46459/100000: episode: 682, duration: 0.312s, episode steps: 57, steps per second: 183, episode reward: 141.798, mean reward: 2.488 [1.509, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.560, 10.148], loss: 0.222027, mae: 0.425260, mean_q: 4.298400
 46484/100000: episode: 683, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 89.834, mean reward: 3.593 [2.499, 7.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.823, 10.100], loss: 0.615860, mae: 0.468225, mean_q: 4.344627
 46503/100000: episode: 684, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 38.147, mean reward: 2.008 [1.666, 2.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.357, 10.100], loss: 0.213999, mae: 0.464406, mean_q: 4.350788
 46522/100000: episode: 685, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 42.085, mean reward: 2.215 [1.613, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.528, 10.100], loss: 0.144953, mae: 0.391801, mean_q: 4.367072
 46572/100000: episode: 686, duration: 0.271s, episode steps: 50, steps per second: 185, episode reward: 191.865, mean reward: 3.837 [2.352, 10.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.096, 10.449], loss: 0.195084, mae: 0.413109, mean_q: 4.389842
 46609/100000: episode: 687, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 75.365, mean reward: 2.037 [1.641, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.371, 10.100], loss: 0.557195, mae: 0.498385, mean_q: 4.415032
 46646/100000: episode: 688, duration: 0.207s, episode steps: 37, steps per second: 179, episode reward: 91.559, mean reward: 2.475 [2.036, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.399, 10.100], loss: 0.247272, mae: 0.436043, mean_q: 4.414128
 46696/100000: episode: 689, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 165.746, mean reward: 3.315 [2.273, 6.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.737, 10.424], loss: 0.428789, mae: 0.472050, mean_q: 4.401254
 46713/100000: episode: 690, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 42.252, mean reward: 2.485 [1.977, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.257, 10.100], loss: 0.202776, mae: 0.439727, mean_q: 4.370739
 46740/100000: episode: 691, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 90.672, mean reward: 3.358 [2.224, 7.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.708, 10.100], loss: 0.246762, mae: 0.478854, mean_q: 4.495265
 46765/100000: episode: 692, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 56.836, mean reward: 2.273 [1.584, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.104, 10.100], loss: 0.673654, mae: 0.507332, mean_q: 4.513254
 46782/100000: episode: 693, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 56.879, mean reward: 3.346 [1.892, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.525, 10.100], loss: 0.436596, mae: 0.510218, mean_q: 4.513121
 46805/100000: episode: 694, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 50.588, mean reward: 2.199 [1.714, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.333, 10.100], loss: 0.234373, mae: 0.459404, mean_q: 4.511492
 46862/100000: episode: 695, duration: 0.316s, episode steps: 57, steps per second: 180, episode reward: 118.079, mean reward: 2.072 [1.474, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-1.148, 10.100], loss: 0.203787, mae: 0.431655, mean_q: 4.440978
 46919/100000: episode: 696, duration: 0.279s, episode steps: 57, steps per second: 204, episode reward: 111.734, mean reward: 1.960 [1.488, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.305, 10.274], loss: 0.170964, mae: 0.406151, mean_q: 4.441241
 46942/100000: episode: 697, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 61.091, mean reward: 2.656 [2.041, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.258, 10.100], loss: 0.203416, mae: 0.435686, mean_q: 4.484611
 46979/100000: episode: 698, duration: 0.182s, episode steps: 37, steps per second: 204, episode reward: 102.334, mean reward: 2.766 [1.842, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.453, 10.100], loss: 0.283640, mae: 0.451636, mean_q: 4.510398
 47036/100000: episode: 699, duration: 0.293s, episode steps: 57, steps per second: 195, episode reward: 112.669, mean reward: 1.977 [1.488, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.671, 10.100], loss: 0.465623, mae: 0.507065, mean_q: 4.550436
 47065/100000: episode: 700, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 69.504, mean reward: 2.397 [1.903, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.154, 10.100], loss: 0.221911, mae: 0.427631, mean_q: 4.486601
 47088/100000: episode: 701, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 56.460, mean reward: 2.455 [1.821, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.905, 10.100], loss: 0.287719, mae: 0.466462, mean_q: 4.487842
 47107/100000: episode: 702, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 54.406, mean reward: 2.863 [1.718, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.301, 10.100], loss: 0.283605, mae: 0.479903, mean_q: 4.549246
 47159/100000: episode: 703, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 112.520, mean reward: 2.164 [1.512, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.196, 10.154], loss: 0.246194, mae: 0.464877, mean_q: 4.511047
 47209/100000: episode: 704, duration: 0.264s, episode steps: 50, steps per second: 189, episode reward: 170.919, mean reward: 3.418 [1.998, 6.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.378, 10.468], loss: 0.207283, mae: 0.441678, mean_q: 4.490867
 47266/100000: episode: 705, duration: 0.276s, episode steps: 57, steps per second: 206, episode reward: 119.195, mean reward: 2.091 [1.479, 10.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.392, 10.152], loss: 0.624528, mae: 0.518041, mean_q: 4.561956
 47316/100000: episode: 706, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 115.555, mean reward: 2.311 [1.513, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.794, 10.100], loss: 0.254245, mae: 0.463147, mean_q: 4.542190
 47333/100000: episode: 707, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 38.094, mean reward: 2.241 [2.012, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.326, 10.100], loss: 0.359379, mae: 0.462044, mean_q: 4.572043
 47385/100000: episode: 708, duration: 0.278s, episode steps: 52, steps per second: 187, episode reward: 151.240, mean reward: 2.908 [2.129, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.373, 10.100], loss: 0.296240, mae: 0.467721, mean_q: 4.576036
 47414/100000: episode: 709, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 68.956, mean reward: 2.378 [1.961, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.335, 10.100], loss: 0.270820, mae: 0.471876, mean_q: 4.624406
[Info] 2-TH LEVEL FOUND: 7.620820045471191, Considering 10/90 traces
 47431/100000: episode: 710, duration: 4.190s, episode steps: 17, steps per second: 4, episode reward: 43.380, mean reward: 2.552 [1.736, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.428, 10.100], loss: 0.281578, mae: 0.501814, mean_q: 4.620267
 47436/100000: episode: 711, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 17.133, mean reward: 3.427 [2.437, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.491, 10.100], loss: 0.532039, mae: 0.656655, mean_q: 4.922519
 47439/100000: episode: 712, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 34.638, mean reward: 11.546 [8.664, 14.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.546, 10.100], loss: 0.222289, mae: 0.488532, mean_q: 4.497763
 47452/100000: episode: 713, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 48.421, mean reward: 3.725 [3.173, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.457, 10.100], loss: 0.253371, mae: 0.437104, mean_q: 4.499652
 47465/100000: episode: 714, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 48.554, mean reward: 3.735 [2.837, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.401, 10.100], loss: 0.247383, mae: 0.445328, mean_q: 4.584304
 47478/100000: episode: 715, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 49.229, mean reward: 3.787 [3.010, 4.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.302, 10.100], loss: 0.275952, mae: 0.463900, mean_q: 4.650489
 47515/100000: episode: 716, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 185.577, mean reward: 5.016 [2.896, 11.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.308, 10.450], loss: 0.634984, mae: 0.556024, mean_q: 4.754655
[Info] FALSIFICATION!
 47533/100000: episode: 717, duration: 0.253s, episode steps: 18, steps per second: 71, episode reward: 1615.498, mean reward: 89.750 [5.103, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.817 [-0.435, 9.482], loss: 0.268856, mae: 0.497056, mean_q: 4.602517
 47536/100000: episode: 718, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 23.505, mean reward: 7.835 [5.237, 11.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.446, 10.100], loss: 491.771393, mae: 3.834002, mean_q: 5.038481
 47556/100000: episode: 719, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 48.851, mean reward: 2.443 [2.131, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.963, 10.100], loss: 75.069847, mae: 1.967814, mean_q: 5.546674
 47580/100000: episode: 720, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 78.561, mean reward: 3.273 [1.984, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.151, 10.100], loss: 698.931335, mae: 3.276389, mean_q: 5.514391
 47593/100000: episode: 721, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 61.221, mean reward: 4.709 [2.901, 7.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.399, 10.100], loss: 1172.042480, mae: 5.200032, mean_q: 7.272454
 47620/100000: episode: 722, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 116.050, mean reward: 4.298 [2.301, 11.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.879, 10.100], loss: 9.218136, mae: 2.683496, mean_q: 6.538387
 47633/100000: episode: 723, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 51.116, mean reward: 3.932 [2.924, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.581, 10.100], loss: 0.982484, mae: 1.056907, mean_q: 4.701458
 47657/100000: episode: 724, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 105.003, mean reward: 4.375 [2.657, 12.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.556, 10.100], loss: 0.864203, mae: 0.874196, mean_q: 4.655198
 47662/100000: episode: 725, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 16.212, mean reward: 3.242 [2.787, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.496, 10.100], loss: 0.721121, mae: 0.655763, mean_q: 5.037435
 47665/100000: episode: 726, duration: 0.026s, episode steps: 3, steps per second: 118, episode reward: 15.603, mean reward: 5.201 [5.114, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.476, 10.100], loss: 1.445172, mae: 0.768035, mean_q: 4.484455
 47670/100000: episode: 727, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 28.122, mean reward: 5.624 [4.496, 6.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.533, 10.100], loss: 0.447796, mae: 0.618818, mean_q: 4.728859
 47673/100000: episode: 728, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 19.822, mean reward: 6.607 [4.727, 9.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.520, 10.100], loss: 0.902445, mae: 0.797365, mean_q: 4.938028
 47686/100000: episode: 729, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 54.635, mean reward: 4.203 [2.638, 5.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.497, 10.100], loss: 116.374458, mae: 1.536055, mean_q: 5.044967
 47691/100000: episode: 730, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 20.579, mean reward: 4.116 [3.013, 5.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.515, 10.100], loss: 0.712696, mae: 0.844839, mean_q: 5.134042
 47728/100000: episode: 731, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 116.074, mean reward: 3.137 [2.215, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.129, 10.465], loss: 4.938584, mae: 0.883169, mean_q: 5.269897
 47746/100000: episode: 732, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 68.718, mean reward: 3.818 [2.423, 5.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.753, 10.100], loss: 0.516002, mae: 0.629068, mean_q: 4.882401
 47751/100000: episode: 733, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 23.273, mean reward: 4.655 [3.972, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.456, 10.100], loss: 1.190143, mae: 0.753293, mean_q: 5.006956
 47760/100000: episode: 734, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 24.529, mean reward: 2.725 [2.374, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.334, 10.100], loss: 0.558792, mae: 0.653826, mean_q: 4.774420
 47773/100000: episode: 735, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 53.525, mean reward: 4.117 [2.651, 7.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.469, 10.100], loss: 0.833126, mae: 0.706225, mean_q: 4.958023
 47793/100000: episode: 736, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 96.526, mean reward: 4.826 [3.596, 8.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.556, 10.100], loss: 5.181517, mae: 0.808624, mean_q: 5.079342
 47811/100000: episode: 737, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 73.420, mean reward: 4.079 [2.232, 7.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.225, 10.100], loss: 0.859949, mae: 0.759284, mean_q: 5.134646
 47829/100000: episode: 738, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 69.810, mean reward: 3.878 [3.189, 5.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.443, 10.100], loss: 0.888302, mae: 0.781304, mean_q: 5.234762
 47838/100000: episode: 739, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 27.807, mean reward: 3.090 [2.580, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.494, 10.100], loss: 0.880786, mae: 0.692847, mean_q: 5.162217
 47847/100000: episode: 740, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 33.248, mean reward: 3.694 [3.422, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.333, 10.100], loss: 18.290209, mae: 1.098354, mean_q: 5.035552
 47871/100000: episode: 741, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 62.071, mean reward: 2.586 [2.026, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.195, 10.100], loss: 4.454230, mae: 0.847018, mean_q: 5.223925
 47876/100000: episode: 742, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 22.877, mean reward: 4.575 [4.012, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.522, 10.100], loss: 0.404674, mae: 0.648982, mean_q: 5.196764
 47881/100000: episode: 743, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 15.268, mean reward: 3.054 [2.526, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.405, 10.100], loss: 0.517899, mae: 0.631294, mean_q: 5.016831
 47901/100000: episode: 744, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 80.631, mean reward: 4.032 [2.432, 7.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.408, 10.100], loss: 760.578247, mae: 3.548472, mean_q: 6.593285
 47914/100000: episode: 745, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 49.039, mean reward: 3.772 [2.661, 7.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.241, 10.100], loss: 1140.303711, mae: 4.439444, mean_q: 6.324570
 47919/100000: episode: 746, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 21.537, mean reward: 4.307 [3.385, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.335, 10.100], loss: 8.592992, mae: 2.452965, mean_q: 7.152750
 47939/100000: episode: 747, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 64.931, mean reward: 3.247 [2.715, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.266, 10.100], loss: 2.365245, mae: 1.170412, mean_q: 5.492884
 47944/100000: episode: 748, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 15.635, mean reward: 3.127 [2.342, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.560, 10.100], loss: 0.881116, mae: 0.876305, mean_q: 4.944794
 47949/100000: episode: 749, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 14.769, mean reward: 2.954 [2.537, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.450, 10.100], loss: 1.217378, mae: 0.921950, mean_q: 4.731308
 47976/100000: episode: 750, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 85.569, mean reward: 3.169 [2.020, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.173, 10.100], loss: 6.355346, mae: 1.862081, mean_q: 6.407386
 47981/100000: episode: 751, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 18.007, mean reward: 3.601 [3.160, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.379, 10.100], loss: 4.587388, mae: 1.392296, mean_q: 5.709332
 47986/100000: episode: 752, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 17.049, mean reward: 3.410 [3.103, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.401, 10.100], loss: 4.474164, mae: 1.148946, mean_q: 5.412491
 48010/100000: episode: 753, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 60.960, mean reward: 2.540 [1.705, 4.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.276, 10.100], loss: 66.416878, mae: 1.476508, mean_q: 5.209634
 48023/100000: episode: 754, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 40.154, mean reward: 3.089 [2.560, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.399, 10.100], loss: 1.274201, mae: 0.919264, mean_q: 5.127961
 48036/100000: episode: 755, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 61.051, mean reward: 4.696 [3.738, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.358, 10.100], loss: 118.286125, mae: 1.849142, mean_q: 5.505539
 48056/100000: episode: 756, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 66.164, mean reward: 3.308 [2.486, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.389, 10.100], loss: 743.699341, mae: 3.385017, mean_q: 6.517908
 48076/100000: episode: 757, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 68.078, mean reward: 3.404 [2.903, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.392, 10.100], loss: 4.329970, mae: 1.497430, mean_q: 5.623975
 48081/100000: episode: 758, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 14.610, mean reward: 2.922 [2.442, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.444, 10.100], loss: 1.608072, mae: 0.950962, mean_q: 5.202456
 48101/100000: episode: 759, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 52.396, mean reward: 2.620 [1.892, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.323, 10.100], loss: 1.525493, mae: 0.958587, mean_q: 5.010929
 48138/100000: episode: 760, duration: 0.195s, episode steps: 37, steps per second: 189, episode reward: 177.613, mean reward: 4.800 [2.594, 8.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.900, 10.492], loss: 832.223511, mae: 3.203414, mean_q: 5.810567
 48141/100000: episode: 761, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 14.325, mean reward: 4.775 [4.264, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.610, 10.100], loss: 16.356699, mae: 4.199022, mean_q: 8.938018
 48146/100000: episode: 762, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 27.285, mean reward: 5.457 [3.515, 7.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.461, 10.100], loss: 21.776302, mae: 4.093981, mean_q: 8.731309
 48151/100000: episode: 763, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 13.803, mean reward: 2.761 [2.321, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.447, 10.100], loss: 297.099823, mae: 5.289511, mean_q: 7.916163
 48164/100000: episode: 764, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 47.457, mean reward: 3.651 [2.989, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.585, 10.100], loss: 7.018107, mae: 1.508513, mean_q: 5.396815
 48177/100000: episode: 765, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 48.666, mean reward: 3.744 [3.099, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.442, 10.100], loss: 1.916595, mae: 1.138349, mean_q: 5.228096
 48197/100000: episode: 766, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 118.589, mean reward: 5.929 [3.119, 13.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.520, 10.100], loss: 740.041870, mae: 2.790044, mean_q: 5.595366
 48234/100000: episode: 767, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 140.983, mean reward: 3.810 [2.098, 10.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.344, 10.400], loss: 400.019348, mae: 2.580126, mean_q: 6.337716
 48271/100000: episode: 768, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 101.909, mean reward: 2.754 [1.902, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.392, 10.360], loss: 399.687836, mae: 3.044625, mean_q: 6.828697
 48276/100000: episode: 769, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 23.914, mean reward: 4.783 [4.109, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.410, 10.100], loss: 3.744797, mae: 1.473570, mean_q: 5.893845
 48285/100000: episode: 770, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 27.151, mean reward: 3.017 [2.624, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.385, 10.100], loss: 1.990081, mae: 1.166664, mean_q: 5.248486
 48290/100000: episode: 771, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 16.688, mean reward: 3.338 [2.923, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.474, 10.100], loss: 2.240790, mae: 1.278626, mean_q: 5.383199
 48295/100000: episode: 772, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 17.701, mean reward: 3.540 [3.269, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.321, 10.100], loss: 2.421443, mae: 1.184847, mean_q: 5.285944
 48300/100000: episode: 773, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 17.833, mean reward: 3.567 [2.683, 5.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.574, 10.100], loss: 3.504223, mae: 1.040547, mean_q: 4.768085
 48318/100000: episode: 774, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 87.095, mean reward: 4.839 [3.346, 9.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.345, 10.100], loss: 94.436722, mae: 1.751787, mean_q: 5.656738
 48321/100000: episode: 775, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 29.231, mean reward: 9.744 [5.387, 16.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.577, 10.100], loss: 2.862225, mae: 1.034908, mean_q: 5.378157
 48345/100000: episode: 776, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 106.485, mean reward: 4.437 [2.716, 6.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.537, 10.100], loss: 2.541064, mae: 1.083376, mean_q: 5.703172
 48354/100000: episode: 777, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 33.159, mean reward: 3.684 [3.186, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.386, 10.100], loss: 175.841537, mae: 2.313070, mean_q: 6.033792
 48359/100000: episode: 778, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 12.335, mean reward: 2.467 [2.172, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.578, 10.100], loss: 2.181046, mae: 1.148746, mean_q: 5.547785
 48377/100000: episode: 779, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 89.167, mean reward: 4.954 [3.350, 10.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.426, 10.100], loss: 2.149007, mae: 0.996346, mean_q: 5.713774
 48382/100000: episode: 780, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 22.477, mean reward: 4.495 [4.087, 5.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.554, 10.100], loss: 1.040284, mae: 0.871365, mean_q: 5.313973
 48419/100000: episode: 781, duration: 0.216s, episode steps: 37, steps per second: 172, episode reward: 145.161, mean reward: 3.923 [1.772, 6.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.745, 10.219], loss: 403.991974, mae: 2.777974, mean_q: 6.295887
 48428/100000: episode: 782, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 31.874, mean reward: 3.542 [3.051, 4.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.442, 10.100], loss: 5.086973, mae: 1.362398, mean_q: 6.450062
 48452/100000: episode: 783, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 65.702, mean reward: 2.738 [1.885, 5.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.250, 10.100], loss: 614.401428, mae: 2.328623, mean_q: 5.341442
 48465/100000: episode: 784, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 41.977, mean reward: 3.229 [2.766, 4.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.224, 10.100], loss: 13.602880, mae: 2.112192, mean_q: 7.027247
 48470/100000: episode: 785, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 20.192, mean reward: 4.038 [3.135, 5.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.467, 10.100], loss: 6.473021, mae: 2.005153, mean_q: 6.457333
 48494/100000: episode: 786, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 67.115, mean reward: 2.796 [1.474, 5.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.100], loss: 3.497748, mae: 1.346639, mean_q: 5.789066
 48531/100000: episode: 787, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 100.462, mean reward: 2.715 [1.559, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.959, 10.205], loss: 401.173889, mae: 2.491861, mean_q: 6.399780
 48551/100000: episode: 788, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 71.509, mean reward: 3.575 [2.474, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.622, 10.100], loss: 79.107506, mae: 1.755945, mean_q: 5.851241
 48571/100000: episode: 789, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 58.426, mean reward: 2.921 [2.257, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.948, 10.100], loss: 3.830404, mae: 1.265932, mean_q: 5.925442
 48576/100000: episode: 790, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 15.167, mean reward: 3.033 [2.508, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.474, 10.100], loss: 4.740606, mae: 1.377495, mean_q: 6.154601
 48594/100000: episode: 791, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 59.363, mean reward: 3.298 [2.663, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.167, 10.100], loss: 1.483619, mae: 0.919367, mean_q: 5.341079
 48614/100000: episode: 792, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 91.276, mean reward: 4.564 [2.600, 11.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.712, 10.100], loss: 2.635277, mae: 1.104686, mean_q: 5.606261
 48619/100000: episode: 793, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 14.607, mean reward: 2.921 [2.704, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.423, 10.100], loss: 3.687150, mae: 1.067557, mean_q: 5.600139
 48632/100000: episode: 794, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 50.857, mean reward: 3.912 [2.833, 5.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.167, 10.100], loss: 121.652878, mae: 1.839822, mean_q: 5.840853
 48645/100000: episode: 795, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 56.127, mean reward: 4.317 [3.350, 6.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.312, 10.100], loss: 3.931891, mae: 1.190622, mean_q: 5.901881
 48669/100000: episode: 796, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 83.913, mean reward: 3.496 [2.584, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.432, 10.100], loss: 618.994385, mae: 2.391803, mean_q: 5.831701
 48696/100000: episode: 797, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 102.654, mean reward: 3.802 [2.542, 6.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.384, 10.100], loss: 7.659713, mae: 2.067516, mean_q: 6.985299
 48705/100000: episode: 798, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 52.741, mean reward: 5.860 [3.469, 18.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.510, 10.100], loss: 2.889480, mae: 1.131639, mean_q: 6.364515
 48742/100000: episode: 799, duration: 0.183s, episode steps: 37, steps per second: 202, episode reward: 176.717, mean reward: 4.776 [2.544, 18.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.194, 10.403], loss: 1.649587, mae: 0.951522, mean_q: 5.495360
[Info] Complete ISplit Iteration
[Info] Levels: [5.2605147, 7.62082, 28.72412]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 48779/100000: episode: 800, duration: 4.816s, episode steps: 37, steps per second: 8, episode reward: 152.895, mean reward: 4.132 [2.143, 7.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.560, 10.362], loss: 1.670531, mae: 0.909879, mean_q: 5.487053
 48879/100000: episode: 801, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 191.951, mean reward: 1.920 [1.459, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.262, 10.098], loss: 1.282856, mae: 0.829081, mean_q: 5.618594
 48979/100000: episode: 802, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 200.706, mean reward: 2.007 [1.452, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.522, 10.153], loss: 167.030533, mae: 1.490920, mean_q: 5.935595
 49079/100000: episode: 803, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.332, mean reward: 1.963 [1.442, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.022, 10.098], loss: 4.489142, mae: 0.950726, mean_q: 5.818560
 49179/100000: episode: 804, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 196.358, mean reward: 1.964 [1.469, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.595, 10.098], loss: 3.994389, mae: 0.846535, mean_q: 5.662599
 49279/100000: episode: 805, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 198.143, mean reward: 1.981 [1.457, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.175], loss: 2.754248, mae: 0.844709, mean_q: 5.656725
 49379/100000: episode: 806, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.401, mean reward: 1.924 [1.490, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.257, 10.098], loss: 3.131786, mae: 0.779089, mean_q: 5.616991
 49479/100000: episode: 807, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.420, mean reward: 1.814 [1.447, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.439, 10.267], loss: 17.643461, mae: 0.892653, mean_q: 5.697336
 49579/100000: episode: 808, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.640, mean reward: 1.866 [1.463, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.357, 10.151], loss: 299.388824, mae: 2.103880, mean_q: 6.360027
 49679/100000: episode: 809, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 189.614, mean reward: 1.896 [1.460, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.425, 10.260], loss: 17.674839, mae: 1.011575, mean_q: 5.675344
 49779/100000: episode: 810, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 204.581, mean reward: 2.046 [1.465, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.511, 10.098], loss: 16.243187, mae: 0.859752, mean_q: 5.561602
 49879/100000: episode: 811, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 192.328, mean reward: 1.923 [1.471, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.735, 10.245], loss: 2.513128, mae: 0.769620, mean_q: 5.494330
 49979/100000: episode: 812, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 217.945, mean reward: 2.179 [1.459, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.355, 10.098], loss: 33.695515, mae: 1.194251, mean_q: 5.924225
 50079/100000: episode: 813, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 183.504, mean reward: 1.835 [1.455, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.098], loss: 179.134506, mae: 1.615312, mean_q: 5.966188
 50179/100000: episode: 814, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 194.808, mean reward: 1.948 [1.450, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.981, 10.098], loss: 297.911560, mae: 1.941985, mean_q: 5.919050
 50279/100000: episode: 815, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 200.708, mean reward: 2.007 [1.469, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.084, 10.198], loss: 18.482698, mae: 1.382120, mean_q: 5.801845
 50379/100000: episode: 816, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 195.952, mean reward: 1.960 [1.466, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.098], loss: 31.009714, mae: 1.089364, mean_q: 5.623917
 50479/100000: episode: 817, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.258, mean reward: 1.873 [1.484, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.337, 10.131], loss: 149.711227, mae: 1.420802, mean_q: 5.788021
 50579/100000: episode: 818, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 198.763, mean reward: 1.988 [1.455, 4.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.275, 10.227], loss: 17.259991, mae: 0.886163, mean_q: 5.376853
 50679/100000: episode: 819, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.415, mean reward: 1.884 [1.457, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.098], loss: 15.699640, mae: 0.843358, mean_q: 5.413531
 50779/100000: episode: 820, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 217.063, mean reward: 2.171 [1.513, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.431, 10.174], loss: 150.760239, mae: 1.420802, mean_q: 5.725372
 50879/100000: episode: 821, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.974, mean reward: 1.900 [1.447, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.168], loss: 152.878952, mae: 1.557151, mean_q: 5.626243
 50979/100000: episode: 822, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 200.188, mean reward: 2.002 [1.466, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.562, 10.285], loss: 149.435852, mae: 1.194715, mean_q: 5.461092
 51079/100000: episode: 823, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 197.436, mean reward: 1.974 [1.490, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.631, 10.286], loss: 306.479614, mae: 2.120291, mean_q: 6.042827
 51179/100000: episode: 824, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 178.301, mean reward: 1.783 [1.457, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.807, 10.124], loss: 162.341644, mae: 1.406254, mean_q: 5.336185
 51279/100000: episode: 825, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 201.543, mean reward: 2.015 [1.441, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.566, 10.098], loss: 320.432465, mae: 2.830172, mean_q: 6.418706
 51379/100000: episode: 826, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.415, mean reward: 1.924 [1.473, 2.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.204, 10.263], loss: 149.513931, mae: 1.202481, mean_q: 5.264753
 51479/100000: episode: 827, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 180.606, mean reward: 1.806 [1.487, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.837, 10.098], loss: 145.547806, mae: 1.725434, mean_q: 5.662373
 51579/100000: episode: 828, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 191.764, mean reward: 1.918 [1.445, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.033, 10.098], loss: 164.229614, mae: 1.646178, mean_q: 5.409729
 51679/100000: episode: 829, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 186.659, mean reward: 1.867 [1.498, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.802, 10.277], loss: 291.247711, mae: 1.828967, mean_q: 5.462770
 51779/100000: episode: 830, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 191.103, mean reward: 1.911 [1.437, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.204, 10.098], loss: 19.092514, mae: 1.260173, mean_q: 5.514834
 51879/100000: episode: 831, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.182, mean reward: 1.932 [1.472, 7.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.061, 10.222], loss: 178.945755, mae: 1.882239, mean_q: 5.680558
 51979/100000: episode: 832, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 201.995, mean reward: 2.020 [1.492, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.971, 10.108], loss: 177.780289, mae: 1.902127, mean_q: 5.784042
 52079/100000: episode: 833, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.079, mean reward: 1.851 [1.453, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.689, 10.185], loss: 161.939117, mae: 1.565003, mean_q: 5.626182
 52179/100000: episode: 834, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.935, mean reward: 2.019 [1.477, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.076, 10.098], loss: 146.049026, mae: 1.311002, mean_q: 5.180682
 52279/100000: episode: 835, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 186.769, mean reward: 1.868 [1.452, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.337, 10.109], loss: 4.389166, mae: 1.146025, mean_q: 5.263052
 52379/100000: episode: 836, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 225.280, mean reward: 2.253 [1.464, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.325, 10.098], loss: 148.794144, mae: 1.456358, mean_q: 5.239223
 52479/100000: episode: 837, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 187.582, mean reward: 1.876 [1.456, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.239, 10.098], loss: 1.714551, mae: 0.687683, mean_q: 4.732525
 52579/100000: episode: 838, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.642, mean reward: 1.956 [1.492, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.296], loss: 0.470200, mae: 0.533873, mean_q: 4.584701
 52679/100000: episode: 839, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.562, mean reward: 1.996 [1.478, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.829, 10.098], loss: 0.423520, mae: 0.513218, mean_q: 4.494647
 52779/100000: episode: 840, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 194.946, mean reward: 1.949 [1.487, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.747, 10.348], loss: 0.317466, mae: 0.476082, mean_q: 4.449278
 52879/100000: episode: 841, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.220, mean reward: 1.942 [1.459, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.356, 10.098], loss: 0.436382, mae: 0.486758, mean_q: 4.409809
 52979/100000: episode: 842, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 189.485, mean reward: 1.895 [1.541, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.098], loss: 0.280691, mae: 0.439619, mean_q: 4.328770
 53079/100000: episode: 843, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 182.442, mean reward: 1.824 [1.442, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.762, 10.098], loss: 0.257055, mae: 0.430845, mean_q: 4.300766
 53179/100000: episode: 844, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 210.532, mean reward: 2.105 [1.445, 7.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.446, 10.098], loss: 0.235575, mae: 0.427340, mean_q: 4.228967
 53279/100000: episode: 845, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.290, mean reward: 1.973 [1.458, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.246], loss: 0.208287, mae: 0.400368, mean_q: 4.150472
 53379/100000: episode: 846, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.152, mean reward: 1.932 [1.466, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.888, 10.098], loss: 0.152890, mae: 0.376542, mean_q: 4.103636
 53479/100000: episode: 847, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 180.895, mean reward: 1.809 [1.457, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.098], loss: 0.152835, mae: 0.366967, mean_q: 4.045755
 53579/100000: episode: 848, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.113, mean reward: 1.851 [1.445, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.166, 10.119], loss: 0.181336, mae: 0.373296, mean_q: 4.054049
 53679/100000: episode: 849, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.090, mean reward: 1.841 [1.449, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.529, 10.098], loss: 0.134339, mae: 0.360205, mean_q: 3.946341
 53779/100000: episode: 850, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 202.701, mean reward: 2.027 [1.475, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.327, 10.098], loss: 0.117101, mae: 0.349134, mean_q: 3.873937
 53879/100000: episode: 851, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 189.160, mean reward: 1.892 [1.478, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.250, 10.139], loss: 0.121811, mae: 0.354562, mean_q: 3.857403
 53979/100000: episode: 852, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 180.594, mean reward: 1.806 [1.450, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.361], loss: 0.119054, mae: 0.343262, mean_q: 3.842992
 54079/100000: episode: 853, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 192.084, mean reward: 1.921 [1.443, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.095, 10.098], loss: 0.110312, mae: 0.335261, mean_q: 3.830675
 54179/100000: episode: 854, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 205.101, mean reward: 2.051 [1.523, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 0.120480, mae: 0.349888, mean_q: 3.854522
 54279/100000: episode: 855, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 209.475, mean reward: 2.095 [1.482, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.517, 10.098], loss: 0.109724, mae: 0.329450, mean_q: 3.856875
 54379/100000: episode: 856, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.803, mean reward: 1.898 [1.453, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.328], loss: 0.112338, mae: 0.339447, mean_q: 3.849220
 54479/100000: episode: 857, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 207.484, mean reward: 2.075 [1.473, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.323, 10.335], loss: 0.116318, mae: 0.345361, mean_q: 3.846164
 54579/100000: episode: 858, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 204.814, mean reward: 2.048 [1.465, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.937, 10.098], loss: 0.129404, mae: 0.354059, mean_q: 3.861455
 54679/100000: episode: 859, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 204.645, mean reward: 2.046 [1.478, 4.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.629, 10.107], loss: 0.106932, mae: 0.325570, mean_q: 3.838044
 54779/100000: episode: 860, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 185.975, mean reward: 1.860 [1.497, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.694, 10.098], loss: 0.097168, mae: 0.319472, mean_q: 3.850968
 54879/100000: episode: 861, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 194.177, mean reward: 1.942 [1.478, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.533, 10.098], loss: 0.105380, mae: 0.324103, mean_q: 3.858095
 54979/100000: episode: 862, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 199.652, mean reward: 1.997 [1.463, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.932, 10.098], loss: 0.101501, mae: 0.327030, mean_q: 3.828518
 55079/100000: episode: 863, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 179.771, mean reward: 1.798 [1.441, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.660, 10.099], loss: 0.113825, mae: 0.331765, mean_q: 3.874900
 55179/100000: episode: 864, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 186.083, mean reward: 1.861 [1.469, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.562, 10.098], loss: 0.104026, mae: 0.334839, mean_q: 3.866940
 55279/100000: episode: 865, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 197.617, mean reward: 1.976 [1.451, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.592, 10.098], loss: 0.126870, mae: 0.336855, mean_q: 3.859982
 55379/100000: episode: 866, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 198.308, mean reward: 1.983 [1.465, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.858, 10.163], loss: 0.119295, mae: 0.338221, mean_q: 3.856781
 55479/100000: episode: 867, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 213.167, mean reward: 2.132 [1.460, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.339, 10.335], loss: 0.106837, mae: 0.314176, mean_q: 3.843860
 55579/100000: episode: 868, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 179.151, mean reward: 1.792 [1.446, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.731, 10.229], loss: 0.114997, mae: 0.340899, mean_q: 3.866791
 55679/100000: episode: 869, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 213.206, mean reward: 2.132 [1.466, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.611, 10.098], loss: 0.101283, mae: 0.324125, mean_q: 3.865323
 55779/100000: episode: 870, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 187.633, mean reward: 1.876 [1.460, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.409, 10.184], loss: 0.113620, mae: 0.336121, mean_q: 3.880018
 55879/100000: episode: 871, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 218.972, mean reward: 2.190 [1.464, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.479], loss: 0.120628, mae: 0.341416, mean_q: 3.856508
 55979/100000: episode: 872, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 209.021, mean reward: 2.090 [1.460, 7.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.987, 10.488], loss: 0.132376, mae: 0.343206, mean_q: 3.870390
 56079/100000: episode: 873, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.574, mean reward: 1.936 [1.454, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.297, 10.202], loss: 0.121531, mae: 0.336486, mean_q: 3.849503
 56179/100000: episode: 874, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 179.633, mean reward: 1.796 [1.461, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.242, 10.098], loss: 0.123907, mae: 0.344851, mean_q: 3.874982
 56279/100000: episode: 875, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 190.955, mean reward: 1.910 [1.441, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.599, 10.103], loss: 0.122179, mae: 0.339665, mean_q: 3.858361
 56379/100000: episode: 876, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.183, mean reward: 2.032 [1.460, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.448, 10.381], loss: 0.122620, mae: 0.344593, mean_q: 3.888033
 56479/100000: episode: 877, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 199.746, mean reward: 1.997 [1.468, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.230, 10.098], loss: 0.113869, mae: 0.331726, mean_q: 3.870924
 56579/100000: episode: 878, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.676, mean reward: 1.857 [1.495, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.098], loss: 0.117538, mae: 0.335340, mean_q: 3.880849
 56679/100000: episode: 879, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 198.597, mean reward: 1.986 [1.446, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.642, 10.360], loss: 0.109004, mae: 0.335685, mean_q: 3.876015
 56779/100000: episode: 880, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.536, mean reward: 1.925 [1.472, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.980, 10.098], loss: 0.113585, mae: 0.329674, mean_q: 3.859435
 56879/100000: episode: 881, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.849, mean reward: 1.928 [1.443, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.274], loss: 0.122914, mae: 0.346189, mean_q: 3.874891
 56979/100000: episode: 882, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 223.692, mean reward: 2.237 [1.526, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.098], loss: 0.129624, mae: 0.347085, mean_q: 3.883188
 57079/100000: episode: 883, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 187.898, mean reward: 1.879 [1.439, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.172], loss: 0.110000, mae: 0.333967, mean_q: 3.892052
 57179/100000: episode: 884, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.146, mean reward: 1.901 [1.441, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.187, 10.098], loss: 0.097332, mae: 0.319617, mean_q: 3.889198
 57279/100000: episode: 885, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 208.227, mean reward: 2.082 [1.463, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.264, 10.167], loss: 0.104589, mae: 0.321336, mean_q: 3.905596
 57379/100000: episode: 886, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 214.206, mean reward: 2.142 [1.441, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.562, 10.098], loss: 0.106184, mae: 0.324622, mean_q: 3.883696
 57479/100000: episode: 887, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 215.017, mean reward: 2.150 [1.469, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.816, 10.219], loss: 0.111895, mae: 0.326458, mean_q: 3.903503
 57579/100000: episode: 888, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 191.345, mean reward: 1.913 [1.468, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.758, 10.304], loss: 0.112449, mae: 0.331796, mean_q: 3.881419
 57679/100000: episode: 889, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 221.025, mean reward: 2.210 [1.497, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.032, 10.359], loss: 0.110560, mae: 0.325351, mean_q: 3.889434
 57779/100000: episode: 890, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.578, mean reward: 1.836 [1.476, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.166], loss: 0.100584, mae: 0.320760, mean_q: 3.905573
 57879/100000: episode: 891, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 243.674, mean reward: 2.437 [1.456, 5.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.392, 10.098], loss: 0.119336, mae: 0.340653, mean_q: 3.910319
 57979/100000: episode: 892, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 196.999, mean reward: 1.970 [1.450, 5.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.639, 10.305], loss: 0.117522, mae: 0.330513, mean_q: 3.924629
 58079/100000: episode: 893, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 193.482, mean reward: 1.935 [1.455, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.830, 10.098], loss: 0.103333, mae: 0.321238, mean_q: 3.925724
 58179/100000: episode: 894, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.628, mean reward: 1.926 [1.487, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.140, 10.278], loss: 0.106799, mae: 0.320670, mean_q: 3.894734
 58279/100000: episode: 895, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.474, mean reward: 1.825 [1.460, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.236, 10.250], loss: 0.102697, mae: 0.320242, mean_q: 3.919480
 58379/100000: episode: 896, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 219.772, mean reward: 2.198 [1.501, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.157, 10.250], loss: 0.103241, mae: 0.322295, mean_q: 3.894482
 58479/100000: episode: 897, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 227.263, mean reward: 2.273 [1.500, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.642, 10.098], loss: 0.100763, mae: 0.317275, mean_q: 3.913153
 58579/100000: episode: 898, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.476, mean reward: 1.965 [1.437, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.595, 10.202], loss: 0.106779, mae: 0.319062, mean_q: 3.936960
 58679/100000: episode: 899, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 183.372, mean reward: 1.834 [1.486, 2.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.098], loss: 0.116622, mae: 0.331112, mean_q: 3.964150
[Info] 1-TH LEVEL FOUND: 5.606840133666992, Considering 10/90 traces
 58779/100000: episode: 900, duration: 4.841s, episode steps: 100, steps per second: 21, episode reward: 199.563, mean reward: 1.996 [1.503, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.802, 10.151], loss: 0.110181, mae: 0.331498, mean_q: 3.956811
 58812/100000: episode: 901, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 73.483, mean reward: 2.227 [1.497, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.890, 10.133], loss: 0.128608, mae: 0.358553, mean_q: 4.005385
 58844/100000: episode: 902, duration: 0.221s, episode steps: 32, steps per second: 145, episode reward: 99.298, mean reward: 3.103 [2.077, 7.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.694, 10.399], loss: 0.105958, mae: 0.331231, mean_q: 3.960890
 58877/100000: episode: 903, duration: 0.212s, episode steps: 33, steps per second: 155, episode reward: 83.828, mean reward: 2.540 [1.701, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.285, 10.100], loss: 0.088237, mae: 0.304265, mean_q: 3.944514
 58913/100000: episode: 904, duration: 0.204s, episode steps: 36, steps per second: 177, episode reward: 75.475, mean reward: 2.097 [1.471, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.114, 10.100], loss: 0.108824, mae: 0.322005, mean_q: 3.976132
 58944/100000: episode: 905, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 80.872, mean reward: 2.609 [1.972, 5.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.530, 10.434], loss: 0.099706, mae: 0.321597, mean_q: 4.004928
 58975/100000: episode: 906, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 72.808, mean reward: 2.349 [1.612, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.848, 10.198], loss: 0.102833, mae: 0.318981, mean_q: 3.946093
 59003/100000: episode: 907, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 139.583, mean reward: 4.985 [2.415, 10.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.575, 10.100], loss: 0.108542, mae: 0.319830, mean_q: 3.981836
 59034/100000: episode: 908, duration: 0.207s, episode steps: 31, steps per second: 150, episode reward: 78.707, mean reward: 2.539 [1.689, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.136, 10.318], loss: 0.133233, mae: 0.332969, mean_q: 3.999012
 59067/100000: episode: 909, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 75.871, mean reward: 2.299 [1.543, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.176, 10.205], loss: 0.144707, mae: 0.345545, mean_q: 4.039305
 59099/100000: episode: 910, duration: 0.188s, episode steps: 32, steps per second: 171, episode reward: 80.989, mean reward: 2.531 [1.881, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.427], loss: 0.140549, mae: 0.354651, mean_q: 4.024257
 59135/100000: episode: 911, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 110.543, mean reward: 3.071 [1.740, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.981, 10.100], loss: 0.147259, mae: 0.354476, mean_q: 4.028461
 59153/100000: episode: 912, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 42.288, mean reward: 2.349 [1.967, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.605, 10.416], loss: 0.160816, mae: 0.364959, mean_q: 4.037513
 59184/100000: episode: 913, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 109.435, mean reward: 3.530 [2.200, 6.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.488, 10.547], loss: 0.153688, mae: 0.355504, mean_q: 4.089723
 59220/100000: episode: 914, duration: 0.222s, episode steps: 36, steps per second: 162, episode reward: 96.555, mean reward: 2.682 [1.640, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.599, 10.100], loss: 0.143421, mae: 0.358694, mean_q: 4.100078
 59252/100000: episode: 915, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 73.867, mean reward: 2.308 [1.520, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.510, 10.278], loss: 0.138547, mae: 0.362357, mean_q: 4.046052
 59283/100000: episode: 916, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 72.446, mean reward: 2.337 [1.934, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.150, 10.238], loss: 0.145987, mae: 0.365589, mean_q: 4.082948
 59314/100000: episode: 917, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 68.746, mean reward: 2.218 [1.550, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.638, 10.183], loss: 0.145726, mae: 0.364181, mean_q: 4.063897
 59345/100000: episode: 918, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 62.332, mean reward: 2.011 [1.596, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.068, 10.172], loss: 0.151628, mae: 0.369652, mean_q: 4.040100
 59376/100000: episode: 919, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 70.175, mean reward: 2.264 [1.859, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.522, 10.309], loss: 0.153898, mae: 0.340880, mean_q: 4.091539
 59413/100000: episode: 920, duration: 0.233s, episode steps: 37, steps per second: 159, episode reward: 88.875, mean reward: 2.402 [1.592, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.132, 10.104], loss: 0.131674, mae: 0.342143, mean_q: 4.027884
 59450/100000: episode: 921, duration: 0.220s, episode steps: 37, steps per second: 168, episode reward: 89.686, mean reward: 2.424 [1.941, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.423, 10.100], loss: 0.129720, mae: 0.336073, mean_q: 4.042710
 59483/100000: episode: 922, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 75.952, mean reward: 2.302 [1.449, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.301, 10.143], loss: 0.166792, mae: 0.380726, mean_q: 4.108944
 59520/100000: episode: 923, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 95.664, mean reward: 2.586 [1.476, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.111, 10.156], loss: 0.166717, mae: 0.360156, mean_q: 4.128056
 59552/100000: episode: 924, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 92.573, mean reward: 2.893 [2.170, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.298, 10.355], loss: 0.148543, mae: 0.363090, mean_q: 4.118820
 59569/100000: episode: 925, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 39.992, mean reward: 2.352 [1.712, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.102, 10.100], loss: 0.164105, mae: 0.395400, mean_q: 4.107772
 59602/100000: episode: 926, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 72.479, mean reward: 2.196 [1.736, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.336, 10.271], loss: 0.150245, mae: 0.366373, mean_q: 4.115509
 59630/100000: episode: 927, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 88.940, mean reward: 3.176 [2.156, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.632, 10.100], loss: 0.173062, mae: 0.387169, mean_q: 4.167629
 59648/100000: episode: 928, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 41.604, mean reward: 2.311 [1.771, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-1.172, 10.328], loss: 0.153762, mae: 0.361209, mean_q: 4.176169
 59681/100000: episode: 929, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 95.224, mean reward: 2.886 [1.984, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.308, 10.100], loss: 0.157726, mae: 0.373113, mean_q: 4.147437
 59713/100000: episode: 930, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 82.360, mean reward: 2.574 [1.556, 4.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.311, 10.187], loss: 0.133708, mae: 0.356614, mean_q: 4.112281
 59744/100000: episode: 931, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 72.162, mean reward: 2.328 [1.945, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.977, 10.309], loss: 0.173066, mae: 0.371143, mean_q: 4.196678
 59777/100000: episode: 932, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 171.462, mean reward: 5.196 [2.324, 15.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.094, 10.697], loss: 0.165533, mae: 0.394934, mean_q: 4.216692
 59795/100000: episode: 933, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 40.419, mean reward: 2.246 [2.028, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.313], loss: 0.159213, mae: 0.380528, mean_q: 4.100908
 59827/100000: episode: 934, duration: 0.315s, episode steps: 32, steps per second: 102, episode reward: 79.460, mean reward: 2.483 [1.960, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.873, 10.327], loss: 0.180427, mae: 0.380693, mean_q: 4.207416
 59858/100000: episode: 935, duration: 0.250s, episode steps: 31, steps per second: 124, episode reward: 91.955, mean reward: 2.966 [2.161, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.087, 10.478], loss: 0.151976, mae: 0.359468, mean_q: 4.166183
 59875/100000: episode: 936, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 38.240, mean reward: 2.249 [1.938, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.181, 10.100], loss: 0.127442, mae: 0.351316, mean_q: 4.110499
 59907/100000: episode: 937, duration: 0.209s, episode steps: 32, steps per second: 153, episode reward: 80.120, mean reward: 2.504 [1.859, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.853, 10.304], loss: 0.136193, mae: 0.362065, mean_q: 4.180748
 59924/100000: episode: 938, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 49.671, mean reward: 2.922 [2.335, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.545, 10.100], loss: 0.310807, mae: 0.427585, mean_q: 4.301740
 59955/100000: episode: 939, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 87.142, mean reward: 2.811 [1.819, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.918, 10.270], loss: 0.197839, mae: 0.376182, mean_q: 4.223973
 59988/100000: episode: 940, duration: 0.195s, episode steps: 33, steps per second: 169, episode reward: 87.266, mean reward: 2.644 [1.692, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.200], loss: 0.239716, mae: 0.415668, mean_q: 4.266196
 60025/100000: episode: 941, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 96.943, mean reward: 2.620 [1.722, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.823, 10.100], loss: 0.191928, mae: 0.391044, mean_q: 4.230045
 60061/100000: episode: 942, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 82.033, mean reward: 2.279 [1.734, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.175, 10.100], loss: 0.202992, mae: 0.433582, mean_q: 4.273343
 60078/100000: episode: 943, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 35.479, mean reward: 2.087 [1.620, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.312, 10.100], loss: 0.203739, mae: 0.407248, mean_q: 4.312220
 60095/100000: episode: 944, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 40.167, mean reward: 2.363 [1.773, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.404, 10.100], loss: 0.198600, mae: 0.437065, mean_q: 4.302577
 60128/100000: episode: 945, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 74.013, mean reward: 2.243 [1.852, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.232, 10.392], loss: 0.293637, mae: 0.450935, mean_q: 4.319127
 60156/100000: episode: 946, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 67.851, mean reward: 2.423 [1.664, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.257, 10.100], loss: 0.232191, mae: 0.402295, mean_q: 4.206551
 60189/100000: episode: 947, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 89.477, mean reward: 2.711 [1.717, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.229, 10.329], loss: 0.200819, mae: 0.431193, mean_q: 4.302816
 60220/100000: episode: 948, duration: 0.153s, episode steps: 31, steps per second: 202, episode reward: 62.066, mean reward: 2.002 [1.444, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.249, 10.153], loss: 0.147789, mae: 0.379803, mean_q: 4.279747
 60253/100000: episode: 949, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 96.729, mean reward: 2.931 [1.875, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.243, 10.100], loss: 0.142475, mae: 0.371138, mean_q: 4.270081
 60271/100000: episode: 950, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 50.125, mean reward: 2.785 [2.395, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.432, 10.409], loss: 0.170432, mae: 0.383807, mean_q: 4.295176
 60304/100000: episode: 951, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 88.108, mean reward: 2.670 [1.560, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.682, 10.160], loss: 0.170289, mae: 0.398117, mean_q: 4.334013
 60340/100000: episode: 952, duration: 0.197s, episode steps: 36, steps per second: 182, episode reward: 94.197, mean reward: 2.617 [1.728, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.159, 10.100], loss: 0.192639, mae: 0.384773, mean_q: 4.312992
 60371/100000: episode: 953, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 80.988, mean reward: 2.613 [1.845, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.179, 10.274], loss: 0.329565, mae: 0.446599, mean_q: 4.391296
 60404/100000: episode: 954, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 116.464, mean reward: 3.529 [2.474, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.582, 10.100], loss: 0.167230, mae: 0.394272, mean_q: 4.345325
 60432/100000: episode: 955, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 97.009, mean reward: 3.465 [2.206, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.790, 10.100], loss: 0.265829, mae: 0.434489, mean_q: 4.402635
 60465/100000: episode: 956, duration: 0.160s, episode steps: 33, steps per second: 207, episode reward: 89.182, mean reward: 2.702 [1.919, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.312], loss: 0.160580, mae: 0.374119, mean_q: 4.363750
 60501/100000: episode: 957, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 94.241, mean reward: 2.618 [1.872, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.159, 10.100], loss: 0.217652, mae: 0.400947, mean_q: 4.374671
 60529/100000: episode: 958, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 86.485, mean reward: 3.089 [2.211, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.661, 10.100], loss: 0.197369, mae: 0.416390, mean_q: 4.384960
 60562/100000: episode: 959, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 90.134, mean reward: 2.731 [2.034, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.352], loss: 0.212442, mae: 0.404666, mean_q: 4.404907
 60595/100000: episode: 960, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 78.169, mean reward: 2.369 [1.781, 4.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.247, 10.100], loss: 0.133849, mae: 0.359227, mean_q: 4.401124
 60628/100000: episode: 961, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 80.740, mean reward: 2.447 [1.882, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.921, 10.423], loss: 0.168640, mae: 0.392639, mean_q: 4.418953
 60665/100000: episode: 962, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 103.468, mean reward: 2.796 [1.817, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.227, 10.100], loss: 0.230743, mae: 0.430182, mean_q: 4.440705
 60683/100000: episode: 963, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 42.181, mean reward: 2.343 [1.971, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.355], loss: 0.224473, mae: 0.451091, mean_q: 4.545636
 60714/100000: episode: 964, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 72.895, mean reward: 2.351 [1.797, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.300, 10.247], loss: 0.247246, mae: 0.434191, mean_q: 4.448543
 60745/100000: episode: 965, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 63.512, mean reward: 2.049 [1.602, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.052, 10.259], loss: 0.298689, mae: 0.464608, mean_q: 4.504212
 60778/100000: episode: 966, duration: 0.203s, episode steps: 33, steps per second: 163, episode reward: 74.006, mean reward: 2.243 [1.536, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.678, 10.147], loss: 0.202436, mae: 0.412034, mean_q: 4.440687
 60815/100000: episode: 967, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 138.400, mean reward: 3.741 [2.415, 8.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.695, 10.100], loss: 0.203727, mae: 0.420194, mean_q: 4.464808
 60833/100000: episode: 968, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 43.943, mean reward: 2.441 [1.542, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.301], loss: 0.152857, mae: 0.369719, mean_q: 4.413882
 60870/100000: episode: 969, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 88.516, mean reward: 2.392 [1.731, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.829, 10.100], loss: 0.187247, mae: 0.396736, mean_q: 4.401675
 60901/100000: episode: 970, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 69.536, mean reward: 2.243 [1.808, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.280], loss: 0.240605, mae: 0.446860, mean_q: 4.467751
 60934/100000: episode: 971, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 74.817, mean reward: 2.267 [1.466, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.100], loss: 0.168684, mae: 0.394495, mean_q: 4.464139
 60966/100000: episode: 972, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 82.056, mean reward: 2.564 [1.764, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.655, 10.262], loss: 0.160548, mae: 0.393207, mean_q: 4.431509
 60997/100000: episode: 973, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 107.375, mean reward: 3.464 [1.948, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.963, 10.586], loss: 0.247660, mae: 0.404852, mean_q: 4.488752
 61030/100000: episode: 974, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 75.500, mean reward: 2.288 [1.736, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.629, 10.100], loss: 0.171486, mae: 0.405133, mean_q: 4.436347
 61063/100000: episode: 975, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 97.227, mean reward: 2.946 [2.263, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.496, 10.100], loss: 0.212384, mae: 0.441219, mean_q: 4.508079
 61080/100000: episode: 976, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 38.099, mean reward: 2.241 [1.595, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.219, 10.100], loss: 0.238020, mae: 0.458381, mean_q: 4.564367
 61111/100000: episode: 977, duration: 0.154s, episode steps: 31, steps per second: 202, episode reward: 77.669, mean reward: 2.505 [2.048, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.514, 10.412], loss: 0.180438, mae: 0.394711, mean_q: 4.434696
 61148/100000: episode: 978, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 90.348, mean reward: 2.442 [1.504, 5.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-1.161, 10.105], loss: 0.195553, mae: 0.403199, mean_q: 4.509692
 61185/100000: episode: 979, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 84.789, mean reward: 2.292 [1.490, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.522, 10.101], loss: 0.274173, mae: 0.419259, mean_q: 4.549086
 61218/100000: episode: 980, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 85.129, mean reward: 2.580 [1.913, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.515, 10.100], loss: 0.233463, mae: 0.429447, mean_q: 4.596072
 61251/100000: episode: 981, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 74.833, mean reward: 2.268 [1.557, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.199, 10.130], loss: 0.259834, mae: 0.428266, mean_q: 4.601585
 61282/100000: episode: 982, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 70.221, mean reward: 2.265 [1.888, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.305], loss: 0.282649, mae: 0.456568, mean_q: 4.497738
 61315/100000: episode: 983, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 81.312, mean reward: 2.464 [1.889, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.419, 10.100], loss: 0.190033, mae: 0.423538, mean_q: 4.568156
 61346/100000: episode: 984, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 76.640, mean reward: 2.472 [1.867, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.109, 10.391], loss: 0.160743, mae: 0.395610, mean_q: 4.538590
 61377/100000: episode: 985, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 78.970, mean reward: 2.547 [2.069, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.525, 10.369], loss: 0.176104, mae: 0.414031, mean_q: 4.564677
 61408/100000: episode: 986, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 74.964, mean reward: 2.418 [1.908, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.223, 10.396], loss: 0.201559, mae: 0.417665, mean_q: 4.567962
 61444/100000: episode: 987, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 87.937, mean reward: 2.443 [1.770, 5.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.538, 10.100], loss: 0.198370, mae: 0.394211, mean_q: 4.517383
 61475/100000: episode: 988, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 81.212, mean reward: 2.620 [1.761, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.946, 10.291], loss: 0.315410, mae: 0.459818, mean_q: 4.596313
 61508/100000: episode: 989, duration: 0.162s, episode steps: 33, steps per second: 203, episode reward: 83.760, mean reward: 2.538 [1.652, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.186, 10.100], loss: 0.247129, mae: 0.445527, mean_q: 4.638021
[Info] 2-TH LEVEL FOUND: 7.133650779724121, Considering 10/90 traces
 61544/100000: episode: 990, duration: 4.454s, episode steps: 36, steps per second: 8, episode reward: 89.648, mean reward: 2.490 [1.769, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.573, 10.100], loss: 0.257153, mae: 0.428414, mean_q: 4.613641
 61567/100000: episode: 991, duration: 0.140s, episode steps: 23, steps per second: 165, episode reward: 122.161, mean reward: 5.311 [3.301, 7.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.483, 10.100], loss: 0.210692, mae: 0.401880, mean_q: 4.554972
 61592/100000: episode: 992, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 99.742, mean reward: 3.990 [2.294, 8.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.568, 10.100], loss: 0.198973, mae: 0.379790, mean_q: 4.565289
 61615/100000: episode: 993, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 64.381, mean reward: 2.799 [2.297, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.435, 10.100], loss: 0.268229, mae: 0.446768, mean_q: 4.609787
 61645/100000: episode: 994, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 75.490, mean reward: 2.516 [1.872, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.411, 10.100], loss: 0.174460, mae: 0.408831, mean_q: 4.646897
 61668/100000: episode: 995, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 69.422, mean reward: 3.018 [2.179, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.307, 10.100], loss: 0.303209, mae: 0.450516, mean_q: 4.723435
 61684/100000: episode: 996, duration: 0.106s, episode steps: 16, steps per second: 150, episode reward: 46.454, mean reward: 2.903 [1.951, 4.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.863, 10.100], loss: 0.283378, mae: 0.450740, mean_q: 4.694522
 61700/100000: episode: 997, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 76.902, mean reward: 4.806 [3.788, 6.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.161, 10.100], loss: 0.214485, mae: 0.432732, mean_q: 4.703806
 61725/100000: episode: 998, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 67.071, mean reward: 2.683 [1.844, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.742, 10.100], loss: 0.246850, mae: 0.441683, mean_q: 4.659843
 61741/100000: episode: 999, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 56.110, mean reward: 3.507 [2.722, 4.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.334, 10.100], loss: 0.275078, mae: 0.472957, mean_q: 4.767255
 61757/100000: episode: 1000, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 88.534, mean reward: 5.533 [3.927, 9.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.371, 10.100], loss: 0.277357, mae: 0.427784, mean_q: 4.670601
 61779/100000: episode: 1001, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 68.946, mean reward: 3.134 [2.416, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.725, 10.100], loss: 0.397740, mae: 0.525440, mean_q: 4.841270
 61804/100000: episode: 1002, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 72.531, mean reward: 2.901 [2.344, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-2.329, 10.100], loss: 0.293405, mae: 0.471477, mean_q: 4.746250
 61820/100000: episode: 1003, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 52.405, mean reward: 3.275 [1.984, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.148, 10.100], loss: 0.322000, mae: 0.502528, mean_q: 4.822134
 61850/100000: episode: 1004, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 102.776, mean reward: 3.426 [2.541, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.531, 10.100], loss: 0.278550, mae: 0.485527, mean_q: 4.745861
 61872/100000: episode: 1005, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 73.439, mean reward: 3.338 [2.586, 5.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.748, 10.100], loss: 0.215599, mae: 0.461793, mean_q: 4.843998
 61894/100000: episode: 1006, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 63.535, mean reward: 2.888 [2.374, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.364, 10.100], loss: 0.283400, mae: 0.463455, mean_q: 4.727911
 61919/100000: episode: 1007, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 106.596, mean reward: 4.264 [2.333, 9.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.571, 10.100], loss: 0.243040, mae: 0.464049, mean_q: 4.741410
 61942/100000: episode: 1008, duration: 0.144s, episode steps: 23, steps per second: 159, episode reward: 85.824, mean reward: 3.731 [2.239, 5.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.388, 10.100], loss: 0.317323, mae: 0.500402, mean_q: 4.885954
 61964/100000: episode: 1009, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 87.936, mean reward: 3.997 [2.528, 9.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.685, 10.100], loss: 0.312214, mae: 0.461564, mean_q: 4.758892
 61989/100000: episode: 1010, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 89.377, mean reward: 3.575 [2.620, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.461, 10.100], loss: 0.247309, mae: 0.466805, mean_q: 4.858664
 62013/100000: episode: 1011, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 84.183, mean reward: 3.508 [2.281, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.208, 10.365], loss: 0.287029, mae: 0.494575, mean_q: 4.844982
 62035/100000: episode: 1012, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 71.038, mean reward: 3.229 [2.503, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.188, 10.100], loss: 0.221728, mae: 0.451570, mean_q: 4.772719
 62051/100000: episode: 1013, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 57.425, mean reward: 3.589 [2.495, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.333, 10.100], loss: 0.368073, mae: 0.505773, mean_q: 4.962296
 62073/100000: episode: 1014, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 71.008, mean reward: 3.228 [2.140, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.222, 10.100], loss: 0.279091, mae: 0.479705, mean_q: 4.825015
 62087/100000: episode: 1015, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 48.399, mean reward: 3.457 [2.770, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.450, 10.495], loss: 0.267386, mae: 0.492342, mean_q: 4.908283
 62110/100000: episode: 1016, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 70.960, mean reward: 3.085 [2.073, 6.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.593, 10.100], loss: 0.276778, mae: 0.501403, mean_q: 4.853663
 62132/100000: episode: 1017, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 69.852, mean reward: 3.175 [2.582, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.371, 10.100], loss: 0.332359, mae: 0.495286, mean_q: 4.933793
 62156/100000: episode: 1018, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 73.165, mean reward: 3.049 [1.885, 5.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.858, 10.278], loss: 0.304688, mae: 0.527184, mean_q: 4.854221
 62172/100000: episode: 1019, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 69.532, mean reward: 4.346 [3.173, 9.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.511, 10.100], loss: 0.383892, mae: 0.575699, mean_q: 4.887206
 62194/100000: episode: 1020, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 51.942, mean reward: 2.361 [1.679, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.115, 10.100], loss: 0.312047, mae: 0.499478, mean_q: 4.969432
 62216/100000: episode: 1021, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 63.540, mean reward: 2.888 [2.023, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.141, 10.100], loss: 0.258226, mae: 0.464728, mean_q: 4.919907
 62232/100000: episode: 1022, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 68.763, mean reward: 4.298 [2.616, 10.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.789, 10.100], loss: 0.305669, mae: 0.498923, mean_q: 4.926774
 62256/100000: episode: 1023, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 79.536, mean reward: 3.314 [2.308, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.023, 10.507], loss: 0.260077, mae: 0.486360, mean_q: 5.023508
 62272/100000: episode: 1024, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 82.478, mean reward: 5.155 [2.567, 16.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.237, 10.100], loss: 0.298508, mae: 0.487383, mean_q: 5.134819
 62288/100000: episode: 1025, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 50.707, mean reward: 3.169 [2.618, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.806, 10.100], loss: 0.411316, mae: 0.543836, mean_q: 4.855302
 62310/100000: episode: 1026, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 59.262, mean reward: 2.694 [1.964, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.321, 10.100], loss: 0.273615, mae: 0.533867, mean_q: 4.960539
 62333/100000: episode: 1027, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 73.563, mean reward: 3.198 [2.204, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.250, 10.100], loss: 0.289279, mae: 0.504695, mean_q: 5.049266
 62355/100000: episode: 1028, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 58.545, mean reward: 2.661 [2.166, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.729, 10.100], loss: 0.273357, mae: 0.480952, mean_q: 4.953976
 62371/100000: episode: 1029, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 57.814, mean reward: 3.613 [2.970, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.458, 10.100], loss: 0.483521, mae: 0.540331, mean_q: 5.074777
 62394/100000: episode: 1030, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 76.767, mean reward: 3.338 [2.594, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.412, 10.100], loss: 0.305945, mae: 0.492342, mean_q: 4.957036
 62417/100000: episode: 1031, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 71.991, mean reward: 3.130 [2.372, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.769, 10.100], loss: 0.310507, mae: 0.527957, mean_q: 5.148328
 62433/100000: episode: 1032, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 62.781, mean reward: 3.924 [2.722, 5.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.764, 10.100], loss: 0.334677, mae: 0.514139, mean_q: 4.975188
 62457/100000: episode: 1033, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 77.771, mean reward: 3.240 [1.943, 7.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.245, 10.357], loss: 0.461583, mae: 0.560770, mean_q: 5.064165
 62480/100000: episode: 1034, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 114.671, mean reward: 4.986 [3.072, 14.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.267, 10.100], loss: 0.349060, mae: 0.484140, mean_q: 5.036582
 62505/100000: episode: 1035, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 102.063, mean reward: 4.083 [2.092, 8.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.365, 10.100], loss: 0.382983, mae: 0.560490, mean_q: 5.079097
 62528/100000: episode: 1036, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 61.417, mean reward: 2.670 [1.874, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.568, 10.100], loss: 0.457821, mae: 0.563114, mean_q: 5.189488
 62552/100000: episode: 1037, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 83.366, mean reward: 3.474 [2.643, 5.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.490], loss: 0.431557, mae: 0.570780, mean_q: 5.174079
 62582/100000: episode: 1038, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 108.967, mean reward: 3.632 [2.634, 5.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.493, 10.100], loss: 0.463229, mae: 0.578007, mean_q: 5.158135
 62607/100000: episode: 1039, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 110.037, mean reward: 4.401 [2.627, 8.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.946, 10.100], loss: 0.426808, mae: 0.578945, mean_q: 5.261259
 62629/100000: episode: 1040, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 116.977, mean reward: 5.317 [2.290, 9.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.427, 10.100], loss: 0.329458, mae: 0.543096, mean_q: 5.308977
 62651/100000: episode: 1041, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 85.559, mean reward: 3.889 [2.835, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.551, 10.100], loss: 0.368849, mae: 0.535849, mean_q: 5.274976
 62665/100000: episode: 1042, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 79.569, mean reward: 5.683 [2.573, 15.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.054, 10.448], loss: 0.368618, mae: 0.522777, mean_q: 5.345312
 62681/100000: episode: 1043, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 81.498, mean reward: 5.094 [3.424, 9.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.804, 10.100], loss: 0.313439, mae: 0.546254, mean_q: 5.330519
 62706/100000: episode: 1044, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 80.135, mean reward: 3.205 [1.896, 5.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.171, 10.100], loss: 0.333559, mae: 0.564378, mean_q: 5.316655
 62730/100000: episode: 1045, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 94.184, mean reward: 3.924 [2.866, 5.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.463, 10.561], loss: 0.310261, mae: 0.534451, mean_q: 5.208905
 62746/100000: episode: 1046, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 47.295, mean reward: 2.956 [1.925, 5.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.099, 10.100], loss: 0.357053, mae: 0.568603, mean_q: 5.281374
 62770/100000: episode: 1047, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 108.529, mean reward: 4.522 [2.705, 7.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.453, 10.467], loss: 0.248866, mae: 0.491221, mean_q: 5.117408
 62795/100000: episode: 1048, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 73.801, mean reward: 2.952 [2.270, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.322, 10.100], loss: 0.353091, mae: 0.543990, mean_q: 5.338683
 62819/100000: episode: 1049, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 98.314, mean reward: 4.096 [2.216, 7.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.057, 10.387], loss: 0.288899, mae: 0.512788, mean_q: 5.184860
 62835/100000: episode: 1050, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 53.329, mean reward: 3.333 [2.470, 6.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.331, 10.100], loss: 0.274735, mae: 0.507949, mean_q: 5.213088
 62859/100000: episode: 1051, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 76.861, mean reward: 3.203 [1.809, 6.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.950, 10.357], loss: 0.426295, mae: 0.558424, mean_q: 5.242833
 62881/100000: episode: 1052, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 116.467, mean reward: 5.294 [2.925, 8.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.411, 10.100], loss: 0.400126, mae: 0.565750, mean_q: 5.388341
 62906/100000: episode: 1053, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 110.571, mean reward: 4.423 [3.141, 7.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.579, 10.100], loss: 0.505953, mae: 0.602145, mean_q: 5.289033
 62936/100000: episode: 1054, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 138.179, mean reward: 4.606 [3.144, 7.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.625, 10.100], loss: 0.335757, mae: 0.561731, mean_q: 5.344704
 62966/100000: episode: 1055, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 133.385, mean reward: 4.446 [3.361, 8.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.357, 10.100], loss: 0.370357, mae: 0.561995, mean_q: 5.394286
 62996/100000: episode: 1056, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 78.220, mean reward: 2.607 [1.699, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.180, 10.100], loss: 0.675818, mae: 0.678565, mean_q: 5.597530
 63018/100000: episode: 1057, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 84.734, mean reward: 3.852 [2.301, 6.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.317, 10.100], loss: 0.564205, mae: 0.638448, mean_q: 5.493555
 63042/100000: episode: 1058, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 69.394, mean reward: 2.891 [1.803, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.057, 10.281], loss: 0.457058, mae: 0.584113, mean_q: 5.541950
 63072/100000: episode: 1059, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 95.339, mean reward: 3.178 [2.365, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.322, 10.100], loss: 0.417391, mae: 0.578880, mean_q: 5.477727
 63097/100000: episode: 1060, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 92.499, mean reward: 3.700 [2.768, 4.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.392, 10.100], loss: 0.431485, mae: 0.639181, mean_q: 5.490941
 63113/100000: episode: 1061, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 72.690, mean reward: 4.543 [3.229, 6.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.441, 10.100], loss: 0.447120, mae: 0.623606, mean_q: 5.650207
 63135/100000: episode: 1062, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 83.455, mean reward: 3.793 [2.655, 9.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.139, 10.100], loss: 0.437669, mae: 0.565674, mean_q: 5.424267
 63149/100000: episode: 1063, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 62.148, mean reward: 4.439 [3.367, 8.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.287, 10.511], loss: 0.423271, mae: 0.623423, mean_q: 5.608414
 63171/100000: episode: 1064, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 76.599, mean reward: 3.482 [2.542, 5.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.425, 10.100], loss: 0.513132, mae: 0.668191, mean_q: 5.623252
 63194/100000: episode: 1065, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 79.884, mean reward: 3.473 [2.666, 5.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.306, 10.100], loss: 0.471294, mae: 0.623312, mean_q: 5.578746
 63219/100000: episode: 1066, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 92.848, mean reward: 3.714 [3.246, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.333, 10.100], loss: 0.401587, mae: 0.576044, mean_q: 5.518503
 63233/100000: episode: 1067, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 57.019, mean reward: 4.073 [3.266, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.527, 10.499], loss: 0.528764, mae: 0.621701, mean_q: 5.536335
 63258/100000: episode: 1068, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 82.429, mean reward: 3.297 [2.237, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.474, 10.100], loss: 0.528302, mae: 0.622773, mean_q: 5.646127
 63280/100000: episode: 1069, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 72.141, mean reward: 3.279 [2.549, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.443, 10.100], loss: 0.427738, mae: 0.611807, mean_q: 5.704780
 63304/100000: episode: 1070, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 91.709, mean reward: 3.821 [2.143, 9.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.540, 10.387], loss: 0.456115, mae: 0.567103, mean_q: 5.572912
 63320/100000: episode: 1071, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 47.152, mean reward: 2.947 [1.919, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.243, 10.100], loss: 0.549680, mae: 0.682239, mean_q: 5.715003
 63342/100000: episode: 1072, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 102.735, mean reward: 4.670 [2.522, 8.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.937, 10.100], loss: 0.502653, mae: 0.603919, mean_q: 5.655657
 63356/100000: episode: 1073, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 48.579, mean reward: 3.470 [2.800, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.982, 10.523], loss: 0.402149, mae: 0.625630, mean_q: 5.801290
 63381/100000: episode: 1074, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 102.516, mean reward: 4.101 [2.797, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.039, 10.100], loss: 0.483120, mae: 0.597639, mean_q: 5.702670
 63403/100000: episode: 1075, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 60.372, mean reward: 2.744 [2.222, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.334, 10.100], loss: 0.319903, mae: 0.593383, mean_q: 5.642701
 63427/100000: episode: 1076, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 114.957, mean reward: 4.790 [3.121, 7.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.366, 10.635], loss: 0.422144, mae: 0.646318, mean_q: 5.623667
 63443/100000: episode: 1077, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 41.961, mean reward: 2.623 [1.679, 6.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.661, 10.100], loss: 0.470817, mae: 0.589785, mean_q: 5.819895
 63459/100000: episode: 1078, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 54.255, mean reward: 3.391 [2.336, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.274, 10.100], loss: 0.872607, mae: 0.726471, mean_q: 5.902001
 63473/100000: episode: 1079, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 44.876, mean reward: 3.205 [2.008, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.458], loss: 0.528356, mae: 0.606448, mean_q: 5.744201
[Info] 3-TH LEVEL FOUND: 9.152532577514648, Considering 10/90 traces
 63495/100000: episode: 1080, duration: 4.263s, episode steps: 22, steps per second: 5, episode reward: 65.922, mean reward: 2.996 [2.229, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.368, 10.100], loss: 0.342547, mae: 0.581145, mean_q: 5.744938
[Info] FALSIFICATION!
 63504/100000: episode: 1081, duration: 0.228s, episode steps: 9, steps per second: 39, episode reward: 1040.554, mean reward: 115.617 [3.837, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.593, 10.067], loss: 0.313438, mae: 0.523501, mean_q: 5.615810
[Info] FALSIFICATION!
 63521/100000: episode: 1082, duration: 0.255s, episode steps: 17, steps per second: 67, episode reward: 1220.932, mean reward: 71.820 [6.732, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.320, 10.575], loss: 0.324039, mae: 0.531575, mean_q: 5.558868
 63542/100000: episode: 1083, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 116.978, mean reward: 5.570 [2.824, 16.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.473], loss: 0.698949, mae: 0.703766, mean_q: 5.819461
 63562/100000: episode: 1084, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 128.773, mean reward: 6.439 [2.840, 17.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.418, 10.100], loss: 0.622376, mae: 0.661694, mean_q: 5.808611
 63575/100000: episode: 1085, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 125.315, mean reward: 9.640 [5.729, 16.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.684, 10.100], loss: 0.468650, mae: 0.649681, mean_q: 5.893666
 63587/100000: episode: 1086, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 52.488, mean reward: 4.374 [3.319, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.340, 10.100], loss: 0.569237, mae: 0.694151, mean_q: 5.853416
[Info] FALSIFICATION!
 63596/100000: episode: 1087, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 1066.195, mean reward: 118.466 [3.390, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.548, 10.082], loss: 1703.959717, mae: 5.522750, mean_q: 7.601034
 63609/100000: episode: 1088, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 64.011, mean reward: 4.924 [3.267, 9.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.460, 10.100], loss: 3.842695, mae: 2.151399, mean_q: 5.447953
 63617/100000: episode: 1089, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 35.004, mean reward: 4.376 [3.758, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.464, 10.100], loss: 1912.462280, mae: 5.530135, mean_q: 6.770161
 63630/100000: episode: 1090, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 61.581, mean reward: 4.737 [3.374, 6.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.398, 10.100], loss: 1180.633545, mae: 6.013622, mean_q: 9.647401
 63642/100000: episode: 1091, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 50.022, mean reward: 4.168 [3.541, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.474, 10.100], loss: 3.893028, mae: 2.119746, mean_q: 7.663624
 63658/100000: episode: 1092, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 115.949, mean reward: 7.247 [4.268, 10.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.004, 10.100], loss: 969.086731, mae: 3.600792, mean_q: 6.647719
 63679/100000: episode: 1093, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 92.395, mean reward: 4.400 [3.208, 5.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.573], loss: 1.309079, mae: 1.198477, mean_q: 6.658681
 63692/100000: episode: 1094, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 69.574, mean reward: 5.352 [4.492, 8.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.575, 10.100], loss: 0.941164, mae: 0.895408, mean_q: 6.197500
 63708/100000: episode: 1095, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 86.898, mean reward: 5.431 [3.615, 8.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.394, 10.100], loss: 0.791490, mae: 0.816797, mean_q: 6.434346
 63724/100000: episode: 1096, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 154.346, mean reward: 9.647 [5.198, 29.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.551, 10.100], loss: 0.833436, mae: 0.779207, mean_q: 6.368279
[Info] FALSIFICATION!
 63734/100000: episode: 1097, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 1073.198, mean reward: 107.320 [5.385, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.543, 10.082], loss: 1.342478, mae: 0.944857, mean_q: 6.530544
 63747/100000: episode: 1098, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 78.695, mean reward: 6.053 [4.198, 10.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.566, 10.100], loss: 0.601600, mae: 0.701393, mean_q: 6.395942
 63760/100000: episode: 1099, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 71.142, mean reward: 5.472 [3.960, 12.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.139, 10.100], loss: 1178.592529, mae: 4.476902, mean_q: 8.274487
 63772/100000: episode: 1100, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 51.792, mean reward: 4.316 [3.745, 5.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.496, 10.100], loss: 2529.447998, mae: 10.132995, mean_q: 11.599045
 63780/100000: episode: 1101, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 45.675, mean reward: 5.709 [4.079, 7.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.324, 10.100], loss: 52.732502, mae: 5.851542, mean_q: 11.245260
 63792/100000: episode: 1102, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 56.247, mean reward: 4.687 [3.686, 7.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.371, 10.100], loss: 12.731894, mae: 4.151035, mean_q: 3.068784
 63812/100000: episode: 1103, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 114.582, mean reward: 5.729 [3.395, 18.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.557, 10.100], loss: 764.817749, mae: 3.043328, mean_q: 6.288690
 63832/100000: episode: 1104, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 95.780, mean reward: 4.789 [2.904, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.193, 10.100], loss: 3.296113, mae: 2.164151, mean_q: 8.765108
 63844/100000: episode: 1105, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 96.299, mean reward: 8.025 [4.051, 17.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.552, 10.100], loss: 1.271906, mae: 0.954417, mean_q: 6.645105
 63856/100000: episode: 1106, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 46.493, mean reward: 3.874 [2.811, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.380, 10.100], loss: 1273.613159, mae: 3.657654, mean_q: 6.837891
[Info] FALSIFICATION!
 63864/100000: episode: 1107, duration: 0.317s, episode steps: 8, steps per second: 25, episode reward: 1379.843, mean reward: 172.480 [5.194, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.569, 9.756], loss: 1903.004639, mae: 5.959025, mean_q: 8.619500
 63885/100000: episode: 1108, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 96.809, mean reward: 4.610 [2.679, 7.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.274, 10.435], loss: 3.909615, mae: 1.757041, mean_q: 8.382383
 63898/100000: episode: 1109, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 68.569, mean reward: 5.275 [3.548, 8.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.156, 10.100], loss: 1.108509, mae: 0.966103, mean_q: 6.539866
 63910/100000: episode: 1110, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 83.092, mean reward: 6.924 [3.260, 27.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.421, 10.100], loss: 1271.997925, mae: 3.845629, mean_q: 7.320620
 63923/100000: episode: 1111, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 72.851, mean reward: 5.604 [3.563, 7.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.391, 10.100], loss: 4.136089, mae: 2.247078, mean_q: 9.016892
 63943/100000: episode: 1112, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 84.818, mean reward: 4.241 [2.968, 6.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.549], loss: 1521.880737, mae: 4.408948, mean_q: 7.864456
 63953/100000: episode: 1113, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 49.873, mean reward: 4.987 [4.337, 6.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.359, 10.100], loss: 1514.969482, mae: 5.708459, mean_q: 9.463942
 63966/100000: episode: 1114, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 491.182, mean reward: 37.783 [6.465, 200.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.402, 10.100], loss: 1167.061279, mae: 6.180989, mean_q: 10.890385
[Info] FALSIFICATION!
 63973/100000: episode: 1115, duration: 0.301s, episode steps: 7, steps per second: 23, episode reward: 1081.290, mean reward: 154.470 [4.166, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.366, 10.020], loss: 3.272872, mae: 1.818806, mean_q: 8.230493
 63993/100000: episode: 1116, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 120.564, mean reward: 6.028 [4.269, 9.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.527, 10.100], loss: 2.470436, mae: 1.421789, mean_q: 6.342818
 64005/100000: episode: 1117, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 86.772, mean reward: 7.231 [3.824, 14.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.608, 10.100], loss: 1.484408, mae: 1.104842, mean_q: 7.059364
 64025/100000: episode: 1118, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 92.743, mean reward: 4.637 [2.176, 11.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.483], loss: 761.342896, mae: 4.008056, mean_q: 9.320602
 64037/100000: episode: 1119, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 51.407, mean reward: 4.284 [3.535, 6.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.414, 10.100], loss: 8.864416, mae: 2.638397, mean_q: 9.439736
 64050/100000: episode: 1120, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 65.248, mean reward: 5.019 [3.953, 6.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.533, 10.100], loss: 48.105549, mae: 1.794653, mean_q: 7.198940
 64070/100000: episode: 1121, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 91.472, mean reward: 4.574 [2.546, 6.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.284, 10.100], loss: 789.410767, mae: 3.608291, mean_q: 8.189341
 64083/100000: episode: 1122, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 69.657, mean reward: 5.358 [3.283, 16.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.543, 10.100], loss: 2326.291260, mae: 7.817379, mean_q: 10.556042
 64103/100000: episode: 1123, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 153.802, mean reward: 7.690 [4.856, 33.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.321, 10.100], loss: 6.521152, mae: 2.787764, mean_q: 9.931051
 64115/100000: episode: 1124, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 53.891, mean reward: 4.491 [3.104, 12.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.822, 10.100], loss: 9.040370, mae: 1.834774, mean_q: 6.902523
 64128/100000: episode: 1125, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 71.001, mean reward: 5.462 [3.530, 8.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.365, 10.100], loss: 1168.718628, mae: 3.899327, mean_q: 7.806853
 64144/100000: episode: 1126, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 122.619, mean reward: 7.664 [4.817, 14.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.466, 10.100], loss: 68.080605, mae: 2.716566, mean_q: 9.513460
 64156/100000: episode: 1127, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 68.376, mean reward: 5.698 [3.814, 11.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.315, 10.100], loss: 88.221863, mae: 2.012464, mean_q: 7.894067
 64177/100000: episode: 1128, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 108.898, mean reward: 5.186 [2.194, 16.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.438], loss: 2.039754, mae: 1.247834, mean_q: 7.539297
 64189/100000: episode: 1129, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 52.628, mean reward: 4.386 [3.116, 7.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.855, 10.100], loss: 1271.109131, mae: 3.913174, mean_q: 7.745972
 64201/100000: episode: 1130, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 46.410, mean reward: 3.867 [3.216, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.493, 10.100], loss: 5047.859863, mae: 12.960906, mean_q: 9.942966
 64211/100000: episode: 1131, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 56.148, mean reward: 5.615 [4.531, 6.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.392, 10.100], loss: 39.775459, mae: 8.266583, mean_q: 15.866626
 64223/100000: episode: 1132, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 80.041, mean reward: 6.670 [3.903, 12.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.364, 10.100], loss: 13.191480, mae: 4.104585, mean_q: 12.072297
 64231/100000: episode: 1133, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 27.452, mean reward: 3.431 [2.768, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.398, 10.100], loss: 3789.772217, mae: 9.239176, mean_q: 7.490587
 64241/100000: episode: 1134, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 61.883, mean reward: 6.188 [3.356, 9.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.555, 10.100], loss: 1505.260864, mae: 5.289888, mean_q: 9.906956
 64262/100000: episode: 1135, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 90.285, mean reward: 4.299 [3.068, 7.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.581], loss: 776.906860, mae: 6.518220, mean_q: 12.687487
 64274/100000: episode: 1136, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 73.962, mean reward: 6.164 [5.181, 7.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.531, 10.100], loss: 25.417887, mae: 1.800458, mean_q: 8.157119
 64286/100000: episode: 1137, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 43.397, mean reward: 3.616 [2.799, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.468, 10.100], loss: 2.774416, mae: 1.535520, mean_q: 7.197027
 64299/100000: episode: 1138, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 98.218, mean reward: 7.555 [4.407, 12.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.441, 10.100], loss: 5.390739, mae: 1.458423, mean_q: 7.683845
 64312/100000: episode: 1139, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 128.298, mean reward: 9.869 [4.598, 20.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.562, 10.100], loss: 2.404996, mae: 1.272450, mean_q: 7.951837
 64332/100000: episode: 1140, duration: 0.107s, episode steps: 20, steps per second: 188, episode reward: 76.758, mean reward: 3.838 [2.617, 5.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.378], loss: 2320.891602, mae: 7.566492, mean_q: 9.893141
 64344/100000: episode: 1141, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 41.444, mean reward: 3.454 [2.898, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.788, 10.100], loss: 1315.295898, mae: 8.545253, mean_q: 14.006637
 64356/100000: episode: 1142, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 51.871, mean reward: 4.323 [3.326, 7.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.442, 10.100], loss: 7.301043, mae: 2.675104, mean_q: 10.789592
 64366/100000: episode: 1143, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 39.950, mean reward: 3.995 [3.201, 5.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.369, 10.100], loss: 2.327852, mae: 1.404914, mean_q: 7.920938
 64382/100000: episode: 1144, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 90.108, mean reward: 5.632 [3.852, 7.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.829, 10.100], loss: 987.726746, mae: 3.901595, mean_q: 8.434477
 64395/100000: episode: 1145, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 104.441, mean reward: 8.034 [4.270, 13.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.440, 10.100], loss: 1162.697754, mae: 5.481125, mean_q: 10.699956
[Info] FALSIFICATION!
 64400/100000: episode: 1146, duration: 0.282s, episode steps: 5, steps per second: 18, episode reward: 1031.734, mean reward: 206.347 [5.374, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.029, 9.813], loss: 12.371835, mae: 3.808066, mean_q: 11.865514
 64413/100000: episode: 1147, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 64.460, mean reward: 4.958 [4.069, 6.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.493, 10.100], loss: 47.320347, mae: 2.374489, mean_q: 9.605721
 64434/100000: episode: 1148, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 160.068, mean reward: 7.622 [4.465, 21.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.697], loss: 2211.629639, mae: 6.880250, mean_q: 9.830595
 64447/100000: episode: 1149, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 177.855, mean reward: 13.681 [5.092, 50.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.448, 10.100], loss: 1154.984741, mae: 7.359165, mean_q: 13.134447
 64467/100000: episode: 1150, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 90.693, mean reward: 4.535 [3.052, 6.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.429, 10.461], loss: 755.778076, mae: 4.540442, mean_q: 11.268295
 64487/100000: episode: 1151, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 114.257, mean reward: 5.713 [4.091, 7.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.834, 10.529], loss: 3.577579, mae: 1.704728, mean_q: 8.981324
 64500/100000: episode: 1152, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 80.777, mean reward: 6.214 [4.130, 8.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.290, 10.100], loss: 1173.411621, mae: 4.369964, mean_q: 8.135822
 64512/100000: episode: 1153, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 37.519, mean reward: 3.127 [2.527, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.353, 10.100], loss: 1251.781738, mae: 5.116157, mean_q: 10.477257
 64524/100000: episode: 1154, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 50.941, mean reward: 4.245 [3.504, 5.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.466, 10.100], loss: 1253.151978, mae: 7.757319, mean_q: 13.280602
 64545/100000: episode: 1155, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 64.251, mean reward: 3.060 [1.819, 9.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.543, 10.313], loss: 2158.779053, mae: 8.825864, mean_q: 12.989390
[Info] FALSIFICATION!
 64557/100000: episode: 1156, duration: 0.321s, episode steps: 12, steps per second: 37, episode reward: 1086.891, mean reward: 90.574 [4.427, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.439, 10.067], loss: 2475.391846, mae: 13.048008, mean_q: 17.575861
 64569/100000: episode: 1157, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 79.648, mean reward: 6.637 [4.963, 10.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.469, 10.100], loss: 1210.910034, mae: 7.913731, mean_q: 14.565761
 64582/100000: episode: 1158, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 52.114, mean reward: 4.009 [3.510, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.477, 10.100], loss: 1215.705200, mae: 4.647404, mean_q: 9.349842
 64603/100000: episode: 1159, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 112.629, mean reward: 5.363 [3.495, 9.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.632, 10.503], loss: 57.692638, mae: 2.194364, mean_q: 9.005064
 64623/100000: episode: 1160, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 166.028, mean reward: 8.301 [5.233, 11.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.417, 10.100], loss: 753.465820, mae: 3.211007, mean_q: 9.407739
 64643/100000: episode: 1161, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 89.640, mean reward: 4.482 [3.385, 6.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.478, 10.567], loss: 778.038757, mae: 4.159467, mean_q: 10.931014
 64664/100000: episode: 1162, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 109.883, mean reward: 5.233 [2.997, 13.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.533], loss: 4.556661, mae: 1.669049, mean_q: 9.374567
 64674/100000: episode: 1163, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 71.850, mean reward: 7.185 [4.560, 14.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.378, 10.100], loss: 2.519264, mae: 1.372022, mean_q: 8.734947
 64684/100000: episode: 1164, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 44.911, mean reward: 4.491 [3.290, 5.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.431, 10.100], loss: 3.083606, mae: 1.500594, mean_q: 8.092943
 64696/100000: episode: 1165, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 61.308, mean reward: 5.109 [3.462, 7.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.417, 10.100], loss: 75.827934, mae: 2.406775, mean_q: 9.104683
 64709/100000: episode: 1166, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 91.459, mean reward: 7.035 [4.910, 13.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.542, 10.100], loss: 3473.872559, mae: 9.949024, mean_q: 11.218881
 64719/100000: episode: 1167, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 39.347, mean reward: 3.935 [2.780, 5.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.391, 10.100], loss: 13.952939, mae: 4.300603, mean_q: 12.505394
 64735/100000: episode: 1168, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 75.371, mean reward: 4.711 [3.722, 5.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.504, 10.100], loss: 19.800236, mae: 2.353261, mean_q: 10.176847
 64745/100000: episode: 1169, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 52.333, mean reward: 5.233 [4.082, 7.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.412, 10.100], loss: 1500.072510, mae: 4.731730, mean_q: 8.536281
[Info] Complete ISplit Iteration
[Info] Levels: [5.60684, 7.133651, 9.152533, 12.333964]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 1.0]
[Info] Error Prob: 0.0010000000000000002

 64757/100000: episode: 1170, duration: 4.295s, episode steps: 12, steps per second: 3, episode reward: 63.476, mean reward: 5.290 [3.410, 10.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.470, 10.100], loss: 8.389266, mae: 3.027853, mean_q: 11.302940
 64857/100000: episode: 1171, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 175.870, mean reward: 1.759 [1.449, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.294, 10.106], loss: 922.305237, mae: 4.448830, mean_q: 10.664425
 64957/100000: episode: 1172, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.828, mean reward: 1.938 [1.438, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.639, 10.343], loss: 765.695801, mae: 4.392197, mean_q: 10.974902
 65057/100000: episode: 1173, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 195.731, mean reward: 1.957 [1.438, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.267, 10.340], loss: 619.193848, mae: 3.705828, mean_q: 9.561788
 65157/100000: episode: 1174, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 190.580, mean reward: 1.906 [1.485, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.098], loss: 611.303040, mae: 3.794749, mean_q: 9.513623
 65257/100000: episode: 1175, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 266.309, mean reward: 2.663 [1.513, 4.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.678, 10.098], loss: 329.874756, mae: 3.144832, mean_q: 10.068325
 65357/100000: episode: 1176, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.980, mean reward: 1.870 [1.459, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.138, 10.098], loss: 461.389801, mae: 3.015551, mean_q: 9.241476
 65457/100000: episode: 1177, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 190.796, mean reward: 1.908 [1.460, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.093, 10.227], loss: 797.109741, mae: 4.204588, mean_q: 10.090375
 65557/100000: episode: 1178, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 175.035, mean reward: 1.750 [1.441, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.063, 10.140], loss: 1085.081421, mae: 5.324593, mean_q: 11.034218
 65657/100000: episode: 1179, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 216.484, mean reward: 2.165 [1.481, 12.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.203, 10.098], loss: 612.819153, mae: 3.116641, mean_q: 9.303682
 65757/100000: episode: 1180, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 179.643, mean reward: 1.796 [1.487, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.098], loss: 770.597595, mae: 4.318627, mean_q: 10.349682
 65857/100000: episode: 1181, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.233, mean reward: 1.922 [1.477, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.098], loss: 1215.192993, mae: 6.205943, mean_q: 11.258936
 65957/100000: episode: 1182, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 220.571, mean reward: 2.206 [1.475, 8.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.413], loss: 323.513000, mae: 2.570820, mean_q: 8.654816
 66057/100000: episode: 1183, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.773, mean reward: 1.938 [1.451, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.172], loss: 1356.406128, mae: 7.059247, mean_q: 11.235379
 66157/100000: episode: 1184, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.897, mean reward: 1.949 [1.454, 7.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.047, 10.098], loss: 626.204163, mae: 3.784109, mean_q: 9.862500
 66257/100000: episode: 1185, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.021, mean reward: 1.930 [1.446, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.477, 10.131], loss: 929.132812, mae: 4.990066, mean_q: 10.430602
 66357/100000: episode: 1186, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 196.778, mean reward: 1.968 [1.482, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.596, 10.098], loss: 485.245361, mae: 3.937988, mean_q: 9.586649
 66457/100000: episode: 1187, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.692, mean reward: 1.797 [1.440, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.494, 10.098], loss: 755.187988, mae: 4.443309, mean_q: 9.756991
 66557/100000: episode: 1188, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 197.119, mean reward: 1.971 [1.452, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.240], loss: 636.265808, mae: 3.380944, mean_q: 9.147006
 66657/100000: episode: 1189, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 223.439, mean reward: 2.234 [1.582, 4.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.098], loss: 1351.468750, mae: 6.456124, mean_q: 11.015164
 66757/100000: episode: 1190, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.172, mean reward: 1.922 [1.460, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.245, 10.162], loss: 1194.502197, mae: 5.771461, mean_q: 10.800925
 66857/100000: episode: 1191, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 190.408, mean reward: 1.904 [1.498, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.041, 10.157], loss: 916.730957, mae: 4.754048, mean_q: 10.341057
 66957/100000: episode: 1192, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.601, mean reward: 1.906 [1.454, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.793, 10.126], loss: 595.870117, mae: 3.813241, mean_q: 9.841866
 67057/100000: episode: 1193, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 177.541, mean reward: 1.775 [1.439, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.453, 10.106], loss: 1672.791138, mae: 7.269158, mean_q: 11.365129
 67157/100000: episode: 1194, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 189.814, mean reward: 1.898 [1.441, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.897, 10.098], loss: 306.055756, mae: 3.218784, mean_q: 9.474671
 67257/100000: episode: 1195, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 181.852, mean reward: 1.819 [1.457, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.264, 10.098], loss: 765.245483, mae: 3.982721, mean_q: 9.156404
 67357/100000: episode: 1196, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 204.547, mean reward: 2.045 [1.541, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.749, 10.213], loss: 1202.891357, mae: 6.036362, mean_q: 10.710605
 67457/100000: episode: 1197, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 221.428, mean reward: 2.214 [1.513, 5.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.366], loss: 449.527161, mae: 2.732094, mean_q: 8.507405
 67557/100000: episode: 1198, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 176.072, mean reward: 1.761 [1.448, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.507, 10.220], loss: 604.253418, mae: 3.990237, mean_q: 9.494305
 67657/100000: episode: 1199, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.841, mean reward: 1.928 [1.468, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.251, 10.236], loss: 881.988647, mae: 4.475627, mean_q: 9.457964
 67757/100000: episode: 1200, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 218.909, mean reward: 2.189 [1.541, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.366, 10.368], loss: 1052.508301, mae: 5.894560, mean_q: 10.764353
 67857/100000: episode: 1201, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 207.301, mean reward: 2.073 [1.500, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.579, 10.226], loss: 1171.374512, mae: 5.136881, mean_q: 9.057452
 67957/100000: episode: 1202, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 174.066, mean reward: 1.741 [1.434, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.098], loss: 462.419312, mae: 2.822631, mean_q: 8.131369
 68057/100000: episode: 1203, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 204.101, mean reward: 2.041 [1.465, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.700, 10.219], loss: 1347.550171, mae: 6.093521, mean_q: 10.332127
 68157/100000: episode: 1204, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.007, mean reward: 1.870 [1.499, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.770, 10.172], loss: 592.889832, mae: 3.983299, mean_q: 9.329621
 68257/100000: episode: 1205, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 208.780, mean reward: 2.088 [1.518, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.378, 10.402], loss: 1476.582764, mae: 6.437365, mean_q: 9.890352
 68357/100000: episode: 1206, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.911, mean reward: 1.839 [1.447, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.553, 10.098], loss: 1050.437622, mae: 5.253037, mean_q: 9.824060
 68457/100000: episode: 1207, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 186.588, mean reward: 1.866 [1.448, 6.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.812, 10.272], loss: 309.179108, mae: 2.542623, mean_q: 8.122073
 68557/100000: episode: 1208, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.551, mean reward: 1.896 [1.455, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.820, 10.172], loss: 617.122986, mae: 3.256767, mean_q: 7.849054
 68657/100000: episode: 1209, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.463, mean reward: 1.995 [1.487, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.149, 10.258], loss: 332.480743, mae: 2.877812, mean_q: 7.732432
 68757/100000: episode: 1210, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.188, mean reward: 1.972 [1.450, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.411, 10.151], loss: 761.130371, mae: 3.687376, mean_q: 7.714115
 68857/100000: episode: 1211, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 184.441, mean reward: 1.844 [1.432, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.313, 10.098], loss: 301.683228, mae: 1.816594, mean_q: 6.529912
 68957/100000: episode: 1212, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 208.082, mean reward: 2.081 [1.446, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.731, 10.371], loss: 300.946167, mae: 1.884166, mean_q: 6.378811
 69057/100000: episode: 1213, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 212.664, mean reward: 2.127 [1.488, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.677, 10.098], loss: 151.682083, mae: 1.194389, mean_q: 5.625904
 69157/100000: episode: 1214, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 181.582, mean reward: 1.816 [1.450, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.775, 10.214], loss: 151.207291, mae: 0.995341, mean_q: 5.069064
 69257/100000: episode: 1215, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 186.511, mean reward: 1.865 [1.457, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.306, 10.219], loss: 449.531921, mae: 1.865509, mean_q: 5.356130
 69357/100000: episode: 1216, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 180.785, mean reward: 1.808 [1.451, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.207, 10.098], loss: 149.986725, mae: 1.362693, mean_q: 5.266984
 69457/100000: episode: 1217, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 198.584, mean reward: 1.986 [1.500, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.066, 10.098], loss: 1.068208, mae: 0.652280, mean_q: 4.719918
 69557/100000: episode: 1218, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 224.926, mean reward: 2.249 [1.504, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.961, 10.315], loss: 0.684325, mae: 0.531782, mean_q: 4.310215
 69657/100000: episode: 1219, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 189.758, mean reward: 1.898 [1.474, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.272, 10.098], loss: 0.349000, mae: 0.462341, mean_q: 4.197290
 69757/100000: episode: 1220, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 186.474, mean reward: 1.865 [1.457, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.678, 10.134], loss: 0.261724, mae: 0.424862, mean_q: 3.918486
 69857/100000: episode: 1221, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.011, mean reward: 1.920 [1.460, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.480, 10.098], loss: 0.253191, mae: 0.425258, mean_q: 3.875286
 69957/100000: episode: 1222, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 229.265, mean reward: 2.293 [1.466, 8.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.113, 10.098], loss: 0.250180, mae: 0.414375, mean_q: 3.891769
 70057/100000: episode: 1223, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.001, mean reward: 1.980 [1.453, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.891, 10.098], loss: 0.242213, mae: 0.419411, mean_q: 3.909127
 70157/100000: episode: 1224, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 196.066, mean reward: 1.961 [1.479, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.266], loss: 0.196231, mae: 0.402887, mean_q: 3.914932
 70257/100000: episode: 1225, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.738, mean reward: 1.887 [1.461, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.996, 10.143], loss: 0.181173, mae: 0.391746, mean_q: 3.893047
 70357/100000: episode: 1226, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 174.553, mean reward: 1.746 [1.454, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.994, 10.098], loss: 0.215504, mae: 0.395243, mean_q: 3.910405
 70457/100000: episode: 1227, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.976, mean reward: 1.890 [1.452, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.630, 10.126], loss: 0.207381, mae: 0.384773, mean_q: 3.901696
 70557/100000: episode: 1228, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.458, mean reward: 1.945 [1.467, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.869, 10.098], loss: 0.184480, mae: 0.384482, mean_q: 3.890908
 70657/100000: episode: 1229, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 192.956, mean reward: 1.930 [1.489, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.266], loss: 0.166205, mae: 0.375316, mean_q: 3.878809
 70757/100000: episode: 1230, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 186.320, mean reward: 1.863 [1.504, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.455, 10.125], loss: 0.152977, mae: 0.360773, mean_q: 3.862527
 70857/100000: episode: 1231, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 202.890, mean reward: 2.029 [1.500, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.334], loss: 0.158475, mae: 0.372711, mean_q: 3.852150
 70957/100000: episode: 1232, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 209.450, mean reward: 2.095 [1.512, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.161], loss: 0.172154, mae: 0.367789, mean_q: 3.909466
 71057/100000: episode: 1233, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 186.089, mean reward: 1.861 [1.478, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.724, 10.159], loss: 0.141101, mae: 0.354613, mean_q: 3.879397
 71157/100000: episode: 1234, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.272, mean reward: 1.863 [1.471, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.638, 10.098], loss: 0.125710, mae: 0.346683, mean_q: 3.850482
 71257/100000: episode: 1235, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.573, mean reward: 2.116 [1.443, 6.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.278, 10.358], loss: 0.147105, mae: 0.347386, mean_q: 3.856866
 71357/100000: episode: 1236, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.988, mean reward: 1.970 [1.437, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.407], loss: 0.139093, mae: 0.351149, mean_q: 3.884314
 71457/100000: episode: 1237, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.036, mean reward: 1.830 [1.457, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.217, 10.098], loss: 0.133141, mae: 0.341372, mean_q: 3.884935
 71557/100000: episode: 1238, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 186.865, mean reward: 1.869 [1.459, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.129, 10.199], loss: 0.138232, mae: 0.341422, mean_q: 3.874075
 71657/100000: episode: 1239, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 216.738, mean reward: 2.167 [1.430, 9.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.383, 10.098], loss: 0.134635, mae: 0.338740, mean_q: 3.858507
 71757/100000: episode: 1240, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 208.184, mean reward: 2.082 [1.456, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.438, 10.210], loss: 0.134565, mae: 0.332800, mean_q: 3.885249
 71857/100000: episode: 1241, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 175.980, mean reward: 1.760 [1.460, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.530, 10.098], loss: 0.119754, mae: 0.330885, mean_q: 3.871806
 71957/100000: episode: 1242, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.406, mean reward: 2.054 [1.450, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.003, 10.098], loss: 0.133537, mae: 0.338320, mean_q: 3.862009
 72057/100000: episode: 1243, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.854, mean reward: 1.889 [1.464, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.812, 10.098], loss: 0.124983, mae: 0.331787, mean_q: 3.853455
 72157/100000: episode: 1244, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.570, mean reward: 1.846 [1.494, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.098], loss: 0.147418, mae: 0.351011, mean_q: 3.880653
 72257/100000: episode: 1245, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.734, mean reward: 1.987 [1.489, 5.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.296], loss: 0.125134, mae: 0.333478, mean_q: 3.856639
 72357/100000: episode: 1246, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.504, mean reward: 1.915 [1.517, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.094, 10.241], loss: 0.136713, mae: 0.335351, mean_q: 3.870826
 72457/100000: episode: 1247, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 181.314, mean reward: 1.813 [1.471, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.573, 10.170], loss: 0.160652, mae: 0.335930, mean_q: 3.863750
 72557/100000: episode: 1248, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.887, mean reward: 1.979 [1.456, 5.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.098], loss: 0.137757, mae: 0.341952, mean_q: 3.850557
 72657/100000: episode: 1249, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.939, mean reward: 1.889 [1.460, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.989, 10.098], loss: 0.123899, mae: 0.338542, mean_q: 3.866359
 72757/100000: episode: 1250, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 202.309, mean reward: 2.023 [1.457, 5.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.393, 10.098], loss: 0.140654, mae: 0.336839, mean_q: 3.857996
 72857/100000: episode: 1251, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 200.820, mean reward: 2.008 [1.450, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.036, 10.321], loss: 0.135958, mae: 0.338188, mean_q: 3.861371
 72957/100000: episode: 1252, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 207.453, mean reward: 2.075 [1.576, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.482, 10.215], loss: 0.153164, mae: 0.345763, mean_q: 3.883880
 73057/100000: episode: 1253, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 202.748, mean reward: 2.027 [1.451, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.607, 10.308], loss: 0.130772, mae: 0.337616, mean_q: 3.876311
 73157/100000: episode: 1254, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.905, mean reward: 1.939 [1.461, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.717, 10.098], loss: 0.162389, mae: 0.339322, mean_q: 3.869895
 73257/100000: episode: 1255, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.298, mean reward: 1.953 [1.454, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.425, 10.277], loss: 0.105456, mae: 0.321957, mean_q: 3.874187
 73357/100000: episode: 1256, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.950, mean reward: 1.890 [1.457, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.787, 10.252], loss: 0.130077, mae: 0.329731, mean_q: 3.870138
 73457/100000: episode: 1257, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.949, mean reward: 1.879 [1.460, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.181], loss: 0.115808, mae: 0.314750, mean_q: 3.853140
 73557/100000: episode: 1258, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 201.714, mean reward: 2.017 [1.480, 4.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.172, 10.143], loss: 0.119493, mae: 0.317184, mean_q: 3.856786
 73657/100000: episode: 1259, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 198.176, mean reward: 1.982 [1.456, 5.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.040, 10.353], loss: 0.126240, mae: 0.333538, mean_q: 3.865974
 73757/100000: episode: 1260, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 247.673, mean reward: 2.477 [1.477, 7.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.953, 10.468], loss: 0.133301, mae: 0.340078, mean_q: 3.866062
 73857/100000: episode: 1261, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 182.977, mean reward: 1.830 [1.449, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.098], loss: 0.137641, mae: 0.329811, mean_q: 3.879715
 73957/100000: episode: 1262, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 225.749, mean reward: 2.257 [1.478, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.381], loss: 0.121177, mae: 0.333843, mean_q: 3.876467
 74057/100000: episode: 1263, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 194.708, mean reward: 1.947 [1.484, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.243, 10.098], loss: 0.132645, mae: 0.331701, mean_q: 3.906830
 74157/100000: episode: 1264, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 184.147, mean reward: 1.841 [1.491, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.238, 10.277], loss: 0.118064, mae: 0.329704, mean_q: 3.903105
 74257/100000: episode: 1265, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 194.335, mean reward: 1.943 [1.452, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.608, 10.172], loss: 0.134991, mae: 0.335150, mean_q: 3.890268
 74357/100000: episode: 1266, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 203.707, mean reward: 2.037 [1.528, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.322, 10.150], loss: 0.126765, mae: 0.331869, mean_q: 3.902700
 74457/100000: episode: 1267, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 181.785, mean reward: 1.818 [1.435, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.168, 10.099], loss: 0.131022, mae: 0.324159, mean_q: 3.890406
 74557/100000: episode: 1268, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.928, mean reward: 1.989 [1.461, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.335, 10.098], loss: 0.143736, mae: 0.333849, mean_q: 3.910819
 74657/100000: episode: 1269, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.118, mean reward: 1.971 [1.448, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.533, 10.204], loss: 0.115425, mae: 0.330064, mean_q: 3.881338
[Info] 1-TH LEVEL FOUND: 5.479666709899902, Considering 10/90 traces
 74757/100000: episode: 1270, duration: 4.679s, episode steps: 100, steps per second: 21, episode reward: 193.242, mean reward: 1.932 [1.457, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.672, 10.135], loss: 0.132214, mae: 0.334943, mean_q: 3.890334
 74782/100000: episode: 1271, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 50.657, mean reward: 2.026 [1.539, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.327, 10.202], loss: 0.161303, mae: 0.375806, mean_q: 3.966009
 74785/100000: episode: 1272, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 6.585, mean reward: 2.195 [2.068, 2.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.076, 10.209], loss: 0.138252, mae: 0.337795, mean_q: 3.764376
 74804/100000: episode: 1273, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 56.200, mean reward: 2.958 [2.511, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.511, 10.482], loss: 0.117269, mae: 0.342915, mean_q: 3.892739
 74805/100000: episode: 1274, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 2.291, mean reward: 2.291 [2.291, 2.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.256, 10.100], loss: 0.108963, mae: 0.362001, mean_q: 3.822311
 74807/100000: episode: 1275, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 3.621, mean reward: 1.810 [1.647, 1.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.240, 10.100], loss: 0.102525, mae: 0.333601, mean_q: 3.936990
 74808/100000: episode: 1276, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 2.530, mean reward: 2.530 [2.530, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.193, 10.100], loss: 0.067976, mae: 0.281474, mean_q: 3.924643
 74809/100000: episode: 1277, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 2.530, mean reward: 2.530 [2.530, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.232, 10.100], loss: 0.159584, mae: 0.444958, mean_q: 3.780984
 74829/100000: episode: 1278, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 65.724, mean reward: 3.286 [1.791, 6.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.338, 10.346], loss: 0.131235, mae: 0.354089, mean_q: 3.918411
 74849/100000: episode: 1279, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 107.556, mean reward: 5.378 [3.196, 9.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.514], loss: 0.122649, mae: 0.331000, mean_q: 3.904385
 74857/100000: episode: 1280, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 16.784, mean reward: 2.098 [1.590, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.157, 10.168], loss: 0.106786, mae: 0.341059, mean_q: 3.957779
 74874/100000: episode: 1281, duration: 0.099s, episode steps: 17, steps per second: 173, episode reward: 44.998, mean reward: 2.647 [1.824, 5.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.041, 10.288], loss: 0.124285, mae: 0.324220, mean_q: 3.905905
 74875/100000: episode: 1282, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 2.746, mean reward: 2.746 [2.746, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.251, 10.100], loss: 0.134679, mae: 0.352127, mean_q: 3.865776
 74892/100000: episode: 1283, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 57.162, mean reward: 3.362 [2.728, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.797, 10.451], loss: 0.185137, mae: 0.379797, mean_q: 3.969358
 74894/100000: episode: 1284, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 4.162, mean reward: 2.081 [2.021, 2.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.289, 10.100], loss: 0.114092, mae: 0.367533, mean_q: 4.082330
 74908/100000: episode: 1285, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 42.969, mean reward: 3.069 [2.616, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.413], loss: 0.122286, mae: 0.334048, mean_q: 3.918692
 74911/100000: episode: 1286, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 5.911, mean reward: 1.970 [1.586, 2.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.491, 10.155], loss: 0.147466, mae: 0.362860, mean_q: 3.800624
 74925/100000: episode: 1287, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 46.103, mean reward: 3.293 [2.506, 5.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.133, 10.403], loss: 0.192617, mae: 0.383727, mean_q: 4.001364
 74928/100000: episode: 1288, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 4.893, mean reward: 1.631 [1.478, 1.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.088, 10.162], loss: 0.136108, mae: 0.348109, mean_q: 3.779502
 74942/100000: episode: 1289, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 37.602, mean reward: 2.686 [2.117, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.740, 10.371], loss: 0.182168, mae: 0.357892, mean_q: 3.898521
 74959/100000: episode: 1290, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 55.748, mean reward: 3.279 [2.287, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.473], loss: 0.127719, mae: 0.343651, mean_q: 3.953155
 74973/100000: episode: 1291, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 58.845, mean reward: 4.203 [3.087, 6.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.869, 10.571], loss: 0.119553, mae: 0.329529, mean_q: 3.916171
 74975/100000: episode: 1292, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 3.703, mean reward: 1.852 [1.596, 2.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.509, 10.100], loss: 0.137455, mae: 0.371098, mean_q: 3.754853
 74989/100000: episode: 1293, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 41.221, mean reward: 2.944 [2.457, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.445], loss: 0.120949, mae: 0.349944, mean_q: 3.944212
 75003/100000: episode: 1294, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 45.987, mean reward: 3.285 [2.251, 6.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.514], loss: 0.145341, mae: 0.350184, mean_q: 3.969184
 75020/100000: episode: 1295, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 46.570, mean reward: 2.739 [1.917, 8.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.286], loss: 0.159932, mae: 0.370951, mean_q: 4.030386
 75021/100000: episode: 1296, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 2.335, mean reward: 2.335 [2.335, 2.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.261, 10.100], loss: 0.104254, mae: 0.352715, mean_q: 4.103987
 75038/100000: episode: 1297, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 47.225, mean reward: 2.778 [2.145, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.562, 10.359], loss: 0.230259, mae: 0.379363, mean_q: 3.982088
 75040/100000: episode: 1298, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 4.331, mean reward: 2.165 [1.935, 2.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.327, 10.100], loss: 0.066731, mae: 0.267035, mean_q: 3.863572
 75048/100000: episode: 1299, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 15.346, mean reward: 1.918 [1.555, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.234], loss: 0.165234, mae: 0.391822, mean_q: 3.973060
 75065/100000: episode: 1300, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 51.555, mean reward: 3.033 [2.145, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.417], loss: 0.143701, mae: 0.361026, mean_q: 3.988961
 75084/100000: episode: 1301, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 54.502, mean reward: 2.869 [2.229, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.554, 10.426], loss: 0.215815, mae: 0.367964, mean_q: 3.984411
 75092/100000: episode: 1302, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 14.508, mean reward: 1.813 [1.460, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.041, 10.100], loss: 0.267742, mae: 0.443176, mean_q: 4.070153
 75093/100000: episode: 1303, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 2.352, mean reward: 2.352 [2.352, 2.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.236, 10.100], loss: 0.125717, mae: 0.363812, mean_q: 3.896440
 75118/100000: episode: 1304, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 55.740, mean reward: 2.230 [1.764, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.419, 10.297], loss: 0.155761, mae: 0.353459, mean_q: 3.980330
 75121/100000: episode: 1305, duration: 0.027s, episode steps: 3, steps per second: 110, episode reward: 5.536, mean reward: 1.845 [1.616, 2.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.076, 10.161], loss: 0.099046, mae: 0.352838, mean_q: 4.117493
 75135/100000: episode: 1306, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 40.724, mean reward: 2.909 [2.244, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.049, 10.377], loss: 0.127092, mae: 0.320835, mean_q: 3.932876
 75160/100000: episode: 1307, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 67.402, mean reward: 2.696 [1.821, 4.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.547, 10.474], loss: 0.191866, mae: 0.376421, mean_q: 4.028555
 75174/100000: episode: 1308, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 34.383, mean reward: 2.456 [1.780, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.280], loss: 0.137353, mae: 0.360006, mean_q: 3.990772
 75175/100000: episode: 1309, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 2.636, mean reward: 2.636 [2.636, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.205, 10.100], loss: 0.095880, mae: 0.304209, mean_q: 3.951564
 75194/100000: episode: 1310, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 58.011, mean reward: 3.053 [2.436, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.457], loss: 0.183714, mae: 0.374998, mean_q: 4.006773
 75208/100000: episode: 1311, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 47.577, mean reward: 3.398 [2.206, 9.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.658, 10.355], loss: 0.189538, mae: 0.370344, mean_q: 4.022394
 75227/100000: episode: 1312, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 73.546, mean reward: 3.871 [2.313, 8.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.284, 10.530], loss: 0.139754, mae: 0.350469, mean_q: 4.021245
 75241/100000: episode: 1313, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 44.773, mean reward: 3.198 [2.642, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.385, 10.416], loss: 0.133899, mae: 0.363929, mean_q: 4.053894
 75244/100000: episode: 1314, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 5.801, mean reward: 1.934 [1.712, 2.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.103, 10.142], loss: 0.093005, mae: 0.310327, mean_q: 3.980284
 75245/100000: episode: 1315, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 2.322, mean reward: 2.322 [2.322, 2.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.177, 10.100], loss: 0.111572, mae: 0.359935, mean_q: 3.982107
 75253/100000: episode: 1316, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 17.726, mean reward: 2.216 [1.504, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.149, 10.100], loss: 0.212596, mae: 0.376604, mean_q: 3.981725
 75256/100000: episode: 1317, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 4.547, mean reward: 1.516 [1.486, 1.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.141], loss: 0.161064, mae: 0.424370, mean_q: 4.062312
 75275/100000: episode: 1318, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 52.092, mean reward: 2.742 [2.221, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.647, 10.401], loss: 0.236959, mae: 0.406673, mean_q: 4.041940
 75294/100000: episode: 1319, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 46.980, mean reward: 2.473 [1.737, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.226], loss: 0.263090, mae: 0.432589, mean_q: 4.132905
 75311/100000: episode: 1320, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 51.091, mean reward: 3.005 [2.503, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.505], loss: 0.148961, mae: 0.383139, mean_q: 4.037563
 75313/100000: episode: 1321, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 4.020, mean reward: 2.010 [1.913, 2.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.260, 10.100], loss: 0.086983, mae: 0.322156, mean_q: 4.118570
 75321/100000: episode: 1322, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 14.663, mean reward: 1.833 [1.531, 2.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.701, 10.100], loss: 0.190467, mae: 0.386403, mean_q: 4.059757
 75340/100000: episode: 1323, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 176.407, mean reward: 9.285 [2.388, 105.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.526, 10.526], loss: 0.214195, mae: 0.401609, mean_q: 4.028207
 75342/100000: episode: 1324, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 4.336, mean reward: 2.168 [2.049, 2.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.200, 10.100], loss: 0.088724, mae: 0.320967, mean_q: 4.095734
 75345/100000: episode: 1325, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 5.063, mean reward: 1.688 [1.548, 1.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.093, 10.142], loss: 0.152409, mae: 0.341838, mean_q: 4.003421
 75370/100000: episode: 1326, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 62.641, mean reward: 2.506 [1.922, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.040, 10.430], loss: 0.283132, mae: 0.407560, mean_q: 4.042722
 75389/100000: episode: 1327, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 52.623, mean reward: 2.770 [1.950, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.317], loss: 0.186818, mae: 0.398086, mean_q: 4.111934
 75391/100000: episode: 1328, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 4.009, mean reward: 2.004 [1.949, 2.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.231, 10.100], loss: 0.104833, mae: 0.332721, mean_q: 4.087009
 75410/100000: episode: 1329, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 59.049, mean reward: 3.108 [2.634, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.538], loss: 9.000874, mae: 0.569727, mean_q: 4.102540
 75413/100000: episode: 1330, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 6.636, mean reward: 2.212 [2.036, 2.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.053, 10.183], loss: 0.533716, mae: 0.766251, mean_q: 4.773114
 75421/100000: episode: 1331, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 15.534, mean reward: 1.942 [1.615, 2.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.183, 10.100], loss: 0.322072, mae: 0.594676, mean_q: 3.959172
 75422/100000: episode: 1332, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 2.640, mean reward: 2.640 [2.640, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.242, 10.100], loss: 0.391385, mae: 0.598554, mean_q: 4.418264
 75436/100000: episode: 1333, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 43.222, mean reward: 3.087 [2.263, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.418, 10.441], loss: 0.194564, mae: 0.449457, mean_q: 4.047636
 75439/100000: episode: 1334, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 5.076, mean reward: 1.692 [1.608, 1.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.140, 10.174], loss: 0.158901, mae: 0.390987, mean_q: 4.237383
 75453/100000: episode: 1335, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 59.229, mean reward: 4.231 [2.967, 5.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.448, 10.629], loss: 0.183427, mae: 0.421550, mean_q: 4.095931
 75478/100000: episode: 1336, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 60.631, mean reward: 2.425 [1.966, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.756, 10.483], loss: 0.152139, mae: 0.377800, mean_q: 4.078509
 75486/100000: episode: 1337, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 17.243, mean reward: 2.155 [1.480, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.142], loss: 0.195285, mae: 0.413593, mean_q: 4.038440
 75506/100000: episode: 1338, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 88.117, mean reward: 4.406 [2.700, 5.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.389, 10.367], loss: 0.213660, mae: 0.420252, mean_q: 4.142388
 75523/100000: episode: 1339, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 55.044, mean reward: 3.238 [2.476, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.783, 10.479], loss: 0.196778, mae: 0.385601, mean_q: 4.135460
 75524/100000: episode: 1340, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.836, mean reward: 1.836 [1.836, 1.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.291, 10.100], loss: 0.250150, mae: 0.489561, mean_q: 4.201475
 75525/100000: episode: 1341, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.050, mean reward: 2.050 [2.050, 2.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.211, 10.100], loss: 0.079893, mae: 0.315493, mean_q: 4.046130
 75526/100000: episode: 1342, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 4.998, mean reward: 4.998 [4.998, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.301, 10.100], loss: 0.157591, mae: 0.304140, mean_q: 4.159064
 75546/100000: episode: 1343, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 92.990, mean reward: 4.649 [3.610, 7.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.083, 10.533], loss: 8.483546, mae: 0.566010, mean_q: 4.183396
 75554/100000: episode: 1344, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 14.946, mean reward: 1.868 [1.597, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.100], loss: 0.470278, mae: 0.734126, mean_q: 4.663177
 75555/100000: episode: 1345, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 2.530, mean reward: 2.530 [2.530, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.253, 10.100], loss: 0.214187, mae: 0.535251, mean_q: 3.764617
 75558/100000: episode: 1346, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 5.759, mean reward: 1.920 [1.835, 1.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.240], loss: 0.170307, mae: 0.411522, mean_q: 3.940552
 75577/100000: episode: 1347, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 73.374, mean reward: 3.862 [2.678, 7.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.574], loss: 8.999437, mae: 0.788876, mean_q: 4.445008
 75591/100000: episode: 1348, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 35.374, mean reward: 2.527 [1.943, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.147, 10.377], loss: 0.403732, mae: 0.663847, mean_q: 4.227580
 75611/100000: episode: 1349, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 75.882, mean reward: 3.794 [3.057, 5.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.721, 10.473], loss: 0.238432, mae: 0.466840, mean_q: 4.142728
 75625/100000: episode: 1350, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 42.513, mean reward: 3.037 [2.469, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.192, 10.514], loss: 0.228117, mae: 0.421870, mean_q: 4.255270
 75627/100000: episode: 1351, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 3.835, mean reward: 1.917 [1.904, 1.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.268, 10.100], loss: 0.402270, mae: 0.485210, mean_q: 4.149501
 75647/100000: episode: 1352, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 70.622, mean reward: 3.531 [2.003, 6.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.345], loss: 0.257377, mae: 0.439365, mean_q: 4.311640
 75661/100000: episode: 1353, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 40.072, mean reward: 2.862 [2.284, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-1.533, 10.414], loss: 0.204549, mae: 0.441557, mean_q: 4.204342
 75686/100000: episode: 1354, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 70.800, mean reward: 2.832 [1.999, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.419], loss: 0.276048, mae: 0.469081, mean_q: 4.318213
 75700/100000: episode: 1355, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 50.406, mean reward: 3.600 [2.526, 5.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.585], loss: 0.212517, mae: 0.434281, mean_q: 4.243739
 75702/100000: episode: 1356, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 4.390, mean reward: 2.195 [2.145, 2.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.225, 10.100], loss: 0.129110, mae: 0.377878, mean_q: 4.288978
 75705/100000: episode: 1357, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 4.763, mean reward: 1.588 [1.518, 1.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.051, 10.151], loss: 0.172970, mae: 0.386769, mean_q: 4.135952
 75722/100000: episode: 1358, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 45.050, mean reward: 2.650 [1.908, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.517, 10.257], loss: 0.168678, mae: 0.407637, mean_q: 4.246983
 75736/100000: episode: 1359, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 44.154, mean reward: 3.154 [2.826, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.500], loss: 0.192888, mae: 0.440365, mean_q: 4.269383
[Info] 2-TH LEVEL FOUND: 6.816961288452148, Considering 10/90 traces
 75737/100000: episode: 1360, duration: 4.135s, episode steps: 1, steps per second: 0, episode reward: 1.818, mean reward: 1.818 [1.818, 1.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.247, 10.100], loss: 0.129430, mae: 0.396830, mean_q: 4.049111
 75748/100000: episode: 1361, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 49.685, mean reward: 4.517 [3.158, 9.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.510, 10.518], loss: 0.205222, mae: 0.408710, mean_q: 4.309176
 75765/100000: episode: 1362, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 79.293, mean reward: 4.664 [2.864, 8.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.076, 10.568], loss: 0.346221, mae: 0.636363, mean_q: 4.391640
 75781/100000: episode: 1363, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 68.165, mean reward: 4.260 [2.563, 10.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.058, 10.382], loss: 0.268205, mae: 0.498283, mean_q: 4.358217
 75797/100000: episode: 1364, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 49.517, mean reward: 3.095 [2.346, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.075, 10.444], loss: 0.241652, mae: 0.455761, mean_q: 4.314127
 75813/100000: episode: 1365, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 54.196, mean reward: 3.387 [1.707, 5.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.332], loss: 0.281941, mae: 0.453006, mean_q: 4.273791
 75830/100000: episode: 1366, duration: 0.087s, episode steps: 17, steps per second: 194, episode reward: 50.597, mean reward: 2.976 [2.096, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.403, 10.313], loss: 0.231539, mae: 0.422508, mean_q: 4.191427
 75846/100000: episode: 1367, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 53.405, mean reward: 3.338 [2.480, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.460, 10.456], loss: 0.321219, mae: 0.467476, mean_q: 4.324275
 75857/100000: episode: 1368, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 55.509, mean reward: 5.046 [4.328, 5.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.538], loss: 0.292729, mae: 0.487601, mean_q: 4.370059
 75865/100000: episode: 1369, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 42.687, mean reward: 5.336 [4.092, 6.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.550], loss: 0.204967, mae: 0.422414, mean_q: 4.333605
 75879/100000: episode: 1370, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 52.860, mean reward: 3.776 [2.954, 5.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.524], loss: 0.261875, mae: 0.444138, mean_q: 4.323431
 75896/100000: episode: 1371, duration: 0.087s, episode steps: 17, steps per second: 197, episode reward: 80.300, mean reward: 4.724 [3.278, 8.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.744, 10.551], loss: 0.228482, mae: 0.446669, mean_q: 4.292797
 75913/100000: episode: 1372, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 68.685, mean reward: 4.040 [2.718, 5.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.337, 10.502], loss: 0.283829, mae: 0.451338, mean_q: 4.362894
[Info] FALSIFICATION!
 75925/100000: episode: 1373, duration: 0.320s, episode steps: 12, steps per second: 37, episode reward: 1091.827, mean reward: 90.986 [3.951, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.969, 10.393], loss: 0.215809, mae: 0.453805, mean_q: 4.375083
 75939/100000: episode: 1374, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 53.541, mean reward: 3.824 [3.191, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.933, 10.506], loss: 0.159080, mae: 0.410556, mean_q: 4.335795
 75956/100000: episode: 1375, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 57.795, mean reward: 3.400 [2.352, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.430], loss: 10.152040, mae: 0.876285, mean_q: 4.634178
 75967/100000: episode: 1376, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 59.146, mean reward: 5.377 [4.491, 7.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.478], loss: 0.347719, mae: 0.624033, mean_q: 4.325476
 75983/100000: episode: 1377, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 48.454, mean reward: 3.028 [2.556, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.152, 10.508], loss: 0.274224, mae: 0.517595, mean_q: 4.334257
 75999/100000: episode: 1378, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 55.601, mean reward: 3.475 [2.750, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.531], loss: 966.311890, mae: 3.980706, mean_q: 6.283503
 76007/100000: episode: 1379, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 58.947, mean reward: 7.368 [5.340, 10.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.670], loss: 2.075511, mae: 1.693644, mean_q: 4.327499
 76018/100000: episode: 1380, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 37.422, mean reward: 3.402 [3.049, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.454], loss: 1.158348, mae: 1.239229, mean_q: 4.576252
 76029/100000: episode: 1381, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 44.593, mean reward: 4.054 [2.935, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.756, 10.520], loss: 0.952963, mae: 0.876449, mean_q: 4.998617
 76043/100000: episode: 1382, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 43.320, mean reward: 3.094 [1.989, 7.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.401], loss: 1101.854980, mae: 4.564377, mean_q: 7.076736
 76059/100000: episode: 1383, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 70.757, mean reward: 4.422 [2.586, 6.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-1.032, 10.555], loss: 962.335999, mae: 3.338082, mean_q: 5.701251
 76075/100000: episode: 1384, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 56.951, mean reward: 3.559 [2.430, 8.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.503, 10.396], loss: 2.200231, mae: 1.476167, mean_q: 5.748581
 76089/100000: episode: 1385, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 58.060, mean reward: 4.147 [3.327, 5.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.955, 10.577], loss: 0.800718, mae: 0.900785, mean_q: 4.928388
 76100/100000: episode: 1386, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 38.753, mean reward: 3.523 [2.493, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.436], loss: 0.322608, mae: 0.564929, mean_q: 4.971232
 76116/100000: episode: 1387, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 49.887, mean reward: 3.118 [2.425, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.420], loss: 0.711942, mae: 0.613705, mean_q: 5.017302
 76130/100000: episode: 1388, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 70.605, mean reward: 5.043 [2.512, 14.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.492, 10.549], loss: 0.919936, mae: 0.576142, mean_q: 5.024964
 76146/100000: episode: 1389, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 69.055, mean reward: 4.316 [2.852, 9.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.241, 10.551], loss: 0.576181, mae: 0.535047, mean_q: 4.971186
 76157/100000: episode: 1390, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 47.682, mean reward: 4.335 [3.689, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.495], loss: 0.297412, mae: 0.516423, mean_q: 4.962657
 76173/100000: episode: 1391, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 53.146, mean reward: 3.322 [2.745, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.126, 10.434], loss: 0.395854, mae: 0.533302, mean_q: 4.924580
 76181/100000: episode: 1392, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 68.628, mean reward: 8.578 [4.531, 13.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.533, 10.671], loss: 0.327916, mae: 0.500577, mean_q: 4.998409
 76197/100000: episode: 1393, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 72.688, mean reward: 4.543 [3.131, 5.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.466], loss: 0.438254, mae: 0.556940, mean_q: 4.855390
 76205/100000: episode: 1394, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 48.312, mean reward: 6.039 [4.663, 9.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.677], loss: 0.319527, mae: 0.509959, mean_q: 4.918127
 76221/100000: episode: 1395, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 80.250, mean reward: 5.016 [3.027, 16.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.631, 10.554], loss: 0.412120, mae: 0.570056, mean_q: 4.886247
 76238/100000: episode: 1396, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 69.906, mean reward: 4.112 [2.915, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.519, 10.463], loss: 19.708841, mae: 1.000196, mean_q: 5.058123
 76252/100000: episode: 1397, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 39.320, mean reward: 2.809 [1.793, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.519, 10.276], loss: 1097.521973, mae: 2.911520, mean_q: 5.326354
 76268/100000: episode: 1398, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 61.567, mean reward: 3.848 [2.713, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.107, 10.462], loss: 3.226065, mae: 2.037226, mean_q: 6.706527
 76284/100000: episode: 1399, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 62.635, mean reward: 3.915 [2.364, 5.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.384], loss: 0.716280, mae: 0.867464, mean_q: 4.687395
 76298/100000: episode: 1400, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 45.342, mean reward: 3.239 [2.426, 5.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.445], loss: 0.487255, mae: 0.696158, mean_q: 4.932029
 76315/100000: episode: 1401, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 64.488, mean reward: 3.793 [2.761, 5.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.490], loss: 0.681491, mae: 0.643515, mean_q: 5.101562
 76331/100000: episode: 1402, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 48.586, mean reward: 3.037 [2.210, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-1.170, 10.557], loss: 961.223328, mae: 2.557753, mean_q: 5.062669
 76348/100000: episode: 1403, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 73.173, mean reward: 4.304 [2.834, 7.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.944, 10.502], loss: 4.625165, mae: 2.416703, mean_q: 7.324366
 76364/100000: episode: 1404, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 64.483, mean reward: 4.030 [2.427, 5.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.099, 10.538], loss: 0.987423, mae: 0.967319, mean_q: 4.769875
 76378/100000: episode: 1405, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 33.929, mean reward: 2.423 [2.080, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.049, 10.377], loss: 12.281457, mae: 0.935605, mean_q: 5.047015
 76392/100000: episode: 1406, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 52.522, mean reward: 3.752 [3.234, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.175, 10.486], loss: 0.466492, mae: 0.652329, mean_q: 5.160586
 76408/100000: episode: 1407, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 54.990, mean reward: 3.437 [2.793, 5.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.618], loss: 0.706415, mae: 0.634073, mean_q: 5.190263
[Info] FALSIFICATION!
 76414/100000: episode: 1408, duration: 0.193s, episode steps: 6, steps per second: 31, episode reward: 1040.599, mean reward: 173.433 [3.881, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.806, 10.380], loss: 0.362974, mae: 0.537739, mean_q: 4.906352
 76430/100000: episode: 1409, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 45.302, mean reward: 2.831 [2.110, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.410, 10.403], loss: 0.288749, mae: 0.508044, mean_q: 4.907537
 76444/100000: episode: 1410, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 45.617, mean reward: 3.258 [2.145, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.412], loss: 0.360635, mae: 0.557210, mean_q: 4.947498
 76461/100000: episode: 1411, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 67.131, mean reward: 3.949 [2.319, 8.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.238, 10.475], loss: 0.290528, mae: 0.507924, mean_q: 4.985638
 76477/100000: episode: 1412, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 44.090, mean reward: 2.756 [2.245, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.461], loss: 0.348490, mae: 0.528612, mean_q: 5.023584
 76491/100000: episode: 1413, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 38.323, mean reward: 2.737 [2.012, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.040, 10.382], loss: 11.959786, mae: 0.772046, mean_q: 5.057527
 76507/100000: episode: 1414, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 47.289, mean reward: 2.956 [2.466, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.044, 10.367], loss: 10.541785, mae: 0.828540, mean_q: 5.177165
 76515/100000: episode: 1415, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 39.492, mean reward: 4.936 [3.077, 6.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-1.030, 10.608], loss: 0.988314, mae: 0.628246, mean_q: 5.232001
 76532/100000: episode: 1416, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 50.000, mean reward: 2.941 [1.995, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.296], loss: 0.302384, mae: 0.525743, mean_q: 5.017416
 76543/100000: episode: 1417, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 46.183, mean reward: 4.198 [3.178, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.608], loss: 0.567578, mae: 0.559830, mean_q: 4.894229
 76554/100000: episode: 1418, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 40.618, mean reward: 3.693 [3.081, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.589], loss: 1398.314331, mae: 3.818357, mean_q: 5.659583
 76570/100000: episode: 1419, duration: 0.084s, episode steps: 16, steps per second: 189, episode reward: 48.443, mean reward: 3.028 [2.234, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.407], loss: 3.715924, mae: 2.188169, mean_q: 6.890178
 76587/100000: episode: 1420, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 61.759, mean reward: 3.633 [2.423, 6.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.092, 10.431], loss: 0.781103, mae: 0.878444, mean_q: 4.938726
 76598/100000: episode: 1421, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 33.201, mean reward: 3.018 [2.582, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.430], loss: 0.479119, mae: 0.611149, mean_q: 4.808582
 76612/100000: episode: 1422, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 48.934, mean reward: 3.495 [2.967, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.349, 10.491], loss: 12.218111, mae: 0.852205, mean_q: 5.120975
 76628/100000: episode: 1423, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 51.949, mean reward: 3.247 [2.532, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.441, 10.579], loss: 0.360556, mae: 0.628749, mean_q: 5.362419
 76645/100000: episode: 1424, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 53.366, mean reward: 3.139 [2.323, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.405], loss: 0.532203, mae: 0.599838, mean_q: 5.001288
 76661/100000: episode: 1425, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 57.310, mean reward: 3.582 [2.740, 4.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.740, 10.480], loss: 11.064556, mae: 0.794138, mean_q: 5.010801
 76677/100000: episode: 1426, duration: 0.077s, episode steps: 16, steps per second: 206, episode reward: 72.808, mean reward: 4.550 [3.104, 6.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.772, 10.512], loss: 0.356083, mae: 0.584531, mean_q: 5.224364
 76693/100000: episode: 1427, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 53.315, mean reward: 3.332 [2.460, 5.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.126, 10.397], loss: 0.600689, mae: 0.649006, mean_q: 5.301849
 76710/100000: episode: 1428, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 51.780, mean reward: 3.046 [1.923, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.456, 10.371], loss: 10.222516, mae: 0.797321, mean_q: 5.094819
 76726/100000: episode: 1429, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 39.061, mean reward: 2.441 [2.202, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.365], loss: 10.812479, mae: 0.995340, mean_q: 5.510317
 76743/100000: episode: 1430, duration: 0.107s, episode steps: 17, steps per second: 160, episode reward: 63.195, mean reward: 3.717 [2.977, 5.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.522, 10.412], loss: 0.398746, mae: 0.630152, mean_q: 5.144442
 76759/100000: episode: 1431, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 56.131, mean reward: 3.508 [2.930, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.358, 10.449], loss: 0.627580, mae: 0.601266, mean_q: 5.146931
 76775/100000: episode: 1432, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 53.680, mean reward: 3.355 [2.673, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.509], loss: 961.706909, mae: 2.587130, mean_q: 5.300976
 76789/100000: episode: 1433, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 39.317, mean reward: 2.808 [2.259, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.121, 10.409], loss: 2.708966, mae: 1.935755, mean_q: 6.933328
 76803/100000: episode: 1434, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 40.773, mean reward: 2.912 [2.069, 5.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.366], loss: 1097.934204, mae: 3.285397, mean_q: 5.631094
 76817/100000: episode: 1435, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 66.957, mean reward: 4.783 [3.494, 6.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.468, 10.486], loss: 2.897426, mae: 1.839803, mean_q: 6.880031
 76834/100000: episode: 1436, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 46.866, mean reward: 2.757 [1.965, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.261, 10.374], loss: 0.533516, mae: 0.768460, mean_q: 5.009212
 76848/100000: episode: 1437, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 47.190, mean reward: 3.371 [2.380, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.324], loss: 0.484407, mae: 0.638668, mean_q: 5.250254
 76856/100000: episode: 1438, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 65.945, mean reward: 8.243 [4.497, 22.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.558], loss: 0.345203, mae: 0.574551, mean_q: 5.181632
 76873/100000: episode: 1439, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 73.362, mean reward: 4.315 [2.297, 8.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.590], loss: 0.412463, mae: 0.543740, mean_q: 5.298057
 76887/100000: episode: 1440, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 56.105, mean reward: 4.008 [3.100, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.887, 10.409], loss: 0.420532, mae: 0.582576, mean_q: 5.147179
 76898/100000: episode: 1441, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 46.492, mean reward: 4.227 [3.395, 5.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.600], loss: 0.975674, mae: 0.653085, mean_q: 5.271920
 76914/100000: episode: 1442, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 43.761, mean reward: 2.735 [2.049, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.538], loss: 0.695772, mae: 0.644786, mean_q: 5.257833
 76931/100000: episode: 1443, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 73.831, mean reward: 4.343 [2.903, 6.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.835, 10.598], loss: 0.335008, mae: 0.555059, mean_q: 5.090631
 76942/100000: episode: 1444, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 37.979, mean reward: 3.453 [2.617, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.867, 10.442], loss: 15.391461, mae: 0.939101, mean_q: 5.245115
 76956/100000: episode: 1445, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 40.183, mean reward: 2.870 [2.192, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.421], loss: 0.546138, mae: 0.698705, mean_q: 5.538390
 76967/100000: episode: 1446, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 70.080, mean reward: 6.371 [3.926, 11.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.573], loss: 0.591260, mae: 0.648825, mean_q: 5.301072
 76984/100000: episode: 1447, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 57.861, mean reward: 3.404 [2.744, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.903, 10.512], loss: 0.271793, mae: 0.510033, mean_q: 5.325981
 77000/100000: episode: 1448, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 59.745, mean reward: 3.734 [2.435, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.441], loss: 0.305231, mae: 0.525832, mean_q: 5.096230
 77011/100000: episode: 1449, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 49.225, mean reward: 4.475 [3.522, 5.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.423, 10.588], loss: 0.483678, mae: 0.571952, mean_q: 5.292456
[Info] Complete ISplit Iteration
[Info] Levels: [5.4796667, 6.8169613, 8.23692]
[Info] Cond. Prob: [0.1, 0.1, 0.23]
[Info] Error Prob: 0.0023000000000000004

 77019/100000: episode: 1450, duration: 4.330s, episode steps: 8, steps per second: 2, episode reward: 56.260, mean reward: 7.032 [5.525, 8.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.470], loss: 0.419474, mae: 0.609697, mean_q: 5.232809
 77119/100000: episode: 1451, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 200.400, mean reward: 2.004 [1.455, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.819, 10.267], loss: 154.556671, mae: 1.268301, mean_q: 5.549227
 77219/100000: episode: 1452, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.563, mean reward: 1.946 [1.464, 6.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.889, 10.210], loss: 155.033386, mae: 1.324422, mean_q: 5.665485
 77319/100000: episode: 1453, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.884, mean reward: 1.939 [1.488, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.220, 10.098], loss: 0.610191, mae: 0.654337, mean_q: 5.356269
 77419/100000: episode: 1454, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 208.254, mean reward: 2.083 [1.440, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.742, 10.098], loss: 0.567546, mae: 0.631761, mean_q: 5.298269
 77519/100000: episode: 1455, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 200.490, mean reward: 2.005 [1.467, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.388, 10.120], loss: 2.204647, mae: 0.688836, mean_q: 5.284176
 77619/100000: episode: 1456, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 193.207, mean reward: 1.932 [1.436, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.147], loss: 154.946457, mae: 1.330668, mean_q: 5.640254
 77719/100000: episode: 1457, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 184.222, mean reward: 1.842 [1.466, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.694, 10.098], loss: 5.464581, mae: 0.741429, mean_q: 5.355671
 77819/100000: episode: 1458, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 186.095, mean reward: 1.861 [1.455, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.865, 10.103], loss: 0.572064, mae: 0.619547, mean_q: 5.225743
 77919/100000: episode: 1459, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.529, mean reward: 2.015 [1.532, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.965, 10.419], loss: 156.414108, mae: 1.271258, mean_q: 5.546558
 78019/100000: episode: 1460, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 180.704, mean reward: 1.807 [1.451, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.302, 10.098], loss: 0.596664, mae: 0.642626, mean_q: 5.341372
 78119/100000: episode: 1461, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.958, mean reward: 1.930 [1.445, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.782, 10.098], loss: 309.109436, mae: 1.928219, mean_q: 6.047849
 78219/100000: episode: 1462, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 183.048, mean reward: 1.830 [1.453, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.864, 10.098], loss: 154.714645, mae: 1.196109, mean_q: 5.662730
 78319/100000: episode: 1463, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.111, mean reward: 1.801 [1.452, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.098], loss: 157.908112, mae: 1.261173, mean_q: 5.596354
 78419/100000: episode: 1464, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.147, mean reward: 1.871 [1.456, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.516, 10.284], loss: 0.900162, mae: 0.830575, mean_q: 5.613905
 78519/100000: episode: 1465, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 198.328, mean reward: 1.983 [1.446, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.950, 10.098], loss: 0.581332, mae: 0.645372, mean_q: 5.359008
 78619/100000: episode: 1466, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.423, mean reward: 1.894 [1.492, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.516, 10.098], loss: 156.381927, mae: 1.221845, mean_q: 5.552186
 78719/100000: episode: 1467, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.808, mean reward: 1.938 [1.471, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.312, 10.098], loss: 159.467239, mae: 1.334535, mean_q: 5.634994
 78819/100000: episode: 1468, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 198.442, mean reward: 1.984 [1.473, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.703, 10.098], loss: 0.601442, mae: 0.644664, mean_q: 5.260759
 78919/100000: episode: 1469, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.521, mean reward: 1.815 [1.459, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.392, 10.098], loss: 154.946762, mae: 1.338363, mean_q: 5.602794
 79019/100000: episode: 1470, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.874, mean reward: 1.999 [1.483, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.465, 10.317], loss: 311.261444, mae: 1.666219, mean_q: 5.708821
 79119/100000: episode: 1471, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 206.607, mean reward: 2.066 [1.526, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.620, 10.098], loss: 461.375092, mae: 2.221519, mean_q: 6.260705
 79219/100000: episode: 1472, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.232, mean reward: 2.012 [1.450, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.468, 10.137], loss: 309.363098, mae: 1.920643, mean_q: 6.517328
 79319/100000: episode: 1473, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.274, mean reward: 1.893 [1.464, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.257], loss: 0.566786, mae: 0.690507, mean_q: 5.549201
 79419/100000: episode: 1474, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.864, mean reward: 1.859 [1.465, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.639, 10.098], loss: 154.036591, mae: 1.071205, mean_q: 5.545554
 79519/100000: episode: 1475, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.935, mean reward: 1.909 [1.498, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.552, 10.098], loss: 155.948334, mae: 1.270435, mean_q: 5.732485
 79619/100000: episode: 1476, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 188.968, mean reward: 1.890 [1.445, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.483, 10.148], loss: 461.299500, mae: 2.211937, mean_q: 6.221976
 79719/100000: episode: 1477, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 216.412, mean reward: 2.164 [1.466, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.190, 10.098], loss: 308.968384, mae: 1.954954, mean_q: 6.481583
 79819/100000: episode: 1478, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 192.998, mean reward: 1.930 [1.449, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.621, 10.206], loss: 154.676071, mae: 1.339492, mean_q: 5.795107
 79919/100000: episode: 1479, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.667, mean reward: 1.797 [1.448, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.180, 10.098], loss: 155.921371, mae: 1.286293, mean_q: 5.622245
 80019/100000: episode: 1480, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.560, mean reward: 1.986 [1.443, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.766, 10.098], loss: 308.683960, mae: 1.684753, mean_q: 5.683494
 80119/100000: episode: 1481, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 182.361, mean reward: 1.824 [1.450, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.119], loss: 0.483071, mae: 0.625014, mean_q: 5.196303
 80219/100000: episode: 1482, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 196.535, mean reward: 1.965 [1.446, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.098], loss: 308.243195, mae: 2.009942, mean_q: 5.989787
 80319/100000: episode: 1483, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.160, mean reward: 1.902 [1.450, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.310], loss: 1068.902466, mae: 3.883510, mean_q: 6.751070
 80419/100000: episode: 1484, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 182.304, mean reward: 1.823 [1.479, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.400, 10.166], loss: 1.667218, mae: 1.054648, mean_q: 5.953242
 80519/100000: episode: 1485, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 243.914, mean reward: 2.439 [1.474, 6.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.453, 10.375], loss: 307.057678, mae: 1.650282, mean_q: 5.794528
 80619/100000: episode: 1486, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.925, mean reward: 1.889 [1.483, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.687, 10.307], loss: 154.202118, mae: 1.322492, mean_q: 5.726620
 80719/100000: episode: 1487, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.834, mean reward: 1.818 [1.448, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.327, 10.197], loss: 0.527835, mae: 0.641743, mean_q: 5.082314
 80819/100000: episode: 1488, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 208.575, mean reward: 2.086 [1.503, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.011, 10.319], loss: 306.382721, mae: 1.509725, mean_q: 5.437894
 80919/100000: episode: 1489, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.323, mean reward: 1.993 [1.461, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.530, 10.207], loss: 0.432473, mae: 0.555159, mean_q: 4.725502
 81019/100000: episode: 1490, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 183.911, mean reward: 1.839 [1.435, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.145, 10.163], loss: 306.745911, mae: 1.630266, mean_q: 5.281311
 81119/100000: episode: 1491, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.239, mean reward: 1.822 [1.459, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.691, 10.098], loss: 153.894440, mae: 1.095957, mean_q: 5.002749
 81219/100000: episode: 1492, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.759, mean reward: 1.938 [1.459, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.526, 10.258], loss: 0.316303, mae: 0.478934, mean_q: 4.533490
 81319/100000: episode: 1493, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.606, mean reward: 1.976 [1.449, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.738, 10.358], loss: 154.030777, mae: 1.119289, mean_q: 4.773305
 81419/100000: episode: 1494, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 186.145, mean reward: 1.861 [1.454, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.099, 10.136], loss: 0.274895, mae: 0.470612, mean_q: 4.359263
 81519/100000: episode: 1495, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.690, mean reward: 1.807 [1.444, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.269, 10.147], loss: 0.211511, mae: 0.427247, mean_q: 4.185781
 81619/100000: episode: 1496, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 207.782, mean reward: 2.078 [1.442, 5.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.295], loss: 0.177857, mae: 0.406572, mean_q: 4.109567
 81719/100000: episode: 1497, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.805, mean reward: 1.948 [1.449, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.018, 10.098], loss: 0.163208, mae: 0.382540, mean_q: 4.023082
 81819/100000: episode: 1498, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.009, mean reward: 1.960 [1.477, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.593, 10.098], loss: 0.156391, mae: 0.373840, mean_q: 3.956704
 81919/100000: episode: 1499, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.633, mean reward: 1.956 [1.474, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.284, 10.374], loss: 0.140204, mae: 0.360870, mean_q: 3.900068
 82019/100000: episode: 1500, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.105, mean reward: 1.901 [1.474, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.242, 10.206], loss: 0.128459, mae: 0.351782, mean_q: 3.832252
 82119/100000: episode: 1501, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 204.451, mean reward: 2.045 [1.481, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.098], loss: 0.118091, mae: 0.336068, mean_q: 3.805194
 82219/100000: episode: 1502, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 210.052, mean reward: 2.101 [1.451, 5.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.393], loss: 0.115852, mae: 0.343056, mean_q: 3.823577
 82319/100000: episode: 1503, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 198.731, mean reward: 1.987 [1.521, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.411, 10.098], loss: 0.119561, mae: 0.339828, mean_q: 3.815716
 82419/100000: episode: 1504, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.432, mean reward: 1.924 [1.451, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.710, 10.098], loss: 0.113292, mae: 0.333006, mean_q: 3.821803
 82519/100000: episode: 1505, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.288, mean reward: 1.983 [1.460, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.901, 10.098], loss: 0.114224, mae: 0.334938, mean_q: 3.838832
 82619/100000: episode: 1506, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.197, mean reward: 1.942 [1.465, 5.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.588, 10.159], loss: 0.105384, mae: 0.330271, mean_q: 3.824111
 82719/100000: episode: 1507, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 179.262, mean reward: 1.793 [1.479, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.580, 10.098], loss: 0.106703, mae: 0.319889, mean_q: 3.829221
 82819/100000: episode: 1508, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.048, mean reward: 2.030 [1.473, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.548, 10.365], loss: 0.101955, mae: 0.324500, mean_q: 3.817648
 82919/100000: episode: 1509, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.482, mean reward: 1.915 [1.448, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.098], loss: 0.096857, mae: 0.314851, mean_q: 3.824975
 83019/100000: episode: 1510, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.263, mean reward: 1.863 [1.470, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.383], loss: 0.109570, mae: 0.329262, mean_q: 3.852511
 83119/100000: episode: 1511, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 179.712, mean reward: 1.797 [1.442, 2.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.098], loss: 0.101917, mae: 0.312714, mean_q: 3.849259
 83219/100000: episode: 1512, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 212.017, mean reward: 2.120 [1.517, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.731, 10.098], loss: 0.105788, mae: 0.326646, mean_q: 3.833594
 83319/100000: episode: 1513, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 192.344, mean reward: 1.923 [1.454, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.636, 10.149], loss: 0.095070, mae: 0.310844, mean_q: 3.837440
 83419/100000: episode: 1514, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 176.513, mean reward: 1.765 [1.452, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.521, 10.256], loss: 0.103575, mae: 0.320114, mean_q: 3.833424
 83519/100000: episode: 1515, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 210.229, mean reward: 2.102 [1.492, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.951, 10.098], loss: 0.095798, mae: 0.309435, mean_q: 3.829586
 83619/100000: episode: 1516, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 189.180, mean reward: 1.892 [1.451, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.128, 10.173], loss: 0.100010, mae: 0.312209, mean_q: 3.858042
 83719/100000: episode: 1517, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 185.166, mean reward: 1.852 [1.443, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.977, 10.143], loss: 0.100106, mae: 0.311928, mean_q: 3.834747
 83819/100000: episode: 1518, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.123, mean reward: 1.961 [1.464, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.903, 10.203], loss: 0.099658, mae: 0.313004, mean_q: 3.846956
 83919/100000: episode: 1519, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.143, mean reward: 1.931 [1.495, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.687, 10.098], loss: 0.093908, mae: 0.310106, mean_q: 3.832709
 84019/100000: episode: 1520, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 200.450, mean reward: 2.005 [1.436, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.164], loss: 0.096822, mae: 0.313612, mean_q: 3.833122
 84119/100000: episode: 1521, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 179.983, mean reward: 1.800 [1.447, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.964, 10.098], loss: 0.093475, mae: 0.307895, mean_q: 3.853962
 84219/100000: episode: 1522, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.039, mean reward: 1.890 [1.483, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.280], loss: 0.088226, mae: 0.307670, mean_q: 3.824327
 84319/100000: episode: 1523, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 193.732, mean reward: 1.937 [1.467, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.514, 10.098], loss: 0.084360, mae: 0.299181, mean_q: 3.840568
 84419/100000: episode: 1524, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.677, mean reward: 1.987 [1.514, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.649, 10.189], loss: 0.093296, mae: 0.307066, mean_q: 3.845037
 84519/100000: episode: 1525, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 212.289, mean reward: 2.123 [1.484, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.292], loss: 0.092697, mae: 0.307639, mean_q: 3.846905
 84619/100000: episode: 1526, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 181.537, mean reward: 1.815 [1.440, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.098], loss: 0.093691, mae: 0.302350, mean_q: 3.852498
 84719/100000: episode: 1527, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.468, mean reward: 1.845 [1.432, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.978, 10.334], loss: 0.093819, mae: 0.308179, mean_q: 3.844795
 84819/100000: episode: 1528, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 214.593, mean reward: 2.146 [1.447, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.109, 10.487], loss: 0.091232, mae: 0.299758, mean_q: 3.831939
 84919/100000: episode: 1529, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 198.989, mean reward: 1.990 [1.478, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.152, 10.098], loss: 0.095136, mae: 0.305028, mean_q: 3.841010
 85019/100000: episode: 1530, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 195.371, mean reward: 1.954 [1.443, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.786, 10.381], loss: 0.094556, mae: 0.307160, mean_q: 3.849661
 85119/100000: episode: 1531, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.064, mean reward: 1.871 [1.475, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.765, 10.098], loss: 0.090174, mae: 0.305660, mean_q: 3.850134
 85219/100000: episode: 1532, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 200.287, mean reward: 2.003 [1.479, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.039, 10.246], loss: 0.089706, mae: 0.303573, mean_q: 3.852752
 85319/100000: episode: 1533, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 202.426, mean reward: 2.024 [1.478, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.146, 10.154], loss: 0.094958, mae: 0.309572, mean_q: 3.869197
 85419/100000: episode: 1534, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.399, mean reward: 1.824 [1.477, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.906, 10.222], loss: 0.086489, mae: 0.303021, mean_q: 3.850534
 85519/100000: episode: 1535, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 202.911, mean reward: 2.029 [1.443, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.361, 10.098], loss: 0.083206, mae: 0.297601, mean_q: 3.829130
 85619/100000: episode: 1536, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 194.071, mean reward: 1.941 [1.458, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.552, 10.157], loss: 0.092256, mae: 0.304381, mean_q: 3.840453
 85719/100000: episode: 1537, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 178.685, mean reward: 1.787 [1.466, 2.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.329, 10.155], loss: 0.098944, mae: 0.309759, mean_q: 3.845591
 85819/100000: episode: 1538, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 198.140, mean reward: 1.981 [1.460, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.141], loss: 0.084305, mae: 0.297560, mean_q: 3.819241
 85919/100000: episode: 1539, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.047, mean reward: 1.950 [1.470, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.548, 10.237], loss: 0.090626, mae: 0.310322, mean_q: 3.834800
 86019/100000: episode: 1540, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.595, mean reward: 2.016 [1.465, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.283, 10.098], loss: 0.086731, mae: 0.298591, mean_q: 3.846242
 86119/100000: episode: 1541, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.912, mean reward: 1.859 [1.450, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.433, 10.170], loss: 0.084728, mae: 0.305209, mean_q: 3.851253
 86219/100000: episode: 1542, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 183.753, mean reward: 1.838 [1.455, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.641, 10.098], loss: 0.087355, mae: 0.296340, mean_q: 3.839888
 86319/100000: episode: 1543, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 190.781, mean reward: 1.908 [1.434, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.715, 10.098], loss: 0.097046, mae: 0.310195, mean_q: 3.863427
 86419/100000: episode: 1544, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 201.216, mean reward: 2.012 [1.449, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.129, 10.232], loss: 0.096394, mae: 0.307918, mean_q: 3.858866
 86519/100000: episode: 1545, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 179.674, mean reward: 1.797 [1.449, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.684, 10.164], loss: 0.094430, mae: 0.310143, mean_q: 3.848346
 86619/100000: episode: 1546, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 187.003, mean reward: 1.870 [1.465, 2.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.602, 10.330], loss: 0.090729, mae: 0.300936, mean_q: 3.826465
 86719/100000: episode: 1547, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.069, mean reward: 1.951 [1.455, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.548, 10.488], loss: 0.084732, mae: 0.296451, mean_q: 3.837908
 86819/100000: episode: 1548, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.155, mean reward: 1.902 [1.455, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.760, 10.098], loss: 0.083380, mae: 0.292110, mean_q: 3.832246
 86919/100000: episode: 1549, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 214.209, mean reward: 2.142 [1.503, 4.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.415, 10.098], loss: 0.089687, mae: 0.303092, mean_q: 3.829136
[Info] 1-TH LEVEL FOUND: 5.021025657653809, Considering 10/90 traces
 87019/100000: episode: 1550, duration: 4.673s, episode steps: 100, steps per second: 21, episode reward: 199.738, mean reward: 1.997 [1.439, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.224, 10.187], loss: 0.086044, mae: 0.297707, mean_q: 3.842098
 87039/100000: episode: 1551, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 51.926, mean reward: 2.596 [2.008, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.236, 10.415], loss: 0.077835, mae: 0.282896, mean_q: 3.831483
 87071/100000: episode: 1552, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 101.004, mean reward: 3.156 [2.391, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.953, 10.462], loss: 0.078305, mae: 0.285578, mean_q: 3.835783
 87091/100000: episode: 1553, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 40.013, mean reward: 2.001 [1.493, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.166, 10.166], loss: 0.092249, mae: 0.309247, mean_q: 3.884227
 87101/100000: episode: 1554, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 18.593, mean reward: 1.859 [1.568, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.339], loss: 0.105666, mae: 0.295628, mean_q: 3.804815
 87102/100000: episode: 1555, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 3.583, mean reward: 3.583 [3.583, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.552, 10.100], loss: 0.132606, mae: 0.426146, mean_q: 4.088269
 87128/100000: episode: 1556, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 62.093, mean reward: 2.388 [1.845, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.461], loss: 0.095608, mae: 0.307563, mean_q: 3.848949
 87155/100000: episode: 1557, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 63.569, mean reward: 2.354 [2.037, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.143, 10.396], loss: 0.095775, mae: 0.308829, mean_q: 3.873972
 87173/100000: episode: 1558, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 49.214, mean reward: 2.734 [2.071, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.417], loss: 0.069384, mae: 0.277085, mean_q: 3.834336
 87191/100000: episode: 1559, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 41.459, mean reward: 2.303 [1.782, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.344], loss: 0.096939, mae: 0.312007, mean_q: 3.860399
 87192/100000: episode: 1560, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 4.191, mean reward: 4.191 [4.191, 4.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.527, 10.100], loss: 0.094293, mae: 0.346797, mean_q: 3.652814
 87212/100000: episode: 1561, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 50.395, mean reward: 2.520 [1.594, 5.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.746, 10.477], loss: 0.111936, mae: 0.329602, mean_q: 3.862985
 87229/100000: episode: 1562, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 38.866, mean reward: 2.286 [1.790, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.297], loss: 0.096084, mae: 0.316387, mean_q: 3.874963
 87261/100000: episode: 1563, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 176.226, mean reward: 5.507 [2.437, 12.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.625, 10.556], loss: 0.122222, mae: 0.317646, mean_q: 3.892553
 87293/100000: episode: 1564, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 77.811, mean reward: 2.432 [1.921, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.667, 10.313], loss: 0.105083, mae: 0.322825, mean_q: 3.890561
 87303/100000: episode: 1565, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 19.145, mean reward: 1.915 [1.459, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.083, 10.147], loss: 0.080753, mae: 0.291487, mean_q: 3.901202
 87323/100000: episode: 1566, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 44.026, mean reward: 2.201 [1.898, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.814, 10.332], loss: 0.090195, mae: 0.304954, mean_q: 3.871944
 87340/100000: episode: 1567, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 32.467, mean reward: 1.910 [1.594, 2.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.230], loss: 0.105822, mae: 0.318194, mean_q: 3.881586
 87360/100000: episode: 1568, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 51.289, mean reward: 2.564 [2.094, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.342], loss: 0.103014, mae: 0.318319, mean_q: 3.934770
 87370/100000: episode: 1569, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 19.534, mean reward: 1.953 [1.614, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.071, 10.293], loss: 0.088384, mae: 0.319154, mean_q: 3.850649
 87387/100000: episode: 1570, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 30.015, mean reward: 1.766 [1.482, 2.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.230, 10.143], loss: 0.126777, mae: 0.317913, mean_q: 3.898443
 87407/100000: episode: 1571, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 47.704, mean reward: 2.385 [2.043, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.259, 10.422], loss: 0.106165, mae: 0.316284, mean_q: 3.935084
 87425/100000: episode: 1572, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 44.008, mean reward: 2.445 [1.955, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.383], loss: 0.131321, mae: 0.322287, mean_q: 3.912186
 87445/100000: episode: 1573, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 44.020, mean reward: 2.201 [1.944, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.722, 10.323], loss: 0.103276, mae: 0.307968, mean_q: 3.899059
 87472/100000: episode: 1574, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 59.468, mean reward: 2.203 [1.472, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.159], loss: 0.098217, mae: 0.314113, mean_q: 3.834654
 87473/100000: episode: 1575, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 3.907, mean reward: 3.907 [3.907, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.540, 10.100], loss: 0.110514, mae: 0.335836, mean_q: 3.901617
 87505/100000: episode: 1576, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 118.220, mean reward: 3.694 [2.643, 5.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.612, 10.467], loss: 0.104780, mae: 0.317739, mean_q: 3.932861
 87523/100000: episode: 1577, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 35.472, mean reward: 1.971 [1.629, 2.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.094, 10.195], loss: 0.107543, mae: 0.312962, mean_q: 3.941769
 87546/100000: episode: 1578, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 99.561, mean reward: 4.329 [1.941, 12.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.290, 10.600], loss: 0.114133, mae: 0.343223, mean_q: 3.981082
 87564/100000: episode: 1579, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 35.832, mean reward: 1.991 [1.681, 2.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.535, 10.267], loss: 0.175745, mae: 0.345137, mean_q: 3.969908
 87565/100000: episode: 1580, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 4.765, mean reward: 4.765 [4.765, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.587, 10.100], loss: 0.115284, mae: 0.329931, mean_q: 4.161471
 87566/100000: episode: 1581, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 3.985, mean reward: 3.985 [3.985, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.606, 10.100], loss: 0.151121, mae: 0.444500, mean_q: 4.215726
 87592/100000: episode: 1582, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 61.114, mean reward: 2.351 [1.825, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.204, 10.543], loss: 0.136704, mae: 0.353444, mean_q: 3.951484
 87624/100000: episode: 1583, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 85.359, mean reward: 2.667 [1.606, 6.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.282, 10.245], loss: 0.124195, mae: 0.334760, mean_q: 3.984044
 87644/100000: episode: 1584, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 35.729, mean reward: 1.786 [1.478, 2.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.158], loss: 0.148897, mae: 0.334329, mean_q: 3.949016
 87664/100000: episode: 1585, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 38.410, mean reward: 1.921 [1.587, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.057, 10.355], loss: 0.119418, mae: 0.330664, mean_q: 3.985141
 87681/100000: episode: 1586, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 30.293, mean reward: 1.782 [1.496, 2.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.137, 10.142], loss: 0.170449, mae: 0.356979, mean_q: 3.995048
 87701/100000: episode: 1587, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 46.114, mean reward: 2.306 [1.809, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.388, 10.318], loss: 0.155627, mae: 0.368348, mean_q: 3.955781
 87721/100000: episode: 1588, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 50.371, mean reward: 2.519 [1.604, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.137, 10.173], loss: 0.121203, mae: 0.360158, mean_q: 4.023119
 87738/100000: episode: 1589, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 36.015, mean reward: 2.119 [1.863, 2.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.350], loss: 0.176016, mae: 0.357760, mean_q: 3.985084
 87765/100000: episode: 1590, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 103.986, mean reward: 3.851 [2.309, 6.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.642, 10.579], loss: 0.146688, mae: 0.352992, mean_q: 3.969635
 87785/100000: episode: 1591, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 35.222, mean reward: 1.761 [1.489, 2.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.071, 10.155], loss: 0.124737, mae: 0.346914, mean_q: 3.990474
 87805/100000: episode: 1592, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 40.819, mean reward: 2.041 [1.601, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.084, 10.208], loss: 0.095114, mae: 0.315275, mean_q: 3.967349
 87825/100000: episode: 1593, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 49.323, mean reward: 2.466 [1.592, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.930, 10.440], loss: 0.117361, mae: 0.338800, mean_q: 3.980084
 87845/100000: episode: 1594, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 48.001, mean reward: 2.400 [1.710, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.452], loss: 0.120902, mae: 0.333742, mean_q: 3.978579
 87877/100000: episode: 1595, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 85.119, mean reward: 2.660 [1.742, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.121, 10.276], loss: 0.174665, mae: 0.363765, mean_q: 4.031377
 87894/100000: episode: 1596, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 34.622, mean reward: 2.037 [1.702, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.637, 10.280], loss: 0.132945, mae: 0.352102, mean_q: 4.031365
 87904/100000: episode: 1597, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 20.696, mean reward: 2.070 [1.780, 2.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.259], loss: 0.253677, mae: 0.416995, mean_q: 4.018988
 87931/100000: episode: 1598, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 81.332, mean reward: 3.012 [2.373, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.732, 10.439], loss: 0.141625, mae: 0.355635, mean_q: 3.964749
 87958/100000: episode: 1599, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 66.941, mean reward: 2.479 [2.047, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.256, 10.368], loss: 0.148594, mae: 0.388906, mean_q: 4.062165
 87981/100000: episode: 1600, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 47.212, mean reward: 2.053 [1.642, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.086, 10.261], loss: 0.117985, mae: 0.333785, mean_q: 4.102372
 88008/100000: episode: 1601, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 60.655, mean reward: 2.246 [1.871, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.551, 10.317], loss: 0.141379, mae: 0.354701, mean_q: 4.008732
 88040/100000: episode: 1602, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 100.916, mean reward: 3.154 [1.834, 5.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.255], loss: 0.109389, mae: 0.331507, mean_q: 4.051485
 88058/100000: episode: 1603, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 41.052, mean reward: 2.281 [1.892, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.281], loss: 0.127061, mae: 0.343753, mean_q: 4.091741
 88085/100000: episode: 1604, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 75.380, mean reward: 2.792 [2.070, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.412, 10.346], loss: 0.125071, mae: 0.344197, mean_q: 4.077936
 88103/100000: episode: 1605, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 37.310, mean reward: 2.073 [1.667, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.685, 10.259], loss: 0.133662, mae: 0.349022, mean_q: 4.037809
 88135/100000: episode: 1606, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 115.272, mean reward: 3.602 [2.397, 6.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.038, 10.481], loss: 0.160564, mae: 0.353248, mean_q: 4.075852
 88155/100000: episode: 1607, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 43.552, mean reward: 2.178 [1.478, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.117, 10.110], loss: 0.102919, mae: 0.328927, mean_q: 4.017432
 88178/100000: episode: 1608, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 47.383, mean reward: 2.060 [1.514, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.243, 10.252], loss: 0.161286, mae: 0.357298, mean_q: 4.113304
 88196/100000: episode: 1609, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 36.837, mean reward: 2.046 [1.616, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.181], loss: 0.181841, mae: 0.389217, mean_q: 4.081100
 88223/100000: episode: 1610, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 78.652, mean reward: 2.913 [2.245, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.036, 10.436], loss: 0.159670, mae: 0.379030, mean_q: 4.101202
 88224/100000: episode: 1611, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 4.081, mean reward: 4.081 [4.081, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.597, 10.100], loss: 0.192683, mae: 0.404360, mean_q: 4.202713
 88251/100000: episode: 1612, duration: 0.150s, episode steps: 27, steps per second: 179, episode reward: 58.901, mean reward: 2.182 [1.457, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.534, 10.100], loss: 0.148732, mae: 0.366051, mean_q: 4.121578
 88261/100000: episode: 1613, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 21.053, mean reward: 2.105 [1.613, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.350], loss: 0.216967, mae: 0.384227, mean_q: 4.211692
 88278/100000: episode: 1614, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 35.128, mean reward: 2.066 [1.743, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.329], loss: 0.193694, mae: 0.410145, mean_q: 4.054905
 88296/100000: episode: 1615, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 38.920, mean reward: 2.162 [1.478, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.158, 10.104], loss: 0.190548, mae: 0.424497, mean_q: 4.205145
 88313/100000: episode: 1616, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 39.777, mean reward: 2.340 [1.666, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.634, 10.334], loss: 0.178000, mae: 0.398461, mean_q: 4.090391
 88314/100000: episode: 1617, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 3.430, mean reward: 3.430 [3.430, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.581, 10.100], loss: 0.092870, mae: 0.305227, mean_q: 4.142254
 88340/100000: episode: 1618, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 64.960, mean reward: 2.498 [1.748, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.630, 10.226], loss: 0.144491, mae: 0.353776, mean_q: 4.070169
 88360/100000: episode: 1619, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 48.356, mean reward: 2.418 [1.890, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.653, 10.281], loss: 0.129359, mae: 0.363063, mean_q: 4.061674
 88380/100000: episode: 1620, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 37.918, mean reward: 1.896 [1.561, 2.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.137, 10.223], loss: 0.198776, mae: 0.429434, mean_q: 4.158340
 88403/100000: episode: 1621, duration: 0.140s, episode steps: 23, steps per second: 165, episode reward: 50.514, mean reward: 2.196 [1.713, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.249], loss: 0.117282, mae: 0.332209, mean_q: 4.092587
 88420/100000: episode: 1622, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 30.169, mean reward: 1.775 [1.556, 2.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.338, 10.145], loss: 0.123305, mae: 0.344575, mean_q: 4.172517
 88447/100000: episode: 1623, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 55.859, mean reward: 2.069 [1.691, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.260], loss: 0.170248, mae: 0.371036, mean_q: 4.126691
 88467/100000: episode: 1624, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 43.021, mean reward: 2.151 [1.696, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.168, 10.234], loss: 0.161647, mae: 0.379870, mean_q: 4.176850
 88493/100000: episode: 1625, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 51.924, mean reward: 1.997 [1.717, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.119, 10.329], loss: 0.156957, mae: 0.392675, mean_q: 4.114383
 88513/100000: episode: 1626, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 34.192, mean reward: 1.710 [1.453, 2.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.058, 10.100], loss: 0.162691, mae: 0.396504, mean_q: 4.141884
 88533/100000: episode: 1627, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 64.272, mean reward: 3.214 [2.365, 4.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.513], loss: 0.182105, mae: 0.383189, mean_q: 4.148488
 88565/100000: episode: 1628, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 93.381, mean reward: 2.918 [2.171, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.582, 10.436], loss: 0.202204, mae: 0.411935, mean_q: 4.157239
 88588/100000: episode: 1629, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 63.357, mean reward: 2.755 [2.040, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.947, 10.333], loss: 0.138121, mae: 0.372448, mean_q: 4.115284
 88608/100000: episode: 1630, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 40.418, mean reward: 2.021 [1.708, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.251], loss: 0.202608, mae: 0.371114, mean_q: 4.159494
 88628/100000: episode: 1631, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 40.234, mean reward: 2.012 [1.558, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.786, 10.241], loss: 0.156560, mae: 0.391701, mean_q: 4.094211
 88651/100000: episode: 1632, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 62.735, mean reward: 2.728 [1.923, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.190, 10.357], loss: 0.151911, mae: 0.373294, mean_q: 4.157764
 88652/100000: episode: 1633, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 3.972, mean reward: 3.972 [3.972, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.562, 10.100], loss: 0.185453, mae: 0.389220, mean_q: 4.451021
 88669/100000: episode: 1634, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 38.541, mean reward: 2.267 [1.756, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.981, 10.272], loss: 0.148078, mae: 0.351045, mean_q: 4.115746
 88696/100000: episode: 1635, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 65.280, mean reward: 2.418 [2.037, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.341], loss: 0.124757, mae: 0.348777, mean_q: 4.123806
 88728/100000: episode: 1636, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 164.235, mean reward: 5.132 [2.943, 11.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.224, 10.478], loss: 0.154717, mae: 0.378809, mean_q: 4.189779
 88751/100000: episode: 1637, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 64.500, mean reward: 2.804 [2.139, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.853, 10.317], loss: 0.150516, mae: 0.363460, mean_q: 4.174939
 88769/100000: episode: 1638, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 34.395, mean reward: 1.911 [1.664, 2.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.182], loss: 0.144569, mae: 0.374641, mean_q: 4.207567
 88770/100000: episode: 1639, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 3.393, mean reward: 3.393 [3.393, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.573, 10.100], loss: 0.074994, mae: 0.337776, mean_q: 4.415593
[Info] 2-TH LEVEL FOUND: 7.271796703338623, Considering 10/90 traces
 88793/100000: episode: 1640, duration: 4.204s, episode steps: 23, steps per second: 5, episode reward: 71.112, mean reward: 3.092 [2.175, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.191, 10.359], loss: 0.150190, mae: 0.361133, mean_q: 4.166219
 88819/100000: episode: 1641, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 129.976, mean reward: 4.999 [3.404, 9.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.529], loss: 0.170273, mae: 0.366746, mean_q: 4.227262
 88834/100000: episode: 1642, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 54.343, mean reward: 3.623 [2.879, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.129, 10.519], loss: 0.173605, mae: 0.432041, mean_q: 4.223927
 88855/100000: episode: 1643, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 58.515, mean reward: 2.786 [2.198, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.362, 10.425], loss: 0.146090, mae: 0.368657, mean_q: 4.206258
 88867/100000: episode: 1644, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 31.828, mean reward: 2.652 [2.029, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.361], loss: 0.202018, mae: 0.379106, mean_q: 4.278782
 88895/100000: episode: 1645, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 111.207, mean reward: 3.972 [2.797, 6.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.615, 10.467], loss: 0.203019, mae: 0.417140, mean_q: 4.236481
 88924/100000: episode: 1646, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 92.298, mean reward: 3.183 [1.579, 5.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.156, 10.210], loss: 0.195981, mae: 0.418925, mean_q: 4.337718
 88945/100000: episode: 1647, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 99.943, mean reward: 4.759 [3.004, 6.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.295, 10.543], loss: 0.217125, mae: 0.438693, mean_q: 4.364944
 88973/100000: episode: 1648, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 78.226, mean reward: 2.794 [2.154, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.859, 10.439], loss: 0.217126, mae: 0.437323, mean_q: 4.339632
 89002/100000: episode: 1649, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 170.749, mean reward: 5.888 [3.146, 11.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.239, 10.445], loss: 0.178532, mae: 0.406085, mean_q: 4.346582
 89028/100000: episode: 1650, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 100.108, mean reward: 3.850 [2.733, 5.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.111, 10.536], loss: 0.238831, mae: 0.416272, mean_q: 4.409896
 89056/100000: episode: 1651, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 120.096, mean reward: 4.289 [2.619, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.999, 10.540], loss: 0.200947, mae: 0.432536, mean_q: 4.319456
 89085/100000: episode: 1652, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 123.561, mean reward: 4.261 [3.081, 6.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.399, 10.583], loss: 0.247602, mae: 0.485717, mean_q: 4.437078
 89111/100000: episode: 1653, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 84.847, mean reward: 3.263 [2.522, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.790, 10.548], loss: 0.221895, mae: 0.463023, mean_q: 4.467067
 89135/100000: episode: 1654, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 155.115, mean reward: 6.463 [2.759, 39.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.828, 10.584], loss: 0.221816, mae: 0.424468, mean_q: 4.468643
 89165/100000: episode: 1655, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 136.015, mean reward: 4.534 [2.980, 9.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.246, 10.521], loss: 0.221282, mae: 0.442173, mean_q: 4.512907
 89193/100000: episode: 1656, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 77.966, mean reward: 2.784 [2.136, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.396, 10.400], loss: 0.190613, mae: 0.417536, mean_q: 4.526467
 89219/100000: episode: 1657, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 105.895, mean reward: 4.073 [3.327, 5.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.190, 10.596], loss: 0.255178, mae: 0.476199, mean_q: 4.593054
 89248/100000: episode: 1658, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 184.306, mean reward: 6.355 [3.552, 11.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.394, 10.512], loss: 0.204879, mae: 0.416500, mean_q: 4.490701
 89274/100000: episode: 1659, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 112.241, mean reward: 4.317 [2.092, 9.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.464, 10.344], loss: 0.358234, mae: 0.531000, mean_q: 4.589487
 89289/100000: episode: 1660, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 39.586, mean reward: 2.639 [1.960, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.309], loss: 0.353258, mae: 0.484906, mean_q: 4.544237
 89301/100000: episode: 1661, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 43.199, mean reward: 3.600 [2.614, 6.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.438, 10.635], loss: 0.290395, mae: 0.443417, mean_q: 4.558039
 89325/100000: episode: 1662, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 125.377, mean reward: 5.224 [3.968, 9.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.568, 10.501], loss: 0.268961, mae: 0.474283, mean_q: 4.609991
 89349/100000: episode: 1663, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 79.424, mean reward: 3.309 [2.095, 5.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.777, 10.364], loss: 0.334825, mae: 0.510309, mean_q: 4.791954
 89364/100000: episode: 1664, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 55.397, mean reward: 3.693 [3.026, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.257, 10.547], loss: 0.251692, mae: 0.469086, mean_q: 4.724399
 89378/100000: episode: 1665, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 59.940, mean reward: 4.281 [2.645, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.240, 10.407], loss: 0.300253, mae: 0.516010, mean_q: 4.623086
 89393/100000: episode: 1666, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 42.671, mean reward: 2.845 [2.378, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.392, 10.467], loss: 1.493767, mae: 0.519108, mean_q: 4.701419
 89408/100000: episode: 1667, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 49.315, mean reward: 3.288 [2.730, 4.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.524, 10.422], loss: 0.300127, mae: 0.552462, mean_q: 4.661840
 89434/100000: episode: 1668, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 193.038, mean reward: 7.425 [3.554, 25.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.615, 10.627], loss: 0.267655, mae: 0.495869, mean_q: 4.749163
 89449/100000: episode: 1669, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 63.479, mean reward: 4.232 [2.311, 15.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.101, 10.452], loss: 0.293608, mae: 0.474279, mean_q: 4.755381
 89470/100000: episode: 1670, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 63.544, mean reward: 3.026 [2.401, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.068, 10.457], loss: 0.283390, mae: 0.489229, mean_q: 4.696285
 89499/100000: episode: 1671, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 87.529, mean reward: 3.018 [1.863, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.337, 10.323], loss: 0.558505, mae: 0.533315, mean_q: 4.849812
 89514/100000: episode: 1672, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 41.310, mean reward: 2.754 [2.238, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.221, 10.404], loss: 0.290956, mae: 0.522078, mean_q: 4.882722
 89529/100000: episode: 1673, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 43.330, mean reward: 2.889 [2.448, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.452], loss: 0.331483, mae: 0.496196, mean_q: 4.811379
 89544/100000: episode: 1674, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 48.511, mean reward: 3.234 [2.570, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.214, 10.481], loss: 0.384377, mae: 0.525649, mean_q: 4.750455
 89558/100000: episode: 1675, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 38.416, mean reward: 2.744 [2.355, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-1.374, 10.469], loss: 0.338662, mae: 0.513656, mean_q: 4.840875
 89587/100000: episode: 1676, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 87.564, mean reward: 3.019 [2.289, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.459, 10.429], loss: 0.290977, mae: 0.491945, mean_q: 4.872046
 89613/100000: episode: 1677, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 108.042, mean reward: 4.155 [2.484, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.367, 10.490], loss: 0.291363, mae: 0.474783, mean_q: 4.720910
 89643/100000: episode: 1678, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 94.059, mean reward: 3.135 [2.346, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.423], loss: 1.197591, mae: 0.628995, mean_q: 4.782836
 89658/100000: episode: 1679, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 51.992, mean reward: 3.466 [2.500, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.468, 10.540], loss: 0.400155, mae: 0.481006, mean_q: 4.872209
 89673/100000: episode: 1680, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 42.894, mean reward: 2.860 [2.322, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.447], loss: 0.296746, mae: 0.491705, mean_q: 4.855579
 89701/100000: episode: 1681, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 94.000, mean reward: 3.357 [2.527, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.335, 10.448], loss: 0.233973, mae: 0.471777, mean_q: 4.856839
 89730/100000: episode: 1682, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 90.190, mean reward: 3.110 [2.688, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.048, 10.433], loss: 1.174986, mae: 0.590527, mean_q: 5.032949
 89742/100000: episode: 1683, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 51.872, mean reward: 4.323 [2.968, 8.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.346, 10.521], loss: 0.340727, mae: 0.535321, mean_q: 4.881588
 89766/100000: episode: 1684, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 84.480, mean reward: 3.520 [2.237, 7.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.117, 10.459], loss: 0.310534, mae: 0.512325, mean_q: 4.855773
 89778/100000: episode: 1685, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 43.492, mean reward: 3.624 [2.732, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.431], loss: 0.898321, mae: 0.591874, mean_q: 5.018093
 89793/100000: episode: 1686, duration: 0.091s, episode steps: 15, steps per second: 166, episode reward: 39.227, mean reward: 2.615 [2.171, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.280, 10.424], loss: 0.306850, mae: 0.550461, mean_q: 4.831836
 89807/100000: episode: 1687, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 55.601, mean reward: 3.971 [3.088, 5.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.498], loss: 0.372306, mae: 0.543672, mean_q: 4.836234
 89822/100000: episode: 1688, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 49.882, mean reward: 3.325 [2.845, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.457], loss: 0.292203, mae: 0.507509, mean_q: 4.896649
 89848/100000: episode: 1689, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 143.324, mean reward: 5.512 [2.993, 11.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.284, 10.563], loss: 0.376895, mae: 0.534925, mean_q: 5.052070
 89874/100000: episode: 1690, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 131.440, mean reward: 5.055 [2.564, 10.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.488], loss: 0.327803, mae: 0.521452, mean_q: 5.052537
 89895/100000: episode: 1691, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 116.311, mean reward: 5.539 [2.812, 13.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.081, 10.599], loss: 0.337179, mae: 0.525980, mean_q: 4.976359
 89923/100000: episode: 1692, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 102.709, mean reward: 3.668 [2.138, 6.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.584], loss: 0.432005, mae: 0.610838, mean_q: 5.225101
 89953/100000: episode: 1693, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 88.142, mean reward: 2.938 [2.161, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.106, 10.496], loss: 0.337715, mae: 0.528907, mean_q: 4.989395
 89982/100000: episode: 1694, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 121.658, mean reward: 4.195 [2.628, 6.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.360, 10.449], loss: 0.683592, mae: 0.666014, mean_q: 5.322688
 89997/100000: episode: 1695, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 64.219, mean reward: 4.281 [3.235, 7.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.418, 10.534], loss: 0.338120, mae: 0.544541, mean_q: 5.005932
 90012/100000: episode: 1696, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 43.879, mean reward: 2.925 [2.320, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.422], loss: 0.411736, mae: 0.568405, mean_q: 5.172787
 90042/100000: episode: 1697, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 170.978, mean reward: 5.699 [2.897, 13.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.444, 10.567], loss: 0.369280, mae: 0.568401, mean_q: 5.146761
 90070/100000: episode: 1698, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 96.913, mean reward: 3.461 [2.281, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.229, 10.427], loss: 0.437514, mae: 0.596635, mean_q: 5.250628
 90085/100000: episode: 1699, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 71.159, mean reward: 4.744 [2.904, 6.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.826, 10.613], loss: 0.418703, mae: 0.581612, mean_q: 5.441081
 90113/100000: episode: 1700, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 94.496, mean reward: 3.375 [1.915, 5.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.392, 10.387], loss: 0.381102, mae: 0.585182, mean_q: 5.202871
 90128/100000: episode: 1701, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 42.998, mean reward: 2.867 [2.084, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.337], loss: 0.511441, mae: 0.597144, mean_q: 5.257952
 90156/100000: episode: 1702, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 137.418, mean reward: 4.908 [2.191, 9.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.638, 10.652], loss: 0.725173, mae: 0.626563, mean_q: 5.242167
 90171/100000: episode: 1703, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 47.169, mean reward: 3.145 [2.430, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.567], loss: 0.660054, mae: 0.664550, mean_q: 5.354206
 90201/100000: episode: 1704, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 116.796, mean reward: 3.893 [1.910, 10.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.904, 10.274], loss: 0.403599, mae: 0.600921, mean_q: 5.233918
 90230/100000: episode: 1705, duration: 0.175s, episode steps: 29, steps per second: 165, episode reward: 122.903, mean reward: 4.238 [2.955, 6.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.007, 10.521], loss: 0.398148, mae: 0.575056, mean_q: 5.174920
 90258/100000: episode: 1706, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 74.832, mean reward: 2.673 [2.105, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.401], loss: 1.464400, mae: 0.790816, mean_q: 5.380826
 90282/100000: episode: 1707, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 78.508, mean reward: 3.271 [2.394, 4.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.281, 10.393], loss: 1.381417, mae: 0.796510, mean_q: 5.562128
 90296/100000: episode: 1708, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 44.264, mean reward: 3.162 [2.751, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.476], loss: 0.686786, mae: 0.704235, mean_q: 5.583817
 90317/100000: episode: 1709, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 62.954, mean reward: 2.998 [2.280, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.138, 10.375], loss: 0.420122, mae: 0.609335, mean_q: 5.445296
 90346/100000: episode: 1710, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 88.786, mean reward: 3.062 [2.237, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.169, 10.329], loss: 0.642700, mae: 0.615004, mean_q: 5.435918
 90370/100000: episode: 1711, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 138.559, mean reward: 5.773 [1.915, 39.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.142, 10.301], loss: 2.855579, mae: 0.814601, mean_q: 5.353773
 90385/100000: episode: 1712, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 40.658, mean reward: 2.711 [1.692, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.315], loss: 0.591139, mae: 0.668591, mean_q: 5.395929
 90411/100000: episode: 1713, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 98.111, mean reward: 3.774 [2.068, 7.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.347], loss: 0.475636, mae: 0.678947, mean_q: 5.607324
 90440/100000: episode: 1714, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 112.931, mean reward: 3.894 [2.436, 5.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.495, 10.512], loss: 1.037086, mae: 0.622989, mean_q: 5.441019
 90464/100000: episode: 1715, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 88.717, mean reward: 3.697 [1.741, 7.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.157, 10.334], loss: 1.359734, mae: 0.767968, mean_q: 5.657108
 90479/100000: episode: 1716, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 86.535, mean reward: 5.769 [3.162, 13.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.538, 10.602], loss: 0.344805, mae: 0.552027, mean_q: 5.476882
 90505/100000: episode: 1717, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 83.879, mean reward: 3.226 [2.324, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.480], loss: 0.336105, mae: 0.548932, mean_q: 5.447582
 90533/100000: episode: 1718, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 133.413, mean reward: 4.765 [2.642, 8.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.074, 10.625], loss: 0.648491, mae: 0.695913, mean_q: 5.499425
 90562/100000: episode: 1719, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 94.154, mean reward: 3.247 [2.297, 4.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.478, 10.404], loss: 1.138268, mae: 0.688043, mean_q: 5.622350
 90576/100000: episode: 1720, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 62.728, mean reward: 4.481 [2.916, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.119, 10.567], loss: 0.353689, mae: 0.598476, mean_q: 5.387516
 90591/100000: episode: 1721, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 37.080, mean reward: 2.472 [1.958, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.486, 10.343], loss: 3.126792, mae: 0.909266, mean_q: 5.592276
 90606/100000: episode: 1722, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 45.311, mean reward: 3.021 [2.682, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.163, 10.493], loss: 0.492255, mae: 0.682060, mean_q: 5.547548
 90632/100000: episode: 1723, duration: 0.125s, episode steps: 26, steps per second: 209, episode reward: 84.049, mean reward: 3.233 [2.392, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.399], loss: 0.589247, mae: 0.666599, mean_q: 5.744215
 90647/100000: episode: 1724, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 60.175, mean reward: 4.012 [2.941, 5.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.493], loss: 0.590912, mae: 0.713226, mean_q: 5.498161
 90668/100000: episode: 1725, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 71.633, mean reward: 3.411 [2.209, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.025, 10.454], loss: 0.416789, mae: 0.587728, mean_q: 5.567517
 90696/100000: episode: 1726, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 102.898, mean reward: 3.675 [2.261, 5.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.610, 10.425], loss: 1.247668, mae: 0.778020, mean_q: 5.639061
 90725/100000: episode: 1727, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 163.606, mean reward: 5.642 [3.699, 16.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.343, 10.714], loss: 0.533482, mae: 0.660330, mean_q: 5.591912
 90737/100000: episode: 1728, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 72.253, mean reward: 6.021 [2.957, 9.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.526, 10.538], loss: 1.051125, mae: 0.740296, mean_q: 5.534004
 90766/100000: episode: 1729, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 87.666, mean reward: 3.023 [1.950, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.172, 10.322], loss: 1.220918, mae: 0.755847, mean_q: 5.758123
[Info] 3-TH LEVEL FOUND: 11.09981632232666, Considering 10/90 traces
 90781/100000: episode: 1730, duration: 4.226s, episode steps: 15, steps per second: 4, episode reward: 44.598, mean reward: 2.973 [2.439, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.575, 10.397], loss: 0.578216, mae: 0.691696, mean_q: 5.802841
 90802/100000: episode: 1731, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 90.544, mean reward: 4.312 [3.088, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.049, 10.514], loss: 0.404428, mae: 0.624446, mean_q: 5.531091
[Info] FALSIFICATION!
 90805/100000: episode: 1732, duration: 0.178s, episode steps: 3, steps per second: 17, episode reward: 1037.170, mean reward: 345.723 [10.449, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.880, 9.139], loss: 0.532509, mae: 0.797278, mean_q: 6.122519
 90809/100000: episode: 1733, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 40.360, mean reward: 10.090 [8.606, 11.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.637], loss: 0.424425, mae: 0.622307, mean_q: 5.639134
 90833/100000: episode: 1734, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 117.630, mean reward: 4.901 [2.992, 13.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.070, 10.521], loss: 635.575012, mae: 2.058502, mean_q: 5.830757
 90837/100000: episode: 1735, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 21.515, mean reward: 5.379 [4.410, 7.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.464], loss: 19.755585, mae: 5.771157, mean_q: 11.753733
 90860/100000: episode: 1736, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 113.177, mean reward: 4.921 [2.837, 9.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.403], loss: 2.436291, mae: 1.684823, mean_q: 5.756353
 90881/100000: episode: 1737, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 238.349, mean reward: 11.350 [2.788, 55.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.082, 10.454], loss: 2.639299, mae: 1.133488, mean_q: 5.780457
 90902/100000: episode: 1738, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 92.590, mean reward: 4.409 [3.166, 8.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.188, 10.495], loss: 1.782655, mae: 0.964160, mean_q: 5.726748
 90923/100000: episode: 1739, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 110.444, mean reward: 5.259 [1.639, 13.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.214, 10.445], loss: 1.504637, mae: 0.794668, mean_q: 5.631410
 90946/100000: episode: 1740, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 182.665, mean reward: 7.942 [4.379, 14.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.075, 10.547], loss: 669.561584, mae: 2.584993, mean_q: 6.286651
 90950/100000: episode: 1741, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 48.687, mean reward: 12.172 [8.503, 18.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.523], loss: 1.820495, mae: 1.239266, mean_q: 5.863669
 90954/100000: episode: 1742, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 43.660, mean reward: 10.915 [8.557, 13.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.578], loss: 2.251479, mae: 1.099273, mean_q: 6.336944
 90973/100000: episode: 1743, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 92.729, mean reward: 4.880 [2.526, 8.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.350], loss: 0.857795, mae: 0.892888, mean_q: 6.166495
 90977/100000: episode: 1744, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 31.160, mean reward: 7.790 [6.197, 9.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.507], loss: 0.524990, mae: 0.791778, mean_q: 6.362984
 90998/100000: episode: 1745, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 92.322, mean reward: 4.396 [2.648, 6.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.119, 10.433], loss: 730.581299, mae: 3.375545, mean_q: 7.630215
 91017/100000: episode: 1746, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 158.547, mean reward: 8.345 [3.612, 45.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.521, 10.616], loss: 2.345183, mae: 1.636696, mean_q: 6.132564
 91038/100000: episode: 1747, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 114.222, mean reward: 5.439 [3.143, 11.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.340, 10.538], loss: 2.193684, mae: 1.123782, mean_q: 6.605906
 91045/100000: episode: 1748, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 65.643, mean reward: 9.378 [5.797, 14.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.620], loss: 5.295331, mae: 1.199539, mean_q: 6.337255
 91052/100000: episode: 1749, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 31.230, mean reward: 4.461 [3.164, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.593], loss: 0.938900, mae: 0.959402, mean_q: 6.511884
 91056/100000: episode: 1750, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 26.354, mean reward: 6.589 [5.919, 7.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.634], loss: 0.904306, mae: 0.944396, mean_q: 6.384985
 91075/100000: episode: 1751, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 153.220, mean reward: 8.064 [3.961, 11.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.638], loss: 2.729796, mae: 1.016609, mean_q: 6.561529
[Info] FALSIFICATION!
 91083/100000: episode: 1752, duration: 0.213s, episode steps: 8, steps per second: 38, episode reward: 1091.226, mean reward: 136.403 [7.157, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.680, 10.106], loss: 0.936728, mae: 0.930485, mean_q: 6.180787
 91107/100000: episode: 1753, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 152.981, mean reward: 6.374 [4.438, 8.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.104, 10.646], loss: 1.712026, mae: 0.959043, mean_q: 6.425282
 91114/100000: episode: 1754, duration: 0.052s, episode steps: 7, steps per second: 133, episode reward: 37.286, mean reward: 5.327 [4.002, 6.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.607], loss: 1.686705, mae: 1.117276, mean_q: 6.639519
 91138/100000: episode: 1755, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 170.305, mean reward: 7.096 [2.830, 14.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.852, 10.451], loss: 3.908058, mae: 1.123926, mean_q: 6.546268
 91157/100000: episode: 1756, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 107.542, mean reward: 5.660 [3.240, 8.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.334, 10.477], loss: 2.710039, mae: 1.062587, mean_q: 6.716594
 91176/100000: episode: 1757, duration: 0.111s, episode steps: 19, steps per second: 170, episode reward: 167.488, mean reward: 8.815 [3.949, 38.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.856, 10.612], loss: 2.390235, mae: 1.159296, mean_q: 6.863448
 91183/100000: episode: 1758, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 36.317, mean reward: 5.188 [4.094, 7.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.554], loss: 1.097372, mae: 0.993778, mean_q: 6.527264
 91202/100000: episode: 1759, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 73.879, mean reward: 3.888 [1.942, 7.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.674, 10.381], loss: 3.030812, mae: 1.021485, mean_q: 6.553178
 91224/100000: episode: 1760, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 173.919, mean reward: 7.905 [2.878, 24.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.258, 10.472], loss: 1.443883, mae: 1.014300, mean_q: 6.496043
 91231/100000: episode: 1761, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 44.834, mean reward: 6.405 [4.562, 8.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.578], loss: 1.070755, mae: 1.003400, mean_q: 6.507586
 91250/100000: episode: 1762, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 88.046, mean reward: 4.634 [2.441, 10.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.506], loss: 1.446176, mae: 1.073256, mean_q: 6.520753
 91269/100000: episode: 1763, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 147.652, mean reward: 7.771 [4.165, 15.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.594, 10.648], loss: 2.498958, mae: 1.092523, mean_q: 6.640020
 91292/100000: episode: 1764, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 621.454, mean reward: 27.020 [3.783, 376.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.732, 10.460], loss: 1.928757, mae: 1.129081, mean_q: 6.738140
 91311/100000: episode: 1765, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 97.473, mean reward: 5.130 [3.465, 11.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.525], loss: 806.436707, mae: 3.573170, mean_q: 7.938270
 91326/100000: episode: 1766, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 104.178, mean reward: 6.945 [4.700, 10.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.576], loss: 4.301260, mae: 1.829407, mean_q: 6.490635
 91347/100000: episode: 1767, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 206.246, mean reward: 9.821 [3.617, 34.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.036, 10.506], loss: 1.580613, mae: 1.253824, mean_q: 7.114366
 91366/100000: episode: 1768, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 225.408, mean reward: 11.864 [5.017, 86.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.176, 10.634], loss: 115.663704, mae: 2.025658, mean_q: 7.249864
 91385/100000: episode: 1769, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 182.173, mean reward: 9.588 [3.934, 22.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.212, 10.715], loss: 1604.702881, mae: 5.160288, mean_q: 8.162172
[Info] FALSIFICATION!
 91393/100000: episode: 1770, duration: 0.261s, episode steps: 8, steps per second: 31, episode reward: 1059.487, mean reward: 132.436 [5.431, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.217, 10.054], loss: 5.193820, mae: 2.750899, mean_q: 9.469891
 91417/100000: episode: 1771, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 203.262, mean reward: 8.469 [3.001, 20.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.555], loss: 7.105061, mae: 1.965717, mean_q: 7.205840
 91440/100000: episode: 1772, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 158.489, mean reward: 6.891 [3.680, 10.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.724, 10.506], loss: 1421.871826, mae: 6.080726, mean_q: 9.540086
 91444/100000: episode: 1773, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 28.955, mean reward: 7.239 [6.280, 9.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.644], loss: 3775.234619, mae: 11.082450, mean_q: 10.880060
 91451/100000: episode: 1774, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 154.274, mean reward: 22.039 [7.089, 69.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.336, 10.520], loss: 4.618449, mae: 2.329617, mean_q: 8.652950
 91474/100000: episode: 1775, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 242.825, mean reward: 10.558 [2.412, 41.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.526], loss: 8.607445, mae: 2.062576, mean_q: 7.330463
 91497/100000: episode: 1776, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 152.616, mean reward: 6.635 [3.349, 11.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.076, 10.518], loss: 2.604359, mae: 1.446895, mean_q: 7.772477
 91504/100000: episode: 1777, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 42.508, mean reward: 6.073 [4.556, 8.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.632], loss: 5.765706, mae: 1.542927, mean_q: 7.579463
 91508/100000: episode: 1778, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 26.702, mean reward: 6.676 [5.556, 8.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.519], loss: 2.501697, mae: 1.356705, mean_q: 7.758243
 91532/100000: episode: 1779, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 1290.722, mean reward: 53.780 [2.767, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.096, 10.572], loss: 93.124153, mae: 2.246055, mean_q: 8.242544
 91536/100000: episode: 1780, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 19.529, mean reward: 4.882 [4.377, 5.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.602], loss: 3.792825, mae: 1.540165, mean_q: 7.672347
 91540/100000: episode: 1781, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 20.763, mean reward: 5.191 [4.378, 5.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.556], loss: 3.026388, mae: 1.478668, mean_q: 7.099339
 91555/100000: episode: 1782, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 121.503, mean reward: 8.100 [4.437, 13.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.285, 10.594], loss: 12.630062, mae: 1.738821, mean_q: 7.651781
 91562/100000: episode: 1783, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 68.002, mean reward: 9.715 [5.203, 21.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.746], loss: 2203.453613, mae: 6.173017, mean_q: 8.260728
 91581/100000: episode: 1784, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 110.728, mean reward: 5.828 [3.381, 10.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.485], loss: 800.071106, mae: 4.436521, mean_q: 9.669134
 91585/100000: episode: 1785, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 22.089, mean reward: 5.522 [4.681, 7.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.661], loss: 4.403490, mae: 2.439828, mean_q: 9.374970
 91600/100000: episode: 1786, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 111.706, mean reward: 7.447 [4.140, 12.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.014, 10.598], loss: 6.173150, mae: 2.121938, mean_q: 7.825490
 91619/100000: episode: 1787, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 87.770, mean reward: 4.619 [2.850, 7.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.501], loss: 6.098155, mae: 1.859447, mean_q: 8.118688
[Info] FALSIFICATION!
 91630/100000: episode: 1788, duration: 0.237s, episode steps: 11, steps per second: 46, episode reward: 1115.239, mean reward: 101.385 [5.156, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.279, 10.238], loss: 3.992413, mae: 1.592757, mean_q: 8.027189
 91649/100000: episode: 1789, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 96.522, mean reward: 5.080 [4.213, 7.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.645], loss: 4.245161, mae: 1.397985, mean_q: 7.660637
 91664/100000: episode: 1790, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 152.105, mean reward: 10.140 [4.716, 25.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-1.312, 10.604], loss: 5.762276, mae: 1.667300, mean_q: 8.029651
 91686/100000: episode: 1791, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 165.690, mean reward: 7.531 [2.822, 38.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.491, 10.513], loss: 3.760952, mae: 1.454447, mean_q: 8.082951
 91705/100000: episode: 1792, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 128.635, mean reward: 6.770 [3.389, 13.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.577, 10.535], loss: 801.277649, mae: 2.966921, mean_q: 7.919721
 91727/100000: episode: 1793, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 141.314, mean reward: 6.423 [3.643, 12.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.805, 10.440], loss: 699.989807, mae: 5.652478, mean_q: 11.385912
 91731/100000: episode: 1794, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 33.810, mean reward: 8.452 [5.778, 10.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.655], loss: 3861.784424, mae: 9.943359, mean_q: 6.762134
 91735/100000: episode: 1795, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 27.733, mean reward: 6.933 [5.503, 8.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.657], loss: 5.054210, mae: 2.793745, mean_q: 10.425107
 91750/100000: episode: 1796, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 123.958, mean reward: 8.264 [4.895, 18.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.503, 10.620], loss: 13.202909, mae: 2.943149, mean_q: 10.071453
 91773/100000: episode: 1797, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 163.648, mean reward: 7.115 [2.554, 30.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.316, 10.422], loss: 662.617371, mae: 3.594978, mean_q: 9.314794
 91795/100000: episode: 1798, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 127.728, mean reward: 5.806 [2.202, 16.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.301, 10.421], loss: 706.553955, mae: 3.815016, mean_q: 9.173423
 91817/100000: episode: 1799, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 142.751, mean reward: 6.489 [3.130, 13.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.508], loss: 796.947205, mae: 4.814857, mean_q: 10.721943
 91841/100000: episode: 1800, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 99.178, mean reward: 4.132 [2.611, 6.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.300, 10.548], loss: 99.542336, mae: 2.771410, mean_q: 9.578864
[Info] FALSIFICATION!
 91857/100000: episode: 1801, duration: 0.250s, episode steps: 16, steps per second: 64, episode reward: 1519.975, mean reward: 94.998 [7.964, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.623, 10.627], loss: 947.554932, mae: 3.371260, mean_q: 8.514532
 91864/100000: episode: 1802, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 50.134, mean reward: 7.162 [5.377, 9.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.644], loss: 5.010890, mae: 2.591692, mean_q: 10.478350
 91871/100000: episode: 1803, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 53.546, mean reward: 7.649 [5.869, 10.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.382, 10.489], loss: 6.401092, mae: 2.850210, mean_q: 10.671634
 91890/100000: episode: 1804, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 147.529, mean reward: 7.765 [3.481, 16.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.538], loss: 4.580585, mae: 2.039484, mean_q: 7.951449
 91914/100000: episode: 1805, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 111.860, mean reward: 4.661 [3.023, 8.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.316, 10.532], loss: 9.679902, mae: 1.821458, mean_q: 8.951039
 91918/100000: episode: 1806, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 23.548, mean reward: 5.887 [5.412, 6.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.580], loss: 3.841686, mae: 1.672806, mean_q: 9.065365
 91933/100000: episode: 1807, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 222.652, mean reward: 14.843 [4.830, 70.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.586], loss: 1160.179688, mae: 7.989167, mean_q: 13.586583
 91940/100000: episode: 1808, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 58.682, mean reward: 8.383 [5.042, 15.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.615], loss: 311.287415, mae: 5.123587, mean_q: 11.510124
 91955/100000: episode: 1809, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 111.760, mean reward: 7.451 [3.148, 23.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.102, 10.590], loss: 1027.838745, mae: 4.560854, mean_q: 9.802729
 91959/100000: episode: 1810, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 21.659, mean reward: 5.415 [4.477, 6.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.528], loss: 6.404561, mae: 2.963391, mean_q: 11.493399
 91978/100000: episode: 1811, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 453.355, mean reward: 23.861 [5.627, 260.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.385, 10.697], loss: 7.184972, mae: 2.594638, mean_q: 10.687636
[Info] FALSIFICATION!
 91991/100000: episode: 1812, duration: 0.240s, episode steps: 13, steps per second: 54, episode reward: 1085.207, mean reward: 83.477 [5.013, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.115, 10.720], loss: 90.644081, mae: 2.768670, mean_q: 9.265594
 91995/100000: episode: 1813, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 28.554, mean reward: 7.138 [5.351, 10.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.551], loss: 10.686251, mae: 2.411205, mean_q: 10.039831
 92017/100000: episode: 1814, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 84.407, mean reward: 3.837 [2.895, 5.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.486], loss: 2072.738281, mae: 6.925114, mean_q: 10.698302
 92021/100000: episode: 1815, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 24.579, mean reward: 6.145 [4.971, 7.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.616], loss: 59.000671, mae: 10.446080, mean_q: 19.741913
 92042/100000: episode: 1816, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 198.979, mean reward: 9.475 [5.023, 21.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.339, 10.600], loss: 49.074486, mae: 5.118643, mean_q: 12.066498
 92049/100000: episode: 1817, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 35.855, mean reward: 5.122 [3.763, 6.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.611], loss: 8.034359, mae: 2.612594, mean_q: 6.761056
 92056/100000: episode: 1818, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 54.466, mean reward: 7.781 [5.705, 15.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.788, 10.577], loss: 2155.511963, mae: 6.259532, mean_q: 9.403502
 92077/100000: episode: 1819, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 79.957, mean reward: 3.807 [2.309, 7.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.902, 10.517], loss: 1440.795166, mae: 6.863351, mean_q: 12.911274
[Info] Complete ISplit Iteration
[Info] Levels: [5.0210257, 7.2717967, 11.099816, 17.17072]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.6]
[Info] Error Prob: 0.0006000000000000002

 92092/100000: episode: 1820, duration: 4.380s, episode steps: 15, steps per second: 3, episode reward: 177.507, mean reward: 11.834 [6.291, 58.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.619], loss: 5.430901, mae: 2.443652, mean_q: 10.428896
 92192/100000: episode: 1821, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.961, mean reward: 1.990 [1.431, 6.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.891, 10.207], loss: 523.363831, mae: 4.378080, mean_q: 10.883998
 92292/100000: episode: 1822, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 218.253, mean reward: 2.183 [1.471, 5.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.308, 10.448], loss: 340.619110, mae: 3.278339, mean_q: 10.428880
 92392/100000: episode: 1823, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 184.051, mean reward: 1.841 [1.450, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.193], loss: 1238.527466, mae: 5.993319, mean_q: 12.025666
 92492/100000: episode: 1824, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 187.350, mean reward: 1.874 [1.470, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.023, 10.098], loss: 369.105713, mae: 3.426991, mean_q: 10.653175
 92592/100000: episode: 1825, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 208.446, mean reward: 2.084 [1.432, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.475, 10.424], loss: 955.520142, mae: 5.484245, mean_q: 11.978827
 92692/100000: episode: 1826, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 207.463, mean reward: 2.075 [1.467, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.080, 10.527], loss: 1378.026733, mae: 6.952220, mean_q: 13.272582
 92792/100000: episode: 1827, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 193.596, mean reward: 1.936 [1.478, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.171], loss: 336.702057, mae: 3.641519, mean_q: 11.393545
 92892/100000: episode: 1828, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.145, mean reward: 1.891 [1.468, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.769, 10.172], loss: 605.522095, mae: 3.794094, mean_q: 10.488620
 92992/100000: episode: 1829, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.470, mean reward: 1.885 [1.475, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.194, 10.098], loss: 964.868530, mae: 5.434562, mean_q: 11.365810
 93092/100000: episode: 1830, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.002, mean reward: 1.880 [1.460, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.691, 10.098], loss: 914.806946, mae: 6.029534, mean_q: 11.503142
 93192/100000: episode: 1831, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.631, mean reward: 1.856 [1.499, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.968, 10.098], loss: 804.789246, mae: 5.272561, mean_q: 11.690852
 93292/100000: episode: 1832, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.987, mean reward: 1.950 [1.474, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.637, 10.216], loss: 789.432739, mae: 4.857059, mean_q: 11.425141
 93392/100000: episode: 1833, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.129, mean reward: 1.861 [1.474, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.463, 10.194], loss: 631.650452, mae: 4.425749, mean_q: 11.366102
 93492/100000: episode: 1834, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 206.699, mean reward: 2.067 [1.459, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.970, 10.098], loss: 526.915161, mae: 4.477132, mean_q: 11.008681
 93592/100000: episode: 1835, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 204.965, mean reward: 2.050 [1.463, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.141, 10.320], loss: 1215.706177, mae: 6.560810, mean_q: 12.139217
 93692/100000: episode: 1836, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 195.657, mean reward: 1.957 [1.458, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.194, 10.207], loss: 1053.271729, mae: 5.489729, mean_q: 11.606494
 93792/100000: episode: 1837, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 182.776, mean reward: 1.828 [1.497, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.694, 10.260], loss: 1204.370972, mae: 6.116562, mean_q: 11.934159
 93892/100000: episode: 1838, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 196.754, mean reward: 1.968 [1.528, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.568, 10.132], loss: 769.592346, mae: 4.992157, mean_q: 11.437466
 93992/100000: episode: 1839, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.069, mean reward: 1.951 [1.475, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.730, 10.177], loss: 751.749817, mae: 4.980616, mean_q: 11.572162
 94092/100000: episode: 1840, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.068, mean reward: 1.951 [1.446, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.098], loss: 347.147217, mae: 3.442835, mean_q: 10.229913
 94192/100000: episode: 1841, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.600, mean reward: 1.976 [1.457, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.098], loss: 327.130768, mae: 3.197958, mean_q: 9.840497
 94292/100000: episode: 1842, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 215.055, mean reward: 2.151 [1.442, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.451], loss: 327.460480, mae: 2.760153, mean_q: 9.056658
 94392/100000: episode: 1843, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.821, mean reward: 1.858 [1.465, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.760, 10.098], loss: 610.706604, mae: 4.437834, mean_q: 10.116132
 94492/100000: episode: 1844, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 199.804, mean reward: 1.998 [1.437, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.098], loss: 1370.987549, mae: 7.404722, mean_q: 11.744768
 94592/100000: episode: 1845, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 183.397, mean reward: 1.834 [1.456, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.542, 10.224], loss: 491.333130, mae: 3.619598, mean_q: 9.780577
 94692/100000: episode: 1846, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 180.947, mean reward: 1.809 [1.442, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.124], loss: 318.440033, mae: 2.940178, mean_q: 9.154745
 94792/100000: episode: 1847, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 186.795, mean reward: 1.868 [1.442, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.250, 10.098], loss: 623.501160, mae: 4.100128, mean_q: 9.361312
 94892/100000: episode: 1848, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 231.601, mean reward: 2.316 [1.544, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.853, 10.098], loss: 760.815002, mae: 4.180811, mean_q: 8.734286
 94992/100000: episode: 1849, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.942, mean reward: 1.909 [1.470, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.137, 10.098], loss: 898.986938, mae: 5.347146, mean_q: 10.049386
 95092/100000: episode: 1850, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.161, mean reward: 1.932 [1.461, 7.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.066, 10.098], loss: 335.956146, mae: 3.145965, mean_q: 8.820105
 95192/100000: episode: 1851, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 211.608, mean reward: 2.116 [1.444, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.552, 10.182], loss: 841.034546, mae: 5.427148, mean_q: 9.800277
 95292/100000: episode: 1852, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 205.838, mean reward: 2.058 [1.469, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.750, 10.098], loss: 757.403137, mae: 4.645358, mean_q: 9.878692
 95392/100000: episode: 1853, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.249, mean reward: 1.922 [1.490, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.111, 10.117], loss: 610.688843, mae: 4.337748, mean_q: 9.897523
 95492/100000: episode: 1854, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.525, mean reward: 1.895 [1.474, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.215, 10.098], loss: 1154.105713, mae: 6.685444, mean_q: 10.590806
 95592/100000: episode: 1855, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 195.181, mean reward: 1.952 [1.495, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.155], loss: 653.201111, mae: 4.381931, mean_q: 9.684204
 95692/100000: episode: 1856, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 187.903, mean reward: 1.879 [1.484, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.215, 10.227], loss: 1185.859497, mae: 6.130099, mean_q: 10.004375
 95792/100000: episode: 1857, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 208.076, mean reward: 2.081 [1.515, 7.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.480, 10.098], loss: 638.063171, mae: 4.756124, mean_q: 9.207275
 95892/100000: episode: 1858, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 178.958, mean reward: 1.790 [1.444, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.700, 10.171], loss: 594.757019, mae: 3.700801, mean_q: 8.362453
 95992/100000: episode: 1859, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.056, mean reward: 1.851 [1.489, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.999, 10.098], loss: 461.446014, mae: 3.266101, mean_q: 7.891266
 96092/100000: episode: 1860, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.475, mean reward: 1.845 [1.460, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.618, 10.098], loss: 490.072540, mae: 3.274772, mean_q: 7.669568
 96192/100000: episode: 1861, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 212.067, mean reward: 2.121 [1.465, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.307], loss: 323.671143, mae: 2.419144, mean_q: 7.194269
 96292/100000: episode: 1862, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.574, mean reward: 1.866 [1.451, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.330, 10.139], loss: 593.056335, mae: 3.308296, mean_q: 7.308805
 96392/100000: episode: 1863, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.384, mean reward: 1.934 [1.469, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.427, 10.226], loss: 762.638062, mae: 4.850304, mean_q: 8.305519
 96492/100000: episode: 1864, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 222.479, mean reward: 2.225 [1.437, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.783, 10.098], loss: 318.896271, mae: 2.730510, mean_q: 6.903929
 96592/100000: episode: 1865, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.193, mean reward: 1.932 [1.437, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.639, 10.233], loss: 14.617558, mae: 1.344131, mean_q: 5.614680
 96692/100000: episode: 1866, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 178.819, mean reward: 1.788 [1.452, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.417, 10.098], loss: 11.833091, mae: 1.080426, mean_q: 5.035421
 96792/100000: episode: 1867, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 195.104, mean reward: 1.951 [1.440, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.102], loss: 168.204514, mae: 1.617357, mean_q: 5.041768
 96892/100000: episode: 1868, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 208.059, mean reward: 2.081 [1.486, 5.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.763, 10.098], loss: 151.181198, mae: 1.190429, mean_q: 4.626431
 96992/100000: episode: 1869, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.353, mean reward: 1.924 [1.443, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.761, 10.394], loss: 1.310167, mae: 0.745043, mean_q: 4.168717
 97092/100000: episode: 1870, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.437, mean reward: 1.874 [1.453, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.067, 10.098], loss: 0.386369, mae: 0.621202, mean_q: 3.797795
 97192/100000: episode: 1871, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 213.162, mean reward: 2.132 [1.463, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.594, 10.098], loss: 0.307270, mae: 0.583024, mean_q: 3.799033
 97292/100000: episode: 1872, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.729, mean reward: 1.907 [1.447, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.249, 10.098], loss: 0.277350, mae: 0.565136, mean_q: 3.786350
 97392/100000: episode: 1873, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 242.100, mean reward: 2.421 [1.449, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.824, 10.352], loss: 0.255006, mae: 0.545569, mean_q: 3.822473
 97492/100000: episode: 1874, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 199.066, mean reward: 1.991 [1.475, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.579, 10.249], loss: 0.335857, mae: 0.590172, mean_q: 3.911346
 97592/100000: episode: 1875, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.715, mean reward: 1.857 [1.453, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.322, 10.098], loss: 0.295083, mae: 0.559054, mean_q: 3.911488
 97692/100000: episode: 1876, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.754, mean reward: 1.858 [1.483, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.098], loss: 0.277629, mae: 0.547662, mean_q: 3.876956
 97792/100000: episode: 1877, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.141, mean reward: 1.861 [1.472, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.540, 10.098], loss: 0.267785, mae: 0.538175, mean_q: 3.815637
 97892/100000: episode: 1878, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.341, mean reward: 1.883 [1.458, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.616, 10.216], loss: 0.258276, mae: 0.531445, mean_q: 3.900691
 97992/100000: episode: 1879, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.224, mean reward: 2.002 [1.484, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.304, 10.252], loss: 0.269414, mae: 0.546254, mean_q: 3.869574
 98092/100000: episode: 1880, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.335, mean reward: 1.893 [1.440, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.697, 10.098], loss: 0.248176, mae: 0.511700, mean_q: 3.895789
 98192/100000: episode: 1881, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.911, mean reward: 1.989 [1.513, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.515, 10.098], loss: 0.252537, mae: 0.515953, mean_q: 3.891999
 98292/100000: episode: 1882, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.554, mean reward: 1.876 [1.431, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.999, 10.242], loss: 0.249363, mae: 0.526966, mean_q: 3.888814
 98392/100000: episode: 1883, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 214.679, mean reward: 2.147 [1.470, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.035, 10.547], loss: 0.229422, mae: 0.494540, mean_q: 3.914247
 98492/100000: episode: 1884, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.654, mean reward: 1.957 [1.508, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.737, 10.098], loss: 0.264900, mae: 0.523225, mean_q: 3.923332
 98592/100000: episode: 1885, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 203.148, mean reward: 2.031 [1.458, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.178, 10.468], loss: 0.239083, mae: 0.497921, mean_q: 3.905912
 98692/100000: episode: 1886, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.992, mean reward: 1.910 [1.478, 4.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.927, 10.098], loss: 0.253469, mae: 0.513648, mean_q: 3.894304
 98792/100000: episode: 1887, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 225.093, mean reward: 2.251 [1.489, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.342], loss: 0.219206, mae: 0.481984, mean_q: 3.909881
 98892/100000: episode: 1888, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.308, mean reward: 2.003 [1.432, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.205], loss: 0.224335, mae: 0.493375, mean_q: 3.922459
 98992/100000: episode: 1889, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 185.925, mean reward: 1.859 [1.454, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.679, 10.200], loss: 0.242728, mae: 0.489564, mean_q: 3.924941
 99092/100000: episode: 1890, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 196.608, mean reward: 1.966 [1.499, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.650, 10.205], loss: 0.224634, mae: 0.492194, mean_q: 3.906347
 99192/100000: episode: 1891, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 202.466, mean reward: 2.025 [1.437, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.460], loss: 0.216536, mae: 0.477787, mean_q: 3.939169
 99292/100000: episode: 1892, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 202.006, mean reward: 2.020 [1.472, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.559, 10.368], loss: 0.208806, mae: 0.465396, mean_q: 3.894597
 99392/100000: episode: 1893, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 186.775, mean reward: 1.868 [1.473, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.151], loss: 0.211601, mae: 0.464636, mean_q: 3.894526
 99492/100000: episode: 1894, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 181.420, mean reward: 1.814 [1.437, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.103], loss: 0.185435, mae: 0.445039, mean_q: 3.881838
 99592/100000: episode: 1895, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 222.044, mean reward: 2.220 [1.454, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.248, 10.098], loss: 0.188976, mae: 0.446169, mean_q: 3.896657
 99692/100000: episode: 1896, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.250, mean reward: 1.932 [1.442, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.533, 10.340], loss: 0.212957, mae: 0.461199, mean_q: 3.905058
 99792/100000: episode: 1897, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 203.627, mean reward: 2.036 [1.498, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.411, 10.178], loss: 0.211980, mae: 0.459179, mean_q: 3.891940
 99892/100000: episode: 1898, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 208.571, mean reward: 2.086 [1.456, 9.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.745, 10.098], loss: 0.203469, mae: 0.441730, mean_q: 3.869869
 99992/100000: episode: 1899, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 212.084, mean reward: 2.121 [1.470, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.729, 10.105], loss: 0.196969, mae: 0.445150, mean_q: 3.936975
done, took 635.756 seconds
[Info] End Importance Splitting. Falsification occurred 20 times.
