Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.163s, episode steps: 100, steps per second: 614, episode reward: 183.884, mean reward: 1.839 [1.465, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.182, 10.112], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 196.265, mean reward: 1.963 [1.430, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.551, 10.304], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.068s, episode steps: 100, steps per second: 1464, episode reward: 204.701, mean reward: 2.047 [1.435, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.110, 10.183], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1633, episode reward: 192.630, mean reward: 1.926 [1.446, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.006, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 195.986, mean reward: 1.960 [1.494, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.348, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.067s, episode steps: 100, steps per second: 1494, episode reward: 207.993, mean reward: 2.080 [1.471, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.137, 10.123], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 186.296, mean reward: 1.863 [1.439, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.335, 10.159], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.079s, episode steps: 100, steps per second: 1272, episode reward: 205.978, mean reward: 2.060 [1.461, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.530, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 203.761, mean reward: 2.038 [1.457, 5.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.560, 10.150], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 192.880, mean reward: 1.929 [1.450, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.948, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.071s, episode steps: 100, steps per second: 1409, episode reward: 207.806, mean reward: 2.078 [1.485, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.942, 10.121], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 237.163, mean reward: 2.372 [1.484, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.880, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 191.780, mean reward: 1.918 [1.488, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.609, 10.307], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.073s, episode steps: 100, steps per second: 1372, episode reward: 189.962, mean reward: 1.900 [1.457, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.322, 10.424], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.066s, episode steps: 100, steps per second: 1506, episode reward: 203.181, mean reward: 2.032 [1.531, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.296, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 194.310, mean reward: 1.943 [1.487, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.144, 10.151], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.115s, episode steps: 100, steps per second: 868, episode reward: 192.411, mean reward: 1.924 [1.458, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.425, 10.103], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.073s, episode steps: 100, steps per second: 1376, episode reward: 186.344, mean reward: 1.863 [1.438, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.345, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 181.061, mean reward: 1.811 [1.478, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.078s, episode steps: 100, steps per second: 1277, episode reward: 210.977, mean reward: 2.110 [1.490, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.274, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.080s, episode steps: 100, steps per second: 1243, episode reward: 189.863, mean reward: 1.899 [1.483, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.234, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.087s, episode steps: 100, steps per second: 1143, episode reward: 191.540, mean reward: 1.915 [1.469, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.079s, episode steps: 100, steps per second: 1261, episode reward: 193.122, mean reward: 1.931 [1.463, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.567, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 178.120, mean reward: 1.781 [1.466, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.062s, episode steps: 100, steps per second: 1616, episode reward: 222.807, mean reward: 2.228 [1.506, 5.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.510, 10.405], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 196.682, mean reward: 1.967 [1.457, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.078s, episode steps: 100, steps per second: 1289, episode reward: 189.773, mean reward: 1.898 [1.478, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.101s, episode steps: 100, steps per second: 985, episode reward: 179.436, mean reward: 1.794 [1.466, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.105s, episode steps: 100, steps per second: 951, episode reward: 230.194, mean reward: 2.302 [1.523, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.542, 10.374], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.084s, episode steps: 100, steps per second: 1192, episode reward: 181.211, mean reward: 1.812 [1.442, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.023, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: 177.654, mean reward: 1.777 [1.449, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.107, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.072s, episode steps: 100, steps per second: 1379, episode reward: 185.564, mean reward: 1.856 [1.441, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.319, 10.188], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.061s, episode steps: 100, steps per second: 1628, episode reward: 202.260, mean reward: 2.023 [1.460, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.909, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.062s, episode steps: 100, steps per second: 1626, episode reward: 187.246, mean reward: 1.872 [1.437, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.758, 10.181], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 189.003, mean reward: 1.890 [1.465, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.170, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.062s, episode steps: 100, steps per second: 1621, episode reward: 193.904, mean reward: 1.939 [1.459, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.979, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.062s, episode steps: 100, steps per second: 1625, episode reward: 202.453, mean reward: 2.025 [1.444, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.549, 10.399], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.061s, episode steps: 100, steps per second: 1633, episode reward: 216.984, mean reward: 2.170 [1.457, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.807, 10.478], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 192.329, mean reward: 1.923 [1.458, 4.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.898, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.068s, episode steps: 100, steps per second: 1471, episode reward: 214.446, mean reward: 2.144 [1.470, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.373, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.061s, episode steps: 100, steps per second: 1632, episode reward: 211.780, mean reward: 2.118 [1.446, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.925, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 183.428, mean reward: 1.834 [1.436, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.694, 10.101], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 187.103, mean reward: 1.871 [1.445, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.985, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: 192.885, mean reward: 1.929 [1.461, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.796, 10.223], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.071s, episode steps: 100, steps per second: 1417, episode reward: 195.128, mean reward: 1.951 [1.458, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.400, 10.272], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 193.674, mean reward: 1.937 [1.441, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.674, 10.145], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.068s, episode steps: 100, steps per second: 1461, episode reward: 191.979, mean reward: 1.920 [1.480, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.160, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.069s, episode steps: 100, steps per second: 1448, episode reward: 205.191, mean reward: 2.052 [1.465, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.194, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 212.663, mean reward: 2.127 [1.491, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.872, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.071s, episode steps: 100, steps per second: 1400, episode reward: 215.077, mean reward: 2.151 [1.462, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.904, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.404s, episode steps: 100, steps per second: 71, episode reward: 200.340, mean reward: 2.003 [1.502, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.567, 10.268], loss: 0.272196, mae: 0.528690, mean_q: 2.275917
  5200/100000: episode: 52, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 223.704, mean reward: 2.237 [1.492, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.465, 10.098], loss: 0.127570, mae: 0.354216, mean_q: 2.908071
  5300/100000: episode: 53, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 193.703, mean reward: 1.937 [1.459, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.733, 10.098], loss: 0.117818, mae: 0.340178, mean_q: 3.229444
  5400/100000: episode: 54, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 249.862, mean reward: 2.499 [1.500, 6.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.286, 10.490], loss: 0.134205, mae: 0.357846, mean_q: 3.478446
  5500/100000: episode: 55, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 214.081, mean reward: 2.141 [1.550, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.396, 10.098], loss: 0.132993, mae: 0.345313, mean_q: 3.657593
  5600/100000: episode: 56, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 189.594, mean reward: 1.896 [1.458, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.619, 10.098], loss: 0.127339, mae: 0.345478, mean_q: 3.770919
  5700/100000: episode: 57, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 194.228, mean reward: 1.942 [1.459, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.799, 10.301], loss: 0.131146, mae: 0.345139, mean_q: 3.829119
  5800/100000: episode: 58, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.005, mean reward: 1.890 [1.457, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.192], loss: 0.123184, mae: 0.337740, mean_q: 3.867877
  5900/100000: episode: 59, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 194.333, mean reward: 1.943 [1.488, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.824, 10.307], loss: 0.119845, mae: 0.332682, mean_q: 3.881840
  6000/100000: episode: 60, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 203.174, mean reward: 2.032 [1.443, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.632, 10.098], loss: 0.136330, mae: 0.350754, mean_q: 3.937829
  6100/100000: episode: 61, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 199.154, mean reward: 1.992 [1.467, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.550, 10.098], loss: 0.125126, mae: 0.343103, mean_q: 3.954795
  6200/100000: episode: 62, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 195.524, mean reward: 1.955 [1.454, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.217, 10.295], loss: 0.113049, mae: 0.328929, mean_q: 3.945120
  6300/100000: episode: 63, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 205.264, mean reward: 2.053 [1.460, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.715, 10.098], loss: 0.117011, mae: 0.335087, mean_q: 3.935658
  6400/100000: episode: 64, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 192.507, mean reward: 1.925 [1.436, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.480, 10.169], loss: 0.117469, mae: 0.332004, mean_q: 3.941852
  6500/100000: episode: 65, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 191.343, mean reward: 1.913 [1.495, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.184], loss: 0.117712, mae: 0.328735, mean_q: 3.924413
  6600/100000: episode: 66, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 185.450, mean reward: 1.854 [1.463, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.098], loss: 0.128467, mae: 0.342910, mean_q: 3.929916
  6700/100000: episode: 67, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 184.140, mean reward: 1.841 [1.452, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.040, 10.242], loss: 0.124231, mae: 0.331381, mean_q: 3.922008
  6800/100000: episode: 68, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 178.911, mean reward: 1.789 [1.455, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.042, 10.236], loss: 0.129358, mae: 0.337749, mean_q: 3.917176
  6900/100000: episode: 69, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 190.956, mean reward: 1.910 [1.472, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.956, 10.098], loss: 0.110787, mae: 0.326785, mean_q: 3.924672
  7000/100000: episode: 70, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 175.222, mean reward: 1.752 [1.472, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.098], loss: 0.104869, mae: 0.319806, mean_q: 3.911304
  7100/100000: episode: 71, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 190.502, mean reward: 1.905 [1.441, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.832, 10.113], loss: 0.108032, mae: 0.324344, mean_q: 3.914966
  7200/100000: episode: 72, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 186.763, mean reward: 1.868 [1.472, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.244, 10.228], loss: 0.114846, mae: 0.325238, mean_q: 3.914981
  7300/100000: episode: 73, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 247.250, mean reward: 2.473 [1.500, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.782, 10.098], loss: 0.112243, mae: 0.318386, mean_q: 3.889943
  7400/100000: episode: 74, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 180.798, mean reward: 1.808 [1.458, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.067, 10.098], loss: 0.107412, mae: 0.323938, mean_q: 3.894338
  7500/100000: episode: 75, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 194.722, mean reward: 1.947 [1.496, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.685, 10.098], loss: 0.123844, mae: 0.330089, mean_q: 3.896536
  7600/100000: episode: 76, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 192.357, mean reward: 1.924 [1.454, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.587, 10.160], loss: 0.114723, mae: 0.330486, mean_q: 3.905391
  7700/100000: episode: 77, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 201.241, mean reward: 2.012 [1.562, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.098], loss: 0.125102, mae: 0.340158, mean_q: 3.920567
  7800/100000: episode: 78, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 192.085, mean reward: 1.921 [1.444, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.925, 10.186], loss: 0.129146, mae: 0.341741, mean_q: 3.919605
  7900/100000: episode: 79, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 190.824, mean reward: 1.908 [1.474, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.619, 10.098], loss: 0.109632, mae: 0.321618, mean_q: 3.903791
  8000/100000: episode: 80, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 193.508, mean reward: 1.935 [1.482, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.135, 10.194], loss: 0.126525, mae: 0.340143, mean_q: 3.913242
  8100/100000: episode: 81, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 186.046, mean reward: 1.860 [1.460, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.098], loss: 0.110044, mae: 0.320331, mean_q: 3.898376
  8200/100000: episode: 82, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.381, mean reward: 1.814 [1.447, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.508, 10.098], loss: 0.109745, mae: 0.326932, mean_q: 3.914541
  8300/100000: episode: 83, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 173.604, mean reward: 1.736 [1.445, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.185, 10.140], loss: 0.108383, mae: 0.321184, mean_q: 3.907452
  8400/100000: episode: 84, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 183.189, mean reward: 1.832 [1.429, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.948, 10.098], loss: 0.099987, mae: 0.316004, mean_q: 3.897152
  8500/100000: episode: 85, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 189.676, mean reward: 1.897 [1.475, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.001, 10.236], loss: 0.110114, mae: 0.315036, mean_q: 3.884542
  8600/100000: episode: 86, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 180.826, mean reward: 1.808 [1.458, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.039, 10.098], loss: 0.094373, mae: 0.306804, mean_q: 3.891680
  8700/100000: episode: 87, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 207.971, mean reward: 2.080 [1.469, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.602, 10.340], loss: 0.112591, mae: 0.324093, mean_q: 3.902594
  8800/100000: episode: 88, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 205.339, mean reward: 2.053 [1.454, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.369, 10.218], loss: 0.109162, mae: 0.322891, mean_q: 3.887079
  8900/100000: episode: 89, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 196.583, mean reward: 1.966 [1.514, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.422, 10.098], loss: 0.109610, mae: 0.318484, mean_q: 3.892969
  9000/100000: episode: 90, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 189.082, mean reward: 1.891 [1.436, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.123], loss: 0.106689, mae: 0.314830, mean_q: 3.872112
  9100/100000: episode: 91, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 176.412, mean reward: 1.764 [1.443, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.001, 10.162], loss: 0.102595, mae: 0.313871, mean_q: 3.868009
  9200/100000: episode: 92, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 193.655, mean reward: 1.937 [1.462, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.860, 10.159], loss: 0.087306, mae: 0.294769, mean_q: 3.859030
  9300/100000: episode: 93, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 188.415, mean reward: 1.884 [1.473, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.853, 10.098], loss: 0.090863, mae: 0.300186, mean_q: 3.862558
  9400/100000: episode: 94, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 198.248, mean reward: 1.982 [1.451, 5.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.098], loss: 0.084288, mae: 0.301692, mean_q: 3.869073
  9500/100000: episode: 95, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 186.067, mean reward: 1.861 [1.444, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.653, 10.446], loss: 0.109085, mae: 0.319125, mean_q: 3.882576
  9600/100000: episode: 96, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 198.034, mean reward: 1.980 [1.462, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.572, 10.098], loss: 0.089174, mae: 0.296589, mean_q: 3.860248
  9700/100000: episode: 97, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 203.883, mean reward: 2.039 [1.478, 4.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.098], loss: 0.090386, mae: 0.308158, mean_q: 3.876456
  9800/100000: episode: 98, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 203.558, mean reward: 2.036 [1.434, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.277, 10.266], loss: 0.102029, mae: 0.305677, mean_q: 3.876355
  9900/100000: episode: 99, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 197.275, mean reward: 1.973 [1.473, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.464, 10.300], loss: 0.096008, mae: 0.297359, mean_q: 3.851584
 10000/100000: episode: 100, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 186.886, mean reward: 1.869 [1.453, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.098], loss: 0.093008, mae: 0.298543, mean_q: 3.860228
 10100/100000: episode: 101, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 189.519, mean reward: 1.895 [1.477, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.186, 10.190], loss: 0.108444, mae: 0.307065, mean_q: 3.860755
 10200/100000: episode: 102, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 180.599, mean reward: 1.806 [1.496, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.094, 10.098], loss: 0.098360, mae: 0.292558, mean_q: 3.835834
 10300/100000: episode: 103, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 183.724, mean reward: 1.837 [1.453, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.304, 10.200], loss: 0.094254, mae: 0.300344, mean_q: 3.837299
 10400/100000: episode: 104, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 213.592, mean reward: 2.136 [1.468, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.551, 10.350], loss: 0.075085, mae: 0.279710, mean_q: 3.812270
 10500/100000: episode: 105, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 187.555, mean reward: 1.876 [1.443, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.143, 10.098], loss: 0.077188, mae: 0.284089, mean_q: 3.816121
 10600/100000: episode: 106, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 194.312, mean reward: 1.943 [1.472, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.023, 10.183], loss: 0.087724, mae: 0.292193, mean_q: 3.810922
 10700/100000: episode: 107, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.091, mean reward: 1.881 [1.465, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.094, 10.098], loss: 0.075443, mae: 0.286373, mean_q: 3.811880
 10800/100000: episode: 108, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 196.973, mean reward: 1.970 [1.443, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.273, 10.332], loss: 0.072950, mae: 0.280855, mean_q: 3.793474
 10900/100000: episode: 109, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 180.503, mean reward: 1.805 [1.468, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.647, 10.098], loss: 0.080963, mae: 0.286979, mean_q: 3.798111
 11000/100000: episode: 110, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 189.368, mean reward: 1.894 [1.471, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.098], loss: 0.078552, mae: 0.286235, mean_q: 3.795092
 11100/100000: episode: 111, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.085, mean reward: 1.911 [1.461, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.084, 10.209], loss: 0.074394, mae: 0.278737, mean_q: 3.798956
 11200/100000: episode: 112, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 207.626, mean reward: 2.076 [1.478, 5.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.576, 10.098], loss: 0.080518, mae: 0.287423, mean_q: 3.800479
 11300/100000: episode: 113, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 208.162, mean reward: 2.082 [1.501, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.065, 10.098], loss: 0.076548, mae: 0.282477, mean_q: 3.801722
 11400/100000: episode: 114, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 192.619, mean reward: 1.926 [1.468, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.061, 10.098], loss: 0.073879, mae: 0.278052, mean_q: 3.795931
 11500/100000: episode: 115, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 229.200, mean reward: 2.292 [1.531, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.627, 10.098], loss: 0.075711, mae: 0.284431, mean_q: 3.809851
 11600/100000: episode: 116, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.133, mean reward: 1.931 [1.469, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.868, 10.098], loss: 0.076492, mae: 0.283624, mean_q: 3.813041
 11700/100000: episode: 117, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 212.301, mean reward: 2.123 [1.536, 6.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.015, 10.283], loss: 0.077189, mae: 0.278920, mean_q: 3.802000
 11800/100000: episode: 118, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 214.589, mean reward: 2.146 [1.474, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.944, 10.098], loss: 0.083558, mae: 0.297148, mean_q: 3.838868
 11900/100000: episode: 119, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 179.661, mean reward: 1.797 [1.455, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.235, 10.260], loss: 0.079519, mae: 0.289648, mean_q: 3.838161
 12000/100000: episode: 120, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 190.827, mean reward: 1.908 [1.474, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.153, 10.189], loss: 0.077254, mae: 0.286121, mean_q: 3.850422
 12100/100000: episode: 121, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.358, mean reward: 1.934 [1.469, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.931, 10.126], loss: 0.084850, mae: 0.298623, mean_q: 3.844967
 12200/100000: episode: 122, duration: 0.651s, episode steps: 100, steps per second: 153, episode reward: 191.537, mean reward: 1.915 [1.453, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.976, 10.169], loss: 0.081975, mae: 0.293380, mean_q: 3.849769
 12300/100000: episode: 123, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 185.221, mean reward: 1.852 [1.494, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.660, 10.254], loss: 0.083339, mae: 0.292429, mean_q: 3.840628
 12400/100000: episode: 124, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 200.994, mean reward: 2.010 [1.484, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.098], loss: 0.083689, mae: 0.290383, mean_q: 3.830462
 12500/100000: episode: 125, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 197.954, mean reward: 1.980 [1.472, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.240, 10.112], loss: 0.081180, mae: 0.284702, mean_q: 3.827876
 12600/100000: episode: 126, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 205.022, mean reward: 2.050 [1.449, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.998, 10.098], loss: 0.086327, mae: 0.296877, mean_q: 3.849544
 12700/100000: episode: 127, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 201.242, mean reward: 2.012 [1.442, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.928, 10.200], loss: 0.076815, mae: 0.279445, mean_q: 3.807000
 12800/100000: episode: 128, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 202.575, mean reward: 2.026 [1.502, 8.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.144, 10.098], loss: 0.072138, mae: 0.284859, mean_q: 3.815089
 12900/100000: episode: 129, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 184.752, mean reward: 1.848 [1.467, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.672, 10.098], loss: 0.088931, mae: 0.297536, mean_q: 3.823276
 13000/100000: episode: 130, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 203.052, mean reward: 2.031 [1.488, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.248, 10.365], loss: 0.085213, mae: 0.289350, mean_q: 3.816815
 13100/100000: episode: 131, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 206.872, mean reward: 2.069 [1.453, 14.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.678, 10.098], loss: 0.097929, mae: 0.302935, mean_q: 3.839842
 13200/100000: episode: 132, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 195.034, mean reward: 1.950 [1.482, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.493, 10.098], loss: 0.140223, mae: 0.312215, mean_q: 3.838681
 13300/100000: episode: 133, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 185.213, mean reward: 1.852 [1.476, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.077, 10.184], loss: 0.112462, mae: 0.301364, mean_q: 3.846387
 13400/100000: episode: 134, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 183.295, mean reward: 1.833 [1.469, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.399, 10.106], loss: 0.147315, mae: 0.315804, mean_q: 3.877765
 13500/100000: episode: 135, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 190.505, mean reward: 1.905 [1.453, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.123, 10.284], loss: 0.090201, mae: 0.303715, mean_q: 3.864436
 13600/100000: episode: 136, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 197.749, mean reward: 1.977 [1.493, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.276], loss: 0.109696, mae: 0.302893, mean_q: 3.867767
 13700/100000: episode: 137, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 192.336, mean reward: 1.923 [1.475, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.098], loss: 0.139169, mae: 0.319985, mean_q: 3.883827
 13800/100000: episode: 138, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 183.904, mean reward: 1.839 [1.459, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.170, 10.246], loss: 0.081115, mae: 0.296198, mean_q: 3.855369
 13900/100000: episode: 139, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 184.602, mean reward: 1.846 [1.456, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.171], loss: 0.124967, mae: 0.310371, mean_q: 3.864388
 14000/100000: episode: 140, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 192.774, mean reward: 1.928 [1.476, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.832, 10.257], loss: 0.087555, mae: 0.296369, mean_q: 3.852892
 14100/100000: episode: 141, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 182.217, mean reward: 1.822 [1.460, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.284, 10.098], loss: 0.161240, mae: 0.316026, mean_q: 3.853572
 14200/100000: episode: 142, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 178.917, mean reward: 1.789 [1.445, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.877, 10.183], loss: 0.110496, mae: 0.302358, mean_q: 3.857370
 14300/100000: episode: 143, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 195.657, mean reward: 1.957 [1.438, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.269, 10.098], loss: 0.088581, mae: 0.294978, mean_q: 3.840623
 14400/100000: episode: 144, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 199.742, mean reward: 1.997 [1.454, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.912, 10.336], loss: 0.114653, mae: 0.306326, mean_q: 3.846548
 14500/100000: episode: 145, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 197.468, mean reward: 1.975 [1.476, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.846, 10.098], loss: 0.122836, mae: 0.315634, mean_q: 3.866359
 14600/100000: episode: 146, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 205.211, mean reward: 2.052 [1.446, 9.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.917, 10.098], loss: 0.093970, mae: 0.301962, mean_q: 3.842753
 14700/100000: episode: 147, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 214.822, mean reward: 2.148 [1.452, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.155, 10.098], loss: 0.093779, mae: 0.298000, mean_q: 3.844921
 14800/100000: episode: 148, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 194.753, mean reward: 1.948 [1.456, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.305], loss: 0.098904, mae: 0.303343, mean_q: 3.859571
 14900/100000: episode: 149, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 188.707, mean reward: 1.887 [1.469, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.478, 10.108], loss: 0.092548, mae: 0.298837, mean_q: 3.859565
[Info] 1-TH LEVEL FOUND: 4.345518112182617, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.177s, episode steps: 100, steps per second: 19, episode reward: 195.871, mean reward: 1.959 [1.457, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.200], loss: 0.125500, mae: 0.310822, mean_q: 3.850779
 15097/100000: episode: 151, duration: 0.549s, episode steps: 97, steps per second: 177, episode reward: 183.210, mean reward: 1.889 [1.448, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.300, 10.100], loss: 0.111323, mae: 0.301156, mean_q: 3.846321
 15135/100000: episode: 152, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 75.440, mean reward: 1.985 [1.494, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.265, 10.100], loss: 0.109624, mae: 0.299227, mean_q: 3.838128
 15232/100000: episode: 153, duration: 0.614s, episode steps: 97, steps per second: 158, episode reward: 171.211, mean reward: 1.765 [1.455, 2.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-1.291, 10.202], loss: 0.116874, mae: 0.306933, mean_q: 3.859311
 15266/100000: episode: 154, duration: 0.237s, episode steps: 34, steps per second: 144, episode reward: 62.143, mean reward: 1.828 [1.453, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.413, 10.125], loss: 0.095575, mae: 0.304801, mean_q: 3.847394
 15301/100000: episode: 155, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 68.456, mean reward: 1.956 [1.499, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.062, 10.100], loss: 0.189643, mae: 0.335878, mean_q: 3.873204
 15339/100000: episode: 156, duration: 0.248s, episode steps: 38, steps per second: 154, episode reward: 65.838, mean reward: 1.733 [1.491, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.134, 10.168], loss: 0.106104, mae: 0.314358, mean_q: 3.861585
 15345/100000: episode: 157, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 15.460, mean reward: 2.577 [1.927, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.189, 10.100], loss: 0.110157, mae: 0.335885, mean_q: 3.884151
 15385/100000: episode: 158, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 88.160, mean reward: 2.204 [1.496, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-2.084, 10.100], loss: 0.079070, mae: 0.286072, mean_q: 3.804215
 15423/100000: episode: 159, duration: 0.270s, episode steps: 38, steps per second: 141, episode reward: 72.517, mean reward: 1.908 [1.539, 2.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.906, 10.100], loss: 0.108269, mae: 0.306311, mean_q: 3.838060
 15463/100000: episode: 160, duration: 0.249s, episode steps: 40, steps per second: 161, episode reward: 78.510, mean reward: 1.963 [1.655, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.791, 10.158], loss: 0.085049, mae: 0.293192, mean_q: 3.844387
 15471/100000: episode: 161, duration: 0.063s, episode steps: 8, steps per second: 128, episode reward: 19.683, mean reward: 2.460 [1.802, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.301, 10.100], loss: 0.080673, mae: 0.279247, mean_q: 3.755120
 15509/100000: episode: 162, duration: 0.235s, episode steps: 38, steps per second: 162, episode reward: 66.003, mean reward: 1.737 [1.461, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.692, 10.100], loss: 0.112152, mae: 0.299010, mean_q: 3.830176
 15537/100000: episode: 163, duration: 0.171s, episode steps: 28, steps per second: 163, episode reward: 68.016, mean reward: 2.429 [1.781, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.093, 10.299], loss: 0.110732, mae: 0.318377, mean_q: 3.858944
 15575/100000: episode: 164, duration: 0.240s, episode steps: 38, steps per second: 158, episode reward: 71.302, mean reward: 1.876 [1.494, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.186, 10.100], loss: 0.087081, mae: 0.294447, mean_q: 3.831680
 15611/100000: episode: 165, duration: 0.232s, episode steps: 36, steps per second: 155, episode reward: 92.757, mean reward: 2.577 [1.930, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.464], loss: 0.095725, mae: 0.299563, mean_q: 3.868618
 15708/100000: episode: 166, duration: 0.589s, episode steps: 97, steps per second: 165, episode reward: 178.231, mean reward: 1.837 [1.466, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.639, 10.385], loss: 0.113435, mae: 0.322624, mean_q: 3.870743
 15746/100000: episode: 167, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 73.283, mean reward: 1.929 [1.490, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.465, 10.100], loss: 0.101074, mae: 0.301646, mean_q: 3.835942
 15786/100000: episode: 168, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 79.307, mean reward: 1.983 [1.472, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.488, 10.100], loss: 0.099774, mae: 0.306042, mean_q: 3.853665
 15824/100000: episode: 169, duration: 0.270s, episode steps: 38, steps per second: 141, episode reward: 68.032, mean reward: 1.790 [1.453, 2.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.117, 10.164], loss: 0.169173, mae: 0.331413, mean_q: 3.862899
 15832/100000: episode: 170, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 18.407, mean reward: 2.301 [1.791, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.463, 10.100], loss: 0.105285, mae: 0.329061, mean_q: 3.839363
 15872/100000: episode: 171, duration: 0.258s, episode steps: 40, steps per second: 155, episode reward: 87.919, mean reward: 2.198 [1.456, 15.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.740, 10.175], loss: 0.117082, mae: 0.325331, mean_q: 3.857474
 15910/100000: episode: 172, duration: 0.336s, episode steps: 38, steps per second: 113, episode reward: 78.435, mean reward: 2.064 [1.511, 5.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.637, 10.111], loss: 0.180052, mae: 0.319637, mean_q: 3.848601
 15948/100000: episode: 173, duration: 0.248s, episode steps: 38, steps per second: 153, episode reward: 68.756, mean reward: 1.809 [1.500, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.151, 10.100], loss: 0.090071, mae: 0.299644, mean_q: 3.851465
 15956/100000: episode: 174, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 15.631, mean reward: 1.954 [1.738, 2.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.214, 10.100], loss: 0.095532, mae: 0.285453, mean_q: 3.857435
 15991/100000: episode: 175, duration: 0.239s, episode steps: 35, steps per second: 146, episode reward: 72.952, mean reward: 2.084 [1.541, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.647, 10.100], loss: 0.089138, mae: 0.300977, mean_q: 3.848035
 16026/100000: episode: 176, duration: 0.231s, episode steps: 35, steps per second: 152, episode reward: 71.207, mean reward: 2.034 [1.565, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.307, 10.253], loss: 0.115401, mae: 0.321701, mean_q: 3.857122
 16061/100000: episode: 177, duration: 0.250s, episode steps: 35, steps per second: 140, episode reward: 72.652, mean reward: 2.076 [1.529, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.927, 10.100], loss: 0.105025, mae: 0.328100, mean_q: 3.892832
 16097/100000: episode: 178, duration: 0.273s, episode steps: 36, steps per second: 132, episode reward: 82.237, mean reward: 2.284 [1.562, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.113, 10.209], loss: 0.093447, mae: 0.296700, mean_q: 3.860179
 16103/100000: episode: 179, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 15.673, mean reward: 2.612 [2.268, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.190, 10.100], loss: 0.246098, mae: 0.369200, mean_q: 3.901827
 16138/100000: episode: 180, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 70.251, mean reward: 2.007 [1.539, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.672, 10.182], loss: 0.099767, mae: 0.315674, mean_q: 3.848411
 16174/100000: episode: 181, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 69.790, mean reward: 1.939 [1.474, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-1.090, 10.112], loss: 0.077404, mae: 0.289354, mean_q: 3.798693
 16271/100000: episode: 182, duration: 0.530s, episode steps: 97, steps per second: 183, episode reward: 188.904, mean reward: 1.947 [1.450, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.763, 10.100], loss: 0.178956, mae: 0.330775, mean_q: 3.854253
 16306/100000: episode: 183, duration: 0.274s, episode steps: 35, steps per second: 128, episode reward: 68.581, mean reward: 1.959 [1.521, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.180], loss: 0.103990, mae: 0.319693, mean_q: 3.868915
 16340/100000: episode: 184, duration: 0.208s, episode steps: 34, steps per second: 163, episode reward: 66.697, mean reward: 1.962 [1.432, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.258, 10.200], loss: 0.085130, mae: 0.297900, mean_q: 3.855403
 16368/100000: episode: 185, duration: 0.191s, episode steps: 28, steps per second: 146, episode reward: 63.906, mean reward: 2.282 [1.649, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.526, 10.256], loss: 0.247888, mae: 0.369115, mean_q: 3.872926
 16408/100000: episode: 186, duration: 0.272s, episode steps: 40, steps per second: 147, episode reward: 80.723, mean reward: 2.018 [1.504, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.271, 10.367], loss: 0.165045, mae: 0.306781, mean_q: 3.839524
 16448/100000: episode: 187, duration: 0.223s, episode steps: 40, steps per second: 180, episode reward: 81.217, mean reward: 2.030 [1.582, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.263, 10.100], loss: 0.172328, mae: 0.315549, mean_q: 3.844471
 16456/100000: episode: 188, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 14.609, mean reward: 1.826 [1.682, 2.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.270, 10.100], loss: 0.126992, mae: 0.346936, mean_q: 3.801845
 16490/100000: episode: 189, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 90.570, mean reward: 2.664 [1.548, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.337, 10.491], loss: 0.170254, mae: 0.349866, mean_q: 3.834057
 16526/100000: episode: 190, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 74.457, mean reward: 2.068 [1.509, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.035, 10.100], loss: 0.141273, mae: 0.336455, mean_q: 3.855114
 16564/100000: episode: 191, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 69.678, mean reward: 1.834 [1.506, 2.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.808, 10.201], loss: 0.161957, mae: 0.316700, mean_q: 3.843317
 16572/100000: episode: 192, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 19.536, mean reward: 2.442 [1.791, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.242, 10.100], loss: 0.066320, mae: 0.261079, mean_q: 3.735727
 16607/100000: episode: 193, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 75.765, mean reward: 2.165 [1.546, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.077, 10.183], loss: 0.179873, mae: 0.330544, mean_q: 3.834192
 16613/100000: episode: 194, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 16.882, mean reward: 2.814 [2.442, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.261, 10.100], loss: 0.082216, mae: 0.305065, mean_q: 3.898755
 16649/100000: episode: 195, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 70.432, mean reward: 1.956 [1.522, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.475, 10.100], loss: 0.092435, mae: 0.300979, mean_q: 3.815111
 16655/100000: episode: 196, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 16.139, mean reward: 2.690 [2.300, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.231, 10.100], loss: 0.169898, mae: 0.314012, mean_q: 3.843568
 16691/100000: episode: 197, duration: 0.196s, episode steps: 36, steps per second: 183, episode reward: 76.448, mean reward: 2.124 [1.647, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.575, 10.100], loss: 0.163607, mae: 0.308644, mean_q: 3.846887
 16699/100000: episode: 198, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 15.437, mean reward: 1.930 [1.796, 2.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.060, 10.100], loss: 0.099805, mae: 0.315610, mean_q: 3.867023
 16737/100000: episode: 199, duration: 0.226s, episode steps: 38, steps per second: 168, episode reward: 76.629, mean reward: 2.017 [1.508, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.634, 10.100], loss: 0.105336, mae: 0.318702, mean_q: 3.861183
 16775/100000: episode: 200, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 75.472, mean reward: 1.986 [1.443, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.611, 10.100], loss: 0.165324, mae: 0.337136, mean_q: 3.861199
 16781/100000: episode: 201, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 14.700, mean reward: 2.450 [2.207, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.315, 10.100], loss: 0.073177, mae: 0.281775, mean_q: 3.812743
 16815/100000: episode: 202, duration: 0.217s, episode steps: 34, steps per second: 157, episode reward: 66.386, mean reward: 1.953 [1.459, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.073, 10.243], loss: 0.158551, mae: 0.303620, mean_q: 3.847917
 16823/100000: episode: 203, duration: 0.062s, episode steps: 8, steps per second: 129, episode reward: 18.272, mean reward: 2.284 [1.887, 2.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.258, 10.100], loss: 0.193269, mae: 0.338338, mean_q: 3.853937
 16851/100000: episode: 204, duration: 0.177s, episode steps: 28, steps per second: 158, episode reward: 63.816, mean reward: 2.279 [1.616, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.748, 10.260], loss: 0.202605, mae: 0.322995, mean_q: 3.857736
 16948/100000: episode: 205, duration: 0.537s, episode steps: 97, steps per second: 181, episode reward: 212.821, mean reward: 2.194 [1.471, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-1.107, 10.100], loss: 0.097370, mae: 0.305880, mean_q: 3.851578
 16983/100000: episode: 206, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 71.936, mean reward: 2.055 [1.707, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.289], loss: 0.186802, mae: 0.326083, mean_q: 3.899072
 17023/100000: episode: 207, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 79.441, mean reward: 1.986 [1.494, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.485, 10.446], loss: 0.106748, mae: 0.305571, mean_q: 3.854659
 17059/100000: episode: 208, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 92.583, mean reward: 2.572 [1.990, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.445, 10.414], loss: 0.121260, mae: 0.320812, mean_q: 3.877558
 17094/100000: episode: 209, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 77.242, mean reward: 2.207 [1.669, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.374, 10.332], loss: 0.169368, mae: 0.339277, mean_q: 3.877792
 17132/100000: episode: 210, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 83.319, mean reward: 2.193 [1.638, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.216, 10.353], loss: 0.084573, mae: 0.292499, mean_q: 3.809488
 17138/100000: episode: 211, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 16.833, mean reward: 2.806 [2.255, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.241, 10.100], loss: 0.058456, mae: 0.262888, mean_q: 3.875630
 17173/100000: episode: 212, duration: 0.192s, episode steps: 35, steps per second: 183, episode reward: 85.685, mean reward: 2.448 [1.715, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.945, 10.371], loss: 0.132577, mae: 0.329487, mean_q: 3.897862
 17181/100000: episode: 213, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 19.823, mean reward: 2.478 [1.830, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.258, 10.100], loss: 0.116062, mae: 0.347227, mean_q: 3.863106
 17221/100000: episode: 214, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 92.521, mean reward: 2.313 [1.610, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.616, 10.235], loss: 0.091544, mae: 0.317427, mean_q: 3.853238
 17249/100000: episode: 215, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 62.017, mean reward: 2.215 [1.650, 6.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.698, 10.236], loss: 0.223080, mae: 0.349301, mean_q: 3.912650
 17346/100000: episode: 216, duration: 0.587s, episode steps: 97, steps per second: 165, episode reward: 181.016, mean reward: 1.866 [1.453, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.776, 10.160], loss: 0.203079, mae: 0.344884, mean_q: 3.890651
 17384/100000: episode: 217, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 75.901, mean reward: 1.997 [1.537, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.297, 10.323], loss: 0.156562, mae: 0.336262, mean_q: 3.900960
 17390/100000: episode: 218, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 15.471, mean reward: 2.578 [2.373, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.224, 10.100], loss: 0.092931, mae: 0.310375, mean_q: 3.954276
 17396/100000: episode: 219, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 14.606, mean reward: 2.434 [2.162, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.261, 10.100], loss: 0.092786, mae: 0.285345, mean_q: 3.861337
 17493/100000: episode: 220, duration: 0.562s, episode steps: 97, steps per second: 173, episode reward: 200.715, mean reward: 2.069 [1.467, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.984, 10.113], loss: 0.117032, mae: 0.318818, mean_q: 3.895902
 17531/100000: episode: 221, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 68.539, mean reward: 1.804 [1.475, 2.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.625, 10.118], loss: 0.194588, mae: 0.337279, mean_q: 3.916519
 17628/100000: episode: 222, duration: 0.529s, episode steps: 97, steps per second: 183, episode reward: 182.102, mean reward: 1.877 [1.507, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.835, 10.271], loss: 0.157968, mae: 0.325607, mean_q: 3.891128
 17656/100000: episode: 223, duration: 0.240s, episode steps: 28, steps per second: 117, episode reward: 70.522, mean reward: 2.519 [1.833, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.541, 10.377], loss: 0.135461, mae: 0.338328, mean_q: 3.922836
 17696/100000: episode: 224, duration: 0.302s, episode steps: 40, steps per second: 132, episode reward: 81.696, mean reward: 2.042 [1.489, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.940, 10.100], loss: 0.089911, mae: 0.309058, mean_q: 3.865200
 17732/100000: episode: 225, duration: 0.248s, episode steps: 36, steps per second: 145, episode reward: 71.520, mean reward: 1.987 [1.559, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.278, 10.231], loss: 0.100851, mae: 0.312837, mean_q: 3.865601
 17829/100000: episode: 226, duration: 0.621s, episode steps: 97, steps per second: 156, episode reward: 189.043, mean reward: 1.949 [1.478, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.473, 10.100], loss: 0.141143, mae: 0.309606, mean_q: 3.871067
 17863/100000: episode: 227, duration: 0.353s, episode steps: 34, steps per second: 96, episode reward: 83.209, mean reward: 2.447 [1.818, 6.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.685, 10.414], loss: 0.099322, mae: 0.310102, mean_q: 3.857819
 17891/100000: episode: 228, duration: 0.195s, episode steps: 28, steps per second: 143, episode reward: 65.820, mean reward: 2.351 [1.881, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.417, 10.337], loss: 0.126970, mae: 0.333803, mean_q: 3.919395
 17988/100000: episode: 229, duration: 0.959s, episode steps: 97, steps per second: 101, episode reward: 192.280, mean reward: 1.982 [1.450, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-1.206, 10.100], loss: 0.125525, mae: 0.308849, mean_q: 3.874629
 18016/100000: episode: 230, duration: 0.181s, episode steps: 28, steps per second: 155, episode reward: 61.096, mean reward: 2.182 [1.740, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.059, 10.319], loss: 0.192139, mae: 0.329124, mean_q: 3.917910
 18044/100000: episode: 231, duration: 0.189s, episode steps: 28, steps per second: 148, episode reward: 63.833, mean reward: 2.280 [1.564, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.870, 10.189], loss: 0.094524, mae: 0.319045, mean_q: 3.900316
 18084/100000: episode: 232, duration: 0.295s, episode steps: 40, steps per second: 136, episode reward: 72.756, mean reward: 1.819 [1.461, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.170, 10.215], loss: 0.239275, mae: 0.340881, mean_q: 3.902538
 18090/100000: episode: 233, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 18.934, mean reward: 3.156 [2.907, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.228, 10.100], loss: 0.065901, mae: 0.263744, mean_q: 3.816500
 18128/100000: episode: 234, duration: 0.222s, episode steps: 38, steps per second: 171, episode reward: 67.487, mean reward: 1.776 [1.434, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.101, 10.100], loss: 0.110043, mae: 0.333207, mean_q: 3.894696
 18136/100000: episode: 235, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 16.086, mean reward: 2.011 [1.790, 2.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.169, 10.100], loss: 0.094807, mae: 0.282414, mean_q: 3.825749
 18174/100000: episode: 236, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 70.847, mean reward: 1.864 [1.491, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.418, 10.100], loss: 0.110631, mae: 0.305786, mean_q: 3.876184
 18212/100000: episode: 237, duration: 0.327s, episode steps: 38, steps per second: 116, episode reward: 75.100, mean reward: 1.976 [1.477, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.574, 10.249], loss: 0.160740, mae: 0.323190, mean_q: 3.899688
 18250/100000: episode: 238, duration: 0.307s, episode steps: 38, steps per second: 124, episode reward: 70.005, mean reward: 1.842 [1.509, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.713, 10.100], loss: 0.099210, mae: 0.305841, mean_q: 3.870521
 18258/100000: episode: 239, duration: 0.078s, episode steps: 8, steps per second: 102, episode reward: 16.138, mean reward: 2.017 [1.668, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.263, 10.100], loss: 0.110719, mae: 0.330194, mean_q: 3.956602
[Info] 2-TH LEVEL FOUND: 5.433950424194336, Considering 11/89 traces
 18293/100000: episode: 240, duration: 4.807s, episode steps: 35, steps per second: 7, episode reward: 85.266, mean reward: 2.436 [1.860, 4.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.191, 10.372], loss: 0.092320, mae: 0.303652, mean_q: 3.857432
 18321/100000: episode: 241, duration: 0.233s, episode steps: 28, steps per second: 120, episode reward: 67.456, mean reward: 2.409 [1.880, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.497, 10.374], loss: 0.164471, mae: 0.338868, mean_q: 3.864809
 18349/100000: episode: 242, duration: 0.226s, episode steps: 28, steps per second: 124, episode reward: 62.988, mean reward: 2.250 [1.546, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.587, 10.174], loss: 0.102312, mae: 0.315575, mean_q: 3.908539
 18377/100000: episode: 243, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 55.988, mean reward: 2.000 [1.574, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.183], loss: 0.080438, mae: 0.294714, mean_q: 3.832264
 18405/100000: episode: 244, duration: 0.334s, episode steps: 28, steps per second: 84, episode reward: 66.813, mean reward: 2.386 [1.846, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.336], loss: 0.102273, mae: 0.312093, mean_q: 3.903230
 18433/100000: episode: 245, duration: 0.231s, episode steps: 28, steps per second: 121, episode reward: 66.224, mean reward: 2.365 [1.590, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.397, 10.100], loss: 0.112026, mae: 0.327674, mean_q: 3.926643
 18461/100000: episode: 246, duration: 0.293s, episode steps: 28, steps per second: 96, episode reward: 54.517, mean reward: 1.947 [1.588, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.327, 10.193], loss: 0.095761, mae: 0.303770, mean_q: 3.866797
 18489/100000: episode: 247, duration: 0.202s, episode steps: 28, steps per second: 139, episode reward: 69.665, mean reward: 2.488 [1.612, 12.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.405, 10.234], loss: 0.098217, mae: 0.314906, mean_q: 3.903870
 18517/100000: episode: 248, duration: 0.229s, episode steps: 28, steps per second: 122, episode reward: 63.493, mean reward: 2.268 [1.815, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.101, 10.251], loss: 0.145628, mae: 0.313738, mean_q: 3.913997
 18545/100000: episode: 249, duration: 0.195s, episode steps: 28, steps per second: 144, episode reward: 53.225, mean reward: 1.901 [1.503, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.010, 10.267], loss: 0.142295, mae: 0.317622, mean_q: 3.889545
 18573/100000: episode: 250, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 55.574, mean reward: 1.985 [1.492, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.325, 10.203], loss: 0.080789, mae: 0.285728, mean_q: 3.875572
 18601/100000: episode: 251, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 69.252, mean reward: 2.473 [1.501, 13.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.144, 10.217], loss: 0.074022, mae: 0.279402, mean_q: 3.899981
 18629/100000: episode: 252, duration: 0.185s, episode steps: 28, steps per second: 151, episode reward: 56.448, mean reward: 2.016 [1.594, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.170, 10.100], loss: 0.132315, mae: 0.331833, mean_q: 3.908036
 18657/100000: episode: 253, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 54.588, mean reward: 1.950 [1.548, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.185], loss: 0.244746, mae: 0.318691, mean_q: 3.934613
 18691/100000: episode: 254, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 67.366, mean reward: 1.981 [1.540, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.796, 10.100], loss: 0.156465, mae: 0.321423, mean_q: 3.924307
 18719/100000: episode: 255, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 78.008, mean reward: 2.786 [2.057, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.776, 10.442], loss: 0.096014, mae: 0.309723, mean_q: 3.923637
 18747/100000: episode: 256, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 60.889, mean reward: 2.175 [1.618, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.501, 10.208], loss: 0.103588, mae: 0.294687, mean_q: 3.929222
 18775/100000: episode: 257, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 61.124, mean reward: 2.183 [1.625, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.084, 10.255], loss: 0.185065, mae: 0.314686, mean_q: 3.973158
 18809/100000: episode: 258, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 68.561, mean reward: 2.016 [1.513, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.750, 10.100], loss: 0.221409, mae: 0.360505, mean_q: 3.928560
 18837/100000: episode: 259, duration: 0.177s, episode steps: 28, steps per second: 158, episode reward: 54.018, mean reward: 1.929 [1.518, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.175, 10.100], loss: 0.095939, mae: 0.310991, mean_q: 3.910803
 18865/100000: episode: 260, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 90.119, mean reward: 3.219 [2.032, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.469, 10.590], loss: 0.175705, mae: 0.345757, mean_q: 3.956158
 18893/100000: episode: 261, duration: 0.177s, episode steps: 28, steps per second: 159, episode reward: 63.692, mean reward: 2.275 [1.639, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.620, 10.188], loss: 0.184049, mae: 0.365220, mean_q: 3.996302
 18921/100000: episode: 262, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 53.850, mean reward: 1.923 [1.493, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.565, 10.100], loss: 0.128343, mae: 0.331520, mean_q: 3.956095
 18955/100000: episode: 263, duration: 0.227s, episode steps: 34, steps per second: 150, episode reward: 64.645, mean reward: 1.901 [1.538, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.266, 10.172], loss: 0.083031, mae: 0.295677, mean_q: 3.928041
 18983/100000: episode: 264, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 61.963, mean reward: 2.213 [1.532, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.272, 10.387], loss: 0.161111, mae: 0.339463, mean_q: 3.924947
 19011/100000: episode: 265, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 62.919, mean reward: 2.247 [1.735, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.368], loss: 0.099733, mae: 0.329754, mean_q: 3.953323
 19039/100000: episode: 266, duration: 0.214s, episode steps: 28, steps per second: 131, episode reward: 65.740, mean reward: 2.348 [1.527, 7.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.162, 10.100], loss: 0.099048, mae: 0.318068, mean_q: 3.921083
 19067/100000: episode: 267, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 56.321, mean reward: 2.011 [1.677, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.245], loss: 0.080968, mae: 0.282253, mean_q: 3.927175
 19095/100000: episode: 268, duration: 0.177s, episode steps: 28, steps per second: 158, episode reward: 49.038, mean reward: 1.751 [1.445, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.441, 10.174], loss: 0.107247, mae: 0.319851, mean_q: 3.979468
 19123/100000: episode: 269, duration: 0.182s, episode steps: 28, steps per second: 154, episode reward: 63.159, mean reward: 2.256 [1.586, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.514, 10.100], loss: 0.100632, mae: 0.327352, mean_q: 3.978726
 19151/100000: episode: 270, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 61.589, mean reward: 2.200 [1.776, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.395, 10.185], loss: 0.205553, mae: 0.348019, mean_q: 3.946809
 19185/100000: episode: 271, duration: 0.201s, episode steps: 34, steps per second: 170, episode reward: 64.372, mean reward: 1.893 [1.474, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.674, 10.173], loss: 0.243331, mae: 0.337047, mean_q: 3.996324
 19213/100000: episode: 272, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 85.068, mean reward: 3.038 [1.646, 18.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.029, 10.423], loss: 0.125982, mae: 0.345539, mean_q: 3.993260
 19241/100000: episode: 273, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 62.209, mean reward: 2.222 [1.733, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.344], loss: 0.266163, mae: 0.374280, mean_q: 3.958906
 19269/100000: episode: 274, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 62.934, mean reward: 2.248 [1.686, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.246, 10.357], loss: 0.144091, mae: 0.342188, mean_q: 3.985837
 19297/100000: episode: 275, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 55.845, mean reward: 1.994 [1.565, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.151, 10.100], loss: 0.189012, mae: 0.364411, mean_q: 3.996811
 19325/100000: episode: 276, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 58.488, mean reward: 2.089 [1.522, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.071, 10.165], loss: 0.162987, mae: 0.365901, mean_q: 3.992584
 19353/100000: episode: 277, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 60.671, mean reward: 2.167 [1.591, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.439, 10.444], loss: 0.108391, mae: 0.326470, mean_q: 3.960183
 19381/100000: episode: 278, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 57.539, mean reward: 2.055 [1.484, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.353, 10.100], loss: 0.174242, mae: 0.335380, mean_q: 3.998686
 19409/100000: episode: 279, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 59.005, mean reward: 2.107 [1.546, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.152, 10.184], loss: 0.214162, mae: 0.384650, mean_q: 3.981616
 19437/100000: episode: 280, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 58.715, mean reward: 2.097 [1.506, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.194, 10.104], loss: 0.218177, mae: 0.359722, mean_q: 4.045375
 19465/100000: episode: 281, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 56.460, mean reward: 2.016 [1.536, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.204, 10.252], loss: 0.512981, mae: 0.438327, mean_q: 4.050333
 19493/100000: episode: 282, duration: 0.178s, episode steps: 28, steps per second: 157, episode reward: 54.136, mean reward: 1.933 [1.490, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.279, 10.100], loss: 0.188619, mae: 0.385634, mean_q: 3.989615
 19521/100000: episode: 283, duration: 0.186s, episode steps: 28, steps per second: 150, episode reward: 54.572, mean reward: 1.949 [1.442, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.380, 10.102], loss: 0.132303, mae: 0.341339, mean_q: 3.990149
 19549/100000: episode: 284, duration: 0.181s, episode steps: 28, steps per second: 155, episode reward: 70.884, mean reward: 2.532 [1.917, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.591, 10.336], loss: 0.162126, mae: 0.335448, mean_q: 3.902789
 19577/100000: episode: 285, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 64.056, mean reward: 2.288 [1.869, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.329, 10.327], loss: 0.111214, mae: 0.320990, mean_q: 3.965615
 19605/100000: episode: 286, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 65.369, mean reward: 2.335 [1.586, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.219, 10.100], loss: 0.275076, mae: 0.401880, mean_q: 4.006873
 19633/100000: episode: 287, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 71.677, mean reward: 2.560 [1.627, 4.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.383, 10.194], loss: 0.437813, mae: 0.440903, mean_q: 4.020751
 19667/100000: episode: 288, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 85.866, mean reward: 2.525 [1.562, 8.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.289, 10.100], loss: 0.231853, mae: 0.336250, mean_q: 3.933678
 19695/100000: episode: 289, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 62.018, mean reward: 2.215 [1.583, 5.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.218, 10.262], loss: 0.197000, mae: 0.365243, mean_q: 3.961936
 19729/100000: episode: 290, duration: 0.222s, episode steps: 34, steps per second: 153, episode reward: 69.630, mean reward: 2.048 [1.529, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.035, 10.197], loss: 0.224566, mae: 0.364420, mean_q: 3.990391
 19757/100000: episode: 291, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 62.631, mean reward: 2.237 [1.530, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.055, 10.100], loss: 0.374721, mae: 0.365763, mean_q: 3.987405
 19785/100000: episode: 292, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 60.898, mean reward: 2.175 [1.549, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.576, 10.243], loss: 0.139970, mae: 0.364490, mean_q: 3.940561
 19813/100000: episode: 293, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 61.346, mean reward: 2.191 [1.483, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.339, 10.100], loss: 0.113080, mae: 0.330728, mean_q: 3.961082
 19841/100000: episode: 294, duration: 0.197s, episode steps: 28, steps per second: 142, episode reward: 55.703, mean reward: 1.989 [1.504, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.138, 10.115], loss: 0.114127, mae: 0.316067, mean_q: 3.957174
 19869/100000: episode: 295, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 77.991, mean reward: 2.785 [1.808, 5.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.127, 10.390], loss: 0.126320, mae: 0.345494, mean_q: 3.974717
 19897/100000: episode: 296, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 51.003, mean reward: 1.822 [1.472, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.226, 10.120], loss: 0.129425, mae: 0.325216, mean_q: 4.016144
 19925/100000: episode: 297, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 57.417, mean reward: 2.051 [1.726, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.153, 10.279], loss: 0.110622, mae: 0.333567, mean_q: 4.025635
 19953/100000: episode: 298, duration: 0.169s, episode steps: 28, steps per second: 166, episode reward: 52.992, mean reward: 1.893 [1.535, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.078, 10.106], loss: 0.113286, mae: 0.320732, mean_q: 3.970986
 19981/100000: episode: 299, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 63.043, mean reward: 2.252 [1.447, 7.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.509, 10.297], loss: 0.173820, mae: 0.345945, mean_q: 3.956829
 20009/100000: episode: 300, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 61.413, mean reward: 2.193 [1.560, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.124, 10.277], loss: 0.109139, mae: 0.331082, mean_q: 4.004677
 20043/100000: episode: 301, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 75.939, mean reward: 2.233 [1.637, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.107, 10.371], loss: 0.212118, mae: 0.373539, mean_q: 4.014444
 20071/100000: episode: 302, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 55.609, mean reward: 1.986 [1.487, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.224, 10.118], loss: 0.132724, mae: 0.337753, mean_q: 4.024886
 20099/100000: episode: 303, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 63.147, mean reward: 2.255 [1.579, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.214, 10.175], loss: 0.211558, mae: 0.372682, mean_q: 4.044880
 20127/100000: episode: 304, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 57.588, mean reward: 2.057 [1.549, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.517, 10.344], loss: 0.207385, mae: 0.396893, mean_q: 4.011907
 20155/100000: episode: 305, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 61.831, mean reward: 2.208 [1.835, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.428, 10.365], loss: 0.115929, mae: 0.342558, mean_q: 4.001121
 20183/100000: episode: 306, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 67.251, mean reward: 2.402 [1.660, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.089, 10.138], loss: 0.154994, mae: 0.355736, mean_q: 4.028213
 20211/100000: episode: 307, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 55.332, mean reward: 1.976 [1.529, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.386, 10.100], loss: 0.112439, mae: 0.317158, mean_q: 4.009058
 20239/100000: episode: 308, duration: 0.198s, episode steps: 28, steps per second: 142, episode reward: 63.675, mean reward: 2.274 [1.621, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.461, 10.210], loss: 0.179110, mae: 0.357568, mean_q: 4.053668
 20267/100000: episode: 309, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 71.049, mean reward: 2.537 [1.835, 5.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.209, 10.360], loss: 0.114655, mae: 0.334145, mean_q: 3.967449
 20295/100000: episode: 310, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 55.294, mean reward: 1.975 [1.526, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.649, 10.162], loss: 0.292979, mae: 0.361517, mean_q: 4.020127
 20323/100000: episode: 311, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 67.322, mean reward: 2.404 [1.695, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.533, 10.232], loss: 0.230076, mae: 0.348067, mean_q: 3.966841
 20351/100000: episode: 312, duration: 0.181s, episode steps: 28, steps per second: 154, episode reward: 53.912, mean reward: 1.925 [1.507, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.157, 10.152], loss: 0.166073, mae: 0.367529, mean_q: 4.059732
 20379/100000: episode: 313, duration: 0.184s, episode steps: 28, steps per second: 152, episode reward: 52.443, mean reward: 1.873 [1.560, 2.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.148, 10.177], loss: 0.277429, mae: 0.377101, mean_q: 4.031704
 20407/100000: episode: 314, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 62.012, mean reward: 2.215 [1.565, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.083, 10.100], loss: 0.271945, mae: 0.359574, mean_q: 4.004226
 20435/100000: episode: 315, duration: 0.178s, episode steps: 28, steps per second: 158, episode reward: 59.008, mean reward: 2.107 [1.593, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.793, 10.225], loss: 0.245248, mae: 0.376362, mean_q: 4.053008
 20463/100000: episode: 316, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 62.107, mean reward: 2.218 [1.466, 6.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.316, 10.129], loss: 0.183557, mae: 0.342361, mean_q: 4.002656
 20491/100000: episode: 317, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 58.463, mean reward: 2.088 [1.510, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.578, 10.100], loss: 0.133115, mae: 0.338291, mean_q: 4.027320
 20519/100000: episode: 318, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 54.156, mean reward: 1.934 [1.456, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.566, 10.100], loss: 0.186461, mae: 0.358184, mean_q: 4.040703
 20547/100000: episode: 319, duration: 0.165s, episode steps: 28, steps per second: 169, episode reward: 73.479, mean reward: 2.624 [1.926, 9.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.401, 10.293], loss: 0.276978, mae: 0.397662, mean_q: 4.075891
 20581/100000: episode: 320, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 71.410, mean reward: 2.100 [1.770, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.664, 10.240], loss: 0.105376, mae: 0.313997, mean_q: 4.007568
 20615/100000: episode: 321, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 73.998, mean reward: 2.176 [1.695, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.124, 10.285], loss: 0.124484, mae: 0.331438, mean_q: 4.062366
 20643/100000: episode: 322, duration: 0.203s, episode steps: 28, steps per second: 138, episode reward: 56.499, mean reward: 2.018 [1.528, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.266], loss: 0.176832, mae: 0.342918, mean_q: 4.087912
 20671/100000: episode: 323, duration: 0.371s, episode steps: 28, steps per second: 76, episode reward: 63.229, mean reward: 2.258 [1.702, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.238, 10.319], loss: 0.146030, mae: 0.347295, mean_q: 4.032328
 20699/100000: episode: 324, duration: 0.305s, episode steps: 28, steps per second: 92, episode reward: 52.786, mean reward: 1.885 [1.490, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.178], loss: 0.262840, mae: 0.372633, mean_q: 4.113530
 20727/100000: episode: 325, duration: 0.190s, episode steps: 28, steps per second: 147, episode reward: 57.936, mean reward: 2.069 [1.446, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.127, 10.143], loss: 0.144175, mae: 0.328243, mean_q: 4.049402
 20755/100000: episode: 326, duration: 0.223s, episode steps: 28, steps per second: 126, episode reward: 51.908, mean reward: 1.854 [1.508, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.524, 10.121], loss: 0.108925, mae: 0.306346, mean_q: 4.033732
 20783/100000: episode: 327, duration: 0.276s, episode steps: 28, steps per second: 102, episode reward: 62.665, mean reward: 2.238 [1.598, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.056, 10.252], loss: 0.154704, mae: 0.353169, mean_q: 4.045620
 20811/100000: episode: 328, duration: 0.212s, episode steps: 28, steps per second: 132, episode reward: 56.424, mean reward: 2.015 [1.664, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.708, 10.224], loss: 0.297224, mae: 0.372492, mean_q: 4.099630
[Info] 3-TH LEVEL FOUND: 7.252565383911133, Considering 90/10 traces
 20839/100000: episode: 329, duration: 4.670s, episode steps: 28, steps per second: 6, episode reward: 73.024, mean reward: 2.608 [1.900, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.378, 10.291], loss: 0.275853, mae: 0.384849, mean_q: 4.125123
 20867/100000: episode: 330, duration: 0.246s, episode steps: 28, steps per second: 114, episode reward: 64.444, mean reward: 2.302 [1.678, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.567, 10.263], loss: 0.296709, mae: 0.398157, mean_q: 4.069621
 20895/100000: episode: 331, duration: 0.198s, episode steps: 28, steps per second: 141, episode reward: 56.364, mean reward: 2.013 [1.509, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.363, 10.174], loss: 0.121898, mae: 0.337506, mean_q: 4.032851
 20928/100000: episode: 332, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 72.689, mean reward: 2.203 [1.510, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.025, 10.142], loss: 0.262584, mae: 0.408886, mean_q: 4.135420
 20956/100000: episode: 333, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 61.361, mean reward: 2.191 [1.494, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.143, 10.100], loss: 0.102261, mae: 0.304364, mean_q: 4.044800
 20984/100000: episode: 334, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 69.534, mean reward: 2.483 [1.693, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.687, 10.406], loss: 0.145153, mae: 0.328495, mean_q: 4.065546
 21012/100000: episode: 335, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 58.812, mean reward: 2.100 [1.576, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.471, 10.147], loss: 0.303711, mae: 0.412603, mean_q: 4.086725
 21040/100000: episode: 336, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 58.027, mean reward: 2.072 [1.544, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.872, 10.100], loss: 0.147992, mae: 0.357556, mean_q: 4.021093
 21068/100000: episode: 337, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 59.251, mean reward: 2.116 [1.566, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.428, 10.278], loss: 0.102206, mae: 0.329586, mean_q: 4.014962
 21096/100000: episode: 338, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 59.626, mean reward: 2.129 [1.586, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.895, 10.162], loss: 0.121995, mae: 0.326310, mean_q: 4.002564
[Info] NOT FOUND NEW LEVEL, Current Best Level is 7.252565383911133
 21124/100000: episode: 339, duration: 4.421s, episode steps: 28, steps per second: 6, episode reward: 68.388, mean reward: 2.442 [1.543, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.189], loss: 0.174926, mae: 0.353751, mean_q: 4.091962
 21224/100000: episode: 340, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 194.153, mean reward: 1.942 [1.465, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.234], loss: 0.162751, mae: 0.345073, mean_q: 4.057522
 21324/100000: episode: 341, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 201.538, mean reward: 2.015 [1.495, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.568, 10.273], loss: 0.205639, mae: 0.368083, mean_q: 4.091467
 21424/100000: episode: 342, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 199.939, mean reward: 1.999 [1.445, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.705, 10.331], loss: 0.142201, mae: 0.334764, mean_q: 4.047078
 21524/100000: episode: 343, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 195.129, mean reward: 1.951 [1.465, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.732, 10.214], loss: 0.226693, mae: 0.364790, mean_q: 4.058121
 21624/100000: episode: 344, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 182.984, mean reward: 1.830 [1.456, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.287, 10.098], loss: 0.143756, mae: 0.339837, mean_q: 4.065581
 21724/100000: episode: 345, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 181.259, mean reward: 1.813 [1.476, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.294, 10.098], loss: 0.128860, mae: 0.331994, mean_q: 4.042311
 21824/100000: episode: 346, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.448, mean reward: 1.954 [1.458, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.935, 10.195], loss: 0.160501, mae: 0.331458, mean_q: 4.046132
 21924/100000: episode: 347, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 185.931, mean reward: 1.859 [1.467, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.168], loss: 0.140820, mae: 0.337105, mean_q: 4.044952
 22024/100000: episode: 348, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 205.751, mean reward: 2.058 [1.467, 10.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.022, 10.098], loss: 0.159904, mae: 0.337815, mean_q: 4.030611
 22124/100000: episode: 349, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 181.573, mean reward: 1.816 [1.449, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.098], loss: 0.158420, mae: 0.331665, mean_q: 4.042695
 22224/100000: episode: 350, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.534, mean reward: 1.995 [1.499, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.568, 10.285], loss: 0.186581, mae: 0.331213, mean_q: 3.980000
 22324/100000: episode: 351, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 198.381, mean reward: 1.984 [1.475, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.666, 10.214], loss: 0.156811, mae: 0.336159, mean_q: 4.015623
 22424/100000: episode: 352, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.748, mean reward: 1.857 [1.434, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.668, 10.152], loss: 0.128412, mae: 0.314564, mean_q: 4.009512
 22524/100000: episode: 353, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.322, mean reward: 1.903 [1.460, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.210, 10.199], loss: 0.165994, mae: 0.319053, mean_q: 3.979884
 22624/100000: episode: 354, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.689, mean reward: 1.937 [1.457, 4.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.754, 10.098], loss: 0.194387, mae: 0.353049, mean_q: 4.009295
 22724/100000: episode: 355, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 191.050, mean reward: 1.911 [1.472, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.088, 10.098], loss: 0.132158, mae: 0.333570, mean_q: 4.003951
 22824/100000: episode: 356, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 190.010, mean reward: 1.900 [1.474, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.522, 10.237], loss: 0.231814, mae: 0.355178, mean_q: 4.008175
 22924/100000: episode: 357, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 212.018, mean reward: 2.120 [1.453, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.715, 10.368], loss: 0.265324, mae: 0.373208, mean_q: 3.998100
 23024/100000: episode: 358, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 186.386, mean reward: 1.864 [1.438, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.607, 10.098], loss: 0.190274, mae: 0.341599, mean_q: 4.009447
 23124/100000: episode: 359, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 203.275, mean reward: 2.033 [1.479, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.285, 10.121], loss: 0.144745, mae: 0.321711, mean_q: 3.986073
 23224/100000: episode: 360, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 200.304, mean reward: 2.003 [1.460, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.448, 10.098], loss: 0.255864, mae: 0.345787, mean_q: 3.973082
 23324/100000: episode: 361, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 186.692, mean reward: 1.867 [1.443, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.424, 10.121], loss: 0.113533, mae: 0.311461, mean_q: 3.984483
 23424/100000: episode: 362, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.111, mean reward: 1.931 [1.440, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.806, 10.195], loss: 0.157892, mae: 0.323694, mean_q: 3.963077
 23524/100000: episode: 363, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 176.795, mean reward: 1.768 [1.442, 2.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.234, 10.193], loss: 0.133732, mae: 0.322221, mean_q: 3.968117
 23624/100000: episode: 364, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 214.701, mean reward: 2.147 [1.440, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.995, 10.308], loss: 0.109256, mae: 0.306783, mean_q: 3.965894
 23724/100000: episode: 365, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 177.931, mean reward: 1.779 [1.459, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.445, 10.215], loss: 0.121625, mae: 0.306746, mean_q: 3.960683
 23824/100000: episode: 366, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 218.600, mean reward: 2.186 [1.460, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.362], loss: 0.103192, mae: 0.299262, mean_q: 3.946274
 23924/100000: episode: 367, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 213.010, mean reward: 2.130 [1.477, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.627, 10.098], loss: 0.155525, mae: 0.321899, mean_q: 3.971301
 24024/100000: episode: 368, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.081, mean reward: 1.851 [1.455, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.887, 10.106], loss: 0.147334, mae: 0.321324, mean_q: 3.954287
 24124/100000: episode: 369, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 204.151, mean reward: 2.042 [1.455, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.358, 10.098], loss: 0.118955, mae: 0.323005, mean_q: 3.950466
 24224/100000: episode: 370, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 213.579, mean reward: 2.136 [1.467, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.671, 10.258], loss: 0.098688, mae: 0.305242, mean_q: 3.927882
 24324/100000: episode: 371, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 177.899, mean reward: 1.779 [1.445, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.372, 10.132], loss: 0.096923, mae: 0.301299, mean_q: 3.923570
 24424/100000: episode: 372, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 184.269, mean reward: 1.843 [1.446, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.098], loss: 0.104092, mae: 0.294979, mean_q: 3.912555
 24524/100000: episode: 373, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 183.160, mean reward: 1.832 [1.448, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.229], loss: 0.100990, mae: 0.300334, mean_q: 3.931439
 24624/100000: episode: 374, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.261, mean reward: 2.013 [1.459, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.327, 10.200], loss: 0.117465, mae: 0.304323, mean_q: 3.906598
 24724/100000: episode: 375, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 196.492, mean reward: 1.965 [1.487, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.098], loss: 0.088256, mae: 0.287350, mean_q: 3.885899
 24824/100000: episode: 376, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 222.316, mean reward: 2.223 [1.442, 5.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.192, 10.167], loss: 0.107686, mae: 0.311096, mean_q: 3.915753
 24924/100000: episode: 377, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 211.130, mean reward: 2.111 [1.486, 8.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.851, 10.274], loss: 0.110260, mae: 0.306378, mean_q: 3.907221
 25024/100000: episode: 378, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 179.982, mean reward: 1.800 [1.436, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.197, 10.129], loss: 0.117207, mae: 0.304905, mean_q: 3.896759
 25124/100000: episode: 379, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 203.524, mean reward: 2.035 [1.517, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.051, 10.098], loss: 0.099473, mae: 0.295899, mean_q: 3.893754
 25224/100000: episode: 380, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 178.802, mean reward: 1.788 [1.461, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.717, 10.098], loss: 0.091099, mae: 0.292727, mean_q: 3.884810
 25324/100000: episode: 381, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 182.949, mean reward: 1.829 [1.455, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.009, 10.164], loss: 0.093177, mae: 0.286238, mean_q: 3.886304
 25424/100000: episode: 382, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.155, mean reward: 1.862 [1.503, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.923, 10.098], loss: 0.086567, mae: 0.289590, mean_q: 3.856658
 25524/100000: episode: 383, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 195.745, mean reward: 1.957 [1.509, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.656, 10.210], loss: 0.085913, mae: 0.285856, mean_q: 3.886009
 25624/100000: episode: 384, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 202.379, mean reward: 2.024 [1.520, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.929, 10.209], loss: 0.095495, mae: 0.285021, mean_q: 3.860881
 25724/100000: episode: 385, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 195.899, mean reward: 1.959 [1.452, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.715, 10.116], loss: 0.083186, mae: 0.288071, mean_q: 3.869408
 25824/100000: episode: 386, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 177.388, mean reward: 1.774 [1.458, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.132, 10.098], loss: 0.093942, mae: 0.281626, mean_q: 3.844811
 25924/100000: episode: 387, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 183.390, mean reward: 1.834 [1.464, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.503, 10.252], loss: 0.077555, mae: 0.275770, mean_q: 3.846290
 26024/100000: episode: 388, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 225.283, mean reward: 2.253 [1.468, 6.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.479], loss: 0.094728, mae: 0.285325, mean_q: 3.850172
 26124/100000: episode: 389, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.048, mean reward: 1.910 [1.447, 5.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.235, 10.217], loss: 0.096053, mae: 0.290135, mean_q: 3.852177
 26224/100000: episode: 390, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 194.832, mean reward: 1.948 [1.461, 7.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.793, 10.175], loss: 0.094685, mae: 0.289323, mean_q: 3.840301
 26324/100000: episode: 391, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 217.004, mean reward: 2.170 [1.466, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.020, 10.098], loss: 0.126532, mae: 0.303583, mean_q: 3.853351
 26424/100000: episode: 392, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.395, mean reward: 1.924 [1.486, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.079, 10.098], loss: 0.081096, mae: 0.276780, mean_q: 3.825131
 26524/100000: episode: 393, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.036, mean reward: 1.860 [1.453, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.098], loss: 0.095136, mae: 0.287925, mean_q: 3.847609
 26624/100000: episode: 394, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.557, mean reward: 1.886 [1.446, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.912, 10.098], loss: 0.112918, mae: 0.304689, mean_q: 3.866147
 26724/100000: episode: 395, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.967, mean reward: 1.790 [1.433, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.953, 10.098], loss: 0.087255, mae: 0.287594, mean_q: 3.844350
 26824/100000: episode: 396, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.837, mean reward: 1.878 [1.478, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.989, 10.220], loss: 0.109284, mae: 0.295044, mean_q: 3.849002
 26924/100000: episode: 397, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 198.682, mean reward: 1.987 [1.449, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.679, 10.098], loss: 0.093672, mae: 0.285916, mean_q: 3.828560
 27024/100000: episode: 398, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.335, mean reward: 1.943 [1.454, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.221, 10.157], loss: 0.092397, mae: 0.289220, mean_q: 3.840105
 27124/100000: episode: 399, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 235.312, mean reward: 2.353 [1.440, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.975, 10.098], loss: 0.094633, mae: 0.294201, mean_q: 3.857957
 27224/100000: episode: 400, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 198.471, mean reward: 1.985 [1.487, 4.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.280], loss: 0.094222, mae: 0.301992, mean_q: 3.862189
 27324/100000: episode: 401, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 201.443, mean reward: 2.014 [1.490, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.317, 10.098], loss: 0.098464, mae: 0.293082, mean_q: 3.883734
 27424/100000: episode: 402, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.946, mean reward: 1.809 [1.447, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.653, 10.179], loss: 0.106169, mae: 0.301484, mean_q: 3.867482
 27524/100000: episode: 403, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 213.148, mean reward: 2.131 [1.457, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.208, 10.098], loss: 0.101851, mae: 0.297339, mean_q: 3.885305
 27624/100000: episode: 404, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.201, mean reward: 1.962 [1.529, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.946, 10.098], loss: 0.101824, mae: 0.290647, mean_q: 3.875744
 27724/100000: episode: 405, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 197.534, mean reward: 1.975 [1.472, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.226], loss: 0.082297, mae: 0.280096, mean_q: 3.877435
 27824/100000: episode: 406, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.335, mean reward: 1.873 [1.433, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.040, 10.098], loss: 0.091741, mae: 0.287290, mean_q: 3.888276
 27924/100000: episode: 407, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 180.568, mean reward: 1.806 [1.452, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.165, 10.126], loss: 0.089993, mae: 0.285857, mean_q: 3.870871
 28024/100000: episode: 408, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.683, mean reward: 1.877 [1.445, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.221, 10.177], loss: 0.086927, mae: 0.281113, mean_q: 3.856805
 28124/100000: episode: 409, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 197.211, mean reward: 1.972 [1.452, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.697, 10.098], loss: 0.099475, mae: 0.287042, mean_q: 3.850719
 28224/100000: episode: 410, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.312, mean reward: 1.943 [1.470, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.430, 10.315], loss: 0.088577, mae: 0.285582, mean_q: 3.849570
 28324/100000: episode: 411, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 195.420, mean reward: 1.954 [1.435, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.319, 10.124], loss: 0.086972, mae: 0.290412, mean_q: 3.845553
 28424/100000: episode: 412, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 210.509, mean reward: 2.105 [1.490, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.098], loss: 0.084224, mae: 0.288130, mean_q: 3.860486
 28524/100000: episode: 413, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 188.532, mean reward: 1.885 [1.520, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.098], loss: 0.107577, mae: 0.300533, mean_q: 3.855096
 28624/100000: episode: 414, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 213.532, mean reward: 2.135 [1.558, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.007, 10.098], loss: 0.086661, mae: 0.284280, mean_q: 3.871244
 28724/100000: episode: 415, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.240, mean reward: 1.932 [1.461, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.475, 10.174], loss: 0.096657, mae: 0.291654, mean_q: 3.855930
 28824/100000: episode: 416, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.886, mean reward: 1.839 [1.469, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.676, 10.098], loss: 0.086922, mae: 0.277849, mean_q: 3.856521
 28924/100000: episode: 417, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.111, mean reward: 1.941 [1.439, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.121], loss: 0.082640, mae: 0.278642, mean_q: 3.835531
 29024/100000: episode: 418, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.856, mean reward: 1.829 [1.460, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.919, 10.155], loss: 0.092107, mae: 0.293801, mean_q: 3.863993
 29124/100000: episode: 419, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 205.437, mean reward: 2.054 [1.490, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.480, 10.098], loss: 0.085491, mae: 0.283374, mean_q: 3.851070
 29224/100000: episode: 420, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 206.402, mean reward: 2.064 [1.460, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.098], loss: 0.099626, mae: 0.298850, mean_q: 3.866361
 29324/100000: episode: 421, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 215.672, mean reward: 2.157 [1.446, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.098], loss: 0.107797, mae: 0.300704, mean_q: 3.868510
 29424/100000: episode: 422, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.235, mean reward: 1.922 [1.461, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.306], loss: 0.098966, mae: 0.291179, mean_q: 3.883148
 29524/100000: episode: 423, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 229.719, mean reward: 2.297 [1.481, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.723, 10.195], loss: 0.103287, mae: 0.302511, mean_q: 3.884029
 29624/100000: episode: 424, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 203.080, mean reward: 2.031 [1.450, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.098], loss: 0.104436, mae: 0.304910, mean_q: 3.886259
 29724/100000: episode: 425, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.121, mean reward: 1.841 [1.468, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.708, 10.098], loss: 0.094961, mae: 0.294923, mean_q: 3.881659
 29824/100000: episode: 426, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.255, mean reward: 1.973 [1.438, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.647, 10.437], loss: 0.099062, mae: 0.305022, mean_q: 3.886164
 29924/100000: episode: 427, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 192.523, mean reward: 1.925 [1.440, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.793, 10.098], loss: 0.100715, mae: 0.304230, mean_q: 3.882923
 30024/100000: episode: 428, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.343, mean reward: 1.933 [1.486, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.546, 10.098], loss: 0.078773, mae: 0.284189, mean_q: 3.876748
 30124/100000: episode: 429, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 192.785, mean reward: 1.928 [1.456, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.333, 10.098], loss: 0.102312, mae: 0.301557, mean_q: 3.871272
 30224/100000: episode: 430, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.381, mean reward: 1.954 [1.450, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.637, 10.346], loss: 0.085191, mae: 0.283163, mean_q: 3.889370
 30324/100000: episode: 431, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.531, mean reward: 1.885 [1.485, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.303, 10.144], loss: 0.088960, mae: 0.290843, mean_q: 3.862995
 30424/100000: episode: 432, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 184.636, mean reward: 1.846 [1.481, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.879, 10.104], loss: 0.075464, mae: 0.275951, mean_q: 3.880419
 30524/100000: episode: 433, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 204.226, mean reward: 2.042 [1.456, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.975, 10.158], loss: 0.081359, mae: 0.283033, mean_q: 3.875892
 30624/100000: episode: 434, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.742, mean reward: 1.947 [1.449, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.962, 10.271], loss: 0.079513, mae: 0.280914, mean_q: 3.862328
 30724/100000: episode: 435, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.175, mean reward: 1.922 [1.488, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.645, 10.098], loss: 0.083275, mae: 0.279944, mean_q: 3.857491
 30824/100000: episode: 436, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 195.092, mean reward: 1.951 [1.448, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.585, 10.184], loss: 0.084815, mae: 0.287573, mean_q: 3.869538
 30924/100000: episode: 437, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 200.378, mean reward: 2.004 [1.464, 5.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.052, 10.098], loss: 0.079863, mae: 0.279629, mean_q: 3.872537
 31024/100000: episode: 438, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 191.425, mean reward: 1.914 [1.489, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.052, 10.243], loss: 0.076070, mae: 0.277648, mean_q: 3.842523
[Info] 1-TH LEVEL FOUND: 5.720327854156494, Considering 10/90 traces
 31124/100000: episode: 439, duration: 4.550s, episode steps: 100, steps per second: 22, episode reward: 189.829, mean reward: 1.898 [1.460, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.856, 10.098], loss: 0.088266, mae: 0.294590, mean_q: 3.883565
 31153/100000: episode: 440, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 84.045, mean reward: 2.898 [2.268, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.184, 10.100], loss: 0.086770, mae: 0.297816, mean_q: 3.877820
 31168/100000: episode: 441, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 35.408, mean reward: 2.361 [1.939, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.349, 10.100], loss: 0.072410, mae: 0.281374, mean_q: 3.874094
 31263/100000: episode: 442, duration: 0.517s, episode steps: 95, steps per second: 184, episode reward: 184.551, mean reward: 1.943 [1.478, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-1.191, 10.324], loss: 0.085742, mae: 0.289615, mean_q: 3.894686
 31317/100000: episode: 443, duration: 0.279s, episode steps: 54, steps per second: 193, episode reward: 114.533, mean reward: 2.121 [1.475, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.715, 10.210], loss: 0.084369, mae: 0.293651, mean_q: 3.887959
 31371/100000: episode: 444, duration: 0.282s, episode steps: 54, steps per second: 192, episode reward: 123.637, mean reward: 2.290 [1.735, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.506, 10.188], loss: 0.079993, mae: 0.281716, mean_q: 3.871632
 31423/100000: episode: 445, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 121.518, mean reward: 2.337 [1.728, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.577, 10.466], loss: 0.092235, mae: 0.300005, mean_q: 3.896487
 31452/100000: episode: 446, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 116.360, mean reward: 4.012 [2.247, 11.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.774, 10.100], loss: 0.073042, mae: 0.274150, mean_q: 3.905647
 31470/100000: episode: 447, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 59.934, mean reward: 3.330 [2.493, 7.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.637], loss: 0.172628, mae: 0.337538, mean_q: 3.902955
 31521/100000: episode: 448, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 108.203, mean reward: 2.122 [1.758, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.763, 10.145], loss: 0.100024, mae: 0.309911, mean_q: 3.932132
 31545/100000: episode: 449, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 50.584, mean reward: 2.108 [1.756, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.245], loss: 0.093908, mae: 0.325370, mean_q: 3.975919
 31574/100000: episode: 450, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 115.978, mean reward: 3.999 [2.554, 6.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.396, 10.100], loss: 0.068936, mae: 0.265466, mean_q: 3.891144
 31603/100000: episode: 451, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 133.716, mean reward: 4.611 [2.140, 25.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.242, 10.100], loss: 0.098945, mae: 0.306259, mean_q: 3.945147
 31627/100000: episode: 452, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 57.554, mean reward: 2.398 [2.033, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.456, 10.389], loss: 0.110278, mae: 0.291877, mean_q: 3.935841
 31651/100000: episode: 453, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 50.315, mean reward: 2.096 [1.744, 2.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.275], loss: 0.098494, mae: 0.313366, mean_q: 3.944876
 31694/100000: episode: 454, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 122.359, mean reward: 2.846 [1.806, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-1.035, 10.100], loss: 0.096644, mae: 0.303118, mean_q: 3.967373
 31737/100000: episode: 455, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 164.574, mean reward: 3.827 [1.878, 19.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.846, 10.100], loss: 0.111058, mae: 0.325708, mean_q: 3.997240
 31832/100000: episode: 456, duration: 0.486s, episode steps: 95, steps per second: 196, episode reward: 181.871, mean reward: 1.914 [1.463, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-1.066, 10.210], loss: 0.265925, mae: 0.375746, mean_q: 4.030899
 31856/100000: episode: 457, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 63.146, mean reward: 2.631 [1.787, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.237, 10.282], loss: 0.519757, mae: 0.451030, mean_q: 4.106753
 31880/100000: episode: 458, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 71.312, mean reward: 2.971 [2.381, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.066, 10.339], loss: 0.182689, mae: 0.367314, mean_q: 4.012252
 31975/100000: episode: 459, duration: 0.496s, episode steps: 95, steps per second: 192, episode reward: 211.240, mean reward: 2.224 [1.552, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.646, 10.100], loss: 0.140779, mae: 0.338160, mean_q: 4.048689
 31999/100000: episode: 460, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 53.315, mean reward: 2.221 [1.920, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.436, 10.316], loss: 0.159034, mae: 0.383088, mean_q: 4.101259
 32050/100000: episode: 461, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 97.038, mean reward: 1.903 [1.493, 2.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.661, 10.100], loss: 0.188632, mae: 0.345565, mean_q: 4.062457
 32079/100000: episode: 462, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 81.678, mean reward: 2.816 [2.251, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.583, 10.100], loss: 0.104644, mae: 0.330862, mean_q: 4.035931
 32130/100000: episode: 463, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 117.260, mean reward: 2.299 [1.491, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.300, 10.100], loss: 0.112149, mae: 0.324078, mean_q: 4.045267
 32225/100000: episode: 464, duration: 0.486s, episode steps: 95, steps per second: 196, episode reward: 186.043, mean reward: 1.958 [1.467, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.503, 10.100], loss: 0.106733, mae: 0.315875, mean_q: 4.041165
 32277/100000: episode: 465, duration: 0.273s, episode steps: 52, steps per second: 191, episode reward: 134.370, mean reward: 2.584 [1.679, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.309, 10.443], loss: 0.162917, mae: 0.356427, mean_q: 4.048356
 32295/100000: episode: 466, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 59.391, mean reward: 3.299 [2.411, 6.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.407], loss: 0.123806, mae: 0.337601, mean_q: 4.029016
 32346/100000: episode: 467, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 121.566, mean reward: 2.384 [1.541, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.351, 10.100], loss: 0.128501, mae: 0.335911, mean_q: 4.067746
 32397/100000: episode: 468, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 121.035, mean reward: 2.373 [1.745, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.657, 10.324], loss: 0.179650, mae: 0.337864, mean_q: 4.105616
 32451/100000: episode: 469, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 146.584, mean reward: 2.715 [1.754, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.602, 10.408], loss: 0.139712, mae: 0.343598, mean_q: 4.107764
 32505/100000: episode: 470, duration: 0.281s, episode steps: 54, steps per second: 193, episode reward: 120.584, mean reward: 2.233 [1.500, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-1.134, 10.191], loss: 0.577512, mae: 0.454733, mean_q: 4.167353
 32531/100000: episode: 471, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 106.546, mean reward: 4.098 [2.894, 9.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.520, 10.501], loss: 0.132627, mae: 0.356733, mean_q: 4.052323
 32557/100000: episode: 472, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 71.968, mean reward: 2.768 [1.867, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.278], loss: 0.422995, mae: 0.444770, mean_q: 4.198824
 32609/100000: episode: 473, duration: 0.280s, episode steps: 52, steps per second: 186, episode reward: 124.743, mean reward: 2.399 [1.926, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.668, 10.305], loss: 0.138766, mae: 0.353699, mean_q: 4.175473
 32635/100000: episode: 474, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 83.678, mean reward: 3.218 [2.517, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.478, 10.517], loss: 0.304742, mae: 0.382131, mean_q: 4.209520
 32664/100000: episode: 475, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 80.321, mean reward: 2.770 [2.089, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.581, 10.100], loss: 0.354334, mae: 0.371011, mean_q: 4.149209
 32679/100000: episode: 476, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 43.851, mean reward: 2.923 [2.152, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.448, 10.100], loss: 0.616435, mae: 0.456593, mean_q: 4.313779
 32705/100000: episode: 477, duration: 0.128s, episode steps: 26, steps per second: 204, episode reward: 86.847, mean reward: 3.340 [1.638, 6.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.300, 10.237], loss: 0.115407, mae: 0.354787, mean_q: 4.104527
 32723/100000: episode: 478, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 51.555, mean reward: 2.864 [1.948, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.040, 10.353], loss: 0.112872, mae: 0.335220, mean_q: 4.130282
 32747/100000: episode: 479, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 68.095, mean reward: 2.837 [2.238, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.440, 10.416], loss: 0.177952, mae: 0.360494, mean_q: 4.239399
 32842/100000: episode: 480, duration: 0.484s, episode steps: 95, steps per second: 196, episode reward: 182.584, mean reward: 1.922 [1.439, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.529, 10.145], loss: 0.248422, mae: 0.382817, mean_q: 4.217319
 32860/100000: episode: 481, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 53.131, mean reward: 2.952 [2.240, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.456], loss: 0.119344, mae: 0.318218, mean_q: 4.118830
 32875/100000: episode: 482, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 45.541, mean reward: 3.036 [2.284, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.275, 10.100], loss: 0.555751, mae: 0.410177, mean_q: 4.251641
 32904/100000: episode: 483, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 101.886, mean reward: 3.513 [2.697, 5.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.600, 10.100], loss: 0.352162, mae: 0.440912, mean_q: 4.355034
 32958/100000: episode: 484, duration: 0.279s, episode steps: 54, steps per second: 193, episode reward: 126.486, mean reward: 2.342 [1.475, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.855, 10.100], loss: 0.135218, mae: 0.358110, mean_q: 4.226433
 33053/100000: episode: 485, duration: 0.495s, episode steps: 95, steps per second: 192, episode reward: 196.308, mean reward: 2.066 [1.464, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.465, 10.107], loss: 0.204599, mae: 0.364614, mean_q: 4.265476
 33071/100000: episode: 486, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 79.089, mean reward: 4.394 [2.837, 7.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.571, 10.456], loss: 0.134274, mae: 0.352904, mean_q: 4.308710
 33097/100000: episode: 487, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 88.027, mean reward: 3.386 [2.612, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.284, 10.590], loss: 0.115093, mae: 0.332125, mean_q: 4.240972
 33121/100000: episode: 488, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 84.785, mean reward: 3.533 [1.831, 7.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.324, 10.427], loss: 0.154124, mae: 0.367590, mean_q: 4.314620
 33150/100000: episode: 489, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 121.215, mean reward: 4.180 [2.382, 5.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.411, 10.100], loss: 0.171606, mae: 0.360650, mean_q: 4.294659
 33193/100000: episode: 490, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 92.568, mean reward: 2.153 [1.685, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.676, 10.100], loss: 0.165567, mae: 0.374203, mean_q: 4.315052
 33236/100000: episode: 491, duration: 0.218s, episode steps: 43, steps per second: 197, episode reward: 109.108, mean reward: 2.537 [1.589, 7.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.549, 10.100], loss: 0.160084, mae: 0.383450, mean_q: 4.371480
 33288/100000: episode: 492, duration: 0.274s, episode steps: 52, steps per second: 190, episode reward: 114.470, mean reward: 2.201 [1.606, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.310, 10.100], loss: 0.152055, mae: 0.358272, mean_q: 4.344013
 33314/100000: episode: 493, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 119.820, mean reward: 4.608 [2.542, 20.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.523, 10.411], loss: 0.218617, mae: 0.397855, mean_q: 4.283473
 33365/100000: episode: 494, duration: 0.272s, episode steps: 51, steps per second: 187, episode reward: 129.397, mean reward: 2.537 [1.954, 5.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.486, 10.422], loss: 0.291112, mae: 0.377138, mean_q: 4.322635
 33383/100000: episode: 495, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 47.943, mean reward: 2.663 [2.344, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.067, 10.324], loss: 0.203690, mae: 0.431743, mean_q: 4.395231
 33437/100000: episode: 496, duration: 0.271s, episode steps: 54, steps per second: 200, episode reward: 145.871, mean reward: 2.701 [1.610, 9.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.896, 10.361], loss: 0.305760, mae: 0.410394, mean_q: 4.412994
 33489/100000: episode: 497, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 107.252, mean reward: 2.063 [1.488, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.759, 10.100], loss: 0.281531, mae: 0.424735, mean_q: 4.377298
 33513/100000: episode: 498, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 107.496, mean reward: 4.479 [2.294, 12.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.644, 10.672], loss: 0.293534, mae: 0.408933, mean_q: 4.415567
 33556/100000: episode: 499, duration: 0.215s, episode steps: 43, steps per second: 200, episode reward: 175.509, mean reward: 4.082 [2.508, 8.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.417, 10.100], loss: 0.420288, mae: 0.444105, mean_q: 4.446922
 33607/100000: episode: 500, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 116.623, mean reward: 2.287 [1.456, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.033, 10.100], loss: 0.468103, mae: 0.444363, mean_q: 4.496430
[Info] FALSIFICATION!
 33608/100000: episode: 501, duration: 0.412s, episode steps: 1, steps per second: 2, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.004, 9.629], loss: 0.098862, mae: 0.352968, mean_q: 4.133453
 33651/100000: episode: 502, duration: 0.227s, episode steps: 43, steps per second: 189, episode reward: 115.612, mean reward: 2.689 [1.460, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.490, 10.100], loss: 0.240149, mae: 0.439429, mean_q: 4.502997
 33680/100000: episode: 503, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 102.689, mean reward: 3.541 [2.253, 5.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.453, 10.100], loss: 533.378784, mae: 2.194118, mean_q: 5.223815
 33704/100000: episode: 504, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 67.646, mean reward: 2.819 [2.056, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.133, 10.268], loss: 2.013430, mae: 1.269805, mean_q: 4.391764
 33733/100000: episode: 505, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 113.027, mean reward: 3.897 [2.986, 8.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.397, 10.100], loss: 0.739121, mae: 0.724402, mean_q: 4.498643
 33751/100000: episode: 506, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 89.735, mean reward: 4.985 [2.779, 17.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.697], loss: 0.378728, mae: 0.568468, mean_q: 4.337619
 33803/100000: episode: 507, duration: 0.280s, episode steps: 52, steps per second: 186, episode reward: 122.430, mean reward: 2.354 [1.530, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.333, 10.143], loss: 0.466044, mae: 0.511138, mean_q: 4.371672
 33818/100000: episode: 508, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 43.059, mean reward: 2.871 [2.334, 3.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.043, 10.100], loss: 0.317279, mae: 0.480129, mean_q: 4.362072
 33872/100000: episode: 509, duration: 0.281s, episode steps: 54, steps per second: 192, episode reward: 162.206, mean reward: 3.004 [1.986, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.758, 10.459], loss: 286.821381, mae: 1.426043, mean_q: 4.898251
 33887/100000: episode: 510, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 39.855, mean reward: 2.657 [1.971, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.631, 10.100], loss: 0.569189, mae: 0.700671, mean_q: 4.182230
 33938/100000: episode: 511, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 104.949, mean reward: 2.058 [1.541, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.299, 10.120], loss: 0.614780, mae: 0.539810, mean_q: 4.621733
 34033/100000: episode: 512, duration: 0.479s, episode steps: 95, steps per second: 198, episode reward: 185.670, mean reward: 1.954 [1.485, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.754, 10.313], loss: 162.911484, mae: 1.104102, mean_q: 4.953990
 34059/100000: episode: 513, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 66.302, mean reward: 2.550 [1.794, 5.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.177, 10.346], loss: 0.949703, mae: 0.756048, mean_q: 4.542247
 34085/100000: episode: 514, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 84.667, mean reward: 3.256 [2.389, 6.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.347, 10.435], loss: 0.308532, mae: 0.546102, mean_q: 4.748571
 34139/100000: episode: 515, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 121.733, mean reward: 2.254 [1.614, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.454, 10.254], loss: 0.358718, mae: 0.493431, mean_q: 4.675005
 34190/100000: episode: 516, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 104.980, mean reward: 2.058 [1.457, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.483, 10.100], loss: 303.201294, mae: 1.605485, mean_q: 5.391762
 34216/100000: episode: 517, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 79.777, mean reward: 3.068 [1.636, 15.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.340, 10.328], loss: 0.367827, mae: 0.562692, mean_q: 4.618044
 34245/100000: episode: 518, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 120.966, mean reward: 4.171 [2.380, 9.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.776, 10.100], loss: 0.437955, mae: 0.533264, mean_q: 4.629815
 34269/100000: episode: 519, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 69.561, mean reward: 2.898 [1.946, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.752, 10.553], loss: 0.753074, mae: 0.559438, mean_q: 4.741064
 34364/100000: episode: 520, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 186.787, mean reward: 1.966 [1.507, 4.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-1.489, 10.100], loss: 162.686386, mae: 1.167107, mean_q: 5.107852
 34388/100000: episode: 521, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 100.981, mean reward: 4.208 [2.606, 7.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.550], loss: 0.405451, mae: 0.497432, mean_q: 4.722032
 34417/100000: episode: 522, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 83.995, mean reward: 2.896 [2.101, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.413, 10.100], loss: 0.913466, mae: 0.589362, mean_q: 4.821061
 34443/100000: episode: 523, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 51.072, mean reward: 1.964 [1.481, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.190, 10.140], loss: 0.742377, mae: 0.570193, mean_q: 4.878643
 34494/100000: episode: 524, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 95.311, mean reward: 1.869 [1.543, 2.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.302, 10.130], loss: 0.359534, mae: 0.497015, mean_q: 4.706023
 34509/100000: episode: 525, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 43.912, mean reward: 2.927 [2.231, 7.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.765, 10.100], loss: 0.353567, mae: 0.513117, mean_q: 4.664262
 34535/100000: episode: 526, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 65.825, mean reward: 2.532 [1.991, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.398, 10.345], loss: 0.334274, mae: 0.493278, mean_q: 4.742906
 34589/100000: episode: 527, duration: 0.272s, episode steps: 54, steps per second: 199, episode reward: 112.672, mean reward: 2.087 [1.562, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.533, 10.204], loss: 0.520989, mae: 0.517913, mean_q: 4.732646
 34643/100000: episode: 528, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 115.518, mean reward: 2.139 [1.492, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-1.137, 10.100], loss: 0.342408, mae: 0.490432, mean_q: 4.700114
[Info] Complete ISplit Iteration
[Info] Levels: [5.720328, 8.079642]
[Info] Cond. Prob: [0.1, 0.14]
[Info] Error Prob: 0.014000000000000002

 34697/100000: episode: 529, duration: 4.560s, episode steps: 54, steps per second: 12, episode reward: 108.234, mean reward: 2.004 [1.477, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.706, 10.112], loss: 0.466544, mae: 0.510675, mean_q: 4.714655
 34797/100000: episode: 530, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 175.843, mean reward: 1.758 [1.443, 2.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.156, 10.216], loss: 0.490743, mae: 0.504364, mean_q: 4.699662
 34897/100000: episode: 531, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.555, mean reward: 1.876 [1.455, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.870, 10.111], loss: 154.676086, mae: 1.011554, mean_q: 4.944149
 34997/100000: episode: 532, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 219.887, mean reward: 2.199 [1.465, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.711, 10.182], loss: 307.646454, mae: 1.748647, mean_q: 5.550235
 35097/100000: episode: 533, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 180.390, mean reward: 1.804 [1.455, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.633, 10.098], loss: 154.577744, mae: 1.100395, mean_q: 5.061931
 35197/100000: episode: 534, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.786, mean reward: 1.898 [1.470, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.912, 10.098], loss: 154.229919, mae: 1.077391, mean_q: 5.203267
 35297/100000: episode: 535, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 240.918, mean reward: 2.409 [1.479, 11.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.686, 10.098], loss: 307.149292, mae: 1.497674, mean_q: 5.208497
 35397/100000: episode: 536, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.090, mean reward: 1.851 [1.438, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.405, 10.098], loss: 154.012985, mae: 1.546852, mean_q: 5.664413
 35497/100000: episode: 537, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 180.810, mean reward: 1.808 [1.460, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.206], loss: 306.135345, mae: 1.680577, mean_q: 5.555562
 35597/100000: episode: 538, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.548, mean reward: 1.875 [1.462, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.369], loss: 0.860390, mae: 0.728355, mean_q: 5.111115
 35697/100000: episode: 539, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.198, mean reward: 1.852 [1.435, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.396, 10.191], loss: 0.381624, mae: 0.528983, mean_q: 4.872902
 35797/100000: episode: 540, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.181, mean reward: 1.822 [1.448, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.083, 10.196], loss: 0.375035, mae: 0.516940, mean_q: 4.792331
 35897/100000: episode: 541, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 200.750, mean reward: 2.007 [1.464, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.549, 10.098], loss: 154.667236, mae: 1.154636, mean_q: 5.160607
 35997/100000: episode: 542, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 211.443, mean reward: 2.114 [1.454, 5.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.131, 10.210], loss: 154.422318, mae: 1.152867, mean_q: 5.160308
 36097/100000: episode: 543, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.366, mean reward: 1.784 [1.439, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.599, 10.141], loss: 153.520798, mae: 0.990421, mean_q: 4.889492
 36197/100000: episode: 544, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.574, mean reward: 1.936 [1.454, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.953, 10.098], loss: 153.909897, mae: 1.098075, mean_q: 5.210608
 36297/100000: episode: 545, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 181.281, mean reward: 1.813 [1.453, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.508, 10.198], loss: 608.784729, mae: 2.913039, mean_q: 6.430887
 36397/100000: episode: 546, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.419, mean reward: 1.924 [1.472, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.466, 10.098], loss: 0.536303, mae: 0.606911, mean_q: 4.959625
 36497/100000: episode: 547, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.053, mean reward: 1.891 [1.481, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.871, 10.098], loss: 0.577639, mae: 0.557873, mean_q: 4.867140
 36597/100000: episode: 548, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.971, mean reward: 1.850 [1.435, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.144, 10.098], loss: 0.452131, mae: 0.516328, mean_q: 4.696615
 36697/100000: episode: 549, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 208.459, mean reward: 2.085 [1.555, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.114, 10.098], loss: 0.444805, mae: 0.487570, mean_q: 4.646980
 36797/100000: episode: 550, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.047, mean reward: 1.870 [1.436, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.783, 10.235], loss: 154.414398, mae: 1.089049, mean_q: 4.947880
 36897/100000: episode: 551, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 183.133, mean reward: 1.831 [1.453, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.193, 10.218], loss: 0.395235, mae: 0.496215, mean_q: 4.587066
 36997/100000: episode: 552, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.906, mean reward: 1.929 [1.464, 6.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.418, 10.098], loss: 0.323859, mae: 0.467922, mean_q: 4.542988
 37097/100000: episode: 553, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 203.979, mean reward: 2.040 [1.450, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.098], loss: 0.359071, mae: 0.471741, mean_q: 4.497218
 37197/100000: episode: 554, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 192.975, mean reward: 1.930 [1.447, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.795, 10.175], loss: 0.415213, mae: 0.459885, mean_q: 4.476517
 37297/100000: episode: 555, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 213.260, mean reward: 2.133 [1.499, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.794, 10.184], loss: 459.690887, mae: 2.136090, mean_q: 5.176620
 37397/100000: episode: 556, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.316, mean reward: 1.913 [1.474, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.261], loss: 154.124084, mae: 1.288077, mean_q: 5.013472
 37497/100000: episode: 557, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.268, mean reward: 1.963 [1.448, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.334, 10.356], loss: 152.562302, mae: 1.132990, mean_q: 4.976714
 37597/100000: episode: 558, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.131, mean reward: 1.881 [1.472, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.674, 10.098], loss: 153.240936, mae: 1.142701, mean_q: 4.894862
 37697/100000: episode: 559, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 220.657, mean reward: 2.207 [1.545, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.272, 10.294], loss: 0.448712, mae: 0.544323, mean_q: 4.426566
 37797/100000: episode: 560, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.253, mean reward: 1.863 [1.451, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.098], loss: 0.485660, mae: 0.496583, mean_q: 4.387972
 37897/100000: episode: 561, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 188.748, mean reward: 1.887 [1.482, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.673, 10.220], loss: 304.172272, mae: 1.690744, mean_q: 4.957199
 37997/100000: episode: 562, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 213.592, mean reward: 2.136 [1.440, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.086, 10.262], loss: 0.479141, mae: 0.533768, mean_q: 4.379526
 38097/100000: episode: 563, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 209.660, mean reward: 2.097 [1.517, 6.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.179, 10.326], loss: 153.585388, mae: 1.046477, mean_q: 4.570390
 38197/100000: episode: 564, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 203.610, mean reward: 2.036 [1.535, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.668, 10.098], loss: 0.485467, mae: 0.523696, mean_q: 4.305747
 38297/100000: episode: 565, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.849, mean reward: 1.918 [1.435, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.222, 10.105], loss: 0.303348, mae: 0.431526, mean_q: 4.240683
 38397/100000: episode: 566, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 200.916, mean reward: 2.009 [1.449, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.498, 10.521], loss: 0.257509, mae: 0.401956, mean_q: 4.179590
 38497/100000: episode: 567, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 190.749, mean reward: 1.907 [1.465, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.132, 10.098], loss: 0.293343, mae: 0.399687, mean_q: 4.134474
 38597/100000: episode: 568, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.065, mean reward: 1.921 [1.488, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.229, 10.249], loss: 0.252461, mae: 0.379604, mean_q: 4.138701
 38697/100000: episode: 569, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 198.859, mean reward: 1.989 [1.473, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.944, 10.098], loss: 0.227428, mae: 0.357373, mean_q: 4.054105
 38797/100000: episode: 570, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 193.471, mean reward: 1.935 [1.491, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.723, 10.290], loss: 0.170469, mae: 0.343479, mean_q: 4.019449
 38897/100000: episode: 571, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 202.560, mean reward: 2.026 [1.516, 6.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.749, 10.220], loss: 0.161331, mae: 0.340555, mean_q: 4.002014
 38997/100000: episode: 572, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.293, mean reward: 1.893 [1.470, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.937, 10.098], loss: 0.197816, mae: 0.337156, mean_q: 3.960191
 39097/100000: episode: 573, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 183.215, mean reward: 1.832 [1.457, 2.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.178], loss: 0.146693, mae: 0.324419, mean_q: 3.940775
 39197/100000: episode: 574, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.083, mean reward: 1.951 [1.460, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.989, 10.265], loss: 0.134248, mae: 0.314829, mean_q: 3.966282
 39297/100000: episode: 575, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.915, mean reward: 1.979 [1.516, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.319], loss: 0.096472, mae: 0.293410, mean_q: 3.901141
 39397/100000: episode: 576, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 226.985, mean reward: 2.270 [1.469, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.913, 10.098], loss: 0.144505, mae: 0.320559, mean_q: 3.888301
 39497/100000: episode: 577, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 207.198, mean reward: 2.072 [1.480, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.785, 10.318], loss: 0.094676, mae: 0.300559, mean_q: 3.869804
 39597/100000: episode: 578, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 199.957, mean reward: 2.000 [1.510, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.083, 10.098], loss: 0.090212, mae: 0.292348, mean_q: 3.871945
 39697/100000: episode: 579, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 180.501, mean reward: 1.805 [1.446, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.819, 10.098], loss: 0.130408, mae: 0.298604, mean_q: 3.881187
 39797/100000: episode: 580, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 209.716, mean reward: 2.097 [1.469, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.631, 10.117], loss: 0.097507, mae: 0.298352, mean_q: 3.879982
 39897/100000: episode: 581, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.780, mean reward: 1.988 [1.462, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.047, 10.098], loss: 0.083559, mae: 0.280070, mean_q: 3.872199
 39997/100000: episode: 582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.886, mean reward: 1.929 [1.489, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.922, 10.098], loss: 0.089068, mae: 0.292110, mean_q: 3.862125
 40097/100000: episode: 583, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.297, mean reward: 1.943 [1.456, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.697, 10.127], loss: 0.116307, mae: 0.314606, mean_q: 3.885504
 40197/100000: episode: 584, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.979, mean reward: 1.980 [1.449, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.374, 10.098], loss: 0.091645, mae: 0.296806, mean_q: 3.895188
 40297/100000: episode: 585, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 180.113, mean reward: 1.801 [1.450, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.540, 10.110], loss: 0.087925, mae: 0.289683, mean_q: 3.859975
 40397/100000: episode: 586, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 214.472, mean reward: 2.145 [1.471, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.161], loss: 0.091378, mae: 0.298389, mean_q: 3.886788
 40497/100000: episode: 587, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.662, mean reward: 1.857 [1.443, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.143, 10.183], loss: 0.093171, mae: 0.300541, mean_q: 3.897851
 40597/100000: episode: 588, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 193.187, mean reward: 1.932 [1.465, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.432, 10.205], loss: 0.091353, mae: 0.300905, mean_q: 3.875767
 40697/100000: episode: 589, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 222.673, mean reward: 2.227 [1.467, 4.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 0.096652, mae: 0.301779, mean_q: 3.888348
 40797/100000: episode: 590, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.860, mean reward: 1.919 [1.438, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.025, 10.098], loss: 0.085774, mae: 0.298454, mean_q: 3.888107
 40897/100000: episode: 591, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.994, mean reward: 1.920 [1.447, 2.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.442, 10.281], loss: 0.086999, mae: 0.298359, mean_q: 3.903720
 40997/100000: episode: 592, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.720, mean reward: 1.937 [1.462, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.233, 10.249], loss: 0.095278, mae: 0.300102, mean_q: 3.880499
 41097/100000: episode: 593, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.729, mean reward: 1.877 [1.465, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.387, 10.182], loss: 0.094882, mae: 0.300863, mean_q: 3.887913
 41197/100000: episode: 594, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.214, mean reward: 1.872 [1.495, 2.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.098], loss: 0.082914, mae: 0.293363, mean_q: 3.892000
 41297/100000: episode: 595, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.433, mean reward: 1.874 [1.479, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.098], loss: 0.101180, mae: 0.304410, mean_q: 3.886968
 41397/100000: episode: 596, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.920, mean reward: 1.889 [1.444, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.913, 10.098], loss: 0.095175, mae: 0.298241, mean_q: 3.892451
 41497/100000: episode: 597, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.798, mean reward: 1.898 [1.454, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.578, 10.128], loss: 0.080243, mae: 0.283166, mean_q: 3.875323
 41597/100000: episode: 598, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.469, mean reward: 1.895 [1.460, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.425, 10.098], loss: 0.084216, mae: 0.286976, mean_q: 3.875678
 41697/100000: episode: 599, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.490, mean reward: 1.925 [1.430, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.239, 10.126], loss: 0.096881, mae: 0.297629, mean_q: 3.884739
 41797/100000: episode: 600, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 182.997, mean reward: 1.830 [1.496, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.895, 10.286], loss: 0.090769, mae: 0.294215, mean_q: 3.877539
 41897/100000: episode: 601, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.111, mean reward: 1.851 [1.462, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.634, 10.098], loss: 0.086980, mae: 0.287869, mean_q: 3.871788
 41997/100000: episode: 602, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 224.054, mean reward: 2.241 [1.488, 8.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.417], loss: 0.084082, mae: 0.297733, mean_q: 3.893858
 42097/100000: episode: 603, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 246.123, mean reward: 2.461 [1.500, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.469, 10.098], loss: 0.087739, mae: 0.292386, mean_q: 3.901747
 42197/100000: episode: 604, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 182.599, mean reward: 1.826 [1.451, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.054, 10.180], loss: 0.086337, mae: 0.299247, mean_q: 3.885754
 42297/100000: episode: 605, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.852, mean reward: 1.929 [1.456, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.749, 10.105], loss: 0.086739, mae: 0.295855, mean_q: 3.889622
 42397/100000: episode: 606, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 204.575, mean reward: 2.046 [1.477, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.413, 10.098], loss: 0.088852, mae: 0.297744, mean_q: 3.892657
 42497/100000: episode: 607, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 181.743, mean reward: 1.817 [1.452, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.589, 10.098], loss: 0.095846, mae: 0.306588, mean_q: 3.912062
 42597/100000: episode: 608, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.266, mean reward: 1.893 [1.498, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.116], loss: 0.086167, mae: 0.295898, mean_q: 3.906552
 42697/100000: episode: 609, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 198.218, mean reward: 1.982 [1.498, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.549, 10.122], loss: 0.094849, mae: 0.306629, mean_q: 3.908819
 42797/100000: episode: 610, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.278, mean reward: 1.983 [1.549, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.181, 10.102], loss: 0.085626, mae: 0.295408, mean_q: 3.879590
 42897/100000: episode: 611, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.643, mean reward: 1.886 [1.511, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.443, 10.209], loss: 0.091857, mae: 0.302615, mean_q: 3.883572
 42997/100000: episode: 612, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 208.638, mean reward: 2.086 [1.479, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.555, 10.347], loss: 0.080783, mae: 0.288566, mean_q: 3.877058
 43097/100000: episode: 613, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.800, mean reward: 1.818 [1.473, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.170, 10.098], loss: 0.087483, mae: 0.292277, mean_q: 3.876520
 43197/100000: episode: 614, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 187.397, mean reward: 1.874 [1.472, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.442, 10.319], loss: 0.085402, mae: 0.292321, mean_q: 3.877461
 43297/100000: episode: 615, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.655, mean reward: 1.977 [1.447, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.713, 10.098], loss: 0.093885, mae: 0.311000, mean_q: 3.889233
 43397/100000: episode: 616, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.278, mean reward: 1.843 [1.436, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.830, 10.149], loss: 0.075009, mae: 0.281978, mean_q: 3.868854
 43497/100000: episode: 617, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 209.635, mean reward: 2.096 [1.484, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.450], loss: 0.085261, mae: 0.293329, mean_q: 3.852541
 43597/100000: episode: 618, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 176.687, mean reward: 1.767 [1.436, 2.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.347, 10.098], loss: 0.078877, mae: 0.283505, mean_q: 3.854608
 43697/100000: episode: 619, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.512, mean reward: 1.885 [1.459, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.797, 10.310], loss: 0.079993, mae: 0.282019, mean_q: 3.865325
 43797/100000: episode: 620, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.804, mean reward: 2.028 [1.468, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.052, 10.140], loss: 0.094670, mae: 0.307938, mean_q: 3.891440
 43897/100000: episode: 621, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 199.033, mean reward: 1.990 [1.431, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.995, 10.098], loss: 0.076045, mae: 0.276098, mean_q: 3.860318
 43997/100000: episode: 622, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.536, mean reward: 1.865 [1.470, 5.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.267, 10.202], loss: 0.083999, mae: 0.298856, mean_q: 3.881136
 44097/100000: episode: 623, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 212.722, mean reward: 2.127 [1.455, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.932, 10.098], loss: 0.077414, mae: 0.288077, mean_q: 3.872065
 44197/100000: episode: 624, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 201.682, mean reward: 2.017 [1.487, 5.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.126], loss: 0.089497, mae: 0.299997, mean_q: 3.902327
 44297/100000: episode: 625, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 204.276, mean reward: 2.043 [1.438, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.409, 10.110], loss: 0.081596, mae: 0.289862, mean_q: 3.878489
 44397/100000: episode: 626, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 213.991, mean reward: 2.140 [1.541, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.611, 10.098], loss: 0.084813, mae: 0.297334, mean_q: 3.876504
 44497/100000: episode: 627, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.704, mean reward: 1.857 [1.482, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.434, 10.296], loss: 0.087888, mae: 0.295270, mean_q: 3.871264
 44597/100000: episode: 628, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 180.310, mean reward: 1.803 [1.475, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.049, 10.098], loss: 0.073415, mae: 0.281809, mean_q: 3.873937
[Info] 1-TH LEVEL FOUND: 5.550954341888428, Considering 10/90 traces
 44697/100000: episode: 629, duration: 4.635s, episode steps: 100, steps per second: 22, episode reward: 195.216, mean reward: 1.952 [1.456, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.489, 10.301], loss: 0.078335, mae: 0.285509, mean_q: 3.852217
 44721/100000: episode: 630, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 55.316, mean reward: 2.305 [1.814, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.201, 10.100], loss: 0.076056, mae: 0.280213, mean_q: 3.882011
 44737/100000: episode: 631, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 34.156, mean reward: 2.135 [1.766, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.715, 10.100], loss: 0.084779, mae: 0.278239, mean_q: 3.845819
 44753/100000: episode: 632, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 34.482, mean reward: 2.155 [1.707, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.150, 10.100], loss: 0.090772, mae: 0.295590, mean_q: 3.853149
 44770/100000: episode: 633, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 46.155, mean reward: 2.715 [1.907, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.839, 10.100], loss: 0.104203, mae: 0.300872, mean_q: 3.900684
 44794/100000: episode: 634, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 69.291, mean reward: 2.887 [2.342, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.541, 10.100], loss: 0.078149, mae: 0.293207, mean_q: 3.880370
 44816/100000: episode: 635, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 55.303, mean reward: 2.514 [1.870, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.425, 10.100], loss: 0.082301, mae: 0.289716, mean_q: 3.853626
 44840/100000: episode: 636, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 68.864, mean reward: 2.869 [1.935, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.231, 10.100], loss: 0.092068, mae: 0.297846, mean_q: 3.879833
 44862/100000: episode: 637, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 65.843, mean reward: 2.993 [2.227, 5.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.746, 10.100], loss: 0.082239, mae: 0.303920, mean_q: 3.911080
 44891/100000: episode: 638, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 75.889, mean reward: 2.617 [2.035, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.272, 10.373], loss: 0.080171, mae: 0.284131, mean_q: 3.920552
 44915/100000: episode: 639, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 107.046, mean reward: 4.460 [2.115, 8.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.416, 10.100], loss: 0.087178, mae: 0.301474, mean_q: 3.952767
 44931/100000: episode: 640, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 38.225, mean reward: 2.389 [1.657, 5.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.056, 10.100], loss: 0.084572, mae: 0.293426, mean_q: 3.867629
 44935/100000: episode: 641, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 9.691, mean reward: 2.423 [2.289, 2.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.386, 10.100], loss: 0.118392, mae: 0.316585, mean_q: 3.882611
 44959/100000: episode: 642, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 52.788, mean reward: 2.199 [1.905, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.256, 10.100], loss: 0.108060, mae: 0.315742, mean_q: 3.936525
 44988/100000: episode: 643, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 79.240, mean reward: 2.732 [1.988, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.179, 10.387], loss: 0.117843, mae: 0.312881, mean_q: 3.970952
 45007/100000: episode: 644, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 55.953, mean reward: 2.945 [2.376, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.401], loss: 0.087205, mae: 0.299911, mean_q: 3.946700
 45036/100000: episode: 645, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 85.297, mean reward: 2.941 [2.025, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.188, 10.393], loss: 0.106695, mae: 0.332571, mean_q: 3.949482
 45060/100000: episode: 646, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 93.004, mean reward: 3.875 [2.478, 7.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.603, 10.100], loss: 0.081732, mae: 0.304201, mean_q: 3.913451
 45064/100000: episode: 647, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 11.217, mean reward: 2.804 [2.475, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.326, 10.100], loss: 0.150231, mae: 0.359334, mean_q: 4.068424
 45080/100000: episode: 648, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 39.323, mean reward: 2.458 [1.912, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.143, 10.100], loss: 0.102838, mae: 0.319401, mean_q: 3.947280
 45104/100000: episode: 649, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 73.927, mean reward: 3.080 [2.351, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.278, 10.100], loss: 0.099374, mae: 0.319303, mean_q: 3.971934
 45128/100000: episode: 650, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 55.522, mean reward: 2.313 [1.817, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.202, 10.100], loss: 0.094789, mae: 0.303960, mean_q: 3.979266
 45150/100000: episode: 651, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 58.652, mean reward: 2.666 [2.141, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.374, 10.100], loss: 0.097727, mae: 0.302397, mean_q: 3.934421
 45174/100000: episode: 652, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 82.529, mean reward: 3.439 [2.558, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.371, 10.100], loss: 0.128909, mae: 0.341871, mean_q: 4.030663
 45196/100000: episode: 653, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 67.459, mean reward: 3.066 [2.467, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.171, 10.100], loss: 0.092734, mae: 0.308957, mean_q: 3.996957
 45215/100000: episode: 654, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 83.285, mean reward: 4.383 [2.327, 17.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.646, 10.478], loss: 0.089274, mae: 0.306538, mean_q: 3.990620
 45237/100000: episode: 655, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 67.635, mean reward: 3.074 [2.192, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.441, 10.100], loss: 0.162591, mae: 0.364261, mean_q: 4.118134
 45249/100000: episode: 656, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 27.167, mean reward: 2.264 [1.951, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.360, 10.347], loss: 0.104853, mae: 0.330522, mean_q: 3.987166
 45253/100000: episode: 657, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 10.295, mean reward: 2.574 [2.199, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.355, 10.100], loss: 0.180101, mae: 0.400200, mean_q: 4.146638
 45275/100000: episode: 658, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 65.751, mean reward: 2.989 [1.925, 4.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.277, 10.100], loss: 0.240141, mae: 0.335052, mean_q: 3.957492
 45304/100000: episode: 659, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 91.136, mean reward: 3.143 [1.681, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.038, 10.313], loss: 0.100992, mae: 0.331823, mean_q: 4.067985
 45326/100000: episode: 660, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 75.975, mean reward: 3.453 [2.570, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.424, 10.100], loss: 0.120573, mae: 0.324422, mean_q: 4.060742
 45338/100000: episode: 661, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 31.693, mean reward: 2.641 [2.195, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.563, 10.364], loss: 0.124020, mae: 0.307820, mean_q: 4.059156
 45367/100000: episode: 662, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 74.885, mean reward: 2.582 [1.700, 4.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.055, 10.312], loss: 0.262169, mae: 0.398835, mean_q: 4.154607
 45389/100000: episode: 663, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 55.463, mean reward: 2.521 [2.138, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.280, 10.100], loss: 0.117505, mae: 0.347615, mean_q: 4.009248
 45411/100000: episode: 664, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 52.480, mean reward: 2.385 [1.740, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.023, 10.100], loss: 0.119898, mae: 0.337913, mean_q: 4.065872
 45440/100000: episode: 665, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 77.238, mean reward: 2.663 [2.196, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.562, 10.355], loss: 0.241547, mae: 0.368583, mean_q: 4.115532
 45462/100000: episode: 666, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 70.187, mean reward: 3.190 [2.043, 6.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.699, 10.100], loss: 0.103799, mae: 0.324374, mean_q: 4.055250
 45491/100000: episode: 667, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 90.312, mean reward: 3.114 [1.768, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.086, 10.255], loss: 0.121171, mae: 0.332776, mean_q: 4.070296
 45507/100000: episode: 668, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 43.520, mean reward: 2.720 [1.884, 5.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.636, 10.100], loss: 0.128217, mae: 0.337618, mean_q: 4.171289
 45529/100000: episode: 669, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 61.684, mean reward: 2.804 [1.985, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.645, 10.100], loss: 0.120950, mae: 0.345146, mean_q: 4.066176
 45553/100000: episode: 670, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 67.450, mean reward: 2.810 [2.113, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.304, 10.100], loss: 0.120089, mae: 0.351829, mean_q: 4.140708
 45572/100000: episode: 671, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 49.162, mean reward: 2.587 [2.230, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.070, 10.458], loss: 0.141068, mae: 0.364163, mean_q: 4.220378
 45588/100000: episode: 672, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 43.890, mean reward: 2.743 [2.209, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.634, 10.100], loss: 0.138007, mae: 0.353046, mean_q: 4.167085
 45612/100000: episode: 673, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 57.023, mean reward: 2.376 [1.971, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.511, 10.100], loss: 0.128978, mae: 0.334504, mean_q: 4.128877
 45641/100000: episode: 674, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 84.673, mean reward: 2.920 [1.982, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.735, 10.333], loss: 0.150048, mae: 0.356232, mean_q: 4.134361
 45663/100000: episode: 675, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 54.871, mean reward: 2.494 [2.010, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.222, 10.100], loss: 0.118558, mae: 0.339302, mean_q: 4.168538
 45687/100000: episode: 676, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 57.264, mean reward: 2.386 [1.561, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.146, 10.101], loss: 0.110501, mae: 0.325562, mean_q: 4.095717
 45704/100000: episode: 677, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 37.574, mean reward: 2.210 [1.884, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.383, 10.100], loss: 0.122289, mae: 0.368569, mean_q: 4.211547
 45721/100000: episode: 678, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 34.922, mean reward: 2.054 [1.589, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.260, 10.100], loss: 0.121256, mae: 0.332041, mean_q: 4.110956
 45725/100000: episode: 679, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 11.273, mean reward: 2.818 [2.609, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.340, 10.100], loss: 0.125598, mae: 0.333994, mean_q: 4.118461
 45729/100000: episode: 680, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 11.410, mean reward: 2.853 [2.330, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.370, 10.100], loss: 0.190334, mae: 0.366287, mean_q: 4.214745
 45748/100000: episode: 681, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 44.770, mean reward: 2.356 [2.112, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.420], loss: 0.105906, mae: 0.333810, mean_q: 4.172317
 45760/100000: episode: 682, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 26.435, mean reward: 2.203 [1.926, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.351], loss: 0.136247, mae: 0.332707, mean_q: 4.169281
 45782/100000: episode: 683, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 55.952, mean reward: 2.543 [1.909, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.224, 10.100], loss: 0.156003, mae: 0.352895, mean_q: 4.171724
 45806/100000: episode: 684, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 58.120, mean reward: 2.422 [1.662, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.174, 10.100], loss: 0.256018, mae: 0.393447, mean_q: 4.192202
 45823/100000: episode: 685, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 38.348, mean reward: 2.256 [1.682, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.173, 10.100], loss: 0.140381, mae: 0.360796, mean_q: 4.219255
 45845/100000: episode: 686, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 68.788, mean reward: 3.127 [2.247, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.394, 10.100], loss: 0.253539, mae: 0.372035, mean_q: 4.239030
 45864/100000: episode: 687, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 50.642, mean reward: 2.665 [2.033, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.154, 10.536], loss: 0.164890, mae: 0.402524, mean_q: 4.203661
 45886/100000: episode: 688, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 48.917, mean reward: 2.224 [1.942, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.234, 10.100], loss: 0.268793, mae: 0.373533, mean_q: 4.184466
 45890/100000: episode: 689, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 10.431, mean reward: 2.608 [2.437, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.385, 10.100], loss: 0.137843, mae: 0.407407, mean_q: 4.218718
 45906/100000: episode: 690, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 33.149, mean reward: 2.072 [1.843, 2.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.211, 10.100], loss: 0.141188, mae: 0.377616, mean_q: 4.250197
 45928/100000: episode: 691, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 69.206, mean reward: 3.146 [2.325, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.666, 10.100], loss: 0.268376, mae: 0.386720, mean_q: 4.222960
 45950/100000: episode: 692, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 50.810, mean reward: 2.310 [1.925, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.254, 10.100], loss: 0.144508, mae: 0.361016, mean_q: 4.121195
 45979/100000: episode: 693, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 67.070, mean reward: 2.313 [1.722, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.196, 10.266], loss: 0.144941, mae: 0.362803, mean_q: 4.189898
 45996/100000: episode: 694, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 34.223, mean reward: 2.013 [1.805, 2.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.105, 10.100], loss: 0.166987, mae: 0.388469, mean_q: 4.262225
 46008/100000: episode: 695, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 32.191, mean reward: 2.683 [2.022, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.294, 10.289], loss: 0.166126, mae: 0.370899, mean_q: 4.267646
 46027/100000: episode: 696, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 52.164, mean reward: 2.745 [2.091, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.313], loss: 0.137350, mae: 0.348743, mean_q: 4.267392
 46045/100000: episode: 697, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 40.796, mean reward: 2.266 [1.761, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.671, 10.100], loss: 0.109109, mae: 0.323313, mean_q: 4.202081
 46074/100000: episode: 698, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 61.985, mean reward: 2.137 [1.474, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.647, 10.119], loss: 0.246142, mae: 0.394812, mean_q: 4.271368
 46098/100000: episode: 699, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 77.739, mean reward: 3.239 [2.344, 4.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.346, 10.100], loss: 0.152764, mae: 0.377614, mean_q: 4.253265
 46122/100000: episode: 700, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 63.793, mean reward: 2.658 [2.135, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.833, 10.100], loss: 0.253806, mae: 0.398100, mean_q: 4.246839
 46151/100000: episode: 701, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 76.491, mean reward: 2.638 [2.058, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.367], loss: 0.155944, mae: 0.379870, mean_q: 4.253609
 46173/100000: episode: 702, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 49.973, mean reward: 2.272 [1.908, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.842, 10.100], loss: 0.147772, mae: 0.366202, mean_q: 4.249136
 46191/100000: episode: 703, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 46.439, mean reward: 2.580 [1.725, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.418, 10.100], loss: 0.111412, mae: 0.353017, mean_q: 4.318917
 46203/100000: episode: 704, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 29.059, mean reward: 2.422 [2.023, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.242, 10.349], loss: 0.126513, mae: 0.366437, mean_q: 4.341353
 46227/100000: episode: 705, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 89.949, mean reward: 3.748 [2.205, 6.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.916, 10.100], loss: 0.252510, mae: 0.387091, mean_q: 4.275629
 46244/100000: episode: 706, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 39.053, mean reward: 2.297 [1.981, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.239, 10.100], loss: 0.154495, mae: 0.398603, mean_q: 4.295411
 46248/100000: episode: 707, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 8.742, mean reward: 2.185 [2.076, 2.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.442, 10.100], loss: 0.168972, mae: 0.390627, mean_q: 4.147400
 46252/100000: episode: 708, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 10.874, mean reward: 2.718 [2.292, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.315, 10.100], loss: 0.127067, mae: 0.392778, mean_q: 4.438053
 46269/100000: episode: 709, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 38.332, mean reward: 2.255 [2.078, 2.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.244, 10.100], loss: 0.144666, mae: 0.354508, mean_q: 4.289021
 46288/100000: episode: 710, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 56.198, mean reward: 2.958 [2.219, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.081, 10.411], loss: 0.154392, mae: 0.371927, mean_q: 4.296564
 46307/100000: episode: 711, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 44.782, mean reward: 2.357 [2.100, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.368], loss: 0.160849, mae: 0.388504, mean_q: 4.332094
 46329/100000: episode: 712, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 73.066, mean reward: 3.321 [2.300, 6.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.299, 10.100], loss: 0.273632, mae: 0.380176, mean_q: 4.264928
 46347/100000: episode: 713, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 41.996, mean reward: 2.333 [1.753, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.516, 10.100], loss: 0.341036, mae: 0.439986, mean_q: 4.361974
 46369/100000: episode: 714, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 57.680, mean reward: 2.622 [2.193, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.913, 10.100], loss: 0.129518, mae: 0.366213, mean_q: 4.339688
 46388/100000: episode: 715, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 39.701, mean reward: 2.090 [1.565, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.186], loss: 0.158691, mae: 0.393602, mean_q: 4.305104
 46400/100000: episode: 716, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 27.395, mean reward: 2.283 [1.745, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.064, 10.325], loss: 0.127458, mae: 0.364236, mean_q: 4.381800
 46419/100000: episode: 717, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 50.463, mean reward: 2.656 [2.126, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.192, 10.411], loss: 0.373950, mae: 0.456117, mean_q: 4.355822
 46441/100000: episode: 718, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 57.361, mean reward: 2.607 [2.040, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.210, 10.100], loss: 0.135275, mae: 0.377999, mean_q: 4.382164
[Info] 2-TH LEVEL FOUND: 7.394796371459961, Considering 10/90 traces
 46463/100000: episode: 719, duration: 4.198s, episode steps: 22, steps per second: 5, episode reward: 75.012, mean reward: 3.410 [2.385, 6.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.403, 10.100], loss: 0.146401, mae: 0.375727, mean_q: 4.300273
 46481/100000: episode: 720, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 56.396, mean reward: 3.133 [2.350, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-2.243, 10.100], loss: 0.118960, mae: 0.357481, mean_q: 4.346997
 46496/100000: episode: 721, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 43.394, mean reward: 2.893 [2.266, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.471, 10.100], loss: 0.174466, mae: 0.389702, mean_q: 4.341006
 46514/100000: episode: 722, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 59.051, mean reward: 3.281 [2.574, 5.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.420, 10.100], loss: 0.187625, mae: 0.423686, mean_q: 4.351418
[Info] FALSIFICATION!
 46525/100000: episode: 723, duration: 0.217s, episode steps: 11, steps per second: 51, episode reward: 1074.457, mean reward: 97.678 [3.115, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.382, 9.988], loss: 0.175218, mae: 0.389536, mean_q: 4.376710
 46543/100000: episode: 724, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 63.920, mean reward: 3.551 [2.444, 6.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.449, 10.100], loss: 0.123718, mae: 0.370929, mean_q: 4.377594
 46561/100000: episode: 725, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 52.545, mean reward: 2.919 [2.458, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.398, 10.100], loss: 0.135669, mae: 0.366649, mean_q: 4.359178
 46580/100000: episode: 726, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 118.193, mean reward: 6.221 [3.005, 16.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.386, 10.100], loss: 0.290847, mae: 0.444269, mean_q: 4.552709
 46598/100000: episode: 727, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 69.070, mean reward: 3.837 [2.912, 6.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.333, 10.100], loss: 0.309665, mae: 0.427614, mean_q: 4.444920
 46616/100000: episode: 728, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 58.382, mean reward: 3.243 [2.560, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.249, 10.100], loss: 0.330334, mae: 0.403871, mean_q: 4.362005
 46632/100000: episode: 729, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 165.881, mean reward: 10.368 [2.368, 32.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.972, 10.100], loss: 0.304226, mae: 0.473447, mean_q: 4.452539
 46649/100000: episode: 730, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 54.199, mean reward: 3.188 [1.969, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.370, 10.100], loss: 0.223865, mae: 0.448366, mean_q: 4.469085
 46664/100000: episode: 731, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 41.592, mean reward: 2.773 [2.301, 4.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.180, 10.100], loss: 1.391139, mae: 0.605769, mean_q: 4.678634
 46683/100000: episode: 732, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 95.091, mean reward: 5.005 [3.594, 7.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.017, 10.100], loss: 0.429279, mae: 0.554301, mean_q: 4.395916
 46702/100000: episode: 733, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 64.671, mean reward: 3.404 [2.868, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.284, 10.100], loss: 0.156245, mae: 0.405889, mean_q: 4.476798
 46717/100000: episode: 734, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 106.797, mean reward: 7.120 [3.130, 14.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.250, 10.100], loss: 0.834403, mae: 0.543071, mean_q: 4.694425
 46734/100000: episode: 735, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 43.181, mean reward: 2.540 [2.065, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.256, 10.100], loss: 0.377721, mae: 0.464154, mean_q: 4.474657
 46751/100000: episode: 736, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 39.561, mean reward: 2.327 [1.938, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.302, 10.100], loss: 0.354796, mae: 0.494744, mean_q: 4.625472
 46768/100000: episode: 737, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 54.499, mean reward: 3.206 [1.891, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.380, 10.100], loss: 0.689215, mae: 0.566072, mean_q: 4.581333
 46783/100000: episode: 738, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 35.294, mean reward: 2.353 [2.017, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.257, 10.100], loss: 0.261865, mae: 0.478822, mean_q: 4.539186
 46801/100000: episode: 739, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 58.612, mean reward: 3.256 [1.957, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.262, 10.100], loss: 0.860402, mae: 0.548893, mean_q: 4.609918
 46820/100000: episode: 740, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 68.551, mean reward: 3.608 [2.776, 6.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.748, 10.100], loss: 0.399327, mae: 0.512451, mean_q: 4.714942
 46839/100000: episode: 741, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 70.159, mean reward: 3.693 [2.473, 7.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.232, 10.100], loss: 0.353389, mae: 0.451974, mean_q: 4.697003
 46854/100000: episode: 742, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 68.456, mean reward: 4.564 [3.313, 5.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.531, 10.100], loss: 0.429154, mae: 0.503844, mean_q: 4.669797
 46870/100000: episode: 743, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 45.982, mean reward: 2.874 [2.533, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.502, 10.100], loss: 962.285339, mae: 2.607636, mean_q: 4.872636
 46885/100000: episode: 744, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 50.083, mean reward: 3.339 [2.952, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.356, 10.100], loss: 7.614861, mae: 2.412400, mean_q: 6.945803
 46900/100000: episode: 745, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 94.245, mean reward: 6.283 [3.271, 16.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.459, 10.100], loss: 1.907526, mae: 1.011949, mean_q: 4.477429
 46918/100000: episode: 746, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 52.116, mean reward: 2.895 [2.454, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.381, 10.100], loss: 0.656909, mae: 0.751728, mean_q: 4.718843
 46935/100000: episode: 747, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 59.638, mean reward: 3.508 [2.835, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.257, 10.100], loss: 0.913259, mae: 0.626936, mean_q: 4.846627
 46950/100000: episode: 748, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 56.083, mean reward: 3.739 [3.024, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.370, 10.100], loss: 0.346026, mae: 0.536875, mean_q: 4.670821
 46968/100000: episode: 749, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 58.574, mean reward: 3.254 [2.853, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.690, 10.100], loss: 859.861084, mae: 3.085296, mean_q: 5.679413
 46983/100000: episode: 750, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 87.273, mean reward: 5.818 [2.802, 8.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.522, 10.100], loss: 1.629299, mae: 1.161212, mean_q: 4.736728
 47000/100000: episode: 751, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 55.616, mean reward: 3.272 [1.929, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.490, 10.100], loss: 0.840468, mae: 0.796613, mean_q: 5.149837
 47017/100000: episode: 752, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 49.543, mean reward: 2.914 [1.932, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.416, 10.100], loss: 907.785339, mae: 2.901047, mean_q: 5.570736
 47032/100000: episode: 753, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 56.929, mean reward: 3.795 [2.745, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.567, 10.100], loss: 1025.854980, mae: 4.787420, mean_q: 7.580022
 47049/100000: episode: 754, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 54.992, mean reward: 3.235 [2.502, 5.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.736, 10.100], loss: 906.676086, mae: 3.102550, mean_q: 5.086843
 47066/100000: episode: 755, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 54.110, mean reward: 3.183 [2.190, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.274, 10.100], loss: 1.897513, mae: 1.236532, mean_q: 5.719434
 47083/100000: episode: 756, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 51.066, mean reward: 3.004 [2.390, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.820, 10.100], loss: 1.379560, mae: 1.090513, mean_q: 5.816482
 47100/100000: episode: 757, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 43.058, mean reward: 2.533 [1.810, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.430, 10.100], loss: 0.918007, mae: 0.775652, mean_q: 4.954187
 47119/100000: episode: 758, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 92.397, mean reward: 4.863 [2.431, 9.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.390, 10.100], loss: 0.643219, mae: 0.659193, mean_q: 5.028522
 47137/100000: episode: 759, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 55.568, mean reward: 3.087 [2.339, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.280, 10.100], loss: 0.484944, mae: 0.591609, mean_q: 5.010104
 47152/100000: episode: 760, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 41.792, mean reward: 2.786 [2.202, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.391, 10.100], loss: 0.893883, mae: 0.666831, mean_q: 5.005111
 47168/100000: episode: 761, duration: 0.084s, episode steps: 16, steps per second: 189, episode reward: 45.298, mean reward: 2.831 [2.362, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.083, 10.100], loss: 0.498149, mae: 0.591326, mean_q: 4.982711
 47183/100000: episode: 762, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 63.909, mean reward: 4.261 [2.572, 6.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.400, 10.100], loss: 0.978512, mae: 0.653075, mean_q: 4.972821
 47198/100000: episode: 763, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 42.795, mean reward: 2.853 [2.263, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.292, 10.100], loss: 0.443272, mae: 0.597098, mean_q: 5.037953
 47216/100000: episode: 764, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 60.092, mean reward: 3.338 [2.458, 5.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.392, 10.100], loss: 0.710999, mae: 0.640166, mean_q: 4.960209
 47233/100000: episode: 765, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 63.432, mean reward: 3.731 [2.876, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.637, 10.100], loss: 0.724185, mae: 0.606993, mean_q: 4.873279
 47250/100000: episode: 766, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 48.398, mean reward: 2.847 [2.253, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.335, 10.100], loss: 1.329986, mae: 0.638546, mean_q: 5.007835
 47266/100000: episode: 767, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 55.355, mean reward: 3.460 [2.126, 5.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.423, 10.100], loss: 0.561203, mae: 0.646777, mean_q: 5.027684
 47283/100000: episode: 768, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 45.264, mean reward: 2.663 [2.054, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.272, 10.100], loss: 0.850343, mae: 0.646208, mean_q: 5.069026
 47300/100000: episode: 769, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 58.834, mean reward: 3.461 [2.314, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.316, 10.100], loss: 0.505470, mae: 0.610029, mean_q: 4.989158
 47317/100000: episode: 770, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 52.176, mean reward: 3.069 [2.450, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.341, 10.100], loss: 0.910916, mae: 0.669029, mean_q: 5.072849
 47334/100000: episode: 771, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 49.871, mean reward: 2.934 [1.920, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.417, 10.100], loss: 0.400814, mae: 0.526478, mean_q: 4.951351
 47349/100000: episode: 772, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 53.826, mean reward: 3.588 [2.774, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.388, 10.100], loss: 0.737249, mae: 0.637024, mean_q: 4.975985
 47366/100000: episode: 773, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 45.077, mean reward: 2.652 [2.107, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.874, 10.100], loss: 0.566909, mae: 0.611787, mean_q: 4.963366
 47385/100000: episode: 774, duration: 0.106s, episode steps: 19, steps per second: 178, episode reward: 76.413, mean reward: 4.022 [2.477, 7.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.309, 10.100], loss: 0.459770, mae: 0.575739, mean_q: 4.964382
 47404/100000: episode: 775, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 62.342, mean reward: 3.281 [2.412, 7.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.057, 10.100], loss: 1.600553, mae: 0.738285, mean_q: 4.995800
 47422/100000: episode: 776, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 52.993, mean reward: 2.944 [2.531, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.288, 10.100], loss: 0.446264, mae: 0.589036, mean_q: 5.055823
 47439/100000: episode: 777, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 46.505, mean reward: 2.736 [2.156, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.419, 10.100], loss: 0.690828, mae: 0.587731, mean_q: 4.978681
 47454/100000: episode: 778, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 47.111, mean reward: 3.141 [2.066, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.241, 10.100], loss: 1027.586304, mae: 2.722372, mean_q: 5.136959
 47471/100000: episode: 779, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 41.678, mean reward: 2.452 [1.897, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.320, 10.100], loss: 2.012187, mae: 1.444105, mean_q: 6.169563
 47489/100000: episode: 780, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 73.957, mean reward: 4.109 [2.716, 7.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.507, 10.100], loss: 0.846460, mae: 0.858689, mean_q: 5.342134
 47507/100000: episode: 781, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 56.880, mean reward: 3.160 [2.240, 5.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.292, 10.100], loss: 856.733704, mae: 2.603495, mean_q: 5.437826
 47524/100000: episode: 782, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 60.595, mean reward: 3.564 [2.832, 7.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-2.483, 10.100], loss: 1.814577, mae: 1.310307, mean_q: 6.229511
 47541/100000: episode: 783, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 48.703, mean reward: 2.865 [2.277, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.392, 10.100], loss: 0.952325, mae: 0.769381, mean_q: 5.178985
 47558/100000: episode: 784, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 54.501, mean reward: 3.206 [2.154, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.088, 10.100], loss: 0.879639, mae: 0.736346, mean_q: 5.181374
 47575/100000: episode: 785, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 51.771, mean reward: 3.045 [2.469, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.340, 10.100], loss: 0.899060, mae: 0.707378, mean_q: 5.203742
 47592/100000: episode: 786, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 45.488, mean reward: 2.676 [2.152, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.025, 10.100], loss: 0.464556, mae: 0.611850, mean_q: 5.240321
 47610/100000: episode: 787, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 60.970, mean reward: 3.387 [2.840, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.603, 10.100], loss: 1.003436, mae: 0.736267, mean_q: 5.255501
 47627/100000: episode: 788, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 48.751, mean reward: 2.868 [2.221, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.279, 10.100], loss: 0.560490, mae: 0.570720, mean_q: 5.250210
 47642/100000: episode: 789, duration: 0.085s, episode steps: 15, steps per second: 175, episode reward: 42.281, mean reward: 2.819 [1.930, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.338, 10.100], loss: 0.542518, mae: 0.619401, mean_q: 5.178294
 47659/100000: episode: 790, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 47.017, mean reward: 2.766 [2.460, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.501, 10.100], loss: 1.142971, mae: 0.695309, mean_q: 5.172829
 47675/100000: episode: 791, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 49.244, mean reward: 3.078 [2.088, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.932, 10.100], loss: 0.712505, mae: 0.621494, mean_q: 5.266980
 47692/100000: episode: 792, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 63.769, mean reward: 3.751 [2.082, 5.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-2.300, 10.100], loss: 0.578949, mae: 0.648209, mean_q: 5.149395
 47711/100000: episode: 793, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 50.715, mean reward: 2.669 [2.123, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.272, 10.100], loss: 0.704262, mae: 0.711605, mean_q: 5.213588
 47728/100000: episode: 794, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 56.524, mean reward: 3.325 [2.749, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.207, 10.100], loss: 1.279169, mae: 0.707614, mean_q: 5.118791
 47747/100000: episode: 795, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 76.093, mean reward: 4.005 [3.183, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.375, 10.100], loss: 0.637791, mae: 0.609734, mean_q: 5.080865
 47762/100000: episode: 796, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 48.454, mean reward: 3.230 [2.697, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.325, 10.100], loss: 0.418262, mae: 0.597455, mean_q: 5.159874
 47780/100000: episode: 797, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 53.428, mean reward: 2.968 [2.283, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.377, 10.100], loss: 0.565174, mae: 0.655671, mean_q: 5.145247
 47795/100000: episode: 798, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 47.947, mean reward: 3.196 [2.296, 5.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.066, 10.100], loss: 0.473227, mae: 0.618834, mean_q: 5.075051
 47811/100000: episode: 799, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 69.076, mean reward: 4.317 [2.765, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.446, 10.100], loss: 0.521379, mae: 0.645497, mean_q: 5.115488
 47828/100000: episode: 800, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 47.493, mean reward: 2.794 [2.360, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.356, 10.100], loss: 0.344289, mae: 0.563971, mean_q: 5.093597
 47845/100000: episode: 801, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 61.779, mean reward: 3.634 [2.673, 5.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.677, 10.100], loss: 0.527393, mae: 0.599085, mean_q: 5.150460
 47863/100000: episode: 802, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 57.415, mean reward: 3.190 [1.991, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.760, 10.100], loss: 0.441196, mae: 0.623513, mean_q: 5.105935
 47880/100000: episode: 803, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 47.018, mean reward: 2.766 [1.854, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.125, 10.100], loss: 905.927368, mae: 2.611796, mean_q: 5.426905
 47898/100000: episode: 804, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 53.723, mean reward: 2.985 [2.358, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.520, 10.100], loss: 2.891540, mae: 1.679017, mean_q: 6.621457
 47916/100000: episode: 805, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 58.489, mean reward: 3.249 [2.807, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.273, 10.100], loss: 855.545837, mae: 2.557897, mean_q: 5.343621
 47932/100000: episode: 806, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 82.614, mean reward: 5.163 [2.243, 7.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.473, 10.100], loss: 963.160339, mae: 5.025153, mean_q: 8.365980
 47947/100000: episode: 807, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 99.239, mean reward: 6.616 [3.498, 16.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.605, 10.100], loss: 1.972511, mae: 1.326974, mean_q: 5.178691
 47964/100000: episode: 808, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 51.986, mean reward: 3.058 [2.514, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.310, 10.100], loss: 1.505220, mae: 1.025886, mean_q: 5.143538
[Info] Complete ISplit Iteration
[Info] Levels: [5.5509543, 7.3947964, 8.9714365]
[Info] Cond. Prob: [0.1, 0.1, 0.29]
[Info] Error Prob: 0.0029000000000000002

 47979/100000: episode: 809, duration: 4.502s, episode steps: 15, steps per second: 3, episode reward: 44.735, mean reward: 2.982 [2.350, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.582, 10.100], loss: 1.712058, mae: 0.891492, mean_q: 5.631432
 48079/100000: episode: 810, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.751, mean reward: 1.818 [1.436, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.411, 10.098], loss: 0.883972, mae: 0.727053, mean_q: 5.469139
 48179/100000: episode: 811, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 205.854, mean reward: 2.059 [1.463, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.337, 10.098], loss: 0.752098, mae: 0.681912, mean_q: 5.399038
 48279/100000: episode: 812, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.745, mean reward: 1.907 [1.473, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.308, 10.338], loss: 0.690091, mae: 0.666954, mean_q: 5.330356
 48379/100000: episode: 813, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 196.428, mean reward: 1.964 [1.474, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.271, 10.114], loss: 308.702454, mae: 1.694785, mean_q: 5.816851
 48479/100000: episode: 814, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 198.652, mean reward: 1.987 [1.472, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.665, 10.098], loss: 1.155984, mae: 0.813357, mean_q: 5.582264
 48579/100000: episode: 815, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 177.692, mean reward: 1.777 [1.444, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.083, 10.233], loss: 308.208191, mae: 1.661409, mean_q: 5.843065
 48679/100000: episode: 816, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 176.572, mean reward: 1.766 [1.446, 2.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.098], loss: 1.122671, mae: 0.807155, mean_q: 5.540758
 48779/100000: episode: 817, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.406, mean reward: 1.924 [1.449, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.650, 10.107], loss: 154.081818, mae: 1.069920, mean_q: 5.559314
 48879/100000: episode: 818, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 184.534, mean reward: 1.845 [1.455, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.927, 10.098], loss: 461.161713, mae: 2.621341, mean_q: 6.664893
 48979/100000: episode: 819, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 185.034, mean reward: 1.850 [1.454, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.289, 10.098], loss: 308.064209, mae: 1.774493, mean_q: 6.423336
 49079/100000: episode: 820, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 200.160, mean reward: 2.002 [1.446, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.036, 10.201], loss: 0.818583, mae: 0.723520, mean_q: 5.575616
 49179/100000: episode: 821, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 208.455, mean reward: 2.085 [1.478, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.619, 10.379], loss: 153.633286, mae: 0.963793, mean_q: 5.452276
 49279/100000: episode: 822, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.968, mean reward: 1.900 [1.432, 5.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.050, 10.098], loss: 1.300767, mae: 0.926713, mean_q: 5.688457
 49379/100000: episode: 823, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.887, mean reward: 1.919 [1.521, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.536, 10.098], loss: 0.866362, mae: 0.820122, mean_q: 5.639494
 49479/100000: episode: 824, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.457, mean reward: 1.875 [1.435, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.098], loss: 0.585445, mae: 0.637849, mean_q: 5.418658
 49579/100000: episode: 825, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.695, mean reward: 1.857 [1.479, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.648, 10.098], loss: 460.689484, mae: 2.386138, mean_q: 6.555912
 49679/100000: episode: 826, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.936, mean reward: 1.829 [1.466, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.310, 10.160], loss: 154.158401, mae: 1.279801, mean_q: 5.858439
 49779/100000: episode: 827, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 213.347, mean reward: 2.133 [1.448, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.547, 10.189], loss: 0.663752, mae: 0.684057, mean_q: 5.446527
 49879/100000: episode: 828, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.123, mean reward: 1.971 [1.460, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.234, 10.098], loss: 153.481339, mae: 0.927797, mean_q: 5.319118
 49979/100000: episode: 829, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 203.564, mean reward: 2.036 [1.448, 5.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-3.084, 10.098], loss: 306.733185, mae: 1.842299, mean_q: 6.085778
 50079/100000: episode: 830, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.446, mean reward: 1.824 [1.470, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.172, 10.113], loss: 0.715775, mae: 0.666885, mean_q: 5.354382
 50179/100000: episode: 831, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 180.659, mean reward: 1.807 [1.471, 2.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.618, 10.098], loss: 154.276794, mae: 1.220975, mean_q: 5.582426
 50279/100000: episode: 832, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 195.764, mean reward: 1.958 [1.456, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.484, 10.104], loss: 153.822357, mae: 1.084688, mean_q: 5.403384
 50379/100000: episode: 833, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 218.999, mean reward: 2.190 [1.537, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.098], loss: 307.149811, mae: 1.586085, mean_q: 5.662284
 50479/100000: episode: 834, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.338, mean reward: 1.833 [1.471, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.670, 10.196], loss: 306.621857, mae: 1.870590, mean_q: 6.071478
 50579/100000: episode: 835, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 213.963, mean reward: 2.140 [1.489, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.289], loss: 0.557240, mae: 0.624048, mean_q: 5.204411
 50679/100000: episode: 836, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.785, mean reward: 1.988 [1.470, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.443, 10.149], loss: 0.431285, mae: 0.562636, mean_q: 5.024542
 50779/100000: episode: 837, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 191.028, mean reward: 1.910 [1.451, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.376, 10.302], loss: 0.657866, mae: 0.583601, mean_q: 4.995626
 50879/100000: episode: 838, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 177.424, mean reward: 1.774 [1.464, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.625, 10.098], loss: 459.632355, mae: 2.223306, mean_q: 5.874715
 50979/100000: episode: 839, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 197.490, mean reward: 1.975 [1.436, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.812, 10.098], loss: 153.685074, mae: 1.285900, mean_q: 5.633165
 51079/100000: episode: 840, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 221.044, mean reward: 2.210 [1.480, 9.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.246], loss: 306.148346, mae: 1.870347, mean_q: 5.881285
 51179/100000: episode: 841, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 205.842, mean reward: 2.058 [1.479, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.012, 10.442], loss: 305.931183, mae: 1.543211, mean_q: 5.580064
 51279/100000: episode: 842, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.224, mean reward: 1.882 [1.470, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.157, 10.144], loss: 153.194672, mae: 1.052275, mean_q: 5.200734
 51379/100000: episode: 843, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 191.143, mean reward: 1.911 [1.456, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.013, 10.125], loss: 154.042633, mae: 1.361321, mean_q: 5.748951
 51479/100000: episode: 844, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 192.050, mean reward: 1.921 [1.439, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.262, 10.112], loss: 153.067413, mae: 1.063354, mean_q: 5.245769
 51579/100000: episode: 845, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 189.335, mean reward: 1.893 [1.469, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.132, 10.407], loss: 0.430509, mae: 0.542594, mean_q: 4.727412
 51679/100000: episode: 846, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.081, mean reward: 1.911 [1.453, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.129, 10.098], loss: 0.291483, mae: 0.468791, mean_q: 4.552056
 51779/100000: episode: 847, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.574, mean reward: 1.816 [1.483, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.121], loss: 0.285092, mae: 0.458741, mean_q: 4.491804
 51879/100000: episode: 848, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.665, mean reward: 1.957 [1.517, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.160, 10.220], loss: 0.260431, mae: 0.428780, mean_q: 4.351488
 51979/100000: episode: 849, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 183.396, mean reward: 1.834 [1.463, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.123], loss: 0.219434, mae: 0.407331, mean_q: 4.271175
 52079/100000: episode: 850, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.176, mean reward: 1.802 [1.437, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.383, 10.098], loss: 0.204519, mae: 0.384148, mean_q: 4.252953
 52179/100000: episode: 851, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 204.088, mean reward: 2.041 [1.483, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.473, 10.098], loss: 0.199857, mae: 0.382849, mean_q: 4.162097
 52279/100000: episode: 852, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 175.948, mean reward: 1.759 [1.462, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.158], loss: 0.213937, mae: 0.367301, mean_q: 4.129390
 52379/100000: episode: 853, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 212.297, mean reward: 2.123 [1.543, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.862, 10.098], loss: 0.182227, mae: 0.367824, mean_q: 4.080174
 52479/100000: episode: 854, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.571, mean reward: 1.966 [1.443, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.717, 10.098], loss: 0.143630, mae: 0.357122, mean_q: 4.074018
 52579/100000: episode: 855, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 184.290, mean reward: 1.843 [1.440, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.514, 10.180], loss: 0.120561, mae: 0.330743, mean_q: 4.003334
 52679/100000: episode: 856, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 206.200, mean reward: 2.062 [1.527, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.034, 10.098], loss: 0.121117, mae: 0.323377, mean_q: 4.006681
 52779/100000: episode: 857, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 204.684, mean reward: 2.047 [1.466, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.238, 10.313], loss: 0.119286, mae: 0.316719, mean_q: 3.931479
 52879/100000: episode: 858, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 205.523, mean reward: 2.055 [1.512, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.759, 10.174], loss: 0.125962, mae: 0.313590, mean_q: 3.914417
 52979/100000: episode: 859, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 201.680, mean reward: 2.017 [1.459, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.186, 10.251], loss: 0.104775, mae: 0.306879, mean_q: 3.873545
 53079/100000: episode: 860, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.673, mean reward: 1.817 [1.440, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.455, 10.112], loss: 0.084465, mae: 0.289081, mean_q: 3.832185
 53179/100000: episode: 861, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 212.384, mean reward: 2.124 [1.456, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.913, 10.342], loss: 0.092219, mae: 0.292616, mean_q: 3.830960
 53279/100000: episode: 862, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 200.286, mean reward: 2.003 [1.481, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.363, 10.232], loss: 0.093106, mae: 0.300882, mean_q: 3.840489
 53379/100000: episode: 863, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 193.597, mean reward: 1.936 [1.454, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.303], loss: 0.102140, mae: 0.300681, mean_q: 3.842446
 53479/100000: episode: 864, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.061, mean reward: 1.861 [1.447, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.572, 10.269], loss: 0.099258, mae: 0.296657, mean_q: 3.829517
 53579/100000: episode: 865, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 195.925, mean reward: 1.959 [1.463, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.052, 10.340], loss: 0.081920, mae: 0.296350, mean_q: 3.842864
 53679/100000: episode: 866, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 199.496, mean reward: 1.995 [1.470, 5.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.350, 10.427], loss: 0.085853, mae: 0.292232, mean_q: 3.847264
 53779/100000: episode: 867, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.737, mean reward: 1.847 [1.448, 2.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.381, 10.108], loss: 0.085338, mae: 0.300669, mean_q: 3.856024
 53879/100000: episode: 868, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 241.942, mean reward: 2.419 [1.443, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.066, 10.098], loss: 0.091579, mae: 0.291024, mean_q: 3.849765
 53979/100000: episode: 869, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.099, mean reward: 1.901 [1.442, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.155], loss: 0.085486, mae: 0.301803, mean_q: 3.880009
 54079/100000: episode: 870, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.128, mean reward: 1.951 [1.464, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.264], loss: 0.081301, mae: 0.296757, mean_q: 3.877480
 54179/100000: episode: 871, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 190.921, mean reward: 1.909 [1.458, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.447, 10.340], loss: 0.087770, mae: 0.300028, mean_q: 3.868793
 54279/100000: episode: 872, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 225.618, mean reward: 2.256 [1.434, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.672, 10.374], loss: 0.076530, mae: 0.287664, mean_q: 3.861014
 54379/100000: episode: 873, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.934, mean reward: 2.009 [1.434, 5.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.190], loss: 0.087476, mae: 0.298650, mean_q: 3.869553
 54479/100000: episode: 874, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.485, mean reward: 1.815 [1.433, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.204], loss: 0.113588, mae: 0.311238, mean_q: 3.898892
 54579/100000: episode: 875, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.788, mean reward: 1.838 [1.442, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.022, 10.156], loss: 0.083524, mae: 0.294666, mean_q: 3.868156
 54679/100000: episode: 876, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.340, mean reward: 1.863 [1.462, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.268, 10.217], loss: 0.086422, mae: 0.302192, mean_q: 3.890054
 54779/100000: episode: 877, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 201.043, mean reward: 2.010 [1.461, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.495, 10.098], loss: 0.093158, mae: 0.301951, mean_q: 3.867610
 54879/100000: episode: 878, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.465, mean reward: 2.025 [1.498, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.310, 10.163], loss: 0.097559, mae: 0.301892, mean_q: 3.876177
 54979/100000: episode: 879, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 193.439, mean reward: 1.934 [1.456, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.524, 10.281], loss: 0.094562, mae: 0.289761, mean_q: 3.852026
 55079/100000: episode: 880, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 213.065, mean reward: 2.131 [1.486, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.269, 10.370], loss: 0.084190, mae: 0.293050, mean_q: 3.879036
 55179/100000: episode: 881, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.022, mean reward: 1.880 [1.451, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.609, 10.098], loss: 0.089468, mae: 0.299257, mean_q: 3.885763
 55279/100000: episode: 882, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 208.629, mean reward: 2.086 [1.448, 7.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.932, 10.434], loss: 0.079192, mae: 0.298045, mean_q: 3.881720
 55379/100000: episode: 883, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.011, mean reward: 1.960 [1.452, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.277, 10.166], loss: 0.102121, mae: 0.310560, mean_q: 3.889516
 55479/100000: episode: 884, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 195.275, mean reward: 1.953 [1.432, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.290, 10.217], loss: 0.082467, mae: 0.297576, mean_q: 3.893327
 55579/100000: episode: 885, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 190.134, mean reward: 1.901 [1.452, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.718, 10.098], loss: 0.081641, mae: 0.289699, mean_q: 3.872507
 55679/100000: episode: 886, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.828, mean reward: 1.968 [1.436, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.098], loss: 0.082880, mae: 0.290980, mean_q: 3.868429
 55779/100000: episode: 887, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 208.054, mean reward: 2.081 [1.487, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.216, 10.098], loss: 0.085753, mae: 0.299844, mean_q: 3.888030
 55879/100000: episode: 888, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.829, mean reward: 1.928 [1.458, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.177, 10.117], loss: 0.081232, mae: 0.293765, mean_q: 3.887295
 55979/100000: episode: 889, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 181.598, mean reward: 1.816 [1.451, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.162], loss: 0.096288, mae: 0.304083, mean_q: 3.887387
 56079/100000: episode: 890, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 176.581, mean reward: 1.766 [1.450, 2.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.137], loss: 0.081338, mae: 0.289135, mean_q: 3.867075
 56179/100000: episode: 891, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 203.426, mean reward: 2.034 [1.478, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.098], loss: 0.081162, mae: 0.293155, mean_q: 3.883315
 56279/100000: episode: 892, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.607, mean reward: 1.876 [1.441, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.928, 10.098], loss: 0.088755, mae: 0.302345, mean_q: 3.875410
 56379/100000: episode: 893, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.769, mean reward: 1.948 [1.454, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.098], loss: 0.080778, mae: 0.289929, mean_q: 3.874289
 56479/100000: episode: 894, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.532, mean reward: 1.985 [1.442, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.573, 10.458], loss: 0.075772, mae: 0.283180, mean_q: 3.858254
 56579/100000: episode: 895, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.142, mean reward: 1.881 [1.442, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.647, 10.311], loss: 0.082250, mae: 0.282890, mean_q: 3.865266
 56679/100000: episode: 896, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.110, mean reward: 1.901 [1.516, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.402, 10.131], loss: 0.089871, mae: 0.296707, mean_q: 3.876489
 56779/100000: episode: 897, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 197.560, mean reward: 1.976 [1.436, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.589, 10.464], loss: 0.088058, mae: 0.300655, mean_q: 3.860275
 56879/100000: episode: 898, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 184.198, mean reward: 1.842 [1.468, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.617, 10.098], loss: 0.076649, mae: 0.288608, mean_q: 3.862807
 56979/100000: episode: 899, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 235.269, mean reward: 2.353 [1.470, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.455, 10.493], loss: 0.086163, mae: 0.304213, mean_q: 3.884757
 57079/100000: episode: 900, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 199.105, mean reward: 1.991 [1.486, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.324, 10.274], loss: 0.085549, mae: 0.300538, mean_q: 3.886724
 57179/100000: episode: 901, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 196.697, mean reward: 1.967 [1.441, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.151], loss: 0.085931, mae: 0.298598, mean_q: 3.892133
 57279/100000: episode: 902, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 199.891, mean reward: 1.999 [1.433, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.346], loss: 0.097795, mae: 0.312418, mean_q: 3.904695
 57379/100000: episode: 903, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.211, mean reward: 1.912 [1.455, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.214, 10.117], loss: 0.095837, mae: 0.305636, mean_q: 3.903171
 57479/100000: episode: 904, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 195.943, mean reward: 1.959 [1.479, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.510, 10.249], loss: 0.092691, mae: 0.304500, mean_q: 3.902684
 57579/100000: episode: 905, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 186.781, mean reward: 1.868 [1.432, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.839, 10.098], loss: 0.101284, mae: 0.311688, mean_q: 3.913800
 57679/100000: episode: 906, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 200.685, mean reward: 2.007 [1.443, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.150, 10.306], loss: 0.093608, mae: 0.307050, mean_q: 3.914413
 57779/100000: episode: 907, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 184.362, mean reward: 1.844 [1.454, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.172], loss: 0.090282, mae: 0.301532, mean_q: 3.911618
 57879/100000: episode: 908, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.508, mean reward: 1.835 [1.449, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.098], loss: 0.086789, mae: 0.294927, mean_q: 3.898657
[Info] 1-TH LEVEL FOUND: 5.080731391906738, Considering 10/90 traces
 57979/100000: episode: 909, duration: 4.710s, episode steps: 100, steps per second: 21, episode reward: 198.621, mean reward: 1.986 [1.451, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.098], loss: 0.082747, mae: 0.291335, mean_q: 3.880757
 57988/100000: episode: 910, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 19.817, mean reward: 2.202 [1.902, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.234, 10.100], loss: 0.091167, mae: 0.308914, mean_q: 3.909517
 57997/100000: episode: 911, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 23.275, mean reward: 2.586 [2.134, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.078, 10.100], loss: 0.067567, mae: 0.258145, mean_q: 3.875000
 58005/100000: episode: 912, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 18.255, mean reward: 2.282 [1.789, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.338, 10.100], loss: 0.072051, mae: 0.290459, mean_q: 3.926925
 58012/100000: episode: 913, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 16.550, mean reward: 2.364 [2.172, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.263, 10.100], loss: 0.079505, mae: 0.269649, mean_q: 3.778577
 58019/100000: episode: 914, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 15.997, mean reward: 2.285 [2.032, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.858, 10.100], loss: 0.064312, mae: 0.278808, mean_q: 3.895918
 58028/100000: episode: 915, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 22.754, mean reward: 2.528 [2.316, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.260, 10.100], loss: 0.083084, mae: 0.283751, mean_q: 3.915158
 58036/100000: episode: 916, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 17.842, mean reward: 2.230 [1.940, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.256, 10.100], loss: 0.110246, mae: 0.344259, mean_q: 3.947767
 58045/100000: episode: 917, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 20.712, mean reward: 2.301 [1.925, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.363, 10.100], loss: 0.083697, mae: 0.288891, mean_q: 3.838609
 58061/100000: episode: 918, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 44.862, mean reward: 2.804 [2.174, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.658, 10.100], loss: 0.105495, mae: 0.313533, mean_q: 3.917465
 58077/100000: episode: 919, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 51.274, mean reward: 3.205 [2.387, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.426, 10.100], loss: 0.099288, mae: 0.314279, mean_q: 3.902387
 58086/100000: episode: 920, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 19.451, mean reward: 2.161 [1.880, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.192, 10.100], loss: 0.082414, mae: 0.279036, mean_q: 3.900622
 58095/100000: episode: 921, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 21.312, mean reward: 2.368 [1.896, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.100], loss: 0.077456, mae: 0.282026, mean_q: 3.846427
 58107/100000: episode: 922, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 29.601, mean reward: 2.467 [2.141, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.297, 10.100], loss: 0.069598, mae: 0.275310, mean_q: 3.869678
 58116/100000: episode: 923, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 30.944, mean reward: 3.438 [2.227, 10.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.291, 10.100], loss: 0.088033, mae: 0.295430, mean_q: 3.853714
 58122/100000: episode: 924, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 11.972, mean reward: 1.995 [1.670, 2.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.569, 10.100], loss: 0.143001, mae: 0.304000, mean_q: 3.884370
 58134/100000: episode: 925, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 27.193, mean reward: 2.266 [1.840, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.135, 10.100], loss: 0.063381, mae: 0.272521, mean_q: 3.854373
 58143/100000: episode: 926, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 22.152, mean reward: 2.461 [2.033, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.422, 10.100], loss: 0.075530, mae: 0.281768, mean_q: 3.907979
 58156/100000: episode: 927, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 45.202, mean reward: 3.477 [2.978, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.644, 10.100], loss: 0.099961, mae: 0.316494, mean_q: 3.893649
 58169/100000: episode: 928, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 41.546, mean reward: 3.196 [2.362, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.223, 10.100], loss: 0.065519, mae: 0.278135, mean_q: 3.912833
 58176/100000: episode: 929, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 15.384, mean reward: 2.198 [1.794, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.245, 10.100], loss: 0.086295, mae: 0.278914, mean_q: 3.825840
 58183/100000: episode: 930, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 13.353, mean reward: 1.908 [1.719, 2.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.185, 10.100], loss: 0.085861, mae: 0.304268, mean_q: 3.874632
 58195/100000: episode: 931, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 32.475, mean reward: 2.706 [2.263, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.218, 10.100], loss: 0.087758, mae: 0.297428, mean_q: 3.883056
 58203/100000: episode: 932, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 15.504, mean reward: 1.938 [1.632, 2.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.289, 10.100], loss: 0.072093, mae: 0.273398, mean_q: 3.937425
 58210/100000: episode: 933, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 15.047, mean reward: 2.150 [1.908, 2.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.250, 10.100], loss: 0.097554, mae: 0.301811, mean_q: 3.908813
 58217/100000: episode: 934, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 13.982, mean reward: 1.997 [1.926, 2.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.216, 10.100], loss: 0.224260, mae: 0.342781, mean_q: 3.982810
 58233/100000: episode: 935, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 58.427, mean reward: 3.652 [1.948, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.815, 10.100], loss: 0.158597, mae: 0.306540, mean_q: 3.856066
 58242/100000: episode: 936, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 21.453, mean reward: 2.384 [2.036, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.719, 10.100], loss: 0.083953, mae: 0.298467, mean_q: 3.946987
 58249/100000: episode: 937, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 14.357, mean reward: 2.051 [1.888, 2.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.285, 10.100], loss: 0.109936, mae: 0.315605, mean_q: 3.918198
 58258/100000: episode: 938, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 34.001, mean reward: 3.778 [3.168, 4.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.386, 10.100], loss: 0.071537, mae: 0.283784, mean_q: 3.933037
 58274/100000: episode: 939, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 53.021, mean reward: 3.314 [2.334, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.466, 10.100], loss: 0.126031, mae: 0.340263, mean_q: 3.961032
 58281/100000: episode: 940, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 14.555, mean reward: 2.079 [1.840, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.244, 10.100], loss: 0.113262, mae: 0.322849, mean_q: 3.864130
 58288/100000: episode: 941, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 15.829, mean reward: 2.261 [1.781, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.357, 10.100], loss: 0.117352, mae: 0.327667, mean_q: 3.940211
 58298/100000: episode: 942, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 21.965, mean reward: 2.196 [1.893, 2.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.174, 10.100], loss: 0.185730, mae: 0.328846, mean_q: 4.023793
 58311/100000: episode: 943, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 52.692, mean reward: 4.053 [2.885, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.410, 10.100], loss: 0.118534, mae: 0.326397, mean_q: 3.939387
 58321/100000: episode: 944, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 21.297, mean reward: 2.130 [1.866, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.365, 10.100], loss: 0.115735, mae: 0.314498, mean_q: 3.920992
 58329/100000: episode: 945, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 18.767, mean reward: 2.346 [1.873, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.308, 10.100], loss: 0.111699, mae: 0.348161, mean_q: 3.997282
 58335/100000: episode: 946, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 12.831, mean reward: 2.139 [1.527, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.859, 10.100], loss: 0.103454, mae: 0.314500, mean_q: 3.964517
 58342/100000: episode: 947, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 15.490, mean reward: 2.213 [1.973, 2.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.209, 10.100], loss: 0.097240, mae: 0.313528, mean_q: 3.888442
 58354/100000: episode: 948, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 24.899, mean reward: 2.075 [1.756, 2.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.232, 10.100], loss: 0.171032, mae: 0.393859, mean_q: 4.007071
 58364/100000: episode: 949, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 21.500, mean reward: 2.150 [1.935, 2.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.461, 10.100], loss: 0.089539, mae: 0.317264, mean_q: 3.929051
 58374/100000: episode: 950, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 22.568, mean reward: 2.257 [1.848, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.193, 10.100], loss: 0.104726, mae: 0.326709, mean_q: 3.957774
 58383/100000: episode: 951, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 25.265, mean reward: 2.807 [2.214, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.216, 10.100], loss: 0.101140, mae: 0.323208, mean_q: 3.976945
 58390/100000: episode: 952, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 14.328, mean reward: 2.047 [1.908, 2.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.306, 10.100], loss: 0.243263, mae: 0.372462, mean_q: 3.946477
 58397/100000: episode: 953, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 15.263, mean reward: 2.180 [1.972, 2.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.280, 10.100], loss: 0.112830, mae: 0.348863, mean_q: 4.015830
 58409/100000: episode: 954, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 26.486, mean reward: 2.207 [1.778, 2.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.400, 10.100], loss: 0.075466, mae: 0.276842, mean_q: 3.851106
 58416/100000: episode: 955, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 17.944, mean reward: 2.563 [2.159, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.349, 10.100], loss: 0.183173, mae: 0.398969, mean_q: 4.029736
 58432/100000: episode: 956, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 46.942, mean reward: 2.934 [2.339, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.077, 10.100], loss: 0.088003, mae: 0.305534, mean_q: 3.963864
 58439/100000: episode: 957, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 15.365, mean reward: 2.195 [2.034, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.178, 10.100], loss: 0.095808, mae: 0.325852, mean_q: 3.985828
 58449/100000: episode: 958, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 26.918, mean reward: 2.692 [1.989, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.160, 10.100], loss: 0.107760, mae: 0.320393, mean_q: 3.928999
 58456/100000: episode: 959, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 13.376, mean reward: 1.911 [1.613, 2.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.129, 10.100], loss: 0.231532, mae: 0.353029, mean_q: 4.009869
 58468/100000: episode: 960, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 27.791, mean reward: 2.316 [1.889, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.059, 10.100], loss: 0.134850, mae: 0.347964, mean_q: 4.044923
 58476/100000: episode: 961, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 15.008, mean reward: 1.876 [1.634, 2.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.334, 10.100], loss: 0.074624, mae: 0.292280, mean_q: 3.964647
 58485/100000: episode: 962, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 22.952, mean reward: 2.550 [2.257, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.182, 10.100], loss: 0.158906, mae: 0.351360, mean_q: 3.982017
 58501/100000: episode: 963, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 47.133, mean reward: 2.946 [2.213, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.404, 10.100], loss: 0.089878, mae: 0.290480, mean_q: 3.891277
 58511/100000: episode: 964, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 20.573, mean reward: 2.057 [1.859, 2.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.275, 10.100], loss: 0.105129, mae: 0.329372, mean_q: 4.011639
 58518/100000: episode: 965, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 14.524, mean reward: 2.075 [1.603, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.704, 10.100], loss: 0.117008, mae: 0.325107, mean_q: 3.940625
 58527/100000: episode: 966, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 25.298, mean reward: 2.811 [1.957, 4.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.341, 10.100], loss: 0.081815, mae: 0.294726, mean_q: 3.984407
 58537/100000: episode: 967, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 25.901, mean reward: 2.590 [1.922, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.287, 10.100], loss: 0.094233, mae: 0.308231, mean_q: 3.929997
 58544/100000: episode: 968, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 17.214, mean reward: 2.459 [2.189, 2.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.312, 10.100], loss: 0.081309, mae: 0.303977, mean_q: 3.948515
 58550/100000: episode: 969, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 13.911, mean reward: 2.318 [1.607, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.328, 10.100], loss: 0.260819, mae: 0.355173, mean_q: 3.955053
 58562/100000: episode: 970, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 31.449, mean reward: 2.621 [2.187, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.202, 10.100], loss: 0.132469, mae: 0.348345, mean_q: 4.038288
 58571/100000: episode: 971, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 25.467, mean reward: 2.830 [2.319, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.096, 10.100], loss: 0.097477, mae: 0.320521, mean_q: 3.979707
 58580/100000: episode: 972, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 24.340, mean reward: 2.704 [2.348, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.347, 10.100], loss: 0.086270, mae: 0.295711, mean_q: 3.905795
 58589/100000: episode: 973, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 20.249, mean reward: 2.250 [1.921, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.070, 10.100], loss: 0.085590, mae: 0.309422, mean_q: 3.961420
 58598/100000: episode: 974, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 25.110, mean reward: 2.790 [2.566, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.160, 10.100], loss: 0.114289, mae: 0.329845, mean_q: 3.905737
 58605/100000: episode: 975, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 14.305, mean reward: 2.044 [1.858, 2.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.154, 10.100], loss: 0.123202, mae: 0.319296, mean_q: 3.933997
 58621/100000: episode: 976, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 47.788, mean reward: 2.987 [2.104, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.464, 10.100], loss: 0.088703, mae: 0.300088, mean_q: 3.959396
 58629/100000: episode: 977, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 18.067, mean reward: 2.258 [1.804, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.308, 10.100], loss: 0.094512, mae: 0.312015, mean_q: 4.012233
 58635/100000: episode: 978, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 10.708, mean reward: 1.785 [1.617, 2.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.302, 10.100], loss: 0.112049, mae: 0.336515, mean_q: 3.953527
 58641/100000: episode: 979, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 12.254, mean reward: 2.042 [1.827, 2.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.238, 10.100], loss: 0.078986, mae: 0.289557, mean_q: 3.931452
 58654/100000: episode: 980, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 35.060, mean reward: 2.697 [2.057, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.213, 10.100], loss: 0.107032, mae: 0.326873, mean_q: 4.006558
 58661/100000: episode: 981, duration: 0.038s, episode steps: 7, steps per second: 187, episode reward: 15.426, mean reward: 2.204 [1.785, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.255, 10.100], loss: 0.089499, mae: 0.296820, mean_q: 3.964720
 58673/100000: episode: 982, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 33.924, mean reward: 2.827 [2.380, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.335, 10.100], loss: 0.108238, mae: 0.313256, mean_q: 3.935959
 58681/100000: episode: 983, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 17.296, mean reward: 2.162 [2.039, 2.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.443, 10.100], loss: 0.082528, mae: 0.317848, mean_q: 4.042295
 58697/100000: episode: 984, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 41.857, mean reward: 2.616 [1.888, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.376, 10.100], loss: 0.107860, mae: 0.324797, mean_q: 3.959560
 58704/100000: episode: 985, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 17.949, mean reward: 2.564 [1.884, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.212, 10.100], loss: 0.120020, mae: 0.342768, mean_q: 4.048826
 58720/100000: episode: 986, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 43.733, mean reward: 2.733 [1.907, 6.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.183, 10.100], loss: 0.107710, mae: 0.339728, mean_q: 3.946244
 58727/100000: episode: 987, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 15.642, mean reward: 2.235 [1.909, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.288, 10.100], loss: 0.100043, mae: 0.317089, mean_q: 4.044913
 58734/100000: episode: 988, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 12.761, mean reward: 1.823 [1.642, 2.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.177, 10.100], loss: 0.092305, mae: 0.297617, mean_q: 3.883415
 58750/100000: episode: 989, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 52.935, mean reward: 3.308 [1.927, 6.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.654, 10.100], loss: 0.090639, mae: 0.313133, mean_q: 3.998779
 58763/100000: episode: 990, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 39.367, mean reward: 3.028 [2.210, 7.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.223, 10.100], loss: 0.147970, mae: 0.367813, mean_q: 4.040679
 58771/100000: episode: 991, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 20.624, mean reward: 2.578 [2.109, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.174, 10.100], loss: 0.088544, mae: 0.299257, mean_q: 3.917150
 58787/100000: episode: 992, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 51.047, mean reward: 3.190 [2.205, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.357, 10.100], loss: 0.097183, mae: 0.331245, mean_q: 4.066971
 58796/100000: episode: 993, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 18.096, mean reward: 2.011 [1.780, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.254, 10.100], loss: 0.105516, mae: 0.311054, mean_q: 3.928376
 58805/100000: episode: 994, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 24.863, mean reward: 2.763 [2.213, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.310, 10.100], loss: 0.120415, mae: 0.341129, mean_q: 4.030118
 58812/100000: episode: 995, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 13.235, mean reward: 1.891 [1.727, 2.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.181, 10.100], loss: 0.118343, mae: 0.336595, mean_q: 3.973870
 58828/100000: episode: 996, duration: 0.096s, episode steps: 16, steps per second: 168, episode reward: 39.361, mean reward: 2.460 [2.170, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.821, 10.100], loss: 0.090203, mae: 0.317843, mean_q: 4.010002
 58838/100000: episode: 997, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 24.021, mean reward: 2.402 [1.951, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.149, 10.100], loss: 0.126564, mae: 0.319854, mean_q: 3.984709
 58850/100000: episode: 998, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 32.882, mean reward: 2.740 [2.139, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.282, 10.100], loss: 0.089562, mae: 0.312484, mean_q: 3.985953
[Info] 2-TH LEVEL FOUND: 7.025357723236084, Considering 10/90 traces
 58860/100000: episode: 999, duration: 4.170s, episode steps: 10, steps per second: 2, episode reward: 20.247, mean reward: 2.025 [1.833, 2.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.332, 10.100], loss: 0.148230, mae: 0.333273, mean_q: 4.044736
 58867/100000: episode: 1000, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 22.614, mean reward: 3.231 [2.499, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.304, 10.100], loss: 0.088634, mae: 0.322537, mean_q: 3.930481
 58874/100000: episode: 1001, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 39.675, mean reward: 5.668 [3.739, 12.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.420, 10.100], loss: 0.073637, mae: 0.310865, mean_q: 3.988900
 58882/100000: episode: 1002, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 26.692, mean reward: 3.337 [2.389, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.219, 10.100], loss: 0.237990, mae: 0.336763, mean_q: 3.898243
 58893/100000: episode: 1003, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 43.015, mean reward: 3.910 [2.955, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.317, 10.100], loss: 0.101910, mae: 0.335139, mean_q: 4.037244
 58904/100000: episode: 1004, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 29.977, mean reward: 2.725 [2.357, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.335, 10.100], loss: 0.189716, mae: 0.341522, mean_q: 4.043124
 58911/100000: episode: 1005, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 24.349, mean reward: 3.478 [2.911, 4.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.363, 10.100], loss: 0.100575, mae: 0.308617, mean_q: 4.062074
 58918/100000: episode: 1006, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 19.485, mean reward: 2.784 [2.062, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.241, 10.100], loss: 0.140849, mae: 0.344658, mean_q: 3.883430
 58925/100000: episode: 1007, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 21.365, mean reward: 3.052 [2.620, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.353, 10.100], loss: 0.140741, mae: 0.366247, mean_q: 4.104682
 58932/100000: episode: 1008, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 21.090, mean reward: 3.013 [2.667, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.412, 10.100], loss: 0.129687, mae: 0.359957, mean_q: 4.045900
 58939/100000: episode: 1009, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 27.380, mean reward: 3.911 [3.401, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.314, 10.100], loss: 0.132526, mae: 0.372722, mean_q: 4.112074
 58946/100000: episode: 1010, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 34.759, mean reward: 4.966 [3.313, 9.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.351, 10.100], loss: 0.266054, mae: 0.376314, mean_q: 3.982554
 58954/100000: episode: 1011, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 28.605, mean reward: 3.576 [2.940, 5.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.307, 10.100], loss: 0.113760, mae: 0.326755, mean_q: 4.016527
 58962/100000: episode: 1012, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 27.001, mean reward: 3.375 [2.954, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.977, 10.100], loss: 0.144381, mae: 0.343763, mean_q: 4.015741
 58969/100000: episode: 1013, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 23.158, mean reward: 3.308 [2.657, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.464, 10.100], loss: 0.169600, mae: 0.391723, mean_q: 4.139493
 58976/100000: episode: 1014, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 23.896, mean reward: 3.414 [2.916, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.351, 10.100], loss: 0.141742, mae: 0.366261, mean_q: 4.075774
 58983/100000: episode: 1015, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 23.145, mean reward: 3.306 [2.509, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.373, 10.100], loss: 0.207813, mae: 0.391635, mean_q: 4.078511
 58990/100000: episode: 1016, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 27.224, mean reward: 3.889 [3.067, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.426, 10.100], loss: 0.089142, mae: 0.323589, mean_q: 4.091713
 58997/100000: episode: 1017, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 24.170, mean reward: 3.453 [2.636, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.394, 10.100], loss: 0.085289, mae: 0.287896, mean_q: 3.978018
 59004/100000: episode: 1018, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 26.841, mean reward: 3.834 [3.213, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.262, 10.100], loss: 0.138883, mae: 0.341248, mean_q: 4.029440
 59012/100000: episode: 1019, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 20.428, mean reward: 2.553 [2.148, 3.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.516, 10.100], loss: 0.193288, mae: 0.374645, mean_q: 4.096212
 59021/100000: episode: 1020, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 30.052, mean reward: 3.339 [2.837, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.457, 10.100], loss: 0.180873, mae: 0.381078, mean_q: 4.014037
 59030/100000: episode: 1021, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 28.733, mean reward: 3.193 [2.656, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.425, 10.100], loss: 0.124382, mae: 0.357454, mean_q: 4.173014
 59038/100000: episode: 1022, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 32.670, mean reward: 4.084 [3.549, 5.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.327, 10.100], loss: 0.159699, mae: 0.377053, mean_q: 3.922928
 59046/100000: episode: 1023, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 38.864, mean reward: 4.858 [3.508, 8.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.463, 10.100], loss: 0.092528, mae: 0.331812, mean_q: 4.129184
 59053/100000: episode: 1024, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 23.947, mean reward: 3.421 [2.858, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.484, 10.100], loss: 0.233077, mae: 0.373819, mean_q: 4.057406
 59062/100000: episode: 1025, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 29.639, mean reward: 3.293 [2.976, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.332, 10.100], loss: 0.092887, mae: 0.322274, mean_q: 4.105000
 59070/100000: episode: 1026, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 26.635, mean reward: 3.329 [3.031, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.321, 10.100], loss: 0.120774, mae: 0.347075, mean_q: 4.123333
 59077/100000: episode: 1027, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 37.809, mean reward: 5.401 [4.377, 6.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.654, 10.100], loss: 0.120453, mae: 0.344866, mean_q: 4.103381
 59085/100000: episode: 1028, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 23.523, mean reward: 2.940 [2.613, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.310, 10.100], loss: 0.101312, mae: 0.315143, mean_q: 4.107491
 59096/100000: episode: 1029, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 34.412, mean reward: 3.128 [2.480, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.325, 10.100], loss: 0.174920, mae: 0.384358, mean_q: 4.122375
 59104/100000: episode: 1030, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 24.674, mean reward: 3.084 [2.260, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.486, 10.100], loss: 0.288996, mae: 0.433950, mean_q: 4.184079
 59111/100000: episode: 1031, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 32.947, mean reward: 4.707 [3.969, 5.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.339, 10.100], loss: 0.106485, mae: 0.330374, mean_q: 3.852075
 59119/100000: episode: 1032, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 28.752, mean reward: 3.594 [3.233, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.266, 10.100], loss: 0.201517, mae: 0.395438, mean_q: 4.182764
 59126/100000: episode: 1033, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 25.471, mean reward: 3.639 [2.853, 5.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.411, 10.100], loss: 0.156504, mae: 0.356973, mean_q: 4.064582
 59133/100000: episode: 1034, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 17.305, mean reward: 2.472 [2.188, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.391, 10.100], loss: 0.110981, mae: 0.331557, mean_q: 3.933418
 59140/100000: episode: 1035, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 21.689, mean reward: 3.098 [2.741, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.377, 10.100], loss: 0.108510, mae: 0.355481, mean_q: 4.130702
 59147/100000: episode: 1036, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 23.012, mean reward: 3.287 [2.694, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.417, 10.100], loss: 0.108726, mae: 0.330277, mean_q: 4.255420
 59155/100000: episode: 1037, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 24.493, mean reward: 3.062 [2.605, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.622, 10.100], loss: 0.183027, mae: 0.410987, mean_q: 4.175941
 59163/100000: episode: 1038, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 23.893, mean reward: 2.987 [2.264, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.208, 10.100], loss: 0.110851, mae: 0.334767, mean_q: 4.136220
 59171/100000: episode: 1039, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 22.600, mean reward: 2.825 [2.451, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.269, 10.100], loss: 0.228759, mae: 0.400440, mean_q: 4.066505
 59180/100000: episode: 1040, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 29.504, mean reward: 3.278 [2.642, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.783, 10.100], loss: 0.133968, mae: 0.379526, mean_q: 4.194102
 59191/100000: episode: 1041, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 32.992, mean reward: 2.999 [2.314, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.324, 10.100], loss: 0.163987, mae: 0.370758, mean_q: 4.195490
 59199/100000: episode: 1042, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 25.692, mean reward: 3.211 [2.729, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.812, 10.100], loss: 0.177385, mae: 0.385298, mean_q: 4.111309
 59206/100000: episode: 1043, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 24.171, mean reward: 3.453 [3.010, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.361, 10.100], loss: 0.161012, mae: 0.388023, mean_q: 4.097174
 59213/100000: episode: 1044, duration: 0.038s, episode steps: 7, steps per second: 187, episode reward: 27.822, mean reward: 3.975 [3.421, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.362, 10.100], loss: 0.139944, mae: 0.360659, mean_q: 4.056222
 59220/100000: episode: 1045, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 22.808, mean reward: 3.258 [2.448, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.590, 10.100], loss: 0.170350, mae: 0.414980, mean_q: 4.413581
 59231/100000: episode: 1046, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 35.359, mean reward: 3.214 [2.548, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.657, 10.100], loss: 0.187636, mae: 0.374789, mean_q: 4.059996
 59240/100000: episode: 1047, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 37.890, mean reward: 4.210 [2.696, 10.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.498, 10.100], loss: 0.138036, mae: 0.347524, mean_q: 4.207237
 59251/100000: episode: 1048, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 52.465, mean reward: 4.770 [3.508, 8.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.812, 10.100], loss: 0.151922, mae: 0.362092, mean_q: 4.174894
 59258/100000: episode: 1049, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 31.342, mean reward: 4.477 [3.957, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.391, 10.100], loss: 0.267945, mae: 0.453505, mean_q: 4.385150
 59266/100000: episode: 1050, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 24.256, mean reward: 3.032 [2.062, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.471, 10.100], loss: 0.120479, mae: 0.350633, mean_q: 4.221832
 59275/100000: episode: 1051, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 32.505, mean reward: 3.612 [3.137, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.395, 10.100], loss: 0.133666, mae: 0.369089, mean_q: 4.146505
 59284/100000: episode: 1052, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 37.692, mean reward: 4.188 [3.161, 5.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.483, 10.100], loss: 0.118726, mae: 0.354811, mean_q: 4.225216
 59291/100000: episode: 1053, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 20.545, mean reward: 2.935 [2.586, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.401, 10.100], loss: 0.123684, mae: 0.355399, mean_q: 4.045487
 59298/100000: episode: 1054, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 22.825, mean reward: 3.261 [2.662, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.335, 10.100], loss: 0.174696, mae: 0.342228, mean_q: 4.158429
 59309/100000: episode: 1055, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 41.413, mean reward: 3.765 [2.901, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.648, 10.100], loss: 0.150614, mae: 0.368231, mean_q: 4.276523
 59320/100000: episode: 1056, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 35.351, mean reward: 3.214 [2.468, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.306, 10.100], loss: 0.267887, mae: 0.419389, mean_q: 4.222373
 59327/100000: episode: 1057, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 33.788, mean reward: 4.827 [3.931, 5.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.450, 10.100], loss: 0.113162, mae: 0.331407, mean_q: 4.334449
 59335/100000: episode: 1058, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 26.844, mean reward: 3.355 [2.871, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.309, 10.100], loss: 0.202502, mae: 0.390288, mean_q: 4.220097
 59342/100000: episode: 1059, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 25.626, mean reward: 3.661 [3.168, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.325, 10.100], loss: 0.294873, mae: 0.449082, mean_q: 4.290999
 59349/100000: episode: 1060, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 23.775, mean reward: 3.396 [2.880, 4.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.095, 10.100], loss: 0.117574, mae: 0.360527, mean_q: 4.235227
 59357/100000: episode: 1061, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 23.635, mean reward: 2.954 [2.453, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.411, 10.100], loss: 0.177013, mae: 0.395120, mean_q: 4.145203
 59365/100000: episode: 1062, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 21.395, mean reward: 2.674 [2.318, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.549, 10.100], loss: 0.212105, mae: 0.397733, mean_q: 4.330561
 59373/100000: episode: 1063, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 32.859, mean reward: 4.107 [3.247, 5.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.215, 10.100], loss: 0.210303, mae: 0.417161, mean_q: 4.164707
 59381/100000: episode: 1064, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 27.921, mean reward: 3.490 [2.783, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.388, 10.100], loss: 0.136971, mae: 0.386191, mean_q: 4.271043
 59388/100000: episode: 1065, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 39.056, mean reward: 5.579 [4.202, 7.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.045, 10.100], loss: 0.144762, mae: 0.370486, mean_q: 4.240911
 59396/100000: episode: 1066, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 25.186, mean reward: 3.148 [2.414, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.394, 10.100], loss: 0.136829, mae: 0.358060, mean_q: 4.337531
 59404/100000: episode: 1067, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 29.951, mean reward: 3.744 [2.581, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.407, 10.100], loss: 0.243562, mae: 0.431669, mean_q: 4.361803
 59411/100000: episode: 1068, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 22.453, mean reward: 3.208 [2.757, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.335, 10.100], loss: 0.226763, mae: 0.402948, mean_q: 4.162957
 59418/100000: episode: 1069, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 28.541, mean reward: 4.077 [3.699, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.408, 10.100], loss: 0.175197, mae: 0.383737, mean_q: 4.271980
 59425/100000: episode: 1070, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 24.714, mean reward: 3.531 [2.546, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.433, 10.100], loss: 0.153999, mae: 0.399665, mean_q: 4.294292
 59433/100000: episode: 1071, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 22.585, mean reward: 2.823 [2.472, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.304, 10.100], loss: 0.253940, mae: 0.436904, mean_q: 4.191844
 59442/100000: episode: 1072, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 29.244, mean reward: 3.249 [2.507, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.533, 10.100], loss: 0.159234, mae: 0.382899, mean_q: 4.410576
 59449/100000: episode: 1073, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 25.979, mean reward: 3.711 [3.070, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.244, 10.100], loss: 0.339265, mae: 0.455381, mean_q: 4.269367
 59457/100000: episode: 1074, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 22.519, mean reward: 2.815 [2.560, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.302, 10.100], loss: 0.164891, mae: 0.386760, mean_q: 4.315523
 59465/100000: episode: 1075, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 24.796, mean reward: 3.099 [2.796, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.435, 10.100], loss: 0.184126, mae: 0.439532, mean_q: 4.279789
 59472/100000: episode: 1076, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 22.209, mean reward: 3.173 [2.974, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.459, 10.100], loss: 0.164071, mae: 0.375462, mean_q: 4.032249
 59480/100000: episode: 1077, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 31.532, mean reward: 3.942 [2.688, 5.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.548, 10.100], loss: 0.253450, mae: 0.469737, mean_q: 4.415557
 59487/100000: episode: 1078, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 20.369, mean reward: 2.910 [2.304, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.294, 10.100], loss: 0.123979, mae: 0.337035, mean_q: 4.180066
 59495/100000: episode: 1079, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 34.870, mean reward: 4.359 [3.140, 6.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.233, 10.100], loss: 0.148161, mae: 0.392627, mean_q: 4.392819
 59504/100000: episode: 1080, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 34.876, mean reward: 3.875 [3.412, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.389, 10.100], loss: 0.188369, mae: 0.411027, mean_q: 4.442131
 59513/100000: episode: 1081, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 29.973, mean reward: 3.330 [2.913, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.455, 10.100], loss: 0.247398, mae: 0.462097, mean_q: 4.230246
 59524/100000: episode: 1082, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 61.571, mean reward: 5.597 [3.436, 7.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.584, 10.100], loss: 0.167498, mae: 0.413759, mean_q: 4.288855
 59531/100000: episode: 1083, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 20.414, mean reward: 2.916 [2.454, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.331, 10.100], loss: 0.224205, mae: 0.373918, mean_q: 4.306155
 59538/100000: episode: 1084, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 31.119, mean reward: 4.446 [4.011, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.287, 10.100], loss: 0.246337, mae: 0.451938, mean_q: 4.564212
 59546/100000: episode: 1085, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 19.080, mean reward: 2.385 [2.117, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.565, 10.100], loss: 0.137321, mae: 0.359461, mean_q: 4.352441
 59555/100000: episode: 1086, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 27.659, mean reward: 3.073 [2.733, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.489, 10.100], loss: 0.302725, mae: 0.462466, mean_q: 4.510672
 59562/100000: episode: 1087, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 24.607, mean reward: 3.515 [3.124, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.703, 10.100], loss: 0.140122, mae: 0.391491, mean_q: 4.486011
 59573/100000: episode: 1088, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 42.149, mean reward: 3.832 [2.659, 6.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.540, 10.100], loss: 0.362608, mae: 0.470696, mean_q: 4.371533
[Info] 3-TH LEVEL FOUND: 7.966547012329102, Considering 10/90 traces
 59581/100000: episode: 1089, duration: 4.162s, episode steps: 8, steps per second: 2, episode reward: 29.951, mean reward: 3.744 [2.960, 5.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.624, 10.100], loss: 0.123822, mae: 0.357705, mean_q: 4.192984
 59588/100000: episode: 1090, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 24.639, mean reward: 3.520 [3.265, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.535, 10.100], loss: 0.232822, mae: 0.446367, mean_q: 4.221400
 59595/100000: episode: 1091, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 21.599, mean reward: 3.086 [2.827, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.459, 10.100], loss: 0.134397, mae: 0.379846, mean_q: 4.408718
 59604/100000: episode: 1092, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 32.976, mean reward: 3.664 [2.122, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.321, 10.100], loss: 0.286147, mae: 0.416573, mean_q: 4.321054
 59612/100000: episode: 1093, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 47.081, mean reward: 5.885 [3.280, 8.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.513, 10.100], loss: 0.309782, mae: 0.467887, mean_q: 4.483510
 59619/100000: episode: 1094, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 22.632, mean reward: 3.233 [2.405, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.596, 10.100], loss: 0.248765, mae: 0.411201, mean_q: 4.523390
 59627/100000: episode: 1095, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 45.489, mean reward: 5.686 [4.174, 8.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.459, 10.100], loss: 0.164253, mae: 0.384365, mean_q: 4.313614
 59634/100000: episode: 1096, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 33.259, mean reward: 4.751 [3.338, 7.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.510, 10.100], loss: 0.178970, mae: 0.425077, mean_q: 4.410480
 59641/100000: episode: 1097, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 31.845, mean reward: 4.549 [2.779, 11.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.390, 10.100], loss: 0.174384, mae: 0.404566, mean_q: 4.328343
 59650/100000: episode: 1098, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 34.414, mean reward: 3.824 [3.159, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.610, 10.100], loss: 0.137572, mae: 0.335387, mean_q: 4.233359
 59657/100000: episode: 1099, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 34.303, mean reward: 4.900 [3.011, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.531, 10.100], loss: 0.247841, mae: 0.440831, mean_q: 4.550421
 59665/100000: episode: 1100, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 40.408, mean reward: 5.051 [3.264, 9.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.389, 10.100], loss: 0.280695, mae: 0.457928, mean_q: 4.486428
 59673/100000: episode: 1101, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 38.069, mean reward: 4.759 [3.718, 6.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.574, 10.100], loss: 0.330726, mae: 0.490233, mean_q: 4.484544
 59680/100000: episode: 1102, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 26.983, mean reward: 3.855 [3.313, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.560, 10.100], loss: 0.265310, mae: 0.425581, mean_q: 4.334774
 59687/100000: episode: 1103, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 52.325, mean reward: 7.475 [5.130, 13.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.466, 10.100], loss: 0.270208, mae: 0.493238, mean_q: 4.608498
 59694/100000: episode: 1104, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 39.171, mean reward: 5.596 [3.441, 12.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.332, 10.100], loss: 0.126005, mae: 0.350483, mean_q: 4.367250
 59702/100000: episode: 1105, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 30.076, mean reward: 3.760 [3.191, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.398, 10.100], loss: 0.253131, mae: 0.420058, mean_q: 4.551900
 59709/100000: episode: 1106, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 22.506, mean reward: 3.215 [2.753, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.460, 10.100], loss: 0.276932, mae: 0.423865, mean_q: 4.324966
 59716/100000: episode: 1107, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 27.420, mean reward: 3.917 [3.072, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.351, 10.100], loss: 0.188308, mae: 0.397043, mean_q: 4.430615
 59723/100000: episode: 1108, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 54.234, mean reward: 7.748 [4.870, 10.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.468, 10.100], loss: 0.198364, mae: 0.403840, mean_q: 4.354966
 59732/100000: episode: 1109, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 38.078, mean reward: 4.231 [3.537, 5.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.410, 10.100], loss: 0.291160, mae: 0.425577, mean_q: 4.387339
 59739/100000: episode: 1110, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 22.851, mean reward: 3.264 [2.821, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.559, 10.100], loss: 0.168627, mae: 0.362611, mean_q: 4.504817
 59746/100000: episode: 1111, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 21.740, mean reward: 3.106 [2.518, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.772, 10.100], loss: 0.266475, mae: 0.460318, mean_q: 4.544845
 59753/100000: episode: 1112, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 21.848, mean reward: 3.121 [2.689, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.276, 10.100], loss: 0.225048, mae: 0.390988, mean_q: 4.439714
 59762/100000: episode: 1113, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 40.217, mean reward: 4.469 [3.412, 5.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.980, 10.100], loss: 0.420411, mae: 0.541867, mean_q: 4.768214
 59770/100000: episode: 1114, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 35.675, mean reward: 4.459 [3.381, 6.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.472, 10.100], loss: 0.332724, mae: 0.461079, mean_q: 4.341765
 59778/100000: episode: 1115, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 27.772, mean reward: 3.472 [3.160, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.449, 10.100], loss: 0.308957, mae: 0.459071, mean_q: 4.521059
 59785/100000: episode: 1116, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 32.239, mean reward: 4.606 [3.524, 5.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.490, 10.100], loss: 0.282341, mae: 0.489004, mean_q: 4.510163
 59792/100000: episode: 1117, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 33.703, mean reward: 4.815 [4.118, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.317, 10.100], loss: 0.312758, mae: 0.460442, mean_q: 4.546408
 59801/100000: episode: 1118, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 43.485, mean reward: 4.832 [3.374, 10.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.357, 10.100], loss: 0.440794, mae: 0.482403, mean_q: 4.606348
 59810/100000: episode: 1119, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 31.695, mean reward: 3.522 [3.113, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.389, 10.100], loss: 0.279820, mae: 0.478884, mean_q: 4.646509
 59817/100000: episode: 1120, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 28.476, mean reward: 4.068 [3.192, 5.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.436, 10.100], loss: 0.250534, mae: 0.370213, mean_q: 4.374474
 59824/100000: episode: 1121, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 18.870, mean reward: 2.696 [2.458, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.432, 10.100], loss: 0.291884, mae: 0.462599, mean_q: 4.606896
 59832/100000: episode: 1122, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 36.666, mean reward: 4.583 [3.455, 6.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.684, 10.100], loss: 0.256654, mae: 0.458997, mean_q: 4.367977
 59839/100000: episode: 1123, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 20.650, mean reward: 2.950 [2.667, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.426, 10.100], loss: 0.452466, mae: 0.525129, mean_q: 4.812191
 59846/100000: episode: 1124, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 50.984, mean reward: 7.283 [4.239, 14.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.513, 10.100], loss: 0.218427, mae: 0.460363, mean_q: 4.504772
 59853/100000: episode: 1125, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 31.078, mean reward: 4.440 [2.746, 9.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.046, 10.100], loss: 0.273272, mae: 0.516584, mean_q: 4.730091
 59860/100000: episode: 1126, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 26.809, mean reward: 3.830 [2.621, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.422, 10.100], loss: 0.332999, mae: 0.507949, mean_q: 4.647027
 59868/100000: episode: 1127, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 32.054, mean reward: 4.007 [3.343, 5.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.714, 10.100], loss: 0.265847, mae: 0.455916, mean_q: 4.318419
 59875/100000: episode: 1128, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 20.958, mean reward: 2.994 [2.614, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.391, 10.100], loss: 0.482598, mae: 0.502111, mean_q: 4.545547
 59882/100000: episode: 1129, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 21.433, mean reward: 3.062 [2.794, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.472, 10.100], loss: 0.330523, mae: 0.509924, mean_q: 4.796319
 59889/100000: episode: 1130, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 34.453, mean reward: 4.922 [3.758, 5.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.405, 10.100], loss: 0.669070, mae: 0.551778, mean_q: 4.407773
 59896/100000: episode: 1131, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 37.569, mean reward: 5.367 [4.047, 7.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.454, 10.100], loss: 0.344982, mae: 0.549744, mean_q: 4.795106
 59904/100000: episode: 1132, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 36.145, mean reward: 4.518 [3.883, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.633, 10.100], loss: 0.344983, mae: 0.505512, mean_q: 4.490999
 59911/100000: episode: 1133, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 24.952, mean reward: 3.565 [3.088, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.454, 10.100], loss: 0.235837, mae: 0.482080, mean_q: 4.675396
 59920/100000: episode: 1134, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 41.318, mean reward: 4.591 [3.881, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.424, 10.100], loss: 0.226771, mae: 0.425571, mean_q: 4.520937
 59927/100000: episode: 1135, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 29.756, mean reward: 4.251 [3.253, 5.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.457, 10.100], loss: 0.215639, mae: 0.429070, mean_q: 4.489261
 59934/100000: episode: 1136, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 26.282, mean reward: 3.755 [2.925, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.602, 10.100], loss: 0.168224, mae: 0.422079, mean_q: 4.441232
 59943/100000: episode: 1137, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 41.533, mean reward: 4.615 [3.681, 5.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.565, 10.100], loss: 0.222292, mae: 0.432263, mean_q: 4.578178
 59952/100000: episode: 1138, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 52.913, mean reward: 5.879 [4.778, 7.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.536, 10.100], loss: 0.465221, mae: 0.532771, mean_q: 4.745248
 59961/100000: episode: 1139, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 44.351, mean reward: 4.928 [3.767, 5.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.389, 10.100], loss: 0.337994, mae: 0.517640, mean_q: 4.704195
 59968/100000: episode: 1140, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 19.898, mean reward: 2.843 [2.531, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.402, 10.100], loss: 0.568976, mae: 0.534306, mean_q: 4.680745
 59975/100000: episode: 1141, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 23.228, mean reward: 3.318 [3.038, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.380, 10.100], loss: 0.336728, mae: 0.543596, mean_q: 4.588519
 59983/100000: episode: 1142, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 35.371, mean reward: 4.421 [3.491, 6.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.483, 10.100], loss: 0.166479, mae: 0.411910, mean_q: 4.593204
 59991/100000: episode: 1143, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 31.273, mean reward: 3.909 [3.178, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.425, 10.100], loss: 0.300971, mae: 0.496692, mean_q: 4.641953
 59999/100000: episode: 1144, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 28.591, mean reward: 3.574 [2.727, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.340, 10.100], loss: 0.297595, mae: 0.445510, mean_q: 4.500235
 60006/100000: episode: 1145, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 26.327, mean reward: 3.761 [2.921, 5.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.610, 10.100], loss: 0.208900, mae: 0.455221, mean_q: 4.648547
 60013/100000: episode: 1146, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 16.117, mean reward: 2.302 [2.088, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.762, 10.100], loss: 0.400687, mae: 0.492411, mean_q: 4.457982
 60020/100000: episode: 1147, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 25.178, mean reward: 3.597 [3.182, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.971, 10.100], loss: 0.258409, mae: 0.478776, mean_q: 4.836716
 60029/100000: episode: 1148, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 48.758, mean reward: 5.418 [3.698, 11.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.493, 10.100], loss: 0.388749, mae: 0.537530, mean_q: 4.670248
 60036/100000: episode: 1149, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 66.055, mean reward: 9.436 [4.142, 22.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.543, 10.100], loss: 0.239609, mae: 0.454663, mean_q: 4.824928
 60045/100000: episode: 1150, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 44.954, mean reward: 4.995 [4.457, 5.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.560, 10.100], loss: 0.397095, mae: 0.536698, mean_q: 4.745714
 60053/100000: episode: 1151, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 31.406, mean reward: 3.926 [3.247, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.462, 10.100], loss: 0.250125, mae: 0.459249, mean_q: 4.695887
 60061/100000: episode: 1152, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 31.405, mean reward: 3.926 [2.642, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.130, 10.100], loss: 0.512013, mae: 0.502140, mean_q: 4.671850
 60070/100000: episode: 1153, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 45.214, mean reward: 5.024 [4.379, 5.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.786, 10.100], loss: 0.297666, mae: 0.476205, mean_q: 4.807122
 60079/100000: episode: 1154, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 35.546, mean reward: 3.950 [3.043, 6.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.355, 10.100], loss: 0.334207, mae: 0.494845, mean_q: 4.688501
 60086/100000: episode: 1155, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 48.976, mean reward: 6.997 [4.110, 15.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.673, 10.100], loss: 0.351989, mae: 0.507795, mean_q: 4.730393
 60095/100000: episode: 1156, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 64.302, mean reward: 7.145 [6.138, 8.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.815, 10.100], loss: 0.317564, mae: 0.473343, mean_q: 4.581648
 60103/100000: episode: 1157, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 32.665, mean reward: 4.083 [2.971, 6.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.368, 10.100], loss: 0.415039, mae: 0.588949, mean_q: 4.833554
 60110/100000: episode: 1158, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 28.843, mean reward: 4.120 [2.613, 9.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.616, 10.100], loss: 0.397902, mae: 0.551307, mean_q: 4.635488
 60117/100000: episode: 1159, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 20.360, mean reward: 2.909 [2.525, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.602, 10.100], loss: 0.693927, mae: 0.669299, mean_q: 5.257398
 60124/100000: episode: 1160, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 17.388, mean reward: 2.484 [1.949, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.095, 10.100], loss: 0.453135, mae: 0.528567, mean_q: 4.692473
 60132/100000: episode: 1161, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 32.657, mean reward: 4.082 [3.209, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.375, 10.100], loss: 0.354185, mae: 0.546591, mean_q: 4.849008
 60139/100000: episode: 1162, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 29.850, mean reward: 4.264 [3.317, 5.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.623, 10.100], loss: 0.256449, mae: 0.477605, mean_q: 4.569354
 60147/100000: episode: 1163, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 25.683, mean reward: 3.210 [2.989, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.360, 10.100], loss: 0.998818, mae: 0.628844, mean_q: 5.087518
 60154/100000: episode: 1164, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 31.277, mean reward: 4.468 [3.944, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.424, 10.100], loss: 0.323168, mae: 0.529475, mean_q: 4.671918
 60162/100000: episode: 1165, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 28.382, mean reward: 3.548 [3.061, 4.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.498, 10.100], loss: 0.429892, mae: 0.496377, mean_q: 4.779058
 60170/100000: episode: 1166, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 41.197, mean reward: 5.150 [4.365, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.440, 10.100], loss: 0.392921, mae: 0.583494, mean_q: 4.836151
 60179/100000: episode: 1167, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 35.908, mean reward: 3.990 [3.141, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.444, 10.100], loss: 0.280169, mae: 0.485663, mean_q: 4.581058
 60187/100000: episode: 1168, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 29.598, mean reward: 3.700 [3.256, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.512, 10.100], loss: 0.822180, mae: 0.706528, mean_q: 5.189741
 60194/100000: episode: 1169, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 31.696, mean reward: 4.528 [3.411, 5.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.444, 10.100], loss: 0.332736, mae: 0.525195, mean_q: 4.821808
 60201/100000: episode: 1170, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 22.887, mean reward: 3.270 [2.926, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.433, 10.100], loss: 0.403549, mae: 0.584706, mean_q: 4.905723
 60208/100000: episode: 1171, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 24.112, mean reward: 3.445 [2.723, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.419, 10.100], loss: 0.358658, mae: 0.500897, mean_q: 4.657551
 60215/100000: episode: 1172, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 22.804, mean reward: 3.258 [2.337, 5.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.925, 10.100], loss: 0.338944, mae: 0.517711, mean_q: 4.778782
 60222/100000: episode: 1173, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 27.669, mean reward: 3.953 [3.677, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.474, 10.100], loss: 0.289306, mae: 0.517912, mean_q: 4.913835
 60229/100000: episode: 1174, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 21.311, mean reward: 3.044 [2.612, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.892, 10.100], loss: 0.358678, mae: 0.516788, mean_q: 4.906432
 60236/100000: episode: 1175, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 31.277, mean reward: 4.468 [3.359, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.334, 10.100], loss: 0.296464, mae: 0.484602, mean_q: 4.678955
 60244/100000: episode: 1176, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 31.860, mean reward: 3.983 [3.481, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.368, 10.100], loss: 0.493843, mae: 0.599896, mean_q: 5.170214
 60251/100000: episode: 1177, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 22.278, mean reward: 3.183 [2.814, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.461, 10.100], loss: 0.405781, mae: 0.601740, mean_q: 4.845452
 60260/100000: episode: 1178, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 45.715, mean reward: 5.079 [3.563, 7.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.480, 10.100], loss: 0.237606, mae: 0.460786, mean_q: 4.818530
[Info] 4-TH LEVEL FOUND: 10.314558029174805, Considering 10/90 traces
 60267/100000: episode: 1179, duration: 4.158s, episode steps: 7, steps per second: 2, episode reward: 41.606, mean reward: 5.944 [3.234, 9.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.351, 10.100], loss: 0.729790, mae: 0.601095, mean_q: 4.710263
 60274/100000: episode: 1180, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 42.401, mean reward: 6.057 [4.868, 7.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.531, 10.100], loss: 0.734387, mae: 0.652551, mean_q: 5.041293
 60280/100000: episode: 1181, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 28.857, mean reward: 4.809 [4.206, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.492, 10.100], loss: 0.331191, mae: 0.535298, mean_q: 4.916193
 60287/100000: episode: 1182, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 45.246, mean reward: 6.464 [5.015, 8.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.576, 10.100], loss: 0.224600, mae: 0.450186, mean_q: 4.769938
 60293/100000: episode: 1183, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 25.224, mean reward: 4.204 [3.570, 4.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.378, 10.100], loss: 0.421774, mae: 0.604763, mean_q: 5.014826
 60299/100000: episode: 1184, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 19.534, mean reward: 3.256 [2.810, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.786, 10.100], loss: 0.344903, mae: 0.535418, mean_q: 4.753591
 60305/100000: episode: 1185, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 21.905, mean reward: 3.651 [3.369, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.465, 10.100], loss: 0.471973, mae: 0.579113, mean_q: 4.712771
 60312/100000: episode: 1186, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 67.227, mean reward: 9.604 [4.725, 19.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.980, 10.100], loss: 0.366318, mae: 0.545417, mean_q: 4.820899
 60319/100000: episode: 1187, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 34.696, mean reward: 4.957 [4.144, 6.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.599, 10.100], loss: 0.335196, mae: 0.488383, mean_q: 4.742844
 60326/100000: episode: 1188, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 22.542, mean reward: 3.220 [2.644, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.397, 10.100], loss: 0.403690, mae: 0.542188, mean_q: 4.874478
 60332/100000: episode: 1189, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 22.512, mean reward: 3.752 [3.437, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.439, 10.100], loss: 0.460004, mae: 0.562942, mean_q: 4.995213
 60338/100000: episode: 1190, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 34.406, mean reward: 5.734 [3.646, 7.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.607, 10.100], loss: 0.320219, mae: 0.466290, mean_q: 4.906163
 60346/100000: episode: 1191, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 53.935, mean reward: 6.742 [4.951, 9.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.503, 10.100], loss: 0.320682, mae: 0.484870, mean_q: 4.885130
 60352/100000: episode: 1192, duration: 0.047s, episode steps: 6, steps per second: 128, episode reward: 45.167, mean reward: 7.528 [5.064, 8.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.529, 10.100], loss: 0.437131, mae: 0.523351, mean_q: 5.053482
 60359/100000: episode: 1193, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 57.407, mean reward: 8.201 [3.760, 19.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.450, 10.100], loss: 1.738664, mae: 0.773933, mean_q: 5.095031
 60366/100000: episode: 1194, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 51.151, mean reward: 7.307 [5.464, 9.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.158, 10.100], loss: 0.389876, mae: 0.546387, mean_q: 4.645149
 60372/100000: episode: 1195, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 22.574, mean reward: 3.762 [3.072, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.521, 10.100], loss: 0.655507, mae: 0.696083, mean_q: 5.252280
 60378/100000: episode: 1196, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 22.255, mean reward: 3.709 [3.425, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.375, 10.100], loss: 0.328214, mae: 0.554955, mean_q: 5.022894
 60385/100000: episode: 1197, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 51.498, mean reward: 7.357 [5.416, 11.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.508, 10.100], loss: 0.617625, mae: 0.651513, mean_q: 5.268830
 60391/100000: episode: 1198, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 25.019, mean reward: 4.170 [3.811, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.433, 10.100], loss: 0.526447, mae: 0.610196, mean_q: 5.060779
[Info] FALSIFICATION!
 60397/100000: episode: 1199, duration: 0.212s, episode steps: 6, steps per second: 28, episode reward: 1076.006, mean reward: 179.334 [3.688, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.418, 10.098], loss: 0.565911, mae: 0.586393, mean_q: 5.156602
 60404/100000: episode: 1200, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 33.906, mean reward: 4.844 [3.675, 8.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.518, 10.100], loss: 0.376783, mae: 0.536573, mean_q: 4.931380
 60410/100000: episode: 1201, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 56.572, mean reward: 9.429 [3.012, 37.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.482, 10.100], loss: 0.321321, mae: 0.519450, mean_q: 4.792300
 60416/100000: episode: 1202, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 24.945, mean reward: 4.158 [3.059, 5.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.522, 10.100], loss: 0.811746, mae: 0.708236, mean_q: 5.250633
 60423/100000: episode: 1203, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 32.464, mean reward: 4.638 [3.642, 5.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.430, 10.100], loss: 0.471535, mae: 0.669573, mean_q: 5.342777
 60430/100000: episode: 1204, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 30.011, mean reward: 4.287 [3.459, 5.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.624, 10.100], loss: 6.324669, mae: 2.502153, mean_q: 7.331955
 60436/100000: episode: 1205, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 29.523, mean reward: 4.920 [3.755, 6.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.532, 10.100], loss: 4.914978, mae: 2.222856, mean_q: 7.316639
 60442/100000: episode: 1206, duration: 0.048s, episode steps: 6, steps per second: 126, episode reward: 30.158, mean reward: 5.026 [3.810, 7.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.615, 10.100], loss: 1.123765, mae: 1.010689, mean_q: 5.038320
 60448/100000: episode: 1207, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 24.767, mean reward: 4.128 [3.500, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.469, 10.100], loss: 1.661919, mae: 1.256140, mean_q: 4.459467
 60456/100000: episode: 1208, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 62.373, mean reward: 7.797 [4.383, 10.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.575, 10.100], loss: 1.037803, mae: 1.020198, mean_q: 4.823888
[Info] FALSIFICATION!
 60460/100000: episode: 1209, duration: 0.197s, episode steps: 4, steps per second: 20, episode reward: 1014.127, mean reward: 253.532 [3.836, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.456, 10.082], loss: 1.644810, mae: 1.116873, mean_q: 5.255860
 60467/100000: episode: 1210, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 37.956, mean reward: 5.422 [3.982, 6.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.515, 10.100], loss: 0.755083, mae: 0.848748, mean_q: 5.459405
 60474/100000: episode: 1211, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 55.947, mean reward: 7.992 [4.481, 22.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.504, 10.100], loss: 0.545835, mae: 0.711832, mean_q: 5.510421
 60480/100000: episode: 1212, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 35.742, mean reward: 5.957 [3.591, 9.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.528, 10.100], loss: 0.990222, mae: 0.905921, mean_q: 5.658383
 60486/100000: episode: 1213, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 25.926, mean reward: 4.321 [3.308, 5.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.433, 10.100], loss: 1.161022, mae: 0.911371, mean_q: 5.510387
 60493/100000: episode: 1214, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 36.748, mean reward: 5.250 [4.121, 7.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.637, 10.100], loss: 0.828090, mae: 0.776093, mean_q: 5.476300
 60499/100000: episode: 1215, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 21.741, mean reward: 3.623 [3.073, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.351, 10.100], loss: 0.674961, mae: 0.702586, mean_q: 5.468246
 60506/100000: episode: 1216, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 70.298, mean reward: 10.043 [5.369, 17.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.522, 10.100], loss: 1.240912, mae: 0.818291, mean_q: 5.576681
 60512/100000: episode: 1217, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 25.210, mean reward: 4.202 [3.630, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.647, 10.100], loss: 0.638177, mae: 0.716512, mean_q: 5.341480
 60520/100000: episode: 1218, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 43.101, mean reward: 5.388 [4.196, 6.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.524, 10.100], loss: 1.117696, mae: 0.672828, mean_q: 5.429326
 60526/100000: episode: 1219, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 25.892, mean reward: 4.315 [3.709, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.458, 10.100], loss: 1.577556, mae: 0.724068, mean_q: 5.447054
 60532/100000: episode: 1220, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 21.248, mean reward: 3.541 [3.325, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.624, 10.100], loss: 0.633545, mae: 0.681328, mean_q: 5.366206
 60539/100000: episode: 1221, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 30.365, mean reward: 4.338 [3.796, 4.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.469, 10.100], loss: 0.660911, mae: 0.719182, mean_q: 5.297253
 60547/100000: episode: 1222, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 56.358, mean reward: 7.045 [4.328, 11.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.537, 10.100], loss: 0.612401, mae: 0.684835, mean_q: 5.266562
 60553/100000: episode: 1223, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 18.184, mean reward: 3.031 [2.697, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.451, 10.100], loss: 0.688752, mae: 0.707664, mean_q: 5.212969
 60560/100000: episode: 1224, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 40.196, mean reward: 5.742 [3.368, 15.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.614, 10.100], loss: 3.230808, mae: 0.848365, mean_q: 5.368825
 60566/100000: episode: 1225, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 23.594, mean reward: 3.932 [3.064, 4.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.476, 10.100], loss: 0.630870, mae: 0.752887, mean_q: 5.532839
 60573/100000: episode: 1226, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 32.233, mean reward: 4.605 [3.420, 6.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.603, 10.100], loss: 0.600277, mae: 0.659668, mean_q: 5.375434
 60579/100000: episode: 1227, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 23.873, mean reward: 3.979 [3.226, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.507, 10.100], loss: 2.245445, mae: 0.899549, mean_q: 5.562660
 60585/100000: episode: 1228, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 32.028, mean reward: 5.338 [4.443, 6.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.385, 10.100], loss: 8.551307, mae: 1.021786, mean_q: 5.568017
 60592/100000: episode: 1229, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 39.548, mean reward: 5.650 [4.483, 8.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.650, 10.100], loss: 0.466806, mae: 0.635381, mean_q: 5.502239
 60600/100000: episode: 1230, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 64.380, mean reward: 8.047 [4.410, 17.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.533, 10.100], loss: 0.513345, mae: 0.655691, mean_q: 5.456733
 60607/100000: episode: 1231, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 38.353, mean reward: 5.479 [4.138, 8.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.540, 10.100], loss: 1.458630, mae: 0.746149, mean_q: 5.297564
 60613/100000: episode: 1232, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 28.425, mean reward: 4.737 [3.726, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.524, 10.100], loss: 0.568910, mae: 0.693195, mean_q: 5.394739
 60620/100000: episode: 1233, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 37.418, mean reward: 5.345 [4.173, 7.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.641, 10.100], loss: 0.582247, mae: 0.651745, mean_q: 5.544858
 60626/100000: episode: 1234, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 31.273, mean reward: 5.212 [4.002, 7.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.448, 10.100], loss: 0.486163, mae: 0.574659, mean_q: 5.497694
 60632/100000: episode: 1235, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 22.827, mean reward: 3.804 [3.474, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.427, 10.100], loss: 10.960864, mae: 1.202866, mean_q: 5.670624
 60639/100000: episode: 1236, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 46.733, mean reward: 6.676 [4.950, 8.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.480, 10.100], loss: 0.658620, mae: 0.793390, mean_q: 5.990646
 60645/100000: episode: 1237, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 20.154, mean reward: 3.359 [3.106, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.465, 10.100], loss: 0.446647, mae: 0.615369, mean_q: 5.610037
 60651/100000: episode: 1238, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 20.299, mean reward: 3.383 [2.862, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.720, 10.100], loss: 0.396192, mae: 0.575443, mean_q: 5.309739
 60657/100000: episode: 1239, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 22.398, mean reward: 3.733 [3.369, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.460, 10.100], loss: 0.528358, mae: 0.629706, mean_q: 5.343821
 60663/100000: episode: 1240, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 41.362, mean reward: 6.894 [4.910, 10.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.498, 10.100], loss: 0.875839, mae: 0.649900, mean_q: 5.355860
 60670/100000: episode: 1241, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 55.983, mean reward: 7.998 [4.729, 13.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.582, 10.100], loss: 1.224681, mae: 0.834431, mean_q: 5.645458
 60676/100000: episode: 1242, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 31.179, mean reward: 5.196 [3.210, 12.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.614, 10.100], loss: 0.867694, mae: 0.728976, mean_q: 5.708099
 60682/100000: episode: 1243, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 31.789, mean reward: 5.298 [4.607, 6.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.557, 10.100], loss: 0.510783, mae: 0.648128, mean_q: 5.433676
 60688/100000: episode: 1244, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 22.667, mean reward: 3.778 [3.199, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.561, 10.100], loss: 0.414580, mae: 0.528577, mean_q: 5.169486
 60695/100000: episode: 1245, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 30.644, mean reward: 4.378 [3.376, 7.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.617, 10.100], loss: 0.929907, mae: 0.817938, mean_q: 5.690473
 60703/100000: episode: 1246, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 53.338, mean reward: 6.667 [5.372, 9.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.621, 10.100], loss: 0.471047, mae: 0.638117, mean_q: 5.437941
 60710/100000: episode: 1247, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 24.552, mean reward: 3.507 [2.856, 5.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.505, 10.100], loss: 0.551251, mae: 0.673331, mean_q: 5.396832
 60716/100000: episode: 1248, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 21.055, mean reward: 3.509 [3.006, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.508, 10.100], loss: 1.895897, mae: 0.902218, mean_q: 5.779922
 60723/100000: episode: 1249, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 41.277, mean reward: 5.897 [4.037, 9.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.673, 10.100], loss: 0.559455, mae: 0.690072, mean_q: 5.438145
 60729/100000: episode: 1250, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 36.030, mean reward: 6.005 [4.230, 8.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.593, 10.100], loss: 0.505119, mae: 0.663446, mean_q: 5.289867
 60737/100000: episode: 1251, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 49.618, mean reward: 6.202 [5.103, 9.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.812, 10.100], loss: 1924.688477, mae: 5.846154, mean_q: 7.068922
 60745/100000: episode: 1252, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 44.818, mean reward: 5.602 [3.525, 9.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.530, 10.100], loss: 1912.047485, mae: 6.582145, mean_q: 8.208633
 60751/100000: episode: 1253, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 83.772, mean reward: 13.962 [4.454, 23.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.553, 10.100], loss: 5.909386, mae: 2.601964, mean_q: 8.171611
 60757/100000: episode: 1254, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 25.333, mean reward: 4.222 [3.594, 5.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.531, 10.100], loss: 3.618759, mae: 1.882564, mean_q: 7.299723
 60764/100000: episode: 1255, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 61.082, mean reward: 8.726 [5.657, 11.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.716, 10.100], loss: 0.948502, mae: 0.904585, mean_q: 5.979189
 60770/100000: episode: 1256, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 22.734, mean reward: 3.789 [3.371, 4.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.932, 10.100], loss: 1.098540, mae: 0.920756, mean_q: 5.096828
 60777/100000: episode: 1257, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 64.971, mean reward: 9.282 [3.978, 37.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.611, 10.100], loss: 1.329637, mae: 0.948809, mean_q: 5.190449
 60783/100000: episode: 1258, duration: 0.045s, episode steps: 6, steps per second: 133, episode reward: 23.771, mean reward: 3.962 [3.341, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.990, 10.100], loss: 1.134066, mae: 0.840471, mean_q: 5.458029
 60789/100000: episode: 1259, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 24.948, mean reward: 4.158 [3.616, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.479, 10.100], loss: 1.697434, mae: 0.865276, mean_q: 5.650436
 60796/100000: episode: 1260, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 45.152, mean reward: 6.450 [3.869, 16.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.483, 10.100], loss: 1.323105, mae: 0.928177, mean_q: 5.979768
 60803/100000: episode: 1261, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 42.580, mean reward: 6.083 [4.237, 10.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.526, 10.100], loss: 1.685666, mae: 0.872945, mean_q: 5.853095
 60809/100000: episode: 1262, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 32.887, mean reward: 5.481 [4.819, 6.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.429, 10.100], loss: 1.454251, mae: 0.919617, mean_q: 5.957735
 60815/100000: episode: 1263, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 28.960, mean reward: 4.827 [3.672, 5.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.617, 10.100], loss: 7.506515, mae: 0.911443, mean_q: 5.908335
 60821/100000: episode: 1264, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 38.954, mean reward: 6.492 [3.832, 9.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.535, 10.100], loss: 7.673857, mae: 0.978893, mean_q: 5.748413
 60827/100000: episode: 1265, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 28.710, mean reward: 4.785 [3.985, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.529, 10.100], loss: 1.140935, mae: 0.912656, mean_q: 5.973952
 60833/100000: episode: 1266, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 22.938, mean reward: 3.823 [3.597, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.401, 10.100], loss: 2554.824463, mae: 6.150642, mean_q: 6.459858
 60839/100000: episode: 1267, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 34.749, mean reward: 5.792 [4.428, 8.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.506, 10.100], loss: 2542.350098, mae: 7.884128, mean_q: 8.227401
 60845/100000: episode: 1268, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 24.409, mean reward: 4.068 [2.907, 6.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.508, 10.100], loss: 12.980760, mae: 4.089581, mean_q: 10.026498
[Info] Complete ISplit Iteration
[Info] Levels: [5.0807314, 7.0253577, 7.966547, 10.314558, 12.472574]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.66]
[Info] Error Prob: 6.600000000000002e-05

 60851/100000: episode: 1269, duration: 4.378s, episode steps: 6, steps per second: 1, episode reward: 26.116, mean reward: 4.353 [3.478, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.555, 10.100], loss: 7.314264, mae: 2.794198, mean_q: 8.262914
 60951/100000: episode: 1270, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 176.354, mean reward: 1.764 [1.447, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.812, 10.118], loss: 155.789749, mae: 1.549963, mean_q: 6.307650
 61051/100000: episode: 1271, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.882, mean reward: 1.949 [1.492, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.986, 10.270], loss: 155.328094, mae: 1.193926, mean_q: 6.006922
 61151/100000: episode: 1272, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 205.203, mean reward: 2.052 [1.446, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.054, 10.106], loss: 462.139923, mae: 2.364069, mean_q: 6.925031
 61251/100000: episode: 1273, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.336, mean reward: 1.923 [1.435, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.260, 10.098], loss: 2.257947, mae: 1.113668, mean_q: 6.441868
 61351/100000: episode: 1274, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 200.422, mean reward: 2.004 [1.466, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.402, 10.098], loss: 1.677282, mae: 0.880373, mean_q: 5.981738
 61451/100000: episode: 1275, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.680, mean reward: 1.887 [1.437, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.981, 10.221], loss: 2.103838, mae: 0.904857, mean_q: 5.937211
 61551/100000: episode: 1276, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 211.985, mean reward: 2.120 [1.462, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 1.365071, mae: 0.848094, mean_q: 5.830712
 61651/100000: episode: 1277, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 185.402, mean reward: 1.854 [1.466, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.060, 10.105], loss: 309.437347, mae: 1.920641, mean_q: 6.529569
 61751/100000: episode: 1278, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.730, mean reward: 1.927 [1.474, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.171, 10.098], loss: 1.305435, mae: 0.871891, mean_q: 5.847019
 61851/100000: episode: 1279, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.456, mean reward: 1.775 [1.440, 2.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.139], loss: 155.803101, mae: 1.414427, mean_q: 6.247719
 61951/100000: episode: 1280, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 191.418, mean reward: 1.914 [1.494, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.338, 10.098], loss: 308.477936, mae: 1.889213, mean_q: 6.579681
 62051/100000: episode: 1281, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.427, mean reward: 1.954 [1.488, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.134, 10.196], loss: 155.571106, mae: 1.485068, mean_q: 6.362446
 62151/100000: episode: 1282, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.331, mean reward: 1.833 [1.469, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.755, 10.111], loss: 1.455948, mae: 0.909045, mean_q: 5.931348
 62251/100000: episode: 1283, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.929, mean reward: 1.909 [1.497, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.948, 10.098], loss: 155.171997, mae: 1.443530, mean_q: 6.225460
 62351/100000: episode: 1284, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 173.950, mean reward: 1.739 [1.503, 2.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.428, 10.098], loss: 461.213715, mae: 2.539641, mean_q: 6.705019
 62451/100000: episode: 1285, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 182.525, mean reward: 1.825 [1.441, 2.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.098], loss: 308.168640, mae: 1.714832, mean_q: 6.264318
 62551/100000: episode: 1286, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 207.708, mean reward: 2.077 [1.478, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.759, 10.368], loss: 461.075470, mae: 2.419570, mean_q: 7.008811
 62651/100000: episode: 1287, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.945, mean reward: 2.009 [1.447, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.179], loss: 308.824188, mae: 1.992451, mean_q: 7.058474
 62751/100000: episode: 1288, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 186.164, mean reward: 1.862 [1.481, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.042, 10.098], loss: 2.241172, mae: 1.043527, mean_q: 6.067991
 62851/100000: episode: 1289, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 182.080, mean reward: 1.821 [1.454, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.098], loss: 461.281372, mae: 2.280210, mean_q: 6.773240
 62951/100000: episode: 1290, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.526, mean reward: 1.875 [1.476, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.999, 10.336], loss: 1.851023, mae: 0.986022, mean_q: 6.071409
 63051/100000: episode: 1291, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.444, mean reward: 1.924 [1.474, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.299, 10.098], loss: 155.554565, mae: 1.350341, mean_q: 6.020614
 63151/100000: episode: 1292, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.783, mean reward: 1.998 [1.460, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.450], loss: 307.970673, mae: 2.040897, mean_q: 6.779630
 63251/100000: episode: 1293, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.818, mean reward: 1.868 [1.462, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.971, 10.098], loss: 309.406372, mae: 1.814236, mean_q: 6.338734
 63351/100000: episode: 1294, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 207.865, mean reward: 2.079 [1.518, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.379, 10.098], loss: 613.720764, mae: 2.757951, mean_q: 6.988394
 63451/100000: episode: 1295, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.300, mean reward: 1.883 [1.473, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.695, 10.142], loss: 308.707916, mae: 2.134044, mean_q: 7.030526
 63551/100000: episode: 1296, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 188.846, mean reward: 1.888 [1.448, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.364, 10.197], loss: 2.019006, mae: 0.969952, mean_q: 5.950571
 63651/100000: episode: 1297, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.191, mean reward: 2.002 [1.465, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.461, 10.098], loss: 1.890549, mae: 0.919756, mean_q: 5.861450
 63751/100000: episode: 1298, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 194.263, mean reward: 1.943 [1.462, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.711, 10.139], loss: 155.129669, mae: 1.292642, mean_q: 5.975385
 63851/100000: episode: 1299, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 183.617, mean reward: 1.836 [1.472, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.520, 10.113], loss: 1.670579, mae: 0.826679, mean_q: 5.465498
 63951/100000: episode: 1300, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 181.385, mean reward: 1.814 [1.464, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.236, 10.259], loss: 1.951051, mae: 0.824947, mean_q: 5.357043
 64051/100000: episode: 1301, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 209.406, mean reward: 2.094 [1.457, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.772, 10.313], loss: 460.889679, mae: 2.113365, mean_q: 6.132425
 64151/100000: episode: 1302, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 189.670, mean reward: 1.897 [1.456, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.543, 10.139], loss: 154.975601, mae: 1.350532, mean_q: 6.117311
 64251/100000: episode: 1303, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 189.542, mean reward: 1.895 [1.465, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.276, 10.126], loss: 155.222382, mae: 1.213431, mean_q: 5.666723
 64351/100000: episode: 1304, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 186.124, mean reward: 1.861 [1.470, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.454, 10.098], loss: 307.288666, mae: 1.617326, mean_q: 5.947557
 64451/100000: episode: 1305, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 202.900, mean reward: 2.029 [1.489, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.523, 10.098], loss: 1.316054, mae: 0.732157, mean_q: 5.287004
 64551/100000: episode: 1306, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 192.612, mean reward: 1.926 [1.445, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.258, 10.098], loss: 461.190857, mae: 2.186005, mean_q: 5.852127
 64651/100000: episode: 1307, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 187.598, mean reward: 1.876 [1.443, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.201, 10.098], loss: 155.027283, mae: 1.177790, mean_q: 5.555980
 64751/100000: episode: 1308, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 191.194, mean reward: 1.912 [1.477, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.868, 10.256], loss: 154.266235, mae: 0.957452, mean_q: 5.195035
 64851/100000: episode: 1309, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 193.831, mean reward: 1.938 [1.469, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.494, 10.145], loss: 458.564087, mae: 1.897878, mean_q: 5.616423
 64951/100000: episode: 1310, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 193.556, mean reward: 1.936 [1.450, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.915, 10.231], loss: 154.579041, mae: 1.271279, mean_q: 5.519153
 65051/100000: episode: 1311, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 205.832, mean reward: 2.058 [1.432, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.179], loss: 153.806229, mae: 0.969597, mean_q: 5.028709
 65151/100000: episode: 1312, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 191.593, mean reward: 1.916 [1.474, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.098], loss: 456.932220, mae: 1.969569, mean_q: 5.538481
 65251/100000: episode: 1313, duration: 1.562s, episode steps: 100, steps per second: 64, episode reward: 199.275, mean reward: 1.993 [1.463, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.373, 10.098], loss: 153.534470, mae: 1.126906, mean_q: 5.202513
 65351/100000: episode: 1314, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: 179.932, mean reward: 1.799 [1.458, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.907, 10.098], loss: 304.542542, mae: 1.358991, mean_q: 4.973059
 65451/100000: episode: 1315, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 218.210, mean reward: 2.182 [1.489, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.195, 10.393], loss: 1.181514, mae: 0.587165, mean_q: 4.661270
 65551/100000: episode: 1316, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 186.415, mean reward: 1.864 [1.448, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.390, 10.098], loss: 0.568534, mae: 0.449185, mean_q: 4.349134
 65651/100000: episode: 1317, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 183.814, mean reward: 1.838 [1.436, 2.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.376, 10.102], loss: 0.296554, mae: 0.361066, mean_q: 4.096807
 65751/100000: episode: 1318, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 189.948, mean reward: 1.899 [1.453, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.903, 10.098], loss: 0.200298, mae: 0.321884, mean_q: 3.965128
 65851/100000: episode: 1319, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 208.578, mean reward: 2.086 [1.477, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.596, 10.098], loss: 0.104126, mae: 0.288409, mean_q: 3.817714
 65951/100000: episode: 1320, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 212.924, mean reward: 2.129 [1.525, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.656, 10.183], loss: 0.089790, mae: 0.290073, mean_q: 3.824038
 66051/100000: episode: 1321, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 195.804, mean reward: 1.958 [1.449, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.796, 10.274], loss: 0.095693, mae: 0.297215, mean_q: 3.823676
 66151/100000: episode: 1322, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 189.689, mean reward: 1.897 [1.445, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.102, 10.269], loss: 0.092288, mae: 0.289731, mean_q: 3.821065
 66251/100000: episode: 1323, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 189.385, mean reward: 1.894 [1.462, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.687, 10.098], loss: 0.084342, mae: 0.279269, mean_q: 3.805709
 66351/100000: episode: 1324, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 214.815, mean reward: 2.148 [1.474, 5.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.151], loss: 0.088340, mae: 0.287750, mean_q: 3.805774
 66451/100000: episode: 1325, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.565, mean reward: 1.966 [1.457, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.287], loss: 0.082728, mae: 0.282512, mean_q: 3.827874
 66551/100000: episode: 1326, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 186.945, mean reward: 1.869 [1.460, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.629, 10.265], loss: 0.092453, mae: 0.290649, mean_q: 3.816322
 66651/100000: episode: 1327, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 193.857, mean reward: 1.939 [1.502, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.329, 10.098], loss: 0.086292, mae: 0.279872, mean_q: 3.816429
 66751/100000: episode: 1328, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 192.188, mean reward: 1.922 [1.480, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.103, 10.194], loss: 0.090288, mae: 0.288489, mean_q: 3.823663
 66851/100000: episode: 1329, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 188.613, mean reward: 1.886 [1.488, 5.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.214], loss: 0.088219, mae: 0.286194, mean_q: 3.819660
 66951/100000: episode: 1330, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 185.157, mean reward: 1.852 [1.433, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.421, 10.098], loss: 0.079940, mae: 0.274514, mean_q: 3.819054
 67051/100000: episode: 1331, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 204.318, mean reward: 2.043 [1.465, 4.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.730, 10.363], loss: 0.087781, mae: 0.285225, mean_q: 3.824343
 67151/100000: episode: 1332, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 197.197, mean reward: 1.972 [1.452, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.429, 10.139], loss: 0.082532, mae: 0.282090, mean_q: 3.821010
 67251/100000: episode: 1333, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 179.867, mean reward: 1.799 [1.457, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.217, 10.099], loss: 0.087014, mae: 0.292497, mean_q: 3.851310
 67351/100000: episode: 1334, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 193.042, mean reward: 1.930 [1.514, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.098], loss: 0.076624, mae: 0.275543, mean_q: 3.847815
 67451/100000: episode: 1335, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 183.545, mean reward: 1.835 [1.450, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.603, 10.115], loss: 0.089745, mae: 0.286595, mean_q: 3.846061
 67551/100000: episode: 1336, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.487, mean reward: 1.955 [1.499, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.402, 10.098], loss: 0.089912, mae: 0.291008, mean_q: 3.845306
 67651/100000: episode: 1337, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 183.865, mean reward: 1.839 [1.466, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.709, 10.098], loss: 0.085139, mae: 0.284544, mean_q: 3.824263
 67751/100000: episode: 1338, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 198.242, mean reward: 1.982 [1.473, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.616, 10.453], loss: 0.078674, mae: 0.274998, mean_q: 3.817862
 67851/100000: episode: 1339, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.822, mean reward: 1.928 [1.459, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.438, 10.295], loss: 0.081228, mae: 0.268569, mean_q: 3.812758
 67951/100000: episode: 1340, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.032, mean reward: 1.920 [1.503, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.173, 10.292], loss: 0.087888, mae: 0.285828, mean_q: 3.837570
 68051/100000: episode: 1341, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.998, mean reward: 1.800 [1.475, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.840, 10.098], loss: 0.077624, mae: 0.276998, mean_q: 3.844512
 68151/100000: episode: 1342, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 178.509, mean reward: 1.785 [1.438, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.187, 10.179], loss: 0.077618, mae: 0.272139, mean_q: 3.825989
 68251/100000: episode: 1343, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 232.848, mean reward: 2.328 [1.503, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.653, 10.098], loss: 0.085384, mae: 0.283805, mean_q: 3.830176
 68351/100000: episode: 1344, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 194.351, mean reward: 1.944 [1.470, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.121, 10.098], loss: 0.091119, mae: 0.291388, mean_q: 3.841381
 68451/100000: episode: 1345, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 197.213, mean reward: 1.972 [1.434, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.405, 10.262], loss: 0.086260, mae: 0.286076, mean_q: 3.838796
 68551/100000: episode: 1346, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 203.385, mean reward: 2.034 [1.489, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.693, 10.289], loss: 0.086125, mae: 0.286225, mean_q: 3.848521
 68651/100000: episode: 1347, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.071, mean reward: 1.841 [1.450, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.786, 10.173], loss: 0.079856, mae: 0.278719, mean_q: 3.837638
 68751/100000: episode: 1348, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.714, mean reward: 1.877 [1.456, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.708, 10.142], loss: 0.087947, mae: 0.293193, mean_q: 3.853616
 68851/100000: episode: 1349, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.001, mean reward: 1.870 [1.451, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.658, 10.182], loss: 0.084888, mae: 0.288499, mean_q: 3.847996
 68951/100000: episode: 1350, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 200.065, mean reward: 2.001 [1.458, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.376, 10.098], loss: 0.095018, mae: 0.294409, mean_q: 3.867678
 69051/100000: episode: 1351, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 195.681, mean reward: 1.957 [1.468, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.098], loss: 0.083294, mae: 0.281495, mean_q: 3.859240
 69151/100000: episode: 1352, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 226.025, mean reward: 2.260 [1.442, 5.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.054, 10.358], loss: 0.091119, mae: 0.291475, mean_q: 3.852686
 69251/100000: episode: 1353, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.814, mean reward: 1.848 [1.473, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.198], loss: 0.090752, mae: 0.290385, mean_q: 3.860177
 69351/100000: episode: 1354, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 208.484, mean reward: 2.085 [1.489, 6.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.478, 10.255], loss: 0.084345, mae: 0.279339, mean_q: 3.845884
 69451/100000: episode: 1355, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 182.391, mean reward: 1.824 [1.458, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.098], loss: 0.080405, mae: 0.277942, mean_q: 3.833082
 69551/100000: episode: 1356, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 204.056, mean reward: 2.041 [1.459, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.660, 10.247], loss: 0.091734, mae: 0.288361, mean_q: 3.844094
 69651/100000: episode: 1357, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 184.863, mean reward: 1.849 [1.470, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.553, 10.132], loss: 0.086221, mae: 0.283255, mean_q: 3.860686
 69751/100000: episode: 1358, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.643, mean reward: 1.826 [1.479, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.055, 10.177], loss: 0.089200, mae: 0.290778, mean_q: 3.850495
 69851/100000: episode: 1359, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.018, mean reward: 1.860 [1.450, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.098], loss: 0.086808, mae: 0.285795, mean_q: 3.855775
 69951/100000: episode: 1360, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.089, mean reward: 1.991 [1.542, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.236, 10.297], loss: 0.080948, mae: 0.279324, mean_q: 3.851145
 70051/100000: episode: 1361, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.163, mean reward: 1.962 [1.442, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.853, 10.277], loss: 0.082695, mae: 0.282112, mean_q: 3.863421
 70151/100000: episode: 1362, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 198.976, mean reward: 1.990 [1.479, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.654, 10.334], loss: 0.085789, mae: 0.281852, mean_q: 3.850109
 70251/100000: episode: 1363, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 180.491, mean reward: 1.805 [1.461, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.457, 10.218], loss: 0.081707, mae: 0.282150, mean_q: 3.857422
 70351/100000: episode: 1364, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.610, mean reward: 1.926 [1.445, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.845, 10.106], loss: 0.078783, mae: 0.280701, mean_q: 3.857478
 70451/100000: episode: 1365, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 199.947, mean reward: 1.999 [1.463, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.530, 10.098], loss: 0.084841, mae: 0.274599, mean_q: 3.835187
 70551/100000: episode: 1366, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.755, mean reward: 1.928 [1.490, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.804, 10.098], loss: 0.082815, mae: 0.286434, mean_q: 3.848101
 70651/100000: episode: 1367, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 181.518, mean reward: 1.815 [1.470, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.109], loss: 0.081450, mae: 0.282100, mean_q: 3.846611
 70751/100000: episode: 1368, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 213.417, mean reward: 2.134 [1.480, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.999, 10.189], loss: 0.086404, mae: 0.284015, mean_q: 3.835409
[Info] 1-TH LEVEL FOUND: 5.157166004180908, Considering 10/90 traces
 70851/100000: episode: 1369, duration: 4.613s, episode steps: 100, steps per second: 22, episode reward: 219.486, mean reward: 2.195 [1.490, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.635, 10.298], loss: 0.087821, mae: 0.290572, mean_q: 3.863674
 70902/100000: episode: 1370, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 116.788, mean reward: 2.290 [1.533, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.329, 10.100], loss: 0.075813, mae: 0.274364, mean_q: 3.818626
 70953/100000: episode: 1371, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 131.436, mean reward: 2.577 [1.456, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.352, 10.100], loss: 0.086280, mae: 0.287074, mean_q: 3.852144
 71012/100000: episode: 1372, duration: 0.306s, episode steps: 59, steps per second: 193, episode reward: 125.209, mean reward: 2.122 [1.459, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-1.389, 10.100], loss: 0.096172, mae: 0.296858, mean_q: 3.862210
[Info] FALSIFICATION!
 71056/100000: episode: 1373, duration: 0.384s, episode steps: 44, steps per second: 114, episode reward: 1143.981, mean reward: 26.000 [2.301, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.619 [-1.564, 9.320], loss: 0.078855, mae: 0.277435, mean_q: 3.851234
 71118/100000: episode: 1374, duration: 0.324s, episode steps: 62, steps per second: 191, episode reward: 126.870, mean reward: 2.046 [1.502, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.784 [-1.655, 10.144], loss: 0.093158, mae: 0.302779, mean_q: 3.881099
 71137/100000: episode: 1375, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 61.601, mean reward: 3.242 [2.134, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.799, 10.552], loss: 0.086423, mae: 0.304445, mean_q: 3.941771
 71156/100000: episode: 1376, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 62.541, mean reward: 3.292 [2.115, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.650, 10.541], loss: 0.104706, mae: 0.310347, mean_q: 3.907796
 71218/100000: episode: 1377, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 159.410, mean reward: 2.571 [1.722, 5.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-0.591, 10.309], loss: 249.941620, mae: 1.282568, mean_q: 4.353049
 71237/100000: episode: 1378, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 41.464, mean reward: 2.182 [1.597, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.505, 10.173], loss: 0.258762, mae: 0.528424, mean_q: 3.712631
 71288/100000: episode: 1379, duration: 0.272s, episode steps: 51, steps per second: 188, episode reward: 109.634, mean reward: 2.150 [1.659, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.510, 10.272], loss: 302.778717, mae: 1.192693, mean_q: 4.274119
 71347/100000: episode: 1380, duration: 0.308s, episode steps: 59, steps per second: 192, episode reward: 146.556, mean reward: 2.484 [1.493, 6.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-1.023, 10.100], loss: 0.601770, mae: 0.708735, mean_q: 4.307868
 71398/100000: episode: 1381, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 139.239, mean reward: 2.730 [1.818, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.586, 10.545], loss: 0.151249, mae: 0.378889, mean_q: 4.061188
 71457/100000: episode: 1382, duration: 0.298s, episode steps: 59, steps per second: 198, episode reward: 137.934, mean reward: 2.338 [1.469, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-1.516, 10.134], loss: 0.120174, mae: 0.356327, mean_q: 4.046406
 71519/100000: episode: 1383, duration: 0.355s, episode steps: 62, steps per second: 175, episode reward: 166.609, mean reward: 2.687 [1.908, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.796 [-0.982, 10.331], loss: 0.138841, mae: 0.359706, mean_q: 4.057912
 71570/100000: episode: 1384, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 104.043, mean reward: 2.040 [1.539, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.305, 10.100], loss: 0.132686, mae: 0.342862, mean_q: 4.029400
 71622/100000: episode: 1385, duration: 0.276s, episode steps: 52, steps per second: 189, episode reward: 108.480, mean reward: 2.086 [1.549, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.120, 10.100], loss: 0.126368, mae: 0.350017, mean_q: 4.030490
 71641/100000: episode: 1386, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 52.869, mean reward: 2.783 [2.136, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.445], loss: 0.133231, mae: 0.346999, mean_q: 4.004388
 71693/100000: episode: 1387, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 108.923, mean reward: 2.095 [1.572, 4.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.863, 10.265], loss: 297.476837, mae: 1.340728, mean_q: 4.492365
 71744/100000: episode: 1388, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 106.882, mean reward: 2.096 [1.552, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.144, 10.246], loss: 0.325565, mae: 0.600645, mean_q: 3.999905
 71803/100000: episode: 1389, duration: 0.317s, episode steps: 59, steps per second: 186, episode reward: 143.080, mean reward: 2.425 [1.487, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.413, 10.204], loss: 261.821564, mae: 1.366132, mean_q: 4.508638
 71854/100000: episode: 1390, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 132.311, mean reward: 2.594 [1.808, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.371, 10.394], loss: 0.155576, mae: 0.416175, mean_q: 4.198184
 71907/100000: episode: 1391, duration: 0.299s, episode steps: 53, steps per second: 177, episode reward: 115.493, mean reward: 2.179 [1.517, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.405, 10.185], loss: 0.145405, mae: 0.386932, mean_q: 4.179277
 71953/100000: episode: 1392, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 130.713, mean reward: 2.842 [1.665, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.271, 10.303], loss: 0.136112, mae: 0.377899, mean_q: 4.120610
 72004/100000: episode: 1393, duration: 0.257s, episode steps: 51, steps per second: 199, episode reward: 104.039, mean reward: 2.040 [1.526, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.729, 10.100], loss: 0.139067, mae: 0.368292, mean_q: 4.154260
 72057/100000: episode: 1394, duration: 0.282s, episode steps: 53, steps per second: 188, episode reward: 119.316, mean reward: 2.251 [1.494, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.444, 10.138], loss: 0.130304, mae: 0.353928, mean_q: 4.073174
 72110/100000: episode: 1395, duration: 0.273s, episode steps: 53, steps per second: 194, episode reward: 98.748, mean reward: 1.863 [1.477, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-1.053, 10.201], loss: 0.129781, mae: 0.362181, mean_q: 4.094947
 72156/100000: episode: 1396, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 157.682, mean reward: 3.428 [2.404, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.282, 10.417], loss: 0.121694, mae: 0.345852, mean_q: 4.053827
 72218/100000: episode: 1397, duration: 0.312s, episode steps: 62, steps per second: 199, episode reward: 136.240, mean reward: 2.197 [1.598, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.907, 10.249], loss: 0.141463, mae: 0.369504, mean_q: 4.130879
 72271/100000: episode: 1398, duration: 0.271s, episode steps: 53, steps per second: 195, episode reward: 149.715, mean reward: 2.825 [1.897, 6.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.786, 10.484], loss: 0.136938, mae: 0.364594, mean_q: 4.140897
 72276/100000: episode: 1399, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 12.672, mean reward: 2.534 [2.311, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.474], loss: 0.108955, mae: 0.331745, mean_q: 4.134842
 72333/100000: episode: 1400, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 115.938, mean reward: 2.034 [1.436, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-0.614, 10.100], loss: 0.133273, mae: 0.363507, mean_q: 4.147964
 72385/100000: episode: 1401, duration: 0.302s, episode steps: 52, steps per second: 172, episode reward: 114.629, mean reward: 2.204 [1.594, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.350, 10.279], loss: 0.137088, mae: 0.361991, mean_q: 4.164441
 72438/100000: episode: 1402, duration: 0.342s, episode steps: 53, steps per second: 155, episode reward: 123.090, mean reward: 2.322 [1.517, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.571, 10.198], loss: 0.131449, mae: 0.355330, mean_q: 4.145733
 72491/100000: episode: 1403, duration: 0.500s, episode steps: 53, steps per second: 106, episode reward: 105.515, mean reward: 1.991 [1.459, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.801, 10.175], loss: 291.741516, mae: 1.352479, mean_q: 4.645662
 72537/100000: episode: 1404, duration: 0.298s, episode steps: 46, steps per second: 155, episode reward: 115.283, mean reward: 2.506 [1.639, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.384, 10.228], loss: 0.360457, mae: 0.599010, mean_q: 4.199405
 72588/100000: episode: 1405, duration: 0.367s, episode steps: 51, steps per second: 139, episode reward: 116.927, mean reward: 2.293 [1.665, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.941, 10.349], loss: 0.157628, mae: 0.411190, mean_q: 4.217210
 72639/100000: episode: 1406, duration: 0.368s, episode steps: 51, steps per second: 139, episode reward: 162.146, mean reward: 3.179 [2.100, 5.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.383, 10.428], loss: 0.157179, mae: 0.400947, mean_q: 4.233718
 72690/100000: episode: 1407, duration: 0.372s, episode steps: 51, steps per second: 137, episode reward: 135.963, mean reward: 2.666 [1.770, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.435, 10.356], loss: 0.132069, mae: 0.370011, mean_q: 4.177325
 72695/100000: episode: 1408, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 13.268, mean reward: 2.654 [2.220, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.494, 10.296], loss: 0.154906, mae: 0.369606, mean_q: 4.155937
 72748/100000: episode: 1409, duration: 0.358s, episode steps: 53, steps per second: 148, episode reward: 114.527, mean reward: 2.161 [1.657, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-1.307, 10.455], loss: 0.137895, mae: 0.376176, mean_q: 4.239122
 72799/100000: episode: 1410, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 127.709, mean reward: 2.504 [1.724, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.495, 10.262], loss: 0.153795, mae: 0.384106, mean_q: 4.241854
 72856/100000: episode: 1411, duration: 0.395s, episode steps: 57, steps per second: 144, episode reward: 138.814, mean reward: 2.435 [1.828, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-2.222, 10.300], loss: 809.967651, mae: 3.331562, mean_q: 5.760756
 72909/100000: episode: 1412, duration: 0.275s, episode steps: 53, steps per second: 193, episode reward: 111.028, mean reward: 2.095 [1.558, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.893, 10.241], loss: 291.870697, mae: 1.722624, mean_q: 4.852750
 72914/100000: episode: 1413, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 14.967, mean reward: 2.993 [2.087, 4.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.520], loss: 1.462213, mae: 1.382172, mean_q: 5.855929
 72965/100000: episode: 1414, duration: 0.314s, episode steps: 51, steps per second: 162, episode reward: 107.493, mean reward: 2.108 [1.499, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.386, 10.100], loss: 0.412825, mae: 0.657004, mean_q: 4.765633
 73027/100000: episode: 1415, duration: 0.365s, episode steps: 62, steps per second: 170, episode reward: 178.541, mean reward: 2.880 [1.908, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.792 [-0.919, 10.393], loss: 496.387268, mae: 2.091026, mean_q: 5.533864
 73080/100000: episode: 1416, duration: 0.325s, episode steps: 53, steps per second: 163, episode reward: 129.584, mean reward: 2.445 [1.474, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.663, 10.255], loss: 0.331000, mae: 0.574324, mean_q: 4.763473
 73142/100000: episode: 1417, duration: 0.328s, episode steps: 62, steps per second: 189, episode reward: 138.022, mean reward: 2.226 [1.572, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-0.785, 10.144], loss: 248.479675, mae: 1.362545, mean_q: 5.204363
 73193/100000: episode: 1418, duration: 0.303s, episode steps: 51, steps per second: 169, episode reward: 100.150, mean reward: 1.964 [1.523, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.480, 10.158], loss: 0.229846, mae: 0.487123, mean_q: 4.573020
 73239/100000: episode: 1419, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 112.280, mean reward: 2.441 [1.558, 7.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.478, 10.134], loss: 334.927155, mae: 1.499849, mean_q: 5.128366
 73285/100000: episode: 1420, duration: 0.238s, episode steps: 46, steps per second: 194, episode reward: 111.011, mean reward: 2.413 [1.678, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.206, 10.100], loss: 0.308677, mae: 0.565359, mean_q: 4.718953
 73336/100000: episode: 1421, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 118.436, mean reward: 2.322 [1.600, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.338, 10.209], loss: 301.677032, mae: 1.149382, mean_q: 4.613451
 73341/100000: episode: 1422, duration: 0.042s, episode steps: 5, steps per second: 119, episode reward: 11.497, mean reward: 2.299 [2.220, 2.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.394], loss: 1.633498, mae: 1.440787, mean_q: 5.953144
 73346/100000: episode: 1423, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 11.922, mean reward: 2.384 [2.165, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.388], loss: 1.339093, mae: 1.324957, mean_q: 5.975420
 73392/100000: episode: 1424, duration: 0.285s, episode steps: 46, steps per second: 161, episode reward: 127.058, mean reward: 2.762 [1.707, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.266, 10.267], loss: 0.373500, mae: 0.616262, mean_q: 4.752891
 73438/100000: episode: 1425, duration: 0.278s, episode steps: 46, steps per second: 166, episode reward: 89.915, mean reward: 1.955 [1.455, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.254, 10.180], loss: 0.218362, mae: 0.461246, mean_q: 4.586339
 73497/100000: episode: 1426, duration: 0.319s, episode steps: 59, steps per second: 185, episode reward: 118.973, mean reward: 2.016 [1.484, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.558, 10.149], loss: 0.218029, mae: 0.457669, mean_q: 4.552609
 73548/100000: episode: 1427, duration: 0.333s, episode steps: 51, steps per second: 153, episode reward: 127.844, mean reward: 2.507 [1.776, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.552, 10.307], loss: 0.193055, mae: 0.433408, mean_q: 4.450651
 73601/100000: episode: 1428, duration: 0.464s, episode steps: 53, steps per second: 114, episode reward: 109.940, mean reward: 2.074 [1.500, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.830, 10.166], loss: 0.176957, mae: 0.418505, mean_q: 4.410580
 73658/100000: episode: 1429, duration: 0.333s, episode steps: 57, steps per second: 171, episode reward: 142.222, mean reward: 2.495 [1.530, 5.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.861, 10.100], loss: 0.181063, mae: 0.419736, mean_q: 4.414040
 73715/100000: episode: 1430, duration: 0.388s, episode steps: 57, steps per second: 147, episode reward: 125.826, mean reward: 2.207 [1.441, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-0.633, 10.104], loss: 0.163582, mae: 0.406627, mean_q: 4.410308
 73720/100000: episode: 1431, duration: 0.053s, episode steps: 5, steps per second: 95, episode reward: 14.540, mean reward: 2.908 [2.454, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-1.713, 10.414], loss: 0.167512, mae: 0.413594, mean_q: 4.347616
 73782/100000: episode: 1432, duration: 0.704s, episode steps: 62, steps per second: 88, episode reward: 144.663, mean reward: 2.333 [1.481, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.788 [-0.665, 10.160], loss: 248.943222, mae: 1.191636, mean_q: 4.760084
 73834/100000: episode: 1433, duration: 0.569s, episode steps: 52, steps per second: 91, episode reward: 102.145, mean reward: 1.964 [1.487, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.624, 10.123], loss: 0.340515, mae: 0.599044, mean_q: 4.483819
 73886/100000: episode: 1434, duration: 0.300s, episode steps: 52, steps per second: 173, episode reward: 144.313, mean reward: 2.775 [1.559, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.339, 10.100], loss: 296.552307, mae: 1.501829, mean_q: 5.094348
 73905/100000: episode: 1435, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 44.291, mean reward: 2.331 [1.968, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.790, 10.321], loss: 0.302383, mae: 0.572934, mean_q: 4.472792
 73957/100000: episode: 1436, duration: 0.306s, episode steps: 52, steps per second: 170, episode reward: 137.100, mean reward: 2.637 [1.519, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.545, 10.355], loss: 296.380951, mae: 1.462943, mean_q: 5.121081
 73976/100000: episode: 1437, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 54.589, mean reward: 2.873 [2.314, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.401], loss: 0.300369, mae: 0.566519, mean_q: 4.476896
 74022/100000: episode: 1438, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 97.836, mean reward: 2.127 [1.565, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.349, 10.279], loss: 0.229637, mae: 0.497696, mean_q: 4.501167
 74073/100000: episode: 1439, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 123.867, mean reward: 2.429 [1.556, 5.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-1.071, 10.100], loss: 0.214926, mae: 0.466519, mean_q: 4.570397
 74135/100000: episode: 1440, duration: 0.342s, episode steps: 62, steps per second: 182, episode reward: 125.309, mean reward: 2.021 [1.470, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.798 [-0.947, 10.300], loss: 0.189461, mae: 0.424505, mean_q: 4.511007
 74194/100000: episode: 1441, duration: 0.315s, episode steps: 59, steps per second: 187, episode reward: 151.402, mean reward: 2.566 [1.606, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-1.729, 10.157], loss: 0.180507, mae: 0.422444, mean_q: 4.491169
 74213/100000: episode: 1442, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 41.349, mean reward: 2.176 [1.862, 2.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.302], loss: 0.128089, mae: 0.378670, mean_q: 4.442671
 74232/100000: episode: 1443, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 55.290, mean reward: 2.910 [2.249, 4.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.446, 10.369], loss: 1.133590, mae: 1.238468, mean_q: 5.697756
 74283/100000: episode: 1444, duration: 0.333s, episode steps: 51, steps per second: 153, episode reward: 129.716, mean reward: 2.543 [1.484, 5.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.116, 10.100], loss: 0.377522, mae: 0.645721, mean_q: 4.644106
 74329/100000: episode: 1445, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 137.396, mean reward: 2.987 [2.092, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.117, 10.460], loss: 0.224948, mae: 0.492796, mean_q: 4.529622
 74382/100000: episode: 1446, duration: 0.288s, episode steps: 53, steps per second: 184, episode reward: 117.196, mean reward: 2.211 [1.780, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.690, 10.235], loss: 0.173139, mae: 0.434726, mean_q: 4.519367
 74428/100000: episode: 1447, duration: 0.306s, episode steps: 46, steps per second: 150, episode reward: 126.069, mean reward: 2.741 [1.863, 5.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.350, 10.350], loss: 0.188222, mae: 0.444034, mean_q: 4.470769
 74480/100000: episode: 1448, duration: 0.317s, episode steps: 52, steps per second: 164, episode reward: 111.661, mean reward: 2.147 [1.543, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.901, 10.216], loss: 0.176504, mae: 0.436594, mean_q: 4.499475
 74531/100000: episode: 1449, duration: 0.301s, episode steps: 51, steps per second: 169, episode reward: 120.717, mean reward: 2.367 [1.806, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.865, 10.454], loss: 302.672333, mae: 1.451649, mean_q: 5.119147
 74583/100000: episode: 1450, duration: 0.276s, episode steps: 52, steps per second: 189, episode reward: 104.940, mean reward: 2.018 [1.463, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.696, 10.318], loss: 0.215598, mae: 0.470775, mean_q: 4.585665
 74588/100000: episode: 1451, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 14.441, mean reward: 2.888 [2.237, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.509], loss: 0.201108, mae: 0.475617, mean_q: 4.736637
 74641/100000: episode: 1452, duration: 0.299s, episode steps: 53, steps per second: 177, episode reward: 124.830, mean reward: 2.355 [1.569, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.567, 10.233], loss: 290.952637, mae: 1.532608, mean_q: 5.330030
 74692/100000: episode: 1453, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 124.075, mean reward: 2.433 [1.867, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.342, 10.400], loss: 301.988312, mae: 1.304718, mean_q: 4.943488
 74697/100000: episode: 1454, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 12.502, mean reward: 2.500 [2.100, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.550, 10.401], loss: 1.187780, mae: 1.283546, mean_q: 5.845724
 74702/100000: episode: 1455, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 12.425, mean reward: 2.485 [2.197, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.420], loss: 0.704024, mae: 0.928018, mean_q: 5.591244
 74761/100000: episode: 1456, duration: 0.317s, episode steps: 59, steps per second: 186, episode reward: 144.703, mean reward: 2.453 [1.655, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-1.152, 10.259], loss: 0.274571, mae: 0.538212, mean_q: 4.678550
 74823/100000: episode: 1457, duration: 0.482s, episode steps: 62, steps per second: 129, episode reward: 159.073, mean reward: 2.566 [1.901, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.796 [-0.671, 10.375], loss: 0.191191, mae: 0.445050, mean_q: 4.712910
 74876/100000: episode: 1458, duration: 0.571s, episode steps: 53, steps per second: 93, episode reward: 122.264, mean reward: 2.307 [1.493, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.326, 10.100], loss: 0.191610, mae: 0.445744, mean_q: 4.704650
[Info] Complete ISplit Iteration
[Info] Levels: [5.157166, 7.9053144]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 74928/100000: episode: 1459, duration: 5.397s, episode steps: 52, steps per second: 10, episode reward: 123.819, mean reward: 2.381 [1.567, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.990, 10.313], loss: 0.185693, mae: 0.436714, mean_q: 4.596082
 75028/100000: episode: 1460, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 180.598, mean reward: 1.806 [1.460, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.653, 10.176], loss: 154.243149, mae: 0.838544, mean_q: 4.750811
 75128/100000: episode: 1461, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 192.884, mean reward: 1.929 [1.497, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.214], loss: 0.338105, mae: 0.604655, mean_q: 4.691422
 75228/100000: episode: 1462, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 191.171, mean reward: 1.912 [1.478, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.458], loss: 154.321228, mae: 0.991734, mean_q: 4.893755
 75328/100000: episode: 1463, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 200.865, mean reward: 2.009 [1.463, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.681, 10.273], loss: 0.209350, mae: 0.471055, mean_q: 4.660611
 75428/100000: episode: 1464, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 199.161, mean reward: 1.992 [1.483, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.449, 10.098], loss: 154.380173, mae: 1.005625, mean_q: 4.946085
 75528/100000: episode: 1465, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 189.649, mean reward: 1.896 [1.500, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.735, 10.123], loss: 0.216516, mae: 0.484324, mean_q: 4.693570
 75628/100000: episode: 1466, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 196.969, mean reward: 1.970 [1.506, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.768, 10.368], loss: 154.260971, mae: 1.013140, mean_q: 4.937943
 75728/100000: episode: 1467, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 194.286, mean reward: 1.943 [1.484, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.446, 10.098], loss: 461.400818, mae: 2.066494, mean_q: 5.714892
 75828/100000: episode: 1468, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 195.895, mean reward: 1.959 [1.431, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.807, 10.100], loss: 0.305372, mae: 0.577407, mean_q: 4.835724
 75928/100000: episode: 1469, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 190.118, mean reward: 1.901 [1.468, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.917, 10.098], loss: 0.214362, mae: 0.474088, mean_q: 4.710334
 76028/100000: episode: 1470, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 183.816, mean reward: 1.838 [1.466, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.642, 10.189], loss: 154.305862, mae: 1.024269, mean_q: 4.986846
 76128/100000: episode: 1471, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 211.448, mean reward: 2.114 [1.542, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.946, 10.098], loss: 0.200452, mae: 0.454952, mean_q: 4.621851
 76228/100000: episode: 1472, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 192.052, mean reward: 1.921 [1.457, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.844, 10.098], loss: 0.164320, mae: 0.413702, mean_q: 4.543715
 76328/100000: episode: 1473, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.310, mean reward: 1.983 [1.468, 5.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.699, 10.098], loss: 0.156051, mae: 0.395300, mean_q: 4.484641
 76428/100000: episode: 1474, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 185.506, mean reward: 1.855 [1.492, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.424, 10.307], loss: 0.155294, mae: 0.392088, mean_q: 4.421432
 76528/100000: episode: 1475, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.082, mean reward: 1.861 [1.467, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.098], loss: 0.150696, mae: 0.379821, mean_q: 4.424821
 76628/100000: episode: 1476, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.587, mean reward: 1.896 [1.492, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.764, 10.336], loss: 0.135714, mae: 0.369812, mean_q: 4.363489
 76728/100000: episode: 1477, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.297, mean reward: 1.963 [1.449, 5.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.311, 10.098], loss: 0.162619, mae: 0.390940, mean_q: 4.339375
 76828/100000: episode: 1478, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.480, mean reward: 1.895 [1.449, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.311], loss: 0.136446, mae: 0.372504, mean_q: 4.368786
 76928/100000: episode: 1479, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 190.798, mean reward: 1.908 [1.458, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.035, 10.098], loss: 0.136200, mae: 0.364038, mean_q: 4.334619
 77028/100000: episode: 1480, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 189.751, mean reward: 1.898 [1.462, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.307, 10.109], loss: 0.141251, mae: 0.367571, mean_q: 4.305927
 77128/100000: episode: 1481, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.211, mean reward: 1.902 [1.469, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.587, 10.170], loss: 0.139860, mae: 0.363351, mean_q: 4.292455
 77228/100000: episode: 1482, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 195.070, mean reward: 1.951 [1.464, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.650, 10.295], loss: 0.139822, mae: 0.366691, mean_q: 4.259356
 77328/100000: episode: 1483, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 195.582, mean reward: 1.956 [1.457, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.892, 10.098], loss: 0.132456, mae: 0.355676, mean_q: 4.249573
 77428/100000: episode: 1484, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.255, mean reward: 1.923 [1.451, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.061, 10.098], loss: 0.126126, mae: 0.338615, mean_q: 4.215158
 77528/100000: episode: 1485, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.764, mean reward: 1.918 [1.446, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.098], loss: 0.134058, mae: 0.354445, mean_q: 4.196848
 77628/100000: episode: 1486, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.920, mean reward: 1.859 [1.479, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.431, 10.130], loss: 0.120578, mae: 0.341986, mean_q: 4.175856
 77728/100000: episode: 1487, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.253, mean reward: 1.973 [1.493, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.090, 10.098], loss: 0.133353, mae: 0.351436, mean_q: 4.172809
 77828/100000: episode: 1488, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.370, mean reward: 1.834 [1.477, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.148, 10.098], loss: 0.124032, mae: 0.333459, mean_q: 4.141803
 77928/100000: episode: 1489, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 185.007, mean reward: 1.850 [1.442, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.914, 10.256], loss: 0.119834, mae: 0.334008, mean_q: 4.113855
 78028/100000: episode: 1490, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 206.741, mean reward: 2.067 [1.483, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.142], loss: 0.116570, mae: 0.328792, mean_q: 4.116538
 78128/100000: episode: 1491, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.338, mean reward: 1.913 [1.454, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.616, 10.098], loss: 0.098333, mae: 0.312183, mean_q: 4.094268
 78228/100000: episode: 1492, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.838, mean reward: 2.058 [1.485, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.589, 10.098], loss: 0.106449, mae: 0.323899, mean_q: 4.105875
 78328/100000: episode: 1493, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 187.288, mean reward: 1.873 [1.452, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.034, 10.098], loss: 0.106267, mae: 0.315719, mean_q: 4.076738
 78428/100000: episode: 1494, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.850, mean reward: 1.898 [1.487, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.301, 10.098], loss: 0.101976, mae: 0.315978, mean_q: 4.070675
 78528/100000: episode: 1495, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 197.679, mean reward: 1.977 [1.474, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.707, 10.180], loss: 0.113410, mae: 0.326303, mean_q: 4.064943
 78628/100000: episode: 1496, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 211.979, mean reward: 2.120 [1.496, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.237, 10.098], loss: 0.105162, mae: 0.316371, mean_q: 4.031520
 78728/100000: episode: 1497, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.307, mean reward: 1.953 [1.473, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.520, 10.098], loss: 0.118334, mae: 0.328839, mean_q: 4.039790
 78828/100000: episode: 1498, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.744, mean reward: 1.897 [1.470, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.869, 10.165], loss: 0.110563, mae: 0.318667, mean_q: 4.039560
 78928/100000: episode: 1499, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.259, mean reward: 1.913 [1.448, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.427, 10.210], loss: 0.092919, mae: 0.299925, mean_q: 3.988850
 79028/100000: episode: 1500, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.320, mean reward: 1.823 [1.454, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.726, 10.135], loss: 0.102454, mae: 0.310305, mean_q: 4.002746
 79128/100000: episode: 1501, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.619, mean reward: 1.836 [1.430, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.316, 10.098], loss: 0.101553, mae: 0.305466, mean_q: 3.982275
 79228/100000: episode: 1502, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 239.342, mean reward: 2.393 [1.449, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.377, 10.098], loss: 0.097468, mae: 0.301633, mean_q: 3.932281
 79328/100000: episode: 1503, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 196.003, mean reward: 1.960 [1.455, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.495, 10.098], loss: 0.087524, mae: 0.296354, mean_q: 3.947601
 79428/100000: episode: 1504, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.726, mean reward: 1.847 [1.463, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.347, 10.098], loss: 0.089831, mae: 0.294502, mean_q: 3.926274
 79528/100000: episode: 1505, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 186.011, mean reward: 1.860 [1.464, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.168, 10.318], loss: 0.087650, mae: 0.289920, mean_q: 3.897102
 79628/100000: episode: 1506, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.374, mean reward: 1.914 [1.479, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.748, 10.118], loss: 0.080409, mae: 0.277644, mean_q: 3.887839
 79728/100000: episode: 1507, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.224, mean reward: 2.002 [1.450, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.848, 10.098], loss: 0.075488, mae: 0.271706, mean_q: 3.856357
 79828/100000: episode: 1508, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 227.716, mean reward: 2.277 [1.491, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.207, 10.098], loss: 0.085144, mae: 0.283833, mean_q: 3.853344
 79928/100000: episode: 1509, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.487, mean reward: 1.895 [1.472, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.921, 10.098], loss: 0.081662, mae: 0.280064, mean_q: 3.848315
 80028/100000: episode: 1510, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 182.280, mean reward: 1.823 [1.465, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.703, 10.122], loss: 0.084682, mae: 0.282843, mean_q: 3.845558
 80128/100000: episode: 1511, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.802, mean reward: 1.948 [1.445, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.769, 10.098], loss: 0.081588, mae: 0.278203, mean_q: 3.841265
 80228/100000: episode: 1512, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.703, mean reward: 1.957 [1.446, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.752, 10.098], loss: 0.074118, mae: 0.267374, mean_q: 3.823533
 80328/100000: episode: 1513, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 203.842, mean reward: 2.038 [1.467, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.361, 10.354], loss: 0.080827, mae: 0.279125, mean_q: 3.823467
 80428/100000: episode: 1514, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 177.487, mean reward: 1.775 [1.483, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.963, 10.098], loss: 0.080993, mae: 0.279707, mean_q: 3.820712
 80528/100000: episode: 1515, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.568, mean reward: 1.946 [1.445, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.571, 10.098], loss: 0.078546, mae: 0.275052, mean_q: 3.821964
 80628/100000: episode: 1516, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 200.559, mean reward: 2.006 [1.445, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.120], loss: 0.075603, mae: 0.269445, mean_q: 3.835796
 80728/100000: episode: 1517, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.727, mean reward: 1.817 [1.446, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.110], loss: 0.078846, mae: 0.278995, mean_q: 3.841851
 80828/100000: episode: 1518, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 185.339, mean reward: 1.853 [1.475, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.098], loss: 0.086553, mae: 0.284207, mean_q: 3.842758
 80928/100000: episode: 1519, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 187.480, mean reward: 1.875 [1.469, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.092, 10.114], loss: 0.092070, mae: 0.292167, mean_q: 3.836939
 81028/100000: episode: 1520, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 187.992, mean reward: 1.880 [1.437, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.098], loss: 0.087350, mae: 0.283769, mean_q: 3.824098
 81128/100000: episode: 1521, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.335, mean reward: 1.943 [1.463, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.825, 10.227], loss: 0.077668, mae: 0.274354, mean_q: 3.818178
 81228/100000: episode: 1522, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.013, mean reward: 1.860 [1.448, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.152], loss: 0.081961, mae: 0.280693, mean_q: 3.832461
 81328/100000: episode: 1523, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.795, mean reward: 1.888 [1.443, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.331, 10.166], loss: 0.086233, mae: 0.285980, mean_q: 3.820778
 81428/100000: episode: 1524, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 231.059, mean reward: 2.311 [1.432, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.244, 10.427], loss: 0.080557, mae: 0.276558, mean_q: 3.821805
 81528/100000: episode: 1525, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 182.562, mean reward: 1.826 [1.453, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.178], loss: 0.085324, mae: 0.283166, mean_q: 3.845757
 81628/100000: episode: 1526, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.020, mean reward: 1.940 [1.441, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.196], loss: 0.084174, mae: 0.282719, mean_q: 3.828029
 81728/100000: episode: 1527, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.964, mean reward: 1.940 [1.452, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.262, 10.259], loss: 0.083398, mae: 0.283917, mean_q: 3.844089
 81828/100000: episode: 1528, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 181.164, mean reward: 1.812 [1.453, 2.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.715, 10.211], loss: 0.084655, mae: 0.277462, mean_q: 3.832068
 81928/100000: episode: 1529, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 202.439, mean reward: 2.024 [1.464, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.788, 10.318], loss: 0.079765, mae: 0.280807, mean_q: 3.837626
 82028/100000: episode: 1530, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.381, mean reward: 1.864 [1.434, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.577, 10.098], loss: 0.085279, mae: 0.287113, mean_q: 3.840135
 82128/100000: episode: 1531, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 211.179, mean reward: 2.112 [1.485, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.357, 10.098], loss: 0.093168, mae: 0.290522, mean_q: 3.837013
 82228/100000: episode: 1532, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 223.651, mean reward: 2.237 [1.471, 5.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.004, 10.098], loss: 0.092723, mae: 0.290318, mean_q: 3.838088
 82328/100000: episode: 1533, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 182.798, mean reward: 1.828 [1.489, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.904, 10.098], loss: 0.098543, mae: 0.292674, mean_q: 3.854755
 82428/100000: episode: 1534, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.673, mean reward: 1.897 [1.517, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.750, 10.262], loss: 0.082362, mae: 0.284076, mean_q: 3.848527
 82528/100000: episode: 1535, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 205.071, mean reward: 2.051 [1.468, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.648, 10.294], loss: 0.089498, mae: 0.283334, mean_q: 3.842614
 82628/100000: episode: 1536, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 184.738, mean reward: 1.847 [1.439, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.010, 10.098], loss: 0.090808, mae: 0.292535, mean_q: 3.860172
 82728/100000: episode: 1537, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 209.820, mean reward: 2.098 [1.491, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.414, 10.315], loss: 0.087900, mae: 0.284916, mean_q: 3.861737
 82828/100000: episode: 1538, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.651, mean reward: 1.837 [1.441, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.098], loss: 0.092469, mae: 0.286989, mean_q: 3.860274
 82928/100000: episode: 1539, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 203.628, mean reward: 2.036 [1.474, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.731, 10.188], loss: 0.088565, mae: 0.290235, mean_q: 3.872368
 83028/100000: episode: 1540, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.149, mean reward: 1.901 [1.525, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.278, 10.113], loss: 0.084063, mae: 0.278249, mean_q: 3.869719
 83128/100000: episode: 1541, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 194.700, mean reward: 1.947 [1.444, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.029, 10.098], loss: 0.087936, mae: 0.281503, mean_q: 3.860917
 83228/100000: episode: 1542, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.953, mean reward: 1.930 [1.457, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.451, 10.204], loss: 0.086375, mae: 0.286385, mean_q: 3.864924
 83328/100000: episode: 1543, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 207.621, mean reward: 2.076 [1.453, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.005, 10.098], loss: 0.093078, mae: 0.291079, mean_q: 3.851866
 83428/100000: episode: 1544, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 187.715, mean reward: 1.877 [1.488, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.098], loss: 0.085663, mae: 0.288845, mean_q: 3.851862
 83528/100000: episode: 1545, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.475, mean reward: 1.945 [1.502, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.606, 10.098], loss: 0.096276, mae: 0.289863, mean_q: 3.881704
 83628/100000: episode: 1546, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.847, mean reward: 1.978 [1.472, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.213], loss: 0.083906, mae: 0.282705, mean_q: 3.855508
 83728/100000: episode: 1547, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.294, mean reward: 1.893 [1.432, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.770, 10.183], loss: 0.087460, mae: 0.286130, mean_q: 3.857864
 83828/100000: episode: 1548, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.286, mean reward: 1.903 [1.452, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.098], loss: 0.086179, mae: 0.287852, mean_q: 3.838661
 83928/100000: episode: 1549, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.293, mean reward: 1.933 [1.458, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.209], loss: 0.084036, mae: 0.283790, mean_q: 3.845087
 84028/100000: episode: 1550, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.539, mean reward: 1.925 [1.448, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.828, 10.098], loss: 0.095129, mae: 0.295270, mean_q: 3.868850
 84128/100000: episode: 1551, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 204.138, mean reward: 2.041 [1.498, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.420, 10.359], loss: 0.090275, mae: 0.298609, mean_q: 3.897738
 84228/100000: episode: 1552, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 210.728, mean reward: 2.107 [1.501, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.704, 10.098], loss: 0.083721, mae: 0.285692, mean_q: 3.848545
 84328/100000: episode: 1553, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 195.610, mean reward: 1.956 [1.469, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.481, 10.382], loss: 0.092857, mae: 0.291806, mean_q: 3.859299
 84428/100000: episode: 1554, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 192.544, mean reward: 1.925 [1.487, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.181, 10.098], loss: 0.096709, mae: 0.297086, mean_q: 3.853516
 84528/100000: episode: 1555, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 185.713, mean reward: 1.857 [1.465, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.640, 10.098], loss: 0.082757, mae: 0.284474, mean_q: 3.858286
 84628/100000: episode: 1556, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 211.498, mean reward: 2.115 [1.445, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.749, 10.098], loss: 0.087116, mae: 0.289936, mean_q: 3.863914
 84728/100000: episode: 1557, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 225.782, mean reward: 2.258 [1.435, 5.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.171, 10.098], loss: 0.086711, mae: 0.287194, mean_q: 3.845132
 84828/100000: episode: 1558, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.603, mean reward: 1.936 [1.452, 4.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.270], loss: 0.089176, mae: 0.288719, mean_q: 3.875578
[Info] 1-TH LEVEL FOUND: 5.14701509475708, Considering 10/90 traces
 84928/100000: episode: 1559, duration: 4.790s, episode steps: 100, steps per second: 21, episode reward: 193.436, mean reward: 1.934 [1.448, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.908, 10.181], loss: 0.085914, mae: 0.287831, mean_q: 3.860125
 84931/100000: episode: 1560, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 7.529, mean reward: 2.510 [2.318, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.384, 10.100], loss: 0.174534, mae: 0.337037, mean_q: 3.733154
 84955/100000: episode: 1561, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 53.275, mean reward: 2.220 [1.711, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.117, 10.100], loss: 0.104986, mae: 0.310161, mean_q: 3.862149
 84958/100000: episode: 1562, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 11.043, mean reward: 3.681 [3.420, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.412, 10.100], loss: 0.110813, mae: 0.306532, mean_q: 3.883204
 84961/100000: episode: 1563, duration: 0.026s, episode steps: 3, steps per second: 117, episode reward: 9.092, mean reward: 3.031 [3.007, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.364, 10.100], loss: 0.096922, mae: 0.309790, mean_q: 3.889501
 84977/100000: episode: 1564, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 52.136, mean reward: 3.259 [2.464, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.112, 10.100], loss: 0.098242, mae: 0.296724, mean_q: 3.874924
 84997/100000: episode: 1565, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 49.816, mean reward: 2.491 [1.967, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.125, 10.100], loss: 0.103606, mae: 0.288049, mean_q: 3.882853
 85048/100000: episode: 1566, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 121.757, mean reward: 2.387 [1.514, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.402, 10.100], loss: 0.086278, mae: 0.277580, mean_q: 3.848296
 85138/100000: episode: 1567, duration: 0.463s, episode steps: 90, steps per second: 194, episode reward: 193.406, mean reward: 2.149 [1.445, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.806, 10.100], loss: 0.097367, mae: 0.292355, mean_q: 3.867712
 85162/100000: episode: 1568, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 65.271, mean reward: 2.720 [2.047, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.759, 10.100], loss: 0.090582, mae: 0.283808, mean_q: 3.921799
 85182/100000: episode: 1569, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 50.290, mean reward: 2.514 [2.142, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.166, 10.100], loss: 0.120755, mae: 0.323200, mean_q: 3.902884
 85270/100000: episode: 1570, duration: 0.449s, episode steps: 88, steps per second: 196, episode reward: 166.857, mean reward: 1.896 [1.488, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.570 [-1.020, 10.165], loss: 0.094426, mae: 0.290148, mean_q: 3.893620
 85288/100000: episode: 1571, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 61.380, mean reward: 3.410 [2.574, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.057, 10.100], loss: 0.087141, mae: 0.281776, mean_q: 3.831859
 85303/100000: episode: 1572, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 45.919, mean reward: 3.061 [2.125, 4.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.477], loss: 0.101577, mae: 0.297664, mean_q: 3.927315
 85323/100000: episode: 1573, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 54.870, mean reward: 2.744 [2.240, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.360, 10.100], loss: 0.127605, mae: 0.331145, mean_q: 3.922747
 85413/100000: episode: 1574, duration: 0.453s, episode steps: 90, steps per second: 199, episode reward: 188.878, mean reward: 2.099 [1.459, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.856, 10.147], loss: 0.092426, mae: 0.292934, mean_q: 3.901882
 85437/100000: episode: 1575, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 59.694, mean reward: 2.487 [1.485, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.047, 10.206], loss: 0.093420, mae: 0.299226, mean_q: 3.901728
 85453/100000: episode: 1576, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 44.692, mean reward: 2.793 [2.452, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.292, 10.100], loss: 0.087499, mae: 0.293387, mean_q: 3.924154
 85473/100000: episode: 1577, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 58.542, mean reward: 2.927 [2.481, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.228, 10.100], loss: 0.085643, mae: 0.286484, mean_q: 3.927553
 85497/100000: episode: 1578, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 64.900, mean reward: 2.704 [1.901, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.493, 10.100], loss: 0.086383, mae: 0.275082, mean_q: 3.927239
 85500/100000: episode: 1579, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 10.955, mean reward: 3.652 [3.633, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.372, 10.100], loss: 0.078020, mae: 0.281571, mean_q: 3.894441
 85524/100000: episode: 1580, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 54.367, mean reward: 2.265 [1.792, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.449, 10.100], loss: 0.093883, mae: 0.300012, mean_q: 3.957306
 85539/100000: episode: 1581, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 32.340, mean reward: 2.156 [1.741, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.077, 10.286], loss: 0.085183, mae: 0.282514, mean_q: 3.927398
 85627/100000: episode: 1582, duration: 0.472s, episode steps: 88, steps per second: 186, episode reward: 172.781, mean reward: 1.963 [1.440, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-1.149, 10.100], loss: 0.092786, mae: 0.296773, mean_q: 3.931179
 85657/100000: episode: 1583, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 107.138, mean reward: 3.571 [2.655, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.732, 10.541], loss: 0.106002, mae: 0.314900, mean_q: 3.960916
 85681/100000: episode: 1584, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 54.448, mean reward: 2.269 [1.952, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.109, 10.100], loss: 0.101944, mae: 0.304942, mean_q: 3.949885
 85684/100000: episode: 1585, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 10.051, mean reward: 3.350 [3.291, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.292, 10.100], loss: 0.065556, mae: 0.281483, mean_q: 3.914961
 85735/100000: episode: 1586, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 111.819, mean reward: 2.193 [1.734, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.710, 10.258], loss: 0.116724, mae: 0.320325, mean_q: 3.960908
 85765/100000: episode: 1587, duration: 0.184s, episode steps: 30, steps per second: 163, episode reward: 76.476, mean reward: 2.549 [2.066, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.293, 10.385], loss: 0.111734, mae: 0.322415, mean_q: 3.995645
 85780/100000: episode: 1588, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 39.583, mean reward: 2.639 [2.116, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.915, 10.334], loss: 0.093269, mae: 0.319542, mean_q: 3.991324
 85804/100000: episode: 1589, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 63.505, mean reward: 2.646 [2.088, 4.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.917, 10.100], loss: 0.135515, mae: 0.336536, mean_q: 4.041599
 85894/100000: episode: 1590, duration: 0.469s, episode steps: 90, steps per second: 192, episode reward: 175.283, mean reward: 1.948 [1.462, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.750, 10.110], loss: 0.100985, mae: 0.307308, mean_q: 4.002941
 85984/100000: episode: 1591, duration: 0.486s, episode steps: 90, steps per second: 185, episode reward: 173.315, mean reward: 1.926 [1.469, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.835, 10.310], loss: 0.103555, mae: 0.311062, mean_q: 3.993859
 86000/100000: episode: 1592, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 51.586, mean reward: 3.224 [2.323, 6.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.483, 10.100], loss: 0.121462, mae: 0.300779, mean_q: 3.981756
 86015/100000: episode: 1593, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 42.276, mean reward: 2.818 [2.450, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.411], loss: 0.116651, mae: 0.330286, mean_q: 4.068769
 86035/100000: episode: 1594, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 44.879, mean reward: 2.244 [1.524, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.286, 10.237], loss: 0.107386, mae: 0.304728, mean_q: 3.977510
 86065/100000: episode: 1595, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 73.801, mean reward: 2.460 [1.710, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.245, 10.248], loss: 0.109038, mae: 0.330540, mean_q: 4.059143
 86095/100000: episode: 1596, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 70.394, mean reward: 2.346 [1.821, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.321], loss: 0.117032, mae: 0.327401, mean_q: 4.017867
 86115/100000: episode: 1597, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 48.776, mean reward: 2.439 [1.729, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.471, 10.100], loss: 0.101555, mae: 0.318271, mean_q: 4.100537
 86130/100000: episode: 1598, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 34.258, mean reward: 2.284 [1.861, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.616, 10.333], loss: 0.085102, mae: 0.298927, mean_q: 4.067911
 86154/100000: episode: 1599, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 66.812, mean reward: 2.784 [2.297, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.142, 10.100], loss: 0.124355, mae: 0.333811, mean_q: 4.055370
 86157/100000: episode: 1600, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 8.639, mean reward: 2.880 [2.814, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.415, 10.100], loss: 0.084293, mae: 0.297637, mean_q: 4.022775
 86187/100000: episode: 1601, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 73.311, mean reward: 2.444 [1.624, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.138, 10.193], loss: 0.090160, mae: 0.300317, mean_q: 4.048255
 86238/100000: episode: 1602, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 140.158, mean reward: 2.748 [1.752, 5.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.380, 10.227], loss: 0.099084, mae: 0.297062, mean_q: 4.085969
 86241/100000: episode: 1603, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 15.037, mean reward: 5.012 [4.553, 5.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.414, 10.100], loss: 0.143607, mae: 0.332344, mean_q: 4.000924
 86331/100000: episode: 1604, duration: 0.494s, episode steps: 90, steps per second: 182, episode reward: 204.861, mean reward: 2.276 [1.460, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.425, 10.100], loss: 0.107773, mae: 0.324488, mean_q: 4.110204
 86351/100000: episode: 1605, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 59.899, mean reward: 2.995 [2.136, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.748, 10.100], loss: 0.100010, mae: 0.323617, mean_q: 4.083570
 86367/100000: episode: 1606, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 54.259, mean reward: 3.391 [2.687, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.213, 10.100], loss: 0.133815, mae: 0.337788, mean_q: 4.070923
 86391/100000: episode: 1607, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 68.025, mean reward: 2.834 [2.154, 4.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.134, 10.100], loss: 0.132821, mae: 0.357338, mean_q: 4.132896
 86479/100000: episode: 1608, duration: 0.466s, episode steps: 88, steps per second: 189, episode reward: 161.805, mean reward: 1.839 [1.475, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.997, 10.100], loss: 0.112567, mae: 0.327118, mean_q: 4.102708
 86569/100000: episode: 1609, duration: 0.457s, episode steps: 90, steps per second: 197, episode reward: 171.759, mean reward: 1.908 [1.444, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-0.860, 10.261], loss: 0.126126, mae: 0.338817, mean_q: 4.105774
 86599/100000: episode: 1610, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 80.744, mean reward: 2.691 [2.074, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.625, 10.444], loss: 0.112732, mae: 0.323052, mean_q: 4.094599
 86623/100000: episode: 1611, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 54.334, mean reward: 2.264 [1.950, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.332, 10.100], loss: 0.088192, mae: 0.293333, mean_q: 4.126926
 86653/100000: episode: 1612, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 63.655, mean reward: 2.122 [1.553, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.061, 10.350], loss: 0.114908, mae: 0.335532, mean_q: 4.077106
 86743/100000: episode: 1613, duration: 0.521s, episode steps: 90, steps per second: 173, episode reward: 165.542, mean reward: 1.839 [1.439, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.890, 10.174], loss: 0.118188, mae: 0.339181, mean_q: 4.130013
 86767/100000: episode: 1614, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 58.837, mean reward: 2.452 [2.073, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.666, 10.100], loss: 0.114842, mae: 0.329623, mean_q: 4.128176
 86855/100000: episode: 1615, duration: 0.451s, episode steps: 88, steps per second: 195, episode reward: 156.552, mean reward: 1.779 [1.462, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.160, 10.135], loss: 0.114471, mae: 0.330100, mean_q: 4.130652
 86870/100000: episode: 1616, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 45.777, mean reward: 3.052 [2.384, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.371, 10.545], loss: 0.117887, mae: 0.341794, mean_q: 4.104337
 86890/100000: episode: 1617, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 45.503, mean reward: 2.275 [1.759, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.393, 10.100], loss: 0.115976, mae: 0.330119, mean_q: 4.109196
 86906/100000: episode: 1618, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 37.205, mean reward: 2.325 [1.884, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.172, 10.100], loss: 0.114546, mae: 0.339374, mean_q: 4.139184
 86930/100000: episode: 1619, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 54.908, mean reward: 2.288 [2.013, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.449, 10.100], loss: 0.098291, mae: 0.314882, mean_q: 4.109131
 86981/100000: episode: 1620, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 149.480, mean reward: 2.931 [1.621, 4.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.512, 10.357], loss: 0.100643, mae: 0.314458, mean_q: 4.155382
 87071/100000: episode: 1621, duration: 0.488s, episode steps: 90, steps per second: 184, episode reward: 173.830, mean reward: 1.931 [1.460, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-1.077, 10.100], loss: 0.110152, mae: 0.325846, mean_q: 4.118886
 87089/100000: episode: 1622, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 98.716, mean reward: 5.484 [3.670, 10.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.455, 10.100], loss: 0.103165, mae: 0.319444, mean_q: 4.143945
 87179/100000: episode: 1623, duration: 0.459s, episode steps: 90, steps per second: 196, episode reward: 166.926, mean reward: 1.855 [1.462, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.378, 10.350], loss: 0.121404, mae: 0.331096, mean_q: 4.126656
 87199/100000: episode: 1624, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 60.864, mean reward: 3.043 [2.566, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.322, 10.100], loss: 0.113715, mae: 0.317073, mean_q: 4.157355
 87223/100000: episode: 1625, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 71.906, mean reward: 2.996 [2.332, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.242, 10.100], loss: 0.133112, mae: 0.342212, mean_q: 4.123420
 87313/100000: episode: 1626, duration: 0.465s, episode steps: 90, steps per second: 193, episode reward: 180.673, mean reward: 2.007 [1.454, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.578, 10.100], loss: 0.114250, mae: 0.324004, mean_q: 4.149891
 87329/100000: episode: 1627, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 71.265, mean reward: 4.454 [2.584, 7.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.354, 10.100], loss: 0.129994, mae: 0.341694, mean_q: 4.165230
 87380/100000: episode: 1628, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 128.668, mean reward: 2.523 [1.775, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.114, 10.267], loss: 0.124643, mae: 0.341860, mean_q: 4.187195
 87398/100000: episode: 1629, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 49.610, mean reward: 2.756 [1.910, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.809, 10.100], loss: 0.107272, mae: 0.316141, mean_q: 4.180360
 87488/100000: episode: 1630, duration: 0.500s, episode steps: 90, steps per second: 180, episode reward: 175.247, mean reward: 1.947 [1.458, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-1.022, 10.140], loss: 0.145225, mae: 0.354189, mean_q: 4.193247
 87578/100000: episode: 1631, duration: 0.467s, episode steps: 90, steps per second: 193, episode reward: 189.243, mean reward: 2.103 [1.469, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.474, 10.100], loss: 0.123887, mae: 0.340065, mean_q: 4.204997
 87596/100000: episode: 1632, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 48.440, mean reward: 2.691 [1.897, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.177, 10.100], loss: 0.237774, mae: 0.383135, mean_q: 4.265033
 87626/100000: episode: 1633, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 58.604, mean reward: 1.953 [1.553, 2.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.319, 10.152], loss: 0.138060, mae: 0.339622, mean_q: 4.207655
 87650/100000: episode: 1634, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 58.888, mean reward: 2.454 [1.771, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.398, 10.100], loss: 0.109895, mae: 0.314885, mean_q: 4.185523
 87666/100000: episode: 1635, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 51.608, mean reward: 3.226 [2.428, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.199, 10.100], loss: 0.097274, mae: 0.311539, mean_q: 4.178595
 87681/100000: episode: 1636, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 47.741, mean reward: 3.183 [2.705, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.435], loss: 0.107987, mae: 0.320418, mean_q: 4.164359
 87697/100000: episode: 1637, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 39.232, mean reward: 2.452 [1.968, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.385, 10.100], loss: 0.158948, mae: 0.396406, mean_q: 4.289232
 87713/100000: episode: 1638, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 71.957, mean reward: 4.497 [2.469, 8.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.683, 10.100], loss: 0.117164, mae: 0.339704, mean_q: 4.149565
 87801/100000: episode: 1639, duration: 0.463s, episode steps: 88, steps per second: 190, episode reward: 166.574, mean reward: 1.893 [1.463, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.574 [-0.713, 10.227], loss: 0.146045, mae: 0.345242, mean_q: 4.212421
 87825/100000: episode: 1640, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 61.954, mean reward: 2.581 [2.010, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.206, 10.100], loss: 0.147205, mae: 0.366721, mean_q: 4.254774
 87849/100000: episode: 1641, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 57.258, mean reward: 2.386 [2.021, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.150, 10.100], loss: 0.125514, mae: 0.344678, mean_q: 4.301366
 87867/100000: episode: 1642, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 52.652, mean reward: 2.925 [2.214, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.203, 10.100], loss: 0.171150, mae: 0.354410, mean_q: 4.243095
 87883/100000: episode: 1643, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 50.886, mean reward: 3.180 [2.285, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.358, 10.100], loss: 0.123366, mae: 0.333596, mean_q: 4.204443
 87934/100000: episode: 1644, duration: 0.245s, episode steps: 51, steps per second: 208, episode reward: 136.810, mean reward: 2.683 [1.622, 5.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.988, 10.271], loss: 0.146080, mae: 0.360791, mean_q: 4.279319
 87950/100000: episode: 1645, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 52.465, mean reward: 3.279 [2.511, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.343, 10.100], loss: 0.164495, mae: 0.379394, mean_q: 4.327944
 87968/100000: episode: 1646, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 41.794, mean reward: 2.322 [1.682, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.295, 10.100], loss: 0.144105, mae: 0.363117, mean_q: 4.244759
 87984/100000: episode: 1647, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 75.642, mean reward: 4.728 [2.794, 8.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.443, 10.100], loss: 0.130049, mae: 0.359254, mean_q: 4.316555
 88008/100000: episode: 1648, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 51.931, mean reward: 2.164 [1.831, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.222, 10.100], loss: 0.273243, mae: 0.462024, mean_q: 4.383699
[Info] 2-TH LEVEL FOUND: 7.0410075187683105, Considering 10/90 traces
 88098/100000: episode: 1649, duration: 4.694s, episode steps: 90, steps per second: 19, episode reward: 185.057, mean reward: 2.056 [1.566, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-1.053, 10.100], loss: 0.150238, mae: 0.361708, mean_q: 4.268264
 88110/100000: episode: 1650, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 31.972, mean reward: 2.664 [2.356, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.391, 10.100], loss: 0.179503, mae: 0.379739, mean_q: 4.317967
 88122/100000: episode: 1651, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 31.674, mean reward: 2.640 [2.479, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.311, 10.100], loss: 0.177078, mae: 0.364805, mean_q: 4.331903
 88135/100000: episode: 1652, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 41.136, mean reward: 3.164 [2.326, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.075, 10.100], loss: 0.121888, mae: 0.340990, mean_q: 4.201262
 88146/100000: episode: 1653, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 34.418, mean reward: 3.129 [2.247, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.252, 10.100], loss: 0.143102, mae: 0.339585, mean_q: 4.311035
 88159/100000: episode: 1654, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 32.923, mean reward: 2.533 [2.189, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.386, 10.100], loss: 0.135957, mae: 0.379387, mean_q: 4.328818
 88172/100000: episode: 1655, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 50.092, mean reward: 3.853 [2.881, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.391, 10.100], loss: 0.107907, mae: 0.323428, mean_q: 4.223632
 88183/100000: episode: 1656, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 33.641, mean reward: 3.058 [2.290, 5.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.288, 10.100], loss: 0.145673, mae: 0.362693, mean_q: 4.352934
 88195/100000: episode: 1657, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 33.768, mean reward: 2.814 [2.365, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.219, 10.100], loss: 0.183386, mae: 0.387856, mean_q: 4.349415
 88207/100000: episode: 1658, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 40.091, mean reward: 3.341 [2.714, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.044, 10.100], loss: 0.151436, mae: 0.363733, mean_q: 4.340049
 88220/100000: episode: 1659, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 33.912, mean reward: 2.609 [2.174, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.377, 10.100], loss: 0.170042, mae: 0.336106, mean_q: 4.289502
 88232/100000: episode: 1660, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 38.576, mean reward: 3.215 [2.646, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.369, 10.100], loss: 0.195764, mae: 0.376132, mean_q: 4.271049
 88244/100000: episode: 1661, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 39.062, mean reward: 3.255 [2.585, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.267, 10.100], loss: 0.098688, mae: 0.328641, mean_q: 4.331049
 88256/100000: episode: 1662, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 37.446, mean reward: 3.121 [2.183, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.353, 10.100], loss: 0.203789, mae: 0.369642, mean_q: 4.343569
 88298/100000: episode: 1663, duration: 0.218s, episode steps: 42, steps per second: 193, episode reward: 101.117, mean reward: 2.408 [1.462, 8.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.324, 10.209], loss: 0.170842, mae: 0.379578, mean_q: 4.315156
 88310/100000: episode: 1664, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 58.362, mean reward: 4.864 [2.791, 12.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.457, 10.100], loss: 0.166798, mae: 0.354130, mean_q: 4.240988
 88321/100000: episode: 1665, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 45.085, mean reward: 4.099 [2.974, 5.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.300, 10.100], loss: 0.180679, mae: 0.375638, mean_q: 4.306098
 88332/100000: episode: 1666, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 43.518, mean reward: 3.956 [3.295, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.592, 10.100], loss: 0.102185, mae: 0.325013, mean_q: 4.406535
 88344/100000: episode: 1667, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 29.787, mean reward: 2.482 [2.202, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.317, 10.100], loss: 0.207064, mae: 0.407050, mean_q: 4.432510
 88356/100000: episode: 1668, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 56.255, mean reward: 4.688 [2.641, 12.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.521, 10.100], loss: 0.163463, mae: 0.385245, mean_q: 4.444909
 88367/100000: episode: 1669, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 40.536, mean reward: 3.685 [2.551, 5.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.312, 10.100], loss: 0.166060, mae: 0.405263, mean_q: 4.428704
 88379/100000: episode: 1670, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 36.748, mean reward: 3.062 [2.608, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.570, 10.100], loss: 0.229154, mae: 0.427381, mean_q: 4.447209
 88390/100000: episode: 1671, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 50.585, mean reward: 4.599 [3.537, 7.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.438, 10.100], loss: 0.185112, mae: 0.388724, mean_q: 4.460893
 88401/100000: episode: 1672, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 32.623, mean reward: 2.966 [2.366, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.291, 10.100], loss: 0.143954, mae: 0.369059, mean_q: 4.426663
 88412/100000: episode: 1673, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 39.304, mean reward: 3.573 [2.691, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.341, 10.100], loss: 0.345636, mae: 0.461432, mean_q: 4.538500
 88424/100000: episode: 1674, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 35.956, mean reward: 2.996 [2.324, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.416, 10.100], loss: 0.143721, mae: 0.370885, mean_q: 4.372421
 88436/100000: episode: 1675, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 36.754, mean reward: 3.063 [2.153, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.641, 10.100], loss: 0.343632, mae: 0.440206, mean_q: 4.521415
 88448/100000: episode: 1676, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 37.438, mean reward: 3.120 [2.830, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.271, 10.100], loss: 0.205049, mae: 0.385629, mean_q: 4.361749
 88461/100000: episode: 1677, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 38.190, mean reward: 2.938 [2.329, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.430, 10.100], loss: 0.187300, mae: 0.384885, mean_q: 4.471028
 88503/100000: episode: 1678, duration: 0.225s, episode steps: 42, steps per second: 186, episode reward: 139.523, mean reward: 3.322 [1.658, 14.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.603, 10.429], loss: 0.224760, mae: 0.394208, mean_q: 4.472026
 88514/100000: episode: 1679, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 52.031, mean reward: 4.730 [2.994, 8.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.318, 10.100], loss: 0.145616, mae: 0.383440, mean_q: 4.495687
 88526/100000: episode: 1680, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 43.902, mean reward: 3.658 [2.417, 5.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.280, 10.100], loss: 0.176386, mae: 0.400117, mean_q: 4.355061
 88538/100000: episode: 1681, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 47.582, mean reward: 3.965 [3.003, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.468, 10.100], loss: 0.447530, mae: 0.562931, mean_q: 4.686318
 88551/100000: episode: 1682, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 43.162, mean reward: 3.320 [2.908, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.449, 10.100], loss: 0.215646, mae: 0.452475, mean_q: 4.471997
 88564/100000: episode: 1683, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 38.423, mean reward: 2.956 [2.416, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.219, 10.100], loss: 0.232400, mae: 0.403955, mean_q: 4.436691
 88576/100000: episode: 1684, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 39.066, mean reward: 3.255 [2.332, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.440, 10.100], loss: 0.266803, mae: 0.460152, mean_q: 4.613184
 88587/100000: episode: 1685, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 62.502, mean reward: 5.682 [4.255, 10.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.417, 10.100], loss: 0.312433, mae: 0.455309, mean_q: 4.579010
[Info] FALSIFICATION!
 88601/100000: episode: 1686, duration: 0.284s, episode steps: 14, steps per second: 49, episode reward: 1073.968, mean reward: 76.712 [3.036, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.287, 8.599], loss: 0.212196, mae: 0.420773, mean_q: 4.420455
 88612/100000: episode: 1687, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 57.054, mean reward: 5.187 [3.739, 7.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.345, 10.100], loss: 3.370122, mae: 1.841264, mean_q: 6.280419
 88624/100000: episode: 1688, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 38.487, mean reward: 3.207 [2.454, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.523, 10.100], loss: 1.850934, mae: 1.279408, mean_q: 5.290828
 88637/100000: episode: 1689, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 46.133, mean reward: 3.549 [2.514, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.270, 10.100], loss: 0.454622, mae: 0.664435, mean_q: 4.344895
 88649/100000: episode: 1690, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 34.578, mean reward: 2.882 [2.177, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.349, 10.100], loss: 0.296210, mae: 0.515557, mean_q: 4.420928
 88691/100000: episode: 1691, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 194.591, mean reward: 4.633 [2.726, 8.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-1.120, 10.447], loss: 365.400543, mae: 1.622251, mean_q: 5.001563
 88702/100000: episode: 1692, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 45.802, mean reward: 4.164 [3.006, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.468, 10.100], loss: 0.606458, mae: 0.800946, mean_q: 4.739661
 88714/100000: episode: 1693, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 36.817, mean reward: 3.068 [2.374, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.283, 10.100], loss: 1274.775757, mae: 3.663268, mean_q: 5.556406
 88726/100000: episode: 1694, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 32.722, mean reward: 2.727 [2.052, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.400, 10.100], loss: 3.012591, mae: 1.738329, mean_q: 6.351805
 88738/100000: episode: 1695, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 34.270, mean reward: 2.856 [2.218, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.413, 10.100], loss: 0.453809, mae: 0.663074, mean_q: 4.837071
 88749/100000: episode: 1696, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 44.604, mean reward: 4.055 [2.869, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.398, 10.100], loss: 0.784203, mae: 0.647731, mean_q: 4.861610
 88761/100000: episode: 1697, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 44.095, mean reward: 3.675 [2.518, 10.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.326, 10.100], loss: 1270.630981, mae: 3.135167, mean_q: 4.861920
 88774/100000: episode: 1698, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 52.150, mean reward: 4.012 [3.224, 5.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.300, 10.100], loss: 2.903063, mae: 1.636293, mean_q: 6.332629
 88787/100000: episode: 1699, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 38.218, mean reward: 2.940 [2.397, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.398, 10.100], loss: 1.504347, mae: 1.086577, mean_q: 5.611159
 88799/100000: episode: 1700, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 65.238, mean reward: 5.436 [2.221, 27.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.529, 10.100], loss: 0.689422, mae: 0.772628, mean_q: 5.222677
 88810/100000: episode: 1701, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 29.147, mean reward: 2.650 [2.345, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.351, 10.100], loss: 0.426779, mae: 0.617453, mean_q: 4.990923
 88822/100000: episode: 1702, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 38.134, mean reward: 3.178 [2.564, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.342, 10.100], loss: 0.414762, mae: 0.562929, mean_q: 5.159415
 88834/100000: episode: 1703, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 38.666, mean reward: 3.222 [2.652, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.760, 10.100], loss: 0.325240, mae: 0.522471, mean_q: 5.113056
 88846/100000: episode: 1704, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 35.227, mean reward: 2.936 [2.399, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.202, 10.100], loss: 0.265884, mae: 0.483025, mean_q: 4.965942
 88859/100000: episode: 1705, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 42.022, mean reward: 3.232 [2.754, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.797, 10.100], loss: 0.405053, mae: 0.541259, mean_q: 5.011101
 88872/100000: episode: 1706, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 34.762, mean reward: 2.674 [1.843, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.177, 10.100], loss: 0.340144, mae: 0.520670, mean_q: 4.957436
 88885/100000: episode: 1707, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 37.508, mean reward: 2.885 [2.198, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.319, 10.100], loss: 0.287194, mae: 0.523001, mean_q: 5.038722
 88897/100000: episode: 1708, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 33.775, mean reward: 2.815 [2.416, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.192, 10.100], loss: 0.277370, mae: 0.486172, mean_q: 4.784824
 88910/100000: episode: 1709, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 49.690, mean reward: 3.822 [2.649, 6.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.290, 10.100], loss: 0.538080, mae: 0.546611, mean_q: 5.120237
 88923/100000: episode: 1710, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 36.403, mean reward: 2.800 [2.276, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.249, 10.100], loss: 0.246487, mae: 0.472987, mean_q: 4.978583
 88934/100000: episode: 1711, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 49.314, mean reward: 4.483 [3.610, 6.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.422, 10.100], loss: 0.683549, mae: 0.589344, mean_q: 4.901291
 88946/100000: episode: 1712, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 43.723, mean reward: 3.644 [2.392, 6.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.440, 10.100], loss: 0.546271, mae: 0.555650, mean_q: 5.031982
 88958/100000: episode: 1713, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 38.438, mean reward: 3.203 [2.743, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.299, 10.100], loss: 0.302212, mae: 0.532675, mean_q: 4.897698
 88969/100000: episode: 1714, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 69.220, mean reward: 6.293 [4.109, 10.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.311, 10.100], loss: 0.353633, mae: 0.503083, mean_q: 4.833946
 88982/100000: episode: 1715, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 43.114, mean reward: 3.316 [2.448, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.309, 10.100], loss: 0.521402, mae: 0.531840, mean_q: 4.873471
 89024/100000: episode: 1716, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 108.764, mean reward: 2.590 [1.689, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.238, 10.344], loss: 0.262371, mae: 0.472510, mean_q: 4.877439
 89035/100000: episode: 1717, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 37.983, mean reward: 3.453 [2.909, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.999, 10.100], loss: 0.250753, mae: 0.462250, mean_q: 4.928212
 89077/100000: episode: 1718, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 145.519, mean reward: 3.465 [2.252, 6.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.316, 10.450], loss: 722.804016, mae: 2.743653, mean_q: 5.944481
 89089/100000: episode: 1719, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 31.943, mean reward: 2.662 [2.323, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.446, 10.100], loss: 1.593013, mae: 1.162965, mean_q: 5.585390
 89131/100000: episode: 1720, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 122.719, mean reward: 2.922 [1.968, 6.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.669, 10.333], loss: 0.589117, mae: 0.626230, mean_q: 5.135249
 89143/100000: episode: 1721, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 44.172, mean reward: 3.681 [2.725, 5.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.362, 10.100], loss: 1267.080688, mae: 3.671439, mean_q: 5.955759
 89155/100000: episode: 1722, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 34.412, mean reward: 2.868 [2.414, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.576, 10.100], loss: 2.655924, mae: 1.444759, mean_q: 6.214191
 89168/100000: episode: 1723, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 82.525, mean reward: 6.348 [3.425, 18.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.121, 10.100], loss: 0.829691, mae: 0.807506, mean_q: 5.448378
 89179/100000: episode: 1724, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 41.371, mean reward: 3.761 [2.445, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.403, 10.100], loss: 0.407444, mae: 0.590960, mean_q: 5.278648
 89190/100000: episode: 1725, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 36.905, mean reward: 3.355 [2.740, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.463, 10.100], loss: 1374.745605, mae: 3.528528, mean_q: 5.331686
 89202/100000: episode: 1726, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 41.783, mean reward: 3.482 [2.728, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.179, 10.100], loss: 3.016155, mae: 1.520891, mean_q: 6.169409
 89213/100000: episode: 1727, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 30.774, mean reward: 2.798 [2.378, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.384, 10.100], loss: 1.674338, mae: 1.100267, mean_q: 5.884697
 89226/100000: episode: 1728, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 54.312, mean reward: 4.178 [3.636, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.331, 10.100], loss: 1.499876, mae: 0.845597, mean_q: 5.545664
 89268/100000: episode: 1729, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 110.318, mean reward: 2.627 [1.778, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.289, 10.283], loss: 360.571655, mae: 1.828953, mean_q: 5.950087
 89279/100000: episode: 1730, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 39.347, mean reward: 3.577 [2.749, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.283, 10.100], loss: 0.754060, mae: 0.729406, mean_q: 5.412203
 89290/100000: episode: 1731, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 31.279, mean reward: 2.844 [2.532, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.381, 10.100], loss: 2.176095, mae: 0.838699, mean_q: 5.452948
 89303/100000: episode: 1732, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 55.232, mean reward: 4.249 [2.282, 7.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.401, 10.100], loss: 0.465336, mae: 0.636591, mean_q: 5.352656
 89345/100000: episode: 1733, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 120.105, mean reward: 2.860 [1.986, 6.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.854, 10.366], loss: 0.649790, mae: 0.639144, mean_q: 5.213785
 89357/100000: episode: 1734, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 43.674, mean reward: 3.640 [2.809, 5.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.393, 10.100], loss: 1.109529, mae: 0.611053, mean_q: 5.277710
 89368/100000: episode: 1735, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 79.212, mean reward: 7.201 [2.788, 32.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.456, 10.100], loss: 0.417761, mae: 0.624404, mean_q: 5.314914
 89379/100000: episode: 1736, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 40.869, mean reward: 3.715 [3.188, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.331, 10.100], loss: 1.115474, mae: 0.592239, mean_q: 5.114930
[Info] FALSIFICATION!
 89385/100000: episode: 1737, duration: 0.190s, episode steps: 6, steps per second: 32, episode reward: 1025.455, mean reward: 170.909 [2.718, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.269, 9.988], loss: 0.322800, mae: 0.530229, mean_q: 5.450997
 89397/100000: episode: 1738, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 35.582, mean reward: 2.965 [2.520, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.312, 10.100], loss: 0.385292, mae: 0.562361, mean_q: 5.272545
[Info] Complete ISplit Iteration
[Info] Levels: [5.147015, 7.0410075, 8.569107]
[Info] Cond. Prob: [0.1, 0.1, 0.16]
[Info] Error Prob: 0.0016000000000000003

 89439/100000: episode: 1739, duration: 4.477s, episode steps: 42, steps per second: 9, episode reward: 121.489, mean reward: 2.893 [1.757, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.435, 10.108], loss: 0.902062, mae: 0.611695, mean_q: 5.232116
 89539/100000: episode: 1740, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 199.319, mean reward: 1.993 [1.434, 5.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.778, 10.265], loss: 152.595001, mae: 1.063099, mean_q: 5.421240
 89639/100000: episode: 1741, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 196.258, mean reward: 1.963 [1.526, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.727, 10.098], loss: 0.629389, mae: 0.627318, mean_q: 5.245521
 89739/100000: episode: 1742, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.688, mean reward: 1.827 [1.432, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.202], loss: 307.221100, mae: 1.656655, mean_q: 5.618238
 89839/100000: episode: 1743, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 201.084, mean reward: 2.011 [1.469, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.822, 10.098], loss: 0.705040, mae: 0.670270, mean_q: 5.265819
 89939/100000: episode: 1744, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.806, mean reward: 1.968 [1.458, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.240, 10.098], loss: 0.550487, mae: 0.566815, mean_q: 5.110038
 90039/100000: episode: 1745, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.011, mean reward: 1.890 [1.483, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.188], loss: 154.584167, mae: 1.152365, mean_q: 5.441211
 90139/100000: episode: 1746, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.174, mean reward: 1.872 [1.447, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.064, 10.146], loss: 0.614318, mae: 0.565245, mean_q: 5.092322
 90239/100000: episode: 1747, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.587, mean reward: 1.866 [1.506, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.028, 10.098], loss: 307.729187, mae: 1.713848, mean_q: 5.671531
 90339/100000: episode: 1748, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.366, mean reward: 2.034 [1.463, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.297, 10.098], loss: 0.535842, mae: 0.650939, mean_q: 5.087774
 90439/100000: episode: 1749, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 199.316, mean reward: 1.993 [1.445, 4.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.092, 10.276], loss: 0.680018, mae: 0.612804, mean_q: 5.012121
 90539/100000: episode: 1750, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 223.054, mean reward: 2.231 [1.462, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.394, 10.470], loss: 153.063492, mae: 1.073641, mean_q: 5.202247
 90639/100000: episode: 1751, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.358, mean reward: 1.944 [1.481, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.346], loss: 154.714462, mae: 1.110927, mean_q: 5.231598
 90739/100000: episode: 1752, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.491, mean reward: 2.015 [1.498, 5.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.360, 10.098], loss: 0.806288, mae: 0.728273, mean_q: 5.166859
 90839/100000: episode: 1753, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 319.993, mean reward: 3.200 [1.471, 83.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.118, 10.098], loss: 154.484467, mae: 1.122041, mean_q: 5.274230
 90939/100000: episode: 1754, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.271, mean reward: 1.953 [1.505, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.631, 10.098], loss: 457.321716, mae: 2.240079, mean_q: 5.825260
 91039/100000: episode: 1755, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 204.656, mean reward: 2.047 [1.486, 4.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.432, 10.150], loss: 308.175323, mae: 1.902750, mean_q: 6.030906
 91139/100000: episode: 1756, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 185.106, mean reward: 1.851 [1.470, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.306], loss: 306.312012, mae: 1.475341, mean_q: 5.506310
 91239/100000: episode: 1757, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 211.857, mean reward: 2.119 [1.501, 5.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.368], loss: 306.446259, mae: 2.048938, mean_q: 6.283536
 91339/100000: episode: 1758, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 201.135, mean reward: 2.011 [1.492, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.234, 10.098], loss: 308.693878, mae: 1.955222, mean_q: 6.142019
 91439/100000: episode: 1759, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 185.597, mean reward: 1.856 [1.456, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.324, 10.128], loss: 0.636354, mae: 0.679726, mean_q: 5.225220
 91539/100000: episode: 1760, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 190.618, mean reward: 1.906 [1.457, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.962, 10.098], loss: 154.188812, mae: 1.102982, mean_q: 5.222698
 91639/100000: episode: 1761, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 180.766, mean reward: 1.808 [1.459, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.611, 10.126], loss: 611.740967, mae: 2.790064, mean_q: 6.264595
 91739/100000: episode: 1762, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 209.535, mean reward: 2.095 [1.470, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.638, 10.332], loss: 760.319214, mae: 3.194374, mean_q: 6.837388
 91839/100000: episode: 1763, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 208.794, mean reward: 2.088 [1.437, 4.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.773, 10.394], loss: 308.156036, mae: 1.895407, mean_q: 6.401291
 91939/100000: episode: 1764, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 201.542, mean reward: 2.015 [1.450, 4.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.961, 10.098], loss: 152.226639, mae: 1.312635, mean_q: 5.823330
 92039/100000: episode: 1765, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 198.995, mean reward: 1.990 [1.459, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.533, 10.270], loss: 151.205261, mae: 1.367223, mean_q: 5.688173
 92139/100000: episode: 1766, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.696, mean reward: 1.997 [1.483, 6.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.290, 10.098], loss: 153.437485, mae: 0.983330, mean_q: 5.052333
 92239/100000: episode: 1767, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.878, mean reward: 1.879 [1.443, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.915, 10.125], loss: 302.954468, mae: 1.759544, mean_q: 5.793282
 92339/100000: episode: 1768, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 205.509, mean reward: 2.055 [1.460, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.205, 10.098], loss: 305.610138, mae: 1.793701, mean_q: 5.838204
 92439/100000: episode: 1769, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.357, mean reward: 1.864 [1.456, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.797, 10.098], loss: 151.893890, mae: 1.343708, mean_q: 5.608459
 92539/100000: episode: 1770, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 178.935, mean reward: 1.789 [1.463, 2.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.503, 10.228], loss: 154.226639, mae: 1.251375, mean_q: 5.562314
 92639/100000: episode: 1771, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 184.358, mean reward: 1.844 [1.448, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.581, 10.120], loss: 1.581664, mae: 0.657759, mean_q: 5.019277
 92739/100000: episode: 1772, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.334, mean reward: 1.863 [1.460, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.098], loss: 608.669006, mae: 2.601595, mean_q: 5.804943
 92839/100000: episode: 1773, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.635, mean reward: 1.816 [1.460, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.804, 10.098], loss: 304.076721, mae: 1.747386, mean_q: 5.666545
 92939/100000: episode: 1774, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.922, mean reward: 1.849 [1.476, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.355, 10.098], loss: 1.614002, mae: 0.973600, mean_q: 5.634780
 93039/100000: episode: 1775, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 197.640, mean reward: 1.976 [1.450, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.596, 10.098], loss: 153.892136, mae: 1.103778, mean_q: 5.268824
 93139/100000: episode: 1776, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 201.793, mean reward: 2.018 [1.460, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.235], loss: 153.565338, mae: 1.167401, mean_q: 5.262506
 93239/100000: episode: 1777, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.824, mean reward: 1.898 [1.458, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.198, 10.098], loss: 150.673874, mae: 0.932390, mean_q: 4.838975
 93339/100000: episode: 1778, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 212.227, mean reward: 2.122 [1.476, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.623, 10.098], loss: 154.446594, mae: 1.216105, mean_q: 5.287365
 93439/100000: episode: 1779, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.323, mean reward: 1.873 [1.448, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.992, 10.098], loss: 300.294952, mae: 1.527719, mean_q: 5.151644
 93539/100000: episode: 1780, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.276, mean reward: 1.813 [1.443, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.759, 10.216], loss: 305.232605, mae: 1.721359, mean_q: 5.420298
 93639/100000: episode: 1781, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.259, mean reward: 1.873 [1.446, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.098], loss: 0.731265, mae: 0.608359, mean_q: 4.694108
 93739/100000: episode: 1782, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 188.450, mean reward: 1.885 [1.466, 4.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.045, 10.150], loss: 0.378751, mae: 0.489641, mean_q: 4.429889
 93839/100000: episode: 1783, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.628, mean reward: 1.916 [1.453, 6.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.156], loss: 305.448059, mae: 1.441546, mean_q: 4.854097
 93939/100000: episode: 1784, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.393, mean reward: 1.914 [1.479, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.098], loss: 153.958755, mae: 1.027464, mean_q: 4.634342
 94039/100000: episode: 1785, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.751, mean reward: 1.908 [1.448, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.338, 10.298], loss: 152.286987, mae: 0.817504, mean_q: 4.371529
 94139/100000: episode: 1786, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.940, mean reward: 2.029 [1.510, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.133], loss: 152.803696, mae: 0.894862, mean_q: 4.392634
 94239/100000: episode: 1787, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 180.562, mean reward: 1.806 [1.460, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.098], loss: 0.846123, mae: 0.594860, mean_q: 4.300128
 94339/100000: episode: 1788, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.100, mean reward: 1.941 [1.446, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.501, 10.113], loss: 0.237086, mae: 0.386572, mean_q: 3.989872
 94439/100000: episode: 1789, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 218.239, mean reward: 2.182 [1.597, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.553, 10.098], loss: 2.140022, mae: 0.408778, mean_q: 3.942651
 94539/100000: episode: 1790, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.016, mean reward: 1.910 [1.476, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.450, 10.197], loss: 2.098352, mae: 0.396592, mean_q: 3.981198
 94639/100000: episode: 1791, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 179.001, mean reward: 1.790 [1.442, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.757, 10.166], loss: 0.161185, mae: 0.346902, mean_q: 3.953862
 94739/100000: episode: 1792, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.751, mean reward: 1.828 [1.463, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.729, 10.098], loss: 1.110839, mae: 0.355609, mean_q: 3.920774
 94839/100000: episode: 1793, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 202.181, mean reward: 2.022 [1.436, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.462, 10.098], loss: 0.137094, mae: 0.323474, mean_q: 3.910686
 94939/100000: episode: 1794, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.028, mean reward: 1.900 [1.457, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.325, 10.098], loss: 2.091515, mae: 0.378041, mean_q: 3.935543
 95039/100000: episode: 1795, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 174.383, mean reward: 1.744 [1.451, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.098], loss: 0.125538, mae: 0.329827, mean_q: 3.889548
 95139/100000: episode: 1796, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.412, mean reward: 1.834 [1.451, 2.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.908, 10.098], loss: 3.047752, mae: 0.405743, mean_q: 3.949717
 95239/100000: episode: 1797, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.587, mean reward: 1.916 [1.469, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.502, 10.098], loss: 0.128280, mae: 0.325964, mean_q: 3.901212
 95339/100000: episode: 1798, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 199.300, mean reward: 1.993 [1.449, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.347, 10.105], loss: 1.086997, mae: 0.339325, mean_q: 3.897347
 95439/100000: episode: 1799, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.728, mean reward: 1.887 [1.455, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.098], loss: 0.120019, mae: 0.319519, mean_q: 3.871336
 95539/100000: episode: 1800, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 204.011, mean reward: 2.040 [1.437, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.098], loss: 1.088862, mae: 0.345969, mean_q: 3.875770
 95639/100000: episode: 1801, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.015, mean reward: 1.940 [1.479, 5.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.112, 10.143], loss: 1.088709, mae: 0.340489, mean_q: 3.871184
 95739/100000: episode: 1802, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 209.709, mean reward: 2.097 [1.446, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.666, 10.439], loss: 0.106082, mae: 0.314953, mean_q: 3.878089
 95839/100000: episode: 1803, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 208.132, mean reward: 2.081 [1.449, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.423, 10.098], loss: 1.079652, mae: 0.335954, mean_q: 3.892380
 95939/100000: episode: 1804, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 186.774, mean reward: 1.868 [1.462, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.098], loss: 0.094468, mae: 0.299198, mean_q: 3.846565
 96039/100000: episode: 1805, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.134, mean reward: 1.891 [1.465, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.691, 10.098], loss: 0.083028, mae: 0.290962, mean_q: 3.825722
 96139/100000: episode: 1806, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 197.872, mean reward: 1.979 [1.486, 7.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.227], loss: 0.091680, mae: 0.296655, mean_q: 3.823416
 96239/100000: episode: 1807, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.849, mean reward: 1.938 [1.445, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.670, 10.358], loss: 0.105229, mae: 0.305420, mean_q: 3.841272
 96339/100000: episode: 1808, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 178.050, mean reward: 1.780 [1.444, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.178, 10.151], loss: 0.095673, mae: 0.297999, mean_q: 3.814065
 96439/100000: episode: 1809, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.990, mean reward: 1.840 [1.441, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.475, 10.214], loss: 0.095936, mae: 0.303940, mean_q: 3.825187
 96539/100000: episode: 1810, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.488, mean reward: 1.965 [1.501, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.409, 10.098], loss: 0.090656, mae: 0.295502, mean_q: 3.832010
 96639/100000: episode: 1811, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.068, mean reward: 1.871 [1.527, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.057, 10.242], loss: 0.083451, mae: 0.288177, mean_q: 3.824738
 96739/100000: episode: 1812, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.012, mean reward: 1.850 [1.465, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.491, 10.119], loss: 0.087686, mae: 0.294197, mean_q: 3.819103
 96839/100000: episode: 1813, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.437, mean reward: 1.904 [1.497, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.477, 10.098], loss: 0.085303, mae: 0.281258, mean_q: 3.821333
 96939/100000: episode: 1814, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 216.193, mean reward: 2.162 [1.440, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.618, 10.316], loss: 0.073312, mae: 0.272173, mean_q: 3.800730
 97039/100000: episode: 1815, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.452, mean reward: 1.815 [1.457, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.842, 10.098], loss: 0.079195, mae: 0.280451, mean_q: 3.824415
 97139/100000: episode: 1816, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 216.191, mean reward: 2.162 [1.461, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.118, 10.195], loss: 0.091282, mae: 0.281572, mean_q: 3.824502
 97239/100000: episode: 1817, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.267, mean reward: 1.973 [1.486, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.782, 10.105], loss: 0.078379, mae: 0.282912, mean_q: 3.796237
 97339/100000: episode: 1818, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.484, mean reward: 1.995 [1.452, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.562, 10.098], loss: 0.087056, mae: 0.280382, mean_q: 3.803186
 97439/100000: episode: 1819, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 200.165, mean reward: 2.002 [1.524, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.498, 10.382], loss: 0.088256, mae: 0.284903, mean_q: 3.828271
 97539/100000: episode: 1820, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 190.030, mean reward: 1.900 [1.484, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.066, 10.251], loss: 0.085176, mae: 0.287743, mean_q: 3.822658
 97639/100000: episode: 1821, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 255.234, mean reward: 2.552 [1.490, 5.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.145, 10.360], loss: 0.089883, mae: 0.289456, mean_q: 3.826833
 97739/100000: episode: 1822, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.414, mean reward: 2.004 [1.437, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.534, 10.340], loss: 0.078220, mae: 0.275829, mean_q: 3.827969
 97839/100000: episode: 1823, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.002, mean reward: 2.020 [1.484, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.423, 10.175], loss: 0.095236, mae: 0.306345, mean_q: 3.855999
 97939/100000: episode: 1824, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.096, mean reward: 1.931 [1.450, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.181], loss: 0.082420, mae: 0.289192, mean_q: 3.847628
 98039/100000: episode: 1825, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 208.251, mean reward: 2.083 [1.471, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.897, 10.294], loss: 0.081963, mae: 0.292849, mean_q: 3.861746
 98139/100000: episode: 1826, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.553, mean reward: 1.846 [1.468, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.490, 10.098], loss: 0.096355, mae: 0.298791, mean_q: 3.882625
 98239/100000: episode: 1827, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 187.168, mean reward: 1.872 [1.445, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.889, 10.134], loss: 0.098706, mae: 0.301122, mean_q: 3.866114
 98339/100000: episode: 1828, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.923, mean reward: 1.919 [1.463, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.098], loss: 0.076596, mae: 0.280669, mean_q: 3.860588
 98439/100000: episode: 1829, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 179.627, mean reward: 1.796 [1.455, 2.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.485, 10.100], loss: 0.081617, mae: 0.283629, mean_q: 3.850201
 98539/100000: episode: 1830, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 200.095, mean reward: 2.001 [1.455, 7.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.196, 10.098], loss: 0.088819, mae: 0.290794, mean_q: 3.861870
 98639/100000: episode: 1831, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 202.664, mean reward: 2.027 [1.472, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.451, 10.098], loss: 0.083844, mae: 0.289407, mean_q: 3.861378
 98739/100000: episode: 1832, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.054, mean reward: 2.001 [1.505, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.271], loss: 0.085839, mae: 0.287891, mean_q: 3.858990
 98839/100000: episode: 1833, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.451, mean reward: 1.825 [1.459, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.585, 10.216], loss: 0.076946, mae: 0.278867, mean_q: 3.868385
 98939/100000: episode: 1834, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.193, mean reward: 1.952 [1.456, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.358, 10.130], loss: 0.092198, mae: 0.291175, mean_q: 3.867047
 99039/100000: episode: 1835, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 198.232, mean reward: 1.982 [1.467, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.155], loss: 0.099599, mae: 0.297811, mean_q: 3.884219
 99139/100000: episode: 1836, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 181.533, mean reward: 1.815 [1.459, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.548, 10.098], loss: 0.091369, mae: 0.293937, mean_q: 3.877363
 99239/100000: episode: 1837, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 184.128, mean reward: 1.841 [1.454, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.316, 10.098], loss: 0.092176, mae: 0.292891, mean_q: 3.867962
 99339/100000: episode: 1838, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.066, mean reward: 2.111 [1.483, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.661, 10.098], loss: 0.071836, mae: 0.272698, mean_q: 3.849745
[Info] 1-TH LEVEL FOUND: 5.509031772613525, Considering 10/90 traces
 99439/100000: episode: 1839, duration: 4.641s, episode steps: 100, steps per second: 22, episode reward: 198.873, mean reward: 1.989 [1.457, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.163, 10.355], loss: 0.089476, mae: 0.287218, mean_q: 3.863101
 99442/100000: episode: 1840, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 8.369, mean reward: 2.790 [2.438, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.414, 10.100], loss: 0.038299, mae: 0.218781, mean_q: 3.886036
 99456/100000: episode: 1841, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 37.655, mean reward: 2.690 [2.135, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.435], loss: 0.085469, mae: 0.284405, mean_q: 3.814485
 99475/100000: episode: 1842, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 54.972, mean reward: 2.893 [1.797, 6.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.415, 10.100], loss: 0.093259, mae: 0.305850, mean_q: 3.863016
 99478/100000: episode: 1843, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 7.844, mean reward: 2.615 [2.383, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.420, 10.100], loss: 0.060021, mae: 0.247093, mean_q: 3.822111
 99519/100000: episode: 1844, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 117.188, mean reward: 2.858 [2.033, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.045, 10.410], loss: 0.089319, mae: 0.290654, mean_q: 3.868944
 99531/100000: episode: 1845, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 26.566, mean reward: 2.214 [1.928, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.362], loss: 0.080648, mae: 0.289658, mean_q: 3.904405
 99572/100000: episode: 1846, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 133.603, mean reward: 3.259 [2.224, 6.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.307, 10.526], loss: 0.091509, mae: 0.292651, mean_q: 3.863311
 99585/100000: episode: 1847, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 30.748, mean reward: 2.365 [1.985, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.622, 10.321], loss: 0.102084, mae: 0.320448, mean_q: 3.941179
 99599/100000: episode: 1848, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 29.178, mean reward: 2.084 [1.824, 2.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.302], loss: 0.105960, mae: 0.310626, mean_q: 3.893891
 99618/100000: episode: 1849, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 61.290, mean reward: 3.226 [2.215, 6.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.390, 10.100], loss: 0.087536, mae: 0.305143, mean_q: 3.923234
 99631/100000: episode: 1850, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 37.174, mean reward: 2.860 [2.129, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.356], loss: 0.094015, mae: 0.291999, mean_q: 3.881428
 99643/100000: episode: 1851, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 37.366, mean reward: 3.114 [2.371, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.439], loss: 0.127004, mae: 0.341951, mean_q: 3.972975
 99684/100000: episode: 1852, duration: 0.221s, episode steps: 41, steps per second: 185, episode reward: 112.306, mean reward: 2.739 [2.074, 4.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.135, 10.352], loss: 0.093669, mae: 0.296355, mean_q: 3.925438
 99698/100000: episode: 1853, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 35.046, mean reward: 2.503 [2.315, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.430], loss: 0.095257, mae: 0.297573, mean_q: 3.943362
 99709/100000: episode: 1854, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 30.111, mean reward: 2.737 [2.290, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.386], loss: 0.104473, mae: 0.322124, mean_q: 3.945862
 99750/100000: episode: 1855, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 81.403, mean reward: 1.985 [1.459, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.625, 10.119], loss: 0.104822, mae: 0.312784, mean_q: 3.956757
 99769/100000: episode: 1856, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 92.085, mean reward: 4.847 [2.411, 8.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.308, 10.100], loss: 0.145361, mae: 0.322000, mean_q: 3.951980
 99782/100000: episode: 1857, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 34.921, mean reward: 2.686 [2.324, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.056, 10.388], loss: 0.136954, mae: 0.318175, mean_q: 3.926341
 99794/100000: episode: 1858, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 32.423, mean reward: 2.702 [2.193, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.599, 10.370], loss: 0.093667, mae: 0.303151, mean_q: 3.935240
 99807/100000: episode: 1859, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 34.610, mean reward: 2.662 [2.064, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.475], loss: 0.181767, mae: 0.320423, mean_q: 3.905331
 99819/100000: episode: 1860, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 27.924, mean reward: 2.327 [1.752, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.258], loss: 0.083608, mae: 0.299383, mean_q: 3.991054
 99838/100000: episode: 1861, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 66.844, mean reward: 3.518 [2.168, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.348, 10.100], loss: 0.155898, mae: 0.342114, mean_q: 3.987803
 99857/100000: episode: 1862, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 48.888, mean reward: 2.573 [2.081, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.575, 10.100], loss: 0.152036, mae: 0.351678, mean_q: 3.981769
 99869/100000: episode: 1863, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 24.770, mean reward: 2.064 [1.756, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.270], loss: 0.111072, mae: 0.324760, mean_q: 4.017972
 99882/100000: episode: 1864, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 38.202, mean reward: 2.939 [2.237, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.559, 10.323], loss: 0.111778, mae: 0.310558, mean_q: 3.915531
 99901/100000: episode: 1865, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 74.904, mean reward: 3.942 [2.408, 6.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.561, 10.100], loss: 0.106924, mae: 0.310893, mean_q: 4.043124
 99920/100000: episode: 1866, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 62.723, mean reward: 3.301 [2.440, 5.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.940, 10.100], loss: 0.129887, mae: 0.318106, mean_q: 3.953689
 99961/100000: episode: 1867, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 84.322, mean reward: 2.057 [1.521, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.245, 10.164], loss: 0.129286, mae: 0.332854, mean_q: 3.996970
 99964/100000: episode: 1868, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 8.202, mean reward: 2.734 [2.257, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.464, 10.100], loss: 0.068863, mae: 0.261762, mean_q: 3.908910
 99967/100000: episode: 1869, duration: 0.023s, episode steps: 3, steps per second: 131, episode reward: 8.152, mean reward: 2.717 [2.501, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.448, 10.100], loss: 0.146873, mae: 0.345272, mean_q: 4.049108
 99980/100000: episode: 1870, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 28.000, mean reward: 2.154 [1.479, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.168, 10.194], loss: 0.134786, mae: 0.344368, mean_q: 3.985780
 99999/100000: episode: 1871, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 51.368, mean reward: 2.704 [2.359, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.169, 10.100], loss: 0.126349, mae: 0.345378, mean_q: 4.037475
done, took 627.700 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
