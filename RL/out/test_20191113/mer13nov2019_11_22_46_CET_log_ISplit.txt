Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.163s, episode steps: 100, steps per second: 615, episode reward: 200.112, mean reward: 2.001 [1.453, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.281], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 195.451, mean reward: 1.955 [1.464, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.728, 10.279], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 195.534, mean reward: 1.955 [1.496, 6.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.592, 10.248], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.070s, episode steps: 100, steps per second: 1436, episode reward: 193.686, mean reward: 1.937 [1.453, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.826, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.065s, episode steps: 100, steps per second: 1541, episode reward: 1271.858, mean reward: 12.719 [1.475, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.057, 10.651], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 203.501, mean reward: 2.035 [1.459, 6.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.105], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.072s, episode steps: 100, steps per second: 1388, episode reward: 194.512, mean reward: 1.945 [1.493, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.499, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.072s, episode steps: 100, steps per second: 1383, episode reward: 189.899, mean reward: 1.899 [1.495, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.405, 10.205], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 186.729, mean reward: 1.867 [1.466, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.815, 10.320], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 190.243, mean reward: 1.902 [1.449, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.462, 10.384], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 194.267, mean reward: 1.943 [1.455, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.824, 10.164], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 201.055, mean reward: 2.011 [1.516, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.704, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 221.228, mean reward: 2.212 [1.443, 5.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 201.228, mean reward: 2.012 [1.460, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.999, 10.193], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 194.027, mean reward: 1.940 [1.453, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.144, 10.209], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.070s, episode steps: 100, steps per second: 1437, episode reward: 200.509, mean reward: 2.005 [1.462, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.957, 10.202], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 186.546, mean reward: 1.865 [1.485, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.859, 10.224], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 185.768, mean reward: 1.858 [1.451, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.070s, episode steps: 100, steps per second: 1428, episode reward: 186.615, mean reward: 1.866 [1.448, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.799, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.083s, episode steps: 100, steps per second: 1204, episode reward: 181.446, mean reward: 1.814 [1.440, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.241, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 184.939, mean reward: 1.849 [1.442, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.258], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 199.244, mean reward: 1.992 [1.529, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.023, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 187.833, mean reward: 1.878 [1.448, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.198], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.070s, episode steps: 100, steps per second: 1424, episode reward: 218.551, mean reward: 2.186 [1.587, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.247, 10.206], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 180.526, mean reward: 1.805 [1.461, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.191, 10.136], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 184.648, mean reward: 1.846 [1.472, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.651, 10.206], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 190.941, mean reward: 1.909 [1.437, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.106, 10.162], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 180.207, mean reward: 1.802 [1.470, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.182, 10.109], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 177.870, mean reward: 1.779 [1.437, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.432, 10.100], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.074s, episode steps: 100, steps per second: 1360, episode reward: 181.756, mean reward: 1.818 [1.479, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.284, 10.116], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 185.199, mean reward: 1.852 [1.442, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.301], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 211.651, mean reward: 2.117 [1.491, 5.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.005, 10.302], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 210.965, mean reward: 2.110 [1.452, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.638, 10.133], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.068s, episode steps: 100, steps per second: 1468, episode reward: 183.616, mean reward: 1.836 [1.475, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.741, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 185.546, mean reward: 1.855 [1.469, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.619, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.070s, episode steps: 100, steps per second: 1424, episode reward: 199.778, mean reward: 1.998 [1.511, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.336, 10.148], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 189.088, mean reward: 1.891 [1.448, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.288, 10.321], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 174.604, mean reward: 1.746 [1.441, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.359, 10.292], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.065s, episode steps: 100, steps per second: 1547, episode reward: 185.033, mean reward: 1.850 [1.446, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.743, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 198.085, mean reward: 1.981 [1.487, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.245, 10.230], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.069s, episode steps: 100, steps per second: 1455, episode reward: 217.390, mean reward: 2.174 [1.501, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.950, 10.142], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.072s, episode steps: 100, steps per second: 1397, episode reward: 215.080, mean reward: 2.151 [1.474, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.654, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 191.874, mean reward: 1.919 [1.438, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.446, 10.226], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 188.480, mean reward: 1.885 [1.447, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 187.137, mean reward: 1.871 [1.452, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.315, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 180.246, mean reward: 1.802 [1.452, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.802, 10.191], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 198.897, mean reward: 1.989 [1.480, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.385, 10.236], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 184.311, mean reward: 1.843 [1.447, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.108], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 200.713, mean reward: 2.007 [1.492, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.572, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.072s, episode steps: 100, steps per second: 1389, episode reward: 194.342, mean reward: 1.943 [1.434, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.489, 10.101], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 205.474, mean reward: 2.055 [1.489, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.842, 10.263], loss: 0.658718, mae: 0.838857, mean_q: 0.701124
  5200/100000: episode: 52, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 211.139, mean reward: 2.111 [1.440, 5.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.149, 10.098], loss: 155.806107, mae: 0.814897, mean_q: 2.222825
  5300/100000: episode: 53, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 188.363, mean reward: 1.884 [1.452, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.618, 10.188], loss: 0.257474, mae: 0.380161, mean_q: 2.770169
  5400/100000: episode: 54, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 195.113, mean reward: 1.951 [1.486, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.692, 10.149], loss: 156.131027, mae: 1.014682, mean_q: 3.588191
  5500/100000: episode: 55, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 188.306, mean reward: 1.883 [1.449, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.971, 10.255], loss: 0.105277, mae: 0.301526, mean_q: 3.443426
  5600/100000: episode: 56, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 205.798, mean reward: 2.058 [1.536, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.875, 10.098], loss: 0.092951, mae: 0.302572, mean_q: 3.588642
  5700/100000: episode: 57, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 205.596, mean reward: 2.056 [1.479, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.657, 10.229], loss: 0.095836, mae: 0.302420, mean_q: 3.678705
  5800/100000: episode: 58, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 182.213, mean reward: 1.822 [1.449, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.041, 10.098], loss: 0.093510, mae: 0.298910, mean_q: 3.742939
  5900/100000: episode: 59, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 180.720, mean reward: 1.807 [1.456, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.685, 10.098], loss: 0.085170, mae: 0.288627, mean_q: 3.772637
  6000/100000: episode: 60, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 215.260, mean reward: 2.153 [1.484, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.669, 10.098], loss: 0.093440, mae: 0.303380, mean_q: 3.821177
  6100/100000: episode: 61, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 184.323, mean reward: 1.843 [1.450, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.310, 10.098], loss: 0.093363, mae: 0.303680, mean_q: 3.821092
  6200/100000: episode: 62, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 205.721, mean reward: 2.057 [1.491, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.276], loss: 0.097854, mae: 0.303067, mean_q: 3.831822
  6300/100000: episode: 63, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.187, mean reward: 1.962 [1.462, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.044, 10.237], loss: 0.092484, mae: 0.299734, mean_q: 3.838328
  6400/100000: episode: 64, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 199.381, mean reward: 1.994 [1.488, 4.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.546, 10.174], loss: 0.084339, mae: 0.297450, mean_q: 3.835680
  6500/100000: episode: 65, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.195, mean reward: 1.962 [1.525, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.430, 10.368], loss: 0.084368, mae: 0.289293, mean_q: 3.828763
  6600/100000: episode: 66, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 194.299, mean reward: 1.943 [1.492, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.422, 10.098], loss: 0.091854, mae: 0.301624, mean_q: 3.822200
  6700/100000: episode: 67, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 195.051, mean reward: 1.951 [1.534, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.512, 10.098], loss: 0.091341, mae: 0.301246, mean_q: 3.816791
  6800/100000: episode: 68, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.360, mean reward: 1.894 [1.447, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.231, 10.098], loss: 0.089739, mae: 0.297724, mean_q: 3.819452
  6900/100000: episode: 69, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 208.985, mean reward: 2.090 [1.571, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.222, 10.271], loss: 0.091428, mae: 0.304998, mean_q: 3.844625
  7000/100000: episode: 70, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 183.805, mean reward: 1.838 [1.436, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.573, 10.173], loss: 0.095562, mae: 0.309258, mean_q: 3.864922
  7100/100000: episode: 71, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 193.924, mean reward: 1.939 [1.451, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.866, 10.098], loss: 0.094485, mae: 0.304371, mean_q: 3.839369
  7200/100000: episode: 72, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.694, mean reward: 1.957 [1.509, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.903, 10.098], loss: 0.085947, mae: 0.293952, mean_q: 3.837534
  7300/100000: episode: 73, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 174.883, mean reward: 1.749 [1.444, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.354, 10.098], loss: 0.082347, mae: 0.283861, mean_q: 3.827981
  7400/100000: episode: 74, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 190.499, mean reward: 1.905 [1.475, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.502, 10.383], loss: 0.084505, mae: 0.287771, mean_q: 3.825790
  7500/100000: episode: 75, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 204.694, mean reward: 2.047 [1.532, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.739, 10.098], loss: 0.088536, mae: 0.291290, mean_q: 3.827111
  7600/100000: episode: 76, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.058, mean reward: 1.981 [1.448, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.122, 10.098], loss: 0.090584, mae: 0.299007, mean_q: 3.852225
  7700/100000: episode: 77, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.256, mean reward: 1.873 [1.453, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.095, 10.098], loss: 0.090874, mae: 0.305019, mean_q: 3.865933
  7800/100000: episode: 78, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.720, mean reward: 1.837 [1.435, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.817, 10.257], loss: 0.089565, mae: 0.299848, mean_q: 3.844201
  7900/100000: episode: 79, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 203.665, mean reward: 2.037 [1.482, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.396], loss: 0.095913, mae: 0.302647, mean_q: 3.842644
  8000/100000: episode: 80, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 183.744, mean reward: 1.837 [1.453, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.530, 10.098], loss: 0.090960, mae: 0.301073, mean_q: 3.849783
  8100/100000: episode: 81, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.023, mean reward: 1.850 [1.466, 4.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.570, 10.098], loss: 0.102734, mae: 0.310916, mean_q: 3.855315
  8200/100000: episode: 82, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 194.728, mean reward: 1.947 [1.448, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.738, 10.098], loss: 0.085824, mae: 0.294146, mean_q: 3.843606
  8300/100000: episode: 83, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 179.621, mean reward: 1.796 [1.457, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.278], loss: 0.079420, mae: 0.286279, mean_q: 3.836919
  8400/100000: episode: 84, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.268, mean reward: 1.903 [1.453, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.894, 10.375], loss: 0.087227, mae: 0.293821, mean_q: 3.829451
  8500/100000: episode: 85, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 200.131, mean reward: 2.001 [1.472, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.821, 10.098], loss: 0.091514, mae: 0.292483, mean_q: 3.838924
  8600/100000: episode: 86, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 201.703, mean reward: 2.017 [1.487, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.612, 10.098], loss: 0.097109, mae: 0.306263, mean_q: 3.851366
  8700/100000: episode: 87, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.084, mean reward: 1.971 [1.504, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.430, 10.204], loss: 0.096018, mae: 0.307212, mean_q: 3.848556
  8800/100000: episode: 88, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.389, mean reward: 1.894 [1.458, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.322], loss: 0.090205, mae: 0.298879, mean_q: 3.844085
  8900/100000: episode: 89, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 190.413, mean reward: 1.904 [1.467, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.004, 10.161], loss: 0.089777, mae: 0.295373, mean_q: 3.844750
  9000/100000: episode: 90, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 195.765, mean reward: 1.958 [1.454, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.127, 10.115], loss: 0.098088, mae: 0.314768, mean_q: 3.850148
  9100/100000: episode: 91, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.950, mean reward: 2.039 [1.448, 12.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.892, 10.098], loss: 0.095702, mae: 0.308030, mean_q: 3.855319
  9200/100000: episode: 92, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 233.688, mean reward: 2.337 [1.546, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.440, 10.548], loss: 0.112061, mae: 0.303781, mean_q: 3.851948
  9300/100000: episode: 93, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 197.305, mean reward: 1.973 [1.474, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.439, 10.098], loss: 0.100197, mae: 0.310629, mean_q: 3.835231
  9400/100000: episode: 94, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 189.797, mean reward: 1.898 [1.440, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.390, 10.098], loss: 0.114696, mae: 0.313436, mean_q: 3.874615
  9500/100000: episode: 95, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.321, mean reward: 1.923 [1.454, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.098], loss: 0.111687, mae: 0.300567, mean_q: 3.855821
  9600/100000: episode: 96, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 198.426, mean reward: 1.984 [1.484, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.080, 10.098], loss: 0.100850, mae: 0.307637, mean_q: 3.883100
  9700/100000: episode: 97, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 194.605, mean reward: 1.946 [1.484, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.509, 10.098], loss: 0.096467, mae: 0.304813, mean_q: 3.862535
  9800/100000: episode: 98, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.344, mean reward: 1.923 [1.481, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.132, 10.127], loss: 0.094900, mae: 0.300293, mean_q: 3.840950
  9900/100000: episode: 99, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.793, mean reward: 1.938 [1.467, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.193, 10.098], loss: 0.139587, mae: 0.322703, mean_q: 3.863696
 10000/100000: episode: 100, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 182.697, mean reward: 1.827 [1.460, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.562, 10.218], loss: 0.088824, mae: 0.294599, mean_q: 3.858317
 10100/100000: episode: 101, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.359, mean reward: 1.844 [1.475, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.739, 10.098], loss: 0.120213, mae: 0.313683, mean_q: 3.862129
 10200/100000: episode: 102, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.197, mean reward: 1.832 [1.436, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.301], loss: 0.119173, mae: 0.313290, mean_q: 3.861667
 10300/100000: episode: 103, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.523, mean reward: 1.985 [1.505, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.581, 10.315], loss: 0.093217, mae: 0.302770, mean_q: 3.864292
 10400/100000: episode: 104, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.380, mean reward: 1.944 [1.492, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.255, 10.126], loss: 0.116387, mae: 0.308878, mean_q: 3.863841
 10500/100000: episode: 105, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 197.033, mean reward: 1.970 [1.486, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.400, 10.368], loss: 0.098206, mae: 0.300585, mean_q: 3.846536
 10600/100000: episode: 106, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 184.949, mean reward: 1.849 [1.462, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.128], loss: 0.115094, mae: 0.304297, mean_q: 3.862794
 10700/100000: episode: 107, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 180.875, mean reward: 1.809 [1.458, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.890, 10.207], loss: 0.105752, mae: 0.311141, mean_q: 3.855634
 10800/100000: episode: 108, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 177.364, mean reward: 1.774 [1.467, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.835, 10.142], loss: 0.106438, mae: 0.299357, mean_q: 3.849939
 10900/100000: episode: 109, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 193.376, mean reward: 1.934 [1.450, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.570, 10.098], loss: 0.112312, mae: 0.300198, mean_q: 3.832295
 11000/100000: episode: 110, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 204.867, mean reward: 2.049 [1.484, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.232, 10.376], loss: 0.095399, mae: 0.298062, mean_q: 3.824829
 11100/100000: episode: 111, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 221.112, mean reward: 2.211 [1.593, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.145, 10.272], loss: 0.085819, mae: 0.287960, mean_q: 3.813905
 11200/100000: episode: 112, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.841, mean reward: 1.898 [1.495, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.098], loss: 0.121974, mae: 0.314312, mean_q: 3.842399
 11300/100000: episode: 113, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 194.041, mean reward: 1.940 [1.481, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.460, 10.102], loss: 0.092001, mae: 0.295715, mean_q: 3.843462
 11400/100000: episode: 114, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 174.856, mean reward: 1.749 [1.438, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.335, 10.098], loss: 0.098002, mae: 0.302191, mean_q: 3.841716
 11500/100000: episode: 115, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 188.329, mean reward: 1.883 [1.476, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.139], loss: 0.086805, mae: 0.292848, mean_q: 3.831721
 11600/100000: episode: 116, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 188.009, mean reward: 1.880 [1.455, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.239, 10.098], loss: 0.090433, mae: 0.291147, mean_q: 3.818964
 11700/100000: episode: 117, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.541, mean reward: 1.885 [1.461, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.235, 10.328], loss: 0.112340, mae: 0.302272, mean_q: 3.840773
 11800/100000: episode: 118, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.477, mean reward: 1.945 [1.484, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.433, 10.151], loss: 0.113653, mae: 0.304540, mean_q: 3.806090
 11900/100000: episode: 119, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 178.797, mean reward: 1.788 [1.460, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.462, 10.218], loss: 0.099759, mae: 0.284545, mean_q: 3.791494
 12000/100000: episode: 120, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 213.031, mean reward: 2.130 [1.464, 6.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.486, 10.215], loss: 0.108940, mae: 0.300331, mean_q: 3.819669
 12100/100000: episode: 121, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 190.323, mean reward: 1.903 [1.450, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.657, 10.256], loss: 0.085216, mae: 0.286126, mean_q: 3.794046
 12200/100000: episode: 122, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 214.712, mean reward: 2.147 [1.504, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.310, 10.281], loss: 0.122921, mae: 0.317451, mean_q: 3.828317
 12300/100000: episode: 123, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 219.452, mean reward: 2.195 [1.487, 6.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.342], loss: 0.088507, mae: 0.293986, mean_q: 3.807419
 12400/100000: episode: 124, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 196.024, mean reward: 1.960 [1.442, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.098], loss: 0.100382, mae: 0.307266, mean_q: 3.838597
 12500/100000: episode: 125, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 188.975, mean reward: 1.890 [1.448, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.098], loss: 0.119699, mae: 0.313313, mean_q: 3.848938
 12600/100000: episode: 126, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 193.775, mean reward: 1.938 [1.504, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.380, 10.218], loss: 0.096717, mae: 0.296690, mean_q: 3.827002
 12700/100000: episode: 127, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 195.645, mean reward: 1.956 [1.465, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.341, 10.098], loss: 0.103642, mae: 0.304399, mean_q: 3.851161
 12800/100000: episode: 128, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 207.602, mean reward: 2.076 [1.464, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.976, 10.098], loss: 0.094378, mae: 0.299289, mean_q: 3.853560
 12900/100000: episode: 129, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.400, mean reward: 1.934 [1.449, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.911, 10.098], loss: 0.093428, mae: 0.298507, mean_q: 3.849387
 13000/100000: episode: 130, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 199.440, mean reward: 1.994 [1.483, 5.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.462, 10.128], loss: 0.127960, mae: 0.300706, mean_q: 3.854187
 13100/100000: episode: 131, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 191.595, mean reward: 1.916 [1.489, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.868, 10.189], loss: 0.105215, mae: 0.304473, mean_q: 3.861630
 13200/100000: episode: 132, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 207.290, mean reward: 2.073 [1.510, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.412, 10.497], loss: 0.099301, mae: 0.300341, mean_q: 3.876066
 13300/100000: episode: 133, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 232.802, mean reward: 2.328 [1.493, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.117, 10.575], loss: 0.099828, mae: 0.302782, mean_q: 3.860399
 13400/100000: episode: 134, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 185.380, mean reward: 1.854 [1.457, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.607, 10.166], loss: 0.102222, mae: 0.311232, mean_q: 3.869710
 13500/100000: episode: 135, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 208.132, mean reward: 2.081 [1.501, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.247, 10.230], loss: 0.103516, mae: 0.310027, mean_q: 3.867203
 13600/100000: episode: 136, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 200.891, mean reward: 2.009 [1.558, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.608, 10.098], loss: 0.104833, mae: 0.311524, mean_q: 3.872367
 13700/100000: episode: 137, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 187.814, mean reward: 1.878 [1.511, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.057, 10.098], loss: 0.121491, mae: 0.312273, mean_q: 3.874526
 13800/100000: episode: 138, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 209.166, mean reward: 2.092 [1.471, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.982, 10.098], loss: 0.109965, mae: 0.319363, mean_q: 3.864467
 13900/100000: episode: 139, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 189.306, mean reward: 1.893 [1.452, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.492, 10.258], loss: 0.119706, mae: 0.312273, mean_q: 3.861995
 14000/100000: episode: 140, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 206.811, mean reward: 2.068 [1.467, 3.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.141, 10.165], loss: 0.120072, mae: 0.311086, mean_q: 3.867482
 14100/100000: episode: 141, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 189.029, mean reward: 1.890 [1.502, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.362, 10.215], loss: 0.146208, mae: 0.340131, mean_q: 3.864536
 14200/100000: episode: 142, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.997, mean reward: 1.890 [1.430, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.373, 10.112], loss: 0.111290, mae: 0.306152, mean_q: 3.875606
 14300/100000: episode: 143, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 189.717, mean reward: 1.897 [1.459, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.515, 10.147], loss: 0.109101, mae: 0.317668, mean_q: 3.879062
 14400/100000: episode: 144, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 188.497, mean reward: 1.885 [1.461, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.098], loss: 0.098953, mae: 0.306379, mean_q: 3.871955
 14500/100000: episode: 145, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.752, mean reward: 1.908 [1.451, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.522, 10.194], loss: 0.103413, mae: 0.302751, mean_q: 3.846202
 14600/100000: episode: 146, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.207, mean reward: 1.972 [1.441, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.954, 10.098], loss: 0.105507, mae: 0.308476, mean_q: 3.863623
 14700/100000: episode: 147, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 179.798, mean reward: 1.798 [1.461, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.255, 10.156], loss: 0.095067, mae: 0.293743, mean_q: 3.850609
 14800/100000: episode: 148, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 206.687, mean reward: 2.067 [1.519, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.250, 10.098], loss: 0.107650, mae: 0.317506, mean_q: 3.869673
 14900/100000: episode: 149, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 201.455, mean reward: 2.015 [1.480, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.741, 10.108], loss: 0.108048, mae: 0.313898, mean_q: 3.848651
[Info] 1-TH LEVEL FOUND: 4.461198806762695, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.137s, episode steps: 100, steps per second: 19, episode reward: 182.058, mean reward: 1.821 [1.437, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.857, 10.279], loss: 0.103104, mae: 0.310097, mean_q: 3.865685
 15020/100000: episode: 151, duration: 0.138s, episode steps: 20, steps per second: 145, episode reward: 48.075, mean reward: 2.404 [1.770, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.873, 10.482], loss: 0.126446, mae: 0.327656, mean_q: 3.879054
 15041/100000: episode: 152, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 46.586, mean reward: 2.218 [1.771, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.309], loss: 0.100960, mae: 0.313642, mean_q: 3.900740
 15067/100000: episode: 153, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 73.740, mean reward: 2.836 [1.505, 10.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.552, 10.195], loss: 0.126526, mae: 0.311631, mean_q: 3.893337
 15088/100000: episode: 154, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 38.293, mean reward: 1.823 [1.480, 2.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.286], loss: 0.081785, mae: 0.297170, mean_q: 3.853870
 15108/100000: episode: 155, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 44.675, mean reward: 2.234 [1.661, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.350, 10.346], loss: 0.131298, mae: 0.333264, mean_q: 3.903897
 15130/100000: episode: 156, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 64.952, mean reward: 2.952 [1.768, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.599, 10.482], loss: 0.090062, mae: 0.289519, mean_q: 3.868166
 15151/100000: episode: 157, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 57.403, mean reward: 2.733 [1.956, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.528], loss: 0.087395, mae: 0.296002, mean_q: 3.852475
 15175/100000: episode: 158, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 57.922, mean reward: 2.413 [1.778, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.493, 10.241], loss: 0.109900, mae: 0.316684, mean_q: 3.884426
 15193/100000: episode: 159, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 41.020, mean reward: 2.279 [1.683, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.044, 10.277], loss: 0.144729, mae: 0.356851, mean_q: 3.934771
 15214/100000: episode: 160, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 49.957, mean reward: 2.379 [1.899, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.288, 10.312], loss: 0.145072, mae: 0.321879, mean_q: 3.884673
 15236/100000: episode: 161, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 57.058, mean reward: 2.594 [2.066, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.165, 10.386], loss: 0.096215, mae: 0.304246, mean_q: 3.847162
 15256/100000: episode: 162, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 39.391, mean reward: 1.970 [1.770, 2.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.546, 10.271], loss: 0.103573, mae: 0.315671, mean_q: 3.897847
 15277/100000: episode: 163, duration: 0.129s, episode steps: 21, steps per second: 162, episode reward: 55.795, mean reward: 2.657 [1.853, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.827, 10.327], loss: 0.105358, mae: 0.329619, mean_q: 3.909875
 15303/100000: episode: 164, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 109.816, mean reward: 4.224 [1.747, 6.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.181, 10.243], loss: 0.103212, mae: 0.306980, mean_q: 3.901876
 15324/100000: episode: 165, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 59.543, mean reward: 2.835 [2.237, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.359, 10.455], loss: 0.132291, mae: 0.351269, mean_q: 3.943022
 15345/100000: episode: 166, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 44.849, mean reward: 2.136 [1.665, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.156, 10.183], loss: 0.116963, mae: 0.314022, mean_q: 3.865536
 15365/100000: episode: 167, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 38.732, mean reward: 1.937 [1.541, 2.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.714, 10.250], loss: 0.105845, mae: 0.328310, mean_q: 3.933346
 15385/100000: episode: 168, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 41.830, mean reward: 2.091 [1.459, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.094, 10.233], loss: 0.083801, mae: 0.293001, mean_q: 3.849761
 15406/100000: episode: 169, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 46.945, mean reward: 2.235 [1.825, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.906, 10.356], loss: 0.213637, mae: 0.389710, mean_q: 3.983869
 15432/100000: episode: 170, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 69.225, mean reward: 2.663 [1.461, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.134, 10.166], loss: 0.123913, mae: 0.335976, mean_q: 3.953041
 15450/100000: episode: 171, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 41.492, mean reward: 2.305 [1.634, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.723, 10.236], loss: 0.141217, mae: 0.342252, mean_q: 3.940134
 15468/100000: episode: 172, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 36.124, mean reward: 2.007 [1.575, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.434, 10.214], loss: 0.104212, mae: 0.312354, mean_q: 3.939093
 15490/100000: episode: 173, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 45.140, mean reward: 2.052 [1.646, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.311, 10.270], loss: 0.148056, mae: 0.367413, mean_q: 3.958641
 15512/100000: episode: 174, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 46.695, mean reward: 2.122 [1.479, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.633, 10.192], loss: 0.184140, mae: 0.365539, mean_q: 3.930390
 15536/100000: episode: 175, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 51.672, mean reward: 2.153 [1.537, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.535, 10.100], loss: 0.133995, mae: 0.355345, mean_q: 3.926656
 15558/100000: episode: 176, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 87.334, mean reward: 3.970 [2.829, 8.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.489, 10.452], loss: 0.117027, mae: 0.343562, mean_q: 3.982834
 15582/100000: episode: 177, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 55.174, mean reward: 2.299 [1.785, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.140, 10.361], loss: 0.186360, mae: 0.375439, mean_q: 3.957700
 15606/100000: episode: 178, duration: 0.142s, episode steps: 24, steps per second: 168, episode reward: 53.720, mean reward: 2.238 [1.917, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.079, 10.309], loss: 0.139440, mae: 0.350592, mean_q: 3.974422
 15627/100000: episode: 179, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 37.073, mean reward: 1.765 [1.552, 2.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.477, 10.263], loss: 0.140638, mae: 0.361226, mean_q: 3.984692
 15653/100000: episode: 180, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 108.551, mean reward: 4.175 [2.582, 7.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.579, 10.470], loss: 0.117917, mae: 0.339654, mean_q: 3.997731
 15671/100000: episode: 181, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 65.326, mean reward: 3.629 [2.145, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.000, 10.320], loss: 0.158716, mae: 0.368423, mean_q: 4.005674
 15691/100000: episode: 182, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 42.018, mean reward: 2.101 [1.692, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.258, 10.262], loss: 0.191548, mae: 0.399855, mean_q: 3.989754
 15717/100000: episode: 183, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 78.309, mean reward: 3.012 [1.884, 5.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.366, 10.369], loss: 0.156479, mae: 0.349530, mean_q: 3.973038
 15739/100000: episode: 184, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 45.434, mean reward: 2.065 [1.682, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.240], loss: 0.162432, mae: 0.352418, mean_q: 4.012561
 15760/100000: episode: 185, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 36.930, mean reward: 1.759 [1.529, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.252], loss: 0.159403, mae: 0.384552, mean_q: 4.027239
 15782/100000: episode: 186, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 65.564, mean reward: 2.980 [1.816, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.230, 10.365], loss: 0.198601, mae: 0.403245, mean_q: 4.026795
 15803/100000: episode: 187, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 53.918, mean reward: 2.568 [1.950, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.715, 10.422], loss: 0.175757, mae: 0.384949, mean_q: 4.041243
 15829/100000: episode: 188, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 70.077, mean reward: 2.695 [1.631, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.286, 10.290], loss: 0.111955, mae: 0.333677, mean_q: 3.966307
 15850/100000: episode: 189, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 51.924, mean reward: 2.473 [2.095, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.200, 10.365], loss: 0.195665, mae: 0.404063, mean_q: 4.053140
 15871/100000: episode: 190, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 45.611, mean reward: 2.172 [1.803, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.096, 10.327], loss: 0.156055, mae: 0.371564, mean_q: 4.037023
 15893/100000: episode: 191, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 53.002, mean reward: 2.409 [1.658, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.319], loss: 0.134310, mae: 0.344103, mean_q: 4.013806
 15917/100000: episode: 192, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 54.835, mean reward: 2.285 [1.793, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.439, 10.380], loss: 0.173159, mae: 0.362826, mean_q: 4.032732
 15938/100000: episode: 193, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 42.616, mean reward: 2.029 [1.500, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.514, 10.293], loss: 0.138258, mae: 0.360320, mean_q: 4.017754
 15964/100000: episode: 194, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 150.612, mean reward: 5.793 [2.662, 13.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.637], loss: 0.152425, mae: 0.358250, mean_q: 4.003719
 15990/100000: episode: 195, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 88.309, mean reward: 3.396 [2.600, 4.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.545, 10.463], loss: 0.182881, mae: 0.400268, mean_q: 4.099721
 16008/100000: episode: 196, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 42.415, mean reward: 2.356 [1.843, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.278], loss: 0.163425, mae: 0.365208, mean_q: 3.987470
 16026/100000: episode: 197, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 42.173, mean reward: 2.343 [1.838, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.368, 10.293], loss: 0.189463, mae: 0.375073, mean_q: 4.115132
 16044/100000: episode: 198, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 43.730, mean reward: 2.429 [1.891, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.288], loss: 0.222155, mae: 0.437282, mean_q: 4.051379
 16066/100000: episode: 199, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 53.293, mean reward: 2.422 [1.513, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.433, 10.156], loss: 0.212500, mae: 0.401703, mean_q: 4.136074
 16087/100000: episode: 200, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 51.449, mean reward: 2.450 [2.015, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.368], loss: 0.330097, mae: 0.457781, mean_q: 4.126500
 16108/100000: episode: 201, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 47.520, mean reward: 2.263 [1.820, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.491, 10.381], loss: 0.266005, mae: 0.421688, mean_q: 4.144013
 16132/100000: episode: 202, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 58.450, mean reward: 2.435 [1.972, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.593, 10.311], loss: 0.163651, mae: 0.395434, mean_q: 4.082829
 16153/100000: episode: 203, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 37.545, mean reward: 1.788 [1.554, 2.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.122, 10.100], loss: 0.277772, mae: 0.407293, mean_q: 4.147189
 16177/100000: episode: 204, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 57.144, mean reward: 2.381 [1.760, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.130, 10.327], loss: 0.220970, mae: 0.415449, mean_q: 4.094006
 16195/100000: episode: 205, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 35.782, mean reward: 1.988 [1.553, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.248], loss: 0.187308, mae: 0.406362, mean_q: 4.089987
 16221/100000: episode: 206, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 82.094, mean reward: 3.157 [2.445, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.207, 10.473], loss: 0.216977, mae: 0.436022, mean_q: 4.170443
 16242/100000: episode: 207, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 71.749, mean reward: 3.417 [2.486, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.493, 10.436], loss: 0.157225, mae: 0.384544, mean_q: 4.145334
 16268/100000: episode: 208, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 82.099, mean reward: 3.158 [2.048, 7.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.409, 10.383], loss: 0.281610, mae: 0.440604, mean_q: 4.205009
 16290/100000: episode: 209, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 48.480, mean reward: 2.204 [1.927, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.155, 10.387], loss: 0.212221, mae: 0.415762, mean_q: 4.107677
 16308/100000: episode: 210, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 47.234, mean reward: 2.624 [1.693, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.333, 10.170], loss: 0.257073, mae: 0.448736, mean_q: 4.166847
 16334/100000: episode: 211, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 77.637, mean reward: 2.986 [1.742, 7.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.327], loss: 0.230516, mae: 0.434920, mean_q: 4.140522
 16354/100000: episode: 212, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 41.007, mean reward: 2.050 [1.576, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.174], loss: 0.269580, mae: 0.419505, mean_q: 4.114838
 16374/100000: episode: 213, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 47.016, mean reward: 2.351 [1.943, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.352], loss: 0.269221, mae: 0.445343, mean_q: 4.123127
 16394/100000: episode: 214, duration: 0.115s, episode steps: 20, steps per second: 173, episode reward: 40.631, mean reward: 2.032 [1.717, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.620, 10.222], loss: 0.197258, mae: 0.419464, mean_q: 4.191527
 16415/100000: episode: 215, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 48.698, mean reward: 2.319 [1.910, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.218, 10.439], loss: 0.211190, mae: 0.417329, mean_q: 4.135596
 16441/100000: episode: 216, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 83.222, mean reward: 3.201 [2.083, 5.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.372, 10.286], loss: 0.229087, mae: 0.447467, mean_q: 4.180850
 16459/100000: episode: 217, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 45.075, mean reward: 2.504 [1.993, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.342], loss: 0.172428, mae: 0.404520, mean_q: 4.214407
 16479/100000: episode: 218, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 48.396, mean reward: 2.420 [1.922, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.375], loss: 0.200323, mae: 0.419822, mean_q: 4.207217
 16500/100000: episode: 219, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 36.908, mean reward: 1.758 [1.458, 2.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.100], loss: 0.275675, mae: 0.455986, mean_q: 4.246521
 16521/100000: episode: 220, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 42.326, mean reward: 2.016 [1.622, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.082, 10.217], loss: 0.208091, mae: 0.413791, mean_q: 4.221172
 16545/100000: episode: 221, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 61.791, mean reward: 2.575 [2.036, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.230, 10.405], loss: 0.194124, mae: 0.398611, mean_q: 4.162393
 16566/100000: episode: 222, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 39.782, mean reward: 1.894 [1.489, 2.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.138, 10.186], loss: 0.172800, mae: 0.411494, mean_q: 4.173100
 16587/100000: episode: 223, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 44.345, mean reward: 2.112 [1.759, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.856, 10.266], loss: 0.214696, mae: 0.430733, mean_q: 4.232955
 16608/100000: episode: 224, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 37.982, mean reward: 1.809 [1.574, 2.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.073, 10.100], loss: 0.226300, mae: 0.422909, mean_q: 4.185386
 16632/100000: episode: 225, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 56.226, mean reward: 2.343 [1.843, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.295], loss: 0.269610, mae: 0.472089, mean_q: 4.298850
 16656/100000: episode: 226, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 52.707, mean reward: 2.196 [1.850, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.321], loss: 0.266110, mae: 0.444293, mean_q: 4.174914
 16680/100000: episode: 227, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 57.045, mean reward: 2.377 [1.943, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.458, 10.325], loss: 0.267880, mae: 0.432246, mean_q: 4.208489
 16701/100000: episode: 228, duration: 0.133s, episode steps: 21, steps per second: 157, episode reward: 44.172, mean reward: 2.103 [1.847, 2.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.312], loss: 0.182769, mae: 0.412961, mean_q: 4.237664
 16722/100000: episode: 229, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 47.501, mean reward: 2.262 [1.592, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.468, 10.301], loss: 0.312435, mae: 0.445389, mean_q: 4.244451
 16743/100000: episode: 230, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 40.463, mean reward: 1.927 [1.459, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.215, 10.100], loss: 0.229932, mae: 0.419400, mean_q: 4.170779
 16764/100000: episode: 231, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 40.331, mean reward: 1.921 [1.537, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.579, 10.109], loss: 0.269288, mae: 0.437341, mean_q: 4.245028
 16788/100000: episode: 232, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 54.174, mean reward: 2.257 [1.924, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.319, 10.364], loss: 0.171124, mae: 0.404797, mean_q: 4.250029
 16809/100000: episode: 233, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 48.633, mean reward: 2.316 [2.023, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.443, 10.351], loss: 0.263408, mae: 0.443839, mean_q: 4.276439
 16830/100000: episode: 234, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 51.730, mean reward: 2.463 [1.885, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.294, 10.469], loss: 0.161087, mae: 0.396820, mean_q: 4.239381
 16851/100000: episode: 235, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 56.104, mean reward: 2.672 [2.021, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.661, 10.352], loss: 0.175132, mae: 0.395137, mean_q: 4.214109
 16873/100000: episode: 236, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 44.027, mean reward: 2.001 [1.514, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.330, 10.209], loss: 0.164001, mae: 0.387766, mean_q: 4.224195
 16894/100000: episode: 237, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 37.847, mean reward: 1.802 [1.550, 2.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.483, 10.100], loss: 0.168618, mae: 0.398420, mean_q: 4.235245
 16915/100000: episode: 238, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 37.469, mean reward: 1.784 [1.514, 2.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.274, 10.200], loss: 0.144310, mae: 0.373242, mean_q: 4.150751
 16939/100000: episode: 239, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 45.165, mean reward: 1.882 [1.535, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.504, 10.161], loss: 0.254720, mae: 0.414939, mean_q: 4.257575
[Info] 2-TH LEVEL FOUND: 6.03369140625, Considering 13/87 traces
 16957/100000: episode: 240, duration: 4.255s, episode steps: 18, steps per second: 4, episode reward: 43.651, mean reward: 2.425 [1.680, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.039, 10.242], loss: 0.223381, mae: 0.429286, mean_q: 4.282209
 16983/100000: episode: 241, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 69.921, mean reward: 2.689 [1.675, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.389, 10.322], loss: 0.148963, mae: 0.380667, mean_q: 4.216655
 17009/100000: episode: 242, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 80.543, mean reward: 3.098 [1.793, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.342, 10.153], loss: 0.212772, mae: 0.415688, mean_q: 4.196987
 17035/100000: episode: 243, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 78.850, mean reward: 3.033 [2.149, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.616, 10.412], loss: 0.210107, mae: 0.414670, mean_q: 4.292539
 17061/100000: episode: 244, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 92.962, mean reward: 3.575 [2.478, 6.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.334, 10.464], loss: 0.178360, mae: 0.423589, mean_q: 4.202596
 17087/100000: episode: 245, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 73.558, mean reward: 2.829 [1.591, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.101, 10.254], loss: 0.182909, mae: 0.427911, mean_q: 4.256945
 17113/100000: episode: 246, duration: 0.160s, episode steps: 26, steps per second: 162, episode reward: 61.182, mean reward: 2.353 [1.733, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.255], loss: 0.213810, mae: 0.426773, mean_q: 4.267826
 17139/100000: episode: 247, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 90.876, mean reward: 3.495 [1.732, 11.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.514, 10.216], loss: 0.339642, mae: 0.489017, mean_q: 4.282704
 17165/100000: episode: 248, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 96.888, mean reward: 3.726 [2.566, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.558, 10.338], loss: 0.306297, mae: 0.478408, mean_q: 4.286118
 17191/100000: episode: 249, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 81.754, mean reward: 3.144 [1.650, 18.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.456, 10.216], loss: 0.237317, mae: 0.444654, mean_q: 4.311236
 17217/100000: episode: 250, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 91.590, mean reward: 3.523 [2.356, 6.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.161, 10.444], loss: 0.173069, mae: 0.417167, mean_q: 4.351190
 17243/100000: episode: 251, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 65.797, mean reward: 2.531 [1.503, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.570, 10.100], loss: 0.210343, mae: 0.445716, mean_q: 4.304242
 17269/100000: episode: 252, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 76.624, mean reward: 2.947 [1.806, 4.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.977, 10.260], loss: 0.327238, mae: 0.496458, mean_q: 4.409725
 17295/100000: episode: 253, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 65.385, mean reward: 2.515 [1.743, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.292, 10.232], loss: 0.332511, mae: 0.493882, mean_q: 4.361440
 17321/100000: episode: 254, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 100.817, mean reward: 3.878 [2.734, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.376, 10.473], loss: 0.286108, mae: 0.479026, mean_q: 4.403140
 17347/100000: episode: 255, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 73.078, mean reward: 2.811 [1.734, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.525, 10.232], loss: 0.295110, mae: 0.497734, mean_q: 4.424781
 17373/100000: episode: 256, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 72.407, mean reward: 2.785 [2.176, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.376], loss: 0.293583, mae: 0.473043, mean_q: 4.416151
 17399/100000: episode: 257, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 78.092, mean reward: 3.004 [2.009, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.123, 10.542], loss: 0.244764, mae: 0.448374, mean_q: 4.425570
 17425/100000: episode: 258, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 87.124, mean reward: 3.351 [2.280, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.538, 10.471], loss: 0.266789, mae: 0.423921, mean_q: 4.374998
 17451/100000: episode: 259, duration: 0.162s, episode steps: 26, steps per second: 161, episode reward: 102.542, mean reward: 3.944 [2.596, 6.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.156, 10.430], loss: 0.223308, mae: 0.471548, mean_q: 4.458962
 17477/100000: episode: 260, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 68.155, mean reward: 2.621 [1.848, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.696, 10.321], loss: 0.243847, mae: 0.473023, mean_q: 4.356078
 17503/100000: episode: 261, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 72.472, mean reward: 2.787 [1.716, 8.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.336], loss: 0.322371, mae: 0.440747, mean_q: 4.465589
 17529/100000: episode: 262, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 77.603, mean reward: 2.985 [1.739, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.171, 10.254], loss: 0.233595, mae: 0.470527, mean_q: 4.452105
 17555/100000: episode: 263, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 100.582, mean reward: 3.869 [2.781, 8.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.467], loss: 0.228172, mae: 0.451658, mean_q: 4.436046
 17581/100000: episode: 264, duration: 0.164s, episode steps: 26, steps per second: 159, episode reward: 80.269, mean reward: 3.087 [2.136, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.358], loss: 0.242394, mae: 0.474960, mean_q: 4.467360
 17607/100000: episode: 265, duration: 0.167s, episode steps: 26, steps per second: 155, episode reward: 74.350, mean reward: 2.860 [1.788, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.495], loss: 0.426526, mae: 0.569850, mean_q: 4.583202
 17633/100000: episode: 266, duration: 0.174s, episode steps: 26, steps per second: 149, episode reward: 96.768, mean reward: 3.722 [2.566, 6.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.448, 10.511], loss: 0.240933, mae: 0.444384, mean_q: 4.480609
 17659/100000: episode: 267, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 76.375, mean reward: 2.938 [1.961, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.637, 10.373], loss: 0.243679, mae: 0.442982, mean_q: 4.413756
 17685/100000: episode: 268, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 90.744, mean reward: 3.490 [2.552, 5.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.943, 10.586], loss: 0.323887, mae: 0.508375, mean_q: 4.600886
 17711/100000: episode: 269, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 103.451, mean reward: 3.979 [2.185, 8.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.030, 10.356], loss: 0.265191, mae: 0.476262, mean_q: 4.608098
 17737/100000: episode: 270, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 71.740, mean reward: 2.759 [1.709, 6.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.110, 10.217], loss: 0.226917, mae: 0.457474, mean_q: 4.541211
 17763/100000: episode: 271, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 74.109, mean reward: 2.850 [1.960, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.525, 10.306], loss: 0.299563, mae: 0.544331, mean_q: 4.511754
 17789/100000: episode: 272, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 61.870, mean reward: 2.380 [1.512, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.260], loss: 0.474052, mae: 0.508639, mean_q: 4.575246
 17815/100000: episode: 273, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 68.626, mean reward: 2.639 [1.750, 9.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.341, 10.314], loss: 0.228480, mae: 0.485133, mean_q: 4.653390
 17841/100000: episode: 274, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 77.886, mean reward: 2.996 [2.169, 6.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.133, 10.461], loss: 0.287083, mae: 0.463853, mean_q: 4.588637
 17867/100000: episode: 275, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 86.014, mean reward: 3.308 [2.235, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.195, 10.480], loss: 0.269331, mae: 0.466620, mean_q: 4.581741
 17893/100000: episode: 276, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 69.698, mean reward: 2.681 [1.991, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.355], loss: 0.268645, mae: 0.485328, mean_q: 4.610303
 17919/100000: episode: 277, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 71.995, mean reward: 2.769 [1.972, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.444], loss: 0.374634, mae: 0.474886, mean_q: 4.606992
 17945/100000: episode: 278, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 83.407, mean reward: 3.208 [1.880, 7.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.291], loss: 0.222386, mae: 0.442646, mean_q: 4.653031
 17971/100000: episode: 279, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 93.418, mean reward: 3.593 [2.100, 7.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.074, 10.359], loss: 0.289550, mae: 0.501421, mean_q: 4.690147
 17997/100000: episode: 280, duration: 0.168s, episode steps: 26, steps per second: 155, episode reward: 77.810, mean reward: 2.993 [1.989, 6.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.535, 10.579], loss: 0.233036, mae: 0.451879, mean_q: 4.610017
 18023/100000: episode: 281, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 76.595, mean reward: 2.946 [1.685, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.300, 10.327], loss: 0.397816, mae: 0.506379, mean_q: 4.686524
 18049/100000: episode: 282, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 64.182, mean reward: 2.469 [1.797, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.241], loss: 0.212700, mae: 0.473378, mean_q: 4.610136
 18075/100000: episode: 283, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 76.886, mean reward: 2.957 [2.288, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.456, 10.403], loss: 0.316727, mae: 0.491367, mean_q: 4.769740
 18101/100000: episode: 284, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 86.176, mean reward: 3.314 [2.329, 7.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.693, 10.466], loss: 0.388814, mae: 0.500556, mean_q: 4.595164
 18127/100000: episode: 285, duration: 0.164s, episode steps: 26, steps per second: 158, episode reward: 85.127, mean reward: 3.274 [1.836, 7.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.458, 10.309], loss: 0.218395, mae: 0.454166, mean_q: 4.708847
 18153/100000: episode: 286, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 71.833, mean reward: 2.763 [1.799, 5.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.349], loss: 0.302897, mae: 0.483955, mean_q: 4.720797
 18179/100000: episode: 287, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 102.615, mean reward: 3.947 [2.471, 7.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.466], loss: 0.439148, mae: 0.531176, mean_q: 4.752782
 18205/100000: episode: 288, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 85.806, mean reward: 3.300 [2.328, 4.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.605, 10.418], loss: 0.365709, mae: 0.507257, mean_q: 4.717553
 18231/100000: episode: 289, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 65.560, mean reward: 2.522 [1.944, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.303], loss: 0.280065, mae: 0.478086, mean_q: 4.625625
 18257/100000: episode: 290, duration: 0.164s, episode steps: 26, steps per second: 159, episode reward: 87.020, mean reward: 3.347 [2.184, 5.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.355], loss: 0.238291, mae: 0.482806, mean_q: 4.679188
 18283/100000: episode: 291, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 72.109, mean reward: 2.773 [1.690, 6.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.545, 10.226], loss: 0.362675, mae: 0.496791, mean_q: 4.708664
 18309/100000: episode: 292, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 74.042, mean reward: 2.848 [2.205, 4.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.767, 10.256], loss: 0.360422, mae: 0.488742, mean_q: 4.809754
 18335/100000: episode: 293, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 94.001, mean reward: 3.615 [2.219, 5.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.650, 10.456], loss: 0.178924, mae: 0.413847, mean_q: 4.764260
 18361/100000: episode: 294, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 109.822, mean reward: 4.224 [2.610, 14.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.614], loss: 0.170508, mae: 0.408823, mean_q: 4.691611
 18387/100000: episode: 295, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 75.509, mean reward: 2.904 [1.971, 6.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.362], loss: 0.344765, mae: 0.472498, mean_q: 4.739755
 18413/100000: episode: 296, duration: 0.164s, episode steps: 26, steps per second: 158, episode reward: 80.973, mean reward: 3.114 [1.812, 5.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.251, 10.294], loss: 0.220144, mae: 0.440468, mean_q: 4.718950
 18439/100000: episode: 297, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 80.820, mean reward: 3.108 [1.631, 6.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.960, 10.267], loss: 0.323796, mae: 0.478428, mean_q: 4.766795
 18465/100000: episode: 298, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 75.989, mean reward: 2.923 [1.684, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.670, 10.246], loss: 0.238167, mae: 0.482254, mean_q: 4.809726
 18491/100000: episode: 299, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 84.923, mean reward: 3.266 [1.849, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.239, 10.316], loss: 0.367868, mae: 0.474308, mean_q: 4.754948
 18517/100000: episode: 300, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 94.436, mean reward: 3.632 [2.541, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.737, 10.534], loss: 0.396863, mae: 0.497232, mean_q: 4.894058
 18543/100000: episode: 301, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 64.447, mean reward: 2.479 [1.532, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.116, 10.135], loss: 0.337187, mae: 0.536833, mean_q: 4.785495
 18569/100000: episode: 302, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 68.181, mean reward: 2.622 [1.963, 5.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.916, 10.363], loss: 0.287400, mae: 0.509306, mean_q: 4.766875
 18595/100000: episode: 303, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 66.417, mean reward: 2.555 [1.763, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.279, 10.234], loss: 0.423680, mae: 0.530572, mean_q: 4.946870
 18621/100000: episode: 304, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 69.149, mean reward: 2.660 [2.041, 6.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.255, 10.384], loss: 0.207612, mae: 0.461146, mean_q: 4.826758
 18647/100000: episode: 305, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 80.792, mean reward: 3.107 [1.708, 9.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.272], loss: 0.280051, mae: 0.530940, mean_q: 4.903443
 18673/100000: episode: 306, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 79.376, mean reward: 3.053 [2.154, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.731, 10.404], loss: 0.223228, mae: 0.483454, mean_q: 4.774176
 18699/100000: episode: 307, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 71.340, mean reward: 2.744 [1.618, 5.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.361], loss: 0.276853, mae: 0.507553, mean_q: 4.926001
 18725/100000: episode: 308, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 97.046, mean reward: 3.733 [1.899, 6.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.348], loss: 0.348307, mae: 0.527496, mean_q: 4.935262
 18751/100000: episode: 309, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 65.152, mean reward: 2.506 [1.786, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.303], loss: 0.315029, mae: 0.510152, mean_q: 4.969953
 18777/100000: episode: 310, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 90.710, mean reward: 3.489 [1.838, 7.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.687, 10.333], loss: 0.350202, mae: 0.485251, mean_q: 5.036144
 18803/100000: episode: 311, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 78.991, mean reward: 3.038 [2.026, 5.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.375], loss: 0.237620, mae: 0.449768, mean_q: 4.954625
 18829/100000: episode: 312, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 73.497, mean reward: 2.827 [1.699, 6.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.235, 10.327], loss: 0.218399, mae: 0.443816, mean_q: 4.942868
 18855/100000: episode: 313, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 67.830, mean reward: 2.609 [1.576, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.155, 10.170], loss: 0.296130, mae: 0.463800, mean_q: 4.961365
 18881/100000: episode: 314, duration: 0.159s, episode steps: 26, steps per second: 163, episode reward: 87.406, mean reward: 3.362 [2.464, 6.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.467], loss: 0.306052, mae: 0.474991, mean_q: 5.024302
 18907/100000: episode: 315, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 131.557, mean reward: 5.060 [3.215, 10.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.666], loss: 0.344091, mae: 0.532099, mean_q: 5.009410
 18933/100000: episode: 316, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 68.532, mean reward: 2.636 [1.748, 5.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.237], loss: 0.511558, mae: 0.610202, mean_q: 4.999504
 18959/100000: episode: 317, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 64.760, mean reward: 2.491 [1.624, 5.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.239], loss: 0.372763, mae: 0.536148, mean_q: 4.988984
 18985/100000: episode: 318, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 74.258, mean reward: 2.856 [2.163, 5.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.409], loss: 0.267149, mae: 0.514592, mean_q: 4.892891
 19011/100000: episode: 319, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 113.159, mean reward: 4.352 [2.363, 8.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.139, 10.478], loss: 0.307250, mae: 0.511233, mean_q: 4.983839
 19037/100000: episode: 320, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 81.593, mean reward: 3.138 [1.987, 6.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.447, 10.471], loss: 0.328668, mae: 0.492586, mean_q: 4.968097
 19063/100000: episode: 321, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 63.306, mean reward: 2.435 [1.564, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.216, 10.199], loss: 0.450654, mae: 0.559367, mean_q: 5.079402
 19089/100000: episode: 322, duration: 0.166s, episode steps: 26, steps per second: 157, episode reward: 69.358, mean reward: 2.668 [2.039, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.743, 10.345], loss: 0.230255, mae: 0.457056, mean_q: 5.048296
 19115/100000: episode: 323, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 65.274, mean reward: 2.511 [1.455, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.097, 10.173], loss: 0.391016, mae: 0.538532, mean_q: 5.051966
 19141/100000: episode: 324, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 109.501, mean reward: 4.212 [3.142, 7.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.525, 10.406], loss: 0.266925, mae: 0.466805, mean_q: 4.974460
 19167/100000: episode: 325, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 72.915, mean reward: 2.804 [1.747, 6.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.738, 10.267], loss: 0.303615, mae: 0.513810, mean_q: 5.197372
 19193/100000: episode: 326, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 70.847, mean reward: 2.725 [1.665, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.910, 10.281], loss: 0.404923, mae: 0.540442, mean_q: 5.051967
[Info] 3-TH LEVEL FOUND: 9.373116493225098, Considering 100/0 traces
 19219/100000: episode: 327, duration: 4.334s, episode steps: 26, steps per second: 6, episode reward: 73.045, mean reward: 2.809 [1.786, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.268, 10.317], loss: 0.264014, mae: 0.464520, mean_q: 5.090332
[Info] NOT FOUND NEW LEVEL, Current Best Level is 9.373116493225098
 19252/100000: episode: 328, duration: 4.151s, episode steps: 33, steps per second: 8, episode reward: 103.348, mean reward: 3.132 [2.504, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.469, 10.500], loss: 0.272610, mae: 0.518537, mean_q: 5.146100
 19352/100000: episode: 329, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 191.857, mean reward: 1.919 [1.467, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.714, 10.313], loss: 0.348191, mae: 0.531347, mean_q: 5.075867
 19452/100000: episode: 330, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 202.880, mean reward: 2.029 [1.500, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.200], loss: 0.272197, mae: 0.480628, mean_q: 5.034806
 19552/100000: episode: 331, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 181.879, mean reward: 1.819 [1.443, 2.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.865, 10.130], loss: 0.338085, mae: 0.513219, mean_q: 5.141833
 19652/100000: episode: 332, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 198.332, mean reward: 1.983 [1.469, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.219, 10.225], loss: 0.339540, mae: 0.514479, mean_q: 5.133687
 19752/100000: episode: 333, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 223.860, mean reward: 2.239 [1.447, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.881, 10.098], loss: 0.381057, mae: 0.513319, mean_q: 5.223869
 19852/100000: episode: 334, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 194.387, mean reward: 1.944 [1.442, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.347], loss: 0.347378, mae: 0.523632, mean_q: 5.101858
 19952/100000: episode: 335, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 192.799, mean reward: 1.928 [1.457, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.856, 10.188], loss: 0.310241, mae: 0.508646, mean_q: 5.148073
 20052/100000: episode: 336, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 184.082, mean reward: 1.841 [1.456, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.098], loss: 0.374961, mae: 0.545870, mean_q: 5.060869
 20152/100000: episode: 337, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 178.123, mean reward: 1.781 [1.446, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.263, 10.120], loss: 0.350935, mae: 0.506036, mean_q: 5.028159
 20252/100000: episode: 338, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 230.852, mean reward: 2.309 [1.489, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.333], loss: 0.300438, mae: 0.489050, mean_q: 5.012113
 20352/100000: episode: 339, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 201.439, mean reward: 2.014 [1.466, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.245, 10.201], loss: 0.287584, mae: 0.490806, mean_q: 5.058475
 20452/100000: episode: 340, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 188.290, mean reward: 1.883 [1.474, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.028, 10.449], loss: 0.343712, mae: 0.520339, mean_q: 5.026911
 20552/100000: episode: 341, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 199.409, mean reward: 1.994 [1.471, 5.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.358, 10.245], loss: 0.292756, mae: 0.490019, mean_q: 5.014506
 20652/100000: episode: 342, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 191.144, mean reward: 1.911 [1.488, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.605, 10.098], loss: 0.289928, mae: 0.467482, mean_q: 5.005020
 20752/100000: episode: 343, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 222.909, mean reward: 2.229 [1.505, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.513, 10.098], loss: 0.324256, mae: 0.477116, mean_q: 4.946111
 20852/100000: episode: 344, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 198.931, mean reward: 1.989 [1.473, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.098], loss: 0.282501, mae: 0.484130, mean_q: 4.947109
 20952/100000: episode: 345, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 191.343, mean reward: 1.913 [1.482, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.153, 10.257], loss: 0.221699, mae: 0.445372, mean_q: 4.876710
 21052/100000: episode: 346, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 196.780, mean reward: 1.968 [1.467, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.413, 10.098], loss: 0.253401, mae: 0.454653, mean_q: 4.935445
 21152/100000: episode: 347, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 199.127, mean reward: 1.991 [1.488, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.466, 10.098], loss: 0.343236, mae: 0.482600, mean_q: 4.933516
 21252/100000: episode: 348, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 190.136, mean reward: 1.901 [1.446, 5.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.133, 10.098], loss: 0.277174, mae: 0.467640, mean_q: 4.892000
 21352/100000: episode: 349, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 191.351, mean reward: 1.914 [1.498, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.817, 10.098], loss: 0.329437, mae: 0.475146, mean_q: 4.857482
 21452/100000: episode: 350, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 191.768, mean reward: 1.918 [1.469, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.433, 10.250], loss: 0.213857, mae: 0.431403, mean_q: 4.777770
 21552/100000: episode: 351, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 184.860, mean reward: 1.849 [1.435, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.141], loss: 0.263101, mae: 0.465027, mean_q: 4.802444
 21652/100000: episode: 352, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 190.701, mean reward: 1.907 [1.442, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.151], loss: 0.237033, mae: 0.442230, mean_q: 4.769336
 21752/100000: episode: 353, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 185.137, mean reward: 1.851 [1.468, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.322, 10.098], loss: 0.260519, mae: 0.444371, mean_q: 4.818374
 21852/100000: episode: 354, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 186.148, mean reward: 1.861 [1.440, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.889, 10.323], loss: 0.238993, mae: 0.435565, mean_q: 4.740128
 21952/100000: episode: 355, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 190.605, mean reward: 1.906 [1.461, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.329, 10.161], loss: 0.237231, mae: 0.432969, mean_q: 4.753166
 22052/100000: episode: 356, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 208.140, mean reward: 2.081 [1.462, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.223, 10.098], loss: 0.212346, mae: 0.424248, mean_q: 4.715908
 22152/100000: episode: 357, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 214.602, mean reward: 2.146 [1.452, 6.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.021, 10.098], loss: 0.185357, mae: 0.402525, mean_q: 4.628017
 22252/100000: episode: 358, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 189.644, mean reward: 1.896 [1.504, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.033, 10.098], loss: 0.204326, mae: 0.420607, mean_q: 4.626969
 22352/100000: episode: 359, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 185.956, mean reward: 1.860 [1.470, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.833, 10.098], loss: 0.190751, mae: 0.407886, mean_q: 4.562353
 22452/100000: episode: 360, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.635, mean reward: 1.866 [1.450, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.867, 10.191], loss: 0.233886, mae: 0.418673, mean_q: 4.573454
 22552/100000: episode: 361, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 215.858, mean reward: 2.159 [1.482, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.109, 10.396], loss: 0.247498, mae: 0.432057, mean_q: 4.582195
 22652/100000: episode: 362, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 188.154, mean reward: 1.882 [1.442, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.206, 10.217], loss: 0.230594, mae: 0.398358, mean_q: 4.473038
 22752/100000: episode: 363, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 194.741, mean reward: 1.947 [1.472, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.903, 10.447], loss: 0.172602, mae: 0.372457, mean_q: 4.432416
 22852/100000: episode: 364, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.624, mean reward: 1.866 [1.490, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.580, 10.098], loss: 0.196833, mae: 0.381740, mean_q: 4.411242
 22952/100000: episode: 365, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 193.652, mean reward: 1.937 [1.479, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.283], loss: 0.161028, mae: 0.357864, mean_q: 4.377628
 23052/100000: episode: 366, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 197.083, mean reward: 1.971 [1.459, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.509, 10.098], loss: 0.144423, mae: 0.358795, mean_q: 4.338458
 23152/100000: episode: 367, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 203.052, mean reward: 2.031 [1.497, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.967, 10.098], loss: 0.132461, mae: 0.342673, mean_q: 4.286518
 23252/100000: episode: 368, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 187.990, mean reward: 1.880 [1.455, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.226], loss: 0.169762, mae: 0.364884, mean_q: 4.321503
 23352/100000: episode: 369, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 223.294, mean reward: 2.233 [1.479, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.100, 10.098], loss: 0.157725, mae: 0.362678, mean_q: 4.266159
 23452/100000: episode: 370, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 195.276, mean reward: 1.953 [1.443, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.716, 10.098], loss: 0.121631, mae: 0.339103, mean_q: 4.162818
 23552/100000: episode: 371, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 197.810, mean reward: 1.978 [1.476, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.279, 10.098], loss: 0.125823, mae: 0.330468, mean_q: 4.131676
 23652/100000: episode: 372, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 194.812, mean reward: 1.948 [1.514, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.570, 10.127], loss: 0.122871, mae: 0.325010, mean_q: 4.129577
 23752/100000: episode: 373, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 180.000, mean reward: 1.800 [1.463, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.837, 10.098], loss: 0.128524, mae: 0.336016, mean_q: 4.045886
 23852/100000: episode: 374, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 215.251, mean reward: 2.153 [1.502, 6.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.652, 10.098], loss: 0.122915, mae: 0.328696, mean_q: 4.041169
 23952/100000: episode: 375, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 192.567, mean reward: 1.926 [1.443, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.900, 10.098], loss: 0.112257, mae: 0.316875, mean_q: 3.962944
 24052/100000: episode: 376, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 177.315, mean reward: 1.773 [1.456, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.735, 10.193], loss: 0.085300, mae: 0.288780, mean_q: 3.907750
 24152/100000: episode: 377, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 180.949, mean reward: 1.809 [1.474, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.289, 10.098], loss: 0.091287, mae: 0.292859, mean_q: 3.898737
 24252/100000: episode: 378, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 192.878, mean reward: 1.929 [1.461, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.342], loss: 0.081942, mae: 0.285589, mean_q: 3.866554
 24352/100000: episode: 379, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 204.441, mean reward: 2.044 [1.431, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.645, 10.098], loss: 0.089527, mae: 0.288736, mean_q: 3.869691
 24452/100000: episode: 380, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 182.690, mean reward: 1.827 [1.452, 2.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.515, 10.098], loss: 0.080056, mae: 0.282376, mean_q: 3.869546
 24552/100000: episode: 381, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 208.334, mean reward: 2.083 [1.443, 9.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.019, 10.394], loss: 0.105671, mae: 0.301767, mean_q: 3.887617
 24652/100000: episode: 382, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 197.247, mean reward: 1.972 [1.453, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.621, 10.150], loss: 0.085433, mae: 0.290627, mean_q: 3.885934
 24752/100000: episode: 383, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 186.870, mean reward: 1.869 [1.482, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.046, 10.249], loss: 0.088731, mae: 0.288747, mean_q: 3.876218
 24852/100000: episode: 384, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 207.963, mean reward: 2.080 [1.473, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.894, 10.521], loss: 0.090773, mae: 0.283586, mean_q: 3.851246
 24952/100000: episode: 385, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 201.964, mean reward: 2.020 [1.464, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.414, 10.098], loss: 0.085227, mae: 0.276742, mean_q: 3.858043
 25052/100000: episode: 386, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 181.336, mean reward: 1.813 [1.452, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.145], loss: 0.103728, mae: 0.299495, mean_q: 3.872420
 25152/100000: episode: 387, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 192.813, mean reward: 1.928 [1.480, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.103, 10.098], loss: 0.079040, mae: 0.286396, mean_q: 3.896472
 25252/100000: episode: 388, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: 191.307, mean reward: 1.913 [1.460, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.398, 10.195], loss: 0.083332, mae: 0.284001, mean_q: 3.861924
 25352/100000: episode: 389, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 201.433, mean reward: 2.014 [1.477, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.125, 10.185], loss: 0.088367, mae: 0.284798, mean_q: 3.859495
 25452/100000: episode: 390, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 205.560, mean reward: 2.056 [1.458, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.792, 10.100], loss: 0.085443, mae: 0.276341, mean_q: 3.851429
 25552/100000: episode: 391, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 226.055, mean reward: 2.261 [1.436, 4.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.718, 10.193], loss: 0.096701, mae: 0.281131, mean_q: 3.842747
 25652/100000: episode: 392, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 199.643, mean reward: 1.996 [1.475, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.098], loss: 0.090544, mae: 0.290452, mean_q: 3.872692
 25752/100000: episode: 393, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 270.240, mean reward: 2.702 [1.443, 12.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.005, 10.098], loss: 0.099775, mae: 0.282640, mean_q: 3.850859
 25852/100000: episode: 394, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: 185.044, mean reward: 1.850 [1.472, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.782, 10.154], loss: 0.103827, mae: 0.287840, mean_q: 3.877715
 25952/100000: episode: 395, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 199.486, mean reward: 1.995 [1.478, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.332, 10.098], loss: 0.111519, mae: 0.294488, mean_q: 3.882070
 26052/100000: episode: 396, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 189.941, mean reward: 1.899 [1.460, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.368], loss: 0.108334, mae: 0.297962, mean_q: 3.874377
 26152/100000: episode: 397, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 202.237, mean reward: 2.022 [1.463, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.994, 10.273], loss: 0.131064, mae: 0.295409, mean_q: 3.882868
 26252/100000: episode: 398, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 185.844, mean reward: 1.858 [1.454, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.081, 10.214], loss: 0.090454, mae: 0.294965, mean_q: 3.880211
 26352/100000: episode: 399, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 184.916, mean reward: 1.849 [1.433, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.526, 10.098], loss: 0.085763, mae: 0.290690, mean_q: 3.883385
 26452/100000: episode: 400, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 188.143, mean reward: 1.881 [1.455, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.059, 10.098], loss: 0.147593, mae: 0.317643, mean_q: 3.895088
 26552/100000: episode: 401, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 194.030, mean reward: 1.940 [1.481, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.713, 10.099], loss: 0.121186, mae: 0.302421, mean_q: 3.878453
 26652/100000: episode: 402, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 190.920, mean reward: 1.909 [1.482, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.782, 10.098], loss: 0.149687, mae: 0.319672, mean_q: 3.889879
 26752/100000: episode: 403, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 202.552, mean reward: 2.026 [1.441, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.098], loss: 0.102016, mae: 0.291811, mean_q: 3.870266
 26852/100000: episode: 404, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 202.903, mean reward: 2.029 [1.464, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.603, 10.098], loss: 0.080728, mae: 0.284883, mean_q: 3.865878
 26952/100000: episode: 405, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 189.035, mean reward: 1.890 [1.442, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.395, 10.140], loss: 0.104894, mae: 0.290402, mean_q: 3.883109
 27052/100000: episode: 406, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 185.038, mean reward: 1.850 [1.457, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.660, 10.098], loss: 0.128091, mae: 0.311627, mean_q: 3.897134
 27152/100000: episode: 407, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 230.488, mean reward: 2.305 [1.554, 19.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.168, 10.233], loss: 0.162654, mae: 0.324274, mean_q: 3.903638
 27252/100000: episode: 408, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 198.479, mean reward: 1.985 [1.488, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.271, 10.222], loss: 0.140978, mae: 0.308560, mean_q: 3.889967
 27352/100000: episode: 409, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 182.175, mean reward: 1.822 [1.466, 2.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.150], loss: 0.153713, mae: 0.319296, mean_q: 3.895841
 27452/100000: episode: 410, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 205.967, mean reward: 2.060 [1.494, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.898, 10.132], loss: 0.106241, mae: 0.301362, mean_q: 3.884497
 27552/100000: episode: 411, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 186.141, mean reward: 1.861 [1.442, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.496, 10.138], loss: 0.192203, mae: 0.337398, mean_q: 3.897682
 27652/100000: episode: 412, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 185.589, mean reward: 1.856 [1.466, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.047, 10.098], loss: 0.248899, mae: 0.340691, mean_q: 3.898670
 27752/100000: episode: 413, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 190.672, mean reward: 1.907 [1.450, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.280, 10.147], loss: 0.188219, mae: 0.337777, mean_q: 3.907624
 27852/100000: episode: 414, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 195.002, mean reward: 1.950 [1.462, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.345, 10.098], loss: 0.121283, mae: 0.300837, mean_q: 3.876644
 27952/100000: episode: 415, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 191.774, mean reward: 1.918 [1.474, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.932, 10.098], loss: 0.105902, mae: 0.300463, mean_q: 3.886607
 28052/100000: episode: 416, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 188.830, mean reward: 1.888 [1.464, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.548, 10.098], loss: 0.095172, mae: 0.288530, mean_q: 3.880550
 28152/100000: episode: 417, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: 181.533, mean reward: 1.815 [1.440, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.559, 10.098], loss: 0.223354, mae: 0.346364, mean_q: 3.896501
 28252/100000: episode: 418, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 178.352, mean reward: 1.784 [1.434, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.015, 10.098], loss: 0.148786, mae: 0.306286, mean_q: 3.878979
 28352/100000: episode: 419, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 187.853, mean reward: 1.879 [1.485, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.629, 10.264], loss: 0.113589, mae: 0.305286, mean_q: 3.855220
 28452/100000: episode: 420, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 192.106, mean reward: 1.921 [1.467, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.522, 10.098], loss: 0.103175, mae: 0.286800, mean_q: 3.843019
 28552/100000: episode: 421, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 193.815, mean reward: 1.938 [1.444, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.997, 10.139], loss: 0.124234, mae: 0.298314, mean_q: 3.849164
 28652/100000: episode: 422, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 191.654, mean reward: 1.917 [1.440, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.489, 10.129], loss: 0.121297, mae: 0.295003, mean_q: 3.866698
 28752/100000: episode: 423, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 206.246, mean reward: 2.062 [1.473, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.411, 10.294], loss: 0.194438, mae: 0.321003, mean_q: 3.871827
 28852/100000: episode: 424, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 229.961, mean reward: 2.300 [1.494, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.689, 10.098], loss: 0.131255, mae: 0.300416, mean_q: 3.864088
 28952/100000: episode: 425, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 209.602, mean reward: 2.096 [1.487, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.832, 10.098], loss: 0.185190, mae: 0.321469, mean_q: 3.878425
 29052/100000: episode: 426, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 182.503, mean reward: 1.825 [1.453, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.857, 10.098], loss: 0.130262, mae: 0.300500, mean_q: 3.877628
 29152/100000: episode: 427, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 189.247, mean reward: 1.892 [1.456, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.760, 10.098], loss: 0.113785, mae: 0.303465, mean_q: 3.880269
[Info] 1-TH LEVEL FOUND: 5.239614009857178, Considering 10/90 traces
 29252/100000: episode: 428, duration: 5.160s, episode steps: 100, steps per second: 19, episode reward: 189.826, mean reward: 1.898 [1.450, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.422, 10.349], loss: 0.146300, mae: 0.305553, mean_q: 3.892002
 29284/100000: episode: 429, duration: 0.218s, episode steps: 32, steps per second: 147, episode reward: 95.907, mean reward: 2.997 [1.885, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.773, 10.100], loss: 0.247318, mae: 0.327454, mean_q: 3.910351
 29328/100000: episode: 430, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 109.002, mean reward: 2.477 [1.460, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.403, 10.134], loss: 0.304456, mae: 0.347686, mean_q: 3.889904
 29360/100000: episode: 431, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 106.440, mean reward: 3.326 [2.299, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.347, 10.100], loss: 0.091562, mae: 0.301002, mean_q: 3.882877
 29399/100000: episode: 432, duration: 0.235s, episode steps: 39, steps per second: 166, episode reward: 156.612, mean reward: 4.016 [2.434, 12.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.824, 10.512], loss: 0.121403, mae: 0.299155, mean_q: 3.903697
 29431/100000: episode: 433, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 79.763, mean reward: 2.493 [1.724, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.309, 10.100], loss: 0.191146, mae: 0.340107, mean_q: 3.961884
 29474/100000: episode: 434, duration: 0.268s, episode steps: 43, steps per second: 161, episode reward: 103.016, mean reward: 2.396 [1.475, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.099, 10.179], loss: 0.238371, mae: 0.365834, mean_q: 3.931965
 29515/100000: episode: 435, duration: 0.264s, episode steps: 41, steps per second: 155, episode reward: 99.788, mean reward: 2.434 [1.489, 13.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.316, 10.100], loss: 0.181920, mae: 0.354213, mean_q: 3.998131
 29560/100000: episode: 436, duration: 0.286s, episode steps: 45, steps per second: 157, episode reward: 96.171, mean reward: 2.137 [1.489, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.838, 10.100], loss: 0.138498, mae: 0.322383, mean_q: 3.965974
 29603/100000: episode: 437, duration: 0.256s, episode steps: 43, steps per second: 168, episode reward: 94.075, mean reward: 2.188 [1.481, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.714, 10.100], loss: 0.137606, mae: 0.320398, mean_q: 3.920758
 29616/100000: episode: 438, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 35.911, mean reward: 2.762 [2.259, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.452], loss: 0.207275, mae: 0.336875, mean_q: 3.997104
 29623/100000: episode: 439, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 17.240, mean reward: 2.463 [2.100, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.377, 10.402], loss: 0.107343, mae: 0.355672, mean_q: 3.970839
 29662/100000: episode: 440, duration: 0.231s, episode steps: 39, steps per second: 169, episode reward: 94.581, mean reward: 2.425 [1.685, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.265, 10.254], loss: 0.156060, mae: 0.339264, mean_q: 4.002233
 29694/100000: episode: 441, duration: 0.180s, episode steps: 32, steps per second: 177, episode reward: 77.282, mean reward: 2.415 [1.670, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.636, 10.100], loss: 0.175296, mae: 0.354678, mean_q: 4.009846
 29715/100000: episode: 442, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 50.424, mean reward: 2.401 [1.919, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.613, 10.320], loss: 0.215637, mae: 0.365031, mean_q: 3.988279
 29736/100000: episode: 443, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 61.247, mean reward: 2.917 [1.888, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.735, 10.441], loss: 0.251675, mae: 0.359435, mean_q: 3.997105
 29757/100000: episode: 444, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 56.837, mean reward: 2.707 [1.964, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.188, 10.344], loss: 0.428827, mae: 0.423697, mean_q: 4.081320
 29778/100000: episode: 445, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 98.210, mean reward: 4.677 [2.722, 10.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.677], loss: 0.176584, mae: 0.372514, mean_q: 4.029679
 29822/100000: episode: 446, duration: 0.286s, episode steps: 44, steps per second: 154, episode reward: 132.674, mean reward: 3.015 [2.113, 6.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.194, 10.408], loss: 0.156515, mae: 0.331359, mean_q: 3.974872
 29835/100000: episode: 447, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 30.733, mean reward: 2.364 [1.793, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-1.043, 10.299], loss: 0.232826, mae: 0.351088, mean_q: 3.933243
 29856/100000: episode: 448, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 60.946, mean reward: 2.902 [2.379, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.299, 10.442], loss: 0.176940, mae: 0.353136, mean_q: 4.023232
 29897/100000: episode: 449, duration: 0.245s, episode steps: 41, steps per second: 168, episode reward: 83.885, mean reward: 2.046 [1.562, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.119, 10.437], loss: 0.158004, mae: 0.368650, mean_q: 4.047740
 29942/100000: episode: 450, duration: 0.262s, episode steps: 45, steps per second: 172, episode reward: 91.962, mean reward: 2.044 [1.479, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.889, 10.170], loss: 0.305482, mae: 0.408058, mean_q: 4.036140
 29978/100000: episode: 451, duration: 0.229s, episode steps: 36, steps per second: 157, episode reward: 96.484, mean reward: 2.680 [2.015, 5.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.654, 10.100], loss: 0.235173, mae: 0.383362, mean_q: 4.027784
 29999/100000: episode: 452, duration: 0.172s, episode steps: 21, steps per second: 122, episode reward: 55.375, mean reward: 2.637 [2.040, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.152, 10.305], loss: 0.311608, mae: 0.380167, mean_q: 4.012348
 30040/100000: episode: 453, duration: 0.274s, episode steps: 41, steps per second: 150, episode reward: 103.248, mean reward: 2.518 [2.024, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.552, 10.412], loss: 0.245079, mae: 0.403193, mean_q: 4.069803
 30079/100000: episode: 454, duration: 0.339s, episode steps: 39, steps per second: 115, episode reward: 96.427, mean reward: 2.472 [1.804, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.058, 10.310], loss: 0.338956, mae: 0.398499, mean_q: 4.082446
 30120/100000: episode: 455, duration: 0.291s, episode steps: 41, steps per second: 141, episode reward: 122.402, mean reward: 2.985 [2.354, 5.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.865, 10.411], loss: 0.288371, mae: 0.405688, mean_q: 4.111677
 30127/100000: episode: 456, duration: 0.056s, episode steps: 7, steps per second: 125, episode reward: 14.264, mean reward: 2.038 [1.870, 2.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.115, 10.305], loss: 0.145832, mae: 0.353076, mean_q: 4.088895
 30172/100000: episode: 457, duration: 0.303s, episode steps: 45, steps per second: 149, episode reward: 97.955, mean reward: 2.177 [1.448, 5.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.345, 10.242], loss: 0.104102, mae: 0.328677, mean_q: 4.054978
 30217/100000: episode: 458, duration: 0.330s, episode steps: 45, steps per second: 136, episode reward: 102.129, mean reward: 2.270 [1.510, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.169, 10.122], loss: 0.285043, mae: 0.418726, mean_q: 4.087448
 30262/100000: episode: 459, duration: 0.317s, episode steps: 45, steps per second: 142, episode reward: 87.559, mean reward: 1.946 [1.471, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.723, 10.100], loss: 0.360951, mae: 0.412789, mean_q: 4.128100
 30269/100000: episode: 460, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 15.609, mean reward: 2.230 [1.753, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.266], loss: 0.123386, mae: 0.345014, mean_q: 4.141988
 30314/100000: episode: 461, duration: 0.286s, episode steps: 45, steps per second: 157, episode reward: 85.405, mean reward: 1.898 [1.439, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.151, 10.336], loss: 0.189797, mae: 0.384318, mean_q: 4.084351
 30353/100000: episode: 462, duration: 0.246s, episode steps: 39, steps per second: 159, episode reward: 107.730, mean reward: 2.762 [1.717, 14.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.352, 10.352], loss: 0.261179, mae: 0.389665, mean_q: 4.146792
 30360/100000: episode: 463, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 16.357, mean reward: 2.337 [1.723, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.243], loss: 0.166381, mae: 0.398394, mean_q: 4.226627
 30404/100000: episode: 464, duration: 0.283s, episode steps: 44, steps per second: 155, episode reward: 100.416, mean reward: 2.282 [1.765, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.188, 10.270], loss: 0.134129, mae: 0.350625, mean_q: 4.122272
 30443/100000: episode: 465, duration: 0.277s, episode steps: 39, steps per second: 141, episode reward: 84.747, mean reward: 2.173 [1.673, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.485, 10.269], loss: 0.307746, mae: 0.386646, mean_q: 4.139035
 30486/100000: episode: 466, duration: 0.483s, episode steps: 43, steps per second: 89, episode reward: 99.871, mean reward: 2.323 [1.508, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.518, 10.100], loss: 0.295003, mae: 0.373918, mean_q: 4.125793
 30525/100000: episode: 467, duration: 0.344s, episode steps: 39, steps per second: 113, episode reward: 89.837, mean reward: 2.304 [1.570, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.407, 10.249], loss: 0.408949, mae: 0.476755, mean_q: 4.198925
 30564/100000: episode: 468, duration: 0.276s, episode steps: 39, steps per second: 141, episode reward: 94.873, mean reward: 2.433 [1.759, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.914, 10.220], loss: 0.254160, mae: 0.379455, mean_q: 4.083462
 30571/100000: episode: 469, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 16.562, mean reward: 2.366 [2.185, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.337], loss: 0.119064, mae: 0.310483, mean_q: 3.932093
 30612/100000: episode: 470, duration: 0.283s, episode steps: 41, steps per second: 145, episode reward: 118.445, mean reward: 2.889 [1.656, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.119, 10.235], loss: 0.212611, mae: 0.367212, mean_q: 4.138717
 30655/100000: episode: 471, duration: 0.280s, episode steps: 43, steps per second: 154, episode reward: 85.858, mean reward: 1.997 [1.483, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.872, 10.272], loss: 0.169226, mae: 0.356106, mean_q: 4.166487
 30662/100000: episode: 472, duration: 0.053s, episode steps: 7, steps per second: 132, episode reward: 16.971, mean reward: 2.424 [2.166, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.321], loss: 0.082502, mae: 0.304452, mean_q: 4.204086
 30698/100000: episode: 473, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 87.959, mean reward: 2.443 [1.528, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.101, 10.145], loss: 0.175066, mae: 0.344142, mean_q: 4.144182
 30737/100000: episode: 474, duration: 0.260s, episode steps: 39, steps per second: 150, episode reward: 109.001, mean reward: 2.795 [1.516, 7.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.466, 10.175], loss: 0.117088, mae: 0.341380, mean_q: 4.108992
 30758/100000: episode: 475, duration: 0.195s, episode steps: 21, steps per second: 108, episode reward: 57.330, mean reward: 2.730 [1.991, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.719, 10.370], loss: 0.083707, mae: 0.289864, mean_q: 4.075807
 30801/100000: episode: 476, duration: 0.397s, episode steps: 43, steps per second: 108, episode reward: 92.301, mean reward: 2.147 [1.522, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.212, 10.100], loss: 0.312293, mae: 0.416764, mean_q: 4.127033
 30822/100000: episode: 477, duration: 0.136s, episode steps: 21, steps per second: 154, episode reward: 84.363, mean reward: 4.017 [2.710, 6.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.230, 10.529], loss: 0.305089, mae: 0.459951, mean_q: 4.226683
 30858/100000: episode: 478, duration: 0.225s, episode steps: 36, steps per second: 160, episode reward: 99.913, mean reward: 2.775 [1.809, 6.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.283, 10.100], loss: 0.178065, mae: 0.337490, mean_q: 4.139555
 30902/100000: episode: 479, duration: 0.414s, episode steps: 44, steps per second: 106, episode reward: 110.399, mean reward: 2.509 [1.788, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.880, 10.340], loss: 0.291963, mae: 0.390120, mean_q: 4.170517
 30938/100000: episode: 480, duration: 0.289s, episode steps: 36, steps per second: 125, episode reward: 93.157, mean reward: 2.588 [1.874, 5.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.233, 10.100], loss: 0.125926, mae: 0.346304, mean_q: 4.168053
 30959/100000: episode: 481, duration: 0.164s, episode steps: 21, steps per second: 128, episode reward: 110.483, mean reward: 5.261 [2.417, 20.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.092, 10.332], loss: 0.110644, mae: 0.328390, mean_q: 4.146039
 30998/100000: episode: 482, duration: 0.302s, episode steps: 39, steps per second: 129, episode reward: 81.135, mean reward: 2.080 [1.597, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.240, 10.263], loss: 0.154263, mae: 0.356001, mean_q: 4.199742
 31005/100000: episode: 483, duration: 0.061s, episode steps: 7, steps per second: 115, episode reward: 19.705, mean reward: 2.815 [2.302, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.262], loss: 0.358758, mae: 0.427085, mean_q: 4.222856
 31049/100000: episode: 484, duration: 0.322s, episode steps: 44, steps per second: 137, episode reward: 97.391, mean reward: 2.213 [1.712, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-1.755, 10.310], loss: 0.391845, mae: 0.438552, mean_q: 4.178279
 31093/100000: episode: 485, duration: 0.267s, episode steps: 44, steps per second: 165, episode reward: 116.139, mean reward: 2.640 [1.877, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.177, 10.459], loss: 0.251514, mae: 0.432431, mean_q: 4.210388
 31132/100000: episode: 486, duration: 0.237s, episode steps: 39, steps per second: 164, episode reward: 81.133, mean reward: 2.080 [1.564, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.418, 10.377], loss: 0.347318, mae: 0.434609, mean_q: 4.256002
 31176/100000: episode: 487, duration: 0.253s, episode steps: 44, steps per second: 174, episode reward: 90.260, mean reward: 2.051 [1.553, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.323, 10.248], loss: 0.271832, mae: 0.403301, mean_q: 4.186508
 31219/100000: episode: 488, duration: 0.251s, episode steps: 43, steps per second: 171, episode reward: 100.794, mean reward: 2.344 [1.461, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.111, 10.140], loss: 0.234843, mae: 0.380005, mean_q: 4.220860
 31226/100000: episode: 489, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 18.037, mean reward: 2.577 [2.085, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-1.550, 10.320], loss: 0.773295, mae: 0.481819, mean_q: 4.344322
 31247/100000: episode: 490, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 48.523, mean reward: 2.311 [1.628, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.106, 10.133], loss: 0.183514, mae: 0.388757, mean_q: 4.184841
 31286/100000: episode: 491, duration: 0.221s, episode steps: 39, steps per second: 176, episode reward: 78.574, mean reward: 2.015 [1.445, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.044, 10.100], loss: 0.165085, mae: 0.373927, mean_q: 4.263556
 31331/100000: episode: 492, duration: 0.350s, episode steps: 45, steps per second: 129, episode reward: 117.880, mean reward: 2.620 [1.834, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.521, 10.471], loss: 0.150786, mae: 0.372387, mean_q: 4.298062
 31374/100000: episode: 493, duration: 0.266s, episode steps: 43, steps per second: 162, episode reward: 93.257, mean reward: 2.169 [1.847, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.239, 10.292], loss: 0.148695, mae: 0.366011, mean_q: 4.254245
 31415/100000: episode: 494, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 126.540, mean reward: 3.086 [2.213, 6.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.211, 10.376], loss: 0.230884, mae: 0.393941, mean_q: 4.292796
 31428/100000: episode: 495, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 46.753, mean reward: 3.596 [2.633, 6.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.562], loss: 0.196193, mae: 0.399260, mean_q: 4.254206
 31469/100000: episode: 496, duration: 0.255s, episode steps: 41, steps per second: 161, episode reward: 103.483, mean reward: 2.524 [1.704, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.427, 10.271], loss: 0.439949, mae: 0.440884, mean_q: 4.281847
 31482/100000: episode: 497, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 33.355, mean reward: 2.566 [2.093, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.727, 10.348], loss: 0.268641, mae: 0.465540, mean_q: 4.325280
 31521/100000: episode: 498, duration: 0.257s, episode steps: 39, steps per second: 152, episode reward: 80.015, mean reward: 2.052 [1.597, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.374, 10.100], loss: 0.231925, mae: 0.400014, mean_q: 4.289392
 31565/100000: episode: 499, duration: 0.266s, episode steps: 44, steps per second: 165, episode reward: 94.443, mean reward: 2.146 [1.617, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.187, 10.228], loss: 0.270300, mae: 0.403415, mean_q: 4.262611
 31608/100000: episode: 500, duration: 0.274s, episode steps: 43, steps per second: 157, episode reward: 103.753, mean reward: 2.413 [1.509, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.338, 10.187], loss: 0.304799, mae: 0.400031, mean_q: 4.316405
 31651/100000: episode: 501, duration: 0.311s, episode steps: 43, steps per second: 138, episode reward: 83.791, mean reward: 1.949 [1.534, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.339, 10.100], loss: 0.375468, mae: 0.414604, mean_q: 4.358265
 31687/100000: episode: 502, duration: 0.225s, episode steps: 36, steps per second: 160, episode reward: 200.967, mean reward: 5.582 [2.703, 38.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.737, 10.100], loss: 0.371354, mae: 0.418023, mean_q: 4.292803
 31730/100000: episode: 503, duration: 0.323s, episode steps: 43, steps per second: 133, episode reward: 88.684, mean reward: 2.062 [1.599, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.088, 10.251], loss: 0.701657, mae: 0.488228, mean_q: 4.341261
 31774/100000: episode: 504, duration: 0.270s, episode steps: 44, steps per second: 163, episode reward: 100.014, mean reward: 2.273 [1.452, 5.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.913, 10.100], loss: 0.744941, mae: 0.575332, mean_q: 4.378945
 31810/100000: episode: 505, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 132.840, mean reward: 3.690 [2.367, 6.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.311, 10.100], loss: 0.266311, mae: 0.427821, mean_q: 4.380374
 31846/100000: episode: 506, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 85.102, mean reward: 2.364 [1.624, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.343, 10.100], loss: 0.265779, mae: 0.419651, mean_q: 4.346316
 31889/100000: episode: 507, duration: 0.305s, episode steps: 43, steps per second: 141, episode reward: 136.671, mean reward: 3.178 [2.284, 5.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.297, 10.592], loss: 0.143750, mae: 0.364721, mean_q: 4.345043
 31925/100000: episode: 508, duration: 0.232s, episode steps: 36, steps per second: 155, episode reward: 87.186, mean reward: 2.422 [1.477, 7.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.748, 10.137], loss: 0.298019, mae: 0.420699, mean_q: 4.409751
 31957/100000: episode: 509, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 77.558, mean reward: 2.424 [2.064, 5.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.933, 10.100], loss: 0.280617, mae: 0.400130, mean_q: 4.401017
 31970/100000: episode: 510, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 32.208, mean reward: 2.478 [1.923, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.348], loss: 0.147086, mae: 0.373986, mean_q: 4.416802
 32006/100000: episode: 511, duration: 0.276s, episode steps: 36, steps per second: 131, episode reward: 77.263, mean reward: 2.146 [1.475, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.323, 10.100], loss: 1.341656, mae: 0.521249, mean_q: 4.472562
 32045/100000: episode: 512, duration: 0.294s, episode steps: 39, steps per second: 133, episode reward: 94.897, mean reward: 2.433 [1.892, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.026, 10.358], loss: 0.775994, mae: 0.535731, mean_q: 4.426333
 32081/100000: episode: 513, duration: 0.262s, episode steps: 36, steps per second: 137, episode reward: 112.476, mean reward: 3.124 [2.141, 8.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.755, 10.100], loss: 0.214309, mae: 0.406470, mean_q: 4.371759
 32113/100000: episode: 514, duration: 0.221s, episode steps: 32, steps per second: 145, episode reward: 77.753, mean reward: 2.430 [1.655, 6.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.219, 10.100], loss: 0.194943, mae: 0.414029, mean_q: 4.444818
 32134/100000: episode: 515, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 69.207, mean reward: 3.296 [2.398, 6.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.610, 10.347], loss: 0.259136, mae: 0.428602, mean_q: 4.459947
 32147/100000: episode: 516, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 38.684, mean reward: 2.976 [2.290, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.499], loss: 0.161604, mae: 0.406747, mean_q: 4.449948
 32192/100000: episode: 517, duration: 0.272s, episode steps: 45, steps per second: 165, episode reward: 97.672, mean reward: 2.170 [1.532, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.366, 10.154], loss: 0.271971, mae: 0.427551, mean_q: 4.478141
[Info] 2-TH LEVEL FOUND: 8.088963508605957, Considering 14/86 traces
 32233/100000: episode: 518, duration: 5.308s, episode steps: 41, steps per second: 8, episode reward: 121.259, mean reward: 2.958 [1.600, 12.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.360, 10.166], loss: 0.341449, mae: 0.464722, mean_q: 4.511391
 32257/100000: episode: 519, duration: 0.153s, episode steps: 24, steps per second: 156, episode reward: 73.651, mean reward: 3.069 [2.015, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.526], loss: 0.164622, mae: 0.402162, mean_q: 4.443902
 32281/100000: episode: 520, duration: 0.168s, episode steps: 24, steps per second: 142, episode reward: 91.910, mean reward: 3.830 [2.483, 6.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.528], loss: 0.418706, mae: 0.491939, mean_q: 4.551297
 32305/100000: episode: 521, duration: 0.166s, episode steps: 24, steps per second: 144, episode reward: 71.013, mean reward: 2.959 [1.589, 7.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.492, 10.259], loss: 0.304310, mae: 0.457449, mean_q: 4.482250
 32329/100000: episode: 522, duration: 0.172s, episode steps: 24, steps per second: 140, episode reward: 61.095, mean reward: 2.546 [1.675, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.255], loss: 1.119373, mae: 0.473401, mean_q: 4.452219
 32353/100000: episode: 523, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 70.437, mean reward: 2.935 [1.795, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.524, 10.286], loss: 0.261495, mae: 0.487627, mean_q: 4.537609
 32377/100000: episode: 524, duration: 0.148s, episode steps: 24, steps per second: 163, episode reward: 75.828, mean reward: 3.159 [1.888, 5.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.736, 10.425], loss: 0.195670, mae: 0.426316, mean_q: 4.532256
 32404/100000: episode: 525, duration: 0.197s, episode steps: 27, steps per second: 137, episode reward: 256.964, mean reward: 9.517 [3.143, 26.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.716, 10.683], loss: 0.234797, mae: 0.394425, mean_q: 4.513278
 32428/100000: episode: 526, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 70.571, mean reward: 2.940 [2.213, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.168, 10.495], loss: 0.283062, mae: 0.469459, mean_q: 4.598759
 32452/100000: episode: 527, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 80.920, mean reward: 3.372 [2.619, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.614, 10.486], loss: 0.490936, mae: 0.529520, mean_q: 4.636446
 32476/100000: episode: 528, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 59.231, mean reward: 2.468 [1.719, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.324], loss: 0.424371, mae: 0.565449, mean_q: 4.695293
 32500/100000: episode: 529, duration: 0.155s, episode steps: 24, steps per second: 155, episode reward: 70.097, mean reward: 2.921 [2.169, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.985, 10.475], loss: 0.401405, mae: 0.504617, mean_q: 4.616832
 32527/100000: episode: 530, duration: 0.172s, episode steps: 27, steps per second: 157, episode reward: 106.643, mean reward: 3.950 [2.563, 7.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.556, 10.437], loss: 0.358596, mae: 0.525289, mean_q: 4.721899
 32551/100000: episode: 531, duration: 0.172s, episode steps: 24, steps per second: 139, episode reward: 91.517, mean reward: 3.813 [2.655, 10.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.980, 10.649], loss: 0.341915, mae: 0.506660, mean_q: 4.747999
 32575/100000: episode: 532, duration: 0.176s, episode steps: 24, steps per second: 136, episode reward: 80.994, mean reward: 3.375 [2.799, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.504], loss: 0.324510, mae: 0.459307, mean_q: 4.627533
 32599/100000: episode: 533, duration: 0.184s, episode steps: 24, steps per second: 130, episode reward: 75.875, mean reward: 3.161 [2.393, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.877, 10.492], loss: 0.634965, mae: 0.526992, mean_q: 4.828005
 32623/100000: episode: 534, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 85.279, mean reward: 3.553 [2.617, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.014, 10.451], loss: 0.332927, mae: 0.498714, mean_q: 4.682744
 32647/100000: episode: 535, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 79.890, mean reward: 3.329 [2.469, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.195, 10.426], loss: 0.419087, mae: 0.541127, mean_q: 4.716694
 32671/100000: episode: 536, duration: 0.156s, episode steps: 24, steps per second: 154, episode reward: 60.825, mean reward: 2.534 [2.038, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.115, 10.404], loss: 0.491951, mae: 0.560049, mean_q: 4.727839
 32695/100000: episode: 537, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 75.091, mean reward: 3.129 [2.647, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.462], loss: 0.381112, mae: 0.485666, mean_q: 4.574923
 32719/100000: episode: 538, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 63.242, mean reward: 2.635 [1.971, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.214, 10.354], loss: 0.400058, mae: 0.519200, mean_q: 4.837584
 32743/100000: episode: 539, duration: 0.173s, episode steps: 24, steps per second: 139, episode reward: 54.941, mean reward: 2.289 [1.736, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.563, 10.272], loss: 1.108886, mae: 0.485529, mean_q: 4.659117
 32767/100000: episode: 540, duration: 0.155s, episode steps: 24, steps per second: 155, episode reward: 71.254, mean reward: 2.969 [2.127, 5.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.843, 10.395], loss: 0.788080, mae: 0.664610, mean_q: 4.798034
 32791/100000: episode: 541, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 77.090, mean reward: 3.212 [2.492, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.894, 10.501], loss: 0.696206, mae: 0.615386, mean_q: 4.955706
 32818/100000: episode: 542, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 86.628, mean reward: 3.208 [2.232, 6.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.344], loss: 0.616492, mae: 0.527828, mean_q: 4.769327
 32842/100000: episode: 543, duration: 0.158s, episode steps: 24, steps per second: 152, episode reward: 74.771, mean reward: 3.115 [1.903, 8.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.397], loss: 0.688386, mae: 0.573862, mean_q: 4.854033
 32866/100000: episode: 544, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 72.691, mean reward: 3.029 [2.054, 5.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.386, 10.335], loss: 0.412317, mae: 0.501822, mean_q: 4.790786
 32895/100000: episode: 545, duration: 0.183s, episode steps: 29, steps per second: 159, episode reward: 91.014, mean reward: 3.138 [2.276, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.436, 10.444], loss: 1.009517, mae: 0.535181, mean_q: 4.933389
 32919/100000: episode: 546, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 60.473, mean reward: 2.520 [2.064, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.414], loss: 0.554472, mae: 0.602684, mean_q: 4.822948
 32943/100000: episode: 547, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 67.331, mean reward: 2.805 [2.461, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.448], loss: 1.227174, mae: 0.616590, mean_q: 4.955123
 32967/100000: episode: 548, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 75.903, mean reward: 3.163 [2.153, 7.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.601, 10.360], loss: 1.479327, mae: 0.862623, mean_q: 5.023736
 32991/100000: episode: 549, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 53.522, mean reward: 2.230 [1.745, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.435, 10.215], loss: 0.598340, mae: 0.627921, mean_q: 4.900224
 33015/100000: episode: 550, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 71.997, mean reward: 3.000 [2.074, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.317], loss: 0.800419, mae: 0.649206, mean_q: 5.064741
 33039/100000: episode: 551, duration: 0.195s, episode steps: 24, steps per second: 123, episode reward: 64.553, mean reward: 2.690 [1.768, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.764, 10.330], loss: 1.375029, mae: 0.595561, mean_q: 4.961433
 33068/100000: episode: 552, duration: 0.228s, episode steps: 29, steps per second: 127, episode reward: 94.319, mean reward: 3.252 [1.986, 9.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.411, 10.310], loss: 0.465145, mae: 0.581905, mean_q: 4.915040
 33097/100000: episode: 553, duration: 0.176s, episode steps: 29, steps per second: 164, episode reward: 89.690, mean reward: 3.093 [1.674, 7.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.539, 10.279], loss: 0.563051, mae: 0.535619, mean_q: 4.886439
 33121/100000: episode: 554, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 93.613, mean reward: 3.901 [2.599, 5.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.078, 10.558], loss: 0.606726, mae: 0.593455, mean_q: 5.020042
 33145/100000: episode: 555, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 90.001, mean reward: 3.750 [2.166, 7.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.379, 10.409], loss: 0.428492, mae: 0.533943, mean_q: 4.915996
 33169/100000: episode: 556, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 77.681, mean reward: 3.237 [2.064, 5.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.962, 10.306], loss: 1.223654, mae: 0.595881, mean_q: 5.006221
 33193/100000: episode: 557, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 169.150, mean reward: 7.048 [2.577, 21.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.436, 10.702], loss: 0.387112, mae: 0.519825, mean_q: 4.876228
 33217/100000: episode: 558, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 70.513, mean reward: 2.938 [2.535, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.778, 10.450], loss: 0.663916, mae: 0.608037, mean_q: 5.012286
 33241/100000: episode: 559, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 56.882, mean reward: 2.370 [1.769, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.359], loss: 0.260573, mae: 0.486113, mean_q: 5.060892
 33265/100000: episode: 560, duration: 0.158s, episode steps: 24, steps per second: 152, episode reward: 53.073, mean reward: 2.211 [1.644, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.569, 10.268], loss: 0.359663, mae: 0.507023, mean_q: 5.016502
 33289/100000: episode: 561, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 61.051, mean reward: 2.544 [1.727, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.308], loss: 0.811712, mae: 0.601777, mean_q: 5.070586
 33313/100000: episode: 562, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 68.779, mean reward: 2.866 [2.269, 4.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.587, 10.405], loss: 0.653587, mae: 0.583329, mean_q: 5.035061
 33337/100000: episode: 563, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 146.183, mean reward: 6.091 [2.537, 12.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.079, 10.492], loss: 1.318870, mae: 0.576240, mean_q: 4.987528
 33361/100000: episode: 564, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 65.035, mean reward: 2.710 [1.939, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.311], loss: 2.339310, mae: 0.771513, mean_q: 5.013254
 33385/100000: episode: 565, duration: 0.210s, episode steps: 24, steps per second: 114, episode reward: 96.034, mean reward: 4.001 [2.453, 9.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.131, 10.498], loss: 1.266745, mae: 0.686182, mean_q: 5.080420
 33414/100000: episode: 566, duration: 0.210s, episode steps: 29, steps per second: 138, episode reward: 118.928, mean reward: 4.101 [2.229, 15.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.643, 10.431], loss: 0.816520, mae: 0.716067, mean_q: 5.266089
 33438/100000: episode: 567, duration: 0.169s, episode steps: 24, steps per second: 142, episode reward: 69.020, mean reward: 2.876 [1.999, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.533, 10.266], loss: 0.455454, mae: 0.591052, mean_q: 5.127676
 33462/100000: episode: 568, duration: 0.192s, episode steps: 24, steps per second: 125, episode reward: 72.689, mean reward: 3.029 [2.126, 4.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.028, 10.387], loss: 0.499968, mae: 0.590793, mean_q: 5.142526
 33486/100000: episode: 569, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 68.259, mean reward: 2.844 [1.561, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.137, 10.265], loss: 0.601034, mae: 0.555542, mean_q: 5.087982
 33510/100000: episode: 570, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 83.588, mean reward: 3.483 [2.417, 7.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.076, 10.450], loss: 0.561281, mae: 0.558155, mean_q: 5.183259
 33534/100000: episode: 571, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 87.776, mean reward: 3.657 [2.918, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.452], loss: 0.485980, mae: 0.591286, mean_q: 5.115821
 33558/100000: episode: 572, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 64.616, mean reward: 2.692 [1.799, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.373, 10.404], loss: 0.631849, mae: 0.603704, mean_q: 5.169820
 33582/100000: episode: 573, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 63.443, mean reward: 2.643 [1.882, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.355], loss: 1.490320, mae: 0.724705, mean_q: 5.199987
 33606/100000: episode: 574, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 66.073, mean reward: 2.753 [1.799, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.311, 10.332], loss: 1.571538, mae: 0.782376, mean_q: 5.234677
 33630/100000: episode: 575, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 72.108, mean reward: 3.004 [1.894, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.323], loss: 0.883440, mae: 0.724787, mean_q: 5.199978
 33654/100000: episode: 576, duration: 0.147s, episode steps: 24, steps per second: 164, episode reward: 53.544, mean reward: 2.231 [1.825, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.066, 10.304], loss: 0.661451, mae: 0.679901, mean_q: 5.148048
 33678/100000: episode: 577, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 91.102, mean reward: 3.796 [2.502, 6.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.578, 10.478], loss: 1.342583, mae: 0.648540, mean_q: 5.265888
 33702/100000: episode: 578, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 95.747, mean reward: 3.989 [2.332, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.335, 10.464], loss: 0.444422, mae: 0.596151, mean_q: 5.216296
 33726/100000: episode: 579, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 92.694, mean reward: 3.862 [2.379, 5.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.354], loss: 0.676080, mae: 0.699066, mean_q: 5.403049
 33750/100000: episode: 580, duration: 0.151s, episode steps: 24, steps per second: 158, episode reward: 74.723, mean reward: 3.113 [1.953, 5.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.362, 10.377], loss: 0.745815, mae: 0.683002, mean_q: 5.353157
 33774/100000: episode: 581, duration: 0.156s, episode steps: 24, steps per second: 154, episode reward: 73.170, mean reward: 3.049 [2.186, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.687, 10.453], loss: 1.095131, mae: 0.761158, mean_q: 5.278805
 33798/100000: episode: 582, duration: 0.166s, episode steps: 24, steps per second: 144, episode reward: 67.541, mean reward: 2.814 [2.392, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.771, 10.488], loss: 0.413876, mae: 0.562116, mean_q: 5.253853
 33822/100000: episode: 583, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 75.848, mean reward: 3.160 [2.087, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.322, 10.360], loss: 0.611056, mae: 0.574308, mean_q: 5.283556
 33846/100000: episode: 584, duration: 0.158s, episode steps: 24, steps per second: 152, episode reward: 71.939, mean reward: 2.997 [2.388, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.433], loss: 0.632305, mae: 0.673988, mean_q: 5.341858
 33870/100000: episode: 585, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 172.949, mean reward: 7.206 [2.818, 23.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.193, 10.512], loss: 0.645734, mae: 0.645008, mean_q: 5.485791
 33894/100000: episode: 586, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 56.845, mean reward: 2.369 [1.897, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.403, 10.404], loss: 0.465163, mae: 0.570766, mean_q: 5.316556
 33918/100000: episode: 587, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 49.036, mean reward: 2.043 [1.533, 2.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.058, 10.193], loss: 0.741600, mae: 0.626947, mean_q: 5.416955
 33942/100000: episode: 588, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 71.402, mean reward: 2.975 [2.173, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.920, 10.374], loss: 1.745461, mae: 0.821592, mean_q: 5.516174
 33966/100000: episode: 589, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 85.925, mean reward: 3.580 [2.682, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.478], loss: 0.626261, mae: 0.721055, mean_q: 5.381056
 33990/100000: episode: 590, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 69.108, mean reward: 2.880 [2.223, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.373], loss: 0.433404, mae: 0.591395, mean_q: 5.244971
 34014/100000: episode: 591, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 58.235, mean reward: 2.426 [1.703, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.949, 10.281], loss: 0.720839, mae: 0.692581, mean_q: 5.402669
 34038/100000: episode: 592, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 82.588, mean reward: 3.441 [2.490, 5.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.315, 10.520], loss: 0.665922, mae: 0.655582, mean_q: 5.341869
 34062/100000: episode: 593, duration: 0.163s, episode steps: 24, steps per second: 148, episode reward: 71.008, mean reward: 2.959 [2.400, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.463, 10.415], loss: 1.321549, mae: 0.641180, mean_q: 5.376971
 34086/100000: episode: 594, duration: 0.155s, episode steps: 24, steps per second: 155, episode reward: 79.234, mean reward: 3.301 [2.804, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.378, 10.429], loss: 1.075642, mae: 0.864440, mean_q: 5.594764
 34110/100000: episode: 595, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 70.113, mean reward: 2.921 [1.692, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.263], loss: 0.470395, mae: 0.644054, mean_q: 5.415340
 34134/100000: episode: 596, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 68.850, mean reward: 2.869 [1.874, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.611, 10.400], loss: 0.738694, mae: 0.663025, mean_q: 5.549347
 34158/100000: episode: 597, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 91.749, mean reward: 3.823 [2.387, 5.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.453], loss: 1.153589, mae: 0.643648, mean_q: 5.354807
 34182/100000: episode: 598, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 71.776, mean reward: 2.991 [2.222, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.498, 10.398], loss: 0.553912, mae: 0.656696, mean_q: 5.497734
 34206/100000: episode: 599, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 72.461, mean reward: 3.019 [2.352, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.382], loss: 0.379384, mae: 0.534200, mean_q: 5.297376
 34230/100000: episode: 600, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 66.719, mean reward: 2.780 [2.255, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.471], loss: 0.693002, mae: 0.659887, mean_q: 5.543427
 34254/100000: episode: 601, duration: 0.171s, episode steps: 24, steps per second: 141, episode reward: 85.372, mean reward: 3.557 [2.534, 6.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.508], loss: 0.832470, mae: 0.687557, mean_q: 5.414404
 34281/100000: episode: 602, duration: 0.165s, episode steps: 27, steps per second: 163, episode reward: 111.745, mean reward: 4.139 [2.828, 6.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.285, 10.635], loss: 0.479943, mae: 0.605062, mean_q: 5.487583
 34305/100000: episode: 603, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 66.031, mean reward: 2.751 [2.063, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.425, 10.380], loss: 0.635015, mae: 0.640108, mean_q: 5.467332
[Info] 3-TH LEVEL FOUND: 9.706764221191406, Considering 10/90 traces
 34329/100000: episode: 604, duration: 5.704s, episode steps: 24, steps per second: 4, episode reward: 77.522, mean reward: 3.230 [2.294, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.914, 10.585], loss: 1.729945, mae: 0.775732, mean_q: 5.608874
 34354/100000: episode: 605, duration: 0.193s, episode steps: 25, steps per second: 130, episode reward: 115.469, mean reward: 4.619 [2.739, 6.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.070, 10.493], loss: 0.951164, mae: 0.725633, mean_q: 5.513378
 34379/100000: episode: 606, duration: 0.234s, episode steps: 25, steps per second: 107, episode reward: 150.204, mean reward: 6.008 [3.077, 10.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.430, 10.384], loss: 1.348173, mae: 0.696567, mean_q: 5.544786
 34386/100000: episode: 607, duration: 0.081s, episode steps: 7, steps per second: 86, episode reward: 42.877, mean reward: 6.125 [4.696, 7.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.662], loss: 2.123217, mae: 0.992069, mean_q: 5.723298
 34401/100000: episode: 608, duration: 0.163s, episode steps: 15, steps per second: 92, episode reward: 67.118, mean reward: 4.475 [2.909, 8.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.454, 10.565], loss: 0.523198, mae: 0.687083, mean_q: 5.497534
 34408/100000: episode: 609, duration: 0.068s, episode steps: 7, steps per second: 103, episode reward: 89.828, mean reward: 12.833 [6.121, 31.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.365, 10.624], loss: 0.522430, mae: 0.571911, mean_q: 5.387805
 34433/100000: episode: 610, duration: 0.171s, episode steps: 25, steps per second: 146, episode reward: 105.935, mean reward: 4.237 [2.129, 8.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.331], loss: 0.664130, mae: 0.627717, mean_q: 5.520970
 34458/100000: episode: 611, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 130.096, mean reward: 5.204 [3.517, 11.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.277, 10.473], loss: 0.730332, mae: 0.678028, mean_q: 5.590662
 34483/100000: episode: 612, duration: 0.164s, episode steps: 25, steps per second: 152, episode reward: 118.044, mean reward: 4.722 [3.037, 8.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.610], loss: 0.991283, mae: 0.766377, mean_q: 5.565639
 34508/100000: episode: 613, duration: 0.158s, episode steps: 25, steps per second: 159, episode reward: 104.462, mean reward: 4.178 [2.898, 6.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.598], loss: 0.701376, mae: 0.699017, mean_q: 5.839320
 34512/100000: episode: 614, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 16.115, mean reward: 4.029 [2.720, 5.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.379, 10.453], loss: 1.339471, mae: 1.011825, mean_q: 6.140296
 34537/100000: episode: 615, duration: 0.165s, episode steps: 25, steps per second: 152, episode reward: 110.832, mean reward: 4.433 [2.615, 7.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.382, 10.294], loss: 1.172182, mae: 0.711200, mean_q: 5.705512
 34553/100000: episode: 616, duration: 0.100s, episode steps: 16, steps per second: 161, episode reward: 41.088, mean reward: 2.568 [2.148, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.412], loss: 0.501568, mae: 0.727707, mean_q: 5.584845
 34568/100000: episode: 617, duration: 0.103s, episode steps: 15, steps per second: 146, episode reward: 77.224, mean reward: 5.148 [3.426, 8.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.535], loss: 0.460518, mae: 0.611944, mean_q: 5.633428
 34574/100000: episode: 618, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 27.772, mean reward: 4.629 [4.016, 5.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.617], loss: 0.364517, mae: 0.589420, mean_q: 5.598178
 34581/100000: episode: 619, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 52.642, mean reward: 7.520 [5.526, 9.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.760, 10.675], loss: 0.545436, mae: 0.578930, mean_q: 5.798344
 34588/100000: episode: 620, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 46.827, mean reward: 6.690 [4.527, 9.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.663], loss: 1.182291, mae: 0.699945, mean_q: 5.891545
 34595/100000: episode: 621, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 84.485, mean reward: 12.069 [7.580, 28.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.722], loss: 1.870693, mae: 0.994705, mean_q: 6.336259
 34620/100000: episode: 622, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 229.801, mean reward: 9.192 [4.588, 24.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.543, 10.615], loss: 0.580823, mae: 0.704697, mean_q: 5.659761
 34636/100000: episode: 623, duration: 0.109s, episode steps: 16, steps per second: 146, episode reward: 111.857, mean reward: 6.991 [5.081, 11.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.068, 10.597], loss: 1.118629, mae: 0.730935, mean_q: 5.677847
 34650/100000: episode: 624, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 64.576, mean reward: 4.613 [3.170, 6.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.567, 10.521], loss: 0.399264, mae: 0.656872, mean_q: 5.819806
 34665/100000: episode: 625, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 71.654, mean reward: 4.777 [3.264, 12.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.540], loss: 0.622672, mae: 0.652258, mean_q: 5.631427
 34669/100000: episode: 626, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 30.970, mean reward: 7.742 [5.078, 14.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.583, 10.378], loss: 0.337552, mae: 0.516729, mean_q: 5.668962
 34676/100000: episode: 627, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 36.562, mean reward: 5.223 [4.130, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.567], loss: 2.944222, mae: 1.037052, mean_q: 6.236578
 34692/100000: episode: 628, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 63.635, mean reward: 3.977 [2.870, 5.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.620, 10.471], loss: 1.219012, mae: 0.943508, mean_q: 5.796575
 34717/100000: episode: 629, duration: 0.158s, episode steps: 25, steps per second: 158, episode reward: 99.440, mean reward: 3.978 [2.529, 6.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.333, 10.379], loss: 1.281976, mae: 0.900837, mean_q: 5.729942
 34724/100000: episode: 630, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 45.529, mean reward: 6.504 [5.371, 7.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.598], loss: 0.913098, mae: 0.943702, mean_q: 6.240179
 34749/100000: episode: 631, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 261.337, mean reward: 10.453 [2.391, 109.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.514, 10.360], loss: 1.334787, mae: 0.802015, mean_q: 5.759878
 34774/100000: episode: 632, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 80.041, mean reward: 3.202 [2.072, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.457, 10.348], loss: 0.905397, mae: 0.728610, mean_q: 5.915937
 34781/100000: episode: 633, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 36.799, mean reward: 5.257 [3.545, 6.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.481, 10.546], loss: 1.092154, mae: 0.663076, mean_q: 5.880993
 34795/100000: episode: 634, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 75.344, mean reward: 5.382 [3.449, 7.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.521, 10.504], loss: 13.923399, mae: 1.179192, mean_q: 6.240196
 34799/100000: episode: 635, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 14.834, mean reward: 3.709 [3.076, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.454], loss: 0.594020, mae: 0.772941, mean_q: 5.799787
 34815/100000: episode: 636, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 60.578, mean reward: 3.786 [2.761, 5.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-1.329, 10.497], loss: 2.851001, mae: 0.969815, mean_q: 5.861663
 34840/100000: episode: 637, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 103.127, mean reward: 4.125 [2.370, 7.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.515, 10.424], loss: 1.535653, mae: 0.756247, mean_q: 6.084285
 34865/100000: episode: 638, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 109.076, mean reward: 4.363 [2.765, 6.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.268, 10.562], loss: 8.072695, mae: 1.107629, mean_q: 6.210820
 34872/100000: episode: 639, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 45.735, mean reward: 6.534 [4.521, 9.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.661], loss: 0.786655, mae: 0.924787, mean_q: 5.179684
 34888/100000: episode: 640, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 91.514, mean reward: 5.720 [4.481, 9.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.211, 10.590], loss: 1.060464, mae: 0.821934, mean_q: 6.070389
 34892/100000: episode: 641, duration: 0.033s, episode steps: 4, steps per second: 120, episode reward: 23.691, mean reward: 5.923 [5.160, 6.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.125, 10.570], loss: 0.706812, mae: 0.742053, mean_q: 5.913074
 34908/100000: episode: 642, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 59.624, mean reward: 3.727 [3.170, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.526], loss: 1.893051, mae: 0.926312, mean_q: 6.314468
 34912/100000: episode: 643, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 43.445, mean reward: 10.861 [7.290, 17.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.688], loss: 0.768222, mae: 0.951340, mean_q: 6.546373
 34937/100000: episode: 644, duration: 0.160s, episode steps: 25, steps per second: 156, episode reward: 133.008, mean reward: 5.320 [3.543, 9.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.491, 10.605], loss: 3.197526, mae: 0.896394, mean_q: 5.956399
 34952/100000: episode: 645, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 130.069, mean reward: 8.671 [3.817, 35.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.077, 10.758], loss: 0.832644, mae: 0.822098, mean_q: 5.950483
 34966/100000: episode: 646, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 63.376, mean reward: 4.527 [3.030, 7.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.522], loss: 0.707353, mae: 0.783197, mean_q: 6.192246
 34991/100000: episode: 647, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 187.427, mean reward: 7.497 [3.619, 14.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.494, 10.556], loss: 1.851521, mae: 0.874295, mean_q: 6.052612
 35006/100000: episode: 648, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 168.082, mean reward: 11.205 [4.761, 27.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.499], loss: 1.337430, mae: 1.075096, mean_q: 6.373895
 35010/100000: episode: 649, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 63.544, mean reward: 15.886 [5.447, 28.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.357, 10.455], loss: 1.259245, mae: 0.963454, mean_q: 6.360260
 35035/100000: episode: 650, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 112.830, mean reward: 4.513 [1.728, 13.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.268, 10.280], loss: 2.117321, mae: 0.908181, mean_q: 6.147963
 35060/100000: episode: 651, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 101.127, mean reward: 4.045 [2.559, 7.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.618, 10.494], loss: 1.965295, mae: 0.975881, mean_q: 6.309645
[Info] FALSIFICATION!
 35067/100000: episode: 652, duration: 0.478s, episode steps: 7, steps per second: 15, episode reward: 1047.768, mean reward: 149.681 [7.250, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.017, 10.655], loss: 2.836620, mae: 1.084748, mean_q: 6.616669
 35071/100000: episode: 653, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: 17.472, mean reward: 4.368 [3.814, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.401], loss: 0.968316, mae: 0.769571, mean_q: 5.935377
 35077/100000: episode: 654, duration: 0.049s, episode steps: 6, steps per second: 123, episode reward: 26.271, mean reward: 4.378 [3.502, 5.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-1.491, 10.446], loss: 0.676827, mae: 0.720345, mean_q: 5.990569
 35102/100000: episode: 655, duration: 0.155s, episode steps: 25, steps per second: 161, episode reward: 89.077, mean reward: 3.563 [2.156, 5.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.415], loss: 0.761351, mae: 0.774984, mean_q: 6.148522
 35108/100000: episode: 656, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 20.272, mean reward: 3.379 [2.955, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.461], loss: 1.188829, mae: 0.788278, mean_q: 6.308194
 35123/100000: episode: 657, duration: 0.108s, episode steps: 15, steps per second: 139, episode reward: 67.129, mean reward: 4.475 [3.533, 5.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.563], loss: 1030.981689, mae: 5.797072, mean_q: 9.326723
 35148/100000: episode: 658, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 84.483, mean reward: 3.379 [2.024, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.739, 10.386], loss: 22.329189, mae: 2.972094, mean_q: 4.944423
 35152/100000: episode: 659, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 40.519, mean reward: 10.130 [6.002, 16.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.533], loss: 4.015143, mae: 2.410299, mean_q: 7.607707
 35159/100000: episode: 660, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 40.028, mean reward: 5.718 [3.557, 8.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.809, 10.483], loss: 1.630261, mae: 1.421671, mean_q: 6.473534
 35166/100000: episode: 661, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 35.148, mean reward: 5.021 [4.135, 6.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.522], loss: 1.834234, mae: 1.210232, mean_q: 5.843569
 35173/100000: episode: 662, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 56.810, mean reward: 8.116 [6.755, 9.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.200, 10.652], loss: 2184.293945, mae: 5.704469, mean_q: 6.352316
 35188/100000: episode: 663, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 110.697, mean reward: 7.380 [4.869, 14.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.132, 10.538], loss: 1035.597168, mae: 8.062922, mean_q: 10.541449
 35213/100000: episode: 664, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: 137.794, mean reward: 5.512 [3.150, 13.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.040, 10.684], loss: 5.785544, mae: 2.349589, mean_q: 4.998969
 35220/100000: episode: 665, duration: 0.058s, episode steps: 7, steps per second: 120, episode reward: 63.408, mean reward: 9.058 [4.816, 12.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.283, 10.699], loss: 4.734296, mae: 2.174342, mean_q: 7.200725
 35227/100000: episode: 666, duration: 0.062s, episode steps: 7, steps per second: 114, episode reward: 58.521, mean reward: 8.360 [6.507, 10.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.617], loss: 3.584004, mae: 1.722012, mean_q: 6.792192
 35252/100000: episode: 667, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 109.450, mean reward: 4.378 [2.772, 5.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.061, 10.408], loss: 2.798718, mae: 1.150298, mean_q: 6.114736
 35258/100000: episode: 668, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 31.441, mean reward: 5.240 [4.373, 6.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.257, 10.543], loss: 30.022270, mae: 1.590847, mean_q: 6.411659
 35283/100000: episode: 669, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 90.354, mean reward: 3.614 [2.351, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.470], loss: 617.935913, mae: 2.937993, mean_q: 7.385983
 35308/100000: episode: 670, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 98.883, mean reward: 3.955 [2.620, 8.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.149, 10.433], loss: 9.012209, mae: 1.331484, mean_q: 6.544501
 35312/100000: episode: 671, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 15.693, mean reward: 3.923 [3.442, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.554], loss: 1.451613, mae: 1.364609, mean_q: 7.846256
 35319/100000: episode: 672, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 79.392, mean reward: 11.342 [7.668, 16.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.587], loss: 2.285616, mae: 1.446496, mean_q: 7.420515
 35344/100000: episode: 673, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 123.092, mean reward: 4.924 [3.169, 6.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.526, 10.633], loss: 1.275891, mae: 0.964306, mean_q: 6.507170
 35359/100000: episode: 674, duration: 0.097s, episode steps: 15, steps per second: 154, episode reward: 197.045, mean reward: 13.136 [4.888, 65.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.669, 10.504], loss: 1022.241882, mae: 3.166330, mean_q: 6.954150
 35366/100000: episode: 675, duration: 0.052s, episode steps: 7, steps per second: 136, episode reward: 65.663, mean reward: 9.380 [7.174, 12.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.685], loss: 16.990313, mae: 5.173614, mean_q: 11.924940
 35391/100000: episode: 676, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 109.853, mean reward: 4.394 [2.235, 11.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.495, 10.428], loss: 3.448087, mae: 1.559710, mean_q: 6.684926
 35397/100000: episode: 677, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 40.645, mean reward: 6.774 [4.352, 9.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.339], loss: 5.143999, mae: 2.201334, mean_q: 8.655014
 35413/100000: episode: 678, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 99.476, mean reward: 6.217 [4.109, 10.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.647], loss: 4.618857, mae: 2.324563, mean_q: 8.460276
 35419/100000: episode: 679, duration: 0.044s, episode steps: 6, steps per second: 135, episode reward: 22.691, mean reward: 3.782 [3.318, 4.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.572, 10.460], loss: 3.391064, mae: 1.630419, mean_q: 5.890005
 35425/100000: episode: 680, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 21.436, mean reward: 3.573 [3.341, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.507], loss: 4.257475, mae: 2.159121, mean_q: 8.491405
 35431/100000: episode: 681, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 24.482, mean reward: 4.080 [3.583, 5.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.497], loss: 12.094670, mae: 4.322854, mean_q: 11.154290
 35435/100000: episode: 682, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 44.229, mean reward: 11.057 [8.607, 14.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.566], loss: 3.159577, mae: 2.020045, mean_q: 8.653024
 35451/100000: episode: 683, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 125.323, mean reward: 7.833 [4.108, 27.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.595, 10.683], loss: 2.038630, mae: 1.274301, mean_q: 6.496968
 35476/100000: episode: 684, duration: 0.170s, episode steps: 25, steps per second: 147, episode reward: 98.522, mean reward: 3.941 [3.043, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.043, 10.417], loss: 3.476083, mae: 1.386542, mean_q: 7.341913
 35492/100000: episode: 685, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 72.936, mean reward: 4.559 [3.561, 12.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.472], loss: 8.402154, mae: 1.443219, mean_q: 7.683416
 35517/100000: episode: 686, duration: 0.167s, episode steps: 25, steps per second: 150, episode reward: 100.403, mean reward: 4.016 [2.834, 6.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.578, 10.407], loss: 612.650513, mae: 3.452067, mean_q: 8.803733
 35532/100000: episode: 687, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 105.152, mean reward: 7.010 [4.319, 11.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.532, 10.502], loss: 2.950443, mae: 1.436660, mean_q: 6.246985
 35547/100000: episode: 688, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 169.896, mean reward: 11.326 [5.347, 25.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.684], loss: 4.728209, mae: 1.187551, mean_q: 7.067882
 35572/100000: episode: 689, duration: 0.165s, episode steps: 25, steps per second: 152, episode reward: 161.475, mean reward: 6.459 [3.228, 18.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.336, 10.628], loss: 2.407125, mae: 1.126703, mean_q: 7.369734
 35578/100000: episode: 690, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 21.593, mean reward: 3.599 [3.218, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.514], loss: 1.214944, mae: 0.962483, mean_q: 7.268089
 35603/100000: episode: 691, duration: 0.162s, episode steps: 25, steps per second: 154, episode reward: 95.032, mean reward: 3.801 [2.279, 6.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.686, 10.404], loss: 8.486305, mae: 1.201634, mean_q: 7.263379
 35607/100000: episode: 692, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 19.510, mean reward: 4.877 [3.212, 6.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.392], loss: 2.488780, mae: 1.299758, mean_q: 7.548197
 35614/100000: episode: 693, duration: 0.069s, episode steps: 7, steps per second: 102, episode reward: 35.242, mean reward: 5.035 [3.155, 8.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.431], loss: 1.727690, mae: 1.168932, mean_q: 7.349475
[Info] Complete ISplit Iteration
[Info] Levels: [5.239614, 8.0889635, 9.706764, 14.1280575]
[Info] Cond. Prob: [0.1, 0.14, 0.1, 0.08]
[Info] Error Prob: 0.00011200000000000001

 35628/100000: episode: 694, duration: 4.569s, episode steps: 14, steps per second: 3, episode reward: 149.138, mean reward: 10.653 [5.852, 19.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.168, 10.609], loss: 1.597661, mae: 1.058054, mean_q: 7.305493
 35728/100000: episode: 695, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 198.243, mean reward: 1.982 [1.471, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.172], loss: 155.906479, mae: 1.578833, mean_q: 7.387158
 35828/100000: episode: 696, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 184.594, mean reward: 1.846 [1.484, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.107, 10.204], loss: 158.520569, mae: 1.950198, mean_q: 7.647874
 35928/100000: episode: 697, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 186.946, mean reward: 1.869 [1.463, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.027, 10.098], loss: 2.063386, mae: 1.083868, mean_q: 7.123380
 36028/100000: episode: 698, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 185.246, mean reward: 1.852 [1.453, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.477, 10.098], loss: 308.430634, mae: 2.535119, mean_q: 7.880461
 36128/100000: episode: 699, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 189.362, mean reward: 1.894 [1.445, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.336, 10.165], loss: 155.959106, mae: 1.869002, mean_q: 7.560599
 36228/100000: episode: 700, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 234.063, mean reward: 2.341 [1.598, 7.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.840, 10.098], loss: 157.307587, mae: 1.821988, mean_q: 7.627137
 36328/100000: episode: 701, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 194.017, mean reward: 1.940 [1.458, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.090, 10.189], loss: 6.275409, mae: 1.256668, mean_q: 7.259203
 36428/100000: episode: 702, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 197.619, mean reward: 1.976 [1.525, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.988, 10.098], loss: 3.769501, mae: 1.109418, mean_q: 7.166572
 36528/100000: episode: 703, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 227.891, mean reward: 2.279 [1.502, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.799, 10.318], loss: 1.954914, mae: 1.056175, mean_q: 7.044864
 36628/100000: episode: 704, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 187.591, mean reward: 1.876 [1.460, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.826, 10.098], loss: 310.337128, mae: 2.480502, mean_q: 7.666334
 36728/100000: episode: 705, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 184.751, mean reward: 1.848 [1.445, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.118], loss: 6.287754, mae: 1.247621, mean_q: 7.072362
 36828/100000: episode: 706, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 180.119, mean reward: 1.801 [1.445, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.177, 10.155], loss: 2.235396, mae: 1.025349, mean_q: 7.043787
 36928/100000: episode: 707, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 198.816, mean reward: 1.988 [1.439, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.816, 10.283], loss: 308.317993, mae: 2.491078, mean_q: 7.463649
 37028/100000: episode: 708, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 187.470, mean reward: 1.875 [1.516, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.343, 10.261], loss: 5.369150, mae: 1.205050, mean_q: 7.145895
 37128/100000: episode: 709, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 299.066, mean reward: 2.991 [1.498, 8.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.717, 10.592], loss: 154.771439, mae: 1.660425, mean_q: 7.130445
 37228/100000: episode: 710, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 178.251, mean reward: 1.783 [1.460, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.128, 10.102], loss: 2.504157, mae: 1.029435, mean_q: 6.901448
 37328/100000: episode: 711, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 186.730, mean reward: 1.867 [1.452, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.823, 10.134], loss: 159.448883, mae: 1.780435, mean_q: 7.246304
 37428/100000: episode: 712, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 195.454, mean reward: 1.955 [1.477, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.559, 10.237], loss: 2.242327, mae: 0.977342, mean_q: 6.664019
 37528/100000: episode: 713, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 175.954, mean reward: 1.760 [1.470, 2.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.229], loss: 2.347090, mae: 0.967866, mean_q: 6.614027
 37628/100000: episode: 714, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 182.117, mean reward: 1.821 [1.446, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.952, 10.153], loss: 157.409164, mae: 1.701841, mean_q: 6.921055
 37728/100000: episode: 715, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 180.112, mean reward: 1.801 [1.493, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.392, 10.098], loss: 155.344635, mae: 1.736326, mean_q: 6.996705
 37828/100000: episode: 716, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 199.010, mean reward: 1.990 [1.443, 5.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.652, 10.242], loss: 1.444676, mae: 0.846523, mean_q: 6.397657
 37928/100000: episode: 717, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 199.887, mean reward: 1.999 [1.529, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.348, 10.279], loss: 1.687875, mae: 0.883848, mean_q: 6.373177
 38028/100000: episode: 718, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 218.970, mean reward: 2.190 [1.454, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.283, 10.098], loss: 5.341922, mae: 0.952129, mean_q: 6.382146
 38128/100000: episode: 719, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 187.781, mean reward: 1.878 [1.456, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.098], loss: 1.590200, mae: 0.831053, mean_q: 6.164620
 38228/100000: episode: 720, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 191.024, mean reward: 1.910 [1.492, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.243], loss: 154.467575, mae: 1.475546, mean_q: 6.389541
 38328/100000: episode: 721, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 188.110, mean reward: 1.881 [1.444, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.483, 10.098], loss: 2.843133, mae: 0.934494, mean_q: 6.222373
 38428/100000: episode: 722, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 186.107, mean reward: 1.861 [1.477, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.342, 10.098], loss: 4.279071, mae: 0.896746, mean_q: 6.217145
 38528/100000: episode: 723, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 195.990, mean reward: 1.960 [1.493, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.347, 10.203], loss: 154.658752, mae: 1.472915, mean_q: 6.471205
 38628/100000: episode: 724, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 181.087, mean reward: 1.811 [1.444, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.098], loss: 4.155176, mae: 0.941487, mean_q: 5.966080
 38728/100000: episode: 725, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 191.304, mean reward: 1.913 [1.450, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.531, 10.316], loss: 157.620026, mae: 1.524813, mean_q: 6.424292
 38828/100000: episode: 726, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 190.690, mean reward: 1.907 [1.446, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.782, 10.237], loss: 157.501373, mae: 1.639811, mean_q: 6.284296
 38928/100000: episode: 727, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 204.579, mean reward: 2.046 [1.468, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.810, 10.386], loss: 3.643843, mae: 0.832726, mean_q: 5.930259
 39028/100000: episode: 728, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 195.273, mean reward: 1.953 [1.466, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.127, 10.151], loss: 1.016919, mae: 0.692077, mean_q: 5.754943
 39128/100000: episode: 729, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 202.320, mean reward: 2.023 [1.447, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.333, 10.098], loss: 156.529968, mae: 1.486096, mean_q: 6.090681
 39228/100000: episode: 730, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 192.926, mean reward: 1.929 [1.470, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.373, 10.098], loss: 308.357117, mae: 2.020069, mean_q: 6.398267
 39328/100000: episode: 731, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 191.840, mean reward: 1.918 [1.475, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.824, 10.098], loss: 1.081584, mae: 0.689483, mean_q: 5.560740
 39428/100000: episode: 732, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 212.836, mean reward: 2.128 [1.504, 6.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.368, 10.109], loss: 157.434479, mae: 1.384016, mean_q: 5.726837
 39528/100000: episode: 733, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 178.633, mean reward: 1.786 [1.483, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.171, 10.272], loss: 0.997134, mae: 0.679176, mean_q: 5.379650
 39628/100000: episode: 734, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 210.537, mean reward: 2.105 [1.485, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.262], loss: 152.717300, mae: 1.076875, mean_q: 5.352981
 39728/100000: episode: 735, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 190.772, mean reward: 1.908 [1.444, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.396, 10.098], loss: 2.580595, mae: 0.813850, mean_q: 5.274378
 39828/100000: episode: 736, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 196.753, mean reward: 1.968 [1.505, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.048, 10.176], loss: 0.841780, mae: 0.576423, mean_q: 5.041856
 39928/100000: episode: 737, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 175.184, mean reward: 1.752 [1.464, 2.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.121, 10.098], loss: 0.894945, mae: 0.555388, mean_q: 4.831585
 40028/100000: episode: 738, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 203.318, mean reward: 2.033 [1.448, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.571, 10.098], loss: 2.107593, mae: 0.535602, mean_q: 4.608209
 40128/100000: episode: 739, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 202.915, mean reward: 2.029 [1.460, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.495, 10.236], loss: 1.674414, mae: 0.554396, mean_q: 4.693870
 40228/100000: episode: 740, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 190.677, mean reward: 1.907 [1.445, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.281, 10.098], loss: 0.579776, mae: 0.480004, mean_q: 4.507631
 40328/100000: episode: 741, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 182.174, mean reward: 1.822 [1.451, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.896, 10.171], loss: 0.809950, mae: 0.412668, mean_q: 4.353167
 40428/100000: episode: 742, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 227.717, mean reward: 2.277 [1.440, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.553, 10.187], loss: 0.287220, mae: 0.363009, mean_q: 4.126709
 40528/100000: episode: 743, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 184.090, mean reward: 1.841 [1.440, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.704, 10.132], loss: 0.167844, mae: 0.336636, mean_q: 3.998569
 40628/100000: episode: 744, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 180.093, mean reward: 1.801 [1.463, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.056, 10.198], loss: 0.111029, mae: 0.315876, mean_q: 3.883715
 40728/100000: episode: 745, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 176.935, mean reward: 1.769 [1.468, 2.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.194, 10.098], loss: 0.103546, mae: 0.306407, mean_q: 3.891982
 40828/100000: episode: 746, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 220.218, mean reward: 2.202 [1.445, 4.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.072, 10.538], loss: 0.120228, mae: 0.309679, mean_q: 3.864290
 40928/100000: episode: 747, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 189.226, mean reward: 1.892 [1.449, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.589, 10.098], loss: 0.102946, mae: 0.307465, mean_q: 3.882164
 41028/100000: episode: 748, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 185.035, mean reward: 1.850 [1.459, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.657, 10.098], loss: 0.082708, mae: 0.280412, mean_q: 3.876240
 41128/100000: episode: 749, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 200.085, mean reward: 2.001 [1.445, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.698, 10.098], loss: 0.110856, mae: 0.311806, mean_q: 3.909791
 41228/100000: episode: 750, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 189.252, mean reward: 1.893 [1.446, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.866, 10.109], loss: 0.104073, mae: 0.308837, mean_q: 3.867908
 41328/100000: episode: 751, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 195.005, mean reward: 1.950 [1.452, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.288], loss: 0.095369, mae: 0.301886, mean_q: 3.862054
 41428/100000: episode: 752, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 190.663, mean reward: 1.907 [1.492, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.874, 10.098], loss: 0.089859, mae: 0.295095, mean_q: 3.857805
 41528/100000: episode: 753, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 191.580, mean reward: 1.916 [1.536, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.697, 10.098], loss: 0.090869, mae: 0.298398, mean_q: 3.846089
 41628/100000: episode: 754, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 193.770, mean reward: 1.938 [1.477, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.012, 10.098], loss: 0.093004, mae: 0.297565, mean_q: 3.845918
 41728/100000: episode: 755, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 193.748, mean reward: 1.937 [1.443, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.551, 10.176], loss: 0.093330, mae: 0.295104, mean_q: 3.835341
 41828/100000: episode: 756, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 182.195, mean reward: 1.822 [1.467, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.537, 10.143], loss: 0.111228, mae: 0.312637, mean_q: 3.870903
 41928/100000: episode: 757, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 193.493, mean reward: 1.935 [1.472, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.448, 10.098], loss: 0.083777, mae: 0.289831, mean_q: 3.844064
 42028/100000: episode: 758, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 192.916, mean reward: 1.929 [1.466, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.405, 10.098], loss: 0.094872, mae: 0.300817, mean_q: 3.841151
 42128/100000: episode: 759, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 199.375, mean reward: 1.994 [1.441, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.208, 10.098], loss: 0.088864, mae: 0.294599, mean_q: 3.852725
 42228/100000: episode: 760, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 185.773, mean reward: 1.858 [1.452, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.413, 10.103], loss: 0.090187, mae: 0.294505, mean_q: 3.811141
 42328/100000: episode: 761, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 203.575, mean reward: 2.036 [1.458, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.893, 10.114], loss: 0.083992, mae: 0.288764, mean_q: 3.806276
 42428/100000: episode: 762, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 182.441, mean reward: 1.824 [1.496, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.261, 10.168], loss: 0.083259, mae: 0.287865, mean_q: 3.826674
 42528/100000: episode: 763, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 193.194, mean reward: 1.932 [1.465, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.350, 10.223], loss: 0.085276, mae: 0.288704, mean_q: 3.819197
 42628/100000: episode: 764, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 186.590, mean reward: 1.866 [1.447, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.774, 10.098], loss: 0.079940, mae: 0.284811, mean_q: 3.823455
 42728/100000: episode: 765, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 179.453, mean reward: 1.795 [1.430, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.118], loss: 0.099458, mae: 0.305787, mean_q: 3.820518
 42828/100000: episode: 766, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 199.176, mean reward: 1.992 [1.512, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.500, 10.285], loss: 0.092252, mae: 0.302001, mean_q: 3.841779
 42928/100000: episode: 767, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 186.355, mean reward: 1.864 [1.460, 6.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.419, 10.214], loss: 0.085044, mae: 0.294774, mean_q: 3.823886
 43028/100000: episode: 768, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 216.659, mean reward: 2.167 [1.500, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.268, 10.401], loss: 0.094054, mae: 0.299433, mean_q: 3.831769
 43128/100000: episode: 769, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 200.414, mean reward: 2.004 [1.477, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.772, 10.098], loss: 0.095662, mae: 0.304720, mean_q: 3.849775
 43228/100000: episode: 770, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 197.817, mean reward: 1.978 [1.441, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.266, 10.376], loss: 0.082532, mae: 0.283116, mean_q: 3.816530
 43328/100000: episode: 771, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 188.151, mean reward: 1.882 [1.474, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.192], loss: 0.085861, mae: 0.292443, mean_q: 3.825454
 43428/100000: episode: 772, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 222.488, mean reward: 2.225 [1.452, 6.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.397, 10.434], loss: 0.082641, mae: 0.284420, mean_q: 3.827168
 43528/100000: episode: 773, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 198.159, mean reward: 1.982 [1.458, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.098], loss: 0.089113, mae: 0.293907, mean_q: 3.838242
 43628/100000: episode: 774, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 223.972, mean reward: 2.240 [1.514, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.368, 10.098], loss: 0.089593, mae: 0.300290, mean_q: 3.832500
 43728/100000: episode: 775, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 180.854, mean reward: 1.809 [1.473, 2.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.182, 10.098], loss: 0.084687, mae: 0.290818, mean_q: 3.838905
 43828/100000: episode: 776, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 201.300, mean reward: 2.013 [1.458, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.824, 10.098], loss: 0.107943, mae: 0.314313, mean_q: 3.849786
 43928/100000: episode: 777, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: 187.258, mean reward: 1.873 [1.442, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.240, 10.338], loss: 0.096993, mae: 0.304063, mean_q: 3.863719
 44028/100000: episode: 778, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 183.013, mean reward: 1.830 [1.450, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.196, 10.253], loss: 0.086261, mae: 0.289477, mean_q: 3.849059
 44128/100000: episode: 779, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 198.626, mean reward: 1.986 [1.472, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.690, 10.261], loss: 0.093938, mae: 0.299245, mean_q: 3.843144
 44228/100000: episode: 780, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 188.013, mean reward: 1.880 [1.475, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.501, 10.215], loss: 0.096741, mae: 0.299253, mean_q: 3.848762
 44328/100000: episode: 781, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 205.958, mean reward: 2.060 [1.449, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.742, 10.273], loss: 0.093320, mae: 0.296151, mean_q: 3.858701
 44428/100000: episode: 782, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 208.533, mean reward: 2.085 [1.458, 6.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.520, 10.155], loss: 0.089772, mae: 0.294839, mean_q: 3.858847
 44528/100000: episode: 783, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 185.928, mean reward: 1.859 [1.445, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.315, 10.164], loss: 0.097627, mae: 0.303563, mean_q: 3.863435
 44628/100000: episode: 784, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 189.657, mean reward: 1.897 [1.457, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.649, 10.260], loss: 0.091341, mae: 0.291548, mean_q: 3.861198
 44728/100000: episode: 785, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 194.260, mean reward: 1.943 [1.447, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.115], loss: 0.090341, mae: 0.291536, mean_q: 3.847721
 44828/100000: episode: 786, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 179.104, mean reward: 1.791 [1.472, 2.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.465, 10.183], loss: 0.096080, mae: 0.296411, mean_q: 3.838814
 44928/100000: episode: 787, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 184.924, mean reward: 1.849 [1.487, 2.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.219], loss: 0.082197, mae: 0.286173, mean_q: 3.850570
 45028/100000: episode: 788, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 185.217, mean reward: 1.852 [1.431, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.089, 10.098], loss: 0.077552, mae: 0.280681, mean_q: 3.826746
 45128/100000: episode: 789, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 186.167, mean reward: 1.862 [1.432, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.126, 10.273], loss: 0.085867, mae: 0.288352, mean_q: 3.821600
 45228/100000: episode: 790, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 221.065, mean reward: 2.211 [1.455, 36.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.004, 10.098], loss: 0.088052, mae: 0.288018, mean_q: 3.824973
 45328/100000: episode: 791, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 231.623, mean reward: 2.316 [1.508, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.130, 10.422], loss: 0.285046, mae: 0.319593, mean_q: 3.865107
 45428/100000: episode: 792, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 191.124, mean reward: 1.911 [1.444, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.942, 10.336], loss: 0.276608, mae: 0.313225, mean_q: 3.830414
 45528/100000: episode: 793, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 185.158, mean reward: 1.852 [1.468, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.143], loss: 0.466680, mae: 0.330161, mean_q: 3.866018
[Info] 1-TH LEVEL FOUND: 5.134418487548828, Considering 10/90 traces
 45628/100000: episode: 794, duration: 5.886s, episode steps: 100, steps per second: 17, episode reward: 186.877, mean reward: 1.869 [1.460, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.237], loss: 0.446451, mae: 0.314057, mean_q: 3.844627
 45644/100000: episode: 795, duration: 0.116s, episode steps: 16, steps per second: 137, episode reward: 38.923, mean reward: 2.433 [2.033, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.223, 10.100], loss: 0.098110, mae: 0.300198, mean_q: 3.808607
 45662/100000: episode: 796, duration: 0.143s, episode steps: 18, steps per second: 126, episode reward: 38.900, mean reward: 2.161 [1.734, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.161, 10.100], loss: 0.084224, mae: 0.284983, mean_q: 3.831664
 45680/100000: episode: 797, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 35.282, mean reward: 1.960 [1.646, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.379, 10.100], loss: 0.066019, mae: 0.276386, mean_q: 3.869709
 45727/100000: episode: 798, duration: 0.273s, episode steps: 47, steps per second: 172, episode reward: 211.227, mean reward: 4.494 [2.959, 11.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-1.047, 10.399], loss: 0.099164, mae: 0.298906, mean_q: 3.868650
 45743/100000: episode: 799, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 35.983, mean reward: 2.249 [1.926, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.169, 10.100], loss: 0.087209, mae: 0.293953, mean_q: 3.856352
 45761/100000: episode: 800, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 41.117, mean reward: 2.284 [1.912, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.686, 10.100], loss: 0.095331, mae: 0.309762, mean_q: 3.901557
 45779/100000: episode: 801, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 47.315, mean reward: 2.629 [2.332, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.187, 10.100], loss: 1.118361, mae: 0.399474, mean_q: 3.960385
 45805/100000: episode: 802, duration: 0.199s, episode steps: 26, steps per second: 131, episode reward: 97.065, mean reward: 3.733 [2.360, 8.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.209, 10.399], loss: 1.535062, mae: 0.480459, mean_q: 3.947032
 45852/100000: episode: 803, duration: 0.392s, episode steps: 47, steps per second: 120, episode reward: 127.794, mean reward: 2.719 [1.787, 6.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.304, 10.329], loss: 0.524683, mae: 0.353655, mean_q: 3.959051
 45865/100000: episode: 804, duration: 0.100s, episode steps: 13, steps per second: 130, episode reward: 35.370, mean reward: 2.721 [2.276, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.278], loss: 0.117961, mae: 0.344052, mean_q: 3.922437
 45885/100000: episode: 805, duration: 0.149s, episode steps: 20, steps per second: 134, episode reward: 64.195, mean reward: 3.210 [2.189, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.493, 10.100], loss: 0.125913, mae: 0.307011, mean_q: 3.909758
 45898/100000: episode: 806, duration: 0.116s, episode steps: 13, steps per second: 113, episode reward: 44.523, mean reward: 3.425 [3.031, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.458], loss: 0.091826, mae: 0.296579, mean_q: 3.906608
 45924/100000: episode: 807, duration: 0.205s, episode steps: 26, steps per second: 127, episode reward: 79.795, mean reward: 3.069 [2.401, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.102, 10.493], loss: 0.093278, mae: 0.309729, mean_q: 3.958832
 45950/100000: episode: 808, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 90.537, mean reward: 3.482 [2.685, 5.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.723, 10.543], loss: 0.097366, mae: 0.311816, mean_q: 4.010639
 45966/100000: episode: 809, duration: 0.106s, episode steps: 16, steps per second: 150, episode reward: 38.278, mean reward: 2.392 [1.817, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.344, 10.100], loss: 0.165267, mae: 0.324409, mean_q: 3.943646
 45984/100000: episode: 810, duration: 0.126s, episode steps: 18, steps per second: 142, episode reward: 35.425, mean reward: 1.968 [1.588, 2.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.100], loss: 0.198614, mae: 0.363646, mean_q: 4.037149
 46000/100000: episode: 811, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 30.751, mean reward: 1.922 [1.704, 2.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.182, 10.100], loss: 0.102499, mae: 0.323784, mean_q: 4.018080
 46016/100000: episode: 812, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 34.710, mean reward: 2.169 [1.819, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.323, 10.100], loss: 0.076640, mae: 0.272864, mean_q: 3.917898
 46034/100000: episode: 813, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 36.474, mean reward: 2.026 [1.707, 2.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.624, 10.100], loss: 0.093250, mae: 0.306136, mean_q: 3.958739
 46050/100000: episode: 814, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 41.115, mean reward: 2.570 [1.593, 5.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.882, 10.100], loss: 0.085727, mae: 0.296799, mean_q: 3.952280
 46097/100000: episode: 815, duration: 0.276s, episode steps: 47, steps per second: 171, episode reward: 120.712, mean reward: 2.568 [1.662, 5.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.301, 10.287], loss: 0.106453, mae: 0.317802, mean_q: 3.984293
 46113/100000: episode: 816, duration: 0.118s, episode steps: 16, steps per second: 136, episode reward: 32.286, mean reward: 2.018 [1.479, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.085, 10.100], loss: 0.112011, mae: 0.316695, mean_q: 4.028044
 46131/100000: episode: 817, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 39.598, mean reward: 2.200 [1.644, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.771, 10.100], loss: 1.192880, mae: 0.445367, mean_q: 4.040449
 46144/100000: episode: 818, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 47.996, mean reward: 3.692 [2.702, 5.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.462], loss: 0.150554, mae: 0.388076, mean_q: 3.969371
 46191/100000: episode: 819, duration: 0.285s, episode steps: 47, steps per second: 165, episode reward: 116.897, mean reward: 2.487 [1.538, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.301, 10.255], loss: 0.135965, mae: 0.333507, mean_q: 3.988608
 46212/100000: episode: 820, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 42.859, mean reward: 2.041 [1.653, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.285, 10.100], loss: 0.157990, mae: 0.320031, mean_q: 3.949348
 46232/100000: episode: 821, duration: 0.175s, episode steps: 20, steps per second: 114, episode reward: 58.857, mean reward: 2.943 [2.462, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.171, 10.100], loss: 0.115395, mae: 0.323312, mean_q: 4.045394
 46253/100000: episode: 822, duration: 0.161s, episode steps: 21, steps per second: 131, episode reward: 52.483, mean reward: 2.499 [1.846, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.523, 10.100], loss: 0.137715, mae: 0.309395, mean_q: 4.008717
 46278/100000: episode: 823, duration: 0.180s, episode steps: 25, steps per second: 139, episode reward: 65.323, mean reward: 2.613 [2.098, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.695, 10.427], loss: 0.102684, mae: 0.302721, mean_q: 4.008619
 46291/100000: episode: 824, duration: 0.104s, episode steps: 13, steps per second: 125, episode reward: 61.184, mean reward: 4.706 [3.640, 9.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.151, 10.493], loss: 0.079328, mae: 0.285816, mean_q: 4.026151
 46311/100000: episode: 825, duration: 0.147s, episode steps: 20, steps per second: 136, episode reward: 62.790, mean reward: 3.139 [2.474, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.949, 10.100], loss: 0.089751, mae: 0.301022, mean_q: 3.964441
 46329/100000: episode: 826, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 51.431, mean reward: 2.857 [2.231, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.192, 10.100], loss: 0.151350, mae: 0.353747, mean_q: 4.048618
 46350/100000: episode: 827, duration: 0.140s, episode steps: 21, steps per second: 150, episode reward: 49.888, mean reward: 2.376 [1.710, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.951, 10.100], loss: 0.197085, mae: 0.387168, mean_q: 4.023579
 46397/100000: episode: 828, duration: 0.307s, episode steps: 47, steps per second: 153, episode reward: 144.956, mean reward: 3.084 [1.746, 5.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.242, 10.261], loss: 0.155356, mae: 0.340741, mean_q: 4.082624
 46417/100000: episode: 829, duration: 0.131s, episode steps: 20, steps per second: 153, episode reward: 90.347, mean reward: 4.517 [2.669, 13.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.158, 10.100], loss: 0.152600, mae: 0.325277, mean_q: 4.005330
 46438/100000: episode: 830, duration: 0.137s, episode steps: 21, steps per second: 153, episode reward: 39.215, mean reward: 1.867 [1.550, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.226, 10.100], loss: 0.168653, mae: 0.371236, mean_q: 4.081712
 46451/100000: episode: 831, duration: 0.099s, episode steps: 13, steps per second: 131, episode reward: 41.361, mean reward: 3.182 [2.600, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.524], loss: 1.617458, mae: 0.458809, mean_q: 4.114479
 46471/100000: episode: 832, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 66.739, mean reward: 3.337 [2.825, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.895, 10.100], loss: 0.149194, mae: 0.384315, mean_q: 4.105147
 46490/100000: episode: 833, duration: 0.118s, episode steps: 19, steps per second: 162, episode reward: 38.036, mean reward: 2.002 [1.596, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.174, 10.100], loss: 1.078646, mae: 0.421099, mean_q: 4.204034
 46508/100000: episode: 834, duration: 0.122s, episode steps: 18, steps per second: 148, episode reward: 50.783, mean reward: 2.821 [1.978, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.717, 10.100], loss: 0.154652, mae: 0.372750, mean_q: 4.030830
 46555/100000: episode: 835, duration: 0.313s, episode steps: 47, steps per second: 150, episode reward: 137.701, mean reward: 2.930 [1.911, 5.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.315, 10.405], loss: 0.174998, mae: 0.366457, mean_q: 4.150904
 46575/100000: episode: 836, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 52.284, mean reward: 2.614 [1.972, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.158, 10.100], loss: 1.044914, mae: 0.390008, mean_q: 4.195585
 46601/100000: episode: 837, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 109.404, mean reward: 4.208 [2.465, 10.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.898, 10.508], loss: 0.161400, mae: 0.391575, mean_q: 4.131883
 46614/100000: episode: 838, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 39.580, mean reward: 3.045 [2.182, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.743, 10.396], loss: 0.158976, mae: 0.358219, mean_q: 4.177539
 46640/100000: episode: 839, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 64.325, mean reward: 2.474 [1.795, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.390], loss: 0.177038, mae: 0.362154, mean_q: 4.107383
 46687/100000: episode: 840, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 179.010, mean reward: 3.809 [1.745, 8.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.025, 10.643], loss: 0.212579, mae: 0.359707, mean_q: 4.164131
 46705/100000: episode: 841, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 36.271, mean reward: 2.015 [1.563, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.329, 10.100], loss: 0.138505, mae: 0.344577, mean_q: 4.144507
 46723/100000: episode: 842, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 48.555, mean reward: 2.698 [2.134, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.163, 10.100], loss: 0.199396, mae: 0.404802, mean_q: 4.255492
 46739/100000: episode: 843, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 32.257, mean reward: 2.016 [1.785, 2.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.491, 10.100], loss: 0.306034, mae: 0.403034, mean_q: 4.194460
 46760/100000: episode: 844, duration: 0.181s, episode steps: 21, steps per second: 116, episode reward: 41.706, mean reward: 1.986 [1.592, 2.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.049, 10.100], loss: 0.154844, mae: 0.374518, mean_q: 4.285205
 46781/100000: episode: 845, duration: 0.162s, episode steps: 21, steps per second: 130, episode reward: 54.295, mean reward: 2.585 [1.972, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.329, 10.100], loss: 0.195957, mae: 0.387244, mean_q: 4.204293
 46806/100000: episode: 846, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 68.592, mean reward: 2.744 [1.966, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.223, 10.339], loss: 0.303475, mae: 0.392361, mean_q: 4.239323
 46819/100000: episode: 847, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 41.348, mean reward: 3.181 [2.748, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.122, 10.414], loss: 0.208421, mae: 0.411727, mean_q: 4.259985
 46840/100000: episode: 848, duration: 0.138s, episode steps: 21, steps per second: 152, episode reward: 48.185, mean reward: 2.295 [1.876, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.093, 10.100], loss: 0.132579, mae: 0.329503, mean_q: 4.193282
 46860/100000: episode: 849, duration: 0.132s, episode steps: 20, steps per second: 151, episode reward: 46.717, mean reward: 2.336 [1.934, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.303, 10.100], loss: 0.261817, mae: 0.394779, mean_q: 4.306023
 46880/100000: episode: 850, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 57.443, mean reward: 2.872 [1.967, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.169, 10.100], loss: 0.149222, mae: 0.374553, mean_q: 4.202531
 46905/100000: episode: 851, duration: 0.161s, episode steps: 25, steps per second: 156, episode reward: 70.297, mean reward: 2.812 [2.031, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.157, 10.503], loss: 0.273971, mae: 0.440110, mean_q: 4.295617
 46924/100000: episode: 852, duration: 0.130s, episode steps: 19, steps per second: 146, episode reward: 55.136, mean reward: 2.902 [1.887, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.449, 10.100], loss: 0.153280, mae: 0.367804, mean_q: 4.214167
 46944/100000: episode: 853, duration: 0.131s, episode steps: 20, steps per second: 153, episode reward: 58.557, mean reward: 2.928 [2.510, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.511, 10.100], loss: 0.230264, mae: 0.382216, mean_q: 4.308755
 46969/100000: episode: 854, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 95.014, mean reward: 3.801 [2.279, 10.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.520, 10.520], loss: 0.218565, mae: 0.378969, mean_q: 4.202980
 47016/100000: episode: 855, duration: 0.266s, episode steps: 47, steps per second: 176, episode reward: 120.195, mean reward: 2.557 [1.480, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.476, 10.128], loss: 0.189795, mae: 0.400396, mean_q: 4.328628
 47041/100000: episode: 856, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 79.866, mean reward: 3.195 [1.922, 4.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.232, 10.357], loss: 0.312361, mae: 0.435883, mean_q: 4.292532
 47057/100000: episode: 857, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 42.634, mean reward: 2.665 [2.047, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.312, 10.100], loss: 1.339052, mae: 0.476257, mean_q: 4.267889
 47082/100000: episode: 858, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 69.364, mean reward: 2.775 [2.124, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.435], loss: 0.321107, mae: 0.439194, mean_q: 4.412546
 47129/100000: episode: 859, duration: 0.337s, episode steps: 47, steps per second: 139, episode reward: 153.978, mean reward: 3.276 [2.138, 6.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.978, 10.409], loss: 0.617295, mae: 0.442008, mean_q: 4.342741
 47176/100000: episode: 860, duration: 0.298s, episode steps: 47, steps per second: 158, episode reward: 129.009, mean reward: 2.745 [1.442, 16.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.301, 10.184], loss: 0.258162, mae: 0.430844, mean_q: 4.383893
 47196/100000: episode: 861, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 55.897, mean reward: 2.795 [1.961, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.367, 10.100], loss: 0.198233, mae: 0.373437, mean_q: 4.386925
 47243/100000: episode: 862, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 144.989, mean reward: 3.085 [2.418, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.898, 10.421], loss: 0.665371, mae: 0.460509, mean_q: 4.435652
 47261/100000: episode: 863, duration: 0.113s, episode steps: 18, steps per second: 160, episode reward: 51.997, mean reward: 2.889 [2.278, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.151, 10.100], loss: 0.346809, mae: 0.485437, mean_q: 4.376533
 47287/100000: episode: 864, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 62.236, mean reward: 2.394 [1.704, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.353, 10.250], loss: 0.206798, mae: 0.424889, mean_q: 4.355830
 47300/100000: episode: 865, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 38.586, mean reward: 2.968 [1.972, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.028, 10.308], loss: 0.349767, mae: 0.408282, mean_q: 4.338536
 47319/100000: episode: 866, duration: 0.118s, episode steps: 19, steps per second: 162, episode reward: 63.601, mean reward: 3.347 [2.283, 4.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.506, 10.100], loss: 0.231951, mae: 0.426945, mean_q: 4.426726
 47335/100000: episode: 867, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 33.577, mean reward: 2.099 [1.834, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.581, 10.100], loss: 0.328411, mae: 0.417774, mean_q: 4.474863
 47351/100000: episode: 868, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 32.886, mean reward: 2.055 [1.611, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.093, 10.144], loss: 0.255982, mae: 0.467957, mean_q: 4.340540
 47372/100000: episode: 869, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 43.513, mean reward: 2.072 [1.608, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.468, 10.100], loss: 0.320362, mae: 0.439862, mean_q: 4.472356
 47391/100000: episode: 870, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 41.998, mean reward: 2.210 [1.868, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.302, 10.100], loss: 0.264112, mae: 0.409064, mean_q: 4.376140
 47410/100000: episode: 871, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 59.813, mean reward: 3.148 [2.110, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.316, 10.100], loss: 0.234285, mae: 0.408244, mean_q: 4.360611
 47429/100000: episode: 872, duration: 0.128s, episode steps: 19, steps per second: 149, episode reward: 46.104, mean reward: 2.427 [1.762, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.506, 10.100], loss: 0.189916, mae: 0.390725, mean_q: 4.412727
 47476/100000: episode: 873, duration: 0.287s, episode steps: 47, steps per second: 164, episode reward: 117.819, mean reward: 2.507 [1.584, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.305, 10.278], loss: 0.257801, mae: 0.406696, mean_q: 4.455892
 47501/100000: episode: 874, duration: 0.155s, episode steps: 25, steps per second: 161, episode reward: 77.771, mean reward: 3.111 [2.231, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.331, 10.410], loss: 0.228526, mae: 0.423350, mean_q: 4.505756
 47517/100000: episode: 875, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 34.098, mean reward: 2.131 [1.940, 2.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.227, 10.100], loss: 0.173070, mae: 0.389495, mean_q: 4.418431
 47535/100000: episode: 876, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 39.016, mean reward: 2.168 [1.809, 2.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.229, 10.100], loss: 0.197904, mae: 0.416816, mean_q: 4.421194
 47582/100000: episode: 877, duration: 0.307s, episode steps: 47, steps per second: 153, episode reward: 95.722, mean reward: 2.037 [1.467, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.304, 10.219], loss: 0.168625, mae: 0.370183, mean_q: 4.429033
 47598/100000: episode: 878, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 39.000, mean reward: 2.437 [1.970, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.538, 10.100], loss: 0.381709, mae: 0.453470, mean_q: 4.438929
 47617/100000: episode: 879, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 40.780, mean reward: 2.146 [1.723, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.137, 10.100], loss: 0.316786, mae: 0.417526, mean_q: 4.441237
 47633/100000: episode: 880, duration: 0.106s, episode steps: 16, steps per second: 152, episode reward: 35.397, mean reward: 2.212 [1.680, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.882, 10.100], loss: 0.178857, mae: 0.391399, mean_q: 4.513751
 47651/100000: episode: 881, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 53.488, mean reward: 2.972 [2.088, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.331, 10.100], loss: 0.134971, mae: 0.361536, mean_q: 4.374259
 47667/100000: episode: 882, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 33.571, mean reward: 2.098 [1.781, 2.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.354, 10.100], loss: 0.161491, mae: 0.360651, mean_q: 4.434879
 47680/100000: episode: 883, duration: 0.093s, episode steps: 13, steps per second: 140, episode reward: 39.778, mean reward: 3.060 [2.042, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.360], loss: 0.166164, mae: 0.395193, mean_q: 4.452528
[Info] 2-TH LEVEL FOUND: 8.22739315032959, Considering 10/90 traces
 47700/100000: episode: 884, duration: 4.916s, episode steps: 20, steps per second: 4, episode reward: 65.314, mean reward: 3.266 [2.512, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.580, 10.100], loss: 0.152022, mae: 0.383789, mean_q: 4.356524
 47718/100000: episode: 885, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 60.389, mean reward: 3.355 [2.560, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.399], loss: 0.327944, mae: 0.447834, mean_q: 4.472798
 47735/100000: episode: 886, duration: 0.113s, episode steps: 17, steps per second: 150, episode reward: 112.058, mean reward: 6.592 [4.047, 14.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.636, 10.500], loss: 0.163860, mae: 0.398017, mean_q: 4.427173
 47763/100000: episode: 887, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 149.215, mean reward: 5.329 [2.691, 10.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.561, 10.673], loss: 0.267381, mae: 0.463119, mean_q: 4.619511
 47778/100000: episode: 888, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 43.134, mean reward: 2.876 [2.535, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.449], loss: 0.162530, mae: 0.375409, mean_q: 4.447596
 47792/100000: episode: 889, duration: 0.094s, episode steps: 14, steps per second: 148, episode reward: 48.485, mean reward: 3.463 [2.624, 5.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.571, 10.398], loss: 0.281735, mae: 0.449116, mean_q: 4.534064
 47820/100000: episode: 890, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 162.124, mean reward: 5.790 [2.103, 19.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.283, 10.322], loss: 1.060096, mae: 0.565233, mean_q: 4.579646
 47837/100000: episode: 891, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 48.629, mean reward: 2.861 [2.321, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.468], loss: 0.207608, mae: 0.450341, mean_q: 4.622027
 47853/100000: episode: 892, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 65.320, mean reward: 4.083 [3.101, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.578, 10.517], loss: 0.411078, mae: 0.467125, mean_q: 4.545785
 47881/100000: episode: 893, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 114.786, mean reward: 4.100 [2.529, 7.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.450], loss: 0.360051, mae: 0.463081, mean_q: 4.609917
 47909/100000: episode: 894, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 133.599, mean reward: 4.771 [2.914, 13.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.467], loss: 0.953500, mae: 0.474678, mean_q: 4.633611
 47922/100000: episode: 895, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 38.227, mean reward: 2.941 [2.271, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.360], loss: 0.371287, mae: 0.427690, mean_q: 4.446063
 47933/100000: episode: 896, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 50.205, mean reward: 4.564 [3.960, 6.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.142, 10.581], loss: 0.500828, mae: 0.527777, mean_q: 4.671676
 47946/100000: episode: 897, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 59.106, mean reward: 4.547 [3.825, 6.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.581], loss: 0.209681, mae: 0.396589, mean_q: 4.622415
 47974/100000: episode: 898, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 225.852, mean reward: 8.066 [2.794, 34.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.407, 10.513], loss: 0.278606, mae: 0.442612, mean_q: 4.586128
 47991/100000: episode: 899, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 51.132, mean reward: 3.008 [2.070, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.441, 10.546], loss: 0.467487, mae: 0.496168, mean_q: 4.783295
 48007/100000: episode: 900, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 85.672, mean reward: 5.355 [3.515, 13.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.720, 10.554], loss: 1.738314, mae: 0.735841, mean_q: 4.953079
 48020/100000: episode: 901, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 51.290, mean reward: 3.945 [2.449, 6.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.640], loss: 0.449336, mae: 0.561512, mean_q: 4.830904
 48038/100000: episode: 902, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 63.631, mean reward: 3.535 [2.549, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.436], loss: 0.319731, mae: 0.475809, mean_q: 4.666860
 48055/100000: episode: 903, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 72.359, mean reward: 4.256 [3.176, 6.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.504, 10.450], loss: 1.607444, mae: 0.666124, mean_q: 4.891569
 48072/100000: episode: 904, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 80.668, mean reward: 4.745 [2.835, 8.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.049, 10.422], loss: 0.310906, mae: 0.482516, mean_q: 4.782141
 48086/100000: episode: 905, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 86.442, mean reward: 6.174 [2.858, 23.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.608], loss: 0.194796, mae: 0.414251, mean_q: 4.708087
 48102/100000: episode: 906, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 89.829, mean reward: 5.614 [3.619, 14.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.140, 10.694], loss: 0.480013, mae: 0.489144, mean_q: 4.876630
 48119/100000: episode: 907, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 74.574, mean reward: 4.387 [3.280, 6.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.260, 10.506], loss: 0.670718, mae: 0.538577, mean_q: 4.710830
 48137/100000: episode: 908, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 75.570, mean reward: 4.198 [2.908, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.543], loss: 1.153726, mae: 0.659171, mean_q: 4.922523
 48151/100000: episode: 909, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 40.202, mean reward: 2.872 [2.339, 4.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.398], loss: 0.438876, mae: 0.546394, mean_q: 5.031913
 48164/100000: episode: 910, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 41.676, mean reward: 3.206 [2.251, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.433], loss: 0.448441, mae: 0.502912, mean_q: 4.866628
 48180/100000: episode: 911, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 49.525, mean reward: 3.095 [1.760, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.310], loss: 0.715564, mae: 0.591136, mean_q: 5.031083
 48195/100000: episode: 912, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 70.687, mean reward: 4.712 [3.616, 5.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.586], loss: 0.325301, mae: 0.510265, mean_q: 4.811448
 48214/100000: episode: 913, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 61.311, mean reward: 3.227 [2.279, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.528], loss: 0.455006, mae: 0.436016, mean_q: 4.778467
 48242/100000: episode: 914, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 168.156, mean reward: 6.006 [2.783, 12.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.324, 10.492], loss: 1.085498, mae: 0.625251, mean_q: 5.067360
 48258/100000: episode: 915, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 85.029, mean reward: 5.314 [3.511, 7.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.233, 10.609], loss: 0.306798, mae: 0.516401, mean_q: 4.887574
 48286/100000: episode: 916, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 127.481, mean reward: 4.553 [2.392, 9.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.421, 10.325], loss: 0.341390, mae: 0.495611, mean_q: 4.924122
 48297/100000: episode: 917, duration: 0.066s, episode steps: 11, steps per second: 165, episode reward: 44.544, mean reward: 4.049 [3.535, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.045, 10.502], loss: 1.505863, mae: 0.662786, mean_q: 5.151106
 48303/100000: episode: 918, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 27.900, mean reward: 4.650 [4.074, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.539], loss: 0.501826, mae: 0.668837, mean_q: 5.157870
 48319/100000: episode: 919, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 53.926, mean reward: 3.370 [2.332, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.170, 10.367], loss: 0.842571, mae: 0.692404, mean_q: 5.042983
 48347/100000: episode: 920, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 123.781, mean reward: 4.421 [3.139, 6.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.245, 10.452], loss: 0.586994, mae: 0.589824, mean_q: 5.109321
 48364/100000: episode: 921, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 78.244, mean reward: 4.603 [3.433, 5.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.202, 10.596], loss: 0.605522, mae: 0.543589, mean_q: 4.942668
 48370/100000: episode: 922, duration: 0.046s, episode steps: 6, steps per second: 130, episode reward: 26.205, mean reward: 4.368 [3.627, 5.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.616], loss: 0.447254, mae: 0.625066, mean_q: 5.456608
 48389/100000: episode: 923, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 74.767, mean reward: 3.935 [2.862, 5.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.100, 10.510], loss: 1.245128, mae: 0.568363, mean_q: 5.051732
 48403/100000: episode: 924, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 40.106, mean reward: 2.865 [2.202, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.227, 10.404], loss: 0.522398, mae: 0.566844, mean_q: 5.280873
 48421/100000: episode: 925, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 54.806, mean reward: 3.045 [2.291, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.385], loss: 0.478200, mae: 0.533305, mean_q: 5.045290
 48434/100000: episode: 926, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 63.348, mean reward: 4.873 [2.519, 10.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.483, 10.553], loss: 0.428939, mae: 0.522886, mean_q: 5.000564
 48448/100000: episode: 927, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 40.603, mean reward: 2.900 [2.704, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.469], loss: 0.423255, mae: 0.548629, mean_q: 5.136291
 48464/100000: episode: 928, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 61.982, mean reward: 3.874 [2.219, 5.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.465], loss: 0.277805, mae: 0.477666, mean_q: 5.133521
 48470/100000: episode: 929, duration: 0.046s, episode steps: 6, steps per second: 130, episode reward: 21.201, mean reward: 3.533 [3.078, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.489], loss: 0.244520, mae: 0.482310, mean_q: 5.079777
 48488/100000: episode: 930, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 65.416, mean reward: 3.634 [2.686, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.397], loss: 0.885999, mae: 0.572016, mean_q: 5.157284
 48494/100000: episode: 931, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 27.901, mean reward: 4.650 [3.762, 6.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.475], loss: 0.334114, mae: 0.497176, mean_q: 4.875102
 48505/100000: episode: 932, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 45.705, mean reward: 4.155 [3.092, 5.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.534], loss: 0.529594, mae: 0.521299, mean_q: 5.170516
 48519/100000: episode: 933, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 48.155, mean reward: 3.440 [2.625, 4.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.508, 10.490], loss: 0.301555, mae: 0.489850, mean_q: 5.131715
 48536/100000: episode: 934, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 55.626, mean reward: 3.272 [2.869, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.238, 10.485], loss: 0.468046, mae: 0.550733, mean_q: 5.078076
 48564/100000: episode: 935, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 152.934, mean reward: 5.462 [3.500, 9.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.117, 10.549], loss: 0.547925, mae: 0.570433, mean_q: 5.189489
 48582/100000: episode: 936, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 62.954, mean reward: 3.497 [2.442, 5.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.410, 10.521], loss: 1.528475, mae: 0.817104, mean_q: 5.674205
 48601/100000: episode: 937, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 76.084, mean reward: 4.004 [3.028, 7.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.331, 10.430], loss: 0.555062, mae: 0.636750, mean_q: 5.161727
 48617/100000: episode: 938, duration: 0.107s, episode steps: 16, steps per second: 149, episode reward: 73.289, mean reward: 4.581 [2.965, 10.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.590], loss: 0.396528, mae: 0.551191, mean_q: 5.085815
 48623/100000: episode: 939, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 50.925, mean reward: 8.488 [5.832, 12.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.524], loss: 0.657011, mae: 0.705953, mean_q: 5.676626
 48651/100000: episode: 940, duration: 0.158s, episode steps: 28, steps per second: 178, episode reward: 158.612, mean reward: 5.665 [4.105, 10.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.884, 10.601], loss: 1.255091, mae: 0.678577, mean_q: 5.300876
 48666/100000: episode: 941, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 55.095, mean reward: 3.673 [2.570, 5.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.701, 10.438], loss: 0.334245, mae: 0.565358, mean_q: 5.156616
 48685/100000: episode: 942, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 86.237, mean reward: 4.539 [3.334, 5.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.798, 10.594], loss: 1.025680, mae: 0.644662, mean_q: 5.447261
 48691/100000: episode: 943, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 25.412, mean reward: 4.235 [3.564, 5.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.546], loss: 0.243149, mae: 0.458268, mean_q: 4.999589
 48705/100000: episode: 944, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 59.722, mean reward: 4.266 [2.890, 7.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.544, 10.524], loss: 0.359094, mae: 0.549353, mean_q: 5.423706
 48716/100000: episode: 945, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 43.572, mean reward: 3.961 [3.386, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.541], loss: 0.521995, mae: 0.563487, mean_q: 5.263362
 48730/100000: episode: 946, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 39.287, mean reward: 2.806 [2.350, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.748, 10.398], loss: 0.461425, mae: 0.619705, mean_q: 5.493697
 48743/100000: episode: 947, duration: 0.089s, episode steps: 13, steps per second: 145, episode reward: 113.087, mean reward: 8.699 [2.910, 42.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.684, 10.525], loss: 1.557484, mae: 0.771419, mean_q: 5.487233
 48756/100000: episode: 948, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 47.590, mean reward: 3.661 [2.470, 5.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.360, 10.435], loss: 1.754044, mae: 0.677063, mean_q: 5.223274
 48773/100000: episode: 949, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 45.116, mean reward: 2.654 [2.111, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.408], loss: 2.121294, mae: 0.781418, mean_q: 5.669535
 48791/100000: episode: 950, duration: 0.114s, episode steps: 18, steps per second: 157, episode reward: 79.912, mean reward: 4.440 [3.474, 6.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.116, 10.558], loss: 0.527853, mae: 0.617364, mean_q: 5.458168
 48805/100000: episode: 951, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 38.057, mean reward: 2.718 [2.137, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-1.004, 10.372], loss: 2.038423, mae: 0.860542, mean_q: 5.739668
 48820/100000: episode: 952, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 60.414, mean reward: 4.028 [3.108, 6.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.153, 10.525], loss: 0.564200, mae: 0.674846, mean_q: 5.354682
 48848/100000: episode: 953, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 142.184, mean reward: 5.078 [3.832, 7.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.078, 10.609], loss: 0.803750, mae: 0.663599, mean_q: 5.442464
 48861/100000: episode: 954, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 58.711, mean reward: 4.516 [2.682, 7.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.254, 10.444], loss: 0.915111, mae: 0.697076, mean_q: 5.380807
 48889/100000: episode: 955, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 101.413, mean reward: 3.622 [2.778, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.094, 10.575], loss: 2.342976, mae: 0.953962, mean_q: 5.932148
 48903/100000: episode: 956, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 41.450, mean reward: 2.961 [2.231, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.930, 10.407], loss: 0.572502, mae: 0.634354, mean_q: 5.253419
 48931/100000: episode: 957, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 156.488, mean reward: 5.589 [3.048, 12.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.277, 10.484], loss: 1.286452, mae: 0.633660, mean_q: 5.461420
 48949/100000: episode: 958, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 62.022, mean reward: 3.446 [2.895, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.530], loss: 0.677289, mae: 0.648596, mean_q: 5.657576
 48960/100000: episode: 959, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 32.068, mean reward: 2.915 [2.257, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.141, 10.511], loss: 0.580671, mae: 0.585832, mean_q: 5.548266
 48975/100000: episode: 960, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 61.580, mean reward: 4.105 [2.982, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.439], loss: 0.458585, mae: 0.578121, mean_q: 5.543457
 48989/100000: episode: 961, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 109.499, mean reward: 7.821 [3.069, 23.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.503, 10.732], loss: 0.475995, mae: 0.602871, mean_q: 5.664009
 49007/100000: episode: 962, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 59.222, mean reward: 3.290 [2.382, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.522], loss: 0.491574, mae: 0.611650, mean_q: 5.593218
 49026/100000: episode: 963, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 72.383, mean reward: 3.810 [2.430, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.465], loss: 2.162186, mae: 0.759068, mean_q: 5.645838
 49041/100000: episode: 964, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 43.618, mean reward: 2.908 [2.397, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.384], loss: 0.473973, mae: 0.654422, mean_q: 5.767315
 49056/100000: episode: 965, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 44.052, mean reward: 2.937 [1.863, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.440], loss: 0.420647, mae: 0.569032, mean_q: 5.671115
 49067/100000: episode: 966, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 60.976, mean reward: 5.543 [3.658, 7.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.575], loss: 0.643927, mae: 0.644513, mean_q: 5.599987
 49095/100000: episode: 967, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 151.597, mean reward: 5.414 [3.185, 23.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.661, 10.598], loss: 0.743571, mae: 0.656063, mean_q: 5.608429
 49101/100000: episode: 968, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 26.676, mean reward: 4.446 [3.485, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.524], loss: 1.641722, mae: 0.715380, mean_q: 5.662731
 49112/100000: episode: 969, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 41.000, mean reward: 3.727 [2.866, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.509], loss: 0.370371, mae: 0.557976, mean_q: 5.453733
 49128/100000: episode: 970, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 67.798, mean reward: 4.237 [3.383, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.304, 10.512], loss: 0.741735, mae: 0.608269, mean_q: 5.689399
 49142/100000: episode: 971, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 41.303, mean reward: 2.950 [2.409, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.557, 10.507], loss: 0.780043, mae: 0.730292, mean_q: 5.744094
 49161/100000: episode: 972, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 65.395, mean reward: 3.442 [2.574, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.565], loss: 1.329387, mae: 0.715707, mean_q: 5.794465
 49177/100000: episode: 973, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 43.912, mean reward: 2.745 [2.020, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.354], loss: 0.771111, mae: 0.660278, mean_q: 5.881442
[Info] 3-TH LEVEL FOUND: 12.944311141967773, Considering 10/90 traces
 49194/100000: episode: 974, duration: 4.541s, episode steps: 17, steps per second: 4, episode reward: 53.605, mean reward: 3.153 [2.194, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.608, 10.377], loss: 2.175675, mae: 0.854544, mean_q: 5.828951
 49221/100000: episode: 975, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 187.526, mean reward: 6.945 [3.497, 21.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.547, 10.589], loss: 0.676100, mae: 0.652587, mean_q: 5.906475
 49245/100000: episode: 976, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 82.890, mean reward: 3.454 [2.198, 5.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.498, 10.408], loss: 0.444300, mae: 0.580182, mean_q: 5.696632
[Info] FALSIFICATION!
 49253/100000: episode: 977, duration: 0.212s, episode steps: 8, steps per second: 38, episode reward: 1205.495, mean reward: 150.687 [5.908, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.017, 10.147], loss: 0.390713, mae: 0.573901, mean_q: 5.889067
 49280/100000: episode: 978, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 135.852, mean reward: 5.032 [3.255, 8.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.273, 10.499], loss: 566.356567, mae: 2.158756, mean_q: 6.211457
 49305/100000: episode: 979, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 138.437, mean reward: 5.537 [2.835, 8.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.017, 10.494], loss: 7.074265, mae: 2.594807, mean_q: 5.966514
 49330/100000: episode: 980, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 106.153, mean reward: 4.246 [2.912, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.196, 10.438], loss: 2.296983, mae: 1.112596, mean_q: 6.237359
 49357/100000: episode: 981, duration: 0.161s, episode steps: 27, steps per second: 167, episode reward: 106.807, mean reward: 3.956 [2.718, 5.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.857, 10.539], loss: 1.041219, mae: 0.860466, mean_q: 6.001429
 49384/100000: episode: 982, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 140.595, mean reward: 5.207 [3.828, 6.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.571], loss: 1.466049, mae: 0.817235, mean_q: 5.935630
 49411/100000: episode: 983, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 138.714, mean reward: 5.138 [3.866, 8.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.540, 10.588], loss: 569.234741, mae: 2.808674, mean_q: 7.178375
[Info] FALSIFICATION!
 49428/100000: episode: 984, duration: 0.361s, episode steps: 17, steps per second: 47, episode reward: 1180.523, mean reward: 69.443 [5.404, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.828, 10.401], loss: 4.157808, mae: 1.964783, mean_q: 5.740438
 49451/100000: episode: 985, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 145.719, mean reward: 6.336 [4.166, 11.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.047, 10.650], loss: 1.627275, mae: 1.107964, mean_q: 6.060430
 49473/100000: episode: 986, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 219.954, mean reward: 9.998 [3.584, 37.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.452, 10.609], loss: 7.509804, mae: 0.963827, mean_q: 6.281346
 49498/100000: episode: 987, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 125.007, mean reward: 5.000 [3.003, 10.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.283, 10.583], loss: 1224.449341, mae: 5.051356, mean_q: 8.144278
[Info] FALSIFICATION!
 49510/100000: episode: 988, duration: 0.240s, episode steps: 12, steps per second: 50, episode reward: 1200.257, mean reward: 100.021 [4.936, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.663, 10.285], loss: 7.943775, mae: 2.825999, mean_q: 8.999688
[Info] FALSIFICATION!
 49524/100000: episode: 989, duration: 0.338s, episode steps: 14, steps per second: 41, episode reward: 1169.842, mean reward: 83.560 [5.904, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.409, 9.839], loss: 2.695667, mae: 1.221190, mean_q: 6.248522
[Info] FALSIFICATION!
 49526/100000: episode: 990, duration: 0.185s, episode steps: 2, steps per second: 11, episode reward: 1010.541, mean reward: 505.271 [10.541, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.017, 9.239], loss: 1.472995, mae: 1.081205, mean_q: 5.718385
 49552/100000: episode: 991, duration: 0.167s, episode steps: 26, steps per second: 155, episode reward: 92.507, mean reward: 3.558 [2.560, 5.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.751, 10.450], loss: 591.999756, mae: 2.626575, mean_q: 7.039093
 49577/100000: episode: 992, duration: 0.156s, episode steps: 25, steps per second: 160, episode reward: 92.877, mean reward: 3.715 [2.788, 5.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.205, 10.435], loss: 5.791303, mae: 2.121711, mean_q: 7.618519
 49603/100000: episode: 993, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 300.954, mean reward: 11.575 [5.329, 25.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.841, 10.601], loss: 589.701050, mae: 3.103107, mean_q: 7.807652
 49630/100000: episode: 994, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 148.546, mean reward: 5.502 [2.536, 28.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.388, 10.455], loss: 561.834656, mae: 2.903422, mean_q: 7.843312
 49648/100000: episode: 995, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 108.894, mean reward: 6.050 [4.297, 8.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.715, 10.581], loss: 4.073587, mae: 1.371619, mean_q: 6.838293
 49671/100000: episode: 996, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 465.673, mean reward: 20.247 [7.163, 115.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.576], loss: 1325.924927, mae: 5.852484, mean_q: 9.470745
 49698/100000: episode: 997, duration: 0.164s, episode steps: 27, steps per second: 164, episode reward: 148.254, mean reward: 5.491 [2.932, 17.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.566], loss: 1124.687256, mae: 5.671257, mean_q: 9.741548
 49725/100000: episode: 998, duration: 0.172s, episode steps: 27, steps per second: 157, episode reward: 137.632, mean reward: 5.097 [2.869, 11.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.164, 10.525], loss: 11.314437, mae: 1.603507, mean_q: 6.566684
 49747/100000: episode: 999, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 167.115, mean reward: 7.596 [4.534, 15.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.270, 10.625], loss: 4.105991, mae: 1.394506, mean_q: 7.627919
 49774/100000: episode: 1000, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 158.280, mean reward: 5.862 [2.844, 9.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.203, 10.588], loss: 569.145447, mae: 3.439530, mean_q: 8.892470
 49796/100000: episode: 1001, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 82.916, mean reward: 3.769 [2.823, 6.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.297, 10.449], loss: 2.246696, mae: 1.249433, mean_q: 6.921464
 49821/100000: episode: 1002, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 121.366, mean reward: 4.855 [3.250, 8.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.043, 10.619], loss: 7.407458, mae: 1.524415, mean_q: 7.541159
 49845/100000: episode: 1003, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 84.171, mean reward: 3.507 [2.767, 5.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.490], loss: 8.195600, mae: 1.614252, mean_q: 7.865452
 49867/100000: episode: 1004, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 156.447, mean reward: 7.111 [5.119, 11.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.215, 10.633], loss: 1383.272583, mae: 6.193961, mean_q: 10.273978
 49894/100000: episode: 1005, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 165.834, mean reward: 6.142 [2.687, 14.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.694, 10.327], loss: 4.436415, mae: 1.758515, mean_q: 7.540242
 49921/100000: episode: 1006, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 192.952, mean reward: 7.146 [3.699, 21.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.043, 10.622], loss: 1681.627075, mae: 6.193848, mean_q: 9.352070
 49946/100000: episode: 1007, duration: 0.149s, episode steps: 25, steps per second: 167, episode reward: 96.082, mean reward: 3.843 [2.232, 6.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.341, 10.465], loss: 613.624268, mae: 4.305317, mean_q: 9.604608
 49968/100000: episode: 1008, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 137.594, mean reward: 6.254 [2.610, 14.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.242, 10.418], loss: 688.229248, mae: 4.367394, mean_q: 10.124432
 49995/100000: episode: 1009, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 149.791, mean reward: 5.548 [3.371, 10.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.974, 10.614], loss: 6.316207, mae: 1.925079, mean_q: 7.656930
 50019/100000: episode: 1010, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 173.805, mean reward: 7.242 [3.464, 17.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.405, 10.464], loss: 1866.243164, mae: 6.660250, mean_q: 10.092655
 50046/100000: episode: 1011, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 103.427, mean reward: 3.831 [2.650, 5.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.273, 10.460], loss: 14.556737, mae: 2.920008, mean_q: 9.470105
 50072/100000: episode: 1012, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 270.920, mean reward: 10.420 [3.190, 99.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.535], loss: 6.732573, mae: 2.121029, mean_q: 9.320763
 50097/100000: episode: 1013, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 162.939, mean reward: 6.518 [4.424, 10.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.149, 10.642], loss: 604.649780, mae: 4.173697, mean_q: 10.529300
 50124/100000: episode: 1014, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 1052.410, mean reward: 38.978 [4.323, 728.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.974, 10.797], loss: 568.132507, mae: 3.728021, mean_q: 9.775294
 50142/100000: episode: 1015, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 183.924, mean reward: 10.218 [4.588, 22.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.383, 10.542], loss: 8.018894, mae: 2.431382, mean_q: 9.478337
 50167/100000: episode: 1016, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 112.010, mean reward: 4.480 [2.466, 10.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.534], loss: 14.668238, mae: 2.113798, mean_q: 8.596994
 50194/100000: episode: 1017, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 261.701, mean reward: 9.693 [2.697, 120.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.817, 10.397], loss: 568.800171, mae: 3.291017, mean_q: 9.308104
 50216/100000: episode: 1018, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 112.981, mean reward: 5.135 [3.009, 8.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.502], loss: 1753.892944, mae: 8.648247, mean_q: 13.097548
 50238/100000: episode: 1019, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 263.501, mean reward: 11.977 [3.509, 63.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.269, 10.428], loss: 702.516357, mae: 6.470436, mean_q: 12.768098
 50264/100000: episode: 1020, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 110.817, mean reward: 4.262 [3.059, 7.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.341, 10.530], loss: 1164.312012, mae: 5.502832, mean_q: 9.556721
 50288/100000: episode: 1021, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 205.628, mean reward: 8.568 [3.342, 20.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.856, 10.533], loss: 636.030334, mae: 4.890096, mean_q: 12.496872
 50313/100000: episode: 1022, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 88.170, mean reward: 3.527 [2.508, 4.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.131, 10.545], loss: 932.419617, mae: 4.087546, mean_q: 9.343812
 50339/100000: episode: 1023, duration: 0.191s, episode steps: 26, steps per second: 136, episode reward: 92.706, mean reward: 3.566 [2.357, 5.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.275, 10.445], loss: 7.907807, mae: 2.961636, mean_q: 11.566201
 50365/100000: episode: 1024, duration: 0.181s, episode steps: 26, steps per second: 144, episode reward: 179.997, mean reward: 6.923 [4.723, 9.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.756, 10.503], loss: 600.197388, mae: 3.485323, mean_q: 9.439860
 50392/100000: episode: 1025, duration: 0.200s, episode steps: 27, steps per second: 135, episode reward: 156.400, mean reward: 5.793 [3.150, 15.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.556], loss: 584.572876, mae: 3.807857, mean_q: 10.564494
[Info] FALSIFICATION!
 50397/100000: episode: 1026, duration: 0.205s, episode steps: 5, steps per second: 24, episode reward: 1512.915, mean reward: 302.583 [8.029, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.018, 9.802], loss: 4.741829, mae: 2.139870, mean_q: 10.230803
 50415/100000: episode: 1027, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 187.056, mean reward: 10.392 [4.814, 26.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.632], loss: 844.893433, mae: 4.316818, mean_q: 10.743862
 50440/100000: episode: 1028, duration: 0.177s, episode steps: 25, steps per second: 142, episode reward: 99.615, mean reward: 3.985 [2.340, 7.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.377], loss: 736.040222, mae: 5.826262, mean_q: 12.930645
 50467/100000: episode: 1029, duration: 0.168s, episode steps: 27, steps per second: 161, episode reward: 213.130, mean reward: 7.894 [3.491, 19.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.655], loss: 1966.014893, mae: 7.887499, mean_q: 11.427293
 50494/100000: episode: 1030, duration: 0.207s, episode steps: 27, steps per second: 131, episode reward: 120.974, mean reward: 4.481 [2.119, 7.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.236, 10.379], loss: 19.371172, mae: 3.525299, mean_q: 10.430976
 50520/100000: episode: 1031, duration: 0.186s, episode steps: 26, steps per second: 140, episode reward: 182.273, mean reward: 7.011 [4.060, 13.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.239, 10.664], loss: 587.235962, mae: 3.611358, mean_q: 9.638886
 50547/100000: episode: 1032, duration: 0.180s, episode steps: 27, steps per second: 150, episode reward: 317.574, mean reward: 11.762 [3.680, 57.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.336, 10.513], loss: 564.764526, mae: 3.260344, mean_q: 9.945454
[Info] FALSIFICATION!
 50559/100000: episode: 1033, duration: 0.320s, episode steps: 12, steps per second: 38, episode reward: 1679.125, mean reward: 139.927 [7.742, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.212, 10.179], loss: 1259.708984, mae: 5.259819, mean_q: 11.057708
 50586/100000: episode: 1034, duration: 0.200s, episode steps: 27, steps per second: 135, episode reward: 113.360, mean reward: 4.199 [2.889, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.942, 10.511], loss: 7.930240, mae: 2.716116, mean_q: 10.961156
 50610/100000: episode: 1035, duration: 0.169s, episode steps: 24, steps per second: 142, episode reward: 139.745, mean reward: 5.823 [3.453, 12.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.710, 10.553], loss: 1868.889282, mae: 5.950500, mean_q: 9.740720
 50637/100000: episode: 1036, duration: 0.187s, episode steps: 27, steps per second: 144, episode reward: 233.739, mean reward: 8.657 [3.951, 42.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.392, 10.490], loss: 1466.668579, mae: 9.550111, mean_q: 15.351051
 50655/100000: episode: 1037, duration: 0.123s, episode steps: 18, steps per second: 146, episode reward: 111.026, mean reward: 6.168 [4.480, 11.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.493, 10.569], loss: 270.879730, mae: 4.237928, mean_q: 11.888676
 50682/100000: episode: 1038, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 218.011, mean reward: 8.074 [3.527, 52.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.154, 10.499], loss: 1102.076904, mae: 5.202185, mean_q: 11.575792
 50709/100000: episode: 1039, duration: 0.172s, episode steps: 27, steps per second: 157, episode reward: 133.545, mean reward: 4.946 [2.290, 12.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.830, 10.388], loss: 137.420349, mae: 3.379004, mean_q: 11.200895
 50736/100000: episode: 1040, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 100.590, mean reward: 3.726 [2.248, 10.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.603, 10.346], loss: 12.889318, mae: 2.136129, mean_q: 9.408082
 50763/100000: episode: 1041, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 150.665, mean reward: 5.580 [3.782, 11.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.405, 10.508], loss: 327.403717, mae: 3.361824, mean_q: 10.457833
 50789/100000: episode: 1042, duration: 0.175s, episode steps: 26, steps per second: 149, episode reward: 151.658, mean reward: 5.833 [3.562, 21.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.310, 10.503], loss: 1338.720093, mae: 6.667410, mean_q: 12.664330
[Info] FALSIFICATION!
 50790/100000: episode: 1043, duration: 0.262s, episode steps: 1, steps per second: 4, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.020, 9.042], loss: 24.026356, mae: 5.204828, mean_q: 13.918229
 50813/100000: episode: 1044, duration: 0.155s, episode steps: 23, steps per second: 148, episode reward: 248.028, mean reward: 10.784 [2.927, 35.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.435, 10.509], loss: 1210.586426, mae: 6.370330, mean_q: 12.126054
 50838/100000: episode: 1045, duration: 0.184s, episode steps: 25, steps per second: 136, episode reward: 165.111, mean reward: 6.604 [3.154, 23.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.382, 10.669], loss: 1210.252075, mae: 6.085826, mean_q: 12.832775
 50865/100000: episode: 1046, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 94.460, mean reward: 3.499 [2.631, 5.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.399], loss: 1112.559448, mae: 7.422050, mean_q: 14.577617
 50890/100000: episode: 1047, duration: 0.164s, episode steps: 25, steps per second: 153, episode reward: 116.211, mean reward: 4.648 [3.157, 7.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.315, 10.524], loss: 10.554261, mae: 2.112634, mean_q: 9.680563
 50915/100000: episode: 1048, duration: 0.180s, episode steps: 25, steps per second: 139, episode reward: 105.217, mean reward: 4.209 [2.033, 7.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.993, 10.356], loss: 11.373078, mae: 1.940083, mean_q: 10.044549
 50937/100000: episode: 1049, duration: 0.150s, episode steps: 22, steps per second: 146, episode reward: 137.066, mean reward: 6.230 [2.851, 21.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.178, 10.452], loss: 13.481998, mae: 1.931812, mean_q: 10.094239
 50964/100000: episode: 1050, duration: 0.194s, episode steps: 27, steps per second: 139, episode reward: 103.663, mean reward: 3.839 [2.441, 6.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.688, 10.455], loss: 1549.632324, mae: 6.948398, mean_q: 12.369909
 50982/100000: episode: 1051, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 120.465, mean reward: 6.692 [4.980, 11.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.413, 10.645], loss: 7.517232, mae: 2.868170, mean_q: 12.356996
 51009/100000: episode: 1052, duration: 0.176s, episode steps: 27, steps per second: 153, episode reward: 102.377, mean reward: 3.792 [2.455, 5.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.507, 10.440], loss: 576.168518, mae: 3.240140, mean_q: 9.807286
 51036/100000: episode: 1053, duration: 0.197s, episode steps: 27, steps per second: 137, episode reward: 192.045, mean reward: 7.113 [3.499, 38.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.704, 10.541], loss: 1105.265137, mae: 7.158696, mean_q: 14.665459
 51063/100000: episode: 1054, duration: 0.207s, episode steps: 27, steps per second: 131, episode reward: 212.232, mean reward: 7.860 [3.968, 19.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.110, 10.592], loss: 564.116333, mae: 3.385747, mean_q: 10.269621
 51087/100000: episode: 1055, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 215.994, mean reward: 9.000 [2.885, 36.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.287, 10.519], loss: 976.800293, mae: 4.480332, mean_q: 10.245186
 51112/100000: episode: 1056, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 105.645, mean reward: 4.226 [3.390, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.468, 10.560], loss: 1400.888184, mae: 8.500097, mean_q: 14.885752
 51136/100000: episode: 1057, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 129.871, mean reward: 5.411 [3.626, 7.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.167, 10.629], loss: 1442.056763, mae: 5.796307, mean_q: 11.042531
 51160/100000: episode: 1058, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 167.871, mean reward: 6.995 [3.567, 11.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.774, 10.685], loss: 633.294861, mae: 5.562718, mean_q: 13.422709
 51187/100000: episode: 1059, duration: 0.182s, episode steps: 27, steps per second: 148, episode reward: 101.982, mean reward: 3.777 [2.186, 6.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.712, 10.391], loss: 6.627132, mae: 2.266785, mean_q: 11.043359
 51211/100000: episode: 1060, duration: 0.216s, episode steps: 24, steps per second: 111, episode reward: 137.264, mean reward: 5.719 [2.978, 10.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.456, 10.599], loss: 14.401806, mae: 2.230858, mean_q: 10.119573
 51237/100000: episode: 1061, duration: 0.220s, episode steps: 26, steps per second: 118, episode reward: 106.414, mean reward: 4.093 [3.228, 7.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.064, 10.544], loss: 1168.457886, mae: 5.813021, mean_q: 12.107061
 51264/100000: episode: 1062, duration: 0.170s, episode steps: 27, steps per second: 159, episode reward: 103.112, mean reward: 3.819 [2.543, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.623, 10.513], loss: 568.754578, mae: 5.035089, mean_q: 13.525959
 51291/100000: episode: 1063, duration: 0.175s, episode steps: 27, steps per second: 154, episode reward: 111.780, mean reward: 4.140 [3.212, 7.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.554], loss: 178.376175, mae: 2.611885, mean_q: 10.591747
[Info] Complete ISplit Iteration
[Info] Levels: [5.1344185, 8.227393, 12.944311, 25.178305]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.88]
[Info] Error Prob: 0.0008800000000000002

 51309/100000: episode: 1064, duration: 5.535s, episode steps: 18, steps per second: 3, episode reward: 86.369, mean reward: 4.798 [3.416, 7.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.272, 10.511], loss: 1677.593628, mae: 6.187285, mean_q: 11.781528
 51409/100000: episode: 1065, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 192.851, mean reward: 1.929 [1.486, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.583, 10.165], loss: 912.618530, mae: 6.003985, mean_q: 13.040177
 51509/100000: episode: 1066, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 198.954, mean reward: 1.990 [1.436, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.817, 10.374], loss: 1392.438721, mae: 7.468983, mean_q: 13.581321
 51609/100000: episode: 1067, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 178.365, mean reward: 1.784 [1.439, 2.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.351, 10.098], loss: 1048.705811, mae: 5.517463, mean_q: 12.852039
 51709/100000: episode: 1068, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 217.864, mean reward: 2.179 [1.537, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.364, 10.098], loss: 976.229675, mae: 5.686853, mean_q: 12.894936
 51809/100000: episode: 1069, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 201.015, mean reward: 2.010 [1.485, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.923, 10.098], loss: 856.935486, mae: 5.243598, mean_q: 12.463440
 51909/100000: episode: 1070, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 208.831, mean reward: 2.088 [1.492, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.510, 10.276], loss: 970.276611, mae: 6.154377, mean_q: 13.306832
 52009/100000: episode: 1071, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 199.910, mean reward: 1.999 [1.456, 10.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.496, 10.098], loss: 874.951538, mae: 5.306082, mean_q: 12.565329
 52109/100000: episode: 1072, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 205.644, mean reward: 2.056 [1.450, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.127, 10.098], loss: 1015.116333, mae: 6.191877, mean_q: 13.522702
 52209/100000: episode: 1073, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 193.007, mean reward: 1.930 [1.463, 5.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.127, 10.301], loss: 952.407593, mae: 5.432155, mean_q: 12.807047
 52309/100000: episode: 1074, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 203.466, mean reward: 2.035 [1.504, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.364, 10.332], loss: 773.421509, mae: 5.560209, mean_q: 12.765151
 52409/100000: episode: 1075, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 183.098, mean reward: 1.831 [1.455, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.531, 10.164], loss: 1031.791260, mae: 5.963390, mean_q: 13.013402
 52509/100000: episode: 1076, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 193.229, mean reward: 1.932 [1.456, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.539, 10.168], loss: 788.248901, mae: 4.741287, mean_q: 11.520236
 52609/100000: episode: 1077, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 191.187, mean reward: 1.912 [1.477, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.098], loss: 934.642822, mae: 5.718857, mean_q: 12.926457
 52709/100000: episode: 1078, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 198.111, mean reward: 1.981 [1.542, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.030, 10.098], loss: 967.570557, mae: 5.479781, mean_q: 12.604130
 52809/100000: episode: 1079, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 194.099, mean reward: 1.941 [1.473, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.098], loss: 610.924988, mae: 3.848444, mean_q: 11.034118
 52909/100000: episode: 1080, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 196.077, mean reward: 1.961 [1.471, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.282, 10.098], loss: 445.555084, mae: 3.902852, mean_q: 10.988489
 53009/100000: episode: 1081, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 183.540, mean reward: 1.835 [1.461, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.002, 10.105], loss: 1009.519470, mae: 6.032174, mean_q: 12.164408
 53109/100000: episode: 1082, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 185.111, mean reward: 1.851 [1.460, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.363, 10.165], loss: 573.357056, mae: 3.887200, mean_q: 10.739608
 53209/100000: episode: 1083, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 193.639, mean reward: 1.936 [1.446, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.263, 10.219], loss: 634.658264, mae: 4.287464, mean_q: 11.157928
 53309/100000: episode: 1084, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 199.064, mean reward: 1.991 [1.445, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.685, 10.098], loss: 1028.466675, mae: 5.755895, mean_q: 11.875849
 53409/100000: episode: 1085, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 187.404, mean reward: 1.874 [1.478, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.203, 10.195], loss: 989.016174, mae: 5.456315, mean_q: 11.732553
 53509/100000: episode: 1086, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 182.201, mean reward: 1.822 [1.470, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.444, 10.245], loss: 709.837952, mae: 4.239917, mean_q: 10.562501
 53609/100000: episode: 1087, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 186.558, mean reward: 1.866 [1.454, 4.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.423, 10.153], loss: 1130.557495, mae: 6.005992, mean_q: 12.431223
 53709/100000: episode: 1088, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 182.736, mean reward: 1.827 [1.465, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.709, 10.098], loss: 948.114380, mae: 4.731307, mean_q: 11.002275
 53809/100000: episode: 1089, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 185.482, mean reward: 1.855 [1.488, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.333, 10.098], loss: 1211.797119, mae: 6.574048, mean_q: 12.747781
 53909/100000: episode: 1090, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 186.300, mean reward: 1.863 [1.448, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.669, 10.114], loss: 532.898438, mae: 3.533940, mean_q: 10.379151
 54009/100000: episode: 1091, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 179.750, mean reward: 1.797 [1.476, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.467, 10.098], loss: 1407.771362, mae: 6.882938, mean_q: 12.639135
 54109/100000: episode: 1092, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 189.227, mean reward: 1.892 [1.462, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.098], loss: 1112.711304, mae: 5.704082, mean_q: 11.530601
 54209/100000: episode: 1093, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 195.682, mean reward: 1.957 [1.483, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.015, 10.275], loss: 825.623901, mae: 4.698106, mean_q: 11.040901
 54309/100000: episode: 1094, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 212.778, mean reward: 2.128 [1.534, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.946, 10.260], loss: 899.210083, mae: 4.983100, mean_q: 11.169717
 54409/100000: episode: 1095, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 195.447, mean reward: 1.954 [1.442, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.063, 10.132], loss: 1104.827515, mae: 5.638538, mean_q: 10.823138
 54509/100000: episode: 1096, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 188.733, mean reward: 1.887 [1.452, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.377, 10.125], loss: 590.031738, mae: 3.489612, mean_q: 9.461444
 54609/100000: episode: 1097, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 185.831, mean reward: 1.858 [1.480, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.953, 10.098], loss: 351.506836, mae: 3.209811, mean_q: 9.425440
 54709/100000: episode: 1098, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 202.138, mean reward: 2.021 [1.460, 8.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.337, 10.098], loss: 453.648041, mae: 3.050899, mean_q: 8.485472
 54809/100000: episode: 1099, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 175.106, mean reward: 1.751 [1.457, 2.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.855, 10.142], loss: 466.342529, mae: 2.924243, mean_q: 8.335398
 54909/100000: episode: 1100, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 187.856, mean reward: 1.879 [1.474, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.030, 10.098], loss: 666.427917, mae: 3.867022, mean_q: 8.400449
 55009/100000: episode: 1101, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 181.349, mean reward: 1.813 [1.451, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.827, 10.302], loss: 134.648056, mae: 1.789265, mean_q: 7.346551
 55109/100000: episode: 1102, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 230.950, mean reward: 2.309 [1.468, 5.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.767, 10.098], loss: 6.656923, mae: 1.061252, mean_q: 6.579714
 55209/100000: episode: 1103, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 197.867, mean reward: 1.979 [1.457, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.257, 10.150], loss: 618.688660, mae: 3.130611, mean_q: 7.198111
 55309/100000: episode: 1104, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 201.744, mean reward: 2.017 [1.450, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.198, 10.484], loss: 589.196106, mae: 3.023975, mean_q: 7.358778
 55409/100000: episode: 1105, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 192.389, mean reward: 1.924 [1.438, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.685, 10.098], loss: 155.166641, mae: 1.480410, mean_q: 6.358010
 55509/100000: episode: 1106, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 193.103, mean reward: 1.931 [1.568, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.247], loss: 294.933197, mae: 1.916984, mean_q: 6.062373
 55609/100000: episode: 1107, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 184.759, mean reward: 1.848 [1.473, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.711, 10.098], loss: 2.108445, mae: 0.668655, mean_q: 5.064236
 55709/100000: episode: 1108, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 195.719, mean reward: 1.957 [1.430, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.497, 10.098], loss: 0.833163, mae: 0.520490, mean_q: 4.639734
 55809/100000: episode: 1109, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 189.622, mean reward: 1.896 [1.441, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.402], loss: 0.941912, mae: 0.511134, mean_q: 4.588845
 55909/100000: episode: 1110, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 194.431, mean reward: 1.944 [1.481, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.452, 10.123], loss: 0.306588, mae: 0.417088, mean_q: 4.329891
 56009/100000: episode: 1111, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 182.980, mean reward: 1.830 [1.446, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.131], loss: 0.291789, mae: 0.403285, mean_q: 4.251016
 56109/100000: episode: 1112, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 186.450, mean reward: 1.865 [1.454, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.922, 10.187], loss: 0.330572, mae: 0.396820, mean_q: 4.035643
 56209/100000: episode: 1113, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 182.142, mean reward: 1.821 [1.459, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.171, 10.205], loss: 0.136150, mae: 0.337999, mean_q: 3.960914
 56309/100000: episode: 1114, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 184.565, mean reward: 1.846 [1.478, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.155], loss: 0.111057, mae: 0.319222, mean_q: 3.827918
 56409/100000: episode: 1115, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 205.374, mean reward: 2.054 [1.437, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.473, 10.098], loss: 0.128289, mae: 0.323440, mean_q: 3.839989
 56509/100000: episode: 1116, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 186.965, mean reward: 1.870 [1.453, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.538, 10.098], loss: 0.104315, mae: 0.303175, mean_q: 3.819306
 56609/100000: episode: 1117, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 209.412, mean reward: 2.094 [1.475, 11.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.868, 10.098], loss: 0.123522, mae: 0.318018, mean_q: 3.828205
 56709/100000: episode: 1118, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 200.686, mean reward: 2.007 [1.463, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.521, 10.098], loss: 0.102007, mae: 0.303331, mean_q: 3.816833
 56809/100000: episode: 1119, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 192.757, mean reward: 1.928 [1.449, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.528, 10.098], loss: 0.105685, mae: 0.311944, mean_q: 3.821266
 56909/100000: episode: 1120, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 191.512, mean reward: 1.915 [1.435, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.587, 10.145], loss: 0.139495, mae: 0.332212, mean_q: 3.820353
 57009/100000: episode: 1121, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 201.498, mean reward: 2.015 [1.473, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.239, 10.361], loss: 0.127470, mae: 0.314307, mean_q: 3.815023
 57109/100000: episode: 1122, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 231.771, mean reward: 2.318 [1.443, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.351], loss: 0.128243, mae: 0.313856, mean_q: 3.829012
 57209/100000: episode: 1123, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 193.215, mean reward: 1.932 [1.470, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.390, 10.130], loss: 0.087082, mae: 0.297705, mean_q: 3.820999
 57309/100000: episode: 1124, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 209.921, mean reward: 2.099 [1.442, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.371, 10.188], loss: 0.105585, mae: 0.306085, mean_q: 3.823854
 57409/100000: episode: 1125, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 189.854, mean reward: 1.899 [1.464, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.997, 10.098], loss: 0.087677, mae: 0.297109, mean_q: 3.818835
 57509/100000: episode: 1126, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 199.264, mean reward: 1.993 [1.437, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.469, 10.098], loss: 0.097563, mae: 0.299335, mean_q: 3.820064
 57609/100000: episode: 1127, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 197.296, mean reward: 1.973 [1.472, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.186, 10.158], loss: 0.096516, mae: 0.296829, mean_q: 3.829736
 57709/100000: episode: 1128, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 202.197, mean reward: 2.022 [1.456, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.717, 10.098], loss: 0.110495, mae: 0.311575, mean_q: 3.839975
 57809/100000: episode: 1129, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 203.011, mean reward: 2.030 [1.515, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.261], loss: 0.095020, mae: 0.302749, mean_q: 3.837878
 57909/100000: episode: 1130, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 190.276, mean reward: 1.903 [1.453, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.023, 10.216], loss: 0.096244, mae: 0.299416, mean_q: 3.821077
 58009/100000: episode: 1131, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 195.880, mean reward: 1.959 [1.455, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.190], loss: 0.099368, mae: 0.297535, mean_q: 3.856053
 58109/100000: episode: 1132, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 225.141, mean reward: 2.251 [1.442, 4.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.439, 10.170], loss: 0.089866, mae: 0.291114, mean_q: 3.824873
 58209/100000: episode: 1133, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 183.719, mean reward: 1.837 [1.469, 2.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.444, 10.335], loss: 0.110127, mae: 0.303567, mean_q: 3.849537
 58309/100000: episode: 1134, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 210.300, mean reward: 2.103 [1.499, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.483, 10.098], loss: 0.097650, mae: 0.300013, mean_q: 3.848080
 58409/100000: episode: 1135, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 196.475, mean reward: 1.965 [1.459, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.060, 10.329], loss: 0.090211, mae: 0.295261, mean_q: 3.830793
 58509/100000: episode: 1136, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 204.093, mean reward: 2.041 [1.444, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.295, 10.123], loss: 0.107986, mae: 0.309284, mean_q: 3.862872
 58609/100000: episode: 1137, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 184.498, mean reward: 1.845 [1.464, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.026, 10.098], loss: 0.109462, mae: 0.304275, mean_q: 3.851528
 58709/100000: episode: 1138, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 197.616, mean reward: 1.976 [1.477, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.860, 10.209], loss: 0.132997, mae: 0.311035, mean_q: 3.872320
 58809/100000: episode: 1139, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 207.498, mean reward: 2.075 [1.527, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.428, 10.098], loss: 0.144548, mae: 0.314544, mean_q: 3.879861
 58909/100000: episode: 1140, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 187.899, mean reward: 1.879 [1.435, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.566, 10.098], loss: 0.096106, mae: 0.303486, mean_q: 3.880791
 59009/100000: episode: 1141, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 202.782, mean reward: 2.028 [1.477, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.988, 10.098], loss: 0.092358, mae: 0.296191, mean_q: 3.883254
 59109/100000: episode: 1142, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 186.943, mean reward: 1.869 [1.444, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.985, 10.176], loss: 0.103634, mae: 0.309598, mean_q: 3.899126
 59209/100000: episode: 1143, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.487, mean reward: 1.995 [1.486, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.840, 10.098], loss: 0.107728, mae: 0.304488, mean_q: 3.887026
 59309/100000: episode: 1144, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 192.768, mean reward: 1.928 [1.456, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.690, 10.098], loss: 0.128206, mae: 0.304471, mean_q: 3.893428
 59409/100000: episode: 1145, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 194.123, mean reward: 1.941 [1.493, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.649, 10.098], loss: 0.089386, mae: 0.292485, mean_q: 3.865728
 59509/100000: episode: 1146, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 198.670, mean reward: 1.987 [1.447, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.111], loss: 0.094468, mae: 0.299423, mean_q: 3.878256
 59609/100000: episode: 1147, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.008, mean reward: 1.820 [1.441, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.401, 10.098], loss: 0.094884, mae: 0.300547, mean_q: 3.866828
 59709/100000: episode: 1148, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.853, mean reward: 1.859 [1.475, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.239, 10.437], loss: 0.105979, mae: 0.289161, mean_q: 3.880308
 59809/100000: episode: 1149, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 204.248, mean reward: 2.042 [1.518, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.523, 10.323], loss: 0.091372, mae: 0.298203, mean_q: 3.889059
 59909/100000: episode: 1150, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 204.182, mean reward: 2.042 [1.523, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.476, 10.098], loss: 0.117512, mae: 0.303399, mean_q: 3.891895
 60009/100000: episode: 1151, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 182.236, mean reward: 1.822 [1.437, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.098], loss: 0.088195, mae: 0.296834, mean_q: 3.883328
 60109/100000: episode: 1152, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.695, mean reward: 1.877 [1.445, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.238, 10.098], loss: 0.087741, mae: 0.290961, mean_q: 3.883635
 60209/100000: episode: 1153, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 181.726, mean reward: 1.817 [1.489, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.089, 10.211], loss: 0.104590, mae: 0.306153, mean_q: 3.891402
 60309/100000: episode: 1154, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 198.118, mean reward: 1.981 [1.495, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.603, 10.098], loss: 0.101167, mae: 0.293975, mean_q: 3.888663
 60409/100000: episode: 1155, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 185.967, mean reward: 1.860 [1.477, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.332, 10.117], loss: 0.081860, mae: 0.284776, mean_q: 3.865337
 60509/100000: episode: 1156, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 213.701, mean reward: 2.137 [1.459, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.434, 10.098], loss: 0.084602, mae: 0.292911, mean_q: 3.872217
 60609/100000: episode: 1157, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 201.564, mean reward: 2.016 [1.485, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.485, 10.203], loss: 0.096537, mae: 0.292590, mean_q: 3.874653
 60709/100000: episode: 1158, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 198.773, mean reward: 1.988 [1.470, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.045, 10.098], loss: 0.094089, mae: 0.292326, mean_q: 3.884898
 60809/100000: episode: 1159, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 188.534, mean reward: 1.885 [1.458, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.395, 10.098], loss: 0.082735, mae: 0.285769, mean_q: 3.878811
 60909/100000: episode: 1160, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 177.584, mean reward: 1.776 [1.459, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.312, 10.175], loss: 0.112317, mae: 0.304577, mean_q: 3.898691
 61009/100000: episode: 1161, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 184.098, mean reward: 1.841 [1.446, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.447, 10.307], loss: 0.085745, mae: 0.285898, mean_q: 3.886676
 61109/100000: episode: 1162, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 181.100, mean reward: 1.811 [1.476, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.202], loss: 0.078886, mae: 0.282143, mean_q: 3.865847
 61209/100000: episode: 1163, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 188.164, mean reward: 1.882 [1.479, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.257], loss: 0.089097, mae: 0.297033, mean_q: 3.872411
[Info] 1-TH LEVEL FOUND: 5.045370578765869, Considering 10/90 traces
 61309/100000: episode: 1164, duration: 4.823s, episode steps: 100, steps per second: 21, episode reward: 188.798, mean reward: 1.888 [1.465, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.534, 10.150], loss: 0.091514, mae: 0.285955, mean_q: 3.893598
 61328/100000: episode: 1165, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 70.418, mean reward: 3.706 [2.006, 9.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.286, 10.567], loss: 0.086859, mae: 0.297322, mean_q: 3.899611
 61332/100000: episode: 1166, duration: 0.034s, episode steps: 4, steps per second: 118, episode reward: 12.157, mean reward: 3.039 [2.654, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.453], loss: 0.117131, mae: 0.340147, mean_q: 3.888073
 61358/100000: episode: 1167, duration: 0.152s, episode steps: 26, steps per second: 170, episode reward: 58.882, mean reward: 2.265 [1.721, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.268, 10.259], loss: 0.088135, mae: 0.296929, mean_q: 3.896416
 61384/100000: episode: 1168, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 61.507, mean reward: 2.366 [1.771, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.101, 10.281], loss: 0.095039, mae: 0.301747, mean_q: 3.897640
 61397/100000: episode: 1169, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 35.506, mean reward: 2.731 [2.383, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.877, 10.326], loss: 0.077322, mae: 0.281977, mean_q: 3.899611
 61449/100000: episode: 1170, duration: 0.326s, episode steps: 52, steps per second: 160, episode reward: 114.021, mean reward: 2.193 [1.498, 5.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-1.733, 10.100], loss: 0.089140, mae: 0.302905, mean_q: 3.907843
 61493/100000: episode: 1171, duration: 0.260s, episode steps: 44, steps per second: 169, episode reward: 114.757, mean reward: 2.608 [1.564, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.297, 10.194], loss: 0.084973, mae: 0.286267, mean_q: 3.877205
 61530/100000: episode: 1172, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 117.104, mean reward: 3.165 [2.043, 9.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.373, 10.511], loss: 0.107953, mae: 0.316717, mean_q: 3.951290
 61584/100000: episode: 1173, duration: 0.298s, episode steps: 54, steps per second: 181, episode reward: 111.082, mean reward: 2.057 [1.458, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.322, 10.210], loss: 0.097156, mae: 0.304746, mean_q: 3.923098
 61638/100000: episode: 1174, duration: 0.313s, episode steps: 54, steps per second: 172, episode reward: 114.819, mean reward: 2.126 [1.454, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.468, 10.100], loss: 0.102155, mae: 0.301383, mean_q: 3.913038
 61651/100000: episode: 1175, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 39.733, mean reward: 3.056 [2.444, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.426, 10.526], loss: 0.101603, mae: 0.305323, mean_q: 3.864015
 61701/100000: episode: 1176, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 98.971, mean reward: 1.979 [1.460, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.401, 10.104], loss: 0.109165, mae: 0.302847, mean_q: 3.941511
 61745/100000: episode: 1177, duration: 0.259s, episode steps: 44, steps per second: 170, episode reward: 91.977, mean reward: 2.090 [1.467, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.330, 10.135], loss: 0.087292, mae: 0.293291, mean_q: 3.971547
 61749/100000: episode: 1178, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 9.937, mean reward: 2.484 [2.372, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.378], loss: 0.118881, mae: 0.348910, mean_q: 3.994413
 61799/100000: episode: 1179, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 145.266, mean reward: 2.905 [1.655, 6.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.327, 10.213], loss: 0.092097, mae: 0.299269, mean_q: 3.918062
 61812/100000: episode: 1180, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 31.750, mean reward: 2.442 [2.086, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.069, 10.420], loss: 0.075245, mae: 0.281616, mean_q: 3.928287
 61838/100000: episode: 1181, duration: 0.176s, episode steps: 26, steps per second: 148, episode reward: 54.125, mean reward: 2.082 [1.442, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.373, 10.127], loss: 0.082410, mae: 0.290500, mean_q: 3.927779
 61863/100000: episode: 1182, duration: 0.185s, episode steps: 25, steps per second: 135, episode reward: 102.200, mean reward: 4.088 [2.690, 6.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.083, 10.100], loss: 0.132416, mae: 0.316421, mean_q: 3.894775
 61876/100000: episode: 1183, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 34.911, mean reward: 2.685 [2.254, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.464], loss: 0.153730, mae: 0.327740, mean_q: 3.991672
 61926/100000: episode: 1184, duration: 0.303s, episode steps: 50, steps per second: 165, episode reward: 119.059, mean reward: 2.381 [1.443, 7.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.992, 10.100], loss: 0.136763, mae: 0.319509, mean_q: 3.967792
 61976/100000: episode: 1185, duration: 0.315s, episode steps: 50, steps per second: 159, episode reward: 167.705, mean reward: 3.354 [2.100, 6.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.374, 10.423], loss: 0.119116, mae: 0.314110, mean_q: 3.971986
 62002/100000: episode: 1186, duration: 0.163s, episode steps: 26, steps per second: 160, episode reward: 58.931, mean reward: 2.267 [1.860, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.356, 10.348], loss: 0.119904, mae: 0.329473, mean_q: 3.988364
 62039/100000: episode: 1187, duration: 0.245s, episode steps: 37, steps per second: 151, episode reward: 136.344, mean reward: 3.685 [2.402, 9.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.992, 10.641], loss: 0.114314, mae: 0.326538, mean_q: 4.028623
 62058/100000: episode: 1188, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 48.440, mean reward: 2.549 [1.958, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.295], loss: 0.141285, mae: 0.337391, mean_q: 3.996906
 62084/100000: episode: 1189, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 71.346, mean reward: 2.744 [2.135, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.404], loss: 0.140042, mae: 0.324140, mean_q: 4.026707
 62121/100000: episode: 1190, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 95.276, mean reward: 2.575 [1.574, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.765, 10.239], loss: 0.120469, mae: 0.323013, mean_q: 4.030891
 62171/100000: episode: 1191, duration: 0.312s, episode steps: 50, steps per second: 160, episode reward: 171.238, mean reward: 3.425 [2.041, 9.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.350, 10.490], loss: 0.189496, mae: 0.368640, mean_q: 4.111331
 62196/100000: episode: 1192, duration: 0.163s, episode steps: 25, steps per second: 154, episode reward: 79.082, mean reward: 3.163 [2.563, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.492, 10.100], loss: 0.166629, mae: 0.371069, mean_q: 4.059021
 62240/100000: episode: 1193, duration: 0.263s, episode steps: 44, steps per second: 167, episode reward: 124.042, mean reward: 2.819 [1.949, 5.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.855, 10.410], loss: 0.137633, mae: 0.348721, mean_q: 4.121056
 62265/100000: episode: 1194, duration: 0.173s, episode steps: 25, steps per second: 145, episode reward: 94.828, mean reward: 3.793 [2.511, 6.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.217, 10.100], loss: 0.128727, mae: 0.339942, mean_q: 4.160754
 62309/100000: episode: 1195, duration: 0.299s, episode steps: 44, steps per second: 147, episode reward: 131.327, mean reward: 2.985 [1.979, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.669, 10.297], loss: 0.162706, mae: 0.351027, mean_q: 4.149326
 62335/100000: episode: 1196, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 60.048, mean reward: 2.310 [1.578, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.399, 10.128], loss: 0.117213, mae: 0.324405, mean_q: 4.116313
 62339/100000: episode: 1197, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 9.592, mean reward: 2.398 [2.364, 2.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.341], loss: 0.109440, mae: 0.322387, mean_q: 4.012784
 62393/100000: episode: 1198, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 123.648, mean reward: 2.290 [1.606, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.333, 10.217], loss: 0.126708, mae: 0.339918, mean_q: 4.126924
 62397/100000: episode: 1199, duration: 0.032s, episode steps: 4, steps per second: 125, episode reward: 9.954, mean reward: 2.488 [2.276, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.413], loss: 0.182405, mae: 0.387988, mean_q: 4.061049
 62447/100000: episode: 1200, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 108.470, mean reward: 2.169 [1.452, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.325, 10.129], loss: 0.134472, mae: 0.332891, mean_q: 4.136652
 62460/100000: episode: 1201, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 35.732, mean reward: 2.749 [2.292, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.363], loss: 0.115925, mae: 0.356136, mean_q: 4.151723
 62497/100000: episode: 1202, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 84.707, mean reward: 2.289 [1.458, 4.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.726, 10.295], loss: 0.133163, mae: 0.340934, mean_q: 4.129704
 62523/100000: episode: 1203, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 71.111, mean reward: 2.735 [1.768, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.606, 10.247], loss: 0.181486, mae: 0.371220, mean_q: 4.181111
 62549/100000: episode: 1204, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 59.966, mean reward: 2.306 [1.762, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.610, 10.222], loss: 0.175913, mae: 0.361965, mean_q: 4.248236
 62599/100000: episode: 1205, duration: 0.285s, episode steps: 50, steps per second: 175, episode reward: 91.077, mean reward: 1.822 [1.499, 2.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.573, 10.100], loss: 0.177310, mae: 0.378050, mean_q: 4.154833
 62603/100000: episode: 1206, duration: 0.031s, episode steps: 4, steps per second: 127, episode reward: 11.694, mean reward: 2.924 [2.664, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.447], loss: 0.207026, mae: 0.459760, mean_q: 4.482824
 62629/100000: episode: 1207, duration: 0.174s, episode steps: 26, steps per second: 149, episode reward: 71.634, mean reward: 2.755 [1.988, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.414], loss: 0.185964, mae: 0.375533, mean_q: 4.195837
 62633/100000: episode: 1208, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 9.800, mean reward: 2.450 [2.355, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.408], loss: 0.113735, mae: 0.319069, mean_q: 3.987781
 62652/100000: episode: 1209, duration: 0.127s, episode steps: 19, steps per second: 149, episode reward: 46.560, mean reward: 2.451 [1.852, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.347, 10.319], loss: 0.173108, mae: 0.344248, mean_q: 4.179658
 62678/100000: episode: 1210, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 72.818, mean reward: 2.801 [1.850, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.303], loss: 0.142416, mae: 0.358237, mean_q: 4.235908
 62704/100000: episode: 1211, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 70.664, mean reward: 2.718 [2.146, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.382], loss: 0.129026, mae: 0.347360, mean_q: 4.136258
 62723/100000: episode: 1212, duration: 0.131s, episode steps: 19, steps per second: 145, episode reward: 44.045, mean reward: 2.318 [2.047, 2.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.366], loss: 0.162792, mae: 0.359390, mean_q: 4.178672
 62777/100000: episode: 1213, duration: 0.314s, episode steps: 54, steps per second: 172, episode reward: 106.851, mean reward: 1.979 [1.487, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.327, 10.208], loss: 0.178001, mae: 0.371233, mean_q: 4.212740
 62803/100000: episode: 1214, duration: 0.166s, episode steps: 26, steps per second: 157, episode reward: 61.481, mean reward: 2.365 [1.944, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.216, 10.343], loss: 0.180368, mae: 0.385010, mean_q: 4.187515
 62816/100000: episode: 1215, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 30.237, mean reward: 2.326 [2.003, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.913, 10.339], loss: 0.142290, mae: 0.385393, mean_q: 4.265512
 62853/100000: episode: 1216, duration: 0.224s, episode steps: 37, steps per second: 165, episode reward: 112.479, mean reward: 3.040 [1.530, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.617, 10.282], loss: 0.123407, mae: 0.343630, mean_q: 4.182990
 62905/100000: episode: 1217, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 96.834, mean reward: 1.862 [1.456, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.372, 10.200], loss: 0.156795, mae: 0.384216, mean_q: 4.250848
 62955/100000: episode: 1218, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 330.431, mean reward: 6.609 [1.932, 99.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.301, 10.689], loss: 0.160087, mae: 0.359953, mean_q: 4.190344
 63009/100000: episode: 1219, duration: 0.314s, episode steps: 54, steps per second: 172, episode reward: 115.439, mean reward: 2.138 [1.540, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.475, 10.100], loss: 0.177953, mae: 0.386080, mean_q: 4.233132
 63046/100000: episode: 1220, duration: 0.238s, episode steps: 37, steps per second: 156, episode reward: 93.800, mean reward: 2.535 [2.143, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.018, 10.499], loss: 0.175562, mae: 0.380594, mean_q: 4.305280
 63072/100000: episode: 1221, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 61.896, mean reward: 2.381 [1.934, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.403, 10.316], loss: 0.200093, mae: 0.394437, mean_q: 4.342722
 63116/100000: episode: 1222, duration: 0.255s, episode steps: 44, steps per second: 173, episode reward: 106.081, mean reward: 2.411 [1.482, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.231, 10.155], loss: 0.431470, mae: 0.426815, mean_q: 4.295263
 63129/100000: episode: 1223, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 34.506, mean reward: 2.654 [2.109, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.371, 10.426], loss: 0.155345, mae: 0.376880, mean_q: 4.311594
 63142/100000: episode: 1224, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 36.211, mean reward: 2.785 [2.062, 4.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.457], loss: 0.093010, mae: 0.320914, mean_q: 4.221543
 63168/100000: episode: 1225, duration: 0.215s, episode steps: 26, steps per second: 121, episode reward: 58.144, mean reward: 2.236 [1.864, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.380, 10.266], loss: 0.143002, mae: 0.380250, mean_q: 4.277798
 63187/100000: episode: 1226, duration: 0.205s, episode steps: 19, steps per second: 93, episode reward: 51.615, mean reward: 2.717 [1.902, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.409, 10.537], loss: 0.199258, mae: 0.390151, mean_q: 4.262517
 63213/100000: episode: 1227, duration: 0.218s, episode steps: 26, steps per second: 119, episode reward: 81.232, mean reward: 3.124 [2.523, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.351, 10.492], loss: 0.176192, mae: 0.385056, mean_q: 4.355207
 63217/100000: episode: 1228, duration: 0.037s, episode steps: 4, steps per second: 107, episode reward: 12.594, mean reward: 3.149 [2.543, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.346], loss: 0.301013, mae: 0.439288, mean_q: 4.488697
 63261/100000: episode: 1229, duration: 0.349s, episode steps: 44, steps per second: 126, episode reward: 90.549, mean reward: 2.058 [1.491, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.423, 10.211], loss: 0.168599, mae: 0.383235, mean_q: 4.322111
 63305/100000: episode: 1230, duration: 0.273s, episode steps: 44, steps per second: 161, episode reward: 107.902, mean reward: 2.452 [1.908, 5.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.675, 10.444], loss: 0.194397, mae: 0.404046, mean_q: 4.342102
 63324/100000: episode: 1231, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 40.580, mean reward: 2.136 [1.872, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.041, 10.282], loss: 0.151513, mae: 0.372734, mean_q: 4.225106
 63350/100000: episode: 1232, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 66.756, mean reward: 2.568 [1.855, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.337], loss: 0.190035, mae: 0.387401, mean_q: 4.261901
 63387/100000: episode: 1233, duration: 0.217s, episode steps: 37, steps per second: 171, episode reward: 85.917, mean reward: 2.322 [1.872, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.426, 10.414], loss: 0.208312, mae: 0.416953, mean_q: 4.357422
 63437/100000: episode: 1234, duration: 0.328s, episode steps: 50, steps per second: 153, episode reward: 99.618, mean reward: 1.992 [1.501, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.496, 10.155], loss: 0.166183, mae: 0.384064, mean_q: 4.310860
 63487/100000: episode: 1235, duration: 0.354s, episode steps: 50, steps per second: 141, episode reward: 114.521, mean reward: 2.290 [1.624, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.551, 10.254], loss: 3.243881, mae: 0.584857, mean_q: 4.360556
 63541/100000: episode: 1236, duration: 0.327s, episode steps: 54, steps per second: 165, episode reward: 105.265, mean reward: 1.949 [1.470, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.460, 10.100], loss: 5.395765, mae: 0.618570, mean_q: 4.442909
 63585/100000: episode: 1237, duration: 0.260s, episode steps: 44, steps per second: 170, episode reward: 139.617, mean reward: 3.173 [2.240, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.017, 10.455], loss: 0.393466, mae: 0.463010, mean_q: 4.378957
 63622/100000: episode: 1238, duration: 0.229s, episode steps: 37, steps per second: 162, episode reward: 103.847, mean reward: 2.807 [1.522, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.429, 10.100], loss: 0.153837, mae: 0.401033, mean_q: 4.333223
 63659/100000: episode: 1239, duration: 0.221s, episode steps: 37, steps per second: 167, episode reward: 109.990, mean reward: 2.973 [1.871, 5.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.261, 10.331], loss: 4.028104, mae: 0.560126, mean_q: 4.396969
 63696/100000: episode: 1240, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 122.545, mean reward: 3.312 [2.286, 5.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.374, 10.474], loss: 0.500348, mae: 0.553010, mean_q: 4.269081
 63748/100000: episode: 1241, duration: 0.301s, episode steps: 52, steps per second: 172, episode reward: 161.824, mean reward: 3.112 [2.027, 7.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.346, 10.548], loss: 2.922902, mae: 0.546331, mean_q: 4.430757
 63800/100000: episode: 1242, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 112.179, mean reward: 2.157 [1.452, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.330, 10.300], loss: 3.100434, mae: 0.594137, mean_q: 4.518388
 63804/100000: episode: 1243, duration: 0.034s, episode steps: 4, steps per second: 118, episode reward: 9.974, mean reward: 2.494 [2.305, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.343], loss: 0.154094, mae: 0.382699, mean_q: 4.211912
 63817/100000: episode: 1244, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 37.116, mean reward: 2.855 [2.445, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.110, 10.493], loss: 0.193032, mae: 0.429016, mean_q: 4.350996
 63842/100000: episode: 1245, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 92.230, mean reward: 3.689 [2.361, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.393, 10.100], loss: 0.547319, mae: 0.462541, mean_q: 4.472757
 63886/100000: episode: 1246, duration: 0.262s, episode steps: 44, steps per second: 168, episode reward: 106.728, mean reward: 2.426 [1.495, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.688, 10.125], loss: 0.197136, mae: 0.409853, mean_q: 4.377157
 63923/100000: episode: 1247, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 87.407, mean reward: 2.362 [1.589, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.344, 10.312], loss: 0.229794, mae: 0.431439, mean_q: 4.481993
 63936/100000: episode: 1248, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 34.280, mean reward: 2.637 [2.164, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.256, 10.451], loss: 0.155105, mae: 0.385333, mean_q: 4.397723
 63955/100000: episode: 1249, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 47.582, mean reward: 2.504 [1.886, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.611, 10.346], loss: 0.191336, mae: 0.412227, mean_q: 4.388416
 63968/100000: episode: 1250, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 39.179, mean reward: 3.014 [2.217, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.316, 10.381], loss: 0.347041, mae: 0.469320, mean_q: 4.470801
 64012/100000: episode: 1251, duration: 0.279s, episode steps: 44, steps per second: 158, episode reward: 118.850, mean reward: 2.701 [1.505, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.228, 10.115], loss: 0.174811, mae: 0.410302, mean_q: 4.386745
 64049/100000: episode: 1252, duration: 0.218s, episode steps: 37, steps per second: 170, episode reward: 91.806, mean reward: 2.481 [2.052, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.136, 10.367], loss: 0.230603, mae: 0.434451, mean_q: 4.495518
 64068/100000: episode: 1253, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 52.961, mean reward: 2.787 [2.068, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.524, 10.325], loss: 0.251701, mae: 0.434283, mean_q: 4.427154
[Info] 2-TH LEVEL FOUND: 7.531846046447754, Considering 10/90 traces
 64072/100000: episode: 1254, duration: 4.643s, episode steps: 4, steps per second: 1, episode reward: 12.122, mean reward: 3.030 [2.694, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.496], loss: 0.180667, mae: 0.472663, mean_q: 4.541295
 64103/100000: episode: 1255, duration: 0.192s, episode steps: 31, steps per second: 161, episode reward: 90.561, mean reward: 2.921 [2.196, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.636, 10.332], loss: 0.174721, mae: 0.404302, mean_q: 4.497493
 64123/100000: episode: 1256, duration: 0.119s, episode steps: 20, steps per second: 167, episode reward: 64.132, mean reward: 3.207 [1.990, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.048, 10.367], loss: 0.221031, mae: 0.423406, mean_q: 4.490827
[Info] FALSIFICATION!
 64140/100000: episode: 1257, duration: 0.353s, episode steps: 17, steps per second: 48, episode reward: 1125.583, mean reward: 66.211 [4.615, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.375, 10.443], loss: 0.225325, mae: 0.443110, mean_q: 4.534478
 64171/100000: episode: 1258, duration: 0.193s, episode steps: 31, steps per second: 161, episode reward: 90.850, mean reward: 2.931 [1.980, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.388, 10.314], loss: 497.362274, mae: 3.176445, mean_q: 5.574154
 64195/100000: episode: 1259, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 76.838, mean reward: 3.202 [2.360, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.930, 10.366], loss: 1.045962, mae: 1.017169, mean_q: 4.738904
 64215/100000: episode: 1260, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 48.599, mean reward: 2.430 [1.757, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.717, 10.302], loss: 0.323928, mae: 0.576516, mean_q: 4.682739
 64245/100000: episode: 1261, duration: 0.182s, episode steps: 30, steps per second: 164, episode reward: 111.712, mean reward: 3.724 [2.583, 9.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.120, 10.689], loss: 0.318653, mae: 0.529407, mean_q: 4.689696
 64274/100000: episode: 1262, duration: 0.172s, episode steps: 29, steps per second: 169, episode reward: 112.001, mean reward: 3.862 [2.642, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.090, 10.428], loss: 0.198053, mae: 0.454754, mean_q: 4.683621
 64298/100000: episode: 1263, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 105.005, mean reward: 4.375 [2.427, 7.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.439, 10.432], loss: 6.282108, mae: 0.733773, mean_q: 4.837391
 64318/100000: episode: 1264, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 63.433, mean reward: 3.172 [2.114, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.440], loss: 0.207465, mae: 0.461110, mean_q: 4.744117
 64347/100000: episode: 1265, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 96.919, mean reward: 3.342 [2.187, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.422, 10.496], loss: 5.579129, mae: 0.696764, mean_q: 4.939941
 64380/100000: episode: 1266, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 93.490, mean reward: 2.833 [2.170, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.644, 10.358], loss: 470.543182, mae: 2.651681, mean_q: 5.940730
 64411/100000: episode: 1267, duration: 0.175s, episode steps: 31, steps per second: 178, episode reward: 118.932, mean reward: 3.837 [2.706, 5.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.122, 10.410], loss: 0.465975, mae: 0.762217, mean_q: 4.478102
 64440/100000: episode: 1268, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 125.489, mean reward: 4.327 [2.943, 7.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.465, 10.607], loss: 0.258741, mae: 0.498482, mean_q: 4.851223
 64445/100000: episode: 1269, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 23.696, mean reward: 4.739 [3.751, 5.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.620], loss: 0.536266, mae: 0.605732, mean_q: 4.921849
 64450/100000: episode: 1270, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 17.890, mean reward: 3.578 [3.065, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.489], loss: 0.409998, mae: 0.527999, mean_q: 5.085172
 64481/100000: episode: 1271, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 95.325, mean reward: 3.075 [2.313, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.374], loss: 0.268592, mae: 0.491071, mean_q: 4.871222
 64505/100000: episode: 1272, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 72.959, mean reward: 3.040 [2.192, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.414], loss: 0.210047, mae: 0.449987, mean_q: 4.780424
 64525/100000: episode: 1273, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 70.633, mean reward: 3.532 [2.380, 5.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.447, 10.429], loss: 765.678772, mae: 2.823500, mean_q: 5.851185
 64549/100000: episode: 1274, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 96.409, mean reward: 4.017 [3.014, 7.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.253, 10.451], loss: 1.484590, mae: 1.190212, mean_q: 5.226026
 64580/100000: episode: 1275, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 89.855, mean reward: 2.899 [2.334, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.731, 10.430], loss: 0.428237, mae: 0.576437, mean_q: 4.831492
 64609/100000: episode: 1276, duration: 0.171s, episode steps: 29, steps per second: 170, episode reward: 97.609, mean reward: 3.366 [2.471, 5.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.607, 10.476], loss: 528.477722, mae: 2.575796, mean_q: 6.220215
 64633/100000: episode: 1277, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 83.188, mean reward: 3.466 [2.269, 5.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.412], loss: 0.397678, mae: 0.657240, mean_q: 4.613187
 64662/100000: episode: 1278, duration: 0.188s, episode steps: 29, steps per second: 154, episode reward: 96.703, mean reward: 3.335 [2.523, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.293, 10.465], loss: 5.173253, mae: 0.655785, mean_q: 5.227712
 64693/100000: episode: 1279, duration: 0.205s, episode steps: 31, steps per second: 151, episode reward: 82.957, mean reward: 2.676 [1.957, 5.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.556, 10.303], loss: 0.250275, mae: 0.479951, mean_q: 4.992440
 64724/100000: episode: 1280, duration: 0.210s, episode steps: 31, steps per second: 148, episode reward: 119.040, mean reward: 3.840 [2.134, 5.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.773, 10.222], loss: 0.292529, mae: 0.505744, mean_q: 5.019736
 64755/100000: episode: 1281, duration: 0.204s, episode steps: 31, steps per second: 152, episode reward: 94.845, mean reward: 3.060 [1.915, 5.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.260], loss: 5.533780, mae: 0.705473, mean_q: 5.133422
 64779/100000: episode: 1282, duration: 0.155s, episode steps: 24, steps per second: 155, episode reward: 143.872, mean reward: 5.995 [3.827, 12.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.679, 10.636], loss: 6.180426, mae: 0.659497, mean_q: 5.154245
 64784/100000: episode: 1283, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 22.730, mean reward: 4.546 [4.325, 4.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.520], loss: 0.184830, mae: 0.453690, mean_q: 4.996709
 64815/100000: episode: 1284, duration: 0.193s, episode steps: 31, steps per second: 161, episode reward: 80.293, mean reward: 2.590 [1.727, 5.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.827, 10.287], loss: 0.327821, mae: 0.522783, mean_q: 5.101996
 64839/100000: episode: 1285, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 76.897, mean reward: 3.204 [2.083, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.349], loss: 0.327659, mae: 0.543404, mean_q: 5.153292
 64870/100000: episode: 1286, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 108.395, mean reward: 3.497 [2.105, 6.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.657, 10.375], loss: 0.278850, mae: 0.504150, mean_q: 4.987380
[Info] FALSIFICATION!
 64898/100000: episode: 1287, duration: 0.320s, episode steps: 28, steps per second: 88, episode reward: 1118.663, mean reward: 39.952 [2.371, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.232, 10.861], loss: 5.307008, mae: 0.611533, mean_q: 5.019295
 64929/100000: episode: 1288, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 104.710, mean reward: 3.378 [2.745, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.475, 10.494], loss: 492.792023, mae: 1.899652, mean_q: 5.653996
 64949/100000: episode: 1289, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 96.263, mean reward: 4.813 [2.892, 7.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.374, 10.610], loss: 767.138977, mae: 3.838591, mean_q: 7.327992
 64979/100000: episode: 1290, duration: 0.191s, episode steps: 30, steps per second: 157, episode reward: 120.275, mean reward: 4.009 [3.079, 5.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.552], loss: 1.213999, mae: 0.926235, mean_q: 5.296909
 64999/100000: episode: 1291, duration: 0.136s, episode steps: 20, steps per second: 148, episode reward: 63.884, mean reward: 3.194 [2.707, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.457], loss: 764.740906, mae: 3.268786, mean_q: 6.797268
 65030/100000: episode: 1292, duration: 0.187s, episode steps: 31, steps per second: 165, episode reward: 90.458, mean reward: 2.918 [1.764, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.275, 10.277], loss: 983.585205, mae: 3.555706, mean_q: 6.636730
 65054/100000: episode: 1293, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 99.453, mean reward: 4.144 [2.312, 11.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.237, 10.481], loss: 1271.032837, mae: 7.027961, mean_q: 9.871278
 65074/100000: episode: 1294, duration: 0.126s, episode steps: 20, steps per second: 158, episode reward: 54.510, mean reward: 2.725 [1.669, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.256], loss: 3.638079, mae: 1.867401, mean_q: 5.721125
 65103/100000: episode: 1295, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 94.640, mean reward: 3.263 [2.072, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.396], loss: 6.166645, mae: 1.239499, mean_q: 5.908525
 65136/100000: episode: 1296, duration: 0.191s, episode steps: 33, steps per second: 172, episode reward: 104.108, mean reward: 3.155 [2.040, 5.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.120, 10.478], loss: 466.132629, mae: 1.888045, mean_q: 5.959186
 65167/100000: episode: 1297, duration: 0.209s, episode steps: 31, steps per second: 148, episode reward: 163.462, mean reward: 5.273 [3.422, 8.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.297, 10.508], loss: 0.967095, mae: 1.008213, mean_q: 5.835953
 65198/100000: episode: 1298, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 75.099, mean reward: 2.423 [1.865, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.389, 10.385], loss: 0.435007, mae: 0.523380, mean_q: 5.454751
 65229/100000: episode: 1299, duration: 0.184s, episode steps: 31, steps per second: 168, episode reward: 95.799, mean reward: 3.090 [2.010, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.089, 10.338], loss: 499.458984, mae: 2.518178, mean_q: 6.690470
 65249/100000: episode: 1300, duration: 0.142s, episode steps: 20, steps per second: 141, episode reward: 48.761, mean reward: 2.438 [1.860, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.378], loss: 0.512302, mae: 0.689664, mean_q: 5.447412
 65280/100000: episode: 1301, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 130.967, mean reward: 4.225 [1.758, 10.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.352], loss: 0.488197, mae: 0.635296, mean_q: 5.551941
 65313/100000: episode: 1302, duration: 0.207s, episode steps: 33, steps per second: 159, episode reward: 112.265, mean reward: 3.402 [2.014, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.314], loss: 0.390546, mae: 0.561518, mean_q: 5.551802
 65337/100000: episode: 1303, duration: 0.186s, episode steps: 24, steps per second: 129, episode reward: 147.275, mean reward: 6.136 [2.714, 14.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.296, 10.449], loss: 0.352200, mae: 0.546371, mean_q: 5.441639
 65366/100000: episode: 1304, duration: 0.249s, episode steps: 29, steps per second: 117, episode reward: 97.602, mean reward: 3.366 [2.256, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.583], loss: 0.336114, mae: 0.539060, mean_q: 5.410132
 65386/100000: episode: 1305, duration: 0.175s, episode steps: 20, steps per second: 114, episode reward: 63.474, mean reward: 3.174 [2.387, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.063, 10.334], loss: 7.506732, mae: 0.695981, mean_q: 5.300824
 65419/100000: episode: 1306, duration: 0.208s, episode steps: 33, steps per second: 158, episode reward: 110.492, mean reward: 3.348 [2.121, 5.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.487, 10.371], loss: 0.390652, mae: 0.581756, mean_q: 5.566300
 65443/100000: episode: 1307, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 85.755, mean reward: 3.573 [2.032, 5.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.080, 10.358], loss: 0.374529, mae: 0.556557, mean_q: 5.473902
 65474/100000: episode: 1308, duration: 0.188s, episode steps: 31, steps per second: 165, episode reward: 139.172, mean reward: 4.489 [3.158, 6.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.441, 10.512], loss: 493.452972, mae: 2.501777, mean_q: 6.821307
 65504/100000: episode: 1309, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 95.518, mean reward: 3.184 [1.959, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.063, 10.321], loss: 510.961243, mae: 2.363892, mean_q: 6.552280
 65537/100000: episode: 1310, duration: 0.208s, episode steps: 33, steps per second: 159, episode reward: 111.148, mean reward: 3.368 [2.424, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.934, 10.486], loss: 921.871948, mae: 3.497056, mean_q: 6.720661
 65557/100000: episode: 1311, duration: 0.110s, episode steps: 20, steps per second: 183, episode reward: 57.592, mean reward: 2.880 [2.004, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.321], loss: 773.885498, mae: 4.688125, mean_q: 8.030222
 65587/100000: episode: 1312, duration: 0.180s, episode steps: 30, steps per second: 166, episode reward: 79.423, mean reward: 2.647 [2.267, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.277, 10.403], loss: 0.926037, mae: 0.880508, mean_q: 6.168993
 65616/100000: episode: 1313, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 81.902, mean reward: 2.824 [1.596, 6.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.251, 10.183], loss: 524.808350, mae: 1.984128, mean_q: 6.230206
 65640/100000: episode: 1314, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 80.786, mean reward: 3.366 [1.492, 8.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.193, 10.143], loss: 2.049001, mae: 1.351855, mean_q: 6.867550
 65671/100000: episode: 1315, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 115.803, mean reward: 3.736 [2.316, 6.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.543, 10.447], loss: 0.524260, mae: 0.684547, mean_q: 5.756455
 65702/100000: episode: 1316, duration: 0.207s, episode steps: 31, steps per second: 149, episode reward: 72.320, mean reward: 2.333 [1.441, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.038, 10.162], loss: 494.340637, mae: 2.349949, mean_q: 6.798635
 65731/100000: episode: 1317, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 79.026, mean reward: 2.725 [2.082, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.788, 10.393], loss: 5.925507, mae: 0.923372, mean_q: 5.933904
 65761/100000: episode: 1318, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 112.258, mean reward: 3.742 [2.531, 6.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.629], loss: 508.431580, mae: 2.529949, mean_q: 7.162766
 65785/100000: episode: 1319, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 97.973, mean reward: 4.082 [2.846, 7.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.085, 10.498], loss: 633.643433, mae: 2.014844, mean_q: 5.814884
 65814/100000: episode: 1320, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 69.001, mean reward: 2.379 [1.653, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.873, 10.238], loss: 2.070702, mae: 1.534393, mean_q: 7.222084
 65845/100000: episode: 1321, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 113.033, mean reward: 3.646 [1.805, 7.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.637, 10.275], loss: 0.531244, mae: 0.670068, mean_q: 5.747129
 65876/100000: episode: 1322, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 116.965, mean reward: 3.773 [2.647, 6.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.771, 10.497], loss: 0.444415, mae: 0.655426, mean_q: 6.029465
 65896/100000: episode: 1323, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 54.376, mean reward: 2.719 [2.170, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.396, 10.412], loss: 0.816438, mae: 0.692954, mean_q: 5.944489
 65927/100000: episode: 1324, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 148.831, mean reward: 4.801 [2.696, 8.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.146, 10.495], loss: 493.583252, mae: 2.108783, mean_q: 6.559501
 65956/100000: episode: 1325, duration: 0.199s, episode steps: 29, steps per second: 145, episode reward: 74.606, mean reward: 2.573 [1.990, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.437, 10.346], loss: 1.241235, mae: 1.101220, mean_q: 6.320175
 65987/100000: episode: 1326, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 87.625, mean reward: 2.827 [1.852, 5.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.007, 10.312], loss: 492.350464, mae: 2.449705, mean_q: 7.177749
 66016/100000: episode: 1327, duration: 0.171s, episode steps: 29, steps per second: 170, episode reward: 233.239, mean reward: 8.043 [2.012, 56.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.654, 10.663], loss: 532.220825, mae: 2.413260, mean_q: 6.635300
 66045/100000: episode: 1328, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 76.435, mean reward: 2.636 [2.033, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.174, 10.403], loss: 2.885577, mae: 1.154635, mean_q: 6.493941
 66065/100000: episode: 1329, duration: 0.136s, episode steps: 20, steps per second: 147, episode reward: 66.584, mean reward: 3.329 [2.888, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.438], loss: 771.767090, mae: 3.600656, mean_q: 7.775784
 66070/100000: episode: 1330, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 17.728, mean reward: 3.546 [2.796, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.451], loss: 1.780456, mae: 1.276951, mean_q: 7.433011
 66075/100000: episode: 1331, duration: 0.045s, episode steps: 5, steps per second: 112, episode reward: 21.836, mean reward: 4.367 [3.775, 5.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.619], loss: 0.982398, mae: 1.003327, mean_q: 6.294637
 66105/100000: episode: 1332, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 99.777, mean reward: 3.326 [2.578, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.251, 10.477], loss: 508.476562, mae: 2.449587, mean_q: 7.085369
 66125/100000: episode: 1333, duration: 0.138s, episode steps: 20, steps per second: 145, episode reward: 70.784, mean reward: 3.539 [2.313, 5.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.467], loss: 0.719190, mae: 0.809928, mean_q: 6.256682
 66156/100000: episode: 1334, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: 73.752, mean reward: 2.379 [1.839, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.314, 10.387], loss: 0.559769, mae: 0.691439, mean_q: 6.259935
 66187/100000: episode: 1335, duration: 0.192s, episode steps: 31, steps per second: 161, episode reward: 93.979, mean reward: 3.032 [1.844, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.062, 10.229], loss: 5.111583, mae: 0.748154, mean_q: 6.227232
 66211/100000: episode: 1336, duration: 0.163s, episode steps: 24, steps per second: 147, episode reward: 58.729, mean reward: 2.447 [1.608, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.694, 10.172], loss: 0.801503, mae: 0.747574, mean_q: 6.418677
 66241/100000: episode: 1337, duration: 0.191s, episode steps: 30, steps per second: 157, episode reward: 99.936, mean reward: 3.331 [2.331, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.994, 10.537], loss: 0.719176, mae: 0.635117, mean_q: 5.983587
 66271/100000: episode: 1338, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 148.908, mean reward: 4.964 [2.625, 10.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.611, 10.490], loss: 0.999046, mae: 0.708532, mean_q: 6.065605
 66304/100000: episode: 1339, duration: 0.196s, episode steps: 33, steps per second: 169, episode reward: 102.392, mean reward: 3.103 [1.873, 5.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.512, 10.296], loss: 4.784949, mae: 0.767834, mean_q: 6.152194
 66324/100000: episode: 1340, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 60.688, mean reward: 3.034 [2.234, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.522, 10.409], loss: 0.513710, mae: 0.635006, mean_q: 5.973527
 66348/100000: episode: 1341, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 77.254, mean reward: 3.219 [2.040, 5.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.350, 10.310], loss: 0.651915, mae: 0.631329, mean_q: 5.957808
 66378/100000: episode: 1342, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 136.430, mean reward: 4.548 [2.726, 7.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.384, 10.622], loss: 510.981445, mae: 2.637384, mean_q: 7.303679
 66409/100000: episode: 1343, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: 97.958, mean reward: 3.160 [2.302, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.983, 10.410], loss: 986.855286, mae: 4.747675, mean_q: 8.095766
[Info] Complete ISplit Iteration
[Info] Levels: [5.0453706, 7.531846, 17.506464]
[Info] Cond. Prob: [0.1, 0.1, 0.09]
[Info] Error Prob: 0.0009000000000000002

 66433/100000: episode: 1344, duration: 4.982s, episode steps: 24, steps per second: 5, episode reward: 83.469, mean reward: 3.478 [2.272, 6.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.444], loss: 630.393616, mae: 4.412060, mean_q: 9.144879
 66533/100000: episode: 1345, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 202.312, mean reward: 2.023 [1.467, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.452, 10.232], loss: 156.204971, mae: 1.505862, mean_q: 6.461092
 66633/100000: episode: 1346, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 205.596, mean reward: 2.056 [1.457, 8.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.369, 10.308], loss: 154.985397, mae: 1.321100, mean_q: 6.489319
 66733/100000: episode: 1347, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.495, mean reward: 1.855 [1.433, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.143, 10.297], loss: 2.359244, mae: 0.749787, mean_q: 6.105055
 66833/100000: episode: 1348, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 186.546, mean reward: 1.865 [1.488, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.249, 10.160], loss: 306.181000, mae: 1.760531, mean_q: 6.517725
 66933/100000: episode: 1349, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 187.133, mean reward: 1.871 [1.447, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.745, 10.148], loss: 613.510437, mae: 3.048338, mean_q: 7.235622
 67033/100000: episode: 1350, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 190.576, mean reward: 1.906 [1.433, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.032, 10.098], loss: 304.746613, mae: 1.578268, mean_q: 6.518862
 67133/100000: episode: 1351, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 203.384, mean reward: 2.034 [1.509, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.780, 10.098], loss: 155.415695, mae: 1.522925, mean_q: 6.733290
 67233/100000: episode: 1352, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 183.077, mean reward: 1.831 [1.466, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.286, 10.098], loss: 152.502655, mae: 1.255462, mean_q: 6.467032
 67333/100000: episode: 1353, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 184.923, mean reward: 1.849 [1.445, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.376, 10.193], loss: 154.144913, mae: 1.328677, mean_q: 6.310028
 67433/100000: episode: 1354, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 188.780, mean reward: 1.888 [1.517, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.236, 10.098], loss: 0.499690, mae: 0.636072, mean_q: 5.874813
 67533/100000: episode: 1355, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 192.547, mean reward: 1.925 [1.455, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.149], loss: 1.953663, mae: 0.635743, mean_q: 5.768566
 67633/100000: episode: 1356, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 177.953, mean reward: 1.780 [1.472, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.834, 10.098], loss: 154.528870, mae: 1.238310, mean_q: 6.005507
 67733/100000: episode: 1357, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 195.517, mean reward: 1.955 [1.434, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.944, 10.098], loss: 155.372986, mae: 1.315490, mean_q: 6.152396
 67833/100000: episode: 1358, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 202.508, mean reward: 2.025 [1.449, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.098], loss: 458.345154, mae: 2.797492, mean_q: 6.762687
 67933/100000: episode: 1359, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 195.342, mean reward: 1.953 [1.503, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.816, 10.098], loss: 0.987245, mae: 0.671829, mean_q: 5.590153
 68033/100000: episode: 1360, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 212.473, mean reward: 2.125 [1.477, 6.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.521, 10.244], loss: 0.900861, mae: 0.610273, mean_q: 5.522515
 68133/100000: episode: 1361, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 212.397, mean reward: 2.124 [1.449, 8.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.280, 10.098], loss: 305.800598, mae: 1.825746, mean_q: 6.097739
 68233/100000: episode: 1362, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 187.650, mean reward: 1.876 [1.501, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.282], loss: 304.541412, mae: 1.724779, mean_q: 6.158342
 68333/100000: episode: 1363, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 195.892, mean reward: 1.959 [1.487, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.462, 10.251], loss: 305.363556, mae: 1.848112, mean_q: 6.268154
 68433/100000: episode: 1364, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.743, mean reward: 1.937 [1.445, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.050, 10.313], loss: 303.335724, mae: 1.901729, mean_q: 6.467843
 68533/100000: episode: 1365, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 202.055, mean reward: 2.021 [1.486, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.170, 10.145], loss: 1.377457, mae: 0.715103, mean_q: 5.736638
 68633/100000: episode: 1366, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 185.183, mean reward: 1.852 [1.505, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.878, 10.177], loss: 152.703171, mae: 1.028780, mean_q: 5.754283
 68733/100000: episode: 1367, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 221.106, mean reward: 2.211 [1.447, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.463, 10.585], loss: 151.969284, mae: 1.782779, mean_q: 6.133774
 68833/100000: episode: 1368, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 194.269, mean reward: 1.943 [1.458, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.126, 10.248], loss: 153.328415, mae: 1.329312, mean_q: 5.785812
 68933/100000: episode: 1369, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 236.101, mean reward: 2.361 [1.465, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.575, 10.589], loss: 153.751724, mae: 1.330162, mean_q: 5.793058
 69033/100000: episode: 1370, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 186.654, mean reward: 1.867 [1.467, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.098], loss: 153.285034, mae: 1.115201, mean_q: 5.731034
 69133/100000: episode: 1371, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 204.258, mean reward: 2.043 [1.462, 4.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.638, 10.098], loss: 153.028076, mae: 1.299639, mean_q: 5.718172
 69233/100000: episode: 1372, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 236.424, mean reward: 2.364 [1.509, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.699, 10.471], loss: 0.492361, mae: 0.595454, mean_q: 5.289784
 69333/100000: episode: 1373, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 192.297, mean reward: 1.923 [1.453, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.230, 10.121], loss: 304.419830, mae: 1.593130, mean_q: 5.726463
 69433/100000: episode: 1374, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 189.158, mean reward: 1.892 [1.447, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.259, 10.155], loss: 152.511261, mae: 1.186916, mean_q: 5.481942
 69533/100000: episode: 1375, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 212.460, mean reward: 2.125 [1.532, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.400, 10.346], loss: 152.412994, mae: 1.165702, mean_q: 5.442123
 69633/100000: episode: 1376, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 183.091, mean reward: 1.831 [1.448, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.098], loss: 0.931638, mae: 0.606855, mean_q: 4.973051
 69733/100000: episode: 1377, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 181.552, mean reward: 1.816 [1.488, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.016, 10.232], loss: 0.378060, mae: 0.513869, mean_q: 4.914582
 69833/100000: episode: 1378, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 194.151, mean reward: 1.942 [1.460, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.245, 10.569], loss: 0.358680, mae: 0.493798, mean_q: 4.861746
 69933/100000: episode: 1379, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 208.101, mean reward: 2.081 [1.487, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.893, 10.252], loss: 0.660105, mae: 0.487367, mean_q: 4.753069
 70033/100000: episode: 1380, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 189.498, mean reward: 1.895 [1.493, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.117, 10.098], loss: 0.631751, mae: 0.471246, mean_q: 4.686075
 70133/100000: episode: 1381, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 206.558, mean reward: 2.066 [1.505, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.306, 10.181], loss: 0.243695, mae: 0.435152, mean_q: 4.618972
 70233/100000: episode: 1382, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 180.852, mean reward: 1.809 [1.435, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.243, 10.098], loss: 0.218512, mae: 0.412982, mean_q: 4.539965
 70333/100000: episode: 1383, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 221.134, mean reward: 2.211 [1.490, 10.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.115, 10.098], loss: 0.215148, mae: 0.407883, mean_q: 4.453012
 70433/100000: episode: 1384, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 198.484, mean reward: 1.985 [1.450, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.385, 10.098], loss: 0.569767, mae: 0.409602, mean_q: 4.426248
 70533/100000: episode: 1385, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 231.652, mean reward: 2.317 [1.435, 6.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.529, 10.478], loss: 0.569318, mae: 0.413872, mean_q: 4.379465
 70633/100000: episode: 1386, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 191.998, mean reward: 1.920 [1.514, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.357, 10.098], loss: 0.178234, mae: 0.377309, mean_q: 4.354482
 70733/100000: episode: 1387, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 175.695, mean reward: 1.757 [1.446, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.050, 10.226], loss: 1.263074, mae: 0.442939, mean_q: 4.377935
 70833/100000: episode: 1388, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 191.391, mean reward: 1.914 [1.465, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.645, 10.098], loss: 0.520585, mae: 0.394323, mean_q: 4.297873
 70933/100000: episode: 1389, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 197.781, mean reward: 1.978 [1.477, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.154, 10.098], loss: 0.518552, mae: 0.378259, mean_q: 4.214958
 71033/100000: episode: 1390, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 191.186, mean reward: 1.912 [1.446, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.640, 10.098], loss: 0.162096, mae: 0.364715, mean_q: 4.117177
 71133/100000: episode: 1391, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 193.297, mean reward: 1.933 [1.436, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.619, 10.113], loss: 0.137202, mae: 0.337585, mean_q: 4.067962
 71233/100000: episode: 1392, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 190.329, mean reward: 1.903 [1.444, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.655, 10.333], loss: 0.126829, mae: 0.328624, mean_q: 4.019092
 71333/100000: episode: 1393, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 198.697, mean reward: 1.987 [1.467, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.130], loss: 0.110058, mae: 0.320336, mean_q: 3.959434
 71433/100000: episode: 1394, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 196.135, mean reward: 1.961 [1.448, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.174], loss: 0.095400, mae: 0.303785, mean_q: 3.925152
 71533/100000: episode: 1395, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 205.374, mean reward: 2.054 [1.457, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.604, 10.183], loss: 0.111890, mae: 0.309192, mean_q: 3.897393
 71633/100000: episode: 1396, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 198.103, mean reward: 1.981 [1.448, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.762, 10.098], loss: 0.115516, mae: 0.304133, mean_q: 3.898350
 71733/100000: episode: 1397, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 192.836, mean reward: 1.928 [1.454, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.584, 10.364], loss: 0.091143, mae: 0.307809, mean_q: 3.905450
 71833/100000: episode: 1398, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 189.067, mean reward: 1.891 [1.455, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.114], loss: 0.104976, mae: 0.301713, mean_q: 3.902134
 71933/100000: episode: 1399, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 193.096, mean reward: 1.931 [1.470, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.186, 10.155], loss: 0.087112, mae: 0.294965, mean_q: 3.896695
 72033/100000: episode: 1400, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 188.104, mean reward: 1.881 [1.449, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.115, 10.160], loss: 0.105578, mae: 0.316505, mean_q: 3.913200
 72133/100000: episode: 1401, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 193.653, mean reward: 1.937 [1.501, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.652, 10.098], loss: 0.096770, mae: 0.299678, mean_q: 3.905346
 72233/100000: episode: 1402, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 215.980, mean reward: 2.160 [1.491, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.104, 10.150], loss: 0.107739, mae: 0.300470, mean_q: 3.900797
 72333/100000: episode: 1403, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 220.710, mean reward: 2.207 [1.468, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.927, 10.561], loss: 0.101936, mae: 0.299972, mean_q: 3.916187
 72433/100000: episode: 1404, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 192.238, mean reward: 1.922 [1.454, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.129, 10.277], loss: 0.109816, mae: 0.317124, mean_q: 3.939375
 72533/100000: episode: 1405, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 186.902, mean reward: 1.869 [1.446, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.785, 10.366], loss: 0.116475, mae: 0.315191, mean_q: 3.971187
 72633/100000: episode: 1406, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 199.594, mean reward: 1.996 [1.453, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.814, 10.229], loss: 0.082904, mae: 0.287476, mean_q: 3.911128
 72733/100000: episode: 1407, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 191.774, mean reward: 1.918 [1.497, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.546, 10.098], loss: 0.098878, mae: 0.300767, mean_q: 3.949279
 72833/100000: episode: 1408, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 195.210, mean reward: 1.952 [1.487, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.945, 10.098], loss: 0.121776, mae: 0.304282, mean_q: 3.930168
 72933/100000: episode: 1409, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 295.781, mean reward: 2.958 [1.475, 69.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.521, 10.098], loss: 0.145312, mae: 0.326841, mean_q: 3.965222
 73033/100000: episode: 1410, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 184.186, mean reward: 1.842 [1.463, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.258, 10.103], loss: 0.812097, mae: 0.339192, mean_q: 3.948407
 73133/100000: episode: 1411, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 198.910, mean reward: 1.989 [1.485, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.185, 10.258], loss: 0.129137, mae: 0.311747, mean_q: 3.952568
 73233/100000: episode: 1412, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 203.765, mean reward: 2.038 [1.466, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.797, 10.359], loss: 0.101525, mae: 0.301186, mean_q: 3.949138
 73333/100000: episode: 1413, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 260.498, mean reward: 2.605 [1.446, 10.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.740, 10.098], loss: 0.800343, mae: 0.341622, mean_q: 3.963859
 73433/100000: episode: 1414, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 203.974, mean reward: 2.040 [1.496, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.777, 10.213], loss: 0.840590, mae: 0.363424, mean_q: 4.016544
 73533/100000: episode: 1415, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.491, mean reward: 1.935 [1.457, 4.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.833979, mae: 0.364473, mean_q: 4.003457
 73633/100000: episode: 1416, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 197.992, mean reward: 1.980 [1.459, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.414, 10.211], loss: 0.840478, mae: 0.363757, mean_q: 4.017617
 73733/100000: episode: 1417, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 202.589, mean reward: 2.026 [1.473, 4.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.142, 10.186], loss: 0.829339, mae: 0.345912, mean_q: 4.000221
 73833/100000: episode: 1418, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 178.290, mean reward: 1.783 [1.460, 2.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.495, 10.166], loss: 0.118758, mae: 0.312686, mean_q: 3.970888
 73933/100000: episode: 1419, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 181.199, mean reward: 1.812 [1.456, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.689, 10.098], loss: 0.112583, mae: 0.317975, mean_q: 3.962658
 74033/100000: episode: 1420, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 197.881, mean reward: 1.979 [1.481, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.098], loss: 0.102267, mae: 0.298821, mean_q: 3.940506
 74133/100000: episode: 1421, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 194.580, mean reward: 1.946 [1.456, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.737, 10.311], loss: 0.114105, mae: 0.299449, mean_q: 3.925453
 74233/100000: episode: 1422, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.956, mean reward: 1.900 [1.450, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.183], loss: 0.120008, mae: 0.310242, mean_q: 3.904541
 74333/100000: episode: 1423, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 181.955, mean reward: 1.820 [1.474, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.894, 10.098], loss: 0.836584, mae: 0.358330, mean_q: 3.928099
 74433/100000: episode: 1424, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 187.906, mean reward: 1.879 [1.463, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.098], loss: 0.828660, mae: 0.344257, mean_q: 3.942751
 74533/100000: episode: 1425, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 187.905, mean reward: 1.879 [1.506, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.519, 10.244], loss: 1.527517, mae: 0.408615, mean_q: 3.981446
 74633/100000: episode: 1426, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 187.203, mean reward: 1.872 [1.437, 4.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.098], loss: 0.828464, mae: 0.368743, mean_q: 3.976748
 74733/100000: episode: 1427, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 195.169, mean reward: 1.952 [1.472, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.583, 10.098], loss: 0.123784, mae: 0.311904, mean_q: 3.933531
 74833/100000: episode: 1428, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 215.019, mean reward: 2.150 [1.496, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.838, 10.206], loss: 0.808640, mae: 0.348204, mean_q: 3.952899
 74933/100000: episode: 1429, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 191.824, mean reward: 1.918 [1.434, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.098], loss: 0.803224, mae: 0.337140, mean_q: 3.922381
 75033/100000: episode: 1430, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 203.460, mean reward: 2.035 [1.466, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.353, 10.127], loss: 0.821350, mae: 0.355431, mean_q: 3.942148
 75133/100000: episode: 1431, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 189.245, mean reward: 1.892 [1.433, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.557, 10.122], loss: 0.107565, mae: 0.306406, mean_q: 3.919961
 75233/100000: episode: 1432, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 182.781, mean reward: 1.828 [1.456, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.805, 10.098], loss: 0.092643, mae: 0.304252, mean_q: 3.920504
 75333/100000: episode: 1433, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 188.005, mean reward: 1.880 [1.449, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.098], loss: 0.125363, mae: 0.318583, mean_q: 3.896226
 75433/100000: episode: 1434, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.447, mean reward: 1.874 [1.494, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.315, 10.130], loss: 0.799226, mae: 0.341073, mean_q: 3.954032
 75533/100000: episode: 1435, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 191.058, mean reward: 1.911 [1.497, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.048, 10.098], loss: 0.812733, mae: 0.354595, mean_q: 3.913753
 75633/100000: episode: 1436, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 185.412, mean reward: 1.854 [1.436, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.034, 10.257], loss: 2.222018, mae: 0.452452, mean_q: 3.969731
 75733/100000: episode: 1437, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 183.836, mean reward: 1.838 [1.446, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.689, 10.194], loss: 0.801187, mae: 0.343482, mean_q: 3.929038
 75833/100000: episode: 1438, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 228.635, mean reward: 2.286 [1.455, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.953, 10.521], loss: 0.808504, mae: 0.336976, mean_q: 3.918936
 75933/100000: episode: 1439, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 197.684, mean reward: 1.977 [1.510, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.801576, mae: 0.343498, mean_q: 3.926569
 76033/100000: episode: 1440, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 198.463, mean reward: 1.985 [1.437, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.098], loss: 0.113502, mae: 0.303783, mean_q: 3.906845
 76133/100000: episode: 1441, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 181.875, mean reward: 1.819 [1.435, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.157], loss: 0.814981, mae: 0.341272, mean_q: 3.937551
 76233/100000: episode: 1442, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 187.590, mean reward: 1.876 [1.489, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.503, 10.124], loss: 0.081049, mae: 0.285670, mean_q: 3.878300
 76333/100000: episode: 1443, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 184.036, mean reward: 1.840 [1.465, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.346, 10.200], loss: 0.083004, mae: 0.287426, mean_q: 3.876381
[Info] 1-TH LEVEL FOUND: 5.705366611480713, Considering 10/90 traces
 76433/100000: episode: 1444, duration: 4.827s, episode steps: 100, steps per second: 21, episode reward: 196.082, mean reward: 1.961 [1.469, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.177, 10.161], loss: 0.116228, mae: 0.307496, mean_q: 3.892348
 76448/100000: episode: 1445, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 38.041, mean reward: 2.536 [2.044, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.165, 10.306], loss: 4.767973, mae: 0.529145, mean_q: 4.102148
 76468/100000: episode: 1446, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 47.929, mean reward: 2.396 [2.001, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.322, 10.367], loss: 0.106037, mae: 0.345404, mean_q: 3.866261
 76485/100000: episode: 1447, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 56.650, mean reward: 3.332 [2.449, 4.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.179, 10.498], loss: 0.093629, mae: 0.302428, mean_q: 3.915418
 76498/100000: episode: 1448, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 31.193, mean reward: 2.399 [2.101, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.065, 10.100], loss: 0.119197, mae: 0.314344, mean_q: 3.937165
 76526/100000: episode: 1449, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 96.670, mean reward: 3.453 [2.457, 11.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.318, 10.381], loss: 0.166732, mae: 0.321583, mean_q: 3.928815
 76537/100000: episode: 1450, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 30.163, mean reward: 2.742 [2.423, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.416], loss: 0.079859, mae: 0.292406, mean_q: 3.952113
 76539/100000: episode: 1451, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 7.417, mean reward: 3.708 [3.311, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.552], loss: 0.050312, mae: 0.238955, mean_q: 3.919025
 76567/100000: episode: 1452, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 74.873, mean reward: 2.674 [1.957, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.228, 10.242], loss: 0.122608, mae: 0.301126, mean_q: 4.003008
 76595/100000: episode: 1453, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 76.887, mean reward: 2.746 [2.088, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.433], loss: 0.094442, mae: 0.291868, mean_q: 3.954287
 76610/100000: episode: 1454, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 38.268, mean reward: 2.551 [2.148, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.395], loss: 0.075865, mae: 0.275877, mean_q: 3.945286
 76623/100000: episode: 1455, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 29.014, mean reward: 2.232 [2.064, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.335], loss: 5.515957, mae: 0.566642, mean_q: 4.148148
 76643/100000: episode: 1456, duration: 0.135s, episode steps: 20, steps per second: 149, episode reward: 79.141, mean reward: 3.957 [2.651, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.067, 10.397], loss: 0.130763, mae: 0.342908, mean_q: 3.967301
 76663/100000: episode: 1457, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 61.368, mean reward: 3.068 [2.429, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.393], loss: 0.116302, mae: 0.311552, mean_q: 3.988400
 76683/100000: episode: 1458, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 124.466, mean reward: 6.223 [2.290, 15.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.082, 10.619], loss: 0.075314, mae: 0.286074, mean_q: 3.971747
 76696/100000: episode: 1459, duration: 0.092s, episode steps: 13, steps per second: 142, episode reward: 32.199, mean reward: 2.477 [2.025, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.437], loss: 0.154978, mae: 0.316079, mean_q: 3.903036
 76711/100000: episode: 1460, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 44.808, mean reward: 2.987 [2.552, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.455], loss: 0.205842, mae: 0.344983, mean_q: 3.969994
 76724/100000: episode: 1461, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 35.423, mean reward: 2.725 [2.228, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.404], loss: 0.185512, mae: 0.351873, mean_q: 4.002250
 76744/100000: episode: 1462, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 66.546, mean reward: 3.327 [2.454, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.414, 10.462], loss: 0.216389, mae: 0.332319, mean_q: 4.001139
 76764/100000: episode: 1463, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 53.085, mean reward: 2.654 [2.146, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.605, 10.416], loss: 0.107066, mae: 0.317169, mean_q: 3.990790
 76775/100000: episode: 1464, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 24.866, mean reward: 2.261 [1.868, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.756, 10.335], loss: 0.188890, mae: 0.329429, mean_q: 4.040438
 76790/100000: episode: 1465, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 45.576, mean reward: 3.038 [2.349, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.325, 10.472], loss: 0.085649, mae: 0.284461, mean_q: 3.993914
 76810/100000: episode: 1466, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 82.521, mean reward: 4.126 [2.441, 6.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.093, 10.636], loss: 0.147523, mae: 0.316506, mean_q: 3.986322
 76838/100000: episode: 1467, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 75.549, mean reward: 2.698 [2.098, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.033, 10.416], loss: 0.133583, mae: 0.335634, mean_q: 4.083368
 76851/100000: episode: 1468, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 26.100, mean reward: 2.008 [1.732, 2.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.226], loss: 0.323910, mae: 0.402241, mean_q: 4.222602
 76853/100000: episode: 1469, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 7.841, mean reward: 3.921 [3.884, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.535], loss: 0.189065, mae: 0.390934, mean_q: 4.008496
 76864/100000: episode: 1470, duration: 0.077s, episode steps: 11, steps per second: 144, episode reward: 48.752, mean reward: 4.432 [2.724, 6.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.108, 10.618], loss: 0.155066, mae: 0.414023, mean_q: 4.038244
 76880/100000: episode: 1471, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 104.396, mean reward: 6.525 [3.514, 15.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.381, 10.100], loss: 0.106813, mae: 0.320121, mean_q: 3.967345
 76893/100000: episode: 1472, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 36.740, mean reward: 2.826 [1.967, 5.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.372, 10.100], loss: 0.133736, mae: 0.311950, mean_q: 4.034081
 76904/100000: episode: 1473, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 37.957, mean reward: 3.451 [2.292, 6.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-1.116, 10.491], loss: 0.120994, mae: 0.342793, mean_q: 4.118263
 76915/100000: episode: 1474, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 27.980, mean reward: 2.544 [2.186, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.087, 10.415], loss: 0.306036, mae: 0.450706, mean_q: 4.114823
 76943/100000: episode: 1475, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 95.884, mean reward: 3.424 [1.973, 11.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.315, 10.338], loss: 0.344018, mae: 0.411805, mean_q: 4.124786
 76956/100000: episode: 1476, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 31.452, mean reward: 2.419 [2.127, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.370, 10.100], loss: 0.225534, mae: 0.382577, mean_q: 4.143465
 76971/100000: episode: 1477, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 37.161, mean reward: 2.477 [1.768, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.305], loss: 0.202526, mae: 0.358233, mean_q: 4.114851
 76973/100000: episode: 1478, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 9.153, mean reward: 4.576 [4.513, 4.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.574], loss: 0.115616, mae: 0.361273, mean_q: 4.102720
 76986/100000: episode: 1479, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 32.052, mean reward: 2.466 [2.016, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.400], loss: 5.527970, mae: 0.656574, mean_q: 4.364744
 76999/100000: episode: 1480, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 32.894, mean reward: 2.530 [1.952, 5.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.368, 10.100], loss: 0.245712, mae: 0.441636, mean_q: 3.949419
 77012/100000: episode: 1481, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 31.214, mean reward: 2.401 [1.996, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.412], loss: 0.239498, mae: 0.391896, mean_q: 4.173036
 77014/100000: episode: 1482, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 7.414, mean reward: 3.707 [3.601, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.521], loss: 0.071028, mae: 0.294099, mean_q: 4.074523
 77027/100000: episode: 1483, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 32.720, mean reward: 2.517 [2.145, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.403], loss: 0.145929, mae: 0.340083, mean_q: 4.137414
 77055/100000: episode: 1484, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 106.907, mean reward: 3.818 [2.799, 5.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.426, 10.591], loss: 0.314424, mae: 0.400893, mean_q: 4.165961
 77070/100000: episode: 1485, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 39.498, mean reward: 2.633 [2.145, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.403], loss: 0.193228, mae: 0.356871, mean_q: 4.123935
 77083/100000: episode: 1486, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 30.642, mean reward: 2.357 [1.921, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.103, 10.279], loss: 0.136544, mae: 0.340085, mean_q: 4.150817
 77111/100000: episode: 1487, duration: 0.165s, episode steps: 28, steps per second: 169, episode reward: 66.950, mean reward: 2.391 [1.917, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.433, 10.379], loss: 0.324502, mae: 0.414913, mean_q: 4.161914
 77131/100000: episode: 1488, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 63.725, mean reward: 3.186 [2.189, 5.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.529], loss: 0.181449, mae: 0.390898, mean_q: 4.178212
 77144/100000: episode: 1489, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 34.517, mean reward: 2.655 [2.310, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.207, 10.100], loss: 0.259422, mae: 0.397237, mean_q: 4.232124
 77159/100000: episode: 1490, duration: 0.095s, episode steps: 15, steps per second: 159, episode reward: 34.510, mean reward: 2.301 [1.948, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.273], loss: 0.170272, mae: 0.356290, mean_q: 4.162147
 77179/100000: episode: 1491, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 77.660, mean reward: 3.883 [2.330, 8.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.149, 10.502], loss: 0.193220, mae: 0.356211, mean_q: 4.154084
 77190/100000: episode: 1492, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 29.116, mean reward: 2.647 [2.247, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.988, 10.406], loss: 0.143559, mae: 0.331159, mean_q: 4.126928
 77218/100000: episode: 1493, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 69.062, mean reward: 2.467 [1.672, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.487, 10.396], loss: 0.262859, mae: 0.412124, mean_q: 4.201117
 77231/100000: episode: 1494, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 44.303, mean reward: 3.408 [1.941, 9.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.490, 10.100], loss: 0.465935, mae: 0.429932, mean_q: 4.177860
 77251/100000: episode: 1495, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 71.034, mean reward: 3.552 [2.296, 6.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.615], loss: 0.227609, mae: 0.403384, mean_q: 4.192931
 77271/100000: episode: 1496, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 51.397, mean reward: 2.570 [2.228, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.384], loss: 0.133160, mae: 0.346403, mean_q: 4.193142
 77273/100000: episode: 1497, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 8.655, mean reward: 4.327 [3.548, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.595], loss: 0.088521, mae: 0.343803, mean_q: 4.352964
 77301/100000: episode: 1498, duration: 0.181s, episode steps: 28, steps per second: 155, episode reward: 67.938, mean reward: 2.426 [1.808, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.284], loss: 2.910139, mae: 0.522690, mean_q: 4.279582
 77321/100000: episode: 1499, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 50.833, mean reward: 2.542 [1.799, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.053, 10.266], loss: 0.164663, mae: 0.350565, mean_q: 4.271387
 77332/100000: episode: 1500, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 27.745, mean reward: 2.522 [2.099, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.411], loss: 0.143716, mae: 0.354673, mean_q: 4.146908
 77345/100000: episode: 1501, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 35.872, mean reward: 2.759 [1.894, 4.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.440, 10.100], loss: 5.620791, mae: 0.678241, mean_q: 4.515358
 77360/100000: episode: 1502, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 59.499, mean reward: 3.967 [2.799, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.486], loss: 0.237652, mae: 0.410139, mean_q: 4.193246
 77373/100000: episode: 1503, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 29.163, mean reward: 2.243 [1.948, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.182, 10.100], loss: 0.215850, mae: 0.390083, mean_q: 4.319810
 77389/100000: episode: 1504, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 58.046, mean reward: 3.628 [2.811, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.438, 10.100], loss: 0.207039, mae: 0.371303, mean_q: 4.178322
 77417/100000: episode: 1505, duration: 0.164s, episode steps: 28, steps per second: 170, episode reward: 72.200, mean reward: 2.579 [1.738, 6.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.150, 10.387], loss: 0.184557, mae: 0.374376, mean_q: 4.271795
 77430/100000: episode: 1506, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 52.404, mean reward: 4.031 [2.237, 19.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.349, 10.100], loss: 0.131514, mae: 0.329658, mean_q: 4.242278
 77450/100000: episode: 1507, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 57.322, mean reward: 2.866 [2.275, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.714, 10.380], loss: 0.165259, mae: 0.333760, mean_q: 4.224506
 77465/100000: episode: 1508, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 44.662, mean reward: 2.977 [2.251, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.455], loss: 0.114156, mae: 0.324051, mean_q: 4.184021
 77493/100000: episode: 1509, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 75.552, mean reward: 2.698 [2.169, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.252, 10.291], loss: 0.286594, mae: 0.434427, mean_q: 4.389019
 77506/100000: episode: 1510, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 37.766, mean reward: 2.905 [2.098, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.402], loss: 0.130908, mae: 0.346803, mean_q: 4.141298
 77523/100000: episode: 1511, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 50.389, mean reward: 2.964 [2.177, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.262, 10.400], loss: 0.250912, mae: 0.393482, mean_q: 4.251960
 77543/100000: episode: 1512, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 66.347, mean reward: 3.317 [2.110, 5.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.612], loss: 0.171368, mae: 0.391676, mean_q: 4.301899
 77545/100000: episode: 1513, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 6.081, mean reward: 3.040 [2.941, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.479], loss: 0.208549, mae: 0.367224, mean_q: 4.182400
 77560/100000: episode: 1514, duration: 0.080s, episode steps: 15, steps per second: 186, episode reward: 51.803, mean reward: 3.454 [2.731, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.552], loss: 0.522436, mae: 0.439532, mean_q: 4.343666
 77580/100000: episode: 1515, duration: 0.119s, episode steps: 20, steps per second: 167, episode reward: 80.302, mean reward: 4.015 [2.212, 5.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.241, 10.621], loss: 0.682026, mae: 0.541976, mean_q: 4.358625
 77600/100000: episode: 1516, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 64.260, mean reward: 3.213 [2.241, 5.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.701, 10.410], loss: 0.263881, mae: 0.424543, mean_q: 4.328043
 77620/100000: episode: 1517, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 65.208, mean reward: 3.260 [2.363, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.410, 10.372], loss: 0.315517, mae: 0.434328, mean_q: 4.346190
 77631/100000: episode: 1518, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 37.287, mean reward: 3.390 [2.752, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.479], loss: 0.190527, mae: 0.405665, mean_q: 4.355049
 77646/100000: episode: 1519, duration: 0.116s, episode steps: 15, steps per second: 129, episode reward: 48.275, mean reward: 3.218 [2.519, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.037, 10.402], loss: 0.127019, mae: 0.342296, mean_q: 4.207172
 77648/100000: episode: 1520, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 6.614, mean reward: 3.307 [3.187, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.470], loss: 0.142332, mae: 0.377343, mean_q: 4.285045
 77659/100000: episode: 1521, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 27.532, mean reward: 2.503 [2.155, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.550, 10.387], loss: 0.385496, mae: 0.404934, mean_q: 4.372567
 77672/100000: episode: 1522, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 32.386, mean reward: 2.491 [2.159, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.391, 10.405], loss: 0.304971, mae: 0.425480, mean_q: 4.334928
 77685/100000: episode: 1523, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 30.472, mean reward: 2.344 [2.041, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.731, 10.100], loss: 0.155039, mae: 0.369369, mean_q: 4.225547
 77696/100000: episode: 1524, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 29.477, mean reward: 2.680 [2.134, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.135, 10.438], loss: 0.153288, mae: 0.394049, mean_q: 4.268708
 77712/100000: episode: 1525, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 54.061, mean reward: 3.379 [2.416, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.206, 10.100], loss: 0.365618, mae: 0.478876, mean_q: 4.427890
 77725/100000: episode: 1526, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 41.385, mean reward: 3.183 [2.085, 6.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.277, 10.100], loss: 5.751425, mae: 0.784555, mean_q: 4.595881
 77736/100000: episode: 1527, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 25.468, mean reward: 2.315 [2.089, 2.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.308], loss: 0.299358, mae: 0.499599, mean_q: 4.115094
 77756/100000: episode: 1528, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 58.989, mean reward: 2.949 [2.321, 5.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.580, 10.456], loss: 0.317753, mae: 0.420293, mean_q: 4.326080
 77758/100000: episode: 1529, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 7.125, mean reward: 3.562 [3.215, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.542], loss: 0.336076, mae: 0.532695, mean_q: 4.546797
 77771/100000: episode: 1530, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 35.847, mean reward: 2.757 [2.145, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.195, 10.100], loss: 0.331713, mae: 0.436153, mean_q: 4.418615
 77782/100000: episode: 1531, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 42.170, mean reward: 3.834 [2.262, 14.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.619], loss: 0.238086, mae: 0.456046, mean_q: 4.438344
 77799/100000: episode: 1532, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 47.318, mean reward: 2.783 [2.320, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.185, 10.531], loss: 0.541941, mae: 0.463894, mean_q: 4.390220
 77816/100000: episode: 1533, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 46.370, mean reward: 2.728 [1.889, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.033, 10.456], loss: 0.326339, mae: 0.418052, mean_q: 4.392260
[Info] 2-TH LEVEL FOUND: 7.207141876220703, Considering 10/90 traces
 77818/100000: episode: 1534, duration: 4.531s, episode steps: 2, steps per second: 0, episode reward: 9.270, mean reward: 4.635 [4.028, 5.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.035, 10.604], loss: 0.283864, mae: 0.447144, mean_q: 4.476981
 77831/100000: episode: 1535, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 38.012, mean reward: 2.924 [2.602, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.436], loss: 0.586293, mae: 0.523701, mean_q: 4.480617
 77846/100000: episode: 1536, duration: 0.127s, episode steps: 15, steps per second: 118, episode reward: 47.824, mean reward: 3.188 [2.449, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.444], loss: 0.557212, mae: 0.492222, mean_q: 4.334501
 77858/100000: episode: 1537, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 58.084, mean reward: 4.840 [3.195, 9.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.457, 10.100], loss: 0.509839, mae: 0.560623, mean_q: 4.388916
 77882/100000: episode: 1538, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 95.940, mean reward: 3.998 [2.641, 15.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.137, 10.540], loss: 0.440970, mae: 0.477521, mean_q: 4.387247
 77890/100000: episode: 1539, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 37.738, mean reward: 4.717 [3.484, 6.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.604], loss: 0.144434, mae: 0.348667, mean_q: 4.259982
 77906/100000: episode: 1540, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 73.324, mean reward: 4.583 [3.551, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.479], loss: 0.244876, mae: 0.378592, mean_q: 4.413081
 77922/100000: episode: 1541, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 58.920, mean reward: 3.682 [2.731, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.534], loss: 0.205143, mae: 0.383606, mean_q: 4.446227
 77938/100000: episode: 1542, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 47.504, mean reward: 2.969 [2.287, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.775, 10.390], loss: 0.360812, mae: 0.425890, mean_q: 4.479932
 77954/100000: episode: 1543, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 53.239, mean reward: 3.327 [2.754, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.088, 10.524], loss: 0.169185, mae: 0.398846, mean_q: 4.386901
 77963/100000: episode: 1544, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 70.473, mean reward: 7.830 [3.556, 32.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.702, 10.548], loss: 0.210593, mae: 0.400844, mean_q: 4.351705
 77975/100000: episode: 1545, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 46.120, mean reward: 3.843 [3.055, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.800, 10.462], loss: 0.240707, mae: 0.416788, mean_q: 4.630845
 77987/100000: episode: 1546, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 43.194, mean reward: 3.600 [2.735, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.403], loss: 0.213196, mae: 0.431732, mean_q: 4.472764
 77999/100000: episode: 1547, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 59.008, mean reward: 4.917 [2.800, 14.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.333, 10.100], loss: 0.148221, mae: 0.386886, mean_q: 4.353333
 78011/100000: episode: 1548, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 49.536, mean reward: 4.128 [3.267, 5.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.525], loss: 0.273465, mae: 0.438623, mean_q: 4.620380
 78020/100000: episode: 1549, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 60.948, mean reward: 6.772 [4.507, 10.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.675], loss: 0.170222, mae: 0.377183, mean_q: 4.405184
 78028/100000: episode: 1550, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 33.194, mean reward: 4.149 [3.080, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.536], loss: 0.171880, mae: 0.418558, mean_q: 4.572276
 78044/100000: episode: 1551, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 60.201, mean reward: 3.763 [2.710, 8.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.618], loss: 0.329058, mae: 0.444440, mean_q: 4.546391
 78052/100000: episode: 1552, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 37.013, mean reward: 4.627 [3.392, 5.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.551], loss: 1.737810, mae: 0.606573, mean_q: 4.759198
 78064/100000: episode: 1553, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 58.794, mean reward: 4.900 [3.748, 5.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.468, 10.100], loss: 0.632119, mae: 0.652394, mean_q: 4.468084
 78077/100000: episode: 1554, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 47.847, mean reward: 3.681 [2.861, 7.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.425, 10.531], loss: 0.539255, mae: 0.577036, mean_q: 4.483368
 78090/100000: episode: 1555, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 59.040, mean reward: 4.542 [3.364, 6.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.595], loss: 0.337035, mae: 0.509105, mean_q: 4.693656
 78103/100000: episode: 1556, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 47.234, mean reward: 3.633 [2.898, 4.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.185, 10.498], loss: 0.268573, mae: 0.483884, mean_q: 4.496266
 78111/100000: episode: 1557, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 34.345, mean reward: 4.293 [3.207, 6.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.991, 10.402], loss: 0.576303, mae: 0.537344, mean_q: 4.624102
 78127/100000: episode: 1558, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 81.318, mean reward: 5.082 [2.812, 10.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.315, 10.506], loss: 0.240248, mae: 0.446931, mean_q: 4.450669
 78143/100000: episode: 1559, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 67.843, mean reward: 4.240 [2.353, 8.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.647], loss: 0.336115, mae: 0.483575, mean_q: 4.612090
 78159/100000: episode: 1560, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 55.406, mean reward: 3.463 [2.328, 5.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.345], loss: 0.581079, mae: 0.475452, mean_q: 4.547351
 78168/100000: episode: 1561, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 39.399, mean reward: 4.378 [3.332, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.457, 10.594], loss: 0.463436, mae: 0.590882, mean_q: 4.829471
 78192/100000: episode: 1562, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 103.765, mean reward: 4.324 [3.339, 6.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.614, 10.476], loss: 1.046231, mae: 0.537495, mean_q: 4.605275
 78208/100000: episode: 1563, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 57.347, mean reward: 3.584 [2.540, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.976, 10.454], loss: 0.402111, mae: 0.493304, mean_q: 4.712570
 78216/100000: episode: 1564, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 25.469, mean reward: 3.184 [2.542, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.442], loss: 0.543930, mae: 0.526898, mean_q: 4.680234
 78225/100000: episode: 1565, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 43.521, mean reward: 4.836 [3.809, 7.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.468], loss: 0.325186, mae: 0.500080, mean_q: 4.756213
 78240/100000: episode: 1566, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 51.794, mean reward: 3.453 [2.657, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.483], loss: 0.530390, mae: 0.520426, mean_q: 4.600408
[Info] FALSIFICATION!
 78251/100000: episode: 1567, duration: 0.224s, episode steps: 11, steps per second: 49, episode reward: 1261.188, mean reward: 114.653 [3.277, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.732, 10.014], loss: 0.184666, mae: 0.395467, mean_q: 4.464879
 78263/100000: episode: 1568, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 55.936, mean reward: 4.661 [3.208, 6.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.482, 10.626], loss: 0.699724, mae: 0.553172, mean_q: 4.816310
 78275/100000: episode: 1569, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 40.304, mean reward: 3.359 [2.915, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.502], loss: 0.375018, mae: 0.499779, mean_q: 4.701817
 78291/100000: episode: 1570, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 131.805, mean reward: 8.238 [3.093, 11.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.496, 10.673], loss: 0.387347, mae: 0.477763, mean_q: 4.673131
 78307/100000: episode: 1571, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 77.568, mean reward: 4.848 [2.582, 7.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.265, 10.592], loss: 0.538734, mae: 0.545309, mean_q: 4.955881
 78319/100000: episode: 1572, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 54.270, mean reward: 4.523 [3.763, 6.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.553], loss: 0.766432, mae: 0.643182, mean_q: 4.966788
 78343/100000: episode: 1573, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 121.222, mean reward: 5.051 [3.266, 20.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.604], loss: 642.822510, mae: 2.938832, mean_q: 6.040223
 78359/100000: episode: 1574, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 58.370, mean reward: 3.648 [2.549, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.924, 10.506], loss: 1.957013, mae: 1.194165, mean_q: 4.657063
 78383/100000: episode: 1575, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 67.570, mean reward: 2.815 [2.088, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.838, 10.371], loss: 0.612859, mae: 0.687129, mean_q: 4.783954
 78396/100000: episode: 1576, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 57.658, mean reward: 4.435 [3.001, 6.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.400], loss: 1.322194, mae: 0.620085, mean_q: 4.803814
 78411/100000: episode: 1577, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 80.964, mean reward: 5.398 [2.811, 10.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.419], loss: 1.400270, mae: 0.651794, mean_q: 4.708477
 78423/100000: episode: 1578, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 60.903, mean reward: 5.075 [3.734, 10.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.499, 10.100], loss: 0.452722, mae: 0.592374, mean_q: 5.000395
 78435/100000: episode: 1579, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 53.364, mean reward: 4.447 [3.423, 6.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.069, 10.562], loss: 0.301187, mae: 0.482345, mean_q: 4.661071
 78459/100000: episode: 1580, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 123.220, mean reward: 5.134 [3.443, 9.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.083, 10.595], loss: 0.466401, mae: 0.524047, mean_q: 4.837399
 78471/100000: episode: 1581, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 58.599, mean reward: 4.883 [3.436, 7.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.590], loss: 0.842683, mae: 0.863369, mean_q: 5.504543
 78479/100000: episode: 1582, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 52.910, mean reward: 6.614 [3.679, 10.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.593], loss: 0.713120, mae: 0.751936, mean_q: 5.135166
 78487/100000: episode: 1583, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 36.340, mean reward: 4.543 [3.395, 5.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.576], loss: 0.503685, mae: 0.591529, mean_q: 4.736378
 78499/100000: episode: 1584, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 47.887, mean reward: 3.991 [3.028, 5.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.117, 10.100], loss: 0.390967, mae: 0.567584, mean_q: 5.014175
 78515/100000: episode: 1585, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 61.874, mean reward: 3.867 [2.591, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.133, 10.614], loss: 0.332483, mae: 0.527338, mean_q: 4.824812
 78527/100000: episode: 1586, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 43.984, mean reward: 3.665 [2.783, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.460], loss: 0.349790, mae: 0.483289, mean_q: 4.825886
 78540/100000: episode: 1587, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 35.892, mean reward: 2.761 [2.154, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.358], loss: 0.652518, mae: 0.591442, mean_q: 4.923017
 78552/100000: episode: 1588, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 59.659, mean reward: 4.972 [3.357, 8.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.622], loss: 43.928558, mae: 1.308548, mean_q: 5.560958
 78560/100000: episode: 1589, duration: 0.067s, episode steps: 8, steps per second: 120, episode reward: 30.750, mean reward: 3.844 [3.039, 4.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.169, 10.483], loss: 0.593693, mae: 0.697978, mean_q: 4.926733
 78572/100000: episode: 1590, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 40.288, mean reward: 3.357 [2.336, 4.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.444], loss: 0.447935, mae: 0.590323, mean_q: 4.917910
 78580/100000: episode: 1591, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 32.924, mean reward: 4.115 [3.173, 6.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.446], loss: 0.475309, mae: 0.617557, mean_q: 5.081737
 78595/100000: episode: 1592, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 45.643, mean reward: 3.043 [2.409, 4.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.471], loss: 0.345033, mae: 0.548711, mean_q: 4.985110
 78607/100000: episode: 1593, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 42.697, mean reward: 3.558 [2.444, 6.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.370, 10.100], loss: 0.454058, mae: 0.559109, mean_q: 4.994308
 78616/100000: episode: 1594, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 36.349, mean reward: 4.039 [3.303, 5.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.822, 10.489], loss: 0.395391, mae: 0.558132, mean_q: 4.928240
 78632/100000: episode: 1595, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 84.703, mean reward: 5.294 [3.368, 7.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.159, 10.600], loss: 1.269431, mae: 0.617403, mean_q: 4.975461
 78656/100000: episode: 1596, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 83.012, mean reward: 3.459 [2.088, 5.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.220, 10.360], loss: 2.040777, mae: 0.700326, mean_q: 5.164069
 78665/100000: episode: 1597, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 34.556, mean reward: 3.840 [3.144, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.519], loss: 1.731595, mae: 0.680984, mean_q: 5.230964
 78677/100000: episode: 1598, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 54.685, mean reward: 4.557 [3.658, 6.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.128, 10.559], loss: 0.620162, mae: 0.608208, mean_q: 4.973289
 78685/100000: episode: 1599, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 31.780, mean reward: 3.972 [3.484, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.184, 10.508], loss: 0.470989, mae: 0.614535, mean_q: 5.278424
 78701/100000: episode: 1600, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 88.981, mean reward: 5.561 [3.064, 10.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.712, 10.560], loss: 0.560642, mae: 0.603981, mean_q: 5.160004
 78717/100000: episode: 1601, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 157.195, mean reward: 9.825 [3.758, 25.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.511, 10.641], loss: 0.433639, mae: 0.533034, mean_q: 5.003382
 78725/100000: episode: 1602, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 27.711, mean reward: 3.464 [2.578, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.405], loss: 1.073800, mae: 0.659276, mean_q: 5.246408
 78733/100000: episode: 1603, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 30.021, mean reward: 3.753 [3.340, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.488], loss: 0.366188, mae: 0.584401, mean_q: 5.101443
 78746/100000: episode: 1604, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 50.122, mean reward: 3.856 [2.752, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.652, 10.525], loss: 0.619174, mae: 0.597256, mean_q: 5.130832
 78761/100000: episode: 1605, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 51.654, mean reward: 3.444 [2.887, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.530], loss: 0.814742, mae: 0.614917, mean_q: 5.203605
 78774/100000: episode: 1606, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 42.971, mean reward: 3.305 [2.812, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-1.705, 10.495], loss: 40.559471, mae: 1.382853, mean_q: 5.858937
 78783/100000: episode: 1607, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 145.934, mean reward: 16.215 [3.220, 112.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.464], loss: 2.113793, mae: 0.842717, mean_q: 5.046977
 78795/100000: episode: 1608, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 36.319, mean reward: 3.027 [2.172, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.307, 10.100], loss: 0.827510, mae: 0.696367, mean_q: 5.214474
 78811/100000: episode: 1609, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 59.479, mean reward: 3.717 [2.705, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.313, 10.489], loss: 957.190186, mae: 4.037859, mean_q: 7.151607
 78823/100000: episode: 1610, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 50.779, mean reward: 4.232 [2.698, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.477, 10.100], loss: 1.437544, mae: 1.191664, mean_q: 4.792845
 78836/100000: episode: 1611, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 55.921, mean reward: 4.302 [2.779, 7.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.811, 10.436], loss: 1.215912, mae: 0.906008, mean_q: 5.359282
 78848/100000: episode: 1612, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 38.704, mean reward: 3.225 [2.312, 4.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.346, 10.100], loss: 0.429129, mae: 0.610011, mean_q: 5.371367
 78864/100000: episode: 1613, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 68.453, mean reward: 4.278 [3.076, 8.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.334, 10.590], loss: 33.788647, mae: 1.124232, mean_q: 5.582430
 78880/100000: episode: 1614, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 61.696, mean reward: 3.856 [2.458, 9.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.850, 10.433], loss: 0.935887, mae: 0.723623, mean_q: 5.225037
 78904/100000: episode: 1615, duration: 0.138s, episode steps: 24, steps per second: 173, episode reward: 144.623, mean reward: 6.026 [3.475, 14.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.356, 10.550], loss: 22.550827, mae: 1.159202, mean_q: 5.767511
 78920/100000: episode: 1616, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 65.383, mean reward: 4.086 [2.732, 5.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.618], loss: 0.436075, mae: 0.628459, mean_q: 5.216171
 78932/100000: episode: 1617, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 53.569, mean reward: 4.464 [3.026, 11.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.301, 10.100], loss: 1.408539, mae: 0.774862, mean_q: 5.492296
 78948/100000: episode: 1618, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 119.746, mean reward: 7.484 [3.966, 15.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.109, 10.709], loss: 12.100873, mae: 0.928239, mean_q: 5.642277
 78960/100000: episode: 1619, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 41.434, mean reward: 3.453 [2.667, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.379, 10.100], loss: 1.411780, mae: 0.701971, mean_q: 5.511387
 78976/100000: episode: 1620, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 68.089, mean reward: 4.256 [2.622, 6.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.430], loss: 32.861065, mae: 1.051110, mean_q: 5.454965
 78988/100000: episode: 1621, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 56.810, mean reward: 4.734 [2.729, 9.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.345, 10.100], loss: 1273.053833, mae: 3.495486, mean_q: 6.140486
 79004/100000: episode: 1622, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 66.998, mean reward: 4.187 [3.216, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.077, 10.463], loss: 4.078207, mae: 2.217094, mean_q: 7.363349
 79020/100000: episode: 1623, duration: 0.108s, episode steps: 16, steps per second: 149, episode reward: 59.418, mean reward: 3.714 [2.847, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.349, 10.495], loss: 12.931938, mae: 1.409238, mean_q: 5.085326
[Info] Complete ISplit Iteration
[Info] Levels: [5.7053666, 7.207142, 12.965151]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 79032/100000: episode: 1624, duration: 4.453s, episode steps: 12, steps per second: 3, episode reward: 39.521, mean reward: 3.293 [2.769, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.205, 10.415], loss: 0.914611, mae: 0.894317, mean_q: 5.860581
 79132/100000: episode: 1625, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 194.212, mean reward: 1.942 [1.473, 4.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.458, 10.148], loss: 0.910525, mae: 0.727111, mean_q: 5.578189
 79232/100000: episode: 1626, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.050, mean reward: 1.851 [1.473, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.854, 10.214], loss: 5.996939, mae: 0.729190, mean_q: 5.563559
 79332/100000: episode: 1627, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 188.092, mean reward: 1.881 [1.462, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.662, 10.244], loss: 6.311207, mae: 0.862546, mean_q: 5.635413
 79432/100000: episode: 1628, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 209.723, mean reward: 2.097 [1.491, 5.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.273, 10.098], loss: 8.001608, mae: 0.867390, mean_q: 5.653964
 79532/100000: episode: 1629, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 185.259, mean reward: 1.853 [1.465, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.323, 10.113], loss: 157.852402, mae: 1.431821, mean_q: 5.784776
 79632/100000: episode: 1630, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 201.443, mean reward: 2.014 [1.447, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.098], loss: 2.675679, mae: 0.748365, mean_q: 5.637623
 79732/100000: episode: 1631, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 185.046, mean reward: 1.850 [1.446, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.098], loss: 11.280020, mae: 0.885106, mean_q: 5.664380
 79832/100000: episode: 1632, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 191.644, mean reward: 1.916 [1.492, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.098], loss: 1.083686, mae: 0.683619, mean_q: 5.545253
 79932/100000: episode: 1633, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 215.775, mean reward: 2.158 [1.459, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.356, 10.098], loss: 308.529663, mae: 2.238677, mean_q: 6.134958
 80032/100000: episode: 1634, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 219.768, mean reward: 2.198 [1.453, 5.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.840, 10.514], loss: 163.752319, mae: 1.350830, mean_q: 6.024116
 80132/100000: episode: 1635, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.079, mean reward: 1.831 [1.462, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.119, 10.098], loss: 157.241653, mae: 1.538965, mean_q: 5.984863
 80232/100000: episode: 1636, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 193.474, mean reward: 1.935 [1.457, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.311, 10.186], loss: 11.245318, mae: 0.926012, mean_q: 5.790440
 80332/100000: episode: 1637, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.306, mean reward: 1.853 [1.469, 2.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.319, 10.098], loss: 2.650232, mae: 0.734169, mean_q: 5.605110
 80432/100000: episode: 1638, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 193.000, mean reward: 1.930 [1.436, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.746, 10.101], loss: 5.964608, mae: 0.755329, mean_q: 5.529881
 80532/100000: episode: 1639, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 180.911, mean reward: 1.809 [1.475, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.779, 10.098], loss: 159.364136, mae: 1.589298, mean_q: 5.973580
 80632/100000: episode: 1640, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 181.474, mean reward: 1.815 [1.448, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.191, 10.117], loss: 11.120039, mae: 0.825450, mean_q: 5.718410
 80732/100000: episode: 1641, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 202.070, mean reward: 2.021 [1.487, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.658, 10.461], loss: 160.886520, mae: 1.560713, mean_q: 6.083522
 80832/100000: episode: 1642, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.515, mean reward: 2.055 [1.507, 7.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.430, 10.098], loss: 153.801117, mae: 1.379399, mean_q: 5.968537
 80932/100000: episode: 1643, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.641, mean reward: 1.866 [1.488, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.374], loss: 7.946148, mae: 0.875778, mean_q: 5.696160
 81032/100000: episode: 1644, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 207.065, mean reward: 2.071 [1.483, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.664, 10.212], loss: 158.943756, mae: 1.467541, mean_q: 5.990125
 81132/100000: episode: 1645, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 185.699, mean reward: 1.857 [1.467, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.899, 10.257], loss: 7.716485, mae: 0.781497, mean_q: 5.625720
 81232/100000: episode: 1646, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 184.023, mean reward: 1.840 [1.450, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.198, 10.098], loss: 7.796185, mae: 0.878104, mean_q: 5.832617
 81332/100000: episode: 1647, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.529, mean reward: 1.825 [1.454, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.098], loss: 314.096619, mae: 2.338408, mean_q: 6.360825
 81432/100000: episode: 1648, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 201.147, mean reward: 2.011 [1.461, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.094, 10.231], loss: 6.109310, mae: 0.847817, mean_q: 5.627851
 81532/100000: episode: 1649, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.959, mean reward: 1.860 [1.462, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.467, 10.180], loss: 154.056000, mae: 1.528870, mean_q: 5.725039
 81632/100000: episode: 1650, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 204.645, mean reward: 2.046 [1.447, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.559, 10.433], loss: 1.033745, mae: 0.700391, mean_q: 5.371611
 81732/100000: episode: 1651, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 180.320, mean reward: 1.803 [1.441, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.156, 10.131], loss: 304.292236, mae: 1.906322, mean_q: 5.948748
 81832/100000: episode: 1652, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 182.646, mean reward: 1.826 [1.475, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.610, 10.208], loss: 468.157349, mae: 2.797527, mean_q: 6.385292
 81932/100000: episode: 1653, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 232.253, mean reward: 2.323 [1.485, 7.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.616, 10.098], loss: 0.780260, mae: 0.679871, mean_q: 5.468187
 82032/100000: episode: 1654, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 189.343, mean reward: 1.893 [1.469, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.038, 10.184], loss: 5.904669, mae: 0.719379, mean_q: 5.315542
 82132/100000: episode: 1655, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 177.631, mean reward: 1.776 [1.458, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.250, 10.098], loss: 5.732441, mae: 0.725562, mean_q: 5.293077
 82232/100000: episode: 1656, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 187.942, mean reward: 1.879 [1.440, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.616, 10.191], loss: 7.592303, mae: 0.702364, mean_q: 5.221752
 82332/100000: episode: 1657, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 195.664, mean reward: 1.957 [1.446, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.910, 10.210], loss: 2.566754, mae: 0.683009, mean_q: 5.160291
 82432/100000: episode: 1658, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 202.674, mean reward: 2.027 [1.457, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.494, 10.098], loss: 158.648926, mae: 1.416485, mean_q: 5.491314
 82532/100000: episode: 1659, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 194.863, mean reward: 1.949 [1.467, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.028, 10.098], loss: 152.875336, mae: 1.266042, mean_q: 5.348948
 82632/100000: episode: 1660, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 182.394, mean reward: 1.824 [1.462, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.040, 10.098], loss: 162.128342, mae: 1.465580, mean_q: 5.502930
 82732/100000: episode: 1661, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 191.191, mean reward: 1.912 [1.472, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.624, 10.098], loss: 304.876404, mae: 1.924197, mean_q: 5.686410
 82832/100000: episode: 1662, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 203.236, mean reward: 2.032 [1.443, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.929, 10.098], loss: 0.949409, mae: 0.609801, mean_q: 5.006736
 82932/100000: episode: 1663, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 181.030, mean reward: 1.810 [1.462, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.377, 10.098], loss: 158.354126, mae: 1.097458, mean_q: 5.164577
 83032/100000: episode: 1664, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 192.025, mean reward: 1.920 [1.490, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.708, 10.098], loss: 318.545105, mae: 2.316228, mean_q: 5.936559
 83132/100000: episode: 1665, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 204.361, mean reward: 2.044 [1.458, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.700, 10.098], loss: 168.459839, mae: 1.604062, mean_q: 5.325838
 83232/100000: episode: 1666, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 191.935, mean reward: 1.919 [1.485, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.957, 10.264], loss: 2.623317, mae: 0.618877, mean_q: 4.781987
 83332/100000: episode: 1667, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 191.303, mean reward: 1.913 [1.468, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.259, 10.195], loss: 0.476674, mae: 0.488898, mean_q: 4.526691
 83432/100000: episode: 1668, duration: 0.727s, episode steps: 100, steps per second: 137, episode reward: 184.591, mean reward: 1.846 [1.464, 2.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.143], loss: 0.433224, mae: 0.449228, mean_q: 4.425067
 83532/100000: episode: 1669, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 198.622, mean reward: 1.986 [1.447, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.950, 10.098], loss: 2.209875, mae: 0.459104, mean_q: 4.292386
 83632/100000: episode: 1670, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.324, mean reward: 1.973 [1.501, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.254], loss: 0.358758, mae: 0.407753, mean_q: 4.201267
 83732/100000: episode: 1671, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 181.545, mean reward: 1.815 [1.448, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.854, 10.199], loss: 0.228572, mae: 0.374742, mean_q: 4.107141
 83832/100000: episode: 1672, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 194.361, mean reward: 1.944 [1.485, 5.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.868, 10.098], loss: 0.192555, mae: 0.354238, mean_q: 3.989245
 83932/100000: episode: 1673, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 208.491, mean reward: 2.085 [1.485, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.794, 10.098], loss: 0.135385, mae: 0.325059, mean_q: 3.917629
 84032/100000: episode: 1674, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 193.467, mean reward: 1.935 [1.432, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.182], loss: 0.112770, mae: 0.315633, mean_q: 3.842681
 84132/100000: episode: 1675, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 178.052, mean reward: 1.781 [1.453, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.098], loss: 0.105475, mae: 0.317281, mean_q: 3.838588
 84232/100000: episode: 1676, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 190.466, mean reward: 1.905 [1.466, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.289], loss: 0.097304, mae: 0.306638, mean_q: 3.846370
 84332/100000: episode: 1677, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 186.883, mean reward: 1.869 [1.446, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.288, 10.135], loss: 0.098892, mae: 0.307129, mean_q: 3.838648
 84432/100000: episode: 1678, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 185.892, mean reward: 1.859 [1.455, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.675, 10.187], loss: 0.094911, mae: 0.302096, mean_q: 3.825858
 84532/100000: episode: 1679, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 181.744, mean reward: 1.817 [1.448, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.937, 10.184], loss: 0.091988, mae: 0.298005, mean_q: 3.816944
 84632/100000: episode: 1680, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 190.091, mean reward: 1.901 [1.482, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.340, 10.308], loss: 0.091299, mae: 0.300171, mean_q: 3.806681
 84732/100000: episode: 1681, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 238.552, mean reward: 2.386 [1.470, 10.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.345, 10.291], loss: 0.093870, mae: 0.298944, mean_q: 3.807880
 84832/100000: episode: 1682, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 188.943, mean reward: 1.889 [1.481, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.571, 10.122], loss: 0.099640, mae: 0.295607, mean_q: 3.815651
 84932/100000: episode: 1683, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 212.501, mean reward: 2.125 [1.496, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.745, 10.098], loss: 0.098805, mae: 0.302682, mean_q: 3.830843
 85032/100000: episode: 1684, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 211.670, mean reward: 2.117 [1.487, 7.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.880, 10.277], loss: 0.114454, mae: 0.305079, mean_q: 3.829396
 85132/100000: episode: 1685, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 188.035, mean reward: 1.880 [1.468, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.362, 10.098], loss: 0.109508, mae: 0.303760, mean_q: 3.822209
 85232/100000: episode: 1686, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 184.476, mean reward: 1.845 [1.464, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.210, 10.098], loss: 0.098914, mae: 0.299698, mean_q: 3.819910
 85332/100000: episode: 1687, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 184.687, mean reward: 1.847 [1.447, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.880, 10.098], loss: 0.087296, mae: 0.295787, mean_q: 3.825993
 85432/100000: episode: 1688, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 185.126, mean reward: 1.851 [1.441, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.404, 10.216], loss: 0.114274, mae: 0.308647, mean_q: 3.845798
 85532/100000: episode: 1689, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 176.288, mean reward: 1.763 [1.458, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.129, 10.103], loss: 0.098849, mae: 0.299483, mean_q: 3.845538
 85632/100000: episode: 1690, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 202.243, mean reward: 2.022 [1.485, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.768, 10.243], loss: 0.104479, mae: 0.298836, mean_q: 3.825687
 85732/100000: episode: 1691, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 185.035, mean reward: 1.850 [1.482, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.597, 10.105], loss: 0.109692, mae: 0.302043, mean_q: 3.843487
 85832/100000: episode: 1692, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 216.363, mean reward: 2.164 [1.560, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.645, 10.098], loss: 0.080531, mae: 0.290871, mean_q: 3.818507
 85932/100000: episode: 1693, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 196.128, mean reward: 1.961 [1.450, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.546, 10.258], loss: 0.082823, mae: 0.275705, mean_q: 3.814356
 86032/100000: episode: 1694, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 202.785, mean reward: 2.028 [1.436, 5.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.587, 10.139], loss: 0.092708, mae: 0.290103, mean_q: 3.819498
 86132/100000: episode: 1695, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 198.748, mean reward: 1.987 [1.441, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.138, 10.098], loss: 0.097422, mae: 0.286488, mean_q: 3.812064
 86232/100000: episode: 1696, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 194.768, mean reward: 1.948 [1.473, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.518, 10.222], loss: 0.093513, mae: 0.287437, mean_q: 3.841740
 86332/100000: episode: 1697, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 190.180, mean reward: 1.902 [1.465, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.098], loss: 0.098466, mae: 0.288288, mean_q: 3.849013
 86432/100000: episode: 1698, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 186.884, mean reward: 1.869 [1.456, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.816, 10.211], loss: 0.092961, mae: 0.293162, mean_q: 3.844920
 86532/100000: episode: 1699, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 186.081, mean reward: 1.861 [1.441, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.193], loss: 0.082741, mae: 0.284132, mean_q: 3.833092
 86632/100000: episode: 1700, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 203.510, mean reward: 2.035 [1.474, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.570, 10.098], loss: 0.099491, mae: 0.283432, mean_q: 3.813797
 86732/100000: episode: 1701, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 233.382, mean reward: 2.334 [1.488, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.670, 10.399], loss: 0.083938, mae: 0.291337, mean_q: 3.840306
 86832/100000: episode: 1702, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 193.295, mean reward: 1.933 [1.438, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.071, 10.158], loss: 0.096223, mae: 0.301431, mean_q: 3.854650
 86932/100000: episode: 1703, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 191.690, mean reward: 1.917 [1.446, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.476, 10.098], loss: 0.078625, mae: 0.283807, mean_q: 3.854870
 87032/100000: episode: 1704, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 240.100, mean reward: 2.401 [1.584, 5.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.402, 10.098], loss: 0.083892, mae: 0.285671, mean_q: 3.847011
 87132/100000: episode: 1705, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 205.865, mean reward: 2.059 [1.473, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.227, 10.143], loss: 0.086058, mae: 0.292832, mean_q: 3.860261
 87232/100000: episode: 1706, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 191.748, mean reward: 1.917 [1.457, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.293, 10.098], loss: 0.094458, mae: 0.291249, mean_q: 3.875184
 87332/100000: episode: 1707, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 192.335, mean reward: 1.923 [1.429, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.347, 10.194], loss: 0.092224, mae: 0.297531, mean_q: 3.877584
 87432/100000: episode: 1708, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 196.485, mean reward: 1.965 [1.431, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.105], loss: 0.105831, mae: 0.300425, mean_q: 3.890316
 87532/100000: episode: 1709, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 205.990, mean reward: 2.060 [1.465, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.338, 10.232], loss: 0.102326, mae: 0.302269, mean_q: 3.879373
 87632/100000: episode: 1710, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 201.036, mean reward: 2.010 [1.524, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.646, 10.189], loss: 0.095231, mae: 0.301134, mean_q: 3.888621
 87732/100000: episode: 1711, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 205.373, mean reward: 2.054 [1.500, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.897, 10.098], loss: 0.116934, mae: 0.311742, mean_q: 3.917012
 87832/100000: episode: 1712, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 194.531, mean reward: 1.945 [1.444, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.892, 10.173], loss: 0.096914, mae: 0.302265, mean_q: 3.902082
 87932/100000: episode: 1713, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 198.624, mean reward: 1.986 [1.445, 5.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.899, 10.266], loss: 0.092102, mae: 0.293173, mean_q: 3.898816
 88032/100000: episode: 1714, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 200.879, mean reward: 2.009 [1.490, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.555, 10.098], loss: 0.099511, mae: 0.303675, mean_q: 3.890215
 88132/100000: episode: 1715, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 216.379, mean reward: 2.164 [1.441, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.721, 10.098], loss: 0.092148, mae: 0.299166, mean_q: 3.902006
 88232/100000: episode: 1716, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 211.151, mean reward: 2.112 [1.453, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.928, 10.098], loss: 0.090004, mae: 0.294520, mean_q: 3.894387
 88332/100000: episode: 1717, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 214.094, mean reward: 2.141 [1.496, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.631, 10.256], loss: 0.095892, mae: 0.302183, mean_q: 3.912323
 88432/100000: episode: 1718, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 181.475, mean reward: 1.815 [1.487, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.536, 10.180], loss: 0.113381, mae: 0.314264, mean_q: 3.912863
 88532/100000: episode: 1719, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 192.096, mean reward: 1.921 [1.479, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.605, 10.176], loss: 0.091271, mae: 0.294887, mean_q: 3.905546
 88632/100000: episode: 1720, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 195.428, mean reward: 1.954 [1.511, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.383, 10.146], loss: 0.116560, mae: 0.316426, mean_q: 3.929426
 88732/100000: episode: 1721, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.873, mean reward: 1.899 [1.476, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.098], loss: 0.114931, mae: 0.308006, mean_q: 3.927209
 88832/100000: episode: 1722, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 206.954, mean reward: 2.070 [1.508, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.920, 10.277], loss: 0.089282, mae: 0.293271, mean_q: 3.922348
 88932/100000: episode: 1723, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 193.548, mean reward: 1.935 [1.454, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.491, 10.232], loss: 0.111808, mae: 0.307363, mean_q: 3.941064
[Info] 1-TH LEVEL FOUND: 5.945688247680664, Considering 10/90 traces
 89032/100000: episode: 1724, duration: 5.017s, episode steps: 100, steps per second: 20, episode reward: 197.732, mean reward: 1.977 [1.446, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.036, 10.098], loss: 0.107938, mae: 0.305914, mean_q: 3.924290
 89074/100000: episode: 1725, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 155.495, mean reward: 3.702 [2.088, 7.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.238, 10.400], loss: 0.127567, mae: 0.330883, mean_q: 3.984523
 89092/100000: episode: 1726, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 40.319, mean reward: 2.240 [1.802, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.262], loss: 0.062876, mae: 0.271746, mean_q: 3.921981
 89117/100000: episode: 1727, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 68.059, mean reward: 2.722 [2.036, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.556, 10.431], loss: 0.112775, mae: 0.320968, mean_q: 3.987053
 89160/100000: episode: 1728, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 115.320, mean reward: 2.682 [1.775, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.217, 10.521], loss: 0.088576, mae: 0.303709, mean_q: 3.958108
 89198/100000: episode: 1729, duration: 0.219s, episode steps: 38, steps per second: 173, episode reward: 86.347, mean reward: 2.272 [1.550, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.306, 10.288], loss: 0.112520, mae: 0.312510, mean_q: 3.984992
 89214/100000: episode: 1730, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 41.539, mean reward: 2.596 [1.868, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.327], loss: 0.116318, mae: 0.298492, mean_q: 3.992452
 89246/100000: episode: 1731, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 87.285, mean reward: 2.728 [1.999, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.455, 10.100], loss: 0.081818, mae: 0.291127, mean_q: 3.936141
 89304/100000: episode: 1732, duration: 0.323s, episode steps: 58, steps per second: 180, episode reward: 157.928, mean reward: 2.723 [1.710, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.215, 10.100], loss: 0.090267, mae: 0.299938, mean_q: 3.978997
 89329/100000: episode: 1733, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 62.816, mean reward: 2.513 [1.882, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.420, 10.313], loss: 0.097112, mae: 0.311424, mean_q: 4.021917
 89387/100000: episode: 1734, duration: 0.331s, episode steps: 58, steps per second: 175, episode reward: 175.287, mean reward: 3.022 [1.789, 13.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.680, 10.100], loss: 0.145415, mae: 0.348098, mean_q: 4.014827
 89405/100000: episode: 1735, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 35.560, mean reward: 1.976 [1.731, 2.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.239], loss: 0.132009, mae: 0.312822, mean_q: 4.006043
 89421/100000: episode: 1736, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 44.816, mean reward: 2.801 [2.246, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.356], loss: 0.095907, mae: 0.313604, mean_q: 4.068855
 89446/100000: episode: 1737, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 54.918, mean reward: 2.197 [1.661, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.691, 10.239], loss: 0.117136, mae: 0.313375, mean_q: 4.004203
 89477/100000: episode: 1738, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 76.188, mean reward: 2.458 [1.884, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.747, 10.372], loss: 0.108334, mae: 0.318742, mean_q: 4.028262
 89495/100000: episode: 1739, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 63.739, mean reward: 3.541 [1.967, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-1.426, 10.555], loss: 0.106924, mae: 0.321956, mean_q: 4.028113
 89526/100000: episode: 1740, duration: 0.201s, episode steps: 31, steps per second: 155, episode reward: 73.748, mean reward: 2.379 [1.801, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.369, 10.269], loss: 0.125936, mae: 0.320799, mean_q: 4.022944
 89564/100000: episode: 1741, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 77.373, mean reward: 2.036 [1.447, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.626, 10.191], loss: 0.163964, mae: 0.364535, mean_q: 4.145514
 89590/100000: episode: 1742, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 65.801, mean reward: 2.531 [1.755, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.220, 10.331], loss: 0.165545, mae: 0.339322, mean_q: 4.057334
 89632/100000: episode: 1743, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 112.671, mean reward: 2.683 [1.735, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.456, 10.454], loss: 0.142783, mae: 0.332495, mean_q: 4.090777
 89658/100000: episode: 1744, duration: 0.185s, episode steps: 26, steps per second: 140, episode reward: 60.317, mean reward: 2.320 [1.652, 5.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.575, 10.196], loss: 0.159500, mae: 0.360979, mean_q: 4.163898
 89674/100000: episode: 1745, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 51.387, mean reward: 3.212 [2.190, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.218, 10.498], loss: 0.106745, mae: 0.326254, mean_q: 4.120204
 89692/100000: episode: 1746, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 46.458, mean reward: 2.581 [1.802, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.428], loss: 0.156697, mae: 0.344152, mean_q: 4.091856
 89710/100000: episode: 1747, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 39.913, mean reward: 2.217 [1.690, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.493], loss: 0.096636, mae: 0.317892, mean_q: 4.027050
 89741/100000: episode: 1748, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 66.123, mean reward: 2.133 [1.574, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.323, 10.192], loss: 0.130882, mae: 0.336815, mean_q: 4.079123
 89766/100000: episode: 1749, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 65.669, mean reward: 2.627 [2.032, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.643, 10.392], loss: 0.134003, mae: 0.363051, mean_q: 4.105901
 89824/100000: episode: 1750, duration: 0.310s, episode steps: 58, steps per second: 187, episode reward: 220.586, mean reward: 3.803 [2.377, 7.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.815 [-1.173, 10.100], loss: 0.145607, mae: 0.345024, mean_q: 4.100133
 89867/100000: episode: 1751, duration: 0.274s, episode steps: 43, steps per second: 157, episode reward: 97.362, mean reward: 2.264 [1.495, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.731, 10.154], loss: 0.134333, mae: 0.344428, mean_q: 4.115527
 89883/100000: episode: 1752, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 41.524, mean reward: 2.595 [1.866, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.315], loss: 0.107639, mae: 0.327711, mean_q: 4.090798
 89915/100000: episode: 1753, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 80.721, mean reward: 2.523 [1.704, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.688, 10.100], loss: 0.102818, mae: 0.325124, mean_q: 4.115139
 89947/100000: episode: 1754, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 72.887, mean reward: 2.278 [1.843, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.230, 10.100], loss: 0.132230, mae: 0.347967, mean_q: 4.155200
 89985/100000: episode: 1755, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 112.004, mean reward: 2.947 [2.337, 4.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.834, 10.458], loss: 0.141759, mae: 0.350265, mean_q: 4.142203
 90003/100000: episode: 1756, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 57.771, mean reward: 3.210 [2.125, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.161, 10.508], loss: 0.110530, mae: 0.329200, mean_q: 4.108842
 90029/100000: episode: 1757, duration: 0.163s, episode steps: 26, steps per second: 159, episode reward: 50.595, mean reward: 1.946 [1.568, 2.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.780, 10.175], loss: 0.131933, mae: 0.363370, mean_q: 4.182486
 90071/100000: episode: 1758, duration: 0.240s, episode steps: 42, steps per second: 175, episode reward: 111.576, mean reward: 2.657 [1.867, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.605, 10.387], loss: 0.133555, mae: 0.344456, mean_q: 4.179576
 90089/100000: episode: 1759, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 43.198, mean reward: 2.400 [1.913, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.752, 10.393], loss: 0.173882, mae: 0.365626, mean_q: 4.198625
 90127/100000: episode: 1760, duration: 0.232s, episode steps: 38, steps per second: 164, episode reward: 91.712, mean reward: 2.413 [1.816, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.011, 10.315], loss: 0.159647, mae: 0.351143, mean_q: 4.210996
 90143/100000: episode: 1761, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 44.612, mean reward: 2.788 [2.149, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.588, 10.347], loss: 0.131614, mae: 0.355948, mean_q: 4.185320
 90186/100000: episode: 1762, duration: 0.252s, episode steps: 43, steps per second: 171, episode reward: 137.393, mean reward: 3.195 [2.151, 5.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.901, 10.412], loss: 0.135003, mae: 0.346936, mean_q: 4.232877
 90204/100000: episode: 1763, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 38.169, mean reward: 2.120 [1.795, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.347], loss: 0.129704, mae: 0.346611, mean_q: 4.202193
 90247/100000: episode: 1764, duration: 0.238s, episode steps: 43, steps per second: 180, episode reward: 85.348, mean reward: 1.985 [1.441, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.153, 10.134], loss: 0.113439, mae: 0.342334, mean_q: 4.208700
 90279/100000: episode: 1765, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 91.000, mean reward: 2.844 [2.381, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.451, 10.100], loss: 0.174880, mae: 0.374220, mean_q: 4.253696
 90297/100000: episode: 1766, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 34.723, mean reward: 1.929 [1.662, 2.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.312], loss: 0.119953, mae: 0.332680, mean_q: 4.182506
 90328/100000: episode: 1767, duration: 0.197s, episode steps: 31, steps per second: 158, episode reward: 101.929, mean reward: 3.288 [2.244, 6.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.077, 10.476], loss: 0.142139, mae: 0.360926, mean_q: 4.280065
 90371/100000: episode: 1768, duration: 0.260s, episode steps: 43, steps per second: 165, episode reward: 107.021, mean reward: 2.489 [1.502, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.550, 10.100], loss: 0.146076, mae: 0.351762, mean_q: 4.264609
 90403/100000: episode: 1769, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 78.025, mean reward: 2.438 [1.639, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.752, 10.100], loss: 0.284272, mae: 0.410654, mean_q: 4.299889
 90429/100000: episode: 1770, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 55.962, mean reward: 2.152 [1.604, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.245], loss: 0.135983, mae: 0.359822, mean_q: 4.287974
 90472/100000: episode: 1771, duration: 0.242s, episode steps: 43, steps per second: 178, episode reward: 111.267, mean reward: 2.588 [1.830, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.071, 10.306], loss: 0.120064, mae: 0.350286, mean_q: 4.304852
 90504/100000: episode: 1772, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 101.587, mean reward: 3.175 [2.571, 4.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.334, 10.100], loss: 0.133521, mae: 0.356491, mean_q: 4.295757
 90522/100000: episode: 1773, duration: 0.123s, episode steps: 18, steps per second: 146, episode reward: 50.772, mean reward: 2.821 [2.038, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.042, 10.522], loss: 0.089062, mae: 0.310963, mean_q: 4.243525
 90554/100000: episode: 1774, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 100.918, mean reward: 3.154 [1.756, 6.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.225, 10.100], loss: 0.133661, mae: 0.358172, mean_q: 4.272382
 90580/100000: episode: 1775, duration: 0.180s, episode steps: 26, steps per second: 145, episode reward: 63.490, mean reward: 2.442 [1.873, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.267], loss: 0.132358, mae: 0.365295, mean_q: 4.388742
 90611/100000: episode: 1776, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 73.367, mean reward: 2.367 [1.476, 5.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.412, 10.161], loss: 0.131181, mae: 0.345316, mean_q: 4.322973
 90636/100000: episode: 1777, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 72.494, mean reward: 2.900 [2.126, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.549, 10.495], loss: 0.205083, mae: 0.399409, mean_q: 4.405608
 90678/100000: episode: 1778, duration: 0.269s, episode steps: 42, steps per second: 156, episode reward: 160.302, mean reward: 3.817 [2.084, 10.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.479, 10.332], loss: 0.167874, mae: 0.400887, mean_q: 4.391968
 90696/100000: episode: 1779, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 42.423, mean reward: 2.357 [1.870, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.348], loss: 0.138632, mae: 0.362781, mean_q: 4.336381
 90721/100000: episode: 1780, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 51.294, mean reward: 2.052 [1.643, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.226], loss: 0.194822, mae: 0.371330, mean_q: 4.382071
 90763/100000: episode: 1781, duration: 0.261s, episode steps: 42, steps per second: 161, episode reward: 192.908, mean reward: 4.593 [2.546, 11.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.454, 10.623], loss: 0.148157, mae: 0.374082, mean_q: 4.396533
 90781/100000: episode: 1782, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 42.819, mean reward: 2.379 [1.939, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.551, 10.343], loss: 0.159843, mae: 0.388371, mean_q: 4.436288
 90812/100000: episode: 1783, duration: 0.188s, episode steps: 31, steps per second: 165, episode reward: 67.034, mean reward: 2.162 [1.702, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.158], loss: 0.238010, mae: 0.441718, mean_q: 4.472163
 90838/100000: episode: 1784, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 70.402, mean reward: 2.708 [1.964, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.077, 10.461], loss: 0.203933, mae: 0.439918, mean_q: 4.365088
 90869/100000: episode: 1785, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 78.193, mean reward: 2.522 [1.680, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.265], loss: 0.150273, mae: 0.383387, mean_q: 4.444163
 90895/100000: episode: 1786, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 52.861, mean reward: 2.033 [1.624, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.664, 10.193], loss: 0.130996, mae: 0.373462, mean_q: 4.375867
 90938/100000: episode: 1787, duration: 0.249s, episode steps: 43, steps per second: 172, episode reward: 90.389, mean reward: 2.102 [1.559, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.059, 10.223], loss: 0.168190, mae: 0.390856, mean_q: 4.468754
 90996/100000: episode: 1788, duration: 0.374s, episode steps: 58, steps per second: 155, episode reward: 159.900, mean reward: 2.757 [1.752, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.286, 10.100], loss: 0.166209, mae: 0.381334, mean_q: 4.406252
 91028/100000: episode: 1789, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 93.612, mean reward: 2.925 [2.308, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.742, 10.100], loss: 0.224972, mae: 0.404815, mean_q: 4.443748
 91059/100000: episode: 1790, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 75.867, mean reward: 2.447 [1.645, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.088, 10.322], loss: 0.131717, mae: 0.366537, mean_q: 4.463628
 91075/100000: episode: 1791, duration: 0.104s, episode steps: 16, steps per second: 153, episode reward: 32.289, mean reward: 2.018 [1.574, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.386, 10.228], loss: 0.175288, mae: 0.376040, mean_q: 4.432822
 91118/100000: episode: 1792, duration: 0.344s, episode steps: 43, steps per second: 125, episode reward: 97.429, mean reward: 2.266 [1.586, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.335, 10.108], loss: 0.185063, mae: 0.397418, mean_q: 4.467859
 91144/100000: episode: 1793, duration: 0.187s, episode steps: 26, steps per second: 139, episode reward: 67.613, mean reward: 2.601 [1.851, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.325, 10.377], loss: 0.136959, mae: 0.365357, mean_q: 4.456896
 91182/100000: episode: 1794, duration: 0.317s, episode steps: 38, steps per second: 120, episode reward: 80.990, mean reward: 2.131 [1.458, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.571, 10.100], loss: 0.212618, mae: 0.395304, mean_q: 4.490324
 91240/100000: episode: 1795, duration: 0.453s, episode steps: 58, steps per second: 128, episode reward: 267.547, mean reward: 4.613 [1.944, 30.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-0.545, 10.100], loss: 0.383622, mae: 0.433503, mean_q: 4.523016
 91256/100000: episode: 1796, duration: 0.130s, episode steps: 16, steps per second: 123, episode reward: 48.251, mean reward: 3.016 [1.849, 4.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.650, 10.334], loss: 0.869095, mae: 0.511626, mean_q: 4.677486
 91282/100000: episode: 1797, duration: 0.204s, episode steps: 26, steps per second: 128, episode reward: 49.218, mean reward: 1.893 [1.479, 2.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.235, 10.100], loss: 0.252634, mae: 0.430334, mean_q: 4.514710
 91300/100000: episode: 1798, duration: 0.152s, episode steps: 18, steps per second: 119, episode reward: 33.199, mean reward: 1.844 [1.515, 2.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.154, 10.213], loss: 0.817285, mae: 0.507889, mean_q: 4.622734
 91316/100000: episode: 1799, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 41.986, mean reward: 2.624 [2.182, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.275, 10.324], loss: 0.178204, mae: 0.423064, mean_q: 4.502845
 91374/100000: episode: 1800, duration: 0.416s, episode steps: 58, steps per second: 139, episode reward: 161.517, mean reward: 2.785 [1.534, 5.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-0.889, 10.100], loss: 0.196158, mae: 0.405556, mean_q: 4.534506
 91399/100000: episode: 1801, duration: 0.160s, episode steps: 25, steps per second: 157, episode reward: 50.327, mean reward: 2.013 [1.474, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.416, 10.163], loss: 0.650485, mae: 0.444923, mean_q: 4.592456
 91442/100000: episode: 1802, duration: 0.368s, episode steps: 43, steps per second: 117, episode reward: 118.591, mean reward: 2.758 [1.910, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.186, 10.260], loss: 0.202417, mae: 0.399046, mean_q: 4.500969
 91458/100000: episode: 1803, duration: 0.157s, episode steps: 16, steps per second: 102, episode reward: 41.138, mean reward: 2.571 [2.270, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.468], loss: 0.309484, mae: 0.434776, mean_q: 4.674164
 91501/100000: episode: 1804, duration: 0.347s, episode steps: 43, steps per second: 124, episode reward: 94.203, mean reward: 2.191 [1.524, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.676, 10.316], loss: 0.251047, mae: 0.420682, mean_q: 4.565347
 91519/100000: episode: 1805, duration: 0.139s, episode steps: 18, steps per second: 129, episode reward: 45.049, mean reward: 2.503 [2.235, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.738, 10.364], loss: 0.272750, mae: 0.415648, mean_q: 4.558635
 91551/100000: episode: 1806, duration: 0.188s, episode steps: 32, steps per second: 171, episode reward: 88.093, mean reward: 2.753 [1.795, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.009, 10.100], loss: 0.589958, mae: 0.473481, mean_q: 4.590298
 91594/100000: episode: 1807, duration: 0.281s, episode steps: 43, steps per second: 153, episode reward: 125.876, mean reward: 2.927 [1.668, 6.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.228, 10.470], loss: 0.238622, mae: 0.423469, mean_q: 4.649188
[Info] FALSIFICATION!
 91602/100000: episode: 1808, duration: 0.204s, episode steps: 8, steps per second: 39, episode reward: 1044.405, mean reward: 130.551 [3.254, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.027, 5.325], loss: 0.249776, mae: 0.433064, mean_q: 4.476024
 91620/100000: episode: 1809, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 57.375, mean reward: 3.188 [2.126, 6.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.255, 10.498], loss: 0.790493, mae: 0.494391, mean_q: 4.680804
 91651/100000: episode: 1810, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 65.034, mean reward: 2.098 [1.599, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.260, 10.298], loss: 0.199394, mae: 0.420619, mean_q: 4.646149
 91709/100000: episode: 1811, duration: 0.315s, episode steps: 58, steps per second: 184, episode reward: 160.320, mean reward: 2.764 [1.669, 5.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.278, 10.100], loss: 0.427907, mae: 0.459556, mean_q: 4.672794
 91740/100000: episode: 1812, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 83.447, mean reward: 2.692 [1.484, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.316, 10.189], loss: 497.319122, mae: 2.567861, mean_q: 5.612382
 91778/100000: episode: 1813, duration: 0.256s, episode steps: 38, steps per second: 149, episode reward: 88.518, mean reward: 2.329 [1.806, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.429, 10.477], loss: 0.317407, mae: 0.562775, mean_q: 4.669770
[Info] Complete ISplit Iteration
[Info] Levels: [5.9456882, 10.215519]
[Info] Cond. Prob: [0.1, 0.04]
[Info] Error Prob: 0.004

 91820/100000: episode: 1814, duration: 4.658s, episode steps: 42, steps per second: 9, episode reward: 100.181, mean reward: 2.385 [1.556, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.184, 10.145], loss: 0.234993, mae: 0.470193, mean_q: 4.599999
 91920/100000: episode: 1815, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 189.990, mean reward: 1.900 [1.448, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.498, 10.211], loss: 0.273232, mae: 0.486383, mean_q: 4.612303
 92020/100000: episode: 1816, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.657, mean reward: 1.957 [1.460, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.788, 10.180], loss: 153.979797, mae: 1.155644, mean_q: 4.946482
 92120/100000: episode: 1817, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 186.831, mean reward: 1.868 [1.443, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.143, 10.098], loss: 305.560303, mae: 1.550951, mean_q: 5.168487
 92220/100000: episode: 1818, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 249.868, mean reward: 2.499 [1.450, 8.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.082, 10.098], loss: 304.990112, mae: 1.937770, mean_q: 5.476879
 92320/100000: episode: 1819, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 204.210, mean reward: 2.042 [1.514, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.637, 10.098], loss: 153.158966, mae: 1.201643, mean_q: 5.230610
 92420/100000: episode: 1820, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 204.119, mean reward: 2.041 [1.441, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.549, 10.322], loss: 152.847107, mae: 0.873139, mean_q: 4.961793
 92520/100000: episode: 1821, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 184.636, mean reward: 1.846 [1.443, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.413, 10.098], loss: 153.098709, mae: 1.267242, mean_q: 5.428014
 92620/100000: episode: 1822, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 204.952, mean reward: 2.050 [1.493, 4.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.613, 10.218], loss: 0.381386, mae: 0.592012, mean_q: 4.901141
 92720/100000: episode: 1823, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 194.087, mean reward: 1.941 [1.489, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.981, 10.191], loss: 152.650970, mae: 0.992846, mean_q: 5.011156
 92820/100000: episode: 1824, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.430, mean reward: 1.934 [1.484, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.169], loss: 152.622696, mae: 1.067007, mean_q: 5.132058
 92920/100000: episode: 1825, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 212.442, mean reward: 2.124 [1.479, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.805, 10.098], loss: 0.329116, mae: 0.539618, mean_q: 4.851858
 93020/100000: episode: 1826, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 195.646, mean reward: 1.956 [1.466, 5.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.023, 10.098], loss: 0.370688, mae: 0.513892, mean_q: 4.747565
 93120/100000: episode: 1827, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 196.149, mean reward: 1.961 [1.470, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.988, 10.434], loss: 0.396820, mae: 0.509329, mean_q: 4.700943
 93220/100000: episode: 1828, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 213.488, mean reward: 2.135 [1.456, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.146], loss: 0.362317, mae: 0.503982, mean_q: 4.699109
 93320/100000: episode: 1829, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 195.509, mean reward: 1.955 [1.463, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.222, 10.446], loss: 0.321356, mae: 0.510885, mean_q: 4.658566
 93420/100000: episode: 1830, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 208.557, mean reward: 2.086 [1.469, 5.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.779, 10.098], loss: 153.154449, mae: 1.173899, mean_q: 4.960801
 93520/100000: episode: 1831, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.723, mean reward: 1.907 [1.460, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.337, 10.098], loss: 0.281704, mae: 0.495071, mean_q: 4.641034
 93620/100000: episode: 1832, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 192.754, mean reward: 1.928 [1.441, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.508, 10.103], loss: 152.539001, mae: 0.901127, mean_q: 4.778952
 93720/100000: episode: 1833, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 184.717, mean reward: 1.847 [1.443, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.952, 10.141], loss: 303.946442, mae: 1.650285, mean_q: 5.219110
 93820/100000: episode: 1834, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 184.629, mean reward: 1.846 [1.467, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.553, 10.155], loss: 301.950043, mae: 1.632613, mean_q: 5.267748
 93920/100000: episode: 1835, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 182.970, mean reward: 1.830 [1.475, 2.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.676, 10.214], loss: 0.485037, mae: 0.601326, mean_q: 4.863426
 94020/100000: episode: 1836, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 209.044, mean reward: 2.090 [1.458, 4.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.408], loss: 0.328166, mae: 0.536206, mean_q: 4.722796
 94120/100000: episode: 1837, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 195.435, mean reward: 1.954 [1.478, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.098], loss: 0.321048, mae: 0.529071, mean_q: 4.677595
 94220/100000: episode: 1838, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 190.778, mean reward: 1.908 [1.465, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.073, 10.098], loss: 0.308396, mae: 0.510603, mean_q: 4.589479
 94320/100000: episode: 1839, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 181.792, mean reward: 1.818 [1.441, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.419, 10.098], loss: 303.494080, mae: 1.579417, mean_q: 5.016204
 94420/100000: episode: 1840, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.572, mean reward: 1.876 [1.435, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.780, 10.205], loss: 302.478943, mae: 1.526694, mean_q: 4.945685
 94520/100000: episode: 1841, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 192.550, mean reward: 1.926 [1.469, 4.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.958, 10.138], loss: 151.667694, mae: 1.347633, mean_q: 5.224028
 94620/100000: episode: 1842, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 196.013, mean reward: 1.960 [1.449, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.101, 10.098], loss: 300.235962, mae: 1.637234, mean_q: 5.237114
 94720/100000: episode: 1843, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.444, mean reward: 1.884 [1.457, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.933, 10.356], loss: 150.615036, mae: 0.919729, mean_q: 4.768631
 94820/100000: episode: 1844, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 243.083, mean reward: 2.431 [1.458, 6.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.261, 10.098], loss: 0.984034, mae: 0.783548, mean_q: 4.921008
 94920/100000: episode: 1845, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 187.143, mean reward: 1.871 [1.467, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.118, 10.164], loss: 0.362700, mae: 0.542047, mean_q: 4.566504
 95020/100000: episode: 1846, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 202.138, mean reward: 2.021 [1.489, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.424, 10.143], loss: 0.308366, mae: 0.511612, mean_q: 4.440661
 95120/100000: episode: 1847, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 186.560, mean reward: 1.866 [1.500, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.412, 10.314], loss: 151.736359, mae: 1.018688, mean_q: 4.619222
 95220/100000: episode: 1848, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 178.603, mean reward: 1.786 [1.480, 2.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.374, 10.105], loss: 300.381958, mae: 1.358975, mean_q: 4.663959
 95320/100000: episode: 1849, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 222.214, mean reward: 2.222 [1.506, 5.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.609, 10.178], loss: 1.155306, mae: 0.819625, mean_q: 4.730821
 95420/100000: episode: 1850, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 211.880, mean reward: 2.119 [1.485, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.176, 10.098], loss: 0.381384, mae: 0.499345, mean_q: 4.410815
 95520/100000: episode: 1851, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.908, mean reward: 1.939 [1.488, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.357, 10.098], loss: 0.291386, mae: 0.474764, mean_q: 4.384422
 95620/100000: episode: 1852, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 236.281, mean reward: 2.363 [1.474, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.193, 10.391], loss: 152.204010, mae: 1.003946, mean_q: 4.512655
 95720/100000: episode: 1853, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 191.215, mean reward: 1.912 [1.444, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.603, 10.158], loss: 0.448918, mae: 0.520718, mean_q: 4.273203
 95820/100000: episode: 1854, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 204.985, mean reward: 2.050 [1.553, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.623, 10.098], loss: 0.233309, mae: 0.448600, mean_q: 4.223059
 95920/100000: episode: 1855, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 186.635, mean reward: 1.866 [1.471, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.992, 10.434], loss: 0.286223, mae: 0.410927, mean_q: 4.150996
 96020/100000: episode: 1856, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 195.424, mean reward: 1.954 [1.474, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.005, 10.210], loss: 301.576935, mae: 1.297737, mean_q: 4.494135
 96120/100000: episode: 1857, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 196.407, mean reward: 1.964 [1.490, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.262], loss: 596.042786, mae: 2.605864, mean_q: 5.026648
 96220/100000: episode: 1858, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.500, mean reward: 1.895 [1.475, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.068, 10.098], loss: 2.178979, mae: 1.136721, mean_q: 4.497592
 96320/100000: episode: 1859, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 188.950, mean reward: 1.890 [1.478, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.098], loss: 150.253952, mae: 1.506828, mean_q: 4.685100
 96420/100000: episode: 1860, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 179.460, mean reward: 1.795 [1.455, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.714, 10.134], loss: 0.458181, mae: 0.616988, mean_q: 4.131487
 96520/100000: episode: 1861, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.686, mean reward: 1.877 [1.474, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.059, 10.098], loss: 0.309167, mae: 0.523512, mean_q: 4.077309
 96620/100000: episode: 1862, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 197.349, mean reward: 1.973 [1.489, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.764, 10.311], loss: 0.237374, mae: 0.461148, mean_q: 3.980581
 96720/100000: episode: 1863, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 207.174, mean reward: 2.072 [1.467, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.422, 10.098], loss: 0.167892, mae: 0.410306, mean_q: 3.910439
 96820/100000: episode: 1864, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 185.475, mean reward: 1.855 [1.467, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.098], loss: 0.181522, mae: 0.408914, mean_q: 3.879752
 96920/100000: episode: 1865, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 186.425, mean reward: 1.864 [1.453, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.183, 10.101], loss: 0.168946, mae: 0.394934, mean_q: 3.916721
 97020/100000: episode: 1866, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 199.640, mean reward: 1.996 [1.475, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.563, 10.388], loss: 0.155924, mae: 0.383566, mean_q: 3.890023
 97120/100000: episode: 1867, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 196.282, mean reward: 1.963 [1.460, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.841, 10.098], loss: 0.143793, mae: 0.369636, mean_q: 3.920124
 97220/100000: episode: 1868, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 199.591, mean reward: 1.996 [1.447, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.342, 10.098], loss: 0.149705, mae: 0.376059, mean_q: 3.923256
 97320/100000: episode: 1869, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.929, mean reward: 1.899 [1.490, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.279, 10.181], loss: 0.120047, mae: 0.349303, mean_q: 3.891597
 97420/100000: episode: 1870, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.718, mean reward: 1.937 [1.442, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.555, 10.146], loss: 0.124112, mae: 0.352274, mean_q: 3.886181
 97520/100000: episode: 1871, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 212.617, mean reward: 2.126 [1.467, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-3.216, 10.112], loss: 0.125547, mae: 0.358733, mean_q: 3.918239
 97620/100000: episode: 1872, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 194.595, mean reward: 1.946 [1.437, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.615, 10.368], loss: 0.121908, mae: 0.357959, mean_q: 3.918505
 97720/100000: episode: 1873, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 199.033, mean reward: 1.990 [1.441, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.086, 10.361], loss: 0.129137, mae: 0.352483, mean_q: 3.907273
 97820/100000: episode: 1874, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 190.237, mean reward: 1.902 [1.449, 8.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.651, 10.098], loss: 0.115406, mae: 0.346599, mean_q: 3.903060
 97920/100000: episode: 1875, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 197.741, mean reward: 1.977 [1.444, 6.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.113, 10.192], loss: 0.126062, mae: 0.352389, mean_q: 3.885287
 98020/100000: episode: 1876, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 207.340, mean reward: 2.073 [1.435, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.226, 10.098], loss: 0.116543, mae: 0.336236, mean_q: 3.887894
 98120/100000: episode: 1877, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 188.633, mean reward: 1.886 [1.471, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.903, 10.211], loss: 0.117485, mae: 0.335146, mean_q: 3.882123
 98220/100000: episode: 1878, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 183.182, mean reward: 1.832 [1.453, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.098], loss: 0.106537, mae: 0.334825, mean_q: 3.865049
 98320/100000: episode: 1879, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 190.045, mean reward: 1.900 [1.455, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.480, 10.128], loss: 0.117517, mae: 0.338417, mean_q: 3.864825
 98420/100000: episode: 1880, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 189.485, mean reward: 1.895 [1.490, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.745, 10.098], loss: 0.107069, mae: 0.325811, mean_q: 3.880721
 98520/100000: episode: 1881, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 182.428, mean reward: 1.824 [1.455, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.218], loss: 0.113376, mae: 0.333629, mean_q: 3.870244
 98620/100000: episode: 1882, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 188.381, mean reward: 1.884 [1.494, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.098], loss: 0.121633, mae: 0.328774, mean_q: 3.858899
 98720/100000: episode: 1883, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.779, mean reward: 1.888 [1.442, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.279], loss: 0.117424, mae: 0.322961, mean_q: 3.848182
 98820/100000: episode: 1884, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 191.348, mean reward: 1.913 [1.459, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.816, 10.098], loss: 0.117943, mae: 0.330611, mean_q: 3.883297
 98920/100000: episode: 1885, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 191.615, mean reward: 1.916 [1.475, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.705, 10.230], loss: 0.099094, mae: 0.315392, mean_q: 3.860063
 99020/100000: episode: 1886, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 205.427, mean reward: 2.054 [1.484, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.695, 10.240], loss: 0.100326, mae: 0.321363, mean_q: 3.893664
 99120/100000: episode: 1887, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 198.651, mean reward: 1.987 [1.436, 5.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.947, 10.163], loss: 0.104331, mae: 0.314265, mean_q: 3.868094
 99220/100000: episode: 1888, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.176, mean reward: 1.862 [1.458, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.749, 10.167], loss: 0.104285, mae: 0.321087, mean_q: 3.863271
 99320/100000: episode: 1889, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 196.255, mean reward: 1.963 [1.454, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.555, 10.230], loss: 0.107734, mae: 0.324609, mean_q: 3.875762
 99420/100000: episode: 1890, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 185.443, mean reward: 1.854 [1.456, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.760, 10.321], loss: 0.113381, mae: 0.324108, mean_q: 3.895975
 99520/100000: episode: 1891, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 182.933, mean reward: 1.829 [1.438, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.497, 10.098], loss: 0.110582, mae: 0.326835, mean_q: 3.882964
 99620/100000: episode: 1892, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 191.921, mean reward: 1.919 [1.473, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.003, 10.395], loss: 0.102440, mae: 0.316809, mean_q: 3.871429
 99720/100000: episode: 1893, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 189.229, mean reward: 1.892 [1.435, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.580, 10.130], loss: 0.096145, mae: 0.311858, mean_q: 3.870022
 99820/100000: episode: 1894, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 221.249, mean reward: 2.212 [1.454, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.649, 10.232], loss: 0.106485, mae: 0.305224, mean_q: 3.845171
 99920/100000: episode: 1895, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 191.263, mean reward: 1.913 [1.476, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.466, 10.197], loss: 0.093765, mae: 0.309176, mean_q: 3.871763
done, took 670.209 seconds
[Info] End Importance Splitting. Falsification occurred 13 times.
