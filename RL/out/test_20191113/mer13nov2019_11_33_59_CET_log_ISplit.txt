Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.191s, episode steps: 100, steps per second: 523, episode reward: 190.032, mean reward: 1.900 [1.459, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.587, 10.167], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1575, episode reward: 184.721, mean reward: 1.847 [1.451, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1585, episode reward: 186.694, mean reward: 1.867 [1.465, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.653, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 185.633, mean reward: 1.856 [1.461, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.469, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 183.607, mean reward: 1.836 [1.464, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.132], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 188.107, mean reward: 1.881 [1.452, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.067, 10.321], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.068s, episode steps: 100, steps per second: 1480, episode reward: 187.123, mean reward: 1.871 [1.479, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 182.325, mean reward: 1.823 [1.464, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.111], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1637, episode reward: 187.954, mean reward: 1.880 [1.440, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 194.864, mean reward: 1.949 [1.524, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.763, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 189.234, mean reward: 1.892 [1.459, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.955, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.066s, episode steps: 100, steps per second: 1519, episode reward: 192.290, mean reward: 1.923 [1.474, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.364, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 195.423, mean reward: 1.954 [1.435, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.232, 10.127], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 189.168, mean reward: 1.892 [1.469, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.061s, episode steps: 100, steps per second: 1636, episode reward: 309.637, mean reward: 3.096 [1.541, 11.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.526, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 185.005, mean reward: 1.850 [1.454, 4.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.412, 10.131], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.075s, episode steps: 100, steps per second: 1334, episode reward: 176.760, mean reward: 1.768 [1.464, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.004, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.086s, episode steps: 100, steps per second: 1166, episode reward: 197.078, mean reward: 1.971 [1.449, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.825, 10.329], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.061s, episode steps: 100, steps per second: 1638, episode reward: 183.649, mean reward: 1.836 [1.462, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.940, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.061s, episode steps: 100, steps per second: 1639, episode reward: 200.465, mean reward: 2.005 [1.476, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.066s, episode steps: 100, steps per second: 1515, episode reward: 213.653, mean reward: 2.137 [1.518, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.567, 10.256], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.061s, episode steps: 100, steps per second: 1649, episode reward: 189.889, mean reward: 1.899 [1.482, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.855, 10.253], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.067s, episode steps: 100, steps per second: 1493, episode reward: 196.702, mean reward: 1.967 [1.490, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.060s, episode steps: 100, steps per second: 1664, episode reward: 180.979, mean reward: 1.810 [1.454, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.139], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.066s, episode steps: 100, steps per second: 1517, episode reward: 230.692, mean reward: 2.307 [1.458, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.707, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 188.865, mean reward: 1.889 [1.463, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.912, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.094s, episode steps: 100, steps per second: 1066, episode reward: 186.763, mean reward: 1.868 [1.488, 2.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.464, 10.331], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 181.844, mean reward: 1.818 [1.462, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.622, 10.100], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: 189.847, mean reward: 1.898 [1.549, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.912, 10.322], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.078s, episode steps: 100, steps per second: 1289, episode reward: 188.728, mean reward: 1.887 [1.503, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.133, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.061s, episode steps: 100, steps per second: 1639, episode reward: 201.120, mean reward: 2.011 [1.481, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.792, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 180.627, mean reward: 1.806 [1.440, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.088, 10.231], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.069s, episode steps: 100, steps per second: 1452, episode reward: 186.372, mean reward: 1.864 [1.468, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.709, 10.282], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.059s, episode steps: 100, steps per second: 1685, episode reward: 180.552, mean reward: 1.806 [1.482, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.775, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.066s, episode steps: 100, steps per second: 1504, episode reward: 187.560, mean reward: 1.876 [1.466, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.068s, episode steps: 100, steps per second: 1462, episode reward: 185.635, mean reward: 1.856 [1.453, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.398, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 188.724, mean reward: 1.887 [1.434, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.152], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.061s, episode steps: 100, steps per second: 1653, episode reward: 182.292, mean reward: 1.823 [1.502, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.489, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 181.590, mean reward: 1.816 [1.458, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.990, 10.236], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 184.889, mean reward: 1.849 [1.435, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.804, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.082s, episode steps: 100, steps per second: 1226, episode reward: 197.928, mean reward: 1.979 [1.505, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.542, 10.266], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 198.780, mean reward: 1.988 [1.455, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.714, 10.150], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.070s, episode steps: 100, steps per second: 1437, episode reward: 208.155, mean reward: 2.082 [1.484, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.176, 10.418], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.068s, episode steps: 100, steps per second: 1468, episode reward: 190.606, mean reward: 1.906 [1.453, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.135, 10.230], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.071s, episode steps: 100, steps per second: 1402, episode reward: 185.025, mean reward: 1.850 [1.450, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.251], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 207.640, mean reward: 2.076 [1.491, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.089, 10.254], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 184.669, mean reward: 1.847 [1.472, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.757, 10.224], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 184.137, mean reward: 1.841 [1.454, 2.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 209.146, mean reward: 2.091 [1.445, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.483, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.069s, episode steps: 100, steps per second: 1457, episode reward: 198.219, mean reward: 1.982 [1.497, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.152, 10.176], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 197.416, mean reward: 1.974 [1.461, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.098], loss: 0.224638, mae: 0.458373, mean_q: 2.246328
  5200/100000: episode: 52, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 189.488, mean reward: 1.895 [1.482, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.875, 10.357], loss: 0.178477, mae: 0.336913, mean_q: 2.934382
  5300/100000: episode: 53, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 179.955, mean reward: 1.800 [1.452, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.170, 10.098], loss: 0.161462, mae: 0.328605, mean_q: 3.252368
  5400/100000: episode: 54, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 180.150, mean reward: 1.802 [1.441, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.596, 10.122], loss: 0.163490, mae: 0.331684, mean_q: 3.472314
  5500/100000: episode: 55, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 187.119, mean reward: 1.871 [1.462, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.674, 10.188], loss: 0.126629, mae: 0.303914, mean_q: 3.584102
  5600/100000: episode: 56, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 202.260, mean reward: 2.023 [1.440, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.900, 10.196], loss: 0.156084, mae: 0.318884, mean_q: 3.686009
  5700/100000: episode: 57, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.109, mean reward: 1.921 [1.458, 7.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.439, 10.226], loss: 0.160472, mae: 0.333023, mean_q: 3.749033
  5800/100000: episode: 58, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.996, mean reward: 1.930 [1.473, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.499, 10.098], loss: 0.106171, mae: 0.300184, mean_q: 3.758078
  5900/100000: episode: 59, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 199.773, mean reward: 1.998 [1.458, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.709, 10.098], loss: 0.141117, mae: 0.313621, mean_q: 3.798091
  6000/100000: episode: 60, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 181.184, mean reward: 1.812 [1.475, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.197], loss: 0.138969, mae: 0.308510, mean_q: 3.812980
  6100/100000: episode: 61, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 182.715, mean reward: 1.827 [1.444, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.830, 10.098], loss: 0.150894, mae: 0.319908, mean_q: 3.826784
  6200/100000: episode: 62, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 196.175, mean reward: 1.962 [1.499, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.153, 10.098], loss: 0.164182, mae: 0.334393, mean_q: 3.850462
  6300/100000: episode: 63, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 182.524, mean reward: 1.825 [1.495, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.864, 10.098], loss: 0.189555, mae: 0.337413, mean_q: 3.861514
  6400/100000: episode: 64, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 225.914, mean reward: 2.259 [1.494, 5.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.368, 10.544], loss: 0.114377, mae: 0.300393, mean_q: 3.822458
  6500/100000: episode: 65, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 190.756, mean reward: 1.908 [1.452, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.122], loss: 0.090844, mae: 0.294638, mean_q: 3.813835
  6600/100000: episode: 66, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 214.324, mean reward: 2.143 [1.457, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.476, 10.228], loss: 0.103219, mae: 0.301369, mean_q: 3.809309
  6700/100000: episode: 67, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 184.777, mean reward: 1.848 [1.445, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.792, 10.182], loss: 0.125497, mae: 0.321502, mean_q: 3.819472
  6800/100000: episode: 68, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 183.943, mean reward: 1.839 [1.452, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.768, 10.114], loss: 0.096637, mae: 0.299083, mean_q: 3.800287
  6900/100000: episode: 69, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 211.057, mean reward: 2.111 [1.472, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.587, 10.119], loss: 0.093043, mae: 0.298958, mean_q: 3.811705
  7000/100000: episode: 70, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 178.515, mean reward: 1.785 [1.462, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.169], loss: 0.114802, mae: 0.316208, mean_q: 3.826112
  7100/100000: episode: 71, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 200.001, mean reward: 2.000 [1.448, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.849, 10.206], loss: 0.095066, mae: 0.298948, mean_q: 3.808521
  7200/100000: episode: 72, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.473, mean reward: 1.885 [1.468, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.500, 10.267], loss: 0.089135, mae: 0.291144, mean_q: 3.801099
  7300/100000: episode: 73, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 184.429, mean reward: 1.844 [1.436, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.344, 10.098], loss: 0.110231, mae: 0.310190, mean_q: 3.806867
  7400/100000: episode: 74, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 196.861, mean reward: 1.969 [1.470, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.855, 10.098], loss: 0.104549, mae: 0.305966, mean_q: 3.808733
  7500/100000: episode: 75, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 199.097, mean reward: 1.991 [1.478, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.883, 10.187], loss: 0.104850, mae: 0.304179, mean_q: 3.789992
  7600/100000: episode: 76, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 188.399, mean reward: 1.884 [1.443, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.550, 10.098], loss: 0.096669, mae: 0.300803, mean_q: 3.796569
  7700/100000: episode: 77, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 175.327, mean reward: 1.753 [1.488, 2.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.449, 10.098], loss: 0.087851, mae: 0.282982, mean_q: 3.788656
  7800/100000: episode: 78, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 198.738, mean reward: 1.987 [1.443, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.367], loss: 0.103097, mae: 0.299552, mean_q: 3.797760
  7900/100000: episode: 79, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 195.665, mean reward: 1.957 [1.442, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.542, 10.112], loss: 0.096123, mae: 0.295771, mean_q: 3.793712
  8000/100000: episode: 80, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.097, mean reward: 1.841 [1.443, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.166], loss: 0.086185, mae: 0.283103, mean_q: 3.791704
  8100/100000: episode: 81, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 203.886, mean reward: 2.039 [1.475, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.832, 10.098], loss: 0.102971, mae: 0.299420, mean_q: 3.798668
  8200/100000: episode: 82, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 183.709, mean reward: 1.837 [1.458, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.098], loss: 0.110896, mae: 0.299640, mean_q: 3.787521
  8300/100000: episode: 83, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.502, mean reward: 1.965 [1.473, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.062, 10.385], loss: 0.099769, mae: 0.299356, mean_q: 3.797666
  8400/100000: episode: 84, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.122, mean reward: 1.961 [1.520, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.449, 10.098], loss: 0.089625, mae: 0.293702, mean_q: 3.793231
  8500/100000: episode: 85, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.984, mean reward: 1.820 [1.459, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.941, 10.111], loss: 0.109524, mae: 0.307625, mean_q: 3.792418
  8600/100000: episode: 86, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 199.864, mean reward: 1.999 [1.470, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.603, 10.120], loss: 0.100782, mae: 0.305943, mean_q: 3.819414
  8700/100000: episode: 87, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.365, mean reward: 1.934 [1.461, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.250], loss: 0.089892, mae: 0.295737, mean_q: 3.800325
  8800/100000: episode: 88, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.737, mean reward: 1.857 [1.500, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.696, 10.098], loss: 0.090705, mae: 0.295047, mean_q: 3.812131
  8900/100000: episode: 89, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 195.638, mean reward: 1.956 [1.444, 7.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.173], loss: 0.096964, mae: 0.297541, mean_q: 3.813437
  9000/100000: episode: 90, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 188.607, mean reward: 1.886 [1.482, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.508, 10.120], loss: 0.111767, mae: 0.317017, mean_q: 3.836682
  9100/100000: episode: 91, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 209.440, mean reward: 2.094 [1.499, 5.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.320, 10.098], loss: 0.106629, mae: 0.305225, mean_q: 3.825131
  9200/100000: episode: 92, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 210.390, mean reward: 2.104 [1.455, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.934, 10.098], loss: 0.103935, mae: 0.301279, mean_q: 3.834620
  9300/100000: episode: 93, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 211.256, mean reward: 2.113 [1.474, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.166, 10.362], loss: 0.093784, mae: 0.299239, mean_q: 3.834328
  9400/100000: episode: 94, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 192.967, mean reward: 1.930 [1.455, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.726, 10.263], loss: 0.096038, mae: 0.302620, mean_q: 3.827827
  9500/100000: episode: 95, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 194.081, mean reward: 1.941 [1.463, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.723, 10.214], loss: 0.099551, mae: 0.302110, mean_q: 3.809662
  9600/100000: episode: 96, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 190.963, mean reward: 1.910 [1.451, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.349, 10.115], loss: 0.106977, mae: 0.314069, mean_q: 3.824420
  9700/100000: episode: 97, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 189.387, mean reward: 1.894 [1.473, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.042, 10.098], loss: 0.090706, mae: 0.294274, mean_q: 3.818295
  9800/100000: episode: 98, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 173.466, mean reward: 1.735 [1.444, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.845, 10.176], loss: 0.103769, mae: 0.304091, mean_q: 3.805277
  9900/100000: episode: 99, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 186.141, mean reward: 1.861 [1.488, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.799, 10.098], loss: 0.125421, mae: 0.326990, mean_q: 3.824590
 10000/100000: episode: 100, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 213.856, mean reward: 2.139 [1.523, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.864, 10.533], loss: 0.093019, mae: 0.289097, mean_q: 3.804542
 10100/100000: episode: 101, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 194.866, mean reward: 1.949 [1.436, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.523, 10.152], loss: 0.098659, mae: 0.305979, mean_q: 3.823083
 10200/100000: episode: 102, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 188.717, mean reward: 1.887 [1.470, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.791, 10.098], loss: 0.098837, mae: 0.298591, mean_q: 3.812563
 10300/100000: episode: 103, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 231.538, mean reward: 2.315 [1.512, 14.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.545, 10.131], loss: 0.095397, mae: 0.305771, mean_q: 3.826420
 10400/100000: episode: 104, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 175.828, mean reward: 1.758 [1.456, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.654, 10.098], loss: 0.092722, mae: 0.295511, mean_q: 3.821845
 10500/100000: episode: 105, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 188.406, mean reward: 1.884 [1.487, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.628, 10.211], loss: 0.195477, mae: 0.341336, mean_q: 3.865736
 10600/100000: episode: 106, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 200.725, mean reward: 2.007 [1.467, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.309], loss: 0.106988, mae: 0.317803, mean_q: 3.846013
 10700/100000: episode: 107, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 199.307, mean reward: 1.993 [1.452, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.883, 10.098], loss: 0.089535, mae: 0.297128, mean_q: 3.845286
 10800/100000: episode: 108, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.343, mean reward: 1.883 [1.472, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.089, 10.119], loss: 0.092132, mae: 0.305501, mean_q: 3.840750
 10900/100000: episode: 109, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 179.619, mean reward: 1.796 [1.443, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.157, 10.098], loss: 0.093609, mae: 0.301789, mean_q: 3.837639
 11000/100000: episode: 110, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 188.392, mean reward: 1.884 [1.527, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.401, 10.098], loss: 0.105299, mae: 0.306243, mean_q: 3.826793
 11100/100000: episode: 111, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 196.291, mean reward: 1.963 [1.452, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.200], loss: 0.146888, mae: 0.314946, mean_q: 3.848886
 11200/100000: episode: 112, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 211.855, mean reward: 2.119 [1.464, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.424, 10.098], loss: 0.104489, mae: 0.309896, mean_q: 3.837120
 11300/100000: episode: 113, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 202.855, mean reward: 2.029 [1.460, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.189, 10.343], loss: 0.121355, mae: 0.319653, mean_q: 3.854012
 11400/100000: episode: 114, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 196.218, mean reward: 1.962 [1.504, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.278, 10.098], loss: 0.092383, mae: 0.306750, mean_q: 3.845841
 11500/100000: episode: 115, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 221.137, mean reward: 2.211 [1.457, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.166, 10.255], loss: 0.119053, mae: 0.315951, mean_q: 3.859749
 11600/100000: episode: 116, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.763, mean reward: 2.028 [1.474, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.288, 10.270], loss: 0.121653, mae: 0.310169, mean_q: 3.856855
 11700/100000: episode: 117, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 193.749, mean reward: 1.937 [1.460, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.536, 10.098], loss: 0.147756, mae: 0.319009, mean_q: 3.877501
 11800/100000: episode: 118, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.925, mean reward: 1.939 [1.492, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.951, 10.323], loss: 0.097626, mae: 0.299916, mean_q: 3.839609
 11900/100000: episode: 119, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.971, mean reward: 1.940 [1.482, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.285, 10.099], loss: 0.127032, mae: 0.316589, mean_q: 3.849416
 12000/100000: episode: 120, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.517, mean reward: 1.835 [1.481, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.710, 10.190], loss: 0.118588, mae: 0.311595, mean_q: 3.858994
 12100/100000: episode: 121, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 186.407, mean reward: 1.864 [1.489, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.542, 10.098], loss: 0.085318, mae: 0.292994, mean_q: 3.834392
 12200/100000: episode: 122, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 186.782, mean reward: 1.868 [1.463, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.269, 10.098], loss: 0.111547, mae: 0.328412, mean_q: 3.872061
 12300/100000: episode: 123, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 201.852, mean reward: 2.019 [1.462, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.279], loss: 0.114418, mae: 0.301915, mean_q: 3.837804
 12400/100000: episode: 124, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 206.647, mean reward: 2.066 [1.454, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.392, 10.098], loss: 0.095202, mae: 0.305112, mean_q: 3.850209
 12500/100000: episode: 125, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 219.957, mean reward: 2.200 [1.478, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.290], loss: 0.091165, mae: 0.304986, mean_q: 3.841151
 12600/100000: episode: 126, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 181.951, mean reward: 1.820 [1.454, 2.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.215, 10.228], loss: 0.086470, mae: 0.305068, mean_q: 3.861359
 12700/100000: episode: 127, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 188.686, mean reward: 1.887 [1.453, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.232, 10.098], loss: 0.105799, mae: 0.326055, mean_q: 3.864911
 12800/100000: episode: 128, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.956, mean reward: 1.830 [1.469, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.826, 10.135], loss: 0.125373, mae: 0.319678, mean_q: 3.862978
 12900/100000: episode: 129, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 199.206, mean reward: 1.992 [1.463, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.834, 10.240], loss: 0.095426, mae: 0.310728, mean_q: 3.855416
 13000/100000: episode: 130, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 194.196, mean reward: 1.942 [1.505, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.161, 10.155], loss: 0.100178, mae: 0.310485, mean_q: 3.863021
 13100/100000: episode: 131, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.774, mean reward: 1.908 [1.453, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.644, 10.182], loss: 0.135833, mae: 0.329603, mean_q: 3.870546
 13200/100000: episode: 132, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 189.450, mean reward: 1.895 [1.461, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.776, 10.098], loss: 0.102023, mae: 0.318848, mean_q: 3.885636
 13300/100000: episode: 133, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 211.198, mean reward: 2.112 [1.435, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.378, 10.415], loss: 0.090598, mae: 0.302698, mean_q: 3.867770
 13400/100000: episode: 134, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 213.522, mean reward: 2.135 [1.451, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.748, 10.098], loss: 0.135193, mae: 0.335978, mean_q: 3.886229
 13500/100000: episode: 135, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.772, mean reward: 1.998 [1.490, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.098], loss: 0.133911, mae: 0.324899, mean_q: 3.883256
 13600/100000: episode: 136, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 182.790, mean reward: 1.828 [1.439, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.101194, mae: 0.322445, mean_q: 3.888108
 13700/100000: episode: 137, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 201.312, mean reward: 2.013 [1.433, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.009, 10.098], loss: 0.085726, mae: 0.301487, mean_q: 3.857478
 13800/100000: episode: 138, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 176.055, mean reward: 1.761 [1.439, 2.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.175, 10.196], loss: 0.090071, mae: 0.307561, mean_q: 3.861220
 13900/100000: episode: 139, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 199.788, mean reward: 1.998 [1.445, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.826, 10.098], loss: 0.088922, mae: 0.306660, mean_q: 3.874743
 14000/100000: episode: 140, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 174.706, mean reward: 1.747 [1.432, 2.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.795, 10.189], loss: 0.145074, mae: 0.328762, mean_q: 3.892591
 14100/100000: episode: 141, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.182, mean reward: 1.822 [1.449, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.178, 10.101], loss: 0.111734, mae: 0.310348, mean_q: 3.887872
 14200/100000: episode: 142, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 185.272, mean reward: 1.853 [1.480, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.385, 10.098], loss: 0.136378, mae: 0.316278, mean_q: 3.883078
 14300/100000: episode: 143, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 210.912, mean reward: 2.109 [1.574, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.525, 10.103], loss: 0.105589, mae: 0.299656, mean_q: 3.849403
 14400/100000: episode: 144, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 184.204, mean reward: 1.842 [1.451, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.708, 10.223], loss: 0.115382, mae: 0.313283, mean_q: 3.852411
 14500/100000: episode: 145, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.433, mean reward: 1.924 [1.462, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.098], loss: 0.110263, mae: 0.307838, mean_q: 3.862194
 14600/100000: episode: 146, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 187.088, mean reward: 1.871 [1.467, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.176, 10.154], loss: 0.092127, mae: 0.309671, mean_q: 3.863313
 14700/100000: episode: 147, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 185.293, mean reward: 1.853 [1.467, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.542, 10.098], loss: 0.099523, mae: 0.315751, mean_q: 3.864400
 14800/100000: episode: 148, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 178.559, mean reward: 1.786 [1.444, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.188, 10.111], loss: 0.133637, mae: 0.312915, mean_q: 3.853285
 14900/100000: episode: 149, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 217.495, mean reward: 2.175 [1.529, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.695, 10.396], loss: 0.107628, mae: 0.304101, mean_q: 3.844894
[Info] 1-TH LEVEL FOUND: 4.472513198852539, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.962s, episode steps: 100, steps per second: 20, episode reward: 190.144, mean reward: 1.901 [1.459, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.814, 10.168], loss: 0.087732, mae: 0.307345, mean_q: 3.854815
 15051/100000: episode: 151, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 126.975, mean reward: 2.490 [1.831, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.341, 10.247], loss: 0.089798, mae: 0.307125, mean_q: 3.860519
 15089/100000: episode: 152, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 149.556, mean reward: 3.936 [2.229, 6.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.581, 10.456], loss: 0.156735, mae: 0.331119, mean_q: 3.867634
 15141/100000: episode: 153, duration: 0.285s, episode steps: 52, steps per second: 182, episode reward: 102.683, mean reward: 1.975 [1.456, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.359, 10.149], loss: 0.099897, mae: 0.316609, mean_q: 3.869008
 15241/100000: episode: 154, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.482, mean reward: 1.995 [1.454, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.510, 10.214], loss: 0.179933, mae: 0.343261, mean_q: 3.898889
 15279/100000: episode: 155, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 129.442, mean reward: 3.406 [2.381, 5.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.584, 10.368], loss: 0.116081, mae: 0.338497, mean_q: 3.920756
 15328/100000: episode: 156, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 107.410, mean reward: 2.192 [1.521, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.722, 10.100], loss: 0.112059, mae: 0.326778, mean_q: 3.904687
 15380/100000: episode: 157, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 134.751, mean reward: 2.591 [1.554, 6.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.351, 10.300], loss: 0.112024, mae: 0.321686, mean_q: 3.899544
 15418/100000: episode: 158, duration: 0.224s, episode steps: 38, steps per second: 170, episode reward: 119.945, mean reward: 3.156 [2.205, 6.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.971, 10.367], loss: 0.095794, mae: 0.321266, mean_q: 3.885475
 15470/100000: episode: 159, duration: 0.303s, episode steps: 52, steps per second: 172, episode reward: 283.355, mean reward: 5.449 [2.350, 17.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.712, 10.561], loss: 0.114575, mae: 0.345853, mean_q: 3.943362
 15519/100000: episode: 160, duration: 0.314s, episode steps: 49, steps per second: 156, episode reward: 117.844, mean reward: 2.405 [1.783, 11.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.322, 10.333], loss: 0.192058, mae: 0.375403, mean_q: 3.973500
 15557/100000: episode: 161, duration: 0.185s, episode steps: 38, steps per second: 206, episode reward: 84.728, mean reward: 2.230 [1.462, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.713, 10.248], loss: 0.237820, mae: 0.388429, mean_q: 3.991998
 15595/100000: episode: 162, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 97.824, mean reward: 2.574 [1.536, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.107, 10.171], loss: 0.123532, mae: 0.341301, mean_q: 3.968587
 15633/100000: episode: 163, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 99.569, mean reward: 2.620 [1.506, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.498, 10.155], loss: 0.162938, mae: 0.353115, mean_q: 4.000226
 15671/100000: episode: 164, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 149.685, mean reward: 3.939 [2.340, 9.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.644, 10.405], loss: 0.175384, mae: 0.385332, mean_q: 4.029569
 15709/100000: episode: 165, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 97.169, mean reward: 2.557 [1.581, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.206, 10.329], loss: 0.323623, mae: 0.430020, mean_q: 4.091920
 15747/100000: episode: 166, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 85.679, mean reward: 2.255 [1.502, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.869, 10.174], loss: 0.174019, mae: 0.368298, mean_q: 4.053374
 15845/100000: episode: 167, duration: 0.519s, episode steps: 98, steps per second: 189, episode reward: 187.773, mean reward: 1.916 [1.471, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.607, 10.100], loss: 0.291456, mae: 0.404669, mean_q: 4.094258
 15897/100000: episode: 168, duration: 0.276s, episode steps: 52, steps per second: 188, episode reward: 112.760, mean reward: 2.168 [1.465, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.945, 10.214], loss: 0.232840, mae: 0.386461, mean_q: 4.104087
 15949/100000: episode: 169, duration: 0.279s, episode steps: 52, steps per second: 187, episode reward: 136.258, mean reward: 2.620 [1.519, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.549, 10.113], loss: 0.229424, mae: 0.397612, mean_q: 4.082931
 16049/100000: episode: 170, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 211.895, mean reward: 2.119 [1.461, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.604, 10.319], loss: 0.173183, mae: 0.376689, mean_q: 4.097860
 16101/100000: episode: 171, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 113.758, mean reward: 2.188 [1.462, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.639, 10.210], loss: 0.274679, mae: 0.404017, mean_q: 4.103310
 16153/100000: episode: 172, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 108.084, mean reward: 2.079 [1.509, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.630, 10.100], loss: 0.264391, mae: 0.401458, mean_q: 4.149528
 16251/100000: episode: 173, duration: 0.529s, episode steps: 98, steps per second: 185, episode reward: 183.732, mean reward: 1.875 [1.453, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.443, 10.303], loss: 0.254864, mae: 0.394853, mean_q: 4.142193
 16302/100000: episode: 174, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 140.945, mean reward: 2.764 [1.677, 5.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.474, 10.279], loss: 0.138073, mae: 0.355946, mean_q: 4.095956
 16400/100000: episode: 175, duration: 0.515s, episode steps: 98, steps per second: 190, episode reward: 194.993, mean reward: 1.990 [1.469, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.482, 10.427], loss: 0.191400, mae: 0.394041, mean_q: 4.131827
 16451/100000: episode: 176, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 130.449, mean reward: 2.558 [1.690, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.976, 10.308], loss: 0.223836, mae: 0.384709, mean_q: 4.131073
 16503/100000: episode: 177, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 188.947, mean reward: 3.634 [1.824, 14.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.783, 10.284], loss: 0.211153, mae: 0.394002, mean_q: 4.141407
 16601/100000: episode: 178, duration: 0.495s, episode steps: 98, steps per second: 198, episode reward: 188.717, mean reward: 1.926 [1.475, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.735, 10.100], loss: 0.231726, mae: 0.383738, mean_q: 4.161765
 16652/100000: episode: 179, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 123.431, mean reward: 2.420 [1.531, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-1.234, 10.100], loss: 0.247301, mae: 0.387996, mean_q: 4.148540
 16703/100000: episode: 180, duration: 0.301s, episode steps: 51, steps per second: 170, episode reward: 107.229, mean reward: 2.103 [1.553, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.414, 10.100], loss: 0.320424, mae: 0.392682, mean_q: 4.164146
 16755/100000: episode: 181, duration: 0.325s, episode steps: 52, steps per second: 160, episode reward: 107.459, mean reward: 2.067 [1.497, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.467, 10.100], loss: 0.189198, mae: 0.388964, mean_q: 4.179053
 16793/100000: episode: 182, duration: 0.225s, episode steps: 38, steps per second: 169, episode reward: 88.905, mean reward: 2.340 [1.823, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.280, 10.296], loss: 0.128424, mae: 0.348189, mean_q: 4.090771
 16845/100000: episode: 183, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 103.289, mean reward: 1.986 [1.473, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.336, 10.100], loss: 0.313025, mae: 0.419927, mean_q: 4.234140
 16896/100000: episode: 184, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 108.682, mean reward: 2.131 [1.498, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.291, 10.100], loss: 0.268654, mae: 0.419853, mean_q: 4.190166
 16994/100000: episode: 185, duration: 0.606s, episode steps: 98, steps per second: 162, episode reward: 182.614, mean reward: 1.863 [1.480, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.434, 10.100], loss: 0.256864, mae: 0.400112, mean_q: 4.188402
 17045/100000: episode: 186, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 107.549, mean reward: 2.109 [1.520, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.395, 10.209], loss: 0.174454, mae: 0.364600, mean_q: 4.210062
 17097/100000: episode: 187, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 107.778, mean reward: 2.073 [1.447, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.078, 10.119], loss: 0.152187, mae: 0.374820, mean_q: 4.185409
 17197/100000: episode: 188, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.870, mean reward: 1.859 [1.449, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.726, 10.213], loss: 0.190027, mae: 0.371667, mean_q: 4.183324
 17249/100000: episode: 189, duration: 0.300s, episode steps: 52, steps per second: 173, episode reward: 133.031, mean reward: 2.558 [1.686, 5.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.268, 10.307], loss: 0.258561, mae: 0.417230, mean_q: 4.186376
 17298/100000: episode: 190, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 104.030, mean reward: 2.123 [1.692, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.094, 10.243], loss: 0.337064, mae: 0.404155, mean_q: 4.230122
 17347/100000: episode: 191, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 126.935, mean reward: 2.591 [1.853, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.542, 10.368], loss: 0.248239, mae: 0.411736, mean_q: 4.193582
 17398/100000: episode: 192, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 132.248, mean reward: 2.593 [1.979, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.714, 10.397], loss: 0.168301, mae: 0.360354, mean_q: 4.151767
 17436/100000: episode: 193, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 128.038, mean reward: 3.369 [2.249, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.317, 10.504], loss: 0.213908, mae: 0.390191, mean_q: 4.223258
 17488/100000: episode: 194, duration: 0.309s, episode steps: 52, steps per second: 168, episode reward: 100.916, mean reward: 1.941 [1.473, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.337, 10.100], loss: 0.200771, mae: 0.377907, mean_q: 4.224762
 17588/100000: episode: 195, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 200.248, mean reward: 2.002 [1.448, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.982, 10.194], loss: 0.192789, mae: 0.377416, mean_q: 4.240207
 17639/100000: episode: 196, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 103.098, mean reward: 2.022 [1.510, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.283, 10.199], loss: 0.227951, mae: 0.380735, mean_q: 4.250029
 17691/100000: episode: 197, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 106.339, mean reward: 2.045 [1.519, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-1.464, 10.120], loss: 0.215822, mae: 0.369499, mean_q: 4.180037
 17743/100000: episode: 198, duration: 0.265s, episode steps: 52, steps per second: 197, episode reward: 137.201, mean reward: 2.638 [1.817, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.312, 10.400], loss: 0.264117, mae: 0.373661, mean_q: 4.272750
 17795/100000: episode: 199, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 141.918, mean reward: 2.729 [1.902, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.889, 10.298], loss: 0.243772, mae: 0.391492, mean_q: 4.293401
 17893/100000: episode: 200, duration: 0.525s, episode steps: 98, steps per second: 187, episode reward: 189.783, mean reward: 1.937 [1.446, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.793, 10.336], loss: 0.287654, mae: 0.413818, mean_q: 4.355518
 17945/100000: episode: 201, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 122.134, mean reward: 2.349 [1.593, 5.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.699, 10.146], loss: 0.161124, mae: 0.371494, mean_q: 4.270159
 17996/100000: episode: 202, duration: 0.284s, episode steps: 51, steps per second: 180, episode reward: 112.564, mean reward: 2.207 [1.527, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.738, 10.109], loss: 0.224295, mae: 0.412770, mean_q: 4.324035
 18047/100000: episode: 203, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 99.962, mean reward: 1.960 [1.591, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.564, 10.100], loss: 0.200805, mae: 0.397256, mean_q: 4.286181
 18098/100000: episode: 204, duration: 0.272s, episode steps: 51, steps per second: 187, episode reward: 113.207, mean reward: 2.220 [1.508, 6.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.217, 10.178], loss: 0.159949, mae: 0.369857, mean_q: 4.227601
 18198/100000: episode: 205, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 193.017, mean reward: 1.930 [1.443, 6.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-1.153, 10.106], loss: 0.234592, mae: 0.422137, mean_q: 4.337045
 18250/100000: episode: 206, duration: 0.279s, episode steps: 52, steps per second: 187, episode reward: 101.834, mean reward: 1.958 [1.566, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.397, 10.256], loss: 0.203460, mae: 0.392307, mean_q: 4.294302
 18301/100000: episode: 207, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 120.009, mean reward: 2.353 [1.575, 6.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.921, 10.310], loss: 0.229422, mae: 0.378397, mean_q: 4.286568
 18352/100000: episode: 208, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 114.149, mean reward: 2.238 [1.470, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.883, 10.367], loss: 0.241402, mae: 0.410337, mean_q: 4.305573
 18403/100000: episode: 209, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 112.617, mean reward: 2.208 [1.498, 9.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.578, 10.204], loss: 0.250071, mae: 0.413802, mean_q: 4.284214
 18454/100000: episode: 210, duration: 0.284s, episode steps: 51, steps per second: 180, episode reward: 97.146, mean reward: 1.905 [1.433, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.281, 10.228], loss: 0.254694, mae: 0.397986, mean_q: 4.349055
 18503/100000: episode: 211, duration: 0.256s, episode steps: 49, steps per second: 192, episode reward: 118.139, mean reward: 2.411 [1.525, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.333, 10.184], loss: 0.308499, mae: 0.410921, mean_q: 4.277675
 18555/100000: episode: 212, duration: 0.280s, episode steps: 52, steps per second: 186, episode reward: 101.132, mean reward: 1.945 [1.463, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.381, 10.211], loss: 0.195604, mae: 0.372448, mean_q: 4.287713
 18607/100000: episode: 213, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 104.519, mean reward: 2.010 [1.463, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.715, 10.155], loss: 0.250182, mae: 0.396860, mean_q: 4.369220
 18645/100000: episode: 214, duration: 0.197s, episode steps: 38, steps per second: 192, episode reward: 134.581, mean reward: 3.542 [1.964, 7.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.551, 10.264], loss: 0.253635, mae: 0.395417, mean_q: 4.313352
 18683/100000: episode: 215, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 108.684, mean reward: 2.860 [2.117, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.968, 10.415], loss: 0.194352, mae: 0.415404, mean_q: 4.367740
 18735/100000: episode: 216, duration: 0.268s, episode steps: 52, steps per second: 194, episode reward: 96.595, mean reward: 1.858 [1.487, 2.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.951, 10.165], loss: 0.209165, mae: 0.404328, mean_q: 4.337163
 18784/100000: episode: 217, duration: 0.260s, episode steps: 49, steps per second: 188, episode reward: 164.485, mean reward: 3.357 [2.019, 7.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.329, 10.375], loss: 0.206052, mae: 0.374719, mean_q: 4.352846
 18835/100000: episode: 218, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 132.543, mean reward: 2.599 [1.863, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.569, 10.302], loss: 0.259505, mae: 0.423807, mean_q: 4.419504
 18884/100000: episode: 219, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 140.958, mean reward: 2.877 [1.910, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.697, 10.414], loss: 0.175294, mae: 0.400059, mean_q: 4.482060
 18935/100000: episode: 220, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 103.966, mean reward: 2.039 [1.462, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.241, 10.116], loss: 0.173675, mae: 0.382436, mean_q: 4.416107
 18984/100000: episode: 221, duration: 0.260s, episode steps: 49, steps per second: 189, episode reward: 117.213, mean reward: 2.392 [1.616, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.991, 10.246], loss: 0.224256, mae: 0.394352, mean_q: 4.379309
 19036/100000: episode: 222, duration: 0.269s, episode steps: 52, steps per second: 194, episode reward: 170.764, mean reward: 3.284 [2.049, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.351, 10.647], loss: 0.367701, mae: 0.458007, mean_q: 4.528655
 19088/100000: episode: 223, duration: 0.263s, episode steps: 52, steps per second: 197, episode reward: 132.706, mean reward: 2.552 [1.853, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.356, 10.366], loss: 0.240582, mae: 0.404137, mean_q: 4.457062
 19139/100000: episode: 224, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 108.891, mean reward: 2.135 [1.519, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.881, 10.113], loss: 0.231761, mae: 0.428461, mean_q: 4.471644
 19190/100000: episode: 225, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 102.119, mean reward: 2.002 [1.525, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.565, 10.100], loss: 0.219198, mae: 0.427444, mean_q: 4.471832
 19242/100000: episode: 226, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 118.364, mean reward: 2.276 [1.492, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.318, 10.135], loss: 0.303351, mae: 0.425379, mean_q: 4.524652
 19294/100000: episode: 227, duration: 0.276s, episode steps: 52, steps per second: 188, episode reward: 118.457, mean reward: 2.278 [1.589, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.561, 10.329], loss: 0.253581, mae: 0.429430, mean_q: 4.472532
 19345/100000: episode: 228, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 103.132, mean reward: 2.022 [1.539, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.762, 10.224], loss: 0.288476, mae: 0.430301, mean_q: 4.502746
 19394/100000: episode: 229, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 121.140, mean reward: 2.472 [1.525, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.639, 10.279], loss: 0.273915, mae: 0.426596, mean_q: 4.468111
 19494/100000: episode: 230, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.850, mean reward: 1.859 [1.491, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.818, 10.100], loss: 0.200490, mae: 0.409796, mean_q: 4.526426
 19545/100000: episode: 231, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 117.057, mean reward: 2.295 [1.721, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.764, 10.289], loss: 0.301118, mae: 0.430876, mean_q: 4.545784
 19596/100000: episode: 232, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 110.895, mean reward: 2.174 [1.465, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.024, 10.130], loss: 0.260762, mae: 0.448689, mean_q: 4.544084
 19647/100000: episode: 233, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 107.101, mean reward: 2.100 [1.461, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.301, 10.154], loss: 0.291325, mae: 0.430165, mean_q: 4.555771
 19747/100000: episode: 234, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.256, mean reward: 1.843 [1.451, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.618, 10.129], loss: 0.228914, mae: 0.401921, mean_q: 4.513858
 19799/100000: episode: 235, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 108.043, mean reward: 2.078 [1.524, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.753, 10.233], loss: 0.221054, mae: 0.405477, mean_q: 4.474826
 19850/100000: episode: 236, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 118.148, mean reward: 2.317 [1.529, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.057, 10.138], loss: 0.392368, mae: 0.466213, mean_q: 4.555401
 19902/100000: episode: 237, duration: 0.266s, episode steps: 52, steps per second: 196, episode reward: 124.679, mean reward: 2.398 [1.775, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.308, 10.487], loss: 0.303055, mae: 0.435629, mean_q: 4.590355
 19954/100000: episode: 238, duration: 0.295s, episode steps: 52, steps per second: 176, episode reward: 133.669, mean reward: 2.571 [2.019, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.312, 10.405], loss: 0.156892, mae: 0.373152, mean_q: 4.519313
 20006/100000: episode: 239, duration: 0.303s, episode steps: 52, steps per second: 172, episode reward: 116.639, mean reward: 2.243 [1.638, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.989, 10.348], loss: 0.169647, mae: 0.391164, mean_q: 4.503981
[Info] 2-TH LEVEL FOUND: 8.40226936340332, Considering 10/90 traces
 20057/100000: episode: 240, duration: 4.479s, episode steps: 51, steps per second: 11, episode reward: 107.492, mean reward: 2.108 [1.512, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.285, 10.271], loss: 0.308417, mae: 0.473149, mean_q: 4.516966
 20088/100000: episode: 241, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 113.576, mean reward: 3.664 [1.637, 12.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.628, 10.320], loss: 0.309129, mae: 0.428214, mean_q: 4.485606
 20115/100000: episode: 242, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 90.921, mean reward: 3.367 [1.751, 8.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.576, 10.330], loss: 0.267450, mae: 0.407337, mean_q: 4.486581
 20149/100000: episode: 243, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 91.398, mean reward: 2.688 [2.027, 4.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.700, 10.453], loss: 0.308275, mae: 0.445710, mean_q: 4.524348
 20181/100000: episode: 244, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 79.904, mean reward: 2.497 [1.821, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.529, 10.186], loss: 0.224370, mae: 0.399904, mean_q: 4.492001
 20212/100000: episode: 245, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 137.957, mean reward: 4.450 [2.888, 8.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.029, 10.669], loss: 0.202827, mae: 0.376772, mean_q: 4.511651
 20246/100000: episode: 246, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 92.753, mean reward: 2.728 [1.898, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.722, 10.335], loss: 0.168570, mae: 0.372837, mean_q: 4.572785
 20273/100000: episode: 247, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 91.404, mean reward: 3.385 [1.926, 5.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.339, 10.339], loss: 0.216953, mae: 0.409601, mean_q: 4.602552
 20306/100000: episode: 248, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 101.181, mean reward: 3.066 [1.999, 6.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.899, 10.381], loss: 0.302869, mae: 0.396296, mean_q: 4.522150
 20338/100000: episode: 249, duration: 0.190s, episode steps: 32, steps per second: 168, episode reward: 74.814, mean reward: 2.338 [1.522, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.202, 10.150], loss: 0.289708, mae: 0.408352, mean_q: 4.611061
 20370/100000: episode: 250, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 84.996, mean reward: 2.656 [1.547, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.631, 10.179], loss: 0.323691, mae: 0.415946, mean_q: 4.543255
 20399/100000: episode: 251, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 100.651, mean reward: 3.471 [2.494, 8.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.728, 10.450], loss: 0.191115, mae: 0.391844, mean_q: 4.571656
 20428/100000: episode: 252, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 134.776, mean reward: 4.647 [3.326, 7.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.735, 10.614], loss: 0.184621, mae: 0.399339, mean_q: 4.593450
 20462/100000: episode: 253, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 89.618, mean reward: 2.636 [1.862, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.219, 10.308], loss: 0.213674, mae: 0.397258, mean_q: 4.526188
 20493/100000: episode: 254, duration: 0.164s, episode steps: 31, steps per second: 188, episode reward: 174.544, mean reward: 5.630 [2.403, 16.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.481], loss: 0.243230, mae: 0.429802, mean_q: 4.640079
 20524/100000: episode: 255, duration: 0.145s, episode steps: 31, steps per second: 213, episode reward: 96.481, mean reward: 3.112 [1.950, 5.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.091, 10.327], loss: 0.201122, mae: 0.410800, mean_q: 4.620894
 20555/100000: episode: 256, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 98.715, mean reward: 3.184 [1.599, 5.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.534, 10.199], loss: 0.196934, mae: 0.409150, mean_q: 4.633075
 20589/100000: episode: 257, duration: 0.171s, episode steps: 34, steps per second: 198, episode reward: 95.651, mean reward: 2.813 [1.951, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.440], loss: 0.234006, mae: 0.429068, mean_q: 4.642832
 20623/100000: episode: 258, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 95.689, mean reward: 2.814 [1.693, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.135, 10.292], loss: 0.297319, mae: 0.424189, mean_q: 4.580361
 20661/100000: episode: 259, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 117.205, mean reward: 3.084 [1.686, 9.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.091, 10.295], loss: 0.244142, mae: 0.427737, mean_q: 4.568280
 20699/100000: episode: 260, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 145.446, mean reward: 3.828 [2.482, 5.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.527, 10.464], loss: 0.237633, mae: 0.412014, mean_q: 4.663742
 20733/100000: episode: 261, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 105.016, mean reward: 3.089 [1.698, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.147, 10.298], loss: 0.164902, mae: 0.388482, mean_q: 4.601009
 20767/100000: episode: 262, duration: 0.196s, episode steps: 34, steps per second: 174, episode reward: 96.014, mean reward: 2.824 [2.272, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.941, 10.353], loss: 0.424708, mae: 0.472737, mean_q: 4.656459
 20805/100000: episode: 263, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 221.459, mean reward: 5.828 [2.927, 15.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.372, 10.497], loss: 0.431057, mae: 0.508636, mean_q: 4.722198
 20834/100000: episode: 264, duration: 0.243s, episode steps: 29, steps per second: 119, episode reward: 96.730, mean reward: 3.336 [2.360, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.433, 10.507], loss: 0.180612, mae: 0.413899, mean_q: 4.688839
 20868/100000: episode: 265, duration: 0.251s, episode steps: 34, steps per second: 135, episode reward: 96.724, mean reward: 2.845 [1.745, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.271, 10.316], loss: 0.188416, mae: 0.412936, mean_q: 4.702911
 20899/100000: episode: 266, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 112.420, mean reward: 3.626 [2.563, 5.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.662, 10.531], loss: 0.193168, mae: 0.397807, mean_q: 4.665219
 20928/100000: episode: 267, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 121.376, mean reward: 4.185 [2.157, 15.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.449, 10.392], loss: 0.233634, mae: 0.430959, mean_q: 4.642558
 20962/100000: episode: 268, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 101.025, mean reward: 2.971 [2.140, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.557, 10.318], loss: 0.313144, mae: 0.470434, mean_q: 4.761834
 20996/100000: episode: 269, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 111.580, mean reward: 3.282 [2.192, 5.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.109, 10.394], loss: 0.285581, mae: 0.479623, mean_q: 4.793658
 21030/100000: episode: 270, duration: 0.193s, episode steps: 34, steps per second: 177, episode reward: 78.621, mean reward: 2.312 [1.529, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.014, 10.100], loss: 0.204251, mae: 0.391701, mean_q: 4.664356
 21064/100000: episode: 271, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 130.453, mean reward: 3.837 [2.207, 6.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.643], loss: 0.279270, mae: 0.456961, mean_q: 4.834200
 21095/100000: episode: 272, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 99.017, mean reward: 3.194 [2.050, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.903, 10.394], loss: 0.247399, mae: 0.444756, mean_q: 4.838476
 21128/100000: episode: 273, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 155.939, mean reward: 4.725 [2.197, 17.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.368, 10.440], loss: 0.337532, mae: 0.476171, mean_q: 4.829290
 21159/100000: episode: 274, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 135.186, mean reward: 4.361 [2.614, 8.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.304, 10.461], loss: 0.284271, mae: 0.450440, mean_q: 4.803576
 21193/100000: episode: 275, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 118.256, mean reward: 3.478 [2.136, 6.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.484, 10.322], loss: 0.190864, mae: 0.398613, mean_q: 4.782883
 21231/100000: episode: 276, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 234.287, mean reward: 6.165 [1.937, 12.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.354, 10.437], loss: 0.337287, mae: 0.481687, mean_q: 4.852872
 21260/100000: episode: 277, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 138.617, mean reward: 4.780 [1.932, 20.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.203, 10.308], loss: 0.478109, mae: 0.558834, mean_q: 4.991696
 21294/100000: episode: 278, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 105.137, mean reward: 3.092 [1.950, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.360], loss: 0.299011, mae: 0.475994, mean_q: 4.908913
 21328/100000: episode: 279, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 74.460, mean reward: 2.190 [1.540, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.116, 10.193], loss: 0.300940, mae: 0.478325, mean_q: 4.946321
 21357/100000: episode: 280, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 90.661, mean reward: 3.126 [2.489, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.043, 10.425], loss: 0.567056, mae: 0.561494, mean_q: 5.064260
 21386/100000: episode: 281, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 96.238, mean reward: 3.319 [2.099, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.147, 10.366], loss: 0.401685, mae: 0.537285, mean_q: 4.864815
 21410/100000: episode: 282, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 149.817, mean reward: 6.242 [3.112, 14.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.027, 10.473], loss: 0.618490, mae: 0.581599, mean_q: 4.986190
 21439/100000: episode: 283, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 92.372, mean reward: 3.185 [1.808, 7.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.814, 10.297], loss: 0.551201, mae: 0.560080, mean_q: 5.053031
 21470/100000: episode: 284, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 102.048, mean reward: 3.292 [2.195, 5.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.291, 10.405], loss: 0.654422, mae: 0.574141, mean_q: 5.122874
 21504/100000: episode: 285, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 132.829, mean reward: 3.907 [2.423, 7.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.123, 10.590], loss: 0.313858, mae: 0.462498, mean_q: 4.928180
 21536/100000: episode: 286, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 88.858, mean reward: 2.777 [1.937, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.676, 10.246], loss: 0.553150, mae: 0.545184, mean_q: 5.145323
 21565/100000: episode: 287, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 107.202, mean reward: 3.697 [2.033, 8.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.405, 10.375], loss: 0.378619, mae: 0.497980, mean_q: 5.087464
 21597/100000: episode: 288, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 104.157, mean reward: 3.255 [1.667, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.199, 10.306], loss: 0.438189, mae: 0.522863, mean_q: 5.222367
 21631/100000: episode: 289, duration: 0.192s, episode steps: 34, steps per second: 178, episode reward: 166.631, mean reward: 4.901 [3.043, 11.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.460, 10.558], loss: 0.470435, mae: 0.529835, mean_q: 5.035513
 21655/100000: episode: 290, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 80.572, mean reward: 3.357 [2.485, 8.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.301, 10.462], loss: 0.482414, mae: 0.561118, mean_q: 5.260260
 21679/100000: episode: 291, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 63.903, mean reward: 2.663 [1.642, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.265], loss: 0.335623, mae: 0.520904, mean_q: 5.154451
 21710/100000: episode: 292, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 173.386, mean reward: 5.593 [3.236, 7.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.440], loss: 0.467080, mae: 0.577831, mean_q: 5.154222
 21739/100000: episode: 293, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 73.130, mean reward: 2.522 [1.442, 5.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.874, 10.121], loss: 0.566078, mae: 0.625843, mean_q: 5.240663
 21773/100000: episode: 294, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 112.495, mean reward: 3.309 [2.129, 5.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.434], loss: 0.626866, mae: 0.597024, mean_q: 5.185414
 21807/100000: episode: 295, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 87.780, mean reward: 2.582 [1.911, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.436, 10.294], loss: 0.346366, mae: 0.515934, mean_q: 5.150014
[Info] FALSIFICATION!
 21819/100000: episode: 296, duration: 0.425s, episode steps: 12, steps per second: 28, episode reward: 1066.506, mean reward: 88.876 [3.862, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.021, 9.857], loss: 0.769091, mae: 0.592907, mean_q: 5.413091
 21851/100000: episode: 297, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 115.936, mean reward: 3.623 [2.738, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.493], loss: 0.451980, mae: 0.529621, mean_q: 5.253685
 21883/100000: episode: 298, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 86.398, mean reward: 2.700 [1.713, 5.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.299], loss: 0.399133, mae: 0.526217, mean_q: 5.252975
 21916/100000: episode: 299, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 97.386, mean reward: 2.951 [1.590, 5.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.260], loss: 0.430973, mae: 0.543768, mean_q: 5.254550
 21950/100000: episode: 300, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 110.735, mean reward: 3.257 [1.517, 10.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.676, 10.277], loss: 0.229140, mae: 0.461432, mean_q: 5.264744
 21984/100000: episode: 301, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 89.532, mean reward: 2.633 [1.665, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.230], loss: 0.404721, mae: 0.533628, mean_q: 5.353760
 22013/100000: episode: 302, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 101.887, mean reward: 3.513 [2.383, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.701, 10.447], loss: 0.389542, mae: 0.540826, mean_q: 5.259411
 22037/100000: episode: 303, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 59.031, mean reward: 2.460 [1.895, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.136, 10.379], loss: 643.687256, mae: 4.081543, mean_q: 7.695602
 22066/100000: episode: 304, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 111.569, mean reward: 3.847 [3.062, 5.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.040, 10.479], loss: 2.290532, mae: 1.578339, mean_q: 5.038428
 22095/100000: episode: 305, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 83.862, mean reward: 2.892 [1.865, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.293, 10.384], loss: 0.650174, mae: 0.762741, mean_q: 5.731250
 22122/100000: episode: 306, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 124.318, mean reward: 4.604 [2.403, 6.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.567, 10.475], loss: 0.523439, mae: 0.647934, mean_q: 5.643029
 22146/100000: episode: 307, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 62.629, mean reward: 2.610 [1.963, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.190, 10.378], loss: 0.455204, mae: 0.634188, mean_q: 5.584797
 22180/100000: episode: 308, duration: 0.202s, episode steps: 34, steps per second: 168, episode reward: 101.215, mean reward: 2.977 [2.343, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.522, 10.533], loss: 0.542632, mae: 0.611378, mean_q: 5.577270
 22218/100000: episode: 309, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 134.979, mean reward: 3.552 [1.693, 6.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.455, 10.221], loss: 0.494365, mae: 0.591401, mean_q: 5.536215
 22250/100000: episode: 310, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 95.446, mean reward: 2.983 [1.639, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.305], loss: 0.420855, mae: 0.573344, mean_q: 5.415827
 22279/100000: episode: 311, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 67.828, mean reward: 2.339 [1.607, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.206, 10.249], loss: 0.414914, mae: 0.604343, mean_q: 5.483592
 22317/100000: episode: 312, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 134.475, mean reward: 3.539 [2.470, 6.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.938, 10.414], loss: 0.460609, mae: 0.621502, mean_q: 5.529940
 22351/100000: episode: 313, duration: 0.193s, episode steps: 34, steps per second: 177, episode reward: 105.235, mean reward: 3.095 [1.987, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.184, 10.383], loss: 0.489259, mae: 0.602417, mean_q: 5.457774
 22375/100000: episode: 314, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 78.632, mean reward: 3.276 [2.234, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.130, 10.379], loss: 0.395084, mae: 0.606690, mean_q: 5.528427
 22404/100000: episode: 315, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 68.051, mean reward: 2.347 [1.798, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.545, 10.320], loss: 528.978699, mae: 1.968668, mean_q: 5.917394
 22435/100000: episode: 316, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 103.414, mean reward: 3.336 [1.664, 6.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.628, 10.347], loss: 2.537909, mae: 1.652241, mean_q: 5.991747
 22468/100000: episode: 317, duration: 0.196s, episode steps: 33, steps per second: 169, episode reward: 115.180, mean reward: 3.490 [1.961, 7.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.162, 10.427], loss: 465.534302, mae: 2.571950, mean_q: 6.504109
 22497/100000: episode: 318, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 105.064, mean reward: 3.623 [2.478, 6.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.230, 10.474], loss: 0.616193, mae: 0.779846, mean_q: 5.754990
 22526/100000: episode: 319, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 82.585, mean reward: 2.848 [2.184, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.396], loss: 0.709544, mae: 0.675014, mean_q: 5.681628
 22564/100000: episode: 320, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 162.179, mean reward: 4.268 [1.980, 12.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.225, 10.384], loss: 0.484934, mae: 0.647315, mean_q: 5.748622
 22602/100000: episode: 321, duration: 0.213s, episode steps: 38, steps per second: 179, episode reward: 163.546, mean reward: 4.304 [2.525, 9.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.497, 10.506], loss: 0.639039, mae: 0.654583, mean_q: 5.751346
 22634/100000: episode: 322, duration: 0.189s, episode steps: 32, steps per second: 169, episode reward: 114.998, mean reward: 3.594 [2.512, 8.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.879, 10.491], loss: 0.609534, mae: 0.646680, mean_q: 5.749462
 22658/100000: episode: 323, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 59.850, mean reward: 2.494 [1.793, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.055, 10.250], loss: 0.562598, mae: 0.606738, mean_q: 5.595445
 22689/100000: episode: 324, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 131.995, mean reward: 4.258 [1.818, 7.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.067, 10.369], loss: 0.545370, mae: 0.644906, mean_q: 5.783017
 22716/100000: episode: 325, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 81.981, mean reward: 3.036 [1.760, 6.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.237], loss: 567.399170, mae: 2.039535, mean_q: 6.118854
 22754/100000: episode: 326, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 139.880, mean reward: 3.681 [2.423, 7.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.188, 10.369], loss: 2.067761, mae: 1.469359, mean_q: 6.206749
 22792/100000: episode: 327, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 115.585, mean reward: 3.042 [2.116, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.327, 10.457], loss: 0.720774, mae: 0.718055, mean_q: 5.872271
 22823/100000: episode: 328, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 84.113, mean reward: 2.713 [1.882, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.097, 10.376], loss: 0.745688, mae: 0.699837, mean_q: 5.847321
 22856/100000: episode: 329, duration: 0.196s, episode steps: 33, steps per second: 168, episode reward: 85.019, mean reward: 2.576 [2.014, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.101, 10.329], loss: 0.962563, mae: 0.763816, mean_q: 5.861767
[Info] Complete ISplit Iteration
[Info] Levels: [4.472513, 8.402269, 12.826714]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 22890/100000: episode: 330, duration: 4.467s, episode steps: 34, steps per second: 8, episode reward: 169.952, mean reward: 4.999 [2.304, 12.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.818, 10.657], loss: 450.220978, mae: 1.578647, mean_q: 5.870275
 22990/100000: episode: 331, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 224.008, mean reward: 2.240 [1.492, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.772, 10.235], loss: 154.837799, mae: 1.697621, mean_q: 6.482947
 23090/100000: episode: 332, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 185.394, mean reward: 1.854 [1.454, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.461, 10.098], loss: 153.631882, mae: 1.129106, mean_q: 6.169545
 23190/100000: episode: 333, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.462, mean reward: 1.885 [1.447, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.620, 10.135], loss: 306.367340, mae: 1.871019, mean_q: 6.272889
 23290/100000: episode: 334, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 215.677, mean reward: 2.157 [1.527, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.887, 10.105], loss: 153.401428, mae: 1.696791, mean_q: 6.644979
 23390/100000: episode: 335, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 214.768, mean reward: 2.148 [1.499, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.472, 10.098], loss: 0.776635, mae: 0.779546, mean_q: 6.017524
 23490/100000: episode: 336, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 180.044, mean reward: 1.800 [1.461, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.423, 10.309], loss: 153.852814, mae: 1.382095, mean_q: 6.224130
 23590/100000: episode: 337, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.916, mean reward: 1.869 [1.449, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.873, 10.165], loss: 306.499634, mae: 1.980244, mean_q: 6.519808
 23690/100000: episode: 338, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 188.712, mean reward: 1.887 [1.451, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.394, 10.256], loss: 306.123169, mae: 1.931434, mean_q: 6.465649
 23790/100000: episode: 339, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 184.729, mean reward: 1.847 [1.451, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.119, 10.110], loss: 0.857363, mae: 0.852528, mean_q: 5.848266
 23890/100000: episode: 340, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 190.349, mean reward: 1.903 [1.442, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.722, 10.199], loss: 0.688459, mae: 0.731297, mean_q: 5.720897
 23990/100000: episode: 341, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.804, mean reward: 1.908 [1.483, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.907, 10.129], loss: 0.634655, mae: 0.704753, mean_q: 5.674610
 24090/100000: episode: 342, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.980, mean reward: 1.900 [1.450, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.135, 10.098], loss: 153.697830, mae: 1.375671, mean_q: 6.024844
 24190/100000: episode: 343, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.209, mean reward: 1.882 [1.465, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.527, 10.235], loss: 0.652738, mae: 0.687244, mean_q: 5.558604
 24290/100000: episode: 344, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 204.255, mean reward: 2.043 [1.456, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.089, 10.353], loss: 0.616938, mae: 0.689486, mean_q: 5.564547
 24390/100000: episode: 345, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.819, mean reward: 1.928 [1.480, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.206, 10.216], loss: 153.881760, mae: 1.375247, mean_q: 5.878439
 24490/100000: episode: 346, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 191.416, mean reward: 1.914 [1.500, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.731, 10.269], loss: 153.582047, mae: 1.384584, mean_q: 6.018919
 24590/100000: episode: 347, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 179.590, mean reward: 1.796 [1.465, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.695, 10.098], loss: 0.580858, mae: 0.665113, mean_q: 5.578336
 24690/100000: episode: 348, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.443, mean reward: 1.884 [1.463, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.200], loss: 0.595792, mae: 0.683666, mean_q: 5.592444
 24790/100000: episode: 349, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 203.798, mean reward: 2.038 [1.438, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.409, 10.331], loss: 153.605835, mae: 1.306626, mean_q: 5.891133
 24890/100000: episode: 350, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 201.978, mean reward: 2.020 [1.454, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.609, 10.098], loss: 153.671997, mae: 1.419607, mean_q: 6.107982
 24990/100000: episode: 351, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 182.904, mean reward: 1.829 [1.485, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.802, 10.162], loss: 0.669140, mae: 0.715433, mean_q: 5.630701
 25090/100000: episode: 352, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.889, mean reward: 1.879 [1.439, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.498, 10.098], loss: 153.462311, mae: 1.236005, mean_q: 5.844192
 25190/100000: episode: 353, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.578, mean reward: 1.916 [1.469, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.098], loss: 0.783730, mae: 0.787735, mean_q: 5.456378
 25290/100000: episode: 354, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 190.300, mean reward: 1.903 [1.472, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.515, 10.230], loss: 0.646634, mae: 0.645931, mean_q: 5.396935
 25390/100000: episode: 355, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.559, mean reward: 1.816 [1.485, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.945, 10.098], loss: 153.541199, mae: 1.235505, mean_q: 5.698540
 25490/100000: episode: 356, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 184.744, mean reward: 1.847 [1.449, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.713, 10.098], loss: 153.300903, mae: 1.334026, mean_q: 5.722934
 25590/100000: episode: 357, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.018, mean reward: 1.920 [1.468, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.245], loss: 0.633361, mae: 0.657775, mean_q: 5.283843
 25690/100000: episode: 358, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 196.051, mean reward: 1.961 [1.457, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.098], loss: 1.091581, mae: 0.875724, mean_q: 5.495091
 25790/100000: episode: 359, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 206.438, mean reward: 2.064 [1.477, 8.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.197, 10.371], loss: 0.502901, mae: 0.572307, mean_q: 5.091190
 25890/100000: episode: 360, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 185.520, mean reward: 1.855 [1.464, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.969, 10.111], loss: 0.630843, mae: 0.601807, mean_q: 5.039936
 25990/100000: episode: 361, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.410, mean reward: 1.864 [1.467, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.820, 10.098], loss: 0.508853, mae: 0.546956, mean_q: 4.923032
 26090/100000: episode: 362, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 184.119, mean reward: 1.841 [1.462, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.634, 10.098], loss: 153.488556, mae: 1.184994, mean_q: 5.174272
 26190/100000: episode: 363, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 180.570, mean reward: 1.806 [1.474, 2.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.525, 10.098], loss: 152.908051, mae: 1.141556, mean_q: 5.123535
 26290/100000: episode: 364, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 271.075, mean reward: 2.711 [1.506, 6.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.768, 10.608], loss: 153.068497, mae: 1.172274, mean_q: 5.136054
 26390/100000: episode: 365, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 184.569, mean reward: 1.846 [1.447, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.116, 10.128], loss: 302.368835, mae: 1.179562, mean_q: 4.848872
 26490/100000: episode: 366, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.252, mean reward: 1.843 [1.461, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.413, 10.098], loss: 154.731262, mae: 1.622637, mean_q: 5.553397
 26590/100000: episode: 367, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.900, mean reward: 1.909 [1.485, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.098], loss: 0.461124, mae: 0.572631, mean_q: 4.684091
 26690/100000: episode: 368, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 186.255, mean reward: 1.863 [1.472, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.136], loss: 152.088608, mae: 0.820845, mean_q: 4.590379
 26790/100000: episode: 369, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 180.657, mean reward: 1.807 [1.476, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.215, 10.098], loss: 1.149767, mae: 0.802843, mean_q: 4.832241
 26890/100000: episode: 370, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 188.903, mean reward: 1.889 [1.439, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.426, 10.189], loss: 0.333582, mae: 0.486112, mean_q: 4.425327
 26990/100000: episode: 371, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.381, mean reward: 2.084 [1.512, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.000, 10.355], loss: 0.295860, mae: 0.447928, mean_q: 4.316520
 27090/100000: episode: 372, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.785, mean reward: 1.898 [1.453, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.447, 10.297], loss: 0.265340, mae: 0.417506, mean_q: 4.274329
 27190/100000: episode: 373, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 189.739, mean reward: 1.897 [1.437, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.098], loss: 0.229130, mae: 0.405453, mean_q: 4.194450
 27290/100000: episode: 374, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 206.185, mean reward: 2.062 [1.486, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.071, 10.098], loss: 0.214745, mae: 0.400377, mean_q: 4.183733
 27390/100000: episode: 375, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 182.940, mean reward: 1.829 [1.454, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.236, 10.185], loss: 0.201267, mae: 0.377339, mean_q: 4.142950
 27490/100000: episode: 376, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.944, mean reward: 1.919 [1.445, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.624, 10.110], loss: 0.188901, mae: 0.371926, mean_q: 4.073408
 27590/100000: episode: 377, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 180.144, mean reward: 1.801 [1.473, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.157], loss: 0.169611, mae: 0.349357, mean_q: 4.002733
 27690/100000: episode: 378, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.405, mean reward: 1.994 [1.476, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.497, 10.098], loss: 0.144590, mae: 0.340535, mean_q: 3.950482
 27790/100000: episode: 379, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 182.130, mean reward: 1.821 [1.465, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.251, 10.191], loss: 0.125099, mae: 0.316746, mean_q: 3.895516
 27890/100000: episode: 380, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.925, mean reward: 1.879 [1.443, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.197, 10.098], loss: 0.121872, mae: 0.316327, mean_q: 3.856296
 27990/100000: episode: 381, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.359, mean reward: 1.974 [1.477, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.603, 10.098], loss: 0.097635, mae: 0.296844, mean_q: 3.827071
 28090/100000: episode: 382, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 219.031, mean reward: 2.190 [1.456, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.328, 10.267], loss: 0.117149, mae: 0.311126, mean_q: 3.838195
 28190/100000: episode: 383, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 197.032, mean reward: 1.970 [1.485, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.652, 10.145], loss: 0.107491, mae: 0.304626, mean_q: 3.838386
 28290/100000: episode: 384, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 188.058, mean reward: 1.881 [1.465, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.828, 10.136], loss: 0.098771, mae: 0.291499, mean_q: 3.828396
 28390/100000: episode: 385, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 184.178, mean reward: 1.842 [1.438, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.245], loss: 0.105787, mae: 0.292318, mean_q: 3.796121
 28490/100000: episode: 386, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 187.380, mean reward: 1.874 [1.448, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.210], loss: 0.094036, mae: 0.290792, mean_q: 3.808102
 28590/100000: episode: 387, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 209.215, mean reward: 2.092 [1.487, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.801, 10.098], loss: 0.108783, mae: 0.304783, mean_q: 3.818702
 28690/100000: episode: 388, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.373, mean reward: 1.974 [1.454, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.270, 10.102], loss: 0.087439, mae: 0.285731, mean_q: 3.797286
 28790/100000: episode: 389, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 200.423, mean reward: 2.004 [1.503, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.098], loss: 0.104994, mae: 0.301216, mean_q: 3.809142
 28890/100000: episode: 390, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.219, mean reward: 1.922 [1.432, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.498, 10.211], loss: 0.104872, mae: 0.306525, mean_q: 3.831296
 28990/100000: episode: 391, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 218.626, mean reward: 2.186 [1.467, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.060, 10.098], loss: 0.095979, mae: 0.294898, mean_q: 3.815927
 29090/100000: episode: 392, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.210, mean reward: 1.772 [1.472, 2.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.841, 10.228], loss: 0.098955, mae: 0.300702, mean_q: 3.820355
 29190/100000: episode: 393, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 179.788, mean reward: 1.798 [1.449, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.771, 10.098], loss: 0.110411, mae: 0.302679, mean_q: 3.826498
 29290/100000: episode: 394, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 176.775, mean reward: 1.768 [1.436, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.244, 10.098], loss: 0.104848, mae: 0.295530, mean_q: 3.828469
 29390/100000: episode: 395, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 180.529, mean reward: 1.805 [1.442, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.149, 10.098], loss: 0.110859, mae: 0.303181, mean_q: 3.833534
 29490/100000: episode: 396, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 182.633, mean reward: 1.826 [1.450, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.600, 10.277], loss: 0.092946, mae: 0.291303, mean_q: 3.807263
 29590/100000: episode: 397, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 203.168, mean reward: 2.032 [1.466, 6.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.858, 10.371], loss: 0.092439, mae: 0.292337, mean_q: 3.812206
 29690/100000: episode: 398, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.169, mean reward: 1.882 [1.438, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.225, 10.349], loss: 0.088202, mae: 0.279010, mean_q: 3.795384
 29790/100000: episode: 399, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.013, mean reward: 1.960 [1.485, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.059, 10.098], loss: 0.118042, mae: 0.301606, mean_q: 3.816719
 29890/100000: episode: 400, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 202.019, mean reward: 2.020 [1.481, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.056, 10.198], loss: 0.096618, mae: 0.291051, mean_q: 3.803202
 29990/100000: episode: 401, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 203.774, mean reward: 2.038 [1.445, 5.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.227, 10.395], loss: 0.090836, mae: 0.290713, mean_q: 3.814853
 30090/100000: episode: 402, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.482, mean reward: 1.855 [1.463, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.322, 10.282], loss: 0.087297, mae: 0.291995, mean_q: 3.817128
 30190/100000: episode: 403, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 222.943, mean reward: 2.229 [1.579, 4.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.861, 10.098], loss: 0.101455, mae: 0.295127, mean_q: 3.829962
 30290/100000: episode: 404, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 202.973, mean reward: 2.030 [1.457, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.705, 10.168], loss: 0.120608, mae: 0.319322, mean_q: 3.838972
 30390/100000: episode: 405, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.013, mean reward: 1.960 [1.457, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.336], loss: 0.117033, mae: 0.305362, mean_q: 3.837969
 30490/100000: episode: 406, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.122, mean reward: 1.961 [1.443, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.813, 10.098], loss: 0.090607, mae: 0.294246, mean_q: 3.827376
 30590/100000: episode: 407, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 203.922, mean reward: 2.039 [1.475, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.211, 10.098], loss: 0.098638, mae: 0.307667, mean_q: 3.843860
 30690/100000: episode: 408, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.427, mean reward: 1.844 [1.491, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.279], loss: 0.106129, mae: 0.301763, mean_q: 3.852499
 30790/100000: episode: 409, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.519, mean reward: 1.855 [1.441, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.013, 10.221], loss: 0.103112, mae: 0.306872, mean_q: 3.854461
 30890/100000: episode: 410, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 180.564, mean reward: 1.806 [1.472, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.286, 10.175], loss: 0.092591, mae: 0.293686, mean_q: 3.834007
 30990/100000: episode: 411, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 191.553, mean reward: 1.916 [1.451, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.839, 10.169], loss: 0.094543, mae: 0.292587, mean_q: 3.842468
 31090/100000: episode: 412, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 187.622, mean reward: 1.876 [1.444, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.585, 10.098], loss: 0.092768, mae: 0.295616, mean_q: 3.830889
 31190/100000: episode: 413, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 196.611, mean reward: 1.966 [1.454, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.106, 10.286], loss: 0.096182, mae: 0.288432, mean_q: 3.826175
 31290/100000: episode: 414, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 196.384, mean reward: 1.964 [1.502, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.642, 10.196], loss: 0.081885, mae: 0.281393, mean_q: 3.820091
 31390/100000: episode: 415, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.751, mean reward: 1.888 [1.468, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.295], loss: 0.082781, mae: 0.278817, mean_q: 3.815320
 31490/100000: episode: 416, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 203.541, mean reward: 2.035 [1.464, 6.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.603, 10.229], loss: 0.091176, mae: 0.290348, mean_q: 3.809288
 31590/100000: episode: 417, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 203.904, mean reward: 2.039 [1.436, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.175, 10.324], loss: 0.083800, mae: 0.286374, mean_q: 3.824295
 31690/100000: episode: 418, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.635, mean reward: 1.956 [1.483, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.456, 10.098], loss: 0.098717, mae: 0.298377, mean_q: 3.841762
 31790/100000: episode: 419, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 178.997, mean reward: 1.790 [1.462, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.160, 10.273], loss: 0.091202, mae: 0.294823, mean_q: 3.836096
 31890/100000: episode: 420, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 185.634, mean reward: 1.856 [1.476, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.520, 10.098], loss: 0.091814, mae: 0.294879, mean_q: 3.834356
 31990/100000: episode: 421, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 199.376, mean reward: 1.994 [1.449, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.378, 10.098], loss: 0.096034, mae: 0.296902, mean_q: 3.841131
 32090/100000: episode: 422, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 188.310, mean reward: 1.883 [1.496, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.958, 10.205], loss: 0.091296, mae: 0.289042, mean_q: 3.837712
 32190/100000: episode: 423, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 177.945, mean reward: 1.779 [1.432, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.902, 10.153], loss: 0.084304, mae: 0.284640, mean_q: 3.833121
 32290/100000: episode: 424, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 182.092, mean reward: 1.821 [1.454, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.482, 10.175], loss: 0.102262, mae: 0.298967, mean_q: 3.843086
 32390/100000: episode: 425, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 190.543, mean reward: 1.905 [1.454, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.473, 10.171], loss: 0.100828, mae: 0.298170, mean_q: 3.829148
 32490/100000: episode: 426, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 188.486, mean reward: 1.885 [1.450, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.627, 10.098], loss: 0.096321, mae: 0.295632, mean_q: 3.819423
 32590/100000: episode: 427, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 206.647, mean reward: 2.066 [1.441, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.369, 10.115], loss: 0.086471, mae: 0.283284, mean_q: 3.810152
 32690/100000: episode: 428, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 189.015, mean reward: 1.890 [1.442, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.148], loss: 0.083558, mae: 0.283001, mean_q: 3.791207
 32790/100000: episode: 429, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 179.128, mean reward: 1.791 [1.471, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.068, 10.098], loss: 0.089942, mae: 0.290253, mean_q: 3.812699
[Info] 1-TH LEVEL FOUND: 4.784271240234375, Considering 10/90 traces
 32890/100000: episode: 430, duration: 4.591s, episode steps: 100, steps per second: 22, episode reward: 197.469, mean reward: 1.975 [1.476, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.512, 10.098], loss: 0.085016, mae: 0.282204, mean_q: 3.813252
 32923/100000: episode: 431, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 116.794, mean reward: 3.539 [2.364, 8.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.167, 10.100], loss: 0.073589, mae: 0.271878, mean_q: 3.818895
 32951/100000: episode: 432, duration: 0.164s, episode steps: 28, steps per second: 170, episode reward: 73.917, mean reward: 2.640 [1.830, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.876, 10.434], loss: 0.089590, mae: 0.285068, mean_q: 3.821292
 32963/100000: episode: 433, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 32.056, mean reward: 2.671 [2.239, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.167, 10.469], loss: 0.075829, mae: 0.271085, mean_q: 3.810183
 32976/100000: episode: 434, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 28.805, mean reward: 2.216 [1.876, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.380], loss: 0.114013, mae: 0.317779, mean_q: 3.826525
 32993/100000: episode: 435, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 34.847, mean reward: 2.050 [1.635, 2.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.225], loss: 0.094486, mae: 0.290730, mean_q: 3.816500
 33048/100000: episode: 436, duration: 0.305s, episode steps: 55, steps per second: 180, episode reward: 173.297, mean reward: 3.151 [1.793, 7.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.406, 10.267], loss: 0.108225, mae: 0.312615, mean_q: 3.834713
 33103/100000: episode: 437, duration: 0.301s, episode steps: 55, steps per second: 183, episode reward: 125.515, mean reward: 2.282 [1.526, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.713, 10.155], loss: 0.094835, mae: 0.299854, mean_q: 3.856572
 33141/100000: episode: 438, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 113.567, mean reward: 2.989 [2.068, 6.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.256, 10.324], loss: 0.126141, mae: 0.320054, mean_q: 3.866227
 33169/100000: episode: 439, duration: 0.168s, episode steps: 28, steps per second: 166, episode reward: 62.544, mean reward: 2.234 [1.620, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.188, 10.374], loss: 0.105992, mae: 0.316331, mean_q: 3.880316
 33202/100000: episode: 440, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 87.650, mean reward: 2.656 [2.182, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.451, 10.440], loss: 0.088142, mae: 0.290425, mean_q: 3.824685
 33235/100000: episode: 441, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 84.163, mean reward: 2.550 [1.614, 6.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.060, 10.147], loss: 0.093855, mae: 0.298976, mean_q: 3.877512
 33266/100000: episode: 442, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 86.932, mean reward: 2.804 [2.041, 5.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.134, 10.100], loss: 0.125020, mae: 0.333606, mean_q: 3.913920
 33279/100000: episode: 443, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 31.942, mean reward: 2.457 [1.954, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-1.518, 10.433], loss: 0.108664, mae: 0.334405, mean_q: 3.885863
 33312/100000: episode: 444, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 76.964, mean reward: 2.332 [1.469, 4.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.178, 10.233], loss: 0.141247, mae: 0.332798, mean_q: 3.901402
 33340/100000: episode: 445, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 70.531, mean reward: 2.519 [1.788, 5.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.667, 10.378], loss: 0.112432, mae: 0.328921, mean_q: 3.921485
 33373/100000: episode: 446, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 83.464, mean reward: 2.529 [1.832, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.973, 10.100], loss: 0.093015, mae: 0.292407, mean_q: 3.881050
 33386/100000: episode: 447, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 33.468, mean reward: 2.574 [2.223, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.106, 10.413], loss: 0.116046, mae: 0.350364, mean_q: 3.994450
 33403/100000: episode: 448, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 51.842, mean reward: 3.050 [1.955, 6.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.632], loss: 0.098017, mae: 0.306452, mean_q: 3.921414
 33436/100000: episode: 449, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 80.776, mean reward: 2.448 [1.722, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.724, 10.100], loss: 0.157477, mae: 0.337777, mean_q: 3.936602
 33474/100000: episode: 450, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 77.092, mean reward: 2.029 [1.555, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.046, 10.332], loss: 0.151943, mae: 0.356563, mean_q: 3.964567
 33529/100000: episode: 451, duration: 0.313s, episode steps: 55, steps per second: 176, episode reward: 115.555, mean reward: 2.101 [1.512, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.561, 10.122], loss: 0.144337, mae: 0.335837, mean_q: 3.985610
 33562/100000: episode: 452, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 73.063, mean reward: 2.214 [1.785, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.300, 10.292], loss: 0.130980, mae: 0.327987, mean_q: 3.935728
 33579/100000: episode: 453, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 38.313, mean reward: 2.254 [1.831, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.267], loss: 0.128363, mae: 0.333110, mean_q: 4.004328
 33592/100000: episode: 454, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 40.336, mean reward: 3.103 [2.056, 7.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.370], loss: 0.121241, mae: 0.323313, mean_q: 3.981650
 33647/100000: episode: 455, duration: 0.338s, episode steps: 55, steps per second: 163, episode reward: 126.067, mean reward: 2.292 [1.535, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.406, 10.194], loss: 0.103007, mae: 0.307610, mean_q: 3.954553
 33659/100000: episode: 456, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 30.461, mean reward: 2.538 [2.143, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.439, 10.390], loss: 0.139781, mae: 0.337298, mean_q: 3.958007
 33714/100000: episode: 457, duration: 0.319s, episode steps: 55, steps per second: 172, episode reward: 128.429, mean reward: 2.335 [1.589, 5.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.963, 10.430], loss: 0.131024, mae: 0.322301, mean_q: 3.973326
 33742/100000: episode: 458, duration: 0.131s, episode steps: 28, steps per second: 214, episode reward: 60.966, mean reward: 2.177 [1.623, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.813, 10.190], loss: 0.151101, mae: 0.339846, mean_q: 4.007564
 33755/100000: episode: 459, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 36.683, mean reward: 2.822 [2.215, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.579, 10.382], loss: 0.161862, mae: 0.361187, mean_q: 3.971110
 33788/100000: episode: 460, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 104.241, mean reward: 3.159 [2.147, 8.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.005, 10.100], loss: 0.140742, mae: 0.339050, mean_q: 3.982605
 33826/100000: episode: 461, duration: 0.222s, episode steps: 38, steps per second: 171, episode reward: 111.613, mean reward: 2.937 [2.071, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.401, 10.522], loss: 0.103508, mae: 0.325947, mean_q: 4.026738
 33857/100000: episode: 462, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 89.532, mean reward: 2.888 [1.887, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.202, 10.100], loss: 0.136331, mae: 0.327241, mean_q: 4.017412
 33869/100000: episode: 463, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 26.105, mean reward: 2.175 [2.019, 2.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.341], loss: 0.133426, mae: 0.341540, mean_q: 4.009103
 33902/100000: episode: 464, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 97.464, mean reward: 2.953 [2.086, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.864, 10.412], loss: 0.152497, mae: 0.350130, mean_q: 4.011480
 33933/100000: episode: 465, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 65.024, mean reward: 2.098 [1.652, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.906, 10.100], loss: 0.156062, mae: 0.342012, mean_q: 4.031209
 33950/100000: episode: 466, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 38.711, mean reward: 2.277 [1.743, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.006, 10.266], loss: 0.165578, mae: 0.352165, mean_q: 4.026593
 34005/100000: episode: 467, duration: 0.332s, episode steps: 55, steps per second: 166, episode reward: 160.354, mean reward: 2.916 [2.114, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.396, 10.402], loss: 0.130778, mae: 0.348359, mean_q: 4.021514
 34038/100000: episode: 468, duration: 0.229s, episode steps: 33, steps per second: 144, episode reward: 85.047, mean reward: 2.577 [1.717, 4.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.226, 10.100], loss: 0.137635, mae: 0.351839, mean_q: 4.074465
 34093/100000: episode: 469, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 205.808, mean reward: 3.742 [2.464, 6.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.396, 10.495], loss: 0.133164, mae: 0.349610, mean_q: 4.105720
 34106/100000: episode: 470, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 28.532, mean reward: 2.195 [1.823, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.343], loss: 0.175471, mae: 0.369705, mean_q: 4.043396
 34118/100000: episode: 471, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 31.804, mean reward: 2.650 [2.065, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.248], loss: 0.126450, mae: 0.353342, mean_q: 4.051786
 34149/100000: episode: 472, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 70.276, mean reward: 2.267 [1.894, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-2.083, 10.100], loss: 0.171381, mae: 0.367382, mean_q: 4.109849
 34187/100000: episode: 473, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 91.353, mean reward: 2.404 [1.739, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.626, 10.275], loss: 0.140125, mae: 0.351858, mean_q: 4.128370
 34220/100000: episode: 474, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 70.361, mean reward: 2.132 [1.524, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.965, 10.107], loss: 0.127995, mae: 0.334985, mean_q: 4.066806
 34275/100000: episode: 475, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 152.694, mean reward: 2.776 [1.696, 6.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.803, 10.229], loss: 0.156328, mae: 0.353394, mean_q: 4.100838
 34306/100000: episode: 476, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 78.953, mean reward: 2.547 [1.926, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.348, 10.100], loss: 0.194955, mae: 0.395352, mean_q: 4.144187
 34339/100000: episode: 477, duration: 0.199s, episode steps: 33, steps per second: 165, episode reward: 65.253, mean reward: 1.977 [1.571, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.472, 10.100], loss: 0.152106, mae: 0.367382, mean_q: 4.218884
 34370/100000: episode: 478, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 83.115, mean reward: 2.681 [1.884, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.305, 10.100], loss: 0.163180, mae: 0.367213, mean_q: 4.199244
 34387/100000: episode: 479, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 38.916, mean reward: 2.289 [1.981, 2.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.387], loss: 0.141534, mae: 0.353285, mean_q: 4.200091
 34442/100000: episode: 480, duration: 0.312s, episode steps: 55, steps per second: 176, episode reward: 127.164, mean reward: 2.312 [1.452, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-1.193, 10.100], loss: 0.173138, mae: 0.376159, mean_q: 4.169022
 34497/100000: episode: 481, duration: 0.292s, episode steps: 55, steps per second: 189, episode reward: 113.315, mean reward: 2.060 [1.471, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.593, 10.100], loss: 0.135770, mae: 0.353612, mean_q: 4.167343
 34510/100000: episode: 482, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 25.704, mean reward: 1.977 [1.811, 2.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.236, 10.257], loss: 0.200045, mae: 0.421957, mean_q: 4.222441
 34538/100000: episode: 483, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 65.880, mean reward: 2.353 [1.818, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.069, 10.358], loss: 0.171203, mae: 0.370614, mean_q: 4.191932
 34576/100000: episode: 484, duration: 0.238s, episode steps: 38, steps per second: 160, episode reward: 103.334, mean reward: 2.719 [2.177, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.503, 10.379], loss: 0.138342, mae: 0.352901, mean_q: 4.218126
 34609/100000: episode: 485, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 82.078, mean reward: 2.487 [2.027, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.752, 10.100], loss: 0.175019, mae: 0.389416, mean_q: 4.204083
 34637/100000: episode: 486, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 59.839, mean reward: 2.137 [1.575, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.340, 10.268], loss: 0.145420, mae: 0.372618, mean_q: 4.228774
 34654/100000: episode: 487, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 37.855, mean reward: 2.227 [1.541, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.960, 10.218], loss: 0.111869, mae: 0.330943, mean_q: 4.154636
 34687/100000: episode: 488, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 89.004, mean reward: 2.697 [2.160, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.309, 10.100], loss: 0.153306, mae: 0.368987, mean_q: 4.193460
 34720/100000: episode: 489, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 92.949, mean reward: 2.817 [1.896, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.866, 10.100], loss: 0.137740, mae: 0.380405, mean_q: 4.203113
 34733/100000: episode: 490, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 27.314, mean reward: 2.101 [1.899, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.620, 10.302], loss: 0.162605, mae: 0.404262, mean_q: 4.151174
 34745/100000: episode: 491, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 37.946, mean reward: 3.162 [2.609, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.437], loss: 0.126329, mae: 0.347898, mean_q: 4.138678
 34778/100000: episode: 492, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 71.678, mean reward: 2.172 [1.795, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.351, 10.100], loss: 0.148786, mae: 0.377347, mean_q: 4.200179
 34795/100000: episode: 493, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 42.956, mean reward: 2.527 [2.284, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.484], loss: 0.145590, mae: 0.370355, mean_q: 4.205021
 34850/100000: episode: 494, duration: 0.294s, episode steps: 55, steps per second: 187, episode reward: 153.261, mean reward: 2.787 [2.072, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.993, 10.485], loss: 0.144370, mae: 0.365453, mean_q: 4.258459
 34883/100000: episode: 495, duration: 0.206s, episode steps: 33, steps per second: 160, episode reward: 90.470, mean reward: 2.742 [1.832, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.461, 10.100], loss: 0.150349, mae: 0.371216, mean_q: 4.194649
 34911/100000: episode: 496, duration: 0.198s, episode steps: 28, steps per second: 141, episode reward: 74.783, mean reward: 2.671 [1.773, 7.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.154, 10.419], loss: 0.194451, mae: 0.397271, mean_q: 4.282323
 34923/100000: episode: 497, duration: 0.089s, episode steps: 12, steps per second: 134, episode reward: 32.612, mean reward: 2.718 [2.478, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.818, 10.418], loss: 0.148781, mae: 0.387849, mean_q: 4.360439
 34951/100000: episode: 498, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 71.457, mean reward: 2.552 [1.667, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.739, 10.387], loss: 0.138409, mae: 0.379605, mean_q: 4.280962
 34963/100000: episode: 499, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 33.382, mean reward: 2.782 [2.248, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.920, 10.251], loss: 0.130262, mae: 0.355057, mean_q: 4.276575
 34991/100000: episode: 500, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 68.065, mean reward: 2.431 [1.716, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.360, 10.419], loss: 0.132256, mae: 0.351331, mean_q: 4.245108
 35022/100000: episode: 501, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 61.961, mean reward: 1.999 [1.674, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.286, 10.100], loss: 0.142955, mae: 0.359342, mean_q: 4.272861
 35039/100000: episode: 502, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 44.485, mean reward: 2.617 [2.238, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.415], loss: 0.222890, mae: 0.404318, mean_q: 4.290032
 35070/100000: episode: 503, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: 93.396, mean reward: 3.013 [1.970, 5.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.348, 10.100], loss: 0.147543, mae: 0.373170, mean_q: 4.306152
 35103/100000: episode: 504, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 77.779, mean reward: 2.357 [1.665, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.630, 10.242], loss: 0.172269, mae: 0.392506, mean_q: 4.337795
 35134/100000: episode: 505, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 64.511, mean reward: 2.081 [1.455, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.595, 10.127], loss: 0.163326, mae: 0.365044, mean_q: 4.241174
 35162/100000: episode: 506, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 64.856, mean reward: 2.316 [1.744, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.591, 10.290], loss: 0.150544, mae: 0.371858, mean_q: 4.318222
 35174/100000: episode: 507, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 27.624, mean reward: 2.302 [1.963, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.334], loss: 0.173042, mae: 0.398261, mean_q: 4.376318
 35191/100000: episode: 508, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 36.329, mean reward: 2.137 [1.735, 2.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.369], loss: 0.120880, mae: 0.337414, mean_q: 4.263035
 35203/100000: episode: 509, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 28.111, mean reward: 2.343 [2.054, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.085, 10.439], loss: 0.131050, mae: 0.367695, mean_q: 4.310581
 35241/100000: episode: 510, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 169.539, mean reward: 4.462 [2.621, 9.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.050, 10.619], loss: 0.162437, mae: 0.386354, mean_q: 4.347906
 35269/100000: episode: 511, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 64.657, mean reward: 2.309 [1.753, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.826, 10.268], loss: 0.164381, mae: 0.392838, mean_q: 4.383883
 35282/100000: episode: 512, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 27.386, mean reward: 2.107 [1.706, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.353], loss: 0.234195, mae: 0.444157, mean_q: 4.308459
 35315/100000: episode: 513, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 74.739, mean reward: 2.265 [1.471, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.928, 10.100], loss: 0.209210, mae: 0.412740, mean_q: 4.363412
 35328/100000: episode: 514, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 30.730, mean reward: 2.364 [1.994, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.339], loss: 0.214474, mae: 0.396179, mean_q: 4.454780
 35361/100000: episode: 515, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 74.879, mean reward: 2.269 [1.539, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.185, 10.230], loss: 0.160903, mae: 0.379578, mean_q: 4.328789
 35392/100000: episode: 516, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 66.107, mean reward: 2.132 [1.541, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.556, 10.301], loss: 0.177693, mae: 0.403381, mean_q: 4.382137
 35430/100000: episode: 517, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 85.388, mean reward: 2.247 [1.734, 6.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.475, 10.256], loss: 0.175278, mae: 0.387250, mean_q: 4.344457
 35442/100000: episode: 518, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 31.428, mean reward: 2.619 [2.071, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.080, 10.440], loss: 0.194156, mae: 0.417689, mean_q: 4.285481
 35497/100000: episode: 519, duration: 0.331s, episode steps: 55, steps per second: 166, episode reward: 135.387, mean reward: 2.462 [1.481, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.489, 10.225], loss: 0.181525, mae: 0.405268, mean_q: 4.394451
[Info] 2-TH LEVEL FOUND: 6.480899810791016, Considering 10/90 traces
 35535/100000: episode: 520, duration: 4.257s, episode steps: 38, steps per second: 9, episode reward: 112.020, mean reward: 2.948 [2.097, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.287, 10.391], loss: 0.180342, mae: 0.402485, mean_q: 4.320944
 35567/100000: episode: 521, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 97.407, mean reward: 3.044 [1.822, 5.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.729, 10.301], loss: 0.177859, mae: 0.421586, mean_q: 4.376822
 35603/100000: episode: 522, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 126.981, mean reward: 3.527 [1.560, 12.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.903, 10.302], loss: 0.190044, mae: 0.393666, mean_q: 4.376823
 35635/100000: episode: 523, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 120.262, mean reward: 3.758 [2.548, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.404, 10.515], loss: 0.212326, mae: 0.450836, mean_q: 4.416368
 35668/100000: episode: 524, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 138.528, mean reward: 4.198 [2.727, 7.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.635, 10.503], loss: 0.182372, mae: 0.412259, mean_q: 4.439722
 35697/100000: episode: 525, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 80.665, mean reward: 2.782 [1.701, 13.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.558, 10.243], loss: 0.229005, mae: 0.446285, mean_q: 4.435732
 35733/100000: episode: 526, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 95.474, mean reward: 2.652 [1.823, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.035, 10.245], loss: 0.244209, mae: 0.434397, mean_q: 4.462613
 35769/100000: episode: 527, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 85.551, mean reward: 2.376 [1.798, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.412, 10.420], loss: 0.207986, mae: 0.442586, mean_q: 4.459426
 35798/100000: episode: 528, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 83.300, mean reward: 2.872 [2.144, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.111, 10.418], loss: 0.175587, mae: 0.408331, mean_q: 4.505311
 35822/100000: episode: 529, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 56.691, mean reward: 2.362 [2.018, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.365], loss: 0.260276, mae: 0.429312, mean_q: 4.565926
 35851/100000: episode: 530, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 69.404, mean reward: 2.393 [1.846, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.295], loss: 0.191511, mae: 0.417479, mean_q: 4.517603
 35883/100000: episode: 531, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 89.551, mean reward: 2.798 [1.955, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.371], loss: 0.320708, mae: 0.473456, mean_q: 4.568694
 35915/100000: episode: 532, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 90.123, mean reward: 2.816 [1.785, 5.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.302, 10.393], loss: 0.160774, mae: 0.400199, mean_q: 4.525697
 35951/100000: episode: 533, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 101.835, mean reward: 2.829 [2.020, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.826, 10.399], loss: 0.233149, mae: 0.441595, mean_q: 4.525568
 35982/100000: episode: 534, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 127.624, mean reward: 4.117 [2.575, 5.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.564], loss: 0.210815, mae: 0.434682, mean_q: 4.597921
 36011/100000: episode: 535, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 89.434, mean reward: 3.084 [2.145, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.051, 10.343], loss: 0.280159, mae: 0.506369, mean_q: 4.598415
 36040/100000: episode: 536, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 84.513, mean reward: 2.914 [2.084, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.124, 10.399], loss: 0.174266, mae: 0.423581, mean_q: 4.643421
 36076/100000: episode: 537, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 79.964, mean reward: 2.221 [1.840, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.240], loss: 0.190236, mae: 0.426852, mean_q: 4.604310
 36102/100000: episode: 538, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 77.819, mean reward: 2.993 [2.277, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.458, 10.503], loss: 0.384450, mae: 0.488747, mean_q: 4.641850
 36131/100000: episode: 539, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 102.492, mean reward: 3.534 [2.421, 5.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.740, 10.402], loss: 0.352588, mae: 0.502073, mean_q: 4.605366
 36163/100000: episode: 540, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 121.139, mean reward: 3.786 [2.444, 8.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.497], loss: 0.266179, mae: 0.471506, mean_q: 4.631403
 36194/100000: episode: 541, duration: 0.185s, episode steps: 31, steps per second: 167, episode reward: 82.699, mean reward: 2.668 [1.456, 6.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.169], loss: 0.232668, mae: 0.468037, mean_q: 4.662955
 36230/100000: episode: 542, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 89.039, mean reward: 2.473 [1.875, 4.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.299, 10.469], loss: 0.265309, mae: 0.489860, mean_q: 4.758178
 36256/100000: episode: 543, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 78.330, mean reward: 3.013 [2.008, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.246, 10.347], loss: 0.188851, mae: 0.412831, mean_q: 4.635451
 36285/100000: episode: 544, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 96.527, mean reward: 3.329 [1.959, 5.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.473, 10.328], loss: 0.170987, mae: 0.411827, mean_q: 4.627243
 36309/100000: episode: 545, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 69.754, mean reward: 2.906 [2.142, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.383], loss: 0.210250, mae: 0.453605, mean_q: 4.756258
 36341/100000: episode: 546, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 76.611, mean reward: 2.394 [1.685, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.469, 10.298], loss: 0.219266, mae: 0.458436, mean_q: 4.715608
 36370/100000: episode: 547, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 105.110, mean reward: 3.624 [2.089, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.958, 10.496], loss: 0.228126, mae: 0.465956, mean_q: 4.678073
 36399/100000: episode: 548, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 72.570, mean reward: 2.502 [2.056, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.351], loss: 0.221020, mae: 0.443936, mean_q: 4.710039
 36431/100000: episode: 549, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 106.499, mean reward: 3.328 [2.365, 6.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.659, 10.390], loss: 0.292470, mae: 0.462829, mean_q: 4.764071
 36464/100000: episode: 550, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 124.388, mean reward: 3.769 [2.249, 6.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-1.301, 10.368], loss: 0.233809, mae: 0.463319, mean_q: 4.801830
 36496/100000: episode: 551, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 98.955, mean reward: 3.092 [1.849, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.370], loss: 0.247842, mae: 0.463310, mean_q: 4.742774
 36529/100000: episode: 552, duration: 0.242s, episode steps: 33, steps per second: 137, episode reward: 90.225, mean reward: 2.734 [1.944, 4.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.089, 10.308], loss: 0.212878, mae: 0.430482, mean_q: 4.854195
 36565/100000: episode: 553, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 90.270, mean reward: 2.507 [1.577, 6.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.652, 10.100], loss: 0.228182, mae: 0.425448, mean_q: 4.738960
 36591/100000: episode: 554, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 65.035, mean reward: 2.501 [1.572, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.215], loss: 0.269815, mae: 0.450635, mean_q: 4.858845
 36623/100000: episode: 555, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 98.399, mean reward: 3.075 [1.862, 5.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.400, 10.499], loss: 0.233545, mae: 0.449618, mean_q: 4.808499
 36652/100000: episode: 556, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 137.375, mean reward: 4.737 [2.678, 9.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.598, 10.658], loss: 0.240036, mae: 0.443773, mean_q: 4.812817
 36684/100000: episode: 557, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 82.265, mean reward: 2.571 [2.058, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.166, 10.368], loss: 0.223393, mae: 0.468399, mean_q: 4.886435
 36720/100000: episode: 558, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 88.781, mean reward: 2.466 [1.497, 6.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.175], loss: 0.233626, mae: 0.463786, mean_q: 4.877651
 36756/100000: episode: 559, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 139.306, mean reward: 3.870 [2.394, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.565, 10.457], loss: 0.312503, mae: 0.496269, mean_q: 4.910119
 36792/100000: episode: 560, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 121.250, mean reward: 3.368 [2.270, 7.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.688, 10.403], loss: 0.269846, mae: 0.503270, mean_q: 4.968851
 36821/100000: episode: 561, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 78.022, mean reward: 2.690 [1.928, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.329], loss: 0.248183, mae: 0.483043, mean_q: 4.946666
 36857/100000: episode: 562, duration: 0.217s, episode steps: 36, steps per second: 166, episode reward: 104.768, mean reward: 2.910 [2.209, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.574, 10.415], loss: 0.344407, mae: 0.498910, mean_q: 4.963385
 36883/100000: episode: 563, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 107.034, mean reward: 4.117 [2.102, 10.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.165, 10.413], loss: 0.347630, mae: 0.552622, mean_q: 5.001269
 36907/100000: episode: 564, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 144.027, mean reward: 6.001 [2.884, 10.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.726, 10.585], loss: 0.270060, mae: 0.460660, mean_q: 5.020418
 36933/100000: episode: 565, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 62.163, mean reward: 2.391 [1.517, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.504, 10.190], loss: 0.287796, mae: 0.498861, mean_q: 4.990793
 36957/100000: episode: 566, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 115.309, mean reward: 4.805 [2.755, 6.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.613, 10.434], loss: 0.356301, mae: 0.499377, mean_q: 5.004741
 36993/100000: episode: 567, duration: 0.202s, episode steps: 36, steps per second: 179, episode reward: 97.751, mean reward: 2.715 [1.840, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.371], loss: 0.226451, mae: 0.474721, mean_q: 4.952082
 37022/100000: episode: 568, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 97.347, mean reward: 3.357 [2.521, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.420], loss: 0.239540, mae: 0.483712, mean_q: 5.034260
 37048/100000: episode: 569, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 90.349, mean reward: 3.475 [2.587, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.415, 10.412], loss: 0.271028, mae: 0.513447, mean_q: 5.082116
 37077/100000: episode: 570, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 71.059, mean reward: 2.450 [1.952, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.406, 10.414], loss: 0.276659, mae: 0.498624, mean_q: 5.117316
 37106/100000: episode: 571, duration: 0.178s, episode steps: 29, steps per second: 163, episode reward: 96.215, mean reward: 3.318 [2.232, 7.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.361, 10.396], loss: 0.249180, mae: 0.487245, mean_q: 5.118630
 37139/100000: episode: 572, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 90.067, mean reward: 2.729 [1.691, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.621, 10.232], loss: 0.314536, mae: 0.513705, mean_q: 5.112822
 37168/100000: episode: 573, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 67.520, mean reward: 2.328 [1.794, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.843, 10.287], loss: 0.246240, mae: 0.480807, mean_q: 5.107802
 37197/100000: episode: 574, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 119.526, mean reward: 4.122 [2.743, 6.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.725, 10.624], loss: 0.338584, mae: 0.526704, mean_q: 5.142951
 37233/100000: episode: 575, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 97.951, mean reward: 2.721 [2.139, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.300, 10.306], loss: 0.257537, mae: 0.478738, mean_q: 5.094634
 37262/100000: episode: 576, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 89.097, mean reward: 3.072 [1.732, 8.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.293], loss: 0.318431, mae: 0.561966, mean_q: 5.219421
 37288/100000: episode: 577, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 69.306, mean reward: 2.666 [1.592, 6.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.477, 10.217], loss: 0.275469, mae: 0.532740, mean_q: 5.207346
 37320/100000: episode: 578, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 113.205, mean reward: 3.538 [2.664, 6.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.477, 10.461], loss: 0.265907, mae: 0.512674, mean_q: 5.269547
 37352/100000: episode: 579, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 97.152, mean reward: 3.036 [2.079, 6.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.175, 10.361], loss: 0.333704, mae: 0.524472, mean_q: 5.238501
 37378/100000: episode: 580, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 93.355, mean reward: 3.591 [2.090, 6.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.314, 10.390], loss: 0.341497, mae: 0.531631, mean_q: 5.254520
 37414/100000: episode: 581, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 80.332, mean reward: 2.231 [1.762, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.711, 10.298], loss: 0.303541, mae: 0.548566, mean_q: 5.170023
 37445/100000: episode: 582, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 98.070, mean reward: 3.164 [2.366, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.058, 10.471], loss: 0.342120, mae: 0.526965, mean_q: 5.175766
 37478/100000: episode: 583, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 85.210, mean reward: 2.582 [1.510, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.390, 10.211], loss: 0.305986, mae: 0.505107, mean_q: 5.225907
 37511/100000: episode: 584, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 213.243, mean reward: 6.462 [3.674, 10.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.125, 10.691], loss: 0.279571, mae: 0.518962, mean_q: 5.277490
 37544/100000: episode: 585, duration: 0.286s, episode steps: 33, steps per second: 115, episode reward: 160.755, mean reward: 4.871 [2.531, 8.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.741, 10.515], loss: 0.395028, mae: 0.552948, mean_q: 5.347259
 37570/100000: episode: 586, duration: 0.220s, episode steps: 26, steps per second: 118, episode reward: 67.161, mean reward: 2.583 [1.847, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.326, 10.333], loss: 0.430769, mae: 0.608368, mean_q: 5.315776
 37602/100000: episode: 587, duration: 0.242s, episode steps: 32, steps per second: 132, episode reward: 94.226, mean reward: 2.945 [1.916, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.830, 10.250], loss: 0.317416, mae: 0.533599, mean_q: 5.346002
 37634/100000: episode: 588, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 99.168, mean reward: 3.099 [1.965, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.900, 10.502], loss: 0.307568, mae: 0.537194, mean_q: 5.382779
 37670/100000: episode: 589, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 76.355, mean reward: 2.121 [1.443, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.108, 10.100], loss: 0.287994, mae: 0.513413, mean_q: 5.296191
 37694/100000: episode: 590, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 67.097, mean reward: 2.796 [1.799, 4.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.664, 10.254], loss: 0.512061, mae: 0.612518, mean_q: 5.422592
 37723/100000: episode: 591, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 112.248, mean reward: 3.871 [2.633, 7.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.148, 10.517], loss: 0.358509, mae: 0.553190, mean_q: 5.345243
 37754/100000: episode: 592, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 98.093, mean reward: 3.164 [2.432, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.757, 10.508], loss: 0.292001, mae: 0.525880, mean_q: 5.370309
 37790/100000: episode: 593, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 78.349, mean reward: 2.176 [1.612, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.035, 10.216], loss: 0.342250, mae: 0.551303, mean_q: 5.454548
 37819/100000: episode: 594, duration: 0.173s, episode steps: 29, steps per second: 168, episode reward: 86.315, mean reward: 2.976 [2.163, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.216, 10.359], loss: 0.374098, mae: 0.586178, mean_q: 5.431068
 37848/100000: episode: 595, duration: 0.139s, episode steps: 29, steps per second: 208, episode reward: 101.021, mean reward: 3.483 [2.567, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.654, 10.537], loss: 0.303294, mae: 0.509118, mean_q: 5.417367
 37881/100000: episode: 596, duration: 0.192s, episode steps: 33, steps per second: 171, episode reward: 114.784, mean reward: 3.478 [1.638, 8.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.102, 10.188], loss: 0.328621, mae: 0.506591, mean_q: 5.426958
 37910/100000: episode: 597, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 90.039, mean reward: 3.105 [1.624, 8.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.191, 10.180], loss: 0.350738, mae: 0.544982, mean_q: 5.447511
 37946/100000: episode: 598, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 113.318, mean reward: 3.148 [1.943, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.344, 10.323], loss: 0.318783, mae: 0.533927, mean_q: 5.407938
 37982/100000: episode: 599, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 96.297, mean reward: 2.675 [1.906, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.499, 10.339], loss: 0.369133, mae: 0.569786, mean_q: 5.402090
 38018/100000: episode: 600, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 188.401, mean reward: 5.233 [2.263, 12.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.715, 10.471], loss: 0.548034, mae: 0.590564, mean_q: 5.470461
 38051/100000: episode: 601, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 87.006, mean reward: 2.637 [1.512, 4.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.880, 10.147], loss: 0.346936, mae: 0.576456, mean_q: 5.510002
 38087/100000: episode: 602, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 99.942, mean reward: 2.776 [1.874, 7.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.264, 10.397], loss: 0.384567, mae: 0.576287, mean_q: 5.421993
 38118/100000: episode: 603, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 99.586, mean reward: 3.212 [2.517, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.041, 10.491], loss: 0.321781, mae: 0.554051, mean_q: 5.427579
 38149/100000: episode: 604, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 104.755, mean reward: 3.379 [2.481, 6.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.531], loss: 0.391147, mae: 0.560110, mean_q: 5.573906
 38185/100000: episode: 605, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 88.007, mean reward: 2.445 [1.756, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.270, 10.271], loss: 0.322332, mae: 0.498509, mean_q: 5.449789
 38214/100000: episode: 606, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 83.028, mean reward: 2.863 [2.063, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.246, 10.330], loss: 0.330202, mae: 0.534576, mean_q: 5.583241
 38250/100000: episode: 607, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 101.512, mean reward: 2.820 [2.131, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.243, 10.383], loss: 0.345591, mae: 0.558911, mean_q: 5.546409
[Info] FALSIFICATION!
 38269/100000: episode: 608, duration: 0.259s, episode steps: 19, steps per second: 73, episode reward: 1081.792, mean reward: 56.936 [3.133, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.385, 10.732], loss: 0.355694, mae: 0.580525, mean_q: 5.488338
 38300/100000: episode: 609, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 86.036, mean reward: 2.775 [2.160, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.365], loss: 0.395756, mae: 0.586648, mean_q: 5.541545
[Info] Complete ISplit Iteration
[Info] Levels: [4.7842712, 6.4809, 10.460868]
[Info] Cond. Prob: [0.1, 0.1, 0.06]
[Info] Error Prob: 0.0006000000000000001

 38326/100000: episode: 610, duration: 4.363s, episode steps: 26, steps per second: 6, episode reward: 74.160, mean reward: 2.852 [2.023, 6.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.264, 10.358], loss: 596.794128, mae: 3.434615, mean_q: 7.437042
 38426/100000: episode: 611, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 187.595, mean reward: 1.876 [1.486, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.419, 10.098], loss: 155.813217, mae: 1.425636, mean_q: 5.322180
 38526/100000: episode: 612, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 177.718, mean reward: 1.777 [1.455, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.245, 10.098], loss: 309.715240, mae: 1.818031, mean_q: 6.139557
 38626/100000: episode: 613, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.521, mean reward: 1.845 [1.452, 5.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.374, 10.144], loss: 0.851899, mae: 0.949983, mean_q: 6.012224
 38726/100000: episode: 614, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 193.941, mean reward: 1.939 [1.479, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.167], loss: 0.490634, mae: 0.687643, mean_q: 5.743345
 38826/100000: episode: 615, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 231.424, mean reward: 2.314 [1.480, 11.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-2.152, 10.098], loss: 0.406733, mae: 0.629088, mean_q: 5.631454
 38926/100000: episode: 616, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.882, mean reward: 1.919 [1.471, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.539, 10.172], loss: 0.345879, mae: 0.572256, mean_q: 5.532064
 39026/100000: episode: 617, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.703, mean reward: 1.877 [1.460, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.608, 10.098], loss: 0.319343, mae: 0.558220, mean_q: 5.481258
 39126/100000: episode: 618, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 199.166, mean reward: 1.992 [1.445, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.299], loss: 154.637100, mae: 1.187287, mean_q: 5.824184
 39226/100000: episode: 619, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 183.963, mean reward: 1.840 [1.470, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.725, 10.216], loss: 154.725037, mae: 1.156124, mean_q: 5.378538
 39326/100000: episode: 620, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.211, mean reward: 1.962 [1.484, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.098], loss: 1.032369, mae: 0.980897, mean_q: 5.809106
 39426/100000: episode: 621, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 203.581, mean reward: 2.036 [1.454, 13.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.494, 10.183], loss: 308.132050, mae: 2.057027, mean_q: 6.242698
 39526/100000: episode: 622, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.898, mean reward: 1.969 [1.452, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.968, 10.214], loss: 0.432842, mae: 0.591862, mean_q: 5.485838
 39626/100000: episode: 623, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.361, mean reward: 1.934 [1.468, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.098], loss: 154.351990, mae: 1.272418, mean_q: 5.896042
 39726/100000: episode: 624, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 185.869, mean reward: 1.859 [1.451, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.098], loss: 0.473483, mae: 0.639482, mean_q: 5.385337
 39826/100000: episode: 625, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 255.083, mean reward: 2.551 [1.491, 6.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.421, 10.500], loss: 0.419444, mae: 0.583309, mean_q: 5.396842
 39926/100000: episode: 626, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 180.406, mean reward: 1.804 [1.482, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.503, 10.109], loss: 0.334771, mae: 0.529889, mean_q: 5.280978
 40026/100000: episode: 627, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 196.218, mean reward: 1.962 [1.463, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.206, 10.283], loss: 153.762222, mae: 0.871162, mean_q: 5.265520
 40126/100000: episode: 628, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 190.809, mean reward: 1.908 [1.446, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.534, 10.098], loss: 307.731354, mae: 1.937362, mean_q: 6.133641
 40226/100000: episode: 629, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 183.435, mean reward: 1.834 [1.462, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.412, 10.098], loss: 0.603808, mae: 0.680668, mean_q: 5.589639
 40326/100000: episode: 630, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 190.440, mean reward: 1.904 [1.485, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.015, 10.250], loss: 153.642380, mae: 0.940280, mean_q: 5.473042
 40426/100000: episode: 631, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 192.977, mean reward: 1.930 [1.451, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.100], loss: 0.977161, mae: 0.839605, mean_q: 5.547152
 40526/100000: episode: 632, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.199, mean reward: 1.812 [1.434, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.884, 10.240], loss: 154.237106, mae: 1.234978, mean_q: 5.566456
 40626/100000: episode: 633, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 201.670, mean reward: 2.017 [1.439, 5.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.033, 10.098], loss: 153.469574, mae: 0.855402, mean_q: 5.241944
 40726/100000: episode: 634, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.352, mean reward: 1.904 [1.461, 2.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.702, 10.143], loss: 154.559006, mae: 1.445256, mean_q: 5.803035
 40826/100000: episode: 635, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 221.461, mean reward: 2.215 [1.453, 10.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.991, 10.255], loss: 153.755386, mae: 1.130453, mean_q: 5.488977
 40926/100000: episode: 636, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.797, mean reward: 1.848 [1.452, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.062, 10.098], loss: 0.380201, mae: 0.531738, mean_q: 5.131495
 41026/100000: episode: 637, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 199.838, mean reward: 1.998 [1.445, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.285], loss: 0.286265, mae: 0.481690, mean_q: 5.064993
 41126/100000: episode: 638, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.949, mean reward: 1.789 [1.471, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.442, 10.117], loss: 0.318829, mae: 0.466665, mean_q: 4.938418
 41226/100000: episode: 639, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 217.537, mean reward: 2.175 [1.467, 6.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.136, 10.098], loss: 0.246839, mae: 0.440743, mean_q: 4.850159
 41326/100000: episode: 640, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.512, mean reward: 1.985 [1.483, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.061, 10.098], loss: 0.288549, mae: 0.463411, mean_q: 4.858117
 41426/100000: episode: 641, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.745, mean reward: 1.857 [1.455, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.418, 10.098], loss: 154.124893, mae: 1.111645, mean_q: 5.126982
 41526/100000: episode: 642, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.787, mean reward: 1.848 [1.445, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.684, 10.098], loss: 0.291065, mae: 0.473669, mean_q: 4.683671
 41626/100000: episode: 643, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 199.532, mean reward: 1.995 [1.468, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.827, 10.098], loss: 0.338927, mae: 0.458553, mean_q: 4.665676
 41726/100000: episode: 644, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 224.352, mean reward: 2.244 [1.512, 4.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.870, 10.162], loss: 0.296407, mae: 0.433875, mean_q: 4.600316
 41826/100000: episode: 645, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.360, mean reward: 1.834 [1.457, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.468, 10.162], loss: 0.260245, mae: 0.423938, mean_q: 4.535367
 41926/100000: episode: 646, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 222.218, mean reward: 2.222 [1.494, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.148, 10.438], loss: 0.227169, mae: 0.400981, mean_q: 4.504760
 42026/100000: episode: 647, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.510, mean reward: 1.835 [1.492, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.198, 10.134], loss: 154.342010, mae: 1.128816, mean_q: 4.792668
 42126/100000: episode: 648, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.581, mean reward: 1.866 [1.447, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.042, 10.134], loss: 153.887680, mae: 1.062114, mean_q: 4.819345
 42226/100000: episode: 649, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 207.859, mean reward: 2.079 [1.462, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.098], loss: 0.239948, mae: 0.426225, mean_q: 4.479588
 42326/100000: episode: 650, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 194.183, mean reward: 1.942 [1.470, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.416, 10.414], loss: 0.227757, mae: 0.408739, mean_q: 4.369709
 42426/100000: episode: 651, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 209.489, mean reward: 2.095 [1.471, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.605, 10.361], loss: 0.203785, mae: 0.385389, mean_q: 4.318108
 42526/100000: episode: 652, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 204.066, mean reward: 2.041 [1.501, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.758, 10.245], loss: 0.273818, mae: 0.394924, mean_q: 4.274194
 42626/100000: episode: 653, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 222.567, mean reward: 2.226 [1.490, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.425, 10.098], loss: 0.135149, mae: 0.346006, mean_q: 4.150541
 42726/100000: episode: 654, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 204.962, mean reward: 2.050 [1.474, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.467, 10.375], loss: 307.261230, mae: 1.499471, mean_q: 4.719562
 42826/100000: episode: 655, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 188.262, mean reward: 1.883 [1.440, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.158, 10.098], loss: 153.680923, mae: 1.015148, mean_q: 4.526027
 42926/100000: episode: 656, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.876, mean reward: 1.889 [1.517, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.288, 10.098], loss: 153.409378, mae: 0.974861, mean_q: 4.467857
 43026/100000: episode: 657, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.450, mean reward: 1.935 [1.453, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.788, 10.105], loss: 153.364655, mae: 0.932411, mean_q: 4.462169
 43126/100000: episode: 658, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 181.997, mean reward: 1.820 [1.458, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.162], loss: 152.722534, mae: 0.712692, mean_q: 4.174543
 43226/100000: episode: 659, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 182.408, mean reward: 1.824 [1.433, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.845, 10.098], loss: 153.411499, mae: 1.140185, mean_q: 4.643293
 43326/100000: episode: 660, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.966, mean reward: 1.830 [1.446, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.512, 10.209], loss: 0.127536, mae: 0.340781, mean_q: 4.002590
 43426/100000: episode: 661, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.978, mean reward: 1.880 [1.461, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.141], loss: 0.164797, mae: 0.331057, mean_q: 3.957697
 43526/100000: episode: 662, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.356, mean reward: 1.904 [1.439, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.952, 10.098], loss: 0.119137, mae: 0.308093, mean_q: 3.916834
 43626/100000: episode: 663, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.916, mean reward: 1.989 [1.495, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.098], loss: 0.112434, mae: 0.311842, mean_q: 3.905022
 43726/100000: episode: 664, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.790, mean reward: 1.898 [1.468, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.603, 10.144], loss: 0.115233, mae: 0.330341, mean_q: 3.907256
 43826/100000: episode: 665, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 190.416, mean reward: 1.904 [1.447, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.233, 10.154], loss: 0.124404, mae: 0.313146, mean_q: 3.881553
 43926/100000: episode: 666, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 211.522, mean reward: 2.115 [1.488, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.739, 10.497], loss: 0.143088, mae: 0.327017, mean_q: 3.907705
 44026/100000: episode: 667, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.580, mean reward: 1.866 [1.454, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.098], loss: 0.130057, mae: 0.316901, mean_q: 3.917628
 44126/100000: episode: 668, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 208.925, mean reward: 2.089 [1.469, 10.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.698, 10.314], loss: 0.144671, mae: 0.313775, mean_q: 3.898174
 44226/100000: episode: 669, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 204.952, mean reward: 2.050 [1.431, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.117, 10.098], loss: 0.125547, mae: 0.320828, mean_q: 3.893463
 44326/100000: episode: 670, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 238.234, mean reward: 2.382 [1.489, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.638, 10.207], loss: 0.159907, mae: 0.326866, mean_q: 3.904367
 44426/100000: episode: 671, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 222.094, mean reward: 2.221 [1.503, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.472, 10.334], loss: 0.117054, mae: 0.319382, mean_q: 3.931720
 44526/100000: episode: 672, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 183.705, mean reward: 1.837 [1.455, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.258, 10.300], loss: 0.112432, mae: 0.303947, mean_q: 3.900831
 44626/100000: episode: 673, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 206.514, mean reward: 2.065 [1.511, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.167], loss: 0.112320, mae: 0.305094, mean_q: 3.908078
 44726/100000: episode: 674, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 194.945, mean reward: 1.949 [1.477, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.526, 10.098], loss: 0.156029, mae: 0.337866, mean_q: 3.939705
 44826/100000: episode: 675, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 193.473, mean reward: 1.935 [1.475, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.079, 10.098], loss: 0.153397, mae: 0.332289, mean_q: 3.945963
 44926/100000: episode: 676, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 192.885, mean reward: 1.929 [1.468, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.713, 10.098], loss: 0.110485, mae: 0.312011, mean_q: 3.911780
 45026/100000: episode: 677, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 194.892, mean reward: 1.949 [1.516, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.030, 10.130], loss: 0.098171, mae: 0.299064, mean_q: 3.894341
 45126/100000: episode: 678, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 200.242, mean reward: 2.002 [1.461, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.871, 10.327], loss: 0.125797, mae: 0.321342, mean_q: 3.928510
 45226/100000: episode: 679, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 179.883, mean reward: 1.799 [1.458, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.953, 10.098], loss: 0.119456, mae: 0.308796, mean_q: 3.909584
 45326/100000: episode: 680, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 202.759, mean reward: 2.028 [1.444, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.132, 10.199], loss: 0.124218, mae: 0.323377, mean_q: 3.912504
 45426/100000: episode: 681, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 184.627, mean reward: 1.846 [1.452, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.286, 10.210], loss: 0.124062, mae: 0.316776, mean_q: 3.918329
 45526/100000: episode: 682, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 181.567, mean reward: 1.816 [1.488, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.371, 10.287], loss: 0.110008, mae: 0.307146, mean_q: 3.917089
 45626/100000: episode: 683, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 196.107, mean reward: 1.961 [1.478, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.028, 10.098], loss: 0.127041, mae: 0.321098, mean_q: 3.916074
 45726/100000: episode: 684, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.992, mean reward: 1.900 [1.478, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.229, 10.098], loss: 0.100212, mae: 0.299702, mean_q: 3.903601
 45826/100000: episode: 685, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 240.795, mean reward: 2.408 [1.496, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.181, 10.098], loss: 0.091655, mae: 0.302706, mean_q: 3.903509
 45926/100000: episode: 686, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 205.685, mean reward: 2.057 [1.494, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.920, 10.307], loss: 0.105651, mae: 0.320074, mean_q: 3.929759
 46026/100000: episode: 687, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.218, mean reward: 1.882 [1.444, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.811, 10.110], loss: 0.111234, mae: 0.318416, mean_q: 3.934617
 46126/100000: episode: 688, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.463, mean reward: 1.885 [1.447, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.221, 10.098], loss: 0.107386, mae: 0.313513, mean_q: 3.940738
 46226/100000: episode: 689, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 194.352, mean reward: 1.944 [1.470, 5.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.529, 10.098], loss: 0.102634, mae: 0.306110, mean_q: 3.928735
 46326/100000: episode: 690, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 197.410, mean reward: 1.974 [1.454, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.608, 10.219], loss: 0.102319, mae: 0.299268, mean_q: 3.908372
 46426/100000: episode: 691, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 191.810, mean reward: 1.918 [1.456, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.416, 10.133], loss: 0.100335, mae: 0.307255, mean_q: 3.915679
 46526/100000: episode: 692, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.892, mean reward: 1.869 [1.489, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.547, 10.355], loss: 0.091562, mae: 0.303989, mean_q: 3.923651
 46626/100000: episode: 693, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 192.626, mean reward: 1.926 [1.446, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.734, 10.098], loss: 0.105001, mae: 0.310234, mean_q: 3.902819
 46726/100000: episode: 694, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 207.291, mean reward: 2.073 [1.455, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.750, 10.293], loss: 0.092087, mae: 0.301150, mean_q: 3.915283
 46826/100000: episode: 695, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.614, mean reward: 1.866 [1.451, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.710, 10.178], loss: 0.113612, mae: 0.307017, mean_q: 3.927273
 46926/100000: episode: 696, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 202.302, mean reward: 2.023 [1.485, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.937, 10.098], loss: 0.104232, mae: 0.308187, mean_q: 3.906452
 47026/100000: episode: 697, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 193.087, mean reward: 1.931 [1.452, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.025, 10.220], loss: 0.100693, mae: 0.303846, mean_q: 3.920581
 47126/100000: episode: 698, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 201.022, mean reward: 2.010 [1.477, 5.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.098], loss: 0.089237, mae: 0.301113, mean_q: 3.902071
 47226/100000: episode: 699, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 186.622, mean reward: 1.866 [1.510, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.824, 10.283], loss: 0.110905, mae: 0.303144, mean_q: 3.908134
 47326/100000: episode: 700, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.421, mean reward: 1.844 [1.480, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.479, 10.098], loss: 0.091520, mae: 0.297859, mean_q: 3.898648
 47426/100000: episode: 701, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 178.156, mean reward: 1.782 [1.443, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.311, 10.142], loss: 0.077428, mae: 0.280862, mean_q: 3.871691
 47526/100000: episode: 702, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.466, mean reward: 1.815 [1.452, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.366, 10.098], loss: 0.100311, mae: 0.301080, mean_q: 3.914345
 47626/100000: episode: 703, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.071, mean reward: 1.891 [1.437, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.497, 10.215], loss: 0.097270, mae: 0.290757, mean_q: 3.886007
 47726/100000: episode: 704, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 193.513, mean reward: 1.935 [1.476, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.432, 10.098], loss: 0.096489, mae: 0.292577, mean_q: 3.866309
 47826/100000: episode: 705, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.999, mean reward: 1.870 [1.468, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.106466, mae: 0.295916, mean_q: 3.905051
 47926/100000: episode: 706, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.772, mean reward: 1.938 [1.482, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.786, 10.098], loss: 0.085370, mae: 0.285222, mean_q: 3.861284
 48026/100000: episode: 707, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.588, mean reward: 2.016 [1.467, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.277, 10.098], loss: 0.095206, mae: 0.284540, mean_q: 3.862953
 48126/100000: episode: 708, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 181.524, mean reward: 1.815 [1.450, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.739, 10.167], loss: 0.105250, mae: 0.293186, mean_q: 3.856012
 48226/100000: episode: 709, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 228.585, mean reward: 2.286 [1.464, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.379, 10.595], loss: 0.082768, mae: 0.279247, mean_q: 3.848538
[Info] 1-TH LEVEL FOUND: 5.568549156188965, Considering 10/90 traces
 48326/100000: episode: 710, duration: 4.513s, episode steps: 100, steps per second: 22, episode reward: 193.859, mean reward: 1.939 [1.460, 4.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.116, 10.108], loss: 0.100094, mae: 0.297334, mean_q: 3.887470
 48345/100000: episode: 711, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 57.054, mean reward: 3.003 [2.388, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.457], loss: 0.071906, mae: 0.275402, mean_q: 3.882293
 48361/100000: episode: 712, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 38.993, mean reward: 2.437 [2.060, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.461], loss: 0.079686, mae: 0.289922, mean_q: 3.894287
 48372/100000: episode: 713, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 23.898, mean reward: 2.173 [1.817, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.354], loss: 0.085427, mae: 0.294063, mean_q: 3.886199
 48399/100000: episode: 714, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 70.712, mean reward: 2.619 [1.701, 8.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.323, 10.220], loss: 0.077774, mae: 0.268177, mean_q: 3.879723
 48408/100000: episode: 715, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 19.712, mean reward: 2.190 [1.947, 2.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.327], loss: 0.062868, mae: 0.275768, mean_q: 3.925060
 48427/100000: episode: 716, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 47.447, mean reward: 2.497 [2.315, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.398], loss: 0.094393, mae: 0.307278, mean_q: 3.913891
 48460/100000: episode: 717, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 113.751, mean reward: 3.447 [2.660, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.489], loss: 0.080891, mae: 0.293192, mean_q: 3.940441
 48484/100000: episode: 718, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 62.549, mean reward: 2.606 [1.525, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.959, 10.229], loss: 0.069568, mae: 0.263848, mean_q: 3.891597
 48500/100000: episode: 719, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 43.926, mean reward: 2.745 [2.376, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.427, 10.403], loss: 0.090622, mae: 0.301786, mean_q: 3.914145
 48527/100000: episode: 720, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 74.751, mean reward: 2.769 [1.998, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.071, 10.436], loss: 0.073570, mae: 0.285197, mean_q: 3.941420
 48546/100000: episode: 721, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 53.467, mean reward: 2.814 [2.211, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.903, 10.436], loss: 0.090114, mae: 0.309556, mean_q: 3.936429
 48579/100000: episode: 722, duration: 0.207s, episode steps: 33, steps per second: 159, episode reward: 100.557, mean reward: 3.047 [1.780, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.001, 10.273], loss: 0.084646, mae: 0.291703, mean_q: 3.949706
 48606/100000: episode: 723, duration: 0.157s, episode steps: 27, steps per second: 171, episode reward: 85.617, mean reward: 3.171 [2.492, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.425], loss: 0.075528, mae: 0.276906, mean_q: 3.960389
 48625/100000: episode: 724, duration: 0.129s, episode steps: 19, steps per second: 148, episode reward: 58.883, mean reward: 3.099 [2.370, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.709, 10.537], loss: 0.100644, mae: 0.318821, mean_q: 4.072517
 48636/100000: episode: 725, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 31.470, mean reward: 2.861 [2.113, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.284, 10.266], loss: 0.086430, mae: 0.286044, mean_q: 3.996018
 48652/100000: episode: 726, duration: 0.140s, episode steps: 16, steps per second: 114, episode reward: 49.376, mean reward: 3.086 [2.139, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.324], loss: 0.074471, mae: 0.279190, mean_q: 4.039172
 48685/100000: episode: 727, duration: 0.322s, episode steps: 33, steps per second: 102, episode reward: 139.645, mean reward: 4.232 [2.545, 19.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.666, 10.396], loss: 0.098328, mae: 0.298666, mean_q: 4.000667
 48711/100000: episode: 728, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 53.064, mean reward: 2.041 [1.601, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.307], loss: 0.088015, mae: 0.300750, mean_q: 3.965831
 48738/100000: episode: 729, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 77.966, mean reward: 2.888 [2.065, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.479], loss: 0.297360, mae: 0.355289, mean_q: 4.037451
 48747/100000: episode: 730, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 21.949, mean reward: 2.439 [2.203, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.300], loss: 0.189739, mae: 0.323894, mean_q: 4.058901
 48774/100000: episode: 731, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 66.793, mean reward: 2.474 [1.608, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.424, 10.297], loss: 0.103283, mae: 0.309140, mean_q: 4.027162
 48807/100000: episode: 732, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 110.673, mean reward: 3.354 [2.022, 5.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.417, 10.339], loss: 0.114179, mae: 0.325889, mean_q: 4.051130
 48826/100000: episode: 733, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 53.675, mean reward: 2.825 [2.431, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.318, 10.413], loss: 0.113190, mae: 0.332846, mean_q: 4.115864
 48850/100000: episode: 734, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 61.600, mean reward: 2.567 [1.656, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.049, 10.241], loss: 0.091002, mae: 0.305390, mean_q: 4.016209
 48866/100000: episode: 735, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 42.853, mean reward: 2.678 [2.036, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.071, 10.432], loss: 0.111756, mae: 0.333585, mean_q: 4.152105
 48895/100000: episode: 736, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 88.739, mean reward: 3.060 [2.279, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.211, 10.442], loss: 0.117638, mae: 0.320370, mean_q: 4.057294
 48922/100000: episode: 737, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 62.248, mean reward: 2.305 [1.718, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.268], loss: 0.106883, mae: 0.326497, mean_q: 4.044039
 48936/100000: episode: 738, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 44.025, mean reward: 3.145 [2.359, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.487], loss: 0.106272, mae: 0.327655, mean_q: 4.142484
 48955/100000: episode: 739, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 60.435, mean reward: 3.181 [2.483, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.061, 10.473], loss: 0.343533, mae: 0.341369, mean_q: 4.052053
 48966/100000: episode: 740, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 30.790, mean reward: 2.799 [2.434, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.551, 10.390], loss: 0.167535, mae: 0.386363, mean_q: 4.156518
 48985/100000: episode: 741, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 49.025, mean reward: 2.580 [1.728, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.576, 10.409], loss: 0.119607, mae: 0.339748, mean_q: 4.079360
 48996/100000: episode: 742, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 25.747, mean reward: 2.341 [2.086, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.650, 10.333], loss: 0.103743, mae: 0.334638, mean_q: 4.070067
 49007/100000: episode: 743, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 27.620, mean reward: 2.511 [2.163, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.392], loss: 0.094379, mae: 0.319811, mean_q: 4.116565
 49016/100000: episode: 744, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 21.955, mean reward: 2.439 [2.150, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.366], loss: 0.124140, mae: 0.358917, mean_q: 4.119915
 49043/100000: episode: 745, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 78.372, mean reward: 2.903 [1.876, 4.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.545, 10.315], loss: 0.087050, mae: 0.295377, mean_q: 4.108144
 49070/100000: episode: 746, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 105.914, mean reward: 3.923 [2.401, 5.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.508], loss: 0.111248, mae: 0.328822, mean_q: 4.188540
 49099/100000: episode: 747, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 120.640, mean reward: 4.160 [2.302, 14.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.886, 10.431], loss: 0.111717, mae: 0.306506, mean_q: 4.122966
 49125/100000: episode: 748, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 56.872, mean reward: 2.187 [1.745, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.813, 10.217], loss: 0.297084, mae: 0.354040, mean_q: 4.203323
 49158/100000: episode: 749, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 96.989, mean reward: 2.939 [2.180, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.411], loss: 0.126972, mae: 0.331257, mean_q: 4.130693
 49177/100000: episode: 750, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 46.917, mean reward: 2.469 [1.863, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.354], loss: 0.103106, mae: 0.310676, mean_q: 4.175415
 49201/100000: episode: 751, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 60.578, mean reward: 2.524 [1.842, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.793, 10.314], loss: 0.219091, mae: 0.349849, mean_q: 4.176935
 49217/100000: episode: 752, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 34.743, mean reward: 2.171 [1.893, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.249], loss: 0.134827, mae: 0.363169, mean_q: 4.189137
 49226/100000: episode: 753, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 22.371, mean reward: 2.486 [1.989, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.374], loss: 0.595853, mae: 0.380976, mean_q: 4.244766
 49240/100000: episode: 754, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 63.915, mean reward: 4.565 [2.806, 6.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.031, 10.558], loss: 0.104838, mae: 0.328834, mean_q: 4.074026
 49254/100000: episode: 755, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 44.281, mean reward: 3.163 [2.612, 5.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.349], loss: 0.126329, mae: 0.355362, mean_q: 4.259987
 49265/100000: episode: 756, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 26.322, mean reward: 2.393 [1.959, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.919, 10.436], loss: 0.097558, mae: 0.316471, mean_q: 4.235787
 49279/100000: episode: 757, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 44.198, mean reward: 3.157 [2.461, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.405], loss: 0.089064, mae: 0.305213, mean_q: 4.161268
 49303/100000: episode: 758, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 57.863, mean reward: 2.411 [1.715, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.450, 10.299], loss: 0.184269, mae: 0.338801, mean_q: 4.187112
 49314/100000: episode: 759, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 23.120, mean reward: 2.102 [1.799, 2.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.327], loss: 0.423260, mae: 0.385718, mean_q: 4.173917
 49340/100000: episode: 760, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 88.360, mean reward: 3.398 [2.658, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.156, 10.529], loss: 0.127062, mae: 0.354589, mean_q: 4.272494
 49354/100000: episode: 761, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 34.049, mean reward: 2.432 [2.115, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.448, 10.403], loss: 0.131563, mae: 0.341302, mean_q: 4.193439
 49380/100000: episode: 762, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 50.017, mean reward: 1.924 [1.545, 2.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.310], loss: 0.294970, mae: 0.363308, mean_q: 4.171719
 49413/100000: episode: 763, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 132.427, mean reward: 4.013 [2.514, 10.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.535], loss: 0.124827, mae: 0.350942, mean_q: 4.223835
 49429/100000: episode: 764, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 35.580, mean reward: 2.224 [1.870, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.513, 10.336], loss: 0.159264, mae: 0.349693, mean_q: 4.252779
 49456/100000: episode: 765, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 95.903, mean reward: 3.552 [2.265, 6.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.204, 10.454], loss: 0.302843, mae: 0.371745, mean_q: 4.148692
 49483/100000: episode: 766, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 78.386, mean reward: 2.903 [1.982, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.361, 10.370], loss: 0.146251, mae: 0.367887, mean_q: 4.302217
 49497/100000: episode: 767, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 43.212, mean reward: 3.087 [2.112, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.569], loss: 0.269710, mae: 0.412367, mean_q: 4.371254
 49524/100000: episode: 768, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 79.525, mean reward: 2.945 [2.273, 5.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.352], loss: 0.110078, mae: 0.328067, mean_q: 4.176479
 49538/100000: episode: 769, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 43.157, mean reward: 3.083 [2.550, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.476], loss: 0.565413, mae: 0.449085, mean_q: 4.418678
 49565/100000: episode: 770, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 101.091, mean reward: 3.744 [2.580, 5.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.563], loss: 0.129383, mae: 0.352931, mean_q: 4.173471
 49581/100000: episode: 771, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 39.454, mean reward: 2.466 [1.997, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.211, 10.398], loss: 0.127998, mae: 0.355057, mean_q: 4.325910
 49597/100000: episode: 772, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 38.317, mean reward: 2.395 [2.102, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.240, 10.334], loss: 0.109653, mae: 0.322290, mean_q: 4.218811
 49630/100000: episode: 773, duration: 0.201s, episode steps: 33, steps per second: 165, episode reward: 94.307, mean reward: 2.858 [1.799, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.484, 10.320], loss: 0.252450, mae: 0.399340, mean_q: 4.327120
 49659/100000: episode: 774, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 87.934, mean reward: 3.032 [2.056, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.684, 10.481], loss: 0.324762, mae: 0.416295, mean_q: 4.311131
 49668/100000: episode: 775, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 23.808, mean reward: 2.645 [2.358, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.399], loss: 0.130391, mae: 0.342958, mean_q: 4.199965
 49684/100000: episode: 776, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 54.950, mean reward: 3.434 [2.467, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.193, 10.543], loss: 0.340899, mae: 0.390234, mean_q: 4.393046
 49708/100000: episode: 777, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 68.670, mean reward: 2.861 [2.497, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.346], loss: 0.221854, mae: 0.398603, mean_q: 4.374914
 49732/100000: episode: 778, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 53.876, mean reward: 2.245 [1.775, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.297], loss: 0.215223, mae: 0.386915, mean_q: 4.319645
 49746/100000: episode: 779, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 41.955, mean reward: 2.997 [2.461, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.428, 10.288], loss: 0.113210, mae: 0.341649, mean_q: 4.355439
 49755/100000: episode: 780, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 20.228, mean reward: 2.248 [2.076, 2.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.346], loss: 0.101689, mae: 0.318922, mean_q: 4.390532
 49771/100000: episode: 781, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 58.556, mean reward: 3.660 [2.416, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.563], loss: 0.238845, mae: 0.384119, mean_q: 4.319267
 49785/100000: episode: 782, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 46.450, mean reward: 3.318 [2.491, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.408], loss: 0.149453, mae: 0.357987, mean_q: 4.385631
 49811/100000: episode: 783, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 70.500, mean reward: 2.712 [2.078, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.056, 10.363], loss: 0.139711, mae: 0.353696, mean_q: 4.378660
 49840/100000: episode: 784, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 73.510, mean reward: 2.535 [1.972, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.398, 10.337], loss: 0.284334, mae: 0.405784, mean_q: 4.341868
 49864/100000: episode: 785, duration: 0.153s, episode steps: 24, steps per second: 156, episode reward: 70.143, mean reward: 2.923 [1.901, 6.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.298, 10.343], loss: 0.150294, mae: 0.366624, mean_q: 4.463639
 49897/100000: episode: 786, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 91.498, mean reward: 2.773 [1.693, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.362, 10.254], loss: 0.142927, mae: 0.370619, mean_q: 4.344577
 49906/100000: episode: 787, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 21.025, mean reward: 2.336 [2.103, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.329, 10.391], loss: 0.133320, mae: 0.361281, mean_q: 4.421288
 49920/100000: episode: 788, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 44.060, mean reward: 3.147 [2.234, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.342, 10.578], loss: 0.107139, mae: 0.337015, mean_q: 4.275763
 49944/100000: episode: 789, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 64.182, mean reward: 2.674 [2.345, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.688, 10.384], loss: 0.129204, mae: 0.333619, mean_q: 4.326511
 49973/100000: episode: 790, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 72.525, mean reward: 2.501 [2.101, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.045, 10.424], loss: 0.204449, mae: 0.397164, mean_q: 4.493551
 50006/100000: episode: 791, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 175.079, mean reward: 5.305 [1.859, 25.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.301], loss: 0.145766, mae: 0.358407, mean_q: 4.392130
 50039/100000: episode: 792, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 106.524, mean reward: 3.228 [2.155, 5.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.589, 10.385], loss: 0.326464, mae: 0.350243, mean_q: 4.379381
 50055/100000: episode: 793, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 38.608, mean reward: 2.413 [2.058, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.381], loss: 0.222701, mae: 0.439618, mean_q: 4.379523
 50079/100000: episode: 794, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 66.529, mean reward: 2.772 [1.830, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.637, 10.294], loss: 0.390422, mae: 0.466544, mean_q: 4.635990
 50095/100000: episode: 795, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 49.339, mean reward: 3.084 [2.326, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.434], loss: 0.150931, mae: 0.354889, mean_q: 4.361061
 50122/100000: episode: 796, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 95.885, mean reward: 3.551 [1.966, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.659, 10.365], loss: 0.304473, mae: 0.396864, mean_q: 4.562456
 50148/100000: episode: 797, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 56.535, mean reward: 2.174 [1.700, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.551, 10.292], loss: 0.170906, mae: 0.368915, mean_q: 4.413770
 50162/100000: episode: 798, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 42.746, mean reward: 3.053 [2.554, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.501], loss: 0.519748, mae: 0.485457, mean_q: 4.593231
 50178/100000: episode: 799, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 34.726, mean reward: 2.170 [1.719, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.236, 10.218], loss: 0.155331, mae: 0.390865, mean_q: 4.418988
[Info] 2-TH LEVEL FOUND: 8.386617660522461, Considering 10/90 traces
 50202/100000: episode: 800, duration: 4.249s, episode steps: 24, steps per second: 6, episode reward: 78.259, mean reward: 3.261 [1.526, 6.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.051, 10.356], loss: 0.140704, mae: 0.366212, mean_q: 4.545978
 50227/100000: episode: 801, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 107.478, mean reward: 4.299 [2.521, 7.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.989, 10.354], loss: 0.231407, mae: 0.424216, mean_q: 4.545754
 50243/100000: episode: 802, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 45.157, mean reward: 2.822 [1.914, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.352], loss: 0.276916, mae: 0.415718, mean_q: 4.636302
 50256/100000: episode: 803, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 54.668, mean reward: 4.205 [2.328, 8.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.479], loss: 0.440234, mae: 0.386463, mean_q: 4.583075
 50276/100000: episode: 804, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 72.619, mean reward: 3.631 [2.274, 5.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.138, 10.425], loss: 0.165089, mae: 0.412149, mean_q: 4.632132
 50285/100000: episode: 805, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 33.910, mean reward: 3.768 [3.147, 4.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.519], loss: 0.993218, mae: 0.508375, mean_q: 4.815692
 50305/100000: episode: 806, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 69.817, mean reward: 3.491 [2.600, 5.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.496], loss: 0.170679, mae: 0.392055, mean_q: 4.579189
[Info] FALSIFICATION!
 50321/100000: episode: 807, duration: 0.340s, episode steps: 16, steps per second: 47, episode reward: 1138.268, mean reward: 71.142 [5.035, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.760, 10.169], loss: 0.246207, mae: 0.461931, mean_q: 4.829140
 50344/100000: episode: 808, duration: 0.149s, episode steps: 23, steps per second: 154, episode reward: 81.887, mean reward: 3.560 [2.444, 9.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.506], loss: 0.148275, mae: 0.386172, mean_q: 4.633048
 50353/100000: episode: 809, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 30.627, mean reward: 3.403 [2.485, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.429], loss: 0.125725, mae: 0.352923, mean_q: 4.578082
 50376/100000: episode: 810, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 78.882, mean reward: 3.430 [2.620, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.466, 10.501], loss: 668.855957, mae: 2.415710, mean_q: 5.340240
 50392/100000: episode: 811, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 92.187, mean reward: 5.762 [3.286, 18.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.667], loss: 3.897531, mae: 1.927418, mean_q: 4.390825
 50401/100000: episode: 812, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 39.064, mean reward: 4.340 [2.873, 6.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.579], loss: 1701.484375, mae: 4.467169, mean_q: 5.376936
 50424/100000: episode: 813, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 89.876, mean reward: 3.908 [2.155, 6.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.852, 10.347], loss: 3.191669, mae: 1.756992, mean_q: 5.915250
 50453/100000: episode: 814, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 89.685, mean reward: 3.093 [2.051, 5.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.077, 10.325], loss: 0.800110, mae: 0.860902, mean_q: 4.851756
 50466/100000: episode: 815, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 48.677, mean reward: 3.744 [2.428, 6.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-1.136, 10.402], loss: 0.309063, mae: 0.601558, mean_q: 4.998721
 50487/100000: episode: 816, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 116.279, mean reward: 5.537 [3.668, 12.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.521], loss: 0.694245, mae: 0.647943, mean_q: 4.945343
 50510/100000: episode: 817, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 87.803, mean reward: 3.818 [2.236, 6.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.986, 10.345], loss: 1.034344, mae: 0.595684, mean_q: 4.948912
 50533/100000: episode: 818, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 123.136, mean reward: 5.354 [2.722, 12.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.450], loss: 0.288761, mae: 0.524699, mean_q: 4.866040
 50554/100000: episode: 819, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 75.190, mean reward: 3.580 [2.566, 5.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.161, 10.445], loss: 0.763587, mae: 0.567283, mean_q: 4.884428
 50583/100000: episode: 820, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 162.205, mean reward: 5.593 [2.824, 17.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.539], loss: 0.757048, mae: 0.556199, mean_q: 4.909616
 50603/100000: episode: 821, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 58.346, mean reward: 2.917 [2.145, 5.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.211, 10.364], loss: 0.531925, mae: 0.550410, mean_q: 5.014625
 50626/100000: episode: 822, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 83.640, mean reward: 3.637 [2.664, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.304, 10.473], loss: 0.328049, mae: 0.503571, mean_q: 4.897937
 50651/100000: episode: 823, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 78.646, mean reward: 3.146 [2.278, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.513, 10.453], loss: 0.681275, mae: 0.508538, mean_q: 4.931133
 50680/100000: episode: 824, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 109.678, mean reward: 3.782 [1.911, 7.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.711, 10.278], loss: 528.631226, mae: 1.877378, mean_q: 5.362196
 50703/100000: episode: 825, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 83.737, mean reward: 3.641 [2.745, 6.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.489, 10.508], loss: 666.173218, mae: 3.708800, mean_q: 7.370507
 50726/100000: episode: 826, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 89.411, mean reward: 3.887 [2.343, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.209, 10.371], loss: 663.951477, mae: 3.134942, mean_q: 6.866488
 50739/100000: episode: 827, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 46.314, mean reward: 3.563 [2.907, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.385], loss: 0.770478, mae: 0.884399, mean_q: 4.639009
 50762/100000: episode: 828, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 76.872, mean reward: 3.342 [1.746, 5.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.192, 10.387], loss: 0.547653, mae: 0.687077, mean_q: 5.102870
 50785/100000: episode: 829, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 79.911, mean reward: 3.474 [2.238, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.419, 10.371], loss: 0.541862, mae: 0.628061, mean_q: 5.196080
 50806/100000: episode: 830, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 66.887, mean reward: 3.185 [2.247, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.382], loss: 0.515665, mae: 0.606009, mean_q: 5.277588
[Info] FALSIFICATION!
 50817/100000: episode: 831, duration: 0.218s, episode steps: 11, steps per second: 50, episode reward: 1117.066, mean reward: 101.551 [3.900, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.128, 10.541], loss: 0.319998, mae: 0.553542, mean_q: 5.150841
 50826/100000: episode: 832, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 48.725, mean reward: 5.414 [3.702, 8.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.520], loss: 0.466506, mae: 0.605207, mean_q: 5.251876
 50851/100000: episode: 833, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 90.980, mean reward: 3.639 [2.645, 5.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.735, 10.407], loss: 0.452368, mae: 0.575373, mean_q: 5.151145
 50874/100000: episode: 834, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 63.125, mean reward: 2.745 [2.245, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.471], loss: 668.504944, mae: 3.188857, mean_q: 6.725822
 50890/100000: episode: 835, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 73.830, mean reward: 4.614 [3.442, 6.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.537], loss: 0.781587, mae: 0.828182, mean_q: 5.009280
 50911/100000: episode: 836, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 77.749, mean reward: 3.702 [2.884, 5.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.461], loss: 731.931519, mae: 2.738461, mean_q: 5.875822
 50920/100000: episode: 837, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 49.285, mean reward: 5.476 [4.314, 10.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.548], loss: 1692.281006, mae: 8.999044, mean_q: 10.975049
 50940/100000: episode: 838, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 85.184, mean reward: 4.259 [3.082, 7.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.160, 10.483], loss: 7.798115, mae: 2.541793, mean_q: 7.156358
 50963/100000: episode: 839, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 79.429, mean reward: 3.453 [1.812, 8.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.729, 10.306], loss: 2.526388, mae: 1.560097, mean_q: 5.180614
 50972/100000: episode: 840, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 44.743, mean reward: 4.971 [3.376, 6.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.599], loss: 1711.243042, mae: 4.842525, mean_q: 6.123901
 50995/100000: episode: 841, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 79.323, mean reward: 3.449 [2.866, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.193, 10.524], loss: 2.432760, mae: 1.566526, mean_q: 6.684655
 51011/100000: episode: 842, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 52.386, mean reward: 3.274 [1.950, 5.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.480], loss: 955.609741, mae: 2.775989, mean_q: 5.518668
 51031/100000: episode: 843, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 83.404, mean reward: 4.170 [2.632, 7.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.376, 10.474], loss: 3.891736, mae: 1.978980, mean_q: 7.404090
 51060/100000: episode: 844, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 131.283, mean reward: 4.527 [2.273, 16.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.858, 10.405], loss: 528.322021, mae: 2.553676, mean_q: 6.746349
 51076/100000: episode: 845, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 76.135, mean reward: 4.758 [2.415, 8.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.515, 10.454], loss: 1.030318, mae: 0.800022, mean_q: 5.587729
 51099/100000: episode: 846, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 339.203, mean reward: 14.748 [3.282, 230.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.238, 10.519], loss: 1.187813, mae: 0.794421, mean_q: 5.712750
 51119/100000: episode: 847, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 65.218, mean reward: 3.261 [2.282, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.070, 10.418], loss: 0.538103, mae: 0.670210, mean_q: 5.814246
 51142/100000: episode: 848, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 115.553, mean reward: 5.024 [3.830, 7.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.476], loss: 0.729696, mae: 0.672748, mean_q: 5.722599
 51165/100000: episode: 849, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 96.729, mean reward: 4.206 [2.182, 8.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.426], loss: 665.659485, mae: 2.405330, mean_q: 6.171997
 51185/100000: episode: 850, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 60.267, mean reward: 3.013 [1.792, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.476, 10.250], loss: 42.432487, mae: 2.055062, mean_q: 7.048938
 51208/100000: episode: 851, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 94.358, mean reward: 4.103 [2.895, 6.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.557], loss: 1.179373, mae: 0.743071, mean_q: 5.694803
 51231/100000: episode: 852, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 125.062, mean reward: 5.437 [3.283, 14.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.417, 10.632], loss: 9.684834, mae: 3.372782, mean_q: 9.244044
 51260/100000: episode: 853, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 108.703, mean reward: 3.748 [2.676, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.163, 10.465], loss: 1.347750, mae: 0.962631, mean_q: 5.607271
 51283/100000: episode: 854, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 75.480, mean reward: 3.282 [2.472, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.289, 10.527], loss: 0.507938, mae: 0.687964, mean_q: 5.928385
 51306/100000: episode: 855, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 78.070, mean reward: 3.394 [1.973, 6.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.227, 10.385], loss: 0.654085, mae: 0.740970, mean_q: 5.970181
 51315/100000: episode: 856, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 43.623, mean reward: 4.847 [3.780, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.533], loss: 0.773230, mae: 0.683239, mean_q: 6.001913
 51344/100000: episode: 857, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 142.129, mean reward: 4.901 [3.603, 9.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.421, 10.630], loss: 527.726318, mae: 2.653735, mean_q: 7.245328
 51367/100000: episode: 858, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 72.669, mean reward: 3.160 [2.334, 5.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.240], loss: 1.261828, mae: 0.769151, mean_q: 5.573050
 51390/100000: episode: 859, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 58.841, mean reward: 2.558 [1.774, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.408, 10.306], loss: 35.333061, mae: 1.178944, mean_q: 6.314099
 51413/100000: episode: 860, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 65.281, mean reward: 2.838 [2.167, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.391, 10.469], loss: 3.928308, mae: 2.037721, mean_q: 7.693256
 51436/100000: episode: 861, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 101.986, mean reward: 4.434 [2.810, 7.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.869, 10.586], loss: 35.457436, mae: 1.091923, mean_q: 5.938629
 51459/100000: episode: 862, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 99.052, mean reward: 4.307 [2.937, 8.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.812, 10.513], loss: 0.573099, mae: 0.733257, mean_q: 6.215220
 51484/100000: episode: 863, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 110.175, mean reward: 4.407 [2.964, 9.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.479, 10.505], loss: 0.761282, mae: 0.773586, mean_q: 5.900259
 51507/100000: episode: 864, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 95.819, mean reward: 4.166 [2.635, 6.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.584], loss: 0.931828, mae: 0.728063, mean_q: 5.876538
 51528/100000: episode: 865, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 74.750, mean reward: 3.560 [2.406, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.273, 10.434], loss: 0.681034, mae: 0.741981, mean_q: 6.107141
 51551/100000: episode: 866, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 71.743, mean reward: 3.119 [2.216, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.215, 10.357], loss: 666.869507, mae: 2.930680, mean_q: 7.164248
 51564/100000: episode: 867, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 35.385, mean reward: 2.722 [2.131, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-1.088, 10.387], loss: 1.576164, mae: 1.039684, mean_q: 6.049261
 51589/100000: episode: 868, duration: 0.158s, episode steps: 25, steps per second: 159, episode reward: 141.306, mean reward: 5.652 [3.200, 11.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.147, 10.417], loss: 1.048724, mae: 0.856619, mean_q: 5.953888
 51612/100000: episode: 869, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 103.573, mean reward: 4.503 [2.999, 6.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.549], loss: 1.026895, mae: 0.802598, mean_q: 6.047160
 51637/100000: episode: 870, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 137.114, mean reward: 5.485 [3.650, 8.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.768, 10.647], loss: 0.690355, mae: 0.724178, mean_q: 5.957441
 51657/100000: episode: 871, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 53.228, mean reward: 2.661 [2.124, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.111, 10.406], loss: 0.771512, mae: 0.717489, mean_q: 6.032533
 51686/100000: episode: 872, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 134.855, mean reward: 4.650 [2.878, 7.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.547, 10.525], loss: 0.817926, mae: 0.755198, mean_q: 5.961837
 51706/100000: episode: 873, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 93.458, mean reward: 4.673 [3.529, 8.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.107, 10.639], loss: 40.552132, mae: 1.276874, mean_q: 6.467583
 51727/100000: episode: 874, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 79.174, mean reward: 3.770 [2.246, 7.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.480], loss: 0.671489, mae: 0.749023, mean_q: 5.905550
 51743/100000: episode: 875, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 56.949, mean reward: 3.559 [2.158, 11.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.050, 10.409], loss: 0.591965, mae: 0.660370, mean_q: 5.843729
 51768/100000: episode: 876, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 149.092, mean reward: 5.964 [3.113, 22.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.919, 10.573], loss: 1844.213135, mae: 8.144370, mean_q: 10.392360
 51788/100000: episode: 877, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 106.584, mean reward: 5.329 [3.716, 8.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.437, 10.603], loss: 3.659080, mae: 1.890891, mean_q: 7.161736
 51811/100000: episode: 878, duration: 0.116s, episode steps: 23, steps per second: 197, episode reward: 77.060, mean reward: 3.350 [1.997, 5.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.348], loss: 2.194591, mae: 1.222280, mean_q: 5.347393
 51834/100000: episode: 879, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 211.199, mean reward: 9.183 [2.515, 45.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.861, 10.615], loss: 0.700494, mae: 0.755508, mean_q: 6.478411
 51859/100000: episode: 880, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 91.299, mean reward: 3.652 [2.247, 7.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.414], loss: 612.655945, mae: 2.715175, mean_q: 7.517571
 51868/100000: episode: 881, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 71.026, mean reward: 7.892 [4.187, 17.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.678], loss: 1.458524, mae: 1.063428, mean_q: 6.765500
 51889/100000: episode: 882, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 72.102, mean reward: 3.433 [2.689, 4.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.141, 10.583], loss: 727.845825, mae: 2.452256, mean_q: 6.595240
 51898/100000: episode: 883, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 43.433, mean reward: 4.826 [3.947, 5.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.588], loss: 4.745698, mae: 2.627643, mean_q: 8.947763
 51911/100000: episode: 884, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 73.579, mean reward: 5.660 [3.869, 11.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.705], loss: 2.147066, mae: 1.334851, mean_q: 7.391737
 51932/100000: episode: 885, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 70.166, mean reward: 3.341 [2.587, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.521, 10.420], loss: 40.739677, mae: 1.302006, mean_q: 6.357648
 51955/100000: episode: 886, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 89.531, mean reward: 3.893 [2.723, 6.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.459, 10.459], loss: 3.191287, mae: 1.972886, mean_q: 8.416216
 51976/100000: episode: 887, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 63.911, mean reward: 3.043 [2.417, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.305, 10.469], loss: 764.577087, mae: 3.252338, mean_q: 7.638330
 51999/100000: episode: 888, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 76.612, mean reward: 3.331 [2.169, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.267], loss: 697.245972, mae: 2.798369, mean_q: 7.321611
 52022/100000: episode: 889, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 63.975, mean reward: 2.782 [1.990, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.463, 10.357], loss: 2.369051, mae: 1.613236, mean_q: 8.082754
[Info] Complete ISplit Iteration
[Info] Levels: [5.568549, 8.386618, 13.512742]
[Info] Cond. Prob: [0.1, 0.1, 0.03]
[Info] Error Prob: 0.00030000000000000003

 52035/100000: episode: 890, duration: 4.376s, episode steps: 13, steps per second: 3, episode reward: 59.397, mean reward: 4.569 [3.273, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.489], loss: 0.990986, mae: 0.862225, mean_q: 6.174342
 52135/100000: episode: 891, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 180.818, mean reward: 1.808 [1.455, 2.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.575, 10.098], loss: 1.544611, mae: 0.846971, mean_q: 6.673693
 52235/100000: episode: 892, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 179.880, mean reward: 1.799 [1.452, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.914, 10.162], loss: 306.858002, mae: 2.195647, mean_q: 7.151299
 52335/100000: episode: 893, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 188.030, mean reward: 1.880 [1.447, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.408], loss: 24.866318, mae: 1.104114, mean_q: 6.719601
 52435/100000: episode: 894, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.263, mean reward: 1.873 [1.467, 2.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.362, 10.258], loss: 1.123915, mae: 0.758521, mean_q: 6.405419
 52535/100000: episode: 895, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 196.283, mean reward: 1.963 [1.464, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.051, 10.098], loss: 169.683365, mae: 1.554507, mean_q: 6.676100
 52635/100000: episode: 896, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 210.798, mean reward: 2.108 [1.494, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.413, 10.098], loss: 457.851746, mae: 2.639555, mean_q: 7.357612
 52735/100000: episode: 897, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 230.645, mean reward: 2.306 [1.480, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.617, 10.569], loss: 459.479462, mae: 2.691356, mean_q: 7.498440
 52835/100000: episode: 898, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 183.758, mean reward: 1.838 [1.457, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.938, 10.208], loss: 153.743896, mae: 1.417895, mean_q: 6.999998
 52935/100000: episode: 899, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 193.121, mean reward: 1.931 [1.470, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.623, 10.098], loss: 170.010696, mae: 1.608146, mean_q: 6.844308
 53035/100000: episode: 900, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 191.819, mean reward: 1.918 [1.444, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.347, 10.098], loss: 314.061523, mae: 2.242373, mean_q: 7.212206
 53135/100000: episode: 901, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 209.720, mean reward: 2.097 [1.445, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.901, 10.098], loss: 313.754944, mae: 2.066503, mean_q: 7.344635
 53235/100000: episode: 902, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 224.392, mean reward: 2.244 [1.435, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.503], loss: 160.980759, mae: 1.277796, mean_q: 6.385390
 53335/100000: episode: 903, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 182.084, mean reward: 1.821 [1.463, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.985, 10.098], loss: 161.859222, mae: 1.603873, mean_q: 6.918174
 53435/100000: episode: 904, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.179, mean reward: 1.812 [1.473, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.477, 10.098], loss: 319.660797, mae: 2.238936, mean_q: 7.220642
 53535/100000: episode: 905, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.577, mean reward: 1.846 [1.447, 4.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.410, 10.230], loss: 153.680313, mae: 1.373291, mean_q: 6.657109
 53635/100000: episode: 906, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 226.600, mean reward: 2.266 [1.512, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.491, 10.098], loss: 17.730892, mae: 1.315147, mean_q: 6.768840
 53735/100000: episode: 907, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 199.264, mean reward: 1.993 [1.471, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.098], loss: 161.333023, mae: 1.432556, mean_q: 6.433893
 53835/100000: episode: 908, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 184.504, mean reward: 1.845 [1.465, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.249, 10.098], loss: 154.383423, mae: 1.372176, mean_q: 6.511840
 53935/100000: episode: 909, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.116, mean reward: 2.001 [1.476, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.924, 10.098], loss: 9.824429, mae: 0.883109, mean_q: 6.132715
 54035/100000: episode: 910, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 192.501, mean reward: 1.925 [1.467, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.674, 10.196], loss: 153.867477, mae: 1.418604, mean_q: 6.503332
 54135/100000: episode: 911, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 206.357, mean reward: 2.064 [1.454, 6.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.620, 10.098], loss: 762.447876, mae: 3.360266, mean_q: 7.405090
 54235/100000: episode: 912, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.077, mean reward: 1.851 [1.457, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.717, 10.098], loss: 455.511169, mae: 2.262526, mean_q: 7.015043
 54335/100000: episode: 913, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.410, mean reward: 1.954 [1.462, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.593, 10.098], loss: 162.450241, mae: 1.735293, mean_q: 7.010208
 54435/100000: episode: 914, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.847, mean reward: 1.838 [1.457, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.954, 10.113], loss: 1.022986, mae: 0.708455, mean_q: 5.979396
 54535/100000: episode: 915, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.755, mean reward: 1.908 [1.485, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.879, 10.110], loss: 153.011322, mae: 1.146828, mean_q: 6.066731
 54635/100000: episode: 916, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 202.359, mean reward: 2.024 [1.453, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.318], loss: 305.599243, mae: 1.814437, mean_q: 6.373944
 54735/100000: episode: 917, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 205.015, mean reward: 2.050 [1.473, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.824, 10.236], loss: 152.442551, mae: 1.339274, mean_q: 6.319565
 54835/100000: episode: 918, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.284, mean reward: 1.863 [1.463, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: 1.223844, mae: 0.709201, mean_q: 5.841733
 54935/100000: episode: 919, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.947, mean reward: 1.909 [1.489, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.047, 10.208], loss: 8.894303, mae: 0.741604, mean_q: 5.643760
 55035/100000: episode: 920, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.519, mean reward: 1.855 [1.478, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.800, 10.098], loss: 0.854715, mae: 0.648738, mean_q: 5.623500
 55135/100000: episode: 921, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 184.964, mean reward: 1.850 [1.450, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.044, 10.098], loss: 0.688857, mae: 0.618633, mean_q: 5.452687
 55235/100000: episode: 922, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 193.766, mean reward: 1.938 [1.493, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.483, 10.307], loss: 313.233582, mae: 1.777051, mean_q: 6.025709
 55335/100000: episode: 923, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 211.629, mean reward: 2.116 [1.522, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.232, 10.305], loss: 161.606842, mae: 1.387098, mean_q: 5.978385
 55435/100000: episode: 924, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.148, mean reward: 1.971 [1.460, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.807, 10.296], loss: 8.720987, mae: 0.637832, mean_q: 5.363036
 55535/100000: episode: 925, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.890, mean reward: 1.959 [1.458, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.201, 10.135], loss: 8.958828, mae: 0.714027, mean_q: 5.362521
 55635/100000: episode: 926, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 191.492, mean reward: 1.915 [1.455, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.372, 10.098], loss: 0.703329, mae: 0.520433, mean_q: 5.042410
 55735/100000: episode: 927, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 186.597, mean reward: 1.866 [1.486, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.820, 10.183], loss: 8.285445, mae: 0.560938, mean_q: 4.957711
 55835/100000: episode: 928, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 194.158, mean reward: 1.942 [1.464, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.664, 10.098], loss: 0.649145, mae: 0.500858, mean_q: 4.888747
 55935/100000: episode: 929, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.398, mean reward: 1.854 [1.460, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.451, 10.222], loss: 0.346451, mae: 0.464239, mean_q: 4.742639
 56035/100000: episode: 930, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 267.381, mean reward: 2.674 [1.525, 8.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.361, 10.098], loss: 0.545416, mae: 0.451128, mean_q: 4.658846
 56135/100000: episode: 931, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 184.413, mean reward: 1.844 [1.459, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.558, 10.098], loss: 0.312463, mae: 0.442141, mean_q: 4.584119
 56235/100000: episode: 932, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.616, mean reward: 1.916 [1.489, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.344, 10.098], loss: 0.247413, mae: 0.411052, mean_q: 4.505778
 56335/100000: episode: 933, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 201.585, mean reward: 2.016 [1.447, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.671, 10.286], loss: 0.276386, mae: 0.401402, mean_q: 4.442215
 56435/100000: episode: 934, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.374, mean reward: 1.844 [1.439, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.098], loss: 0.527995, mae: 0.428452, mean_q: 4.428783
 56535/100000: episode: 935, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 201.844, mean reward: 2.018 [1.465, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.698, 10.098], loss: 0.538058, mae: 0.421340, mean_q: 4.366479
 56635/100000: episode: 936, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.823, mean reward: 1.868 [1.447, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.639, 10.098], loss: 0.705532, mae: 0.410257, mean_q: 4.261490
 56735/100000: episode: 937, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.738, mean reward: 1.867 [1.449, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.486, 10.299], loss: 0.161418, mae: 0.354691, mean_q: 4.136676
 56835/100000: episode: 938, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.117, mean reward: 1.791 [1.465, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.246, 10.098], loss: 0.125302, mae: 0.329875, mean_q: 3.995075
 56935/100000: episode: 939, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.053, mean reward: 1.951 [1.456, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.872, 10.211], loss: 0.102725, mae: 0.308543, mean_q: 3.920053
 57035/100000: episode: 940, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 199.426, mean reward: 1.994 [1.512, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.753, 10.098], loss: 0.085581, mae: 0.293375, mean_q: 3.866895
 57135/100000: episode: 941, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 217.320, mean reward: 2.173 [1.483, 5.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.638, 10.098], loss: 0.098027, mae: 0.308739, mean_q: 3.878763
 57235/100000: episode: 942, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 180.763, mean reward: 1.808 [1.481, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.311, 10.206], loss: 0.117152, mae: 0.319033, mean_q: 3.909307
 57335/100000: episode: 943, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 205.593, mean reward: 2.056 [1.447, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.917, 10.098], loss: 0.087495, mae: 0.293017, mean_q: 3.887434
 57435/100000: episode: 944, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.358, mean reward: 1.964 [1.463, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.261, 10.394], loss: 0.089782, mae: 0.295517, mean_q: 3.875449
 57535/100000: episode: 945, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 225.216, mean reward: 2.252 [1.489, 4.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.627, 10.098], loss: 0.106559, mae: 0.311627, mean_q: 3.900167
 57635/100000: episode: 946, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.712, mean reward: 1.877 [1.432, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.717, 10.173], loss: 0.085223, mae: 0.292962, mean_q: 3.895991
 57735/100000: episode: 947, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 200.232, mean reward: 2.002 [1.467, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.909, 10.107], loss: 0.118528, mae: 0.323024, mean_q: 3.911764
 57835/100000: episode: 948, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 185.230, mean reward: 1.852 [1.449, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.540, 10.232], loss: 0.102511, mae: 0.311594, mean_q: 3.903665
 57935/100000: episode: 949, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.581, mean reward: 1.856 [1.448, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.142], loss: 0.094854, mae: 0.299707, mean_q: 3.875048
 58035/100000: episode: 950, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 200.075, mean reward: 2.001 [1.472, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.232, 10.098], loss: 0.096901, mae: 0.318145, mean_q: 3.909366
 58135/100000: episode: 951, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 237.108, mean reward: 2.371 [1.471, 5.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.201, 10.098], loss: 0.099143, mae: 0.306480, mean_q: 3.881141
 58235/100000: episode: 952, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 196.268, mean reward: 1.963 [1.461, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.978, 10.101], loss: 0.113552, mae: 0.310489, mean_q: 3.884087
 58335/100000: episode: 953, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.496, mean reward: 1.925 [1.457, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.098], loss: 0.104982, mae: 0.318877, mean_q: 3.897907
 58435/100000: episode: 954, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 197.377, mean reward: 1.974 [1.461, 8.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.487, 10.171], loss: 0.116812, mae: 0.322032, mean_q: 3.900255
 58535/100000: episode: 955, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 200.785, mean reward: 2.008 [1.504, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.729, 10.278], loss: 0.095041, mae: 0.303754, mean_q: 3.894240
 58635/100000: episode: 956, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.385, mean reward: 1.854 [1.447, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.019, 10.123], loss: 0.113210, mae: 0.315669, mean_q: 3.889808
 58735/100000: episode: 957, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.682, mean reward: 1.847 [1.467, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.098], loss: 0.095770, mae: 0.302814, mean_q: 3.870389
 58835/100000: episode: 958, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 195.548, mean reward: 1.955 [1.440, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.672, 10.198], loss: 0.109302, mae: 0.313202, mean_q: 3.892024
 58935/100000: episode: 959, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.205, mean reward: 2.112 [1.458, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.101], loss: 0.107805, mae: 0.318320, mean_q: 3.913585
 59035/100000: episode: 960, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.259, mean reward: 1.893 [1.467, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.572, 10.098], loss: 0.103579, mae: 0.305351, mean_q: 3.871534
 59135/100000: episode: 961, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 179.535, mean reward: 1.795 [1.459, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.183, 10.211], loss: 0.092651, mae: 0.300073, mean_q: 3.882672
 59235/100000: episode: 962, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 211.425, mean reward: 2.114 [1.497, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.379], loss: 0.101237, mae: 0.305448, mean_q: 3.869896
 59335/100000: episode: 963, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.729, mean reward: 1.827 [1.458, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.272, 10.098], loss: 0.082031, mae: 0.291248, mean_q: 3.855716
 59435/100000: episode: 964, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 193.956, mean reward: 1.940 [1.457, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.987, 10.473], loss: 0.084382, mae: 0.296132, mean_q: 3.871591
 59535/100000: episode: 965, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.049, mean reward: 1.860 [1.452, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.641, 10.098], loss: 0.085569, mae: 0.293522, mean_q: 3.877763
 59635/100000: episode: 966, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 214.516, mean reward: 2.145 [1.477, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.507, 10.548], loss: 0.090224, mae: 0.307464, mean_q: 3.897469
 59735/100000: episode: 967, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.397, mean reward: 1.884 [1.491, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.390, 10.145], loss: 0.093248, mae: 0.301656, mean_q: 3.892583
 59835/100000: episode: 968, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.524, mean reward: 1.935 [1.464, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.753, 10.200], loss: 0.105538, mae: 0.313091, mean_q: 3.897886
 59935/100000: episode: 969, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 199.282, mean reward: 1.993 [1.436, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.154, 10.156], loss: 0.094605, mae: 0.303717, mean_q: 3.905795
 60035/100000: episode: 970, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 217.578, mean reward: 2.176 [1.454, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.895, 10.098], loss: 0.098026, mae: 0.302313, mean_q: 3.894256
 60135/100000: episode: 971, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 187.083, mean reward: 1.871 [1.513, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.991, 10.098], loss: 0.099005, mae: 0.311491, mean_q: 3.917289
 60235/100000: episode: 972, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.700, mean reward: 1.937 [1.438, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.767, 10.098], loss: 0.112960, mae: 0.320502, mean_q: 3.930651
 60335/100000: episode: 973, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.465, mean reward: 1.965 [1.458, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.682, 10.370], loss: 0.097644, mae: 0.305168, mean_q: 3.899501
 60435/100000: episode: 974, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 188.630, mean reward: 1.886 [1.464, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.135, 10.219], loss: 0.088984, mae: 0.288905, mean_q: 3.902147
 60535/100000: episode: 975, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 191.394, mean reward: 1.914 [1.472, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.865, 10.098], loss: 0.099044, mae: 0.307355, mean_q: 3.895310
 60635/100000: episode: 976, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 187.252, mean reward: 1.873 [1.462, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.130, 10.163], loss: 0.090369, mae: 0.300053, mean_q: 3.903785
 60735/100000: episode: 977, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.246, mean reward: 2.012 [1.485, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.844, 10.098], loss: 0.115716, mae: 0.314657, mean_q: 3.902367
 60835/100000: episode: 978, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 205.672, mean reward: 2.057 [1.491, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.929, 10.098], loss: 0.092483, mae: 0.292873, mean_q: 3.902900
 60935/100000: episode: 979, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.552, mean reward: 1.866 [1.444, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.677, 10.098], loss: 0.090319, mae: 0.302579, mean_q: 3.910102
 61035/100000: episode: 980, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 208.642, mean reward: 2.086 [1.473, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.045, 10.098], loss: 0.076752, mae: 0.279081, mean_q: 3.871934
 61135/100000: episode: 981, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.226, mean reward: 1.912 [1.469, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.152], loss: 0.091134, mae: 0.289616, mean_q: 3.887665
 61235/100000: episode: 982, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 193.695, mean reward: 1.937 [1.467, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.881, 10.204], loss: 0.085197, mae: 0.288819, mean_q: 3.863846
 61335/100000: episode: 983, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 198.140, mean reward: 1.981 [1.454, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.242, 10.295], loss: 0.077880, mae: 0.285465, mean_q: 3.885280
 61435/100000: episode: 984, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 217.814, mean reward: 2.178 [1.554, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.543, 10.098], loss: 0.083231, mae: 0.285490, mean_q: 3.871854
 61535/100000: episode: 985, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.887, mean reward: 1.919 [1.440, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.682, 10.355], loss: 0.084756, mae: 0.284622, mean_q: 3.894923
 61635/100000: episode: 986, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 175.579, mean reward: 1.756 [1.446, 2.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.109, 10.115], loss: 0.086263, mae: 0.287975, mean_q: 3.878304
 61735/100000: episode: 987, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.390, mean reward: 1.964 [1.455, 4.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.379, 10.136], loss: 0.084698, mae: 0.289592, mean_q: 3.899887
 61835/100000: episode: 988, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.330, mean reward: 1.923 [1.446, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.856, 10.098], loss: 0.092972, mae: 0.298042, mean_q: 3.892782
 61935/100000: episode: 989, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 194.883, mean reward: 1.949 [1.478, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.416, 10.098], loss: 0.088097, mae: 0.297627, mean_q: 3.903291
[Info] 1-TH LEVEL FOUND: 5.5864033699035645, Considering 10/90 traces
 62035/100000: episode: 990, duration: 4.646s, episode steps: 100, steps per second: 22, episode reward: 190.542, mean reward: 1.905 [1.473, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.853, 10.098], loss: 0.086977, mae: 0.292521, mean_q: 3.890913
 62058/100000: episode: 991, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 57.284, mean reward: 2.491 [1.885, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.145, 10.100], loss: 0.082537, mae: 0.286102, mean_q: 3.905945
 62065/100000: episode: 992, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 18.792, mean reward: 2.685 [2.281, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.211, 10.100], loss: 0.085218, mae: 0.284975, mean_q: 3.875500
 62086/100000: episode: 993, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 71.482, mean reward: 3.404 [2.491, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.726, 10.501], loss: 0.109387, mae: 0.304050, mean_q: 3.902193
 62110/100000: episode: 994, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 64.649, mean reward: 2.694 [1.888, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.057, 10.100], loss: 0.098593, mae: 0.296519, mean_q: 3.929121
 62117/100000: episode: 995, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 18.743, mean reward: 2.678 [2.441, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.319, 10.100], loss: 0.059897, mae: 0.254242, mean_q: 3.827608
 62129/100000: episode: 996, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 45.787, mean reward: 3.816 [2.794, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.761, 10.600], loss: 0.074442, mae: 0.279852, mean_q: 3.867054
 62150/100000: episode: 997, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 50.459, mean reward: 2.403 [1.873, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.465], loss: 0.111054, mae: 0.309655, mean_q: 3.961845
 62164/100000: episode: 998, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 42.418, mean reward: 3.030 [2.556, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.459], loss: 0.082999, mae: 0.278867, mean_q: 3.886608
 62187/100000: episode: 999, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 67.917, mean reward: 2.953 [1.917, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.191, 10.100], loss: 0.069861, mae: 0.271529, mean_q: 3.921769
 62208/100000: episode: 1000, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 49.135, mean reward: 2.340 [1.714, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.340, 10.100], loss: 0.103001, mae: 0.280741, mean_q: 3.924085
 62231/100000: episode: 1001, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 61.357, mean reward: 2.668 [2.106, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.419, 10.100], loss: 0.078274, mae: 0.279616, mean_q: 3.960952
 62243/100000: episode: 1002, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 37.648, mean reward: 3.137 [2.402, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.520], loss: 0.080637, mae: 0.284206, mean_q: 3.990856
 62264/100000: episode: 1003, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 57.986, mean reward: 2.761 [1.621, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.251], loss: 0.089660, mae: 0.287520, mean_q: 3.946445
 62285/100000: episode: 1004, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 68.989, mean reward: 3.285 [2.389, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.370, 10.100], loss: 0.088530, mae: 0.294909, mean_q: 3.936542
 62306/100000: episode: 1005, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 54.926, mean reward: 2.616 [2.194, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.565, 10.100], loss: 0.082816, mae: 0.292176, mean_q: 3.945809
 62327/100000: episode: 1006, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 73.345, mean reward: 3.493 [2.497, 6.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.346, 10.100], loss: 0.079773, mae: 0.275618, mean_q: 3.961104
 62353/100000: episode: 1007, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 59.654, mean reward: 2.294 [1.586, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.333, 10.100], loss: 0.092512, mae: 0.296213, mean_q: 3.972082
 62374/100000: episode: 1008, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 54.228, mean reward: 2.582 [1.863, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.325, 10.100], loss: 0.133736, mae: 0.339909, mean_q: 4.001561
 62397/100000: episode: 1009, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 52.630, mean reward: 2.288 [1.880, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.402, 10.100], loss: 0.068714, mae: 0.277087, mean_q: 3.967487
 62404/100000: episode: 1010, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 16.475, mean reward: 2.354 [2.233, 2.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.228, 10.100], loss: 0.091825, mae: 0.281360, mean_q: 3.970224
 62427/100000: episode: 1011, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 53.419, mean reward: 2.323 [1.987, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.925, 10.100], loss: 0.081096, mae: 0.297855, mean_q: 4.002738
 62453/100000: episode: 1012, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 94.074, mean reward: 3.618 [2.719, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.477, 10.100], loss: 0.104835, mae: 0.320550, mean_q: 3.993262
 62474/100000: episode: 1013, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 60.384, mean reward: 2.875 [2.097, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.190, 10.462], loss: 0.078735, mae: 0.289205, mean_q: 4.006365
 62488/100000: episode: 1014, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 57.969, mean reward: 4.141 [3.586, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.263, 10.461], loss: 0.107271, mae: 0.320684, mean_q: 4.055043
 62500/100000: episode: 1015, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 34.783, mean reward: 2.899 [2.329, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.251, 10.516], loss: 0.074488, mae: 0.278571, mean_q: 4.005474
 62526/100000: episode: 1016, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 80.526, mean reward: 3.097 [2.291, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.234, 10.100], loss: 0.099705, mae: 0.316875, mean_q: 4.055365
 62547/100000: episode: 1017, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 104.850, mean reward: 4.993 [2.314, 20.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.105, 10.495], loss: 0.305283, mae: 0.337952, mean_q: 4.009007
 62571/100000: episode: 1018, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 61.451, mean reward: 2.560 [1.520, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.130, 10.100], loss: 0.181547, mae: 0.380348, mean_q: 4.062924
 62597/100000: episode: 1019, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 72.652, mean reward: 2.794 [1.801, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.726, 10.100], loss: 0.110458, mae: 0.329446, mean_q: 4.068237
 62604/100000: episode: 1020, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 22.595, mean reward: 3.228 [2.406, 5.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.276, 10.100], loss: 0.105493, mae: 0.322339, mean_q: 4.099563
 62618/100000: episode: 1021, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 51.376, mean reward: 3.670 [2.768, 5.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.749, 10.508], loss: 0.092131, mae: 0.294381, mean_q: 4.046821
 62639/100000: episode: 1022, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 59.546, mean reward: 2.836 [1.839, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.276, 10.366], loss: 0.168259, mae: 0.345975, mean_q: 4.141997
 62646/100000: episode: 1023, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 22.097, mean reward: 3.157 [2.604, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.296, 10.100], loss: 0.151926, mae: 0.376624, mean_q: 3.933000
 62653/100000: episode: 1024, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 16.746, mean reward: 2.392 [2.143, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.299, 10.100], loss: 0.118370, mae: 0.376401, mean_q: 4.250979
 62667/100000: episode: 1025, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 52.991, mean reward: 3.785 [2.768, 6.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.500], loss: 0.119406, mae: 0.331580, mean_q: 4.058791
 62688/100000: episode: 1026, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 62.953, mean reward: 2.998 [2.208, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.110, 10.439], loss: 0.147172, mae: 0.362901, mean_q: 4.150839
 62711/100000: episode: 1027, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 55.082, mean reward: 2.395 [1.631, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.423, 10.100], loss: 0.115504, mae: 0.321456, mean_q: 4.120454
 62723/100000: episode: 1028, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 30.177, mean reward: 2.515 [2.023, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.348], loss: 0.118172, mae: 0.331514, mean_q: 4.062574
 62749/100000: episode: 1029, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 92.678, mean reward: 3.565 [2.365, 7.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.471, 10.100], loss: 0.140898, mae: 0.357029, mean_q: 4.160389
 62761/100000: episode: 1030, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 28.533, mean reward: 2.378 [1.938, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.144, 10.349], loss: 0.119822, mae: 0.319913, mean_q: 4.065016
 62784/100000: episode: 1031, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 69.381, mean reward: 3.017 [1.672, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.267, 10.100], loss: 0.098207, mae: 0.319050, mean_q: 4.091856
 62798/100000: episode: 1032, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 49.228, mean reward: 3.516 [2.844, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.555], loss: 0.110741, mae: 0.311124, mean_q: 4.088487
 62824/100000: episode: 1033, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 86.588, mean reward: 3.330 [1.999, 6.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.762, 10.100], loss: 0.129094, mae: 0.343162, mean_q: 4.195604
 62847/100000: episode: 1034, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 74.018, mean reward: 3.218 [2.548, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.485, 10.100], loss: 0.118099, mae: 0.346038, mean_q: 4.160547
 62868/100000: episode: 1035, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 67.302, mean reward: 3.205 [2.490, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.724, 10.477], loss: 0.128016, mae: 0.356878, mean_q: 4.181231
 62891/100000: episode: 1036, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 80.070, mean reward: 3.481 [2.559, 5.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.365, 10.100], loss: 0.150957, mae: 0.363835, mean_q: 4.183131
 62917/100000: episode: 1037, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 95.175, mean reward: 3.661 [2.807, 5.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.640, 10.100], loss: 0.130047, mae: 0.358012, mean_q: 4.162671
 62943/100000: episode: 1038, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 73.153, mean reward: 2.814 [2.225, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.401, 10.100], loss: 0.162569, mae: 0.365454, mean_q: 4.235842
 62969/100000: episode: 1039, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 142.642, mean reward: 5.486 [2.563, 15.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.907, 10.100], loss: 0.174957, mae: 0.370364, mean_q: 4.222047
 62995/100000: episode: 1040, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 78.724, mean reward: 3.028 [1.745, 5.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.095, 10.100], loss: 0.394832, mae: 0.425739, mean_q: 4.313276
 63009/100000: episode: 1041, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 53.960, mean reward: 3.854 [2.763, 5.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.185, 10.554], loss: 0.163077, mae: 0.369648, mean_q: 4.264364
 63023/100000: episode: 1042, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 34.633, mean reward: 2.474 [1.958, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.361], loss: 0.136988, mae: 0.373955, mean_q: 4.237497
 63030/100000: episode: 1043, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 17.148, mean reward: 2.450 [2.170, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.333, 10.100], loss: 0.447600, mae: 0.427338, mean_q: 4.464181
 63051/100000: episode: 1044, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 61.064, mean reward: 2.908 [2.193, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.444], loss: 0.125403, mae: 0.367877, mean_q: 4.218080
 63074/100000: episode: 1045, duration: 0.123s, episode steps: 23, steps per second: 186, episode reward: 60.587, mean reward: 2.634 [1.757, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.093, 10.100], loss: 0.182094, mae: 0.395946, mean_q: 4.301654
 63097/100000: episode: 1046, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 56.351, mean reward: 2.450 [1.901, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.146, 10.100], loss: 0.125719, mae: 0.362425, mean_q: 4.214183
 63111/100000: episode: 1047, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 37.682, mean reward: 2.692 [1.967, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.438], loss: 0.280371, mae: 0.383795, mean_q: 4.347195
 63134/100000: episode: 1048, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 63.076, mean reward: 2.742 [1.807, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.516, 10.100], loss: 0.137702, mae: 0.365743, mean_q: 4.203517
 63158/100000: episode: 1049, duration: 0.129s, episode steps: 24, steps per second: 185, episode reward: 90.751, mean reward: 3.781 [1.756, 17.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.396, 10.100], loss: 0.165711, mae: 0.376938, mean_q: 4.247999
 63172/100000: episode: 1050, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 37.972, mean reward: 2.712 [2.155, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.383], loss: 0.236373, mae: 0.403354, mean_q: 4.289519
 63195/100000: episode: 1051, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 52.366, mean reward: 2.277 [1.806, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.242, 10.100], loss: 0.185377, mae: 0.388856, mean_q: 4.304367
 63216/100000: episode: 1052, duration: 0.140s, episode steps: 21, steps per second: 149, episode reward: 67.450, mean reward: 3.212 [2.098, 8.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.505, 10.100], loss: 0.361860, mae: 0.397891, mean_q: 4.321717
 63237/100000: episode: 1053, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 61.167, mean reward: 2.913 [2.445, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.394, 10.100], loss: 0.362995, mae: 0.410807, mean_q: 4.364902
 63258/100000: episode: 1054, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 97.788, mean reward: 4.657 [2.977, 7.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.322, 10.100], loss: 0.164769, mae: 0.374620, mean_q: 4.319821
 63270/100000: episode: 1055, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 33.822, mean reward: 2.818 [2.402, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.313, 10.404], loss: 0.186623, mae: 0.396194, mean_q: 4.344503
 63282/100000: episode: 1056, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 42.453, mean reward: 3.538 [2.762, 4.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.518], loss: 0.190968, mae: 0.419472, mean_q: 4.400226
 63296/100000: episode: 1057, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 46.027, mean reward: 3.288 [2.525, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.421], loss: 0.202134, mae: 0.428562, mean_q: 4.345572
 63319/100000: episode: 1058, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 58.208, mean reward: 2.531 [1.985, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.026, 10.100], loss: 0.373856, mae: 0.456129, mean_q: 4.393099
 63326/100000: episode: 1059, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 19.549, mean reward: 2.793 [2.395, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.347, 10.100], loss: 0.086830, mae: 0.304238, mean_q: 4.212256
 63349/100000: episode: 1060, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 63.372, mean reward: 2.755 [2.211, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.384, 10.100], loss: 0.194747, mae: 0.388169, mean_q: 4.383712
 63372/100000: episode: 1061, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 63.943, mean reward: 2.780 [1.964, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.339, 10.100], loss: 0.130160, mae: 0.362185, mean_q: 4.416057
 63398/100000: episode: 1062, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 62.713, mean reward: 2.412 [1.752, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.341, 10.100], loss: 0.340131, mae: 0.394000, mean_q: 4.405405
 63424/100000: episode: 1063, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 71.875, mean reward: 2.764 [1.453, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.397, 10.113], loss: 0.193637, mae: 0.411529, mean_q: 4.389613
 63445/100000: episode: 1064, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 52.237, mean reward: 2.487 [1.865, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.371], loss: 0.292900, mae: 0.396903, mean_q: 4.357022
 63468/100000: episode: 1065, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 58.882, mean reward: 2.560 [2.184, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.306, 10.100], loss: 0.261314, mae: 0.423487, mean_q: 4.452209
 63482/100000: episode: 1066, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 38.994, mean reward: 2.785 [1.873, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.876, 10.407], loss: 0.158971, mae: 0.394204, mean_q: 4.474805
 63489/100000: episode: 1067, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 20.027, mean reward: 2.861 [2.602, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.372, 10.100], loss: 0.151834, mae: 0.372270, mean_q: 4.378072
 63512/100000: episode: 1068, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 55.912, mean reward: 2.431 [1.910, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.284, 10.100], loss: 0.296145, mae: 0.397430, mean_q: 4.410294
 63538/100000: episode: 1069, duration: 0.168s, episode steps: 26, steps per second: 155, episode reward: 82.072, mean reward: 3.157 [2.570, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.389, 10.100], loss: 0.177070, mae: 0.390773, mean_q: 4.388570
 63564/100000: episode: 1070, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 70.614, mean reward: 2.716 [2.208, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.179, 10.100], loss: 0.357089, mae: 0.422879, mean_q: 4.406760
 63587/100000: episode: 1071, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 67.227, mean reward: 2.923 [2.286, 5.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.313, 10.100], loss: 0.359983, mae: 0.431148, mean_q: 4.524068
 63611/100000: episode: 1072, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 77.400, mean reward: 3.225 [2.778, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.277, 10.100], loss: 0.150535, mae: 0.377931, mean_q: 4.423193
 63637/100000: episode: 1073, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 59.070, mean reward: 2.272 [1.642, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.458, 10.100], loss: 0.239447, mae: 0.372811, mean_q: 4.412337
 63660/100000: episode: 1074, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 56.595, mean reward: 2.461 [1.796, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.038, 10.100], loss: 0.192631, mae: 0.394434, mean_q: 4.498778
 63667/100000: episode: 1075, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 22.405, mean reward: 3.201 [2.553, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.366, 10.100], loss: 0.147575, mae: 0.382190, mean_q: 4.450260
 63679/100000: episode: 1076, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 27.866, mean reward: 2.322 [1.923, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.283], loss: 0.451440, mae: 0.384739, mean_q: 4.418887
 63686/100000: episode: 1077, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 16.737, mean reward: 2.391 [2.068, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.344, 10.100], loss: 0.168075, mae: 0.417597, mean_q: 4.700713
 63712/100000: episode: 1078, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 101.198, mean reward: 3.892 [2.691, 6.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.593, 10.100], loss: 0.182069, mae: 0.411304, mean_q: 4.523685
 63724/100000: episode: 1079, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 36.373, mean reward: 3.031 [2.410, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.486], loss: 0.342624, mae: 0.412333, mean_q: 4.520046
[Info] 2-TH LEVEL FOUND: 7.8563714027404785, Considering 10/90 traces
 63748/100000: episode: 1080, duration: 4.148s, episode steps: 24, steps per second: 6, episode reward: 78.249, mean reward: 3.260 [2.282, 4.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.596, 10.100], loss: 0.173922, mae: 0.410486, mean_q: 4.481392
 63757/100000: episode: 1081, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 56.985, mean reward: 6.332 [3.147, 14.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.069, 10.558], loss: 0.143039, mae: 0.400629, mean_q: 4.482384
 63766/100000: episode: 1082, duration: 0.056s, episode steps: 9, steps per second: 159, episode reward: 27.704, mean reward: 3.078 [2.867, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.472], loss: 0.397116, mae: 0.422233, mean_q: 4.623560
 63773/100000: episode: 1083, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 17.869, mean reward: 2.553 [2.139, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.279], loss: 0.309658, mae: 0.485521, mean_q: 4.610068
 63787/100000: episode: 1084, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 55.723, mean reward: 3.980 [3.234, 5.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.550], loss: 0.186451, mae: 0.423574, mean_q: 4.419882
 63795/100000: episode: 1085, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 29.188, mean reward: 3.648 [3.362, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.517], loss: 0.412982, mae: 0.440852, mean_q: 4.604043
 63803/100000: episode: 1086, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 30.660, mean reward: 3.832 [2.924, 5.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.619], loss: 0.133097, mae: 0.342811, mean_q: 4.418926
 63811/100000: episode: 1087, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 28.017, mean reward: 3.502 [2.831, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.277, 10.466], loss: 0.109463, mae: 0.366109, mean_q: 4.537841
 63824/100000: episode: 1088, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 36.196, mean reward: 2.784 [2.367, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.446], loss: 0.317526, mae: 0.433148, mean_q: 4.606741
 63838/100000: episode: 1089, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 58.275, mean reward: 4.162 [3.267, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.682, 10.413], loss: 0.207627, mae: 0.412948, mean_q: 4.640663
 63851/100000: episode: 1090, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 49.672, mean reward: 3.821 [2.968, 5.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.462, 10.451], loss: 0.411943, mae: 0.452865, mean_q: 4.642065
 63860/100000: episode: 1091, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 27.777, mean reward: 3.086 [2.545, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.439], loss: 0.270986, mae: 0.465045, mean_q: 4.719831
 63867/100000: episode: 1092, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 17.765, mean reward: 2.538 [2.091, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.784, 10.411], loss: 0.166884, mae: 0.396392, mean_q: 4.477793
 63875/100000: episode: 1093, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 19.210, mean reward: 2.401 [2.004, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.365], loss: 0.633293, mae: 0.557406, mean_q: 4.874386
 63888/100000: episode: 1094, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 40.235, mean reward: 3.095 [2.288, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.655, 10.423], loss: 0.189589, mae: 0.425679, mean_q: 4.671995
 63901/100000: episode: 1095, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 43.600, mean reward: 3.354 [2.666, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.184, 10.458], loss: 0.179983, mae: 0.402400, mean_q: 4.547725
 63914/100000: episode: 1096, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 65.453, mean reward: 5.035 [2.901, 7.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.633], loss: 0.316625, mae: 0.413426, mean_q: 4.552012
 63923/100000: episode: 1097, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 39.956, mean reward: 4.440 [3.443, 5.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.259, 10.402], loss: 0.276221, mae: 0.477215, mean_q: 4.581270
 63940/100000: episode: 1098, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 76.422, mean reward: 4.495 [2.745, 6.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.222, 10.558], loss: 0.512664, mae: 0.483464, mean_q: 4.572089
 63947/100000: episode: 1099, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 19.396, mean reward: 2.771 [2.343, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.371], loss: 0.351375, mae: 0.518346, mean_q: 4.717937
 63964/100000: episode: 1100, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 70.808, mean reward: 4.165 [3.296, 6.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.149, 10.501], loss: 0.464828, mae: 0.480484, mean_q: 4.731792
 63981/100000: episode: 1101, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 52.073, mean reward: 3.063 [2.632, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.482, 10.442], loss: 0.127953, mae: 0.359477, mean_q: 4.518137
 63990/100000: episode: 1102, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 48.192, mean reward: 5.355 [2.980, 11.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.405], loss: 0.871510, mae: 0.595184, mean_q: 4.770333
 63999/100000: episode: 1103, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 54.977, mean reward: 6.109 [2.908, 11.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-1.147, 10.574], loss: 0.767122, mae: 0.576803, mean_q: 4.632375
 64016/100000: episode: 1104, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 81.773, mean reward: 4.810 [3.354, 6.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.583], loss: 0.461828, mae: 0.484040, mean_q: 4.749285
 64025/100000: episode: 1105, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 34.261, mean reward: 3.807 [3.194, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.352, 10.560], loss: 0.704763, mae: 0.633165, mean_q: 4.834266
 64032/100000: episode: 1106, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 27.306, mean reward: 3.901 [3.549, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.515], loss: 0.163019, mae: 0.389129, mean_q: 4.443747
 64039/100000: episode: 1107, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 29.509, mean reward: 4.216 [3.380, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.589], loss: 0.212689, mae: 0.470933, mean_q: 5.003715
 64047/100000: episode: 1108, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 27.762, mean reward: 3.470 [2.958, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.527], loss: 0.192033, mae: 0.417810, mean_q: 4.606342
 64056/100000: episode: 1109, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 26.587, mean reward: 2.954 [2.476, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.430], loss: 0.168862, mae: 0.403488, mean_q: 4.570551
 64064/100000: episode: 1110, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 21.397, mean reward: 2.675 [2.090, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.343], loss: 0.204245, mae: 0.429034, mean_q: 4.768240
 64071/100000: episode: 1111, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 23.785, mean reward: 3.398 [2.742, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.498], loss: 0.150509, mae: 0.374513, mean_q: 4.624996
 64080/100000: episode: 1112, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 28.584, mean reward: 3.176 [2.720, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.461], loss: 0.188856, mae: 0.404033, mean_q: 4.615136
 64087/100000: episode: 1113, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 19.960, mean reward: 2.851 [2.251, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.452], loss: 0.731228, mae: 0.518603, mean_q: 4.726776
 64104/100000: episode: 1114, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 79.595, mean reward: 4.682 [3.464, 6.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.199, 10.629], loss: 0.229650, mae: 0.468023, mean_q: 4.826754
 64111/100000: episode: 1115, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 21.087, mean reward: 3.012 [2.760, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.507], loss: 0.146483, mae: 0.381089, mean_q: 4.909756
 64120/100000: episode: 1116, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 37.401, mean reward: 4.156 [2.721, 9.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.516], loss: 0.142981, mae: 0.372460, mean_q: 4.563750
 64133/100000: episode: 1117, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 40.487, mean reward: 3.114 [2.134, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.438], loss: 0.181614, mae: 0.406770, mean_q: 4.731595
 64140/100000: episode: 1118, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 23.829, mean reward: 3.404 [2.285, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.429], loss: 0.156406, mae: 0.393730, mean_q: 4.776326
 64153/100000: episode: 1119, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 40.113, mean reward: 3.086 [2.043, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.618, 10.388], loss: 0.378950, mae: 0.463984, mean_q: 4.864356
 64162/100000: episode: 1120, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 31.349, mean reward: 3.483 [3.072, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.490], loss: 0.210898, mae: 0.411320, mean_q: 4.795665
 64170/100000: episode: 1121, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 43.923, mean reward: 5.490 [4.101, 7.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.589], loss: 0.114642, mae: 0.333931, mean_q: 4.718333
 64183/100000: episode: 1122, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 40.224, mean reward: 3.094 [2.282, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.715, 10.354], loss: 0.381461, mae: 0.418616, mean_q: 4.706602
 64192/100000: episode: 1123, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 37.709, mean reward: 4.190 [3.381, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.496], loss: 0.193265, mae: 0.431062, mean_q: 4.898342
 64200/100000: episode: 1124, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 27.493, mean reward: 3.437 [2.847, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.404], loss: 0.148300, mae: 0.376012, mean_q: 4.731049
 64208/100000: episode: 1125, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 30.823, mean reward: 3.853 [2.407, 7.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-1.124, 10.343], loss: 0.130739, mae: 0.380359, mean_q: 4.870587
 64225/100000: episode: 1126, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 52.188, mean reward: 3.070 [2.421, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.182, 10.497], loss: 0.153696, mae: 0.379242, mean_q: 4.749542
 64233/100000: episode: 1127, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 27.492, mean reward: 3.437 [2.928, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.454], loss: 0.264940, mae: 0.433717, mean_q: 4.728484
 64247/100000: episode: 1128, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 39.817, mean reward: 2.844 [1.835, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.417], loss: 0.239216, mae: 0.431353, mean_q: 4.839376
 64256/100000: episode: 1129, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 24.571, mean reward: 2.730 [2.136, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.363], loss: 0.203698, mae: 0.446679, mean_q: 4.596346
 64265/100000: episode: 1130, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 35.486, mean reward: 3.943 [3.391, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.156, 10.511], loss: 0.517006, mae: 0.479067, mean_q: 4.850430
 64274/100000: episode: 1131, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 27.368, mean reward: 3.041 [2.743, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.477], loss: 0.206055, mae: 0.444480, mean_q: 4.750941
 64283/100000: episode: 1132, duration: 0.056s, episode steps: 9, steps per second: 159, episode reward: 28.905, mean reward: 3.212 [2.478, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.486], loss: 0.481487, mae: 0.480505, mean_q: 4.788461
 64290/100000: episode: 1133, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 24.298, mean reward: 3.471 [2.687, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.576], loss: 0.228212, mae: 0.481444, mean_q: 4.919837
 64299/100000: episode: 1134, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 25.790, mean reward: 2.866 [2.288, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.435], loss: 0.269653, mae: 0.468735, mean_q: 4.856111
 64308/100000: episode: 1135, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 28.194, mean reward: 3.133 [2.149, 5.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.519], loss: 0.157892, mae: 0.401599, mean_q: 4.628744
 64315/100000: episode: 1136, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 26.792, mean reward: 3.827 [3.292, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.501], loss: 0.335000, mae: 0.470923, mean_q: 4.767694
 64324/100000: episode: 1137, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 30.093, mean reward: 3.344 [2.889, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.463], loss: 0.230414, mae: 0.454865, mean_q: 4.744296
 64333/100000: episode: 1138, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 25.207, mean reward: 2.801 [2.552, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.058, 10.448], loss: 0.269000, mae: 0.450095, mean_q: 4.805277
 64350/100000: episode: 1139, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 48.058, mean reward: 2.827 [1.792, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.106, 10.364], loss: 0.339802, mae: 0.507134, mean_q: 4.939621
 64358/100000: episode: 1140, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 25.840, mean reward: 3.230 [2.938, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.367, 10.457], loss: 0.301661, mae: 0.537807, mean_q: 5.163676
 64371/100000: episode: 1141, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 53.813, mean reward: 4.139 [3.015, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.473], loss: 0.243406, mae: 0.451620, mean_q: 4.794054
 64384/100000: episode: 1142, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 50.400, mean reward: 3.877 [2.977, 5.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.554], loss: 0.382139, mae: 0.505409, mean_q: 4.908308
 64391/100000: episode: 1143, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 19.390, mean reward: 2.770 [2.308, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.670, 10.483], loss: 0.736493, mae: 0.557696, mean_q: 5.067525
 64398/100000: episode: 1144, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 22.750, mean reward: 3.250 [2.326, 5.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.169, 10.362], loss: 0.272604, mae: 0.431989, mean_q: 4.547192
 64405/100000: episode: 1145, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 27.763, mean reward: 3.966 [2.850, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.418], loss: 0.228055, mae: 0.455103, mean_q: 4.964700
 64418/100000: episode: 1146, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 39.315, mean reward: 3.024 [2.096, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.361], loss: 0.182730, mae: 0.420682, mean_q: 4.856224
 64431/100000: episode: 1147, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 67.410, mean reward: 5.185 [4.065, 6.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.592], loss: 0.239117, mae: 0.475829, mean_q: 5.034836
 64444/100000: episode: 1148, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 59.336, mean reward: 4.564 [3.695, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.567], loss: 0.142886, mae: 0.387933, mean_q: 4.791219
 64451/100000: episode: 1149, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 19.971, mean reward: 2.853 [2.529, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.573, 10.460], loss: 0.415359, mae: 0.549771, mean_q: 5.271772
 64460/100000: episode: 1150, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 33.125, mean reward: 3.681 [2.731, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.433], loss: 0.467513, mae: 0.500157, mean_q: 4.922660
 64469/100000: episode: 1151, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 31.093, mean reward: 3.455 [2.988, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.470], loss: 0.203465, mae: 0.444195, mean_q: 4.882746
 64482/100000: episode: 1152, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 63.912, mean reward: 4.916 [3.685, 7.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.569], loss: 0.253947, mae: 0.458705, mean_q: 4.830160
 64495/100000: episode: 1153, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 63.612, mean reward: 4.893 [3.490, 6.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.608], loss: 0.244233, mae: 0.470233, mean_q: 4.973197
 64502/100000: episode: 1154, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 25.362, mean reward: 3.623 [2.983, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.436], loss: 0.829769, mae: 0.600680, mean_q: 4.851885
 64511/100000: episode: 1155, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 35.122, mean reward: 3.902 [3.039, 4.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.092, 10.572], loss: 0.263450, mae: 0.481743, mean_q: 4.993899
 64520/100000: episode: 1156, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 34.659, mean reward: 3.851 [3.272, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.513], loss: 0.257017, mae: 0.490472, mean_q: 4.901705
 64529/100000: episode: 1157, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 44.376, mean reward: 4.931 [3.151, 5.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.565], loss: 0.914882, mae: 0.546259, mean_q: 5.055694
 64537/100000: episode: 1158, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 23.525, mean reward: 2.941 [2.650, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.260, 10.463], loss: 0.322153, mae: 0.537269, mean_q: 5.034557
 64544/100000: episode: 1159, duration: 0.054s, episode steps: 7, steps per second: 130, episode reward: 26.343, mean reward: 3.763 [2.903, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.564], loss: 0.957764, mae: 0.581063, mean_q: 4.806441
 64553/100000: episode: 1160, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 35.668, mean reward: 3.963 [3.694, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.514], loss: 0.272656, mae: 0.543993, mean_q: 5.167507
 64567/100000: episode: 1161, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 67.486, mean reward: 4.820 [3.567, 7.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.560], loss: 0.239997, mae: 0.471262, mean_q: 4.980835
 64576/100000: episode: 1162, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 23.659, mean reward: 2.629 [2.304, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.427], loss: 0.225072, mae: 0.446197, mean_q: 4.794599
 64583/100000: episode: 1163, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 19.312, mean reward: 2.759 [2.413, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.488], loss: 0.321742, mae: 0.558451, mean_q: 5.328745
 64592/100000: episode: 1164, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 28.592, mean reward: 3.177 [2.663, 4.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.456], loss: 0.209783, mae: 0.431111, mean_q: 4.943644
 64599/100000: episode: 1165, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 29.586, mean reward: 4.227 [3.391, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.054, 10.566], loss: 0.594432, mae: 0.528709, mean_q: 5.149689
 64606/100000: episode: 1166, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 24.640, mean reward: 3.520 [2.910, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.503], loss: 0.256763, mae: 0.509904, mean_q: 4.784213
 64613/100000: episode: 1167, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 24.959, mean reward: 3.566 [2.701, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.306, 10.395], loss: 0.601757, mae: 0.538671, mean_q: 5.074211
 64620/100000: episode: 1168, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 31.183, mean reward: 4.455 [3.750, 5.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.564], loss: 0.636626, mae: 0.508886, mean_q: 4.798971
 64628/100000: episode: 1169, duration: 0.047s, episode steps: 8, steps per second: 168, episode reward: 27.558, mean reward: 3.445 [3.210, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.515], loss: 0.777015, mae: 0.574412, mean_q: 5.076833
[Info] 3-TH LEVEL FOUND: 10.832469940185547, Considering 10/90 traces
 64642/100000: episode: 1170, duration: 4.147s, episode steps: 14, steps per second: 3, episode reward: 60.619, mean reward: 4.330 [2.913, 5.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.573, 10.561], loss: 0.303400, mae: 0.518292, mean_q: 4.971446
 64645/100000: episode: 1171, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 23.504, mean reward: 7.835 [5.533, 9.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.035, 10.691], loss: 0.526931, mae: 0.569175, mean_q: 5.058270
 64651/100000: episode: 1172, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 34.598, mean reward: 5.766 [4.338, 6.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.709, 10.434], loss: 0.297735, mae: 0.511641, mean_q: 5.158290
 64659/100000: episode: 1173, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 35.776, mean reward: 4.472 [3.833, 5.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.594, 10.530], loss: 0.194688, mae: 0.436496, mean_q: 5.160835
 64667/100000: episode: 1174, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 52.039, mean reward: 6.505 [4.128, 8.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.549], loss: 0.334879, mae: 0.526286, mean_q: 5.020031
 64675/100000: episode: 1175, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 37.888, mean reward: 4.736 [3.704, 6.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.515], loss: 0.229163, mae: 0.504459, mean_q: 5.063346
 64683/100000: episode: 1176, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 36.480, mean reward: 4.560 [4.067, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.444, 10.555], loss: 0.241319, mae: 0.456882, mean_q: 4.861075
 64691/100000: episode: 1177, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 46.645, mean reward: 5.831 [4.720, 7.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.613], loss: 0.207669, mae: 0.441227, mean_q: 4.959556
 64701/100000: episode: 1178, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 66.623, mean reward: 6.662 [4.217, 9.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.471], loss: 0.203030, mae: 0.422961, mean_q: 4.908650
 64709/100000: episode: 1179, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 60.849, mean reward: 7.606 [4.309, 19.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.728, 10.569], loss: 0.289732, mae: 0.485065, mean_q: 5.056871
 64717/100000: episode: 1180, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 46.600, mean reward: 5.825 [4.483, 6.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.643], loss: 0.287610, mae: 0.510586, mean_q: 5.300033
 64725/100000: episode: 1181, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 51.903, mean reward: 6.488 [4.725, 9.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.465, 10.645], loss: 0.233884, mae: 0.472884, mean_q: 5.109124
 64733/100000: episode: 1182, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 60.448, mean reward: 7.556 [4.496, 19.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.898, 10.540], loss: 0.324109, mae: 0.561693, mean_q: 5.126048
 64745/100000: episode: 1183, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 74.970, mean reward: 6.247 [4.411, 9.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.636], loss: 0.762621, mae: 0.598336, mean_q: 5.219171
 64749/100000: episode: 1184, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 21.605, mean reward: 5.401 [4.709, 6.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.467], loss: 0.175668, mae: 0.410335, mean_q: 5.145539
 64761/100000: episode: 1185, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 59.639, mean reward: 4.970 [2.905, 9.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.559], loss: 0.278095, mae: 0.496996, mean_q: 5.037474
 64765/100000: episode: 1186, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 33.501, mean reward: 8.375 [4.953, 11.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.482], loss: 0.263467, mae: 0.508719, mean_q: 5.190163
 64775/100000: episode: 1187, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 40.558, mean reward: 4.056 [3.247, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.565], loss: 0.307508, mae: 0.491026, mean_q: 5.183389
[Info] FALSIFICATION!
 64783/100000: episode: 1188, duration: 0.305s, episode steps: 8, steps per second: 26, episode reward: 1106.633, mean reward: 138.329 [5.220, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.312, 10.730], loss: 0.334502, mae: 0.514751, mean_q: 5.335493
 64791/100000: episode: 1189, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 50.309, mean reward: 6.289 [4.161, 11.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.298, 10.670], loss: 0.407848, mae: 0.536552, mean_q: 5.384450
 64795/100000: episode: 1190, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 15.761, mean reward: 3.940 [3.272, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.505], loss: 0.932809, mae: 0.561869, mean_q: 5.588887
 64799/100000: episode: 1191, duration: 0.032s, episode steps: 4, steps per second: 125, episode reward: 61.111, mean reward: 15.278 [8.027, 29.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.764], loss: 0.453734, mae: 0.528689, mean_q: 5.055441
 64811/100000: episode: 1192, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 57.805, mean reward: 4.817 [3.298, 9.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.658], loss: 0.459547, mae: 0.560652, mean_q: 5.209562
 64819/100000: episode: 1193, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 91.071, mean reward: 11.384 [5.216, 30.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.507, 10.715], loss: 0.827767, mae: 0.598672, mean_q: 5.271751
 64829/100000: episode: 1194, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 52.222, mean reward: 5.222 [3.734, 9.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.139, 10.631], loss: 0.275257, mae: 0.503518, mean_q: 5.079614
 64835/100000: episode: 1195, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 55.187, mean reward: 9.198 [5.805, 12.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.508], loss: 0.348644, mae: 0.537128, mean_q: 5.240004
 64845/100000: episode: 1196, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 40.389, mean reward: 4.039 [3.018, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.464], loss: 1.349686, mae: 0.648321, mean_q: 5.440644
 64849/100000: episode: 1197, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 18.802, mean reward: 4.700 [3.793, 6.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.438], loss: 0.234377, mae: 0.474661, mean_q: 4.994067
 64857/100000: episode: 1198, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 55.637, mean reward: 6.955 [3.989, 10.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.591, 10.698], loss: 1912.357056, mae: 6.122448, mean_q: 6.681681
 64863/100000: episode: 1199, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 35.458, mean reward: 5.910 [4.012, 8.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.419, 10.494], loss: 9.698361, mae: 2.765514, mean_q: 6.697575
 64873/100000: episode: 1200, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 45.421, mean reward: 4.542 [3.694, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.086, 10.580], loss: 2.618038, mae: 1.674878, mean_q: 5.556314
 64876/100000: episode: 1201, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 19.028, mean reward: 6.343 [5.855, 6.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.619], loss: 2.337929, mae: 1.643632, mean_q: 5.222899
 64882/100000: episode: 1202, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 46.953, mean reward: 7.825 [5.011, 13.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.654], loss: 1.283458, mae: 1.206752, mean_q: 5.051441
 64888/100000: episode: 1203, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 35.754, mean reward: 5.959 [4.949, 7.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.558], loss: 1.237203, mae: 1.046145, mean_q: 5.307154
 64900/100000: episode: 1204, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 49.291, mean reward: 4.108 [3.480, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.512], loss: 0.694136, mae: 0.807235, mean_q: 5.263902
 64912/100000: episode: 1205, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 113.128, mean reward: 9.427 [3.991, 19.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.168, 10.557], loss: 1.625277, mae: 0.804914, mean_q: 5.275171
 64924/100000: episode: 1206, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 59.501, mean reward: 4.958 [3.945, 6.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.433, 10.532], loss: 0.856996, mae: 0.770252, mean_q: 5.432959
 64930/100000: episode: 1207, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 28.209, mean reward: 4.702 [3.775, 5.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.547], loss: 1.087508, mae: 0.788672, mean_q: 5.414546
 64938/100000: episode: 1208, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 58.997, mean reward: 7.375 [4.060, 14.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.493], loss: 0.589787, mae: 0.757087, mean_q: 5.540199
 64946/100000: episode: 1209, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 61.735, mean reward: 7.717 [4.816, 9.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.156, 10.491], loss: 0.938436, mae: 0.770229, mean_q: 5.351312
 64956/100000: episode: 1210, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 40.976, mean reward: 4.098 [3.313, 5.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.601], loss: 0.827910, mae: 0.711950, mean_q: 5.335766
 64963/100000: episode: 1211, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 50.383, mean reward: 7.198 [4.736, 10.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.364, 10.616], loss: 0.544298, mae: 0.691171, mean_q: 5.325211
 64971/100000: episode: 1212, duration: 0.047s, episode steps: 8, steps per second: 168, episode reward: 302.411, mean reward: 37.801 [4.789, 162.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.723, 10.608], loss: 0.293834, mae: 0.535812, mean_q: 5.123090
 64981/100000: episode: 1213, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 41.046, mean reward: 4.105 [3.595, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.459], loss: 0.498631, mae: 0.603637, mean_q: 5.176486
 64989/100000: episode: 1214, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 57.507, mean reward: 7.188 [3.828, 17.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.682, 10.635], loss: 5.782795, mae: 0.879307, mean_q: 5.514227
 64993/100000: episode: 1215, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 25.278, mean reward: 6.320 [5.300, 7.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.600], loss: 0.901545, mae: 0.725657, mean_q: 5.567584
 65001/100000: episode: 1216, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 99.706, mean reward: 12.463 [5.556, 42.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.774], loss: 1.994349, mae: 0.810255, mean_q: 5.361432
 65008/100000: episode: 1217, duration: 0.053s, episode steps: 7, steps per second: 133, episode reward: 26.132, mean reward: 3.733 [2.701, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.536], loss: 2.514910, mae: 0.841257, mean_q: 5.715525
 65011/100000: episode: 1218, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 14.173, mean reward: 4.724 [4.365, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.035, 10.513], loss: 7.950963, mae: 1.170928, mean_q: 5.609854
 65019/100000: episode: 1219, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 38.492, mean reward: 4.812 [3.644, 6.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.640], loss: 7.035080, mae: 0.928809, mean_q: 5.601463
 65027/100000: episode: 1220, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 82.610, mean reward: 10.326 [5.455, 18.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.688], loss: 0.547606, mae: 0.712885, mean_q: 5.595859
 65030/100000: episode: 1221, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 23.553, mean reward: 7.851 [6.482, 9.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.556], loss: 0.227047, mae: 0.485329, mean_q: 5.269446
 65037/100000: episode: 1222, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 35.332, mean reward: 5.047 [3.149, 6.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.582], loss: 1.354210, mae: 0.789268, mean_q: 5.413978
 65047/100000: episode: 1223, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 34.123, mean reward: 3.412 [2.765, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.412], loss: 40.150681, mae: 1.346110, mean_q: 5.706520
 65050/100000: episode: 1224, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 19.550, mean reward: 6.517 [5.746, 7.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.263, 10.483], loss: 0.599249, mae: 0.871073, mean_q: 5.817528
 65053/100000: episode: 1225, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 14.406, mean reward: 4.802 [4.287, 5.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.558], loss: 0.911414, mae: 0.955449, mean_q: 5.968880
 65059/100000: episode: 1226, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 41.065, mean reward: 6.844 [5.643, 9.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.807, 10.688], loss: 0.664994, mae: 0.776163, mean_q: 5.569390
 65065/100000: episode: 1227, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 53.306, mean reward: 8.884 [5.794, 15.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.607], loss: 0.715793, mae: 0.720890, mean_q: 5.567194
 65075/100000: episode: 1228, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 38.842, mean reward: 3.884 [2.336, 6.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.823, 10.509], loss: 1528.211670, mae: 3.979740, mean_q: 6.009969
 65082/100000: episode: 1229, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 33.428, mean reward: 4.775 [4.255, 5.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.595], loss: 13.944399, mae: 3.264703, mean_q: 8.590791
 65090/100000: episode: 1230, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 49.144, mean reward: 6.143 [4.040, 9.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.219, 10.520], loss: 1900.197388, mae: 7.404125, mean_q: 8.779236
 65094/100000: episode: 1231, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 22.715, mean reward: 5.679 [4.996, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.538], loss: 11.414244, mae: 3.304309, mean_q: 8.382394
 65102/100000: episode: 1232, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 44.127, mean reward: 5.516 [4.229, 9.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.310, 10.690], loss: 2.707873, mae: 1.591620, mean_q: 6.438128
 65112/100000: episode: 1233, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 51.270, mean reward: 5.127 [2.996, 8.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.575, 10.482], loss: 1.629914, mae: 1.050862, mean_q: 5.704937
 65120/100000: episode: 1234, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 47.886, mean reward: 5.986 [5.022, 7.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.522], loss: 1.069522, mae: 0.974287, mean_q: 5.976855
 65124/100000: episode: 1235, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 19.329, mean reward: 4.832 [4.220, 5.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.613], loss: 1.758976, mae: 0.956807, mean_q: 6.002399
 65130/100000: episode: 1236, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 45.410, mean reward: 7.568 [5.517, 9.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.206, 10.636], loss: 1.421313, mae: 0.937225, mean_q: 5.889000
 65137/100000: episode: 1237, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 81.882, mean reward: 11.697 [5.519, 22.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.662], loss: 1.832198, mae: 1.072410, mean_q: 6.237858
 65144/100000: episode: 1238, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 31.064, mean reward: 4.438 [3.148, 6.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.070, 10.559], loss: 1.564863, mae: 0.878129, mean_q: 5.783551
 65151/100000: episode: 1239, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 49.831, mean reward: 7.119 [5.383, 9.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.625], loss: 0.581185, mae: 0.700027, mean_q: 5.871963
 65163/100000: episode: 1240, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 53.099, mean reward: 4.425 [3.809, 6.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.568, 10.540], loss: 0.821404, mae: 0.798825, mean_q: 5.949600
 65167/100000: episode: 1241, duration: 0.030s, episode steps: 4, steps per second: 136, episode reward: 18.444, mean reward: 4.611 [3.262, 5.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.072, 10.574], loss: 0.488368, mae: 0.656662, mean_q: 5.424958
 65173/100000: episode: 1242, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 25.696, mean reward: 4.283 [3.344, 6.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.485], loss: 1.039413, mae: 0.695130, mean_q: 5.518754
 65179/100000: episode: 1243, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 21.924, mean reward: 3.654 [3.023, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.658, 10.476], loss: 0.610787, mae: 0.782808, mean_q: 5.954311
 65186/100000: episode: 1244, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 40.659, mean reward: 5.808 [4.376, 7.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.666], loss: 0.951359, mae: 0.769399, mean_q: 5.850453
 65196/100000: episode: 1245, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 55.131, mean reward: 5.513 [4.074, 7.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.615], loss: 1528.635498, mae: 4.115528, mean_q: 6.184087
 65200/100000: episode: 1246, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 20.984, mean reward: 5.246 [4.910, 5.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.579], loss: 6.116878, mae: 2.663165, mean_q: 8.386787
 65210/100000: episode: 1247, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 55.708, mean reward: 5.571 [3.775, 8.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.607], loss: 1521.824707, mae: 5.558087, mean_q: 7.943812
 65222/100000: episode: 1248, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 86.331, mean reward: 7.194 [4.114, 25.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.648, 10.646], loss: 7.907684, mae: 2.784610, mean_q: 8.055175
 65229/100000: episode: 1249, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 39.123, mean reward: 5.589 [4.009, 7.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.351, 10.416], loss: 55.389885, mae: 1.682553, mean_q: 6.205235
 65233/100000: episode: 1250, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 22.275, mean reward: 5.569 [5.001, 6.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.582], loss: 0.972045, mae: 0.972953, mean_q: 5.807512
 65240/100000: episode: 1251, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 31.334, mean reward: 4.476 [4.026, 5.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.568], loss: 2.484295, mae: 0.990384, mean_q: 5.913836
 65246/100000: episode: 1252, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 30.000, mean reward: 5.000 [4.169, 6.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.481], loss: 2539.227051, mae: 6.221797, mean_q: 5.880152
 65254/100000: episode: 1253, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 41.464, mean reward: 5.183 [3.611, 6.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.545], loss: 6.827681, mae: 2.658497, mean_q: 8.483456
 65262/100000: episode: 1254, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 31.832, mean reward: 3.979 [3.245, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.586], loss: 5.790121, mae: 2.377799, mean_q: 8.154808
 65266/100000: episode: 1255, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 20.891, mean reward: 5.223 [4.652, 5.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.569], loss: 0.929767, mae: 0.896425, mean_q: 6.512897
 65272/100000: episode: 1256, duration: 0.044s, episode steps: 6, steps per second: 138, episode reward: 39.306, mean reward: 6.551 [4.891, 9.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.655], loss: 3.586166, mae: 1.281049, mean_q: 5.938835
 65278/100000: episode: 1257, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 27.857, mean reward: 4.643 [3.990, 5.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.556], loss: 2.686430, mae: 1.074724, mean_q: 5.995686
 65282/100000: episode: 1258, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 22.417, mean reward: 5.604 [4.463, 6.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.555], loss: 1.661136, mae: 1.083048, mean_q: 6.299581
 65290/100000: episode: 1259, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 35.034, mean reward: 4.379 [3.829, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.552], loss: 0.998175, mae: 0.859353, mean_q: 6.222543
[Info] Complete ISplit Iteration
[Info] Levels: [5.5864034, 7.8563714, 10.83247, 15.498106]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.02]
[Info] Error Prob: 2.0000000000000005e-05

 65297/100000: episode: 1260, duration: 4.462s, episode steps: 7, steps per second: 2, episode reward: 36.416, mean reward: 5.202 [4.200, 6.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.514, 10.567], loss: 1.017414, mae: 0.886052, mean_q: 6.266287
 65397/100000: episode: 1261, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.509, mean reward: 1.875 [1.467, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.609, 10.098], loss: 154.767273, mae: 1.560962, mean_q: 6.689117
 65497/100000: episode: 1262, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.472, mean reward: 2.005 [1.463, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.098], loss: 2.687019, mae: 0.959772, mean_q: 6.318908
 65597/100000: episode: 1263, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 178.916, mean reward: 1.789 [1.447, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.121], loss: 5.004479, mae: 0.873669, mean_q: 6.106807
 65697/100000: episode: 1264, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.361, mean reward: 1.894 [1.468, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.794, 10.114], loss: 8.996334, mae: 0.986457, mean_q: 6.236979
 65797/100000: episode: 1265, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 199.965, mean reward: 2.000 [1.503, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.098], loss: 158.086487, mae: 1.547222, mean_q: 6.441948
 65897/100000: episode: 1266, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 215.200, mean reward: 2.152 [1.466, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.598, 10.098], loss: 307.166962, mae: 2.079677, mean_q: 6.983137
 65997/100000: episode: 1267, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.603, mean reward: 1.876 [1.452, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.845, 10.174], loss: 5.997065, mae: 0.986874, mean_q: 6.418596
 66097/100000: episode: 1268, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 198.181, mean reward: 1.982 [1.460, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.921, 10.383], loss: 2.775743, mae: 0.879966, mean_q: 6.181858
 66197/100000: episode: 1269, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.269, mean reward: 1.893 [1.435, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.422, 10.098], loss: 1.824116, mae: 0.819575, mean_q: 6.096747
 66297/100000: episode: 1270, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.354, mean reward: 1.874 [1.457, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.916, 10.126], loss: 1.206894, mae: 0.759107, mean_q: 5.955776
 66397/100000: episode: 1271, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 197.923, mean reward: 1.979 [1.462, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.693, 10.098], loss: 154.687439, mae: 1.472417, mean_q: 6.483328
 66497/100000: episode: 1272, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.656, mean reward: 1.837 [1.452, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.742, 10.257], loss: 154.160736, mae: 1.532452, mean_q: 6.427103
 66597/100000: episode: 1273, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.159, mean reward: 1.982 [1.487, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.499, 10.098], loss: 153.510284, mae: 1.088552, mean_q: 6.003878
 66697/100000: episode: 1274, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.750, mean reward: 1.897 [1.432, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.288, 10.141], loss: 155.290970, mae: 1.650688, mean_q: 6.702914
 66797/100000: episode: 1275, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.628, mean reward: 2.026 [1.481, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.819, 10.098], loss: 4.933290, mae: 0.844948, mean_q: 6.128740
 66897/100000: episode: 1276, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 202.067, mean reward: 2.021 [1.444, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.392, 10.145], loss: 615.780029, mae: 3.147864, mean_q: 7.485869
 66997/100000: episode: 1277, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 191.566, mean reward: 1.916 [1.481, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.514, 10.125], loss: 1.803492, mae: 0.915317, mean_q: 6.224374
 67097/100000: episode: 1278, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 194.609, mean reward: 1.946 [1.441, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.786, 10.098], loss: 309.076721, mae: 2.023875, mean_q: 6.800709
 67197/100000: episode: 1279, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.781, mean reward: 1.868 [1.528, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.403, 10.098], loss: 1.766779, mae: 0.895579, mean_q: 6.142216
 67297/100000: episode: 1280, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 185.609, mean reward: 1.856 [1.459, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.126], loss: 160.723892, mae: 1.599498, mean_q: 6.534628
 67397/100000: episode: 1281, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.602, mean reward: 1.886 [1.458, 4.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.836, 10.098], loss: 1.533009, mae: 0.832207, mean_q: 6.006934
 67497/100000: episode: 1282, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.826, mean reward: 1.958 [1.442, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.041, 10.341], loss: 1.607656, mae: 0.774078, mean_q: 5.907404
 67597/100000: episode: 1283, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 208.645, mean reward: 2.086 [1.491, 5.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.311, 10.230], loss: 5.295589, mae: 0.796457, mean_q: 5.922126
 67697/100000: episode: 1284, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 204.757, mean reward: 2.048 [1.447, 6.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.328, 10.098], loss: 0.854355, mae: 0.669589, mean_q: 5.590585
 67797/100000: episode: 1285, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 185.710, mean reward: 1.857 [1.469, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.276, 10.109], loss: 158.523438, mae: 1.561299, mean_q: 6.113578
 67897/100000: episode: 1286, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 189.056, mean reward: 1.891 [1.497, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.147, 10.109], loss: 153.160934, mae: 1.388854, mean_q: 6.005391
 67997/100000: episode: 1287, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 195.274, mean reward: 1.953 [1.481, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.547, 10.115], loss: 154.334320, mae: 1.459646, mean_q: 5.989515
 68097/100000: episode: 1288, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.563, mean reward: 1.986 [1.462, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.598, 10.098], loss: 5.327742, mae: 0.803617, mean_q: 5.689905
 68197/100000: episode: 1289, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.101, mean reward: 1.861 [1.491, 2.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.176, 10.145], loss: 157.578598, mae: 1.370430, mean_q: 6.037450
 68297/100000: episode: 1290, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 177.449, mean reward: 1.774 [1.454, 2.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.195, 10.278], loss: 304.426880, mae: 1.766539, mean_q: 6.173471
 68397/100000: episode: 1291, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 197.173, mean reward: 1.972 [1.470, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.054, 10.098], loss: 153.413956, mae: 1.391615, mean_q: 6.008913
 68497/100000: episode: 1292, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 214.209, mean reward: 2.142 [1.468, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.218, 10.098], loss: 152.790588, mae: 1.236274, mean_q: 5.857336
 68597/100000: episode: 1293, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.192, mean reward: 1.832 [1.444, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.561, 10.098], loss: 6.105142, mae: 0.786874, mean_q: 5.571204
 68697/100000: episode: 1294, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 182.341, mean reward: 1.823 [1.472, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.771, 10.266], loss: 1.676577, mae: 0.691230, mean_q: 5.460382
 68797/100000: episode: 1295, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 218.672, mean reward: 2.187 [1.469, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.533, 10.098], loss: 152.973831, mae: 0.990785, mean_q: 5.381232
 68897/100000: episode: 1296, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 205.910, mean reward: 2.059 [1.480, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.163, 10.247], loss: 2.582516, mae: 0.945006, mean_q: 5.605158
 68997/100000: episode: 1297, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.086, mean reward: 1.901 [1.472, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.221, 10.179], loss: 8.506870, mae: 0.742125, mean_q: 5.333387
 69097/100000: episode: 1298, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 196.836, mean reward: 1.968 [1.509, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.076, 10.098], loss: 152.375443, mae: 0.985754, mean_q: 5.254949
 69197/100000: episode: 1299, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 185.280, mean reward: 1.853 [1.473, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.156, 10.098], loss: 5.333014, mae: 0.811153, mean_q: 5.246807
 69297/100000: episode: 1300, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 215.890, mean reward: 2.159 [1.448, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.481, 10.098], loss: 157.049149, mae: 1.279044, mean_q: 5.398052
 69397/100000: episode: 1301, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 207.128, mean reward: 2.071 [1.464, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.958, 10.426], loss: 303.160187, mae: 1.456310, mean_q: 5.325740
 69497/100000: episode: 1302, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 182.685, mean reward: 1.827 [1.465, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.448, 10.267], loss: 155.512192, mae: 1.456867, mean_q: 5.677698
 69597/100000: episode: 1303, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.405, mean reward: 1.944 [1.462, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.226], loss: 156.640686, mae: 1.200265, mean_q: 5.171473
 69697/100000: episode: 1304, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 237.902, mean reward: 2.379 [1.498, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.727, 10.217], loss: 5.175847, mae: 0.665950, mean_q: 4.926936
 69797/100000: episode: 1305, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 182.876, mean reward: 1.829 [1.451, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.493, 10.099], loss: 0.661500, mae: 0.458677, mean_q: 4.516864
 69897/100000: episode: 1306, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 186.949, mean reward: 1.869 [1.462, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.353, 10.252], loss: 1.726063, mae: 0.497063, mean_q: 4.429411
 69997/100000: episode: 1307, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 180.907, mean reward: 1.809 [1.516, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.620, 10.217], loss: 0.315781, mae: 0.382161, mean_q: 4.251991
 70097/100000: episode: 1308, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 205.086, mean reward: 2.051 [1.506, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.310, 10.275], loss: 0.236092, mae: 0.348774, mean_q: 4.087331
 70197/100000: episode: 1309, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 180.079, mean reward: 1.801 [1.465, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.098], loss: 0.097006, mae: 0.291653, mean_q: 3.943340
 70297/100000: episode: 1310, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.169, mean reward: 1.902 [1.458, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.583, 10.098], loss: 0.092142, mae: 0.296815, mean_q: 3.876225
 70397/100000: episode: 1311, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 185.425, mean reward: 1.854 [1.456, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.114], loss: 0.082498, mae: 0.283647, mean_q: 3.842029
 70497/100000: episode: 1312, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 177.864, mean reward: 1.779 [1.455, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.098], loss: 0.081016, mae: 0.275640, mean_q: 3.838410
 70597/100000: episode: 1313, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.500, mean reward: 1.815 [1.440, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.198], loss: 0.075606, mae: 0.276370, mean_q: 3.831787
 70697/100000: episode: 1314, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 207.724, mean reward: 2.077 [1.455, 6.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.150, 10.236], loss: 0.082058, mae: 0.280559, mean_q: 3.840537
 70797/100000: episode: 1315, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 191.543, mean reward: 1.915 [1.471, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.449, 10.172], loss: 0.086707, mae: 0.287148, mean_q: 3.849514
 70897/100000: episode: 1316, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 233.964, mean reward: 2.340 [1.435, 6.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.777, 10.098], loss: 0.081344, mae: 0.280629, mean_q: 3.826860
 70997/100000: episode: 1317, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 179.840, mean reward: 1.798 [1.482, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.339, 10.098], loss: 0.090033, mae: 0.289358, mean_q: 3.864213
 71097/100000: episode: 1318, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 182.089, mean reward: 1.821 [1.494, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.049, 10.098], loss: 0.085367, mae: 0.281940, mean_q: 3.856407
 71197/100000: episode: 1319, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 180.836, mean reward: 1.808 [1.435, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.343], loss: 0.087965, mae: 0.285669, mean_q: 3.857236
 71297/100000: episode: 1320, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 190.548, mean reward: 1.905 [1.487, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.372, 10.104], loss: 0.076360, mae: 0.276452, mean_q: 3.848136
 71397/100000: episode: 1321, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 207.564, mean reward: 2.076 [1.477, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.604, 10.098], loss: 0.070723, mae: 0.267735, mean_q: 3.821066
 71497/100000: episode: 1322, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 199.270, mean reward: 1.993 [1.524, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.760, 10.098], loss: 0.087120, mae: 0.289209, mean_q: 3.856200
 71597/100000: episode: 1323, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.433, mean reward: 1.854 [1.474, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.355, 10.150], loss: 0.080843, mae: 0.286750, mean_q: 3.851789
 71697/100000: episode: 1324, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 196.354, mean reward: 1.964 [1.472, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.900, 10.172], loss: 0.073611, mae: 0.275258, mean_q: 3.864209
 71797/100000: episode: 1325, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.322, mean reward: 2.053 [1.495, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.564, 10.227], loss: 0.082449, mae: 0.288211, mean_q: 3.860026
 71897/100000: episode: 1326, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 210.148, mean reward: 2.101 [1.500, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.253, 10.318], loss: 0.090265, mae: 0.289342, mean_q: 3.882221
 71997/100000: episode: 1327, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 203.102, mean reward: 2.031 [1.432, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.633, 10.174], loss: 0.084038, mae: 0.283690, mean_q: 3.867084
 72097/100000: episode: 1328, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 183.542, mean reward: 1.835 [1.449, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.856, 10.098], loss: 0.086265, mae: 0.279502, mean_q: 3.855948
 72197/100000: episode: 1329, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 204.966, mean reward: 2.050 [1.454, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.098], loss: 0.082749, mae: 0.289548, mean_q: 3.865646
 72297/100000: episode: 1330, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 226.262, mean reward: 2.263 [1.497, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.107, 10.396], loss: 0.081031, mae: 0.278563, mean_q: 3.865016
 72397/100000: episode: 1331, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 194.598, mean reward: 1.946 [1.463, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.631, 10.105], loss: 0.090177, mae: 0.291420, mean_q: 3.893060
 72497/100000: episode: 1332, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.385, mean reward: 1.944 [1.445, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.949, 10.234], loss: 0.075819, mae: 0.271130, mean_q: 3.865233
 72597/100000: episode: 1333, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 219.200, mean reward: 2.192 [1.468, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.572, 10.498], loss: 0.075402, mae: 0.277647, mean_q: 3.872938
 72697/100000: episode: 1334, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 192.199, mean reward: 1.922 [1.451, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.983, 10.098], loss: 0.076367, mae: 0.277337, mean_q: 3.870478
 72797/100000: episode: 1335, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.121, mean reward: 1.981 [1.437, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.878, 10.170], loss: 0.076478, mae: 0.279740, mean_q: 3.882171
 72897/100000: episode: 1336, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 193.990, mean reward: 1.940 [1.487, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.612, 10.098], loss: 0.082358, mae: 0.285403, mean_q: 3.867057
 72997/100000: episode: 1337, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.368, mean reward: 1.974 [1.459, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.014, 10.098], loss: 0.077829, mae: 0.285307, mean_q: 3.877263
 73097/100000: episode: 1338, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 187.654, mean reward: 1.877 [1.459, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.029, 10.098], loss: 0.084566, mae: 0.291701, mean_q: 3.920094
 73197/100000: episode: 1339, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.286, mean reward: 2.063 [1.456, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.687, 10.098], loss: 0.081791, mae: 0.288530, mean_q: 3.902138
 73297/100000: episode: 1340, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 203.654, mean reward: 2.037 [1.500, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.637, 10.098], loss: 0.076442, mae: 0.285944, mean_q: 3.885803
 73397/100000: episode: 1341, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.176, mean reward: 1.842 [1.457, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.398, 10.171], loss: 0.075335, mae: 0.282411, mean_q: 3.922205
 73497/100000: episode: 1342, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 177.514, mean reward: 1.775 [1.453, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.897, 10.128], loss: 0.077725, mae: 0.282161, mean_q: 3.883494
 73597/100000: episode: 1343, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 214.589, mean reward: 2.146 [1.487, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.098], loss: 0.078400, mae: 0.281460, mean_q: 3.888046
 73697/100000: episode: 1344, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 188.048, mean reward: 1.880 [1.452, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.217], loss: 0.069607, mae: 0.272387, mean_q: 3.880609
 73797/100000: episode: 1345, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 184.681, mean reward: 1.847 [1.446, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.471, 10.098], loss: 0.082782, mae: 0.288672, mean_q: 3.905353
 73897/100000: episode: 1346, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 182.279, mean reward: 1.823 [1.456, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.794, 10.098], loss: 0.079788, mae: 0.279585, mean_q: 3.887941
 73997/100000: episode: 1347, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 206.632, mean reward: 2.066 [1.533, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.158, 10.380], loss: 0.068618, mae: 0.273458, mean_q: 3.875580
 74097/100000: episode: 1348, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.752, mean reward: 1.918 [1.473, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.166, 10.176], loss: 0.070683, mae: 0.275026, mean_q: 3.869129
 74197/100000: episode: 1349, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 206.004, mean reward: 2.060 [1.446, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.927, 10.098], loss: 0.072872, mae: 0.278758, mean_q: 3.889005
 74297/100000: episode: 1350, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 211.957, mean reward: 2.120 [1.457, 6.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.581, 10.098], loss: 0.086403, mae: 0.294115, mean_q: 3.912856
 74397/100000: episode: 1351, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 194.288, mean reward: 1.943 [1.482, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.839, 10.419], loss: 0.083278, mae: 0.286962, mean_q: 3.888726
 74497/100000: episode: 1352, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 202.085, mean reward: 2.021 [1.564, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.800, 10.098], loss: 0.083039, mae: 0.283904, mean_q: 3.896539
 74597/100000: episode: 1353, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.992, mean reward: 1.840 [1.455, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.842, 10.175], loss: 0.080610, mae: 0.286197, mean_q: 3.889191
 74697/100000: episode: 1354, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 204.662, mean reward: 2.047 [1.467, 8.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.843, 10.153], loss: 0.091181, mae: 0.295637, mean_q: 3.887164
 74797/100000: episode: 1355, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 209.556, mean reward: 2.096 [1.457, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.257, 10.290], loss: 0.093339, mae: 0.292168, mean_q: 3.884719
 74897/100000: episode: 1356, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 206.233, mean reward: 2.062 [1.445, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.718, 10.179], loss: 0.098507, mae: 0.297799, mean_q: 3.893925
 74997/100000: episode: 1357, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 196.407, mean reward: 1.964 [1.430, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.194, 10.098], loss: 0.085676, mae: 0.290000, mean_q: 3.888613
 75097/100000: episode: 1358, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 182.803, mean reward: 1.828 [1.461, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.180, 10.122], loss: 0.102137, mae: 0.293226, mean_q: 3.889234
 75197/100000: episode: 1359, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 186.721, mean reward: 1.867 [1.449, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.549, 10.103], loss: 0.088172, mae: 0.286259, mean_q: 3.863535
[Info] 1-TH LEVEL FOUND: 5.80858039855957, Considering 10/90 traces
 75297/100000: episode: 1360, duration: 4.531s, episode steps: 100, steps per second: 22, episode reward: 204.763, mean reward: 2.048 [1.553, 6.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.485, 10.098], loss: 0.084488, mae: 0.280038, mean_q: 3.878911
 75310/100000: episode: 1361, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 35.310, mean reward: 2.716 [2.183, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.282, 10.338], loss: 0.090227, mae: 0.306410, mean_q: 3.879519
 75323/100000: episode: 1362, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 36.591, mean reward: 2.815 [2.404, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.333, 10.456], loss: 0.083008, mae: 0.309373, mean_q: 3.917931
 75332/100000: episode: 1363, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 28.102, mean reward: 3.122 [2.205, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.242, 10.428], loss: 0.093477, mae: 0.314396, mean_q: 3.973214
 75353/100000: episode: 1364, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 56.150, mean reward: 2.674 [2.139, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.331, 10.100], loss: 0.117192, mae: 0.304446, mean_q: 3.900747
 75377/100000: episode: 1365, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 55.921, mean reward: 2.330 [1.728, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.173, 10.100], loss: 0.120950, mae: 0.318476, mean_q: 3.938322
 75401/100000: episode: 1366, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 67.973, mean reward: 2.832 [2.359, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.272, 10.100], loss: 0.086934, mae: 0.302263, mean_q: 3.918717
 75422/100000: episode: 1367, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 65.248, mean reward: 3.107 [2.059, 5.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.431, 10.100], loss: 0.118503, mae: 0.324747, mean_q: 3.963438
 75442/100000: episode: 1368, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 83.474, mean reward: 4.174 [1.894, 32.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.258, 10.100], loss: 0.111348, mae: 0.312408, mean_q: 3.963225
 75467/100000: episode: 1369, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 83.771, mean reward: 3.351 [2.187, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.755, 10.100], loss: 0.668343, mae: 0.399061, mean_q: 4.006737
 75505/100000: episode: 1370, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 113.411, mean reward: 2.985 [2.080, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.065, 10.588], loss: 0.133140, mae: 0.334382, mean_q: 4.021814
 75543/100000: episode: 1371, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 166.235, mean reward: 4.375 [2.687, 12.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.651, 10.502], loss: 0.136550, mae: 0.326145, mean_q: 4.005515
 75552/100000: episode: 1372, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 19.691, mean reward: 2.188 [1.969, 2.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.322], loss: 0.095694, mae: 0.320484, mean_q: 3.976827
 75573/100000: episode: 1373, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 79.814, mean reward: 3.801 [2.769, 5.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.505, 10.100], loss: 0.140155, mae: 0.336086, mean_q: 4.000782
 75594/100000: episode: 1374, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 64.837, mean reward: 3.087 [2.163, 7.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.454, 10.100], loss: 0.159192, mae: 0.335664, mean_q: 4.040900
 75614/100000: episode: 1375, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 53.647, mean reward: 2.682 [2.143, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.073, 10.100], loss: 0.101773, mae: 0.322251, mean_q: 4.060799
 75629/100000: episode: 1376, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 37.433, mean reward: 2.496 [2.099, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.414, 10.100], loss: 0.966253, mae: 0.376616, mean_q: 4.073280
 75653/100000: episode: 1377, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 102.941, mean reward: 4.289 [2.370, 9.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.549, 10.100], loss: 0.162497, mae: 0.405993, mean_q: 4.140593
 75677/100000: episode: 1378, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 62.619, mean reward: 2.609 [1.688, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.112, 10.100], loss: 0.151349, mae: 0.312284, mean_q: 4.075281
 75690/100000: episode: 1379, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 36.149, mean reward: 2.781 [2.093, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.319, 10.346], loss: 0.153498, mae: 0.348312, mean_q: 4.082820
 75703/100000: episode: 1380, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 33.014, mean reward: 2.540 [2.210, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.695, 10.422], loss: 0.290504, mae: 0.367229, mean_q: 4.057734
 75716/100000: episode: 1381, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 37.131, mean reward: 2.856 [2.270, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.491, 10.452], loss: 0.123543, mae: 0.342858, mean_q: 4.065971
 75738/100000: episode: 1382, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 56.180, mean reward: 2.554 [2.176, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.188, 10.100], loss: 0.122726, mae: 0.339655, mean_q: 4.066838
 75759/100000: episode: 1383, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 51.380, mean reward: 2.447 [1.863, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.226, 10.100], loss: 0.148881, mae: 0.298742, mean_q: 4.011372
 75774/100000: episode: 1384, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 39.901, mean reward: 2.660 [1.818, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.404, 10.100], loss: 0.184658, mae: 0.368545, mean_q: 4.165677
 75783/100000: episode: 1385, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 24.125, mean reward: 2.681 [2.292, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.145, 10.487], loss: 0.159353, mae: 0.347852, mean_q: 4.135404
 75796/100000: episode: 1386, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 36.540, mean reward: 2.811 [2.379, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.520, 10.429], loss: 0.140489, mae: 0.331306, mean_q: 4.117119
 75805/100000: episode: 1387, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 26.157, mean reward: 2.906 [2.370, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.115, 10.469], loss: 0.131236, mae: 0.342983, mean_q: 3.996298
 75818/100000: episode: 1388, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 34.754, mean reward: 2.673 [1.876, 4.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.387], loss: 0.098558, mae: 0.316559, mean_q: 4.063187
 75843/100000: episode: 1389, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 67.857, mean reward: 2.714 [1.967, 4.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.280, 10.100], loss: 0.089180, mae: 0.294068, mean_q: 4.068655
 75868/100000: episode: 1390, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 66.629, mean reward: 2.665 [1.996, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.698, 10.100], loss: 0.172558, mae: 0.366545, mean_q: 4.093171
 75877/100000: episode: 1391, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 20.734, mean reward: 2.304 [2.014, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.302], loss: 0.133056, mae: 0.350854, mean_q: 4.057415
 75886/100000: episode: 1392, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 23.970, mean reward: 2.663 [2.018, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.473], loss: 0.117068, mae: 0.342483, mean_q: 4.124580
 75908/100000: episode: 1393, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 62.176, mean reward: 2.826 [2.135, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.114, 10.100], loss: 0.102600, mae: 0.323901, mean_q: 4.097091
 75923/100000: episode: 1394, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 44.512, mean reward: 2.967 [2.230, 4.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.433, 10.100], loss: 0.127265, mae: 0.335662, mean_q: 4.200740
 75948/100000: episode: 1395, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 63.449, mean reward: 2.538 [2.187, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.236, 10.100], loss: 0.199659, mae: 0.363747, mean_q: 4.144730
 75968/100000: episode: 1396, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 60.969, mean reward: 3.048 [2.048, 5.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.159, 10.100], loss: 0.141749, mae: 0.335753, mean_q: 4.056860
 75993/100000: episode: 1397, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 56.898, mean reward: 2.276 [1.655, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.257, 10.100], loss: 0.180016, mae: 0.355525, mean_q: 4.071581
 76008/100000: episode: 1398, duration: 0.091s, episode steps: 15, steps per second: 166, episode reward: 37.618, mean reward: 2.508 [2.009, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.213, 10.100], loss: 0.186278, mae: 0.347030, mean_q: 4.166248
 76029/100000: episode: 1399, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 53.201, mean reward: 2.533 [1.808, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.208, 10.100], loss: 0.142187, mae: 0.371156, mean_q: 4.196676
 76067/100000: episode: 1400, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 83.841, mean reward: 2.206 [1.448, 5.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.285, 10.100], loss: 0.117923, mae: 0.329418, mean_q: 4.070459
 76089/100000: episode: 1401, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 53.662, mean reward: 2.439 [1.900, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.535, 10.100], loss: 0.099645, mae: 0.320694, mean_q: 4.142458
 76098/100000: episode: 1402, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.395, mean reward: 2.822 [2.215, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.323], loss: 0.114495, mae: 0.356736, mean_q: 4.209115
 76107/100000: episode: 1403, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 26.601, mean reward: 2.956 [2.425, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.511], loss: 0.140850, mae: 0.330543, mean_q: 4.128036
 76132/100000: episode: 1404, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 71.197, mean reward: 2.848 [2.124, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.350, 10.100], loss: 0.135104, mae: 0.357965, mean_q: 4.187781
 76145/100000: episode: 1405, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 44.167, mean reward: 3.397 [2.604, 5.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-1.416, 10.591], loss: 0.140632, mae: 0.349103, mean_q: 4.091164
 76165/100000: episode: 1406, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 49.664, mean reward: 2.483 [2.013, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.226, 10.100], loss: 0.095919, mae: 0.316459, mean_q: 4.178080
 76187/100000: episode: 1407, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 50.920, mean reward: 2.315 [1.934, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.071, 10.100], loss: 0.244776, mae: 0.393856, mean_q: 4.194563
 76211/100000: episode: 1408, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 62.734, mean reward: 2.614 [2.137, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.310, 10.100], loss: 0.123726, mae: 0.351674, mean_q: 4.215768
 76220/100000: episode: 1409, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 19.274, mean reward: 2.142 [1.936, 2.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.314], loss: 0.219827, mae: 0.400079, mean_q: 4.206804
 76241/100000: episode: 1410, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 45.493, mean reward: 2.166 [1.767, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.865, 10.100], loss: 0.156420, mae: 0.345896, mean_q: 4.126456
 76263/100000: episode: 1411, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 55.615, mean reward: 2.528 [1.546, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.081, 10.100], loss: 0.112443, mae: 0.339305, mean_q: 4.197218
 76285/100000: episode: 1412, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 65.402, mean reward: 2.973 [2.109, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.584, 10.100], loss: 0.172527, mae: 0.369355, mean_q: 4.225058
 76306/100000: episode: 1413, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 64.065, mean reward: 3.051 [2.403, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.281, 10.100], loss: 0.141612, mae: 0.355648, mean_q: 4.233252
 76326/100000: episode: 1414, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 55.784, mean reward: 2.789 [2.012, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.555, 10.100], loss: 0.191196, mae: 0.353542, mean_q: 4.148406
 76335/100000: episode: 1415, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 29.946, mean reward: 3.327 [2.686, 5.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.576], loss: 0.099877, mae: 0.334594, mean_q: 4.263963
 76344/100000: episode: 1416, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 24.701, mean reward: 2.745 [2.173, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.474], loss: 0.086671, mae: 0.307369, mean_q: 4.133054
 76365/100000: episode: 1417, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 55.839, mean reward: 2.659 [2.032, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.260, 10.100], loss: 0.117149, mae: 0.344160, mean_q: 4.168735
 76403/100000: episode: 1418, duration: 0.222s, episode steps: 38, steps per second: 171, episode reward: 82.591, mean reward: 2.173 [1.480, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.705, 10.100], loss: 0.552411, mae: 0.435970, mean_q: 4.263356
 76424/100000: episode: 1419, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 53.790, mean reward: 2.561 [2.249, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.227, 10.100], loss: 0.739943, mae: 0.386434, mean_q: 4.233392
 76462/100000: episode: 1420, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 107.033, mean reward: 2.817 [1.935, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.375, 10.250], loss: 0.186911, mae: 0.395142, mean_q: 4.274234
 76477/100000: episode: 1421, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 39.844, mean reward: 2.656 [2.346, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.287, 10.100], loss: 0.097122, mae: 0.308564, mean_q: 4.132467
 76497/100000: episode: 1422, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 50.520, mean reward: 2.526 [1.823, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.515, 10.100], loss: 0.131232, mae: 0.348729, mean_q: 4.289988
 76518/100000: episode: 1423, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 67.114, mean reward: 3.196 [2.649, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.133, 10.100], loss: 0.845656, mae: 0.457330, mean_q: 4.352523
 76527/100000: episode: 1424, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 30.453, mean reward: 3.384 [2.600, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.457, 10.401], loss: 1.753946, mae: 0.481688, mean_q: 4.110171
 76551/100000: episode: 1425, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 73.288, mean reward: 3.054 [2.118, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.356, 10.100], loss: 0.136287, mae: 0.380978, mean_q: 4.302133
 76572/100000: episode: 1426, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 61.605, mean reward: 2.934 [2.130, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.234, 10.100], loss: 0.191573, mae: 0.366114, mean_q: 4.291893
 76581/100000: episode: 1427, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 19.680, mean reward: 2.187 [2.001, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.307], loss: 0.088259, mae: 0.300552, mean_q: 4.260218
 76619/100000: episode: 1428, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 122.180, mean reward: 3.215 [2.390, 5.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.447, 10.486], loss: 0.119316, mae: 0.341115, mean_q: 4.251601
 76641/100000: episode: 1429, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 53.654, mean reward: 2.439 [1.931, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.258, 10.100], loss: 0.706832, mae: 0.382937, mean_q: 4.261763
 76662/100000: episode: 1430, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 89.076, mean reward: 4.242 [2.961, 5.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.310, 10.100], loss: 0.150438, mae: 0.338415, mean_q: 4.229301
 76671/100000: episode: 1431, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 25.749, mean reward: 2.861 [2.285, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.510], loss: 0.176132, mae: 0.407353, mean_q: 4.436282
 76696/100000: episode: 1432, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 75.807, mean reward: 3.032 [2.462, 5.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.378, 10.100], loss: 0.207015, mae: 0.376362, mean_q: 4.317509
 76705/100000: episode: 1433, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 23.187, mean reward: 2.576 [2.365, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.337], loss: 0.255329, mae: 0.416325, mean_q: 4.564249
 76718/100000: episode: 1434, duration: 0.085s, episode steps: 13, steps per second: 154, episode reward: 38.073, mean reward: 2.929 [2.394, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.461], loss: 0.225702, mae: 0.379980, mean_q: 4.381312
 76742/100000: episode: 1435, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 60.726, mean reward: 2.530 [1.835, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.070, 10.100], loss: 0.145358, mae: 0.357331, mean_q: 4.319438
 76751/100000: episode: 1436, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 26.830, mean reward: 2.981 [2.184, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.743, 10.303], loss: 0.305046, mae: 0.386583, mean_q: 4.304003
 76764/100000: episode: 1437, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 39.269, mean reward: 3.021 [2.630, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.421], loss: 0.199994, mae: 0.405320, mean_q: 4.383480
 76773/100000: episode: 1438, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 20.878, mean reward: 2.320 [2.051, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.332], loss: 0.199194, mae: 0.412346, mean_q: 4.408714
 76795/100000: episode: 1439, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 80.672, mean reward: 3.667 [2.868, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.693, 10.100], loss: 0.744522, mae: 0.420939, mean_q: 4.363544
 76804/100000: episode: 1440, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 23.124, mean reward: 2.569 [2.135, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.376], loss: 0.165876, mae: 0.428777, mean_q: 4.491482
 76825/100000: episode: 1441, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 48.004, mean reward: 2.286 [1.650, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.831, 10.100], loss: 1.372279, mae: 0.471116, mean_q: 4.356297
 76847/100000: episode: 1442, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 70.742, mean reward: 3.216 [2.651, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.842, 10.100], loss: 0.240343, mae: 0.437735, mean_q: 4.424212
 76867/100000: episode: 1443, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 51.032, mean reward: 2.552 [1.707, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.323, 10.100], loss: 0.158159, mae: 0.380094, mean_q: 4.408514
 76876/100000: episode: 1444, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 32.009, mean reward: 3.557 [2.455, 6.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-1.218, 10.464], loss: 0.157210, mae: 0.373659, mean_q: 4.418818
 76897/100000: episode: 1445, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 61.157, mean reward: 2.912 [2.231, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.259, 10.100], loss: 0.097579, mae: 0.316833, mean_q: 4.361897
 76912/100000: episode: 1446, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 38.980, mean reward: 2.599 [2.091, 4.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.652, 10.100], loss: 0.190481, mae: 0.406217, mean_q: 4.474213
 76950/100000: episode: 1447, duration: 0.185s, episode steps: 38, steps per second: 206, episode reward: 98.855, mean reward: 2.601 [1.806, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.360, 10.403], loss: 0.175460, mae: 0.377137, mean_q: 4.381254
 76975/100000: episode: 1448, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 69.881, mean reward: 2.795 [2.148, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.651, 10.100], loss: 0.203499, mae: 0.387370, mean_q: 4.407789
 77000/100000: episode: 1449, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 69.102, mean reward: 2.764 [2.118, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.100, 10.100], loss: 0.717640, mae: 0.488061, mean_q: 4.580800
[Info] 2-TH LEVEL FOUND: 7.067760944366455, Considering 10/90 traces
 77021/100000: episode: 1450, duration: 4.075s, episode steps: 21, steps per second: 5, episode reward: 66.471, mean reward: 3.165 [2.105, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.363, 10.100], loss: 0.232850, mae: 0.395059, mean_q: 4.435064
 77058/100000: episode: 1451, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 126.223, mean reward: 3.411 [1.748, 6.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.800, 10.283], loss: 0.231165, mae: 0.420754, mean_q: 4.451869
 77095/100000: episode: 1452, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 125.326, mean reward: 3.387 [1.841, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.147, 10.326], loss: 0.155662, mae: 0.376917, mean_q: 4.403093
 77132/100000: episode: 1453, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 93.591, mean reward: 2.529 [1.956, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.616, 10.304], loss: 0.504894, mae: 0.409025, mean_q: 4.436824
 77168/100000: episode: 1454, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 113.929, mean reward: 3.165 [2.042, 6.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.970, 10.419], loss: 0.225818, mae: 0.404102, mean_q: 4.441194
 77204/100000: episode: 1455, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 236.223, mean reward: 6.562 [3.377, 10.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.559, 10.478], loss: 0.549131, mae: 0.427318, mean_q: 4.579946
 77240/100000: episode: 1456, duration: 0.215s, episode steps: 36, steps per second: 168, episode reward: 121.839, mean reward: 3.384 [1.515, 7.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.765, 10.100], loss: 0.212390, mae: 0.399612, mean_q: 4.512183
 77249/100000: episode: 1457, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 28.360, mean reward: 3.151 [2.653, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.454], loss: 1.691252, mae: 0.562353, mean_q: 4.653265
 77258/100000: episode: 1458, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 47.970, mean reward: 5.330 [3.849, 9.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.758, 10.491], loss: 0.143087, mae: 0.396931, mean_q: 4.359099
 77294/100000: episode: 1459, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 131.650, mean reward: 3.657 [2.393, 6.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.194, 10.375], loss: 0.227787, mae: 0.429098, mean_q: 4.563854
 77328/100000: episode: 1460, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 136.876, mean reward: 4.026 [3.164, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.540], loss: 0.635055, mae: 0.502684, mean_q: 4.666460
 77365/100000: episode: 1461, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 92.400, mean reward: 2.497 [1.812, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.858, 10.395], loss: 0.564753, mae: 0.449886, mean_q: 4.592284
 77400/100000: episode: 1462, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 81.097, mean reward: 2.317 [1.794, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.062, 10.314], loss: 0.270690, mae: 0.459510, mean_q: 4.644362
 77436/100000: episode: 1463, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 102.091, mean reward: 2.836 [1.947, 6.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.552, 10.391], loss: 0.215722, mae: 0.421556, mean_q: 4.659211
 77472/100000: episode: 1464, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 122.353, mean reward: 3.399 [2.048, 7.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.443, 10.493], loss: 0.191087, mae: 0.412300, mean_q: 4.623404
 77509/100000: episode: 1465, duration: 0.207s, episode steps: 37, steps per second: 179, episode reward: 114.208, mean reward: 3.087 [1.884, 5.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.419, 10.322], loss: 0.194348, mae: 0.412624, mean_q: 4.609967
 77546/100000: episode: 1466, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 94.260, mean reward: 2.548 [1.944, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.583, 10.344], loss: 0.219046, mae: 0.424423, mean_q: 4.648243
 77583/100000: episode: 1467, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 119.429, mean reward: 3.228 [2.192, 5.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.397, 10.548], loss: 0.535587, mae: 0.440648, mean_q: 4.653111
 77592/100000: episode: 1468, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 36.263, mean reward: 4.029 [2.514, 5.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.441], loss: 1.649386, mae: 0.585968, mean_q: 4.764381
 77628/100000: episode: 1469, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 160.547, mean reward: 4.460 [3.109, 7.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.206, 10.451], loss: 0.266858, mae: 0.466812, mean_q: 4.762599
 77663/100000: episode: 1470, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 99.713, mean reward: 2.849 [2.074, 4.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.707, 10.383], loss: 0.278824, mae: 0.466732, mean_q: 4.715129
 77699/100000: episode: 1471, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 84.388, mean reward: 2.344 [1.484, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.120, 10.342], loss: 0.224225, mae: 0.416497, mean_q: 4.736665
 77708/100000: episode: 1472, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 43.636, mean reward: 4.848 [3.101, 6.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-1.036, 10.568], loss: 0.252646, mae: 0.433392, mean_q: 4.585054
 77744/100000: episode: 1473, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 104.016, mean reward: 2.889 [2.052, 5.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.491, 10.364], loss: 0.286593, mae: 0.476481, mean_q: 4.839934
 77781/100000: episode: 1474, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 92.780, mean reward: 2.508 [1.711, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.013, 10.264], loss: 0.625665, mae: 0.477937, mean_q: 4.836445
 77817/100000: episode: 1475, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 106.813, mean reward: 2.967 [1.657, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.295, 10.203], loss: 0.235983, mae: 0.464202, mean_q: 4.784054
 77830/100000: episode: 1476, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 190.941, mean reward: 14.688 [4.240, 40.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.453, 10.100], loss: 0.217027, mae: 0.416580, mean_q: 4.644317
 77866/100000: episode: 1477, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 98.990, mean reward: 2.750 [2.070, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.400, 10.351], loss: 1.272913, mae: 0.554841, mean_q: 4.869958
 77871/100000: episode: 1478, duration: 0.041s, episode steps: 5, steps per second: 122, episode reward: 18.787, mean reward: 3.757 [3.384, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.488], loss: 0.284366, mae: 0.483928, mean_q: 4.905427
 77908/100000: episode: 1479, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 91.950, mean reward: 2.485 [1.860, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.658, 10.240], loss: 0.342337, mae: 0.491894, mean_q: 4.866388
 77944/100000: episode: 1480, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 90.366, mean reward: 2.510 [1.487, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.460, 10.131], loss: 0.623785, mae: 0.476065, mean_q: 4.834898
 77980/100000: episode: 1481, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 159.023, mean reward: 4.417 [2.750, 6.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.149, 10.610], loss: 0.302598, mae: 0.499076, mean_q: 4.882908
 77993/100000: episode: 1482, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 54.430, mean reward: 4.187 [2.269, 8.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.307, 10.100], loss: 0.218118, mae: 0.439156, mean_q: 4.897785
 77998/100000: episode: 1483, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 15.746, mean reward: 3.149 [2.843, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.368], loss: 0.161614, mae: 0.441060, mean_q: 4.879029
 78007/100000: episode: 1484, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 49.862, mean reward: 5.540 [3.231, 11.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.422], loss: 0.610655, mae: 0.659361, mean_q: 5.048324
 78043/100000: episode: 1485, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 155.681, mean reward: 4.324 [3.436, 5.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.719, 10.610], loss: 0.768851, mae: 0.505815, mean_q: 4.931776
 78077/100000: episode: 1486, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 99.661, mean reward: 2.931 [1.792, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.282, 10.273], loss: 0.305075, mae: 0.500925, mean_q: 4.925355
 78113/100000: episode: 1487, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 114.858, mean reward: 3.191 [2.064, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.050, 10.363], loss: 0.285528, mae: 0.451645, mean_q: 4.957680
 78149/100000: episode: 1488, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 205.096, mean reward: 5.697 [2.935, 18.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.291, 10.468], loss: 0.635225, mae: 0.503332, mean_q: 5.096586
 78186/100000: episode: 1489, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 124.021, mean reward: 3.352 [1.871, 7.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.166, 10.525], loss: 1.417242, mae: 0.587882, mean_q: 5.143385
 78191/100000: episode: 1490, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 12.266, mean reward: 2.453 [2.069, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.382], loss: 4.842538, mae: 0.861735, mean_q: 5.175217
 78196/100000: episode: 1491, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 16.171, mean reward: 3.234 [3.035, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.339], loss: 0.476473, mae: 0.759097, mean_q: 5.875001
 78232/100000: episode: 1492, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 111.833, mean reward: 3.106 [2.363, 5.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.275, 10.468], loss: 0.646871, mae: 0.605307, mean_q: 5.121897
 78237/100000: episode: 1493, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 23.911, mean reward: 4.782 [3.573, 6.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.390], loss: 0.472734, mae: 0.670937, mean_q: 5.327570
 78242/100000: episode: 1494, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 15.976, mean reward: 3.195 [2.939, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.494], loss: 0.462127, mae: 0.638149, mean_q: 4.897977
 78278/100000: episode: 1495, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 188.278, mean reward: 5.230 [3.198, 14.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.830, 10.488], loss: 1.927920, mae: 0.682546, mean_q: 5.279677
 78291/100000: episode: 1496, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 51.408, mean reward: 3.954 [3.108, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.463, 10.100], loss: 2.089122, mae: 0.729868, mean_q: 5.191899
 78296/100000: episode: 1497, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 20.076, mean reward: 4.015 [3.237, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.402], loss: 0.453350, mae: 0.682091, mean_q: 5.348250
 78305/100000: episode: 1498, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 30.805, mean reward: 3.423 [2.800, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.533], loss: 0.391766, mae: 0.505235, mean_q: 5.034789
 78341/100000: episode: 1499, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 103.276, mean reward: 2.869 [1.926, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.946, 10.530], loss: 0.840745, mae: 0.569468, mean_q: 5.132799
 78378/100000: episode: 1500, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 82.596, mean reward: 2.232 [1.628, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.035, 10.328], loss: 0.524069, mae: 0.555485, mean_q: 5.214067
 78414/100000: episode: 1501, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 176.701, mean reward: 4.908 [2.555, 10.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.644, 10.415], loss: 1.109533, mae: 0.635657, mean_q: 5.227721
 78427/100000: episode: 1502, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 59.439, mean reward: 4.572 [3.387, 7.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.524, 10.100], loss: 2.021754, mae: 0.745167, mean_q: 5.531046
 78464/100000: episode: 1503, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 127.061, mean reward: 3.434 [2.431, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.129, 10.493], loss: 0.354402, mae: 0.531243, mean_q: 5.191668
 78500/100000: episode: 1504, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 151.706, mean reward: 4.214 [2.078, 18.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.619, 10.386], loss: 1.776579, mae: 0.698077, mean_q: 5.335122
 78537/100000: episode: 1505, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 106.762, mean reward: 2.885 [1.633, 14.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.067, 10.100], loss: 1.584201, mae: 0.654245, mean_q: 5.281264
 78542/100000: episode: 1506, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 18.380, mean reward: 3.676 [3.198, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.421], loss: 0.641796, mae: 0.640716, mean_q: 5.341424
 78579/100000: episode: 1507, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 104.905, mean reward: 2.835 [1.568, 5.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.035, 10.108], loss: 0.994306, mae: 0.643070, mean_q: 5.417807
 78613/100000: episode: 1508, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 90.076, mean reward: 2.649 [1.519, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.163], loss: 0.811135, mae: 0.550273, mean_q: 5.328399
 78647/100000: episode: 1509, duration: 0.203s, episode steps: 34, steps per second: 168, episode reward: 86.064, mean reward: 2.531 [1.473, 4.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.983, 10.100], loss: 0.470206, mae: 0.601825, mean_q: 5.310512
 78684/100000: episode: 1510, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 82.196, mean reward: 2.222 [1.524, 5.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.107, 10.154], loss: 1.374261, mae: 0.710446, mean_q: 5.517828
 78689/100000: episode: 1511, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 17.840, mean reward: 3.568 [2.908, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.556], loss: 0.659930, mae: 0.717206, mean_q: 5.376796
 78698/100000: episode: 1512, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 47.585, mean reward: 5.287 [3.292, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.121, 10.533], loss: 0.973264, mae: 0.674399, mean_q: 5.452120
 78703/100000: episode: 1513, duration: 0.037s, episode steps: 5, steps per second: 134, episode reward: 16.521, mean reward: 3.304 [3.082, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.474], loss: 0.656002, mae: 0.709786, mean_q: 5.257657
 78740/100000: episode: 1514, duration: 0.203s, episode steps: 37, steps per second: 183, episode reward: 102.191, mean reward: 2.762 [1.882, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.137, 10.316], loss: 1.370354, mae: 0.629649, mean_q: 5.384990
 78776/100000: episode: 1515, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 127.545, mean reward: 3.543 [2.587, 7.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.459, 10.404], loss: 0.412606, mae: 0.567710, mean_q: 5.411561
 78810/100000: episode: 1516, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 97.311, mean reward: 2.862 [2.103, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.594, 10.412], loss: 1.142470, mae: 0.615835, mean_q: 5.424811
 78823/100000: episode: 1517, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 85.457, mean reward: 6.574 [4.585, 15.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.640, 10.100], loss: 0.286392, mae: 0.498644, mean_q: 5.313939
 78857/100000: episode: 1518, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 91.419, mean reward: 2.689 [1.680, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.005, 10.223], loss: 1.614402, mae: 0.650593, mean_q: 5.514492
 78870/100000: episode: 1519, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 47.783, mean reward: 3.676 [2.949, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.501, 10.100], loss: 0.631929, mae: 0.672047, mean_q: 5.580060
 78875/100000: episode: 1520, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 17.310, mean reward: 3.462 [2.958, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.113, 10.502], loss: 0.777446, mae: 0.628682, mean_q: 5.322160
 78880/100000: episode: 1521, duration: 0.032s, episode steps: 5, steps per second: 159, episode reward: 15.643, mean reward: 3.129 [2.921, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.451], loss: 0.508997, mae: 0.637075, mean_q: 5.574538
 78917/100000: episode: 1522, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 110.459, mean reward: 2.985 [1.888, 5.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.421, 10.265], loss: 0.861389, mae: 0.661536, mean_q: 5.463324
 78926/100000: episode: 1523, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 43.042, mean reward: 4.782 [3.313, 6.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.641], loss: 1.686158, mae: 0.680173, mean_q: 5.723265
 78963/100000: episode: 1524, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 118.653, mean reward: 3.207 [1.505, 9.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.452, 10.100], loss: 0.416443, mae: 0.585910, mean_q: 5.478966
 79000/100000: episode: 1525, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 133.993, mean reward: 3.621 [1.769, 6.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.943, 10.294], loss: 1.069393, mae: 0.660178, mean_q: 5.507052
 79037/100000: episode: 1526, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 92.580, mean reward: 2.502 [1.739, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.444, 10.337], loss: 0.745950, mae: 0.667541, mean_q: 5.533869
 79042/100000: episode: 1527, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 17.835, mean reward: 3.567 [3.211, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.528], loss: 0.341047, mae: 0.591839, mean_q: 5.759548
 79078/100000: episode: 1528, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 93.596, mean reward: 2.600 [1.465, 7.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.067, 10.188], loss: 0.621971, mae: 0.608604, mean_q: 5.525716
 79114/100000: episode: 1529, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 95.467, mean reward: 2.652 [1.608, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.061, 10.235], loss: 1.040648, mae: 0.710261, mean_q: 5.640734
 79127/100000: episode: 1530, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 62.227, mean reward: 4.787 [3.979, 6.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.404, 10.100], loss: 0.335437, mae: 0.544217, mean_q: 5.453403
 79163/100000: episode: 1531, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 122.574, mean reward: 3.405 [1.786, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.404, 10.297], loss: 1.770523, mae: 0.713866, mean_q: 5.699297
 79176/100000: episode: 1532, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 61.368, mean reward: 4.721 [3.683, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.498, 10.100], loss: 0.325443, mae: 0.535476, mean_q: 5.268654
 79212/100000: episode: 1533, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 179.332, mean reward: 4.981 [3.235, 13.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.431, 10.485], loss: 1.365146, mae: 0.682193, mean_q: 5.702775
 79217/100000: episode: 1534, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 18.503, mean reward: 3.701 [2.905, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.538], loss: 0.446242, mae: 0.651886, mean_q: 5.770659
 79253/100000: episode: 1535, duration: 0.223s, episode steps: 36, steps per second: 161, episode reward: 100.109, mean reward: 2.781 [1.600, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.121, 10.240], loss: 0.431293, mae: 0.587374, mean_q: 5.564518
 79258/100000: episode: 1536, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 17.871, mean reward: 3.574 [3.008, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.542], loss: 0.454235, mae: 0.667593, mean_q: 5.792386
 79267/100000: episode: 1537, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 26.877, mean reward: 2.986 [2.587, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.486], loss: 1.895909, mae: 0.793058, mean_q: 5.746759
 79304/100000: episode: 1538, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 122.236, mean reward: 3.304 [2.100, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.136, 10.346], loss: 0.481305, mae: 0.613065, mean_q: 5.679130
 79340/100000: episode: 1539, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 122.564, mean reward: 3.405 [2.264, 12.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.677, 10.504], loss: 0.539078, mae: 0.642257, mean_q: 5.676817
[Info] 3-TH LEVEL FOUND: 11.386035919189453, Considering 10/90 traces
 79345/100000: episode: 1540, duration: 4.703s, episode steps: 5, steps per second: 1, episode reward: 19.759, mean reward: 3.952 [2.971, 4.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.418], loss: 0.563061, mae: 0.637452, mean_q: 5.650168
 79354/100000: episode: 1541, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 63.642, mean reward: 7.071 [4.646, 13.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.984, 10.100], loss: 0.489539, mae: 0.613152, mean_q: 5.577338
 79380/100000: episode: 1542, duration: 0.167s, episode steps: 26, steps per second: 155, episode reward: 147.830, mean reward: 5.686 [3.614, 17.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.824, 10.644], loss: 1.414335, mae: 0.693701, mean_q: 5.752038
 79404/100000: episode: 1543, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 154.903, mean reward: 6.454 [3.143, 24.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.213, 10.583], loss: 1.242097, mae: 0.706609, mean_q: 5.818396
 79420/100000: episode: 1544, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 71.534, mean reward: 4.471 [3.047, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.373, 10.502], loss: 0.730262, mae: 0.708925, mean_q: 5.736463
 79436/100000: episode: 1545, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 110.346, mean reward: 6.897 [2.809, 39.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.396, 10.350], loss: 0.567899, mae: 0.608953, mean_q: 5.739197
 79445/100000: episode: 1546, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 40.753, mean reward: 4.528 [3.790, 5.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.406, 10.100], loss: 1.602399, mae: 0.803492, mean_q: 5.998112
[Info] FALSIFICATION!
 79448/100000: episode: 1547, duration: 0.244s, episode steps: 3, steps per second: 12, episode reward: 1019.442, mean reward: 339.814 [5.171, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.271, 9.952], loss: 0.848258, mae: 0.956420, mean_q: 6.340666
 79457/100000: episode: 1548, duration: 0.087s, episode steps: 9, steps per second: 103, episode reward: 86.366, mean reward: 9.596 [6.098, 18.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.614, 10.100], loss: 0.601674, mae: 0.759535, mean_q: 5.714784
 79466/100000: episode: 1549, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 44.699, mean reward: 4.967 [3.643, 6.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.519, 10.100], loss: 1.028767, mae: 0.814000, mean_q: 6.117062
 79487/100000: episode: 1550, duration: 0.135s, episode steps: 21, steps per second: 155, episode reward: 139.516, mean reward: 6.644 [4.168, 14.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.538], loss: 0.549524, mae: 0.678022, mean_q: 5.796743
 79508/100000: episode: 1551, duration: 0.122s, episode steps: 21, steps per second: 173, episode reward: 120.317, mean reward: 5.729 [2.750, 16.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.447], loss: 727.141235, mae: 2.226608, mean_q: 5.923532
 79529/100000: episode: 1552, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 169.248, mean reward: 8.059 [3.709, 23.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-1.955, 10.595], loss: 6.795454, mae: 2.486008, mean_q: 7.684979
 79553/100000: episode: 1553, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 100.580, mean reward: 4.191 [2.502, 9.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.448], loss: 641.019470, mae: 2.881786, mean_q: 6.464304
 79569/100000: episode: 1554, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 77.812, mean reward: 4.863 [3.973, 7.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.366, 10.587], loss: 1.650936, mae: 1.142874, mean_q: 6.654655
 79593/100000: episode: 1555, duration: 0.190s, episode steps: 24, steps per second: 126, episode reward: 129.343, mean reward: 5.389 [2.062, 24.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.179, 10.471], loss: 1.264292, mae: 0.936157, mean_q: 6.315836
 79602/100000: episode: 1556, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 50.109, mean reward: 5.568 [3.049, 7.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.620, 10.100], loss: 0.759378, mae: 0.754370, mean_q: 6.244079
 79626/100000: episode: 1557, duration: 0.179s, episode steps: 24, steps per second: 134, episode reward: 99.874, mean reward: 4.161 [2.440, 8.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.239, 10.429], loss: 635.045349, mae: 2.062230, mean_q: 6.217130
 79650/100000: episode: 1558, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 108.656, mean reward: 4.527 [3.259, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.320, 10.516], loss: 4.925241, mae: 2.155696, mean_q: 7.994801
 79666/100000: episode: 1559, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 79.326, mean reward: 4.958 [3.969, 6.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.637], loss: 1.800712, mae: 1.105917, mean_q: 6.771564
 79691/100000: episode: 1560, duration: 0.177s, episode steps: 25, steps per second: 141, episode reward: 288.450, mean reward: 11.538 [1.719, 121.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.222, 10.389], loss: 2.702952, mae: 1.105600, mean_q: 6.485190
 79702/100000: episode: 1561, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 118.789, mean reward: 10.799 [4.166, 28.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.390, 10.100], loss: 0.957950, mae: 0.892048, mean_q: 6.175727
 79723/100000: episode: 1562, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 108.620, mean reward: 5.172 [2.809, 10.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.383, 10.443], loss: 2.338215, mae: 1.051252, mean_q: 6.588204
 79744/100000: episode: 1563, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 89.283, mean reward: 4.252 [2.554, 7.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.408], loss: 1.120629, mae: 0.934371, mean_q: 6.544976
 79769/100000: episode: 1564, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 226.307, mean reward: 9.052 [3.771, 37.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.391, 10.526], loss: 19.540127, mae: 1.389614, mean_q: 6.601717
 79790/100000: episode: 1565, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 68.814, mean reward: 3.277 [1.828, 7.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.243, 10.289], loss: 1.631373, mae: 0.957134, mean_q: 6.401793
 79799/100000: episode: 1566, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 58.756, mean reward: 6.528 [3.567, 14.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.695, 10.100], loss: 1.188846, mae: 0.977107, mean_q: 6.528021
 79809/100000: episode: 1567, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 55.574, mean reward: 5.557 [4.188, 8.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.565, 10.100], loss: 2.181900, mae: 0.993642, mean_q: 6.707297
 79819/100000: episode: 1568, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 122.020, mean reward: 12.202 [5.013, 22.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.617, 10.100], loss: 0.917303, mae: 0.825831, mean_q: 6.357017
 79840/100000: episode: 1569, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 85.229, mean reward: 4.059 [1.876, 8.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.628, 10.330], loss: 11.653077, mae: 1.297848, mean_q: 6.877175
 79856/100000: episode: 1570, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 84.928, mean reward: 5.308 [3.721, 8.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.613], loss: 2.513433, mae: 1.028948, mean_q: 5.913822
 79872/100000: episode: 1571, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 49.236, mean reward: 3.077 [2.048, 4.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.635, 10.397], loss: 2.763134, mae: 1.017688, mean_q: 6.697640
 79896/100000: episode: 1572, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 102.537, mean reward: 4.272 [2.797, 6.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.450, 10.507], loss: 11.693958, mae: 1.203930, mean_q: 6.649437
 79922/100000: episode: 1573, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 151.102, mean reward: 5.812 [3.800, 11.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.039, 10.530], loss: 1.177170, mae: 0.949481, mean_q: 6.392407
 79948/100000: episode: 1574, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 106.301, mean reward: 4.088 [2.753, 6.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.505], loss: 2.930231, mae: 0.987896, mean_q: 6.751356
 79972/100000: episode: 1575, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 145.068, mean reward: 6.044 [3.942, 11.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.609], loss: 1.823949, mae: 0.895546, mean_q: 6.531634
 79998/100000: episode: 1576, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 127.291, mean reward: 4.896 [3.442, 6.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.177, 10.638], loss: 2.682152, mae: 1.529830, mean_q: 7.371301
 80019/100000: episode: 1577, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 121.058, mean reward: 5.765 [3.673, 14.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.711, 10.535], loss: 2.793524, mae: 1.106756, mean_q: 6.680812
 80030/100000: episode: 1578, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 65.413, mean reward: 5.947 [4.457, 9.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.532, 10.100], loss: 2.112383, mae: 0.997181, mean_q: 6.824069
 80051/100000: episode: 1579, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 88.155, mean reward: 4.198 [2.367, 13.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.175, 10.455], loss: 1.029941, mae: 0.907036, mean_q: 6.992125
[Info] FALSIFICATION!
 80063/100000: episode: 1580, duration: 0.251s, episode steps: 12, steps per second: 48, episode reward: 1575.665, mean reward: 131.305 [6.024, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.017, 10.206], loss: 1271.715210, mae: 3.950048, mean_q: 7.068729
 80074/100000: episode: 1581, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 147.986, mean reward: 13.453 [5.516, 27.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.646, 10.100], loss: 8.736609, mae: 2.610046, mean_q: 8.710039
 80090/100000: episode: 1582, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 61.044, mean reward: 3.815 [2.483, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.471], loss: 4.654181, mae: 1.776905, mean_q: 8.134739
 80115/100000: episode: 1583, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 109.216, mean reward: 4.369 [2.265, 8.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.231, 10.437], loss: 10.892501, mae: 1.355272, mean_q: 6.846089
 80125/100000: episode: 1584, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 48.896, mean reward: 4.890 [3.456, 7.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.528, 10.100], loss: 185.836899, mae: 3.037805, mean_q: 7.905384
 80150/100000: episode: 1585, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 111.569, mean reward: 4.463 [2.182, 9.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.138, 10.368], loss: 2.173528, mae: 1.258770, mean_q: 6.744673
 80175/100000: episode: 1586, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 205.696, mean reward: 8.228 [3.782, 20.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.786, 10.524], loss: 1.940289, mae: 1.065699, mean_q: 7.235966
 80184/100000: episode: 1587, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 65.154, mean reward: 7.239 [5.574, 10.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.533, 10.100], loss: 1.514150, mae: 0.785761, mean_q: 6.682703
 80205/100000: episode: 1588, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 70.698, mean reward: 3.367 [2.015, 7.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.604, 10.291], loss: 2.426700, mae: 1.034621, mean_q: 7.045889
 80230/100000: episode: 1589, duration: 0.154s, episode steps: 25, steps per second: 162, episode reward: 93.806, mean reward: 3.752 [1.857, 10.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.244, 10.294], loss: 5.891519, mae: 1.349884, mean_q: 7.422191
 80240/100000: episode: 1590, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 48.257, mean reward: 4.826 [3.714, 7.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.674, 10.100], loss: 181.714157, mae: 2.383474, mean_q: 7.727445
[Info] FALSIFICATION!
 80245/100000: episode: 1591, duration: 0.207s, episode steps: 5, steps per second: 24, episode reward: 1029.087, mean reward: 205.817 [4.536, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.354, 10.020], loss: 4.661544, mae: 2.131686, mean_q: 8.632248
[Info] FALSIFICATION!
 80251/100000: episode: 1592, duration: 0.197s, episode steps: 6, steps per second: 30, episode reward: 1314.358, mean reward: 219.060 [6.364, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.396, 9.381], loss: 2.333285, mae: 1.408469, mean_q: 6.213861
 80261/100000: episode: 1593, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 48.886, mean reward: 4.889 [3.527, 6.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.994, 10.100], loss: 5.916389, mae: 1.290457, mean_q: 6.561929
 80270/100000: episode: 1594, duration: 0.056s, episode steps: 9, steps per second: 159, episode reward: 62.936, mean reward: 6.993 [5.349, 10.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.633, 10.100], loss: 28.866533, mae: 1.783145, mean_q: 7.418933
 80286/100000: episode: 1595, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 60.511, mean reward: 3.782 [2.577, 5.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.413], loss: 2.336425, mae: 1.346701, mean_q: 7.585165
 80312/100000: episode: 1596, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 152.518, mean reward: 5.866 [4.777, 7.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.103, 10.584], loss: 4.327387, mae: 1.327730, mean_q: 7.237625
 80322/100000: episode: 1597, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 54.396, mean reward: 5.440 [3.745, 10.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.308, 10.100], loss: 2.318897, mae: 1.183347, mean_q: 7.672193
 80347/100000: episode: 1598, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 147.335, mean reward: 5.893 [2.799, 15.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.433, 10.435], loss: 642.147766, mae: 4.162838, mean_q: 9.084228
 80357/100000: episode: 1599, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 60.047, mean reward: 6.005 [3.371, 10.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.477, 10.100], loss: 1509.075928, mae: 5.382449, mean_q: 8.955375
 80367/100000: episode: 1600, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 56.092, mean reward: 5.609 [4.473, 7.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.389, 10.100], loss: 27.240667, mae: 2.490082, mean_q: 8.123365
 80378/100000: episode: 1601, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 81.632, mean reward: 7.421 [5.027, 16.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.536, 10.100], loss: 2746.387695, mae: 9.131217, mean_q: 10.460809
 80387/100000: episode: 1602, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 63.699, mean reward: 7.078 [5.509, 10.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.392, 10.100], loss: 16.144493, mae: 4.319037, mean_q: 11.738611
 80397/100000: episode: 1603, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 136.324, mean reward: 13.632 [4.587, 29.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.631, 10.100], loss: 8.559522, mae: 2.978079, mean_q: 7.381796
 80407/100000: episode: 1604, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 56.365, mean reward: 5.637 [3.687, 8.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.435, 10.100], loss: 1510.502075, mae: 4.956671, mean_q: 7.964645
 80418/100000: episode: 1605, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 73.076, mean reward: 6.643 [4.627, 11.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.599, 10.100], loss: 12.308300, mae: 3.291737, mean_q: 9.613542
 80427/100000: episode: 1606, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 48.323, mean reward: 5.369 [4.265, 6.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.127, 10.100], loss: 9.518330, mae: 3.025290, mean_q: 8.987968
 80443/100000: episode: 1607, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 59.492, mean reward: 3.718 [2.792, 5.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.521, 10.494], loss: 1897.932251, mae: 5.848436, mean_q: 8.951593
 80467/100000: episode: 1608, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 282.909, mean reward: 11.788 [3.135, 106.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.246, 10.575], loss: 10.203599, mae: 2.914801, mean_q: 10.375922
 80493/100000: episode: 1609, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 83.918, mean reward: 3.228 [2.678, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.281, 10.520], loss: 15.397051, mae: 1.916850, mean_q: 8.021457
 80503/100000: episode: 1610, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 98.280, mean reward: 9.828 [5.118, 29.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.652, 10.100], loss: 7.600871, mae: 1.823937, mean_q: 8.578871
 80513/100000: episode: 1611, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 161.212, mean reward: 16.121 [7.798, 33.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.588, 10.100], loss: 1518.714600, mae: 4.902325, mean_q: 8.757280
 80534/100000: episode: 1612, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 83.622, mean reward: 3.982 [2.759, 5.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.333, 10.427], loss: 1436.347534, mae: 6.059788, mean_q: 10.768343
 80543/100000: episode: 1613, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 36.342, mean reward: 4.038 [3.359, 5.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.372, 10.100], loss: 203.724411, mae: 3.911302, mean_q: 11.061382
 80568/100000: episode: 1614, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 118.254, mean reward: 4.730 [2.223, 8.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.254, 10.423], loss: 605.527954, mae: 3.077963, mean_q: 8.760713
 80593/100000: episode: 1615, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 144.585, mean reward: 5.783 [3.877, 8.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.582], loss: 5.439932, mae: 2.220836, mean_q: 9.829702
 80617/100000: episode: 1616, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 114.055, mean reward: 4.752 [2.458, 9.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.807, 10.399], loss: 623.071716, mae: 2.860603, mean_q: 7.953246
 80643/100000: episode: 1617, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 111.892, mean reward: 4.304 [2.815, 6.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.226, 10.448], loss: 20.263517, mae: 2.930353, mean_q: 9.388506
 80664/100000: episode: 1618, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 82.702, mean reward: 3.938 [2.235, 7.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.563], loss: 755.631348, mae: 3.799972, mean_q: 8.764915
 80688/100000: episode: 1619, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 105.885, mean reward: 4.412 [2.766, 7.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.477], loss: 7.541455, mae: 2.515139, mean_q: 9.805476
 80697/100000: episode: 1620, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 159.975, mean reward: 17.775 [5.125, 53.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.402, 10.100], loss: 1648.162720, mae: 5.148049, mean_q: 8.220796
 80718/100000: episode: 1621, duration: 0.136s, episode steps: 21, steps per second: 154, episode reward: 116.660, mean reward: 5.555 [3.324, 9.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.382, 10.680], loss: 10.712818, mae: 2.885527, mean_q: 9.561628
 80729/100000: episode: 1622, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 68.955, mean reward: 6.269 [4.330, 9.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.555, 10.100], loss: 2721.588379, mae: 7.494525, mean_q: 8.714011
 80755/100000: episode: 1623, duration: 0.160s, episode steps: 26, steps per second: 162, episode reward: 116.071, mean reward: 4.464 [2.661, 9.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.317, 10.419], loss: 174.773422, mae: 4.416188, mean_q: 11.165604
 80776/100000: episode: 1624, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 85.888, mean reward: 4.090 [2.396, 7.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.603, 10.435], loss: 5.883825, mae: 2.139059, mean_q: 8.122823
 80792/100000: episode: 1625, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 52.635, mean reward: 3.290 [2.513, 6.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.384], loss: 940.831177, mae: 3.600761, mean_q: 8.037525
[Info] FALSIFICATION!
 80811/100000: episode: 1626, duration: 0.280s, episode steps: 19, steps per second: 68, episode reward: 1238.003, mean reward: 65.158 [5.285, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.054, 10.330], loss: 6.292250, mae: 2.347727, mean_q: 9.317456
 80835/100000: episode: 1627, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 453.536, mean reward: 18.897 [2.635, 240.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.802, 10.566], loss: 1266.722656, mae: 5.782247, mean_q: 10.177646
 80860/100000: episode: 1628, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 243.540, mean reward: 9.742 [2.951, 115.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.062, 10.442], loss: 625.453247, mae: 6.089911, mean_q: 12.285647
 80885/100000: episode: 1629, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 76.486, mean reward: 3.059 [2.188, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.786, 10.377], loss: 633.588623, mae: 4.882607, mean_q: 10.294194
[Info] Complete ISplit Iteration
[Info] Levels: [5.8085804, 7.067761, 11.386036, 19.016895]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.76]
[Info] Error Prob: 0.0007600000000000002

 80906/100000: episode: 1630, duration: 4.422s, episode steps: 21, steps per second: 5, episode reward: 94.657, mean reward: 4.507 [3.130, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.436, 10.589], loss: 845.460815, mae: 6.398553, mean_q: 11.999587
 81006/100000: episode: 1631, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 216.040, mean reward: 2.160 [1.569, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.221, 10.098], loss: 309.587616, mae: 3.471568, mean_q: 9.343699
 81106/100000: episode: 1632, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.808, mean reward: 1.908 [1.439, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.167, 10.098], loss: 316.216003, mae: 3.201080, mean_q: 9.125849
 81206/100000: episode: 1633, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 204.211, mean reward: 2.042 [1.451, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.840, 10.200], loss: 604.614868, mae: 3.712685, mean_q: 9.297788
 81306/100000: episode: 1634, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.577, mean reward: 1.906 [1.518, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.655, 10.148], loss: 384.775024, mae: 4.081455, mean_q: 9.956285
 81406/100000: episode: 1635, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 202.706, mean reward: 2.027 [1.467, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.298, 10.098], loss: 802.500610, mae: 4.801045, mean_q: 10.119244
 81506/100000: episode: 1636, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 209.020, mean reward: 2.090 [1.488, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.945, 10.098], loss: 788.724365, mae: 4.843764, mean_q: 10.312897
 81606/100000: episode: 1637, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.771, mean reward: 1.948 [1.487, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.925, 10.098], loss: 472.282410, mae: 4.045351, mean_q: 10.205028
 81706/100000: episode: 1638, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.731, mean reward: 1.957 [1.450, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.158, 10.098], loss: 353.525665, mae: 3.706559, mean_q: 9.764230
 81806/100000: episode: 1639, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.469, mean reward: 1.865 [1.446, 6.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.663, 10.218], loss: 765.676147, mae: 4.747589, mean_q: 9.929354
 81906/100000: episode: 1640, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 195.655, mean reward: 1.957 [1.450, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.136], loss: 333.079468, mae: 3.787617, mean_q: 9.353671
 82006/100000: episode: 1641, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.957, mean reward: 1.930 [1.468, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.537, 10.098], loss: 779.212280, mae: 4.697298, mean_q: 9.572796
 82106/100000: episode: 1642, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 187.794, mean reward: 1.878 [1.486, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.602, 10.098], loss: 885.861633, mae: 5.319520, mean_q: 10.602264
 82206/100000: episode: 1643, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.519, mean reward: 1.925 [1.475, 6.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.800, 10.098], loss: 780.226868, mae: 5.060944, mean_q: 10.166893
 82306/100000: episode: 1644, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 200.516, mean reward: 2.005 [1.443, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.356, 10.258], loss: 309.784668, mae: 3.459431, mean_q: 9.151468
 82406/100000: episode: 1645, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 201.448, mean reward: 2.014 [1.471, 15.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.113], loss: 319.887634, mae: 3.129693, mean_q: 8.713491
 82506/100000: episode: 1646, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 184.521, mean reward: 1.845 [1.462, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.973, 10.103], loss: 773.573059, mae: 4.860915, mean_q: 9.881601
 82606/100000: episode: 1647, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 224.355, mean reward: 2.244 [1.477, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.122, 10.098], loss: 747.742554, mae: 4.913649, mean_q: 9.486497
 82706/100000: episode: 1648, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 197.131, mean reward: 1.971 [1.439, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.098], loss: 487.140015, mae: 3.989698, mean_q: 8.967288
 82806/100000: episode: 1649, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.995, mean reward: 1.970 [1.439, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.230], loss: 909.812561, mae: 5.345032, mean_q: 10.019101
 82906/100000: episode: 1650, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.227, mean reward: 1.952 [1.475, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.265], loss: 189.566956, mae: 3.045440, mean_q: 8.687290
 83006/100000: episode: 1651, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 188.215, mean reward: 1.882 [1.485, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.216], loss: 512.350220, mae: 3.429128, mean_q: 8.356594
 83106/100000: episode: 1652, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 183.359, mean reward: 1.834 [1.443, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.520, 10.103], loss: 322.795532, mae: 3.284346, mean_q: 8.911007
 83206/100000: episode: 1653, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.546, mean reward: 1.955 [1.448, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.545, 10.098], loss: 602.118958, mae: 3.926038, mean_q: 8.877650
 83306/100000: episode: 1654, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 208.694, mean reward: 2.087 [1.490, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.784, 10.098], loss: 754.912659, mae: 4.036381, mean_q: 8.439380
 83406/100000: episode: 1655, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.143, mean reward: 2.001 [1.455, 5.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 195.216843, mae: 3.134321, mean_q: 8.583904
 83506/100000: episode: 1656, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.413, mean reward: 1.914 [1.457, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.812, 10.098], loss: 915.417175, mae: 5.247982, mean_q: 9.316265
 83606/100000: episode: 1657, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.815, mean reward: 1.848 [1.445, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.373, 10.098], loss: 310.667236, mae: 2.975943, mean_q: 8.117909
 83706/100000: episode: 1658, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.663, mean reward: 1.897 [1.454, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.698, 10.215], loss: 898.015076, mae: 4.887150, mean_q: 9.085788
 83806/100000: episode: 1659, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.720, mean reward: 1.807 [1.460, 2.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.597, 10.098], loss: 206.054077, mae: 3.056355, mean_q: 8.336393
 83906/100000: episode: 1660, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.791, mean reward: 1.908 [1.442, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.609, 10.098], loss: 756.347046, mae: 4.765672, mean_q: 8.856370
 84006/100000: episode: 1661, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.688, mean reward: 1.947 [1.513, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 596.129700, mae: 3.143947, mean_q: 7.580979
 84106/100000: episode: 1662, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 183.551, mean reward: 1.836 [1.460, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.161, 10.098], loss: 73.945724, mae: 2.566844, mean_q: 7.694449
 84206/100000: episode: 1663, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.239, mean reward: 1.962 [1.438, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.247, 10.213], loss: 503.197723, mae: 4.047019, mean_q: 8.159026
 84306/100000: episode: 1664, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.951, mean reward: 1.950 [1.470, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.738, 10.098], loss: 312.528137, mae: 2.635689, mean_q: 7.243585
 84406/100000: episode: 1665, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 201.931, mean reward: 2.019 [1.516, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.738, 10.139], loss: 920.086975, mae: 5.544846, mean_q: 8.876246
 84506/100000: episode: 1666, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 218.895, mean reward: 2.189 [1.510, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.362, 10.098], loss: 327.741791, mae: 2.885676, mean_q: 7.008610
 84606/100000: episode: 1667, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 201.849, mean reward: 2.018 [1.451, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.976, 10.098], loss: 495.697632, mae: 3.872149, mean_q: 7.996456
 84706/100000: episode: 1668, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 196.025, mean reward: 1.960 [1.473, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.875, 10.129], loss: 183.793579, mae: 2.701997, mean_q: 6.867283
 84806/100000: episode: 1669, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 194.146, mean reward: 1.941 [1.476, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.228], loss: 327.304291, mae: 3.096268, mean_q: 7.197481
 84906/100000: episode: 1670, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.926, mean reward: 1.919 [1.477, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.372, 10.168], loss: 869.843567, mae: 4.673226, mean_q: 7.675173
 85006/100000: episode: 1671, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 217.993, mean reward: 2.180 [1.452, 5.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.408 [-1.003, 10.098], loss: 321.078918, mae: 3.210858, mean_q: 6.558920
 85106/100000: episode: 1672, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 192.044, mean reward: 1.920 [1.491, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.389], loss: 560.658386, mae: 3.662425, mean_q: 6.591620
 85206/100000: episode: 1673, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 192.166, mean reward: 1.922 [1.437, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.086, 10.098], loss: 319.869171, mae: 2.966411, mean_q: 6.344847
 85306/100000: episode: 1674, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 174.928, mean reward: 1.749 [1.438, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.540, 10.098], loss: 17.120934, mae: 1.735013, mean_q: 5.401177
 85406/100000: episode: 1675, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 227.725, mean reward: 2.277 [1.464, 5.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.190, 10.600], loss: 8.402190, mae: 1.383434, mean_q: 4.948525
 85506/100000: episode: 1676, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 204.679, mean reward: 2.047 [1.482, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.409, 10.098], loss: 146.838806, mae: 1.384464, mean_q: 4.547359
 85606/100000: episode: 1677, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 202.476, mean reward: 2.025 [1.519, 4.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.237, 10.232], loss: 422.124725, mae: 2.871424, mean_q: 5.300903
 85706/100000: episode: 1678, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 207.722, mean reward: 2.077 [1.454, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.109, 10.098], loss: 15.972862, mae: 1.479013, mean_q: 4.502058
 85806/100000: episode: 1679, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 213.540, mean reward: 2.135 [1.494, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.868, 10.295], loss: 5.720260, mae: 1.094656, mean_q: 4.223049
 85906/100000: episode: 1680, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.899, mean reward: 2.009 [1.481, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.131, 10.291], loss: 0.783082, mae: 0.831811, mean_q: 3.975063
 86006/100000: episode: 1681, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 226.203, mean reward: 2.262 [1.487, 4.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.855, 10.098], loss: 0.828254, mae: 0.853601, mean_q: 3.940275
 86106/100000: episode: 1682, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 198.246, mean reward: 1.982 [1.468, 4.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.220, 10.098], loss: 0.726963, mae: 0.815046, mean_q: 3.916342
 86206/100000: episode: 1683, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 204.833, mean reward: 2.048 [1.537, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.557, 10.098], loss: 0.631552, mae: 0.781785, mean_q: 3.891182
 86306/100000: episode: 1684, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.780, mean reward: 2.028 [1.448, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.127], loss: 0.596871, mae: 0.760878, mean_q: 3.901891
 86406/100000: episode: 1685, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 207.814, mean reward: 2.078 [1.498, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.709, 10.431], loss: 0.560047, mae: 0.737809, mean_q: 3.909880
 86506/100000: episode: 1686, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.754, mean reward: 1.898 [1.450, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.238, 10.098], loss: 0.594367, mae: 0.732680, mean_q: 3.914962
 86606/100000: episode: 1687, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 200.314, mean reward: 2.003 [1.473, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.699, 10.277], loss: 0.570905, mae: 0.729274, mean_q: 3.929460
 86706/100000: episode: 1688, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 193.267, mean reward: 1.933 [1.477, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.594, 10.228], loss: 0.518105, mae: 0.692048, mean_q: 3.899172
 86806/100000: episode: 1689, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.468, mean reward: 1.885 [1.456, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.768, 10.098], loss: 0.458443, mae: 0.669929, mean_q: 3.873945
 86906/100000: episode: 1690, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 202.668, mean reward: 2.027 [1.435, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.185, 10.098], loss: 0.437842, mae: 0.663195, mean_q: 3.882231
 87006/100000: episode: 1691, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.549, mean reward: 1.835 [1.454, 2.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.256, 10.098], loss: 0.420936, mae: 0.641948, mean_q: 3.897249
 87106/100000: episode: 1692, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 214.764, mean reward: 2.148 [1.457, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.458, 10.352], loss: 0.442460, mae: 0.632257, mean_q: 3.915922
 87206/100000: episode: 1693, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 196.526, mean reward: 1.965 [1.477, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.810, 10.291], loss: 0.478025, mae: 0.634792, mean_q: 3.895567
 87306/100000: episode: 1694, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.890, mean reward: 1.839 [1.471, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.259, 10.098], loss: 0.407521, mae: 0.615223, mean_q: 3.936627
 87406/100000: episode: 1695, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.780, mean reward: 1.908 [1.451, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.966, 10.098], loss: 0.400139, mae: 0.597614, mean_q: 3.899506
 87506/100000: episode: 1696, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 214.635, mean reward: 2.146 [1.438, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.261, 10.098], loss: 0.332342, mae: 0.571602, mean_q: 3.896054
 87606/100000: episode: 1697, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.325, mean reward: 1.893 [1.449, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.125], loss: 0.345461, mae: 0.575796, mean_q: 3.922556
 87706/100000: episode: 1698, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 188.347, mean reward: 1.883 [1.474, 5.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.890, 10.131], loss: 0.329294, mae: 0.570148, mean_q: 3.901094
 87806/100000: episode: 1699, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 170.163, mean reward: 1.702 [1.438, 2.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.148], loss: 0.336724, mae: 0.573192, mean_q: 3.904977
 87906/100000: episode: 1700, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.253, mean reward: 2.003 [1.467, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.362, 10.098], loss: 0.295826, mae: 0.537742, mean_q: 3.880925
 88006/100000: episode: 1701, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.930, mean reward: 1.959 [1.475, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.195, 10.149], loss: 0.303468, mae: 0.546506, mean_q: 3.895162
 88106/100000: episode: 1702, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 202.133, mean reward: 2.021 [1.493, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.905, 10.336], loss: 0.279041, mae: 0.527567, mean_q: 3.888163
 88206/100000: episode: 1703, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 176.990, mean reward: 1.770 [1.439, 2.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.781, 10.098], loss: 0.294435, mae: 0.539351, mean_q: 3.892663
 88306/100000: episode: 1704, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 205.873, mean reward: 2.059 [1.435, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.969, 10.098], loss: 0.245360, mae: 0.502676, mean_q: 3.898221
 88406/100000: episode: 1705, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 195.977, mean reward: 1.960 [1.488, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.804, 10.169], loss: 0.235907, mae: 0.482913, mean_q: 3.904833
 88506/100000: episode: 1706, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 179.661, mean reward: 1.797 [1.465, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.358, 10.098], loss: 0.219335, mae: 0.468437, mean_q: 3.894037
 88606/100000: episode: 1707, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 196.081, mean reward: 1.961 [1.448, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.366, 10.253], loss: 0.226450, mae: 0.465598, mean_q: 3.882735
 88706/100000: episode: 1708, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 201.480, mean reward: 2.015 [1.474, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.175, 10.372], loss: 0.203345, mae: 0.454466, mean_q: 3.899729
 88806/100000: episode: 1709, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 194.750, mean reward: 1.948 [1.468, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.963, 10.163], loss: 0.190248, mae: 0.448597, mean_q: 3.915964
 88906/100000: episode: 1710, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 208.881, mean reward: 2.089 [1.487, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.167, 10.098], loss: 0.202806, mae: 0.452656, mean_q: 3.898990
 89006/100000: episode: 1711, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 197.654, mean reward: 1.977 [1.489, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.178, 10.098], loss: 0.206357, mae: 0.455345, mean_q: 3.918929
 89106/100000: episode: 1712, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 212.138, mean reward: 2.121 [1.474, 5.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.480, 10.528], loss: 0.197935, mae: 0.434552, mean_q: 3.918877
 89206/100000: episode: 1713, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 185.872, mean reward: 1.859 [1.491, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.213, 10.098], loss: 0.187327, mae: 0.425354, mean_q: 3.931842
 89306/100000: episode: 1714, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 205.168, mean reward: 2.052 [1.447, 6.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.429, 10.308], loss: 0.198792, mae: 0.419658, mean_q: 3.898819
 89406/100000: episode: 1715, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.409, mean reward: 1.834 [1.469, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.402, 10.150], loss: 0.223441, mae: 0.435461, mean_q: 3.929697
 89506/100000: episode: 1716, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.402, mean reward: 1.964 [1.450, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.845, 10.189], loss: 0.163729, mae: 0.401481, mean_q: 3.912309
 89606/100000: episode: 1717, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.795, mean reward: 1.888 [1.483, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.296], loss: 0.163219, mae: 0.404437, mean_q: 3.910904
 89706/100000: episode: 1718, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.092, mean reward: 1.831 [1.520, 2.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.741, 10.280], loss: 0.185041, mae: 0.401973, mean_q: 3.920138
 89806/100000: episode: 1719, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 201.767, mean reward: 2.018 [1.445, 6.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.603, 10.242], loss: 0.155351, mae: 0.383288, mean_q: 3.903537
 89906/100000: episode: 1720, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 181.594, mean reward: 1.816 [1.517, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.452, 10.110], loss: 0.154813, mae: 0.384414, mean_q: 3.905146
 90006/100000: episode: 1721, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.578, mean reward: 1.936 [1.444, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.624, 10.098], loss: 0.128052, mae: 0.370839, mean_q: 3.894518
 90106/100000: episode: 1722, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 190.940, mean reward: 1.909 [1.439, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.209], loss: 0.134056, mae: 0.373395, mean_q: 3.886728
 90206/100000: episode: 1723, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 214.478, mean reward: 2.145 [1.441, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.431, 10.098], loss: 0.133115, mae: 0.361767, mean_q: 3.876632
 90306/100000: episode: 1724, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.685, mean reward: 1.927 [1.439, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.638, 10.098], loss: 0.125179, mae: 0.356916, mean_q: 3.898434
 90406/100000: episode: 1725, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 200.298, mean reward: 2.003 [1.479, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-2.340, 10.098], loss: 0.125587, mae: 0.362371, mean_q: 3.898380
 90506/100000: episode: 1726, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.242, mean reward: 1.912 [1.475, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.172, 10.292], loss: 0.105817, mae: 0.337098, mean_q: 3.902667
 90606/100000: episode: 1727, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.635, mean reward: 1.886 [1.445, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.259], loss: 0.116950, mae: 0.348484, mean_q: 3.878578
 90706/100000: episode: 1728, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 184.789, mean reward: 1.848 [1.450, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.774, 10.184], loss: 0.123941, mae: 0.348145, mean_q: 3.902225
 90806/100000: episode: 1729, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.205, mean reward: 1.892 [1.501, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.641, 10.098], loss: 0.108470, mae: 0.334917, mean_q: 3.864773
[Info] 1-TH LEVEL FOUND: 5.914305210113525, Considering 10/90 traces
 90906/100000: episode: 1730, duration: 4.724s, episode steps: 100, steps per second: 21, episode reward: 187.551, mean reward: 1.876 [1.433, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.651, 10.119], loss: 0.108660, mae: 0.330750, mean_q: 3.860510
 90922/100000: episode: 1731, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 39.393, mean reward: 2.462 [2.157, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.394], loss: 0.127587, mae: 0.351727, mean_q: 3.850610
 90954/100000: episode: 1732, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 63.326, mean reward: 1.979 [1.565, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.415, 10.100], loss: 0.112734, mae: 0.333925, mean_q: 3.851535
 90986/100000: episode: 1733, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 74.963, mean reward: 2.343 [1.717, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.436, 10.344], loss: 0.137501, mae: 0.360311, mean_q: 3.893592
 90998/100000: episode: 1734, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 31.522, mean reward: 2.627 [2.071, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.278, 10.100], loss: 0.119570, mae: 0.351245, mean_q: 3.891645
 91030/100000: episode: 1735, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 72.369, mean reward: 2.262 [1.545, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.033, 10.315], loss: 0.110978, mae: 0.341183, mean_q: 3.848726
 91042/100000: episode: 1736, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 32.516, mean reward: 2.710 [2.228, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.339, 10.100], loss: 0.129201, mae: 0.358752, mean_q: 3.834918
 91062/100000: episode: 1737, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 43.275, mean reward: 2.164 [1.674, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.063, 10.188], loss: 0.100642, mae: 0.332427, mean_q: 3.885576
 91074/100000: episode: 1738, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 39.134, mean reward: 3.261 [2.232, 6.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.666, 10.100], loss: 0.089159, mae: 0.321266, mean_q: 3.879201
 91117/100000: episode: 1739, duration: 0.218s, episode steps: 43, steps per second: 197, episode reward: 150.588, mean reward: 3.502 [2.273, 7.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.361, 10.474], loss: 0.124540, mae: 0.338089, mean_q: 3.871443
 91138/100000: episode: 1740, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 72.811, mean reward: 3.467 [2.399, 4.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.221, 10.100], loss: 0.113133, mae: 0.343156, mean_q: 3.937063
 91150/100000: episode: 1741, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 36.298, mean reward: 3.025 [1.949, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-2.251, 10.100], loss: 0.121970, mae: 0.326277, mean_q: 3.895959
 91182/100000: episode: 1742, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 77.663, mean reward: 2.427 [1.949, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.272], loss: 0.137232, mae: 0.352953, mean_q: 3.924819
 91198/100000: episode: 1743, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 31.914, mean reward: 1.995 [1.574, 2.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.273], loss: 0.107380, mae: 0.330678, mean_q: 3.893677
 91217/100000: episode: 1744, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 56.432, mean reward: 2.970 [1.875, 7.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.069, 10.100], loss: 0.093781, mae: 0.327665, mean_q: 3.960726
 91236/100000: episode: 1745, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 69.015, mean reward: 3.632 [2.348, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.458, 10.100], loss: 0.111242, mae: 0.330110, mean_q: 3.987066
 91294/100000: episode: 1746, duration: 0.324s, episode steps: 58, steps per second: 179, episode reward: 113.953, mean reward: 1.965 [1.477, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.827 [-0.640, 10.100], loss: 0.130564, mae: 0.347789, mean_q: 3.911602
 91310/100000: episode: 1747, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 49.396, mean reward: 3.087 [2.050, 7.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.539], loss: 0.099266, mae: 0.319066, mean_q: 3.939104
 91342/100000: episode: 1748, duration: 0.221s, episode steps: 32, steps per second: 145, episode reward: 79.081, mean reward: 2.471 [1.741, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.150, 10.299], loss: 0.129930, mae: 0.349663, mean_q: 3.911039
 91354/100000: episode: 1749, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 32.746, mean reward: 2.729 [2.159, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.325, 10.100], loss: 0.098935, mae: 0.318001, mean_q: 3.908818
 91373/100000: episode: 1750, duration: 0.126s, episode steps: 19, steps per second: 151, episode reward: 57.234, mean reward: 3.012 [2.144, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.362, 10.100], loss: 0.110441, mae: 0.330500, mean_q: 3.918486
 91393/100000: episode: 1751, duration: 0.147s, episode steps: 20, steps per second: 136, episode reward: 51.586, mean reward: 2.579 [2.006, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.168, 10.308], loss: 0.177615, mae: 0.372016, mean_q: 3.945061
 91413/100000: episode: 1752, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 50.399, mean reward: 2.520 [1.996, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.389], loss: 0.116786, mae: 0.353491, mean_q: 3.954470
 91456/100000: episode: 1753, duration: 0.256s, episode steps: 43, steps per second: 168, episode reward: 129.621, mean reward: 3.014 [1.996, 5.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.731, 10.428], loss: 0.165420, mae: 0.375120, mean_q: 3.949503
 91476/100000: episode: 1754, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 45.576, mean reward: 2.279 [1.786, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.255, 10.180], loss: 0.119479, mae: 0.339321, mean_q: 3.918081
 91496/100000: episode: 1755, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 49.728, mean reward: 2.486 [2.036, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.394, 10.350], loss: 0.139632, mae: 0.373779, mean_q: 4.000923
 91512/100000: episode: 1756, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 37.954, mean reward: 2.372 [2.137, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.386], loss: 0.112809, mae: 0.339292, mean_q: 3.923084
 91524/100000: episode: 1757, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 35.072, mean reward: 2.923 [2.426, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.326, 10.100], loss: 0.121303, mae: 0.348869, mean_q: 3.971814
 91567/100000: episode: 1758, duration: 0.290s, episode steps: 43, steps per second: 148, episode reward: 140.971, mean reward: 3.278 [1.974, 6.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.162, 10.416], loss: 0.126256, mae: 0.355284, mean_q: 3.976394
 91588/100000: episode: 1759, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 56.815, mean reward: 2.705 [2.277, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.814, 10.100], loss: 0.136629, mae: 0.354544, mean_q: 4.007856
 91611/100000: episode: 1760, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 50.724, mean reward: 2.205 [1.785, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.413, 10.258], loss: 0.157580, mae: 0.378647, mean_q: 4.039430
 91631/100000: episode: 1761, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 55.123, mean reward: 2.756 [2.036, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.394], loss: 0.098251, mae: 0.321321, mean_q: 3.978177
 91651/100000: episode: 1762, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 47.577, mean reward: 2.379 [1.782, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.731, 10.318], loss: 0.128883, mae: 0.359723, mean_q: 4.026452
 91672/100000: episode: 1763, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 63.289, mean reward: 3.014 [2.249, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.290, 10.100], loss: 0.124601, mae: 0.360009, mean_q: 3.991575
 91704/100000: episode: 1764, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 68.751, mean reward: 2.148 [1.688, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.744, 10.285], loss: 0.146264, mae: 0.366987, mean_q: 4.026829
 91736/100000: episode: 1765, duration: 0.199s, episode steps: 32, steps per second: 161, episode reward: 76.203, mean reward: 2.381 [1.874, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.322, 10.320], loss: 0.151973, mae: 0.365957, mean_q: 3.995163
 91757/100000: episode: 1766, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 52.746, mean reward: 2.512 [1.794, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.137, 10.100], loss: 0.145333, mae: 0.368806, mean_q: 4.057532
 91769/100000: episode: 1767, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 30.322, mean reward: 2.527 [2.056, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.560, 10.100], loss: 0.180551, mae: 0.379589, mean_q: 4.072593
 91801/100000: episode: 1768, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 64.216, mean reward: 2.007 [1.637, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.361, 10.330], loss: 0.128353, mae: 0.359711, mean_q: 4.039498
 91844/100000: episode: 1769, duration: 0.227s, episode steps: 43, steps per second: 189, episode reward: 121.920, mean reward: 2.835 [1.752, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.610, 10.328], loss: 0.160990, mae: 0.382715, mean_q: 4.054121
 91876/100000: episode: 1770, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 89.059, mean reward: 2.783 [2.178, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.516, 10.423], loss: 0.150358, mae: 0.372544, mean_q: 4.035030
 91897/100000: episode: 1771, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 51.088, mean reward: 2.433 [1.878, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.157, 10.100], loss: 0.136445, mae: 0.360865, mean_q: 4.049212
 91940/100000: episode: 1772, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 109.886, mean reward: 2.555 [1.778, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.537, 10.388], loss: 0.176497, mae: 0.376889, mean_q: 4.125402
 91960/100000: episode: 1773, duration: 0.128s, episode steps: 20, steps per second: 157, episode reward: 43.030, mean reward: 2.152 [1.639, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.302, 10.286], loss: 0.146675, mae: 0.362982, mean_q: 4.117496
 91981/100000: episode: 1774, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 59.256, mean reward: 2.822 [2.119, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.062, 10.100], loss: 0.121007, mae: 0.349206, mean_q: 4.098182
 92024/100000: episode: 1775, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 123.338, mean reward: 2.868 [2.064, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.480, 10.390], loss: 0.139737, mae: 0.350391, mean_q: 4.098177
 92045/100000: episode: 1776, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 55.664, mean reward: 2.651 [2.262, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.194, 10.100], loss: 0.176505, mae: 0.379684, mean_q: 4.143166
 92057/100000: episode: 1777, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 35.042, mean reward: 2.920 [2.369, 5.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.224, 10.100], loss: 0.128756, mae: 0.370424, mean_q: 3.937140
 92080/100000: episode: 1778, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 53.304, mean reward: 2.318 [1.918, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.355], loss: 0.108901, mae: 0.360578, mean_q: 4.087005
 92101/100000: episode: 1779, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 63.280, mean reward: 3.013 [2.021, 5.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.394, 10.100], loss: 0.157078, mae: 0.371564, mean_q: 4.059286
 92121/100000: episode: 1780, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 54.024, mean reward: 2.701 [2.118, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.015, 10.364], loss: 0.180059, mae: 0.379121, mean_q: 4.128360
 92164/100000: episode: 1781, duration: 0.218s, episode steps: 43, steps per second: 197, episode reward: 136.934, mean reward: 3.185 [2.396, 5.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.858, 10.408], loss: 0.157692, mae: 0.378356, mean_q: 4.156812
 92187/100000: episode: 1782, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 48.976, mean reward: 2.129 [1.549, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.086, 10.168], loss: 0.132118, mae: 0.377099, mean_q: 4.137547
 92208/100000: episode: 1783, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 58.440, mean reward: 2.783 [2.320, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.171, 10.100], loss: 0.126210, mae: 0.368365, mean_q: 4.141315
 92228/100000: episode: 1784, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 43.463, mean reward: 2.173 [1.600, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.681, 10.191], loss: 0.151030, mae: 0.362936, mean_q: 4.147881
 92249/100000: episode: 1785, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 56.139, mean reward: 2.673 [1.816, 5.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.367, 10.100], loss: 0.118727, mae: 0.348528, mean_q: 4.138737
 92269/100000: episode: 1786, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 44.098, mean reward: 2.205 [1.803, 2.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.632, 10.251], loss: 0.163821, mae: 0.357192, mean_q: 4.065200
 92292/100000: episode: 1787, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 58.880, mean reward: 2.560 [2.062, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.489, 10.433], loss: 0.142770, mae: 0.368524, mean_q: 4.158201
 92312/100000: episode: 1788, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 46.465, mean reward: 2.323 [1.906, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.363], loss: 0.156896, mae: 0.377631, mean_q: 4.168245
 92331/100000: episode: 1789, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 76.687, mean reward: 4.036 [2.440, 8.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.479, 10.100], loss: 0.147916, mae: 0.379571, mean_q: 4.234676
 92351/100000: episode: 1790, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 47.495, mean reward: 2.375 [1.838, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.089, 10.171], loss: 0.142787, mae: 0.347916, mean_q: 4.194324
 92372/100000: episode: 1791, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 65.853, mean reward: 3.136 [2.357, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.294, 10.100], loss: 0.168142, mae: 0.381040, mean_q: 4.153054
 92391/100000: episode: 1792, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 69.272, mean reward: 3.646 [2.459, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.319, 10.100], loss: 0.155048, mae: 0.375301, mean_q: 4.190663
 92411/100000: episode: 1793, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 43.631, mean reward: 2.182 [1.762, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.383], loss: 0.161323, mae: 0.390340, mean_q: 4.221800
 92427/100000: episode: 1794, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 37.076, mean reward: 2.317 [1.685, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.397, 10.280], loss: 0.211691, mae: 0.401985, mean_q: 4.224572
 92443/100000: episode: 1795, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 40.387, mean reward: 2.524 [1.960, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.133, 10.462], loss: 0.176258, mae: 0.393237, mean_q: 4.284641
 92501/100000: episode: 1796, duration: 0.332s, episode steps: 58, steps per second: 175, episode reward: 131.906, mean reward: 2.274 [1.569, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-1.172, 10.508], loss: 0.165887, mae: 0.385564, mean_q: 4.233132
 92524/100000: episode: 1797, duration: 0.142s, episode steps: 23, steps per second: 163, episode reward: 50.374, mean reward: 2.190 [1.679, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.661, 10.266], loss: 0.169083, mae: 0.384420, mean_q: 4.234440
 92540/100000: episode: 1798, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 36.854, mean reward: 2.303 [1.964, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.316], loss: 0.130437, mae: 0.358452, mean_q: 4.154840
 92598/100000: episode: 1799, duration: 0.311s, episode steps: 58, steps per second: 187, episode reward: 119.321, mean reward: 2.057 [1.490, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.410, 10.123], loss: 0.134684, mae: 0.365955, mean_q: 4.219514
 92610/100000: episode: 1800, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 36.518, mean reward: 3.043 [2.119, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.323, 10.100], loss: 0.158678, mae: 0.372017, mean_q: 4.243321
 92642/100000: episode: 1801, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 77.446, mean reward: 2.420 [1.660, 4.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.285, 10.221], loss: 0.165891, mae: 0.381614, mean_q: 4.232913
 92658/100000: episode: 1802, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 42.454, mean reward: 2.653 [2.024, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.329, 10.418], loss: 0.167251, mae: 0.393697, mean_q: 4.234116
 92701/100000: episode: 1803, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 122.577, mean reward: 2.851 [1.981, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.244, 10.399], loss: 0.131280, mae: 0.366245, mean_q: 4.199136
 92721/100000: episode: 1804, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 46.083, mean reward: 2.304 [1.475, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.647, 10.100], loss: 0.167291, mae: 0.371941, mean_q: 4.216439
 92753/100000: episode: 1805, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 75.466, mean reward: 2.358 [1.659, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.232, 10.251], loss: 0.166672, mae: 0.389907, mean_q: 4.268205
 92772/100000: episode: 1806, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 49.931, mean reward: 2.628 [2.253, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.855, 10.100], loss: 0.141500, mae: 0.364620, mean_q: 4.282039
 92792/100000: episode: 1807, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 60.238, mean reward: 3.012 [1.908, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.427], loss: 0.135215, mae: 0.386970, mean_q: 4.309320
 92811/100000: episode: 1808, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 57.006, mean reward: 3.000 [2.491, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.345, 10.100], loss: 0.142219, mae: 0.366345, mean_q: 4.194450
 92831/100000: episode: 1809, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 63.631, mean reward: 3.182 [1.886, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.196, 10.264], loss: 0.162144, mae: 0.379322, mean_q: 4.280948
 92852/100000: episode: 1810, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 64.866, mean reward: 3.089 [2.049, 6.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.139, 10.100], loss: 0.182696, mae: 0.401827, mean_q: 4.313967
 92864/100000: episode: 1811, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 26.954, mean reward: 2.246 [1.987, 2.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.312, 10.100], loss: 0.164525, mae: 0.410533, mean_q: 4.292967
 92907/100000: episode: 1812, duration: 0.243s, episode steps: 43, steps per second: 177, episode reward: 100.847, mean reward: 2.345 [1.845, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.783, 10.312], loss: 0.149860, mae: 0.374614, mean_q: 4.303354
 92939/100000: episode: 1813, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 117.725, mean reward: 3.679 [1.944, 8.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.601], loss: 0.147308, mae: 0.376700, mean_q: 4.322674
 92959/100000: episode: 1814, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 48.289, mean reward: 2.414 [2.053, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.118, 10.354], loss: 0.163821, mae: 0.384509, mean_q: 4.307693
 92991/100000: episode: 1815, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 77.201, mean reward: 2.413 [1.950, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.347], loss: 0.141161, mae: 0.381522, mean_q: 4.312122
 93003/100000: episode: 1816, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 26.548, mean reward: 2.212 [1.973, 2.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.329, 10.100], loss: 0.177817, mae: 0.433559, mean_q: 4.459180
 93026/100000: episode: 1817, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 51.452, mean reward: 2.237 [1.948, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.177, 10.316], loss: 0.206545, mae: 0.410724, mean_q: 4.349789
 93084/100000: episode: 1818, duration: 0.312s, episode steps: 58, steps per second: 186, episode reward: 114.430, mean reward: 1.973 [1.440, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-0.335, 10.103], loss: 0.173142, mae: 0.395317, mean_q: 4.308843
 93104/100000: episode: 1819, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 46.736, mean reward: 2.337 [1.965, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.717, 10.355], loss: 0.149821, mae: 0.392622, mean_q: 4.374464
[Info] 2-TH LEVEL FOUND: 7.391616344451904, Considering 10/90 traces
 93127/100000: episode: 1820, duration: 4.228s, episode steps: 23, steps per second: 5, episode reward: 53.760, mean reward: 2.337 [1.700, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.336, 10.519], loss: 0.142423, mae: 0.382490, mean_q: 4.320892
 93137/100000: episode: 1821, duration: 0.072s, episode steps: 10, steps per second: 140, episode reward: 34.387, mean reward: 3.439 [2.671, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.139, 10.100], loss: 0.156888, mae: 0.392923, mean_q: 4.430818
 93153/100000: episode: 1822, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 66.231, mean reward: 4.139 [2.824, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.408, 10.100], loss: 0.182490, mae: 0.401553, mean_q: 4.469535
 93166/100000: episode: 1823, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 33.976, mean reward: 2.614 [2.141, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.178, 10.100], loss: 0.135761, mae: 0.374462, mean_q: 4.410434
 93179/100000: episode: 1824, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 66.190, mean reward: 5.092 [3.856, 5.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.552], loss: 0.248410, mae: 0.435094, mean_q: 4.403168
 93192/100000: episode: 1825, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 37.392, mean reward: 2.876 [1.984, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.413, 10.100], loss: 0.139192, mae: 0.387112, mean_q: 4.371687
 93220/100000: episode: 1826, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 159.577, mean reward: 5.699 [2.325, 30.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.621, 10.646], loss: 0.135782, mae: 0.380280, mean_q: 4.384866
 93238/100000: episode: 1827, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 67.239, mean reward: 3.735 [2.474, 5.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.664, 10.100], loss: 0.184269, mae: 0.411574, mean_q: 4.415635
 93248/100000: episode: 1828, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 46.741, mean reward: 4.674 [3.316, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.434, 10.100], loss: 0.142300, mae: 0.391479, mean_q: 4.429408
 93264/100000: episode: 1829, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 68.982, mean reward: 4.311 [3.137, 6.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.935, 10.100], loss: 0.196040, mae: 0.413559, mean_q: 4.487157
 93273/100000: episode: 1830, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 30.292, mean reward: 3.366 [2.216, 5.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.401, 10.100], loss: 1.679337, mae: 0.652954, mean_q: 4.743567
 93286/100000: episode: 1831, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 56.548, mean reward: 4.350 [3.384, 7.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.658], loss: 0.211137, mae: 0.447237, mean_q: 4.427299
 93299/100000: episode: 1832, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 63.567, mean reward: 4.890 [2.730, 6.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.555], loss: 0.230287, mae: 0.428403, mean_q: 4.377545
 93313/100000: episode: 1833, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 64.494, mean reward: 4.607 [3.121, 7.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.344, 10.638], loss: 0.184197, mae: 0.414827, mean_q: 4.513805
 93326/100000: episode: 1834, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 35.913, mean reward: 2.763 [2.346, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.154, 10.100], loss: 0.189676, mae: 0.426284, mean_q: 4.423104
 93344/100000: episode: 1835, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 58.748, mean reward: 3.264 [2.394, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.440, 10.100], loss: 0.304378, mae: 0.470688, mean_q: 4.544962
 93358/100000: episode: 1836, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 44.442, mean reward: 3.174 [2.715, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.528], loss: 0.264187, mae: 0.480217, mean_q: 4.615578
 93374/100000: episode: 1837, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 69.272, mean reward: 4.329 [2.638, 8.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.482, 10.100], loss: 0.259078, mae: 0.493084, mean_q: 4.600355
 93387/100000: episode: 1838, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 33.703, mean reward: 2.593 [2.013, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.241, 10.100], loss: 0.211867, mae: 0.455541, mean_q: 4.609060
 93401/100000: episode: 1839, duration: 0.075s, episode steps: 14, steps per second: 185, episode reward: 40.511, mean reward: 2.894 [2.133, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.475, 10.353], loss: 0.160433, mae: 0.418209, mean_q: 4.569644
 93429/100000: episode: 1840, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 292.604, mean reward: 10.450 [2.045, 115.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.756], loss: 0.253034, mae: 0.467549, mean_q: 4.600330
 93443/100000: episode: 1841, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 45.451, mean reward: 3.246 [2.765, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.937, 10.481], loss: 0.444871, mae: 0.544996, mean_q: 4.631425
 93452/100000: episode: 1842, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 26.302, mean reward: 2.922 [2.522, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.388, 10.100], loss: 0.239929, mae: 0.480689, mean_q: 4.634385
 93480/100000: episode: 1843, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 70.213, mean reward: 2.508 [1.708, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.234, 10.247], loss: 0.264597, mae: 0.453468, mean_q: 4.630951
 93496/100000: episode: 1844, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 56.632, mean reward: 3.540 [2.313, 6.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.315, 10.100], loss: 0.187537, mae: 0.431305, mean_q: 4.572051
 93509/100000: episode: 1845, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 73.268, mean reward: 5.636 [4.224, 6.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.969, 10.594], loss: 0.337733, mae: 0.515641, mean_q: 4.656286
 93537/100000: episode: 1846, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 98.357, mean reward: 3.513 [2.277, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.239, 10.344], loss: 0.237311, mae: 0.456985, mean_q: 4.599948
 93550/100000: episode: 1847, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 38.230, mean reward: 2.941 [2.226, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.811, 10.100], loss: 0.433478, mae: 0.508981, mean_q: 4.744179
 93559/100000: episode: 1848, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 24.986, mean reward: 2.776 [2.626, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.424, 10.100], loss: 0.375446, mae: 0.531654, mean_q: 4.555618
 93587/100000: episode: 1849, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 78.431, mean reward: 2.801 [2.214, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.270, 10.445], loss: 0.532671, mae: 0.543827, mean_q: 4.740891
 93615/100000: episode: 1850, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 91.859, mean reward: 3.281 [2.515, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.203, 10.468], loss: 11.131358, mae: 2.269918, mean_q: 5.213516
 93628/100000: episode: 1851, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 61.413, mean reward: 4.724 [3.349, 11.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.432, 10.100], loss: 1.756074, mae: 1.170758, mean_q: 4.366012
 93641/100000: episode: 1852, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 49.559, mean reward: 3.812 [2.893, 5.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.408, 10.474], loss: 1.146132, mae: 0.830175, mean_q: 4.208085
 93657/100000: episode: 1853, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 44.866, mean reward: 2.804 [2.345, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.803, 10.100], loss: 0.422700, mae: 0.675221, mean_q: 4.371316
 93666/100000: episode: 1854, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 26.266, mean reward: 2.918 [2.540, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.448, 10.100], loss: 0.409965, mae: 0.631026, mean_q: 4.339331
 93682/100000: episode: 1855, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 51.868, mean reward: 3.242 [2.788, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.612, 10.100], loss: 13.129058, mae: 1.045559, mean_q: 4.789998
 93695/100000: episode: 1856, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 55.410, mean reward: 4.262 [3.068, 6.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.259, 10.508], loss: 0.718184, mae: 0.646581, mean_q: 4.395569
 93708/100000: episode: 1857, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 37.621, mean reward: 2.894 [2.518, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.212, 10.100], loss: 0.395240, mae: 0.635587, mean_q: 4.582747
 93718/100000: episode: 1858, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 34.084, mean reward: 3.408 [2.341, 4.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.240, 10.100], loss: 0.373708, mae: 0.632501, mean_q: 4.779209
 93727/100000: episode: 1859, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 25.578, mean reward: 2.842 [2.425, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.282, 10.100], loss: 0.481176, mae: 0.617002, mean_q: 4.574871
 93755/100000: episode: 1860, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 127.518, mean reward: 4.554 [3.573, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.549], loss: 0.809640, mae: 0.657075, mean_q: 4.760540
 93767/100000: episode: 1861, duration: 0.087s, episode steps: 12, steps per second: 137, episode reward: 62.048, mean reward: 5.171 [3.609, 7.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.385, 10.100], loss: 1.774874, mae: 0.788576, mean_q: 4.877466
 93780/100000: episode: 1862, duration: 0.090s, episode steps: 13, steps per second: 145, episode reward: 56.739, mean reward: 4.365 [3.482, 6.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.562], loss: 0.390829, mae: 0.618798, mean_q: 4.631418
 93792/100000: episode: 1863, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 64.120, mean reward: 5.343 [2.209, 13.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.941, 10.100], loss: 0.869212, mae: 0.673419, mean_q: 4.852362
 93805/100000: episode: 1864, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 55.105, mean reward: 4.239 [2.868, 6.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.513], loss: 0.663815, mae: 0.638010, mean_q: 4.818676
 93821/100000: episode: 1865, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 64.651, mean reward: 4.041 [3.003, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.995, 10.100], loss: 0.479833, mae: 0.519248, mean_q: 4.607927
 93834/100000: episode: 1866, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 38.592, mean reward: 2.969 [2.431, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.530, 10.100], loss: 0.273112, mae: 0.522453, mean_q: 4.657620
 93850/100000: episode: 1867, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 55.824, mean reward: 3.489 [2.672, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.381, 10.100], loss: 0.877436, mae: 0.660364, mean_q: 4.752188
 93863/100000: episode: 1868, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 63.291, mean reward: 4.869 [3.500, 6.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.176, 10.618], loss: 0.355489, mae: 0.600418, mean_q: 4.658986
 93877/100000: episode: 1869, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 60.256, mean reward: 4.304 [2.967, 5.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.297, 10.520], loss: 0.426268, mae: 0.635121, mean_q: 4.892254
 93887/100000: episode: 1870, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 47.647, mean reward: 4.765 [2.931, 11.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.336, 10.100], loss: 0.364450, mae: 0.560658, mean_q: 4.854169
 93905/100000: episode: 1871, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 58.372, mean reward: 3.243 [2.221, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.368, 10.100], loss: 11.107677, mae: 0.849419, mean_q: 5.069166
 93923/100000: episode: 1872, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 41.511, mean reward: 2.306 [1.853, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.473, 10.100], loss: 0.784201, mae: 0.610374, mean_q: 4.697387
 93933/100000: episode: 1873, duration: 0.062s, episode steps: 10, steps per second: 163, episode reward: 35.187, mean reward: 3.519 [2.863, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.352, 10.100], loss: 1.149012, mae: 0.685027, mean_q: 5.046666
 93947/100000: episode: 1874, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 51.194, mean reward: 3.657 [2.658, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.516], loss: 0.883648, mae: 0.622954, mean_q: 4.798806
 93965/100000: episode: 1875, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 69.100, mean reward: 3.839 [2.682, 7.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.322, 10.100], loss: 0.339082, mae: 0.564252, mean_q: 4.808917
 93977/100000: episode: 1876, duration: 0.084s, episode steps: 12, steps per second: 144, episode reward: 39.828, mean reward: 3.319 [2.556, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.512, 10.100], loss: 0.250165, mae: 0.496218, mean_q: 4.924488
 93987/100000: episode: 1877, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 35.123, mean reward: 3.512 [3.017, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.361, 10.100], loss: 0.325308, mae: 0.520614, mean_q: 4.797969
 94015/100000: episode: 1878, duration: 0.177s, episode steps: 28, steps per second: 158, episode reward: 72.013, mean reward: 2.572 [1.699, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.801, 10.335], loss: 7.248143, mae: 0.749384, mean_q: 4.948316
 94024/100000: episode: 1879, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 27.347, mean reward: 3.039 [2.760, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.363, 10.100], loss: 0.279911, mae: 0.572001, mean_q: 4.992324
 94052/100000: episode: 1880, duration: 0.185s, episode steps: 28, steps per second: 151, episode reward: 94.771, mean reward: 3.385 [2.361, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.712, 10.430], loss: 0.284438, mae: 0.520000, mean_q: 4.844309
 94064/100000: episode: 1881, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 70.915, mean reward: 5.910 [3.866, 9.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.275, 10.100], loss: 0.296292, mae: 0.522998, mean_q: 4.843641
 94092/100000: episode: 1882, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 64.532, mean reward: 2.305 [1.668, 4.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.550, 10.253], loss: 0.645866, mae: 0.606811, mean_q: 4.944669
 94104/100000: episode: 1883, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 57.346, mean reward: 4.779 [2.887, 8.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.332, 10.100], loss: 0.649907, mae: 0.619896, mean_q: 5.113643
 94132/100000: episode: 1884, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 95.208, mean reward: 3.400 [2.514, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.250, 10.463], loss: 0.377297, mae: 0.553392, mean_q: 4.893978
 94144/100000: episode: 1885, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 41.591, mean reward: 3.466 [2.776, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.475, 10.100], loss: 0.255659, mae: 0.514153, mean_q: 4.940530
 94156/100000: episode: 1886, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 45.673, mean reward: 3.806 [2.753, 7.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.483, 10.100], loss: 0.312510, mae: 0.535052, mean_q: 4.965040
 94168/100000: episode: 1887, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 44.750, mean reward: 3.729 [2.804, 5.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.297, 10.100], loss: 0.364073, mae: 0.552954, mean_q: 4.960119
 94177/100000: episode: 1888, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 36.254, mean reward: 4.028 [2.498, 8.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.356, 10.100], loss: 0.290309, mae: 0.551309, mean_q: 4.994192
 94205/100000: episode: 1889, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 63.715, mean reward: 2.276 [1.729, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.174, 10.201], loss: 0.570236, mae: 0.615120, mean_q: 5.031528
 94218/100000: episode: 1890, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 56.489, mean reward: 4.345 [2.534, 6.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.425, 10.100], loss: 0.481443, mae: 0.591766, mean_q: 5.102719
 94232/100000: episode: 1891, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 38.787, mean reward: 2.771 [2.164, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.587, 10.458], loss: 0.345476, mae: 0.587080, mean_q: 5.012576
 94260/100000: episode: 1892, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 65.210, mean reward: 2.329 [1.748, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.764, 10.265], loss: 0.516224, mae: 0.573007, mean_q: 5.065011
 94270/100000: episode: 1893, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 31.106, mean reward: 3.111 [2.238, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.237, 10.100], loss: 0.430863, mae: 0.623705, mean_q: 5.029524
 94279/100000: episode: 1894, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 28.966, mean reward: 3.218 [2.619, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.435, 10.100], loss: 0.586489, mae: 0.583868, mean_q: 5.048837
 94292/100000: episode: 1895, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 44.153, mean reward: 3.396 [2.462, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.375], loss: 0.398659, mae: 0.566049, mean_q: 5.109369
 94306/100000: episode: 1896, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 39.196, mean reward: 2.800 [2.229, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.192, 10.417], loss: 0.455472, mae: 0.604589, mean_q: 5.058568
 94315/100000: episode: 1897, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 21.910, mean reward: 2.434 [2.055, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.430, 10.100], loss: 0.236594, mae: 0.503620, mean_q: 5.024711
 94343/100000: episode: 1898, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 114.276, mean reward: 4.081 [2.440, 8.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.264, 10.615], loss: 14.425499, mae: 1.056910, mean_q: 5.292989
 94355/100000: episode: 1899, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 58.738, mean reward: 4.895 [2.756, 7.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.322, 10.100], loss: 0.475792, mae: 0.721577, mean_q: 4.868407
 94369/100000: episode: 1900, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 36.888, mean reward: 2.635 [2.038, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.376], loss: 0.492373, mae: 0.676135, mean_q: 5.326081
 94382/100000: episode: 1901, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 50.937, mean reward: 3.918 [2.631, 7.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.277, 10.100], loss: 0.867590, mae: 0.631155, mean_q: 5.201938
 94410/100000: episode: 1902, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 80.172, mean reward: 2.863 [1.846, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.302, 10.276], loss: 0.372538, mae: 0.624672, mean_q: 5.228906
 94424/100000: episode: 1903, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 46.241, mean reward: 3.303 [2.606, 5.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.626, 10.447], loss: 0.288322, mae: 0.554738, mean_q: 5.118056
 94434/100000: episode: 1904, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 44.760, mean reward: 4.476 [3.367, 6.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.220, 10.100], loss: 0.330667, mae: 0.570381, mean_q: 5.106482
 94462/100000: episode: 1905, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 84.087, mean reward: 3.003 [1.938, 6.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.543, 10.324], loss: 7.611009, mae: 0.755597, mean_q: 5.204617
 94474/100000: episode: 1906, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 54.827, mean reward: 4.569 [2.854, 6.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.506, 10.100], loss: 0.571415, mae: 0.810436, mean_q: 5.308595
 94502/100000: episode: 1907, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 97.750, mean reward: 3.491 [2.268, 5.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.551, 10.412], loss: 0.771273, mae: 0.698434, mean_q: 5.160445
 94515/100000: episode: 1908, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 55.935, mean reward: 4.303 [2.628, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.499], loss: 14.864756, mae: 0.939882, mean_q: 5.482744
 94528/100000: episode: 1909, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 37.210, mean reward: 2.862 [2.387, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.312, 10.100], loss: 1.219468, mae: 0.696711, mean_q: 5.181223
[Info] 3-TH LEVEL FOUND: 8.85969352722168, Considering 10/90 traces
 94542/100000: episode: 1910, duration: 4.167s, episode steps: 14, steps per second: 3, episode reward: 52.600, mean reward: 3.757 [2.397, 6.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-1.056, 10.325], loss: 0.622370, mae: 0.661876, mean_q: 5.388856
 94553/100000: episode: 1911, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 56.364, mean reward: 5.124 [2.920, 6.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.612], loss: 0.380423, mae: 0.620528, mean_q: 5.299318
 94565/100000: episode: 1912, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 60.934, mean reward: 5.078 [4.141, 7.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.618], loss: 0.423911, mae: 0.656783, mean_q: 5.407217
 94577/100000: episode: 1913, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 51.763, mean reward: 4.314 [3.327, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.717, 10.562], loss: 0.421130, mae: 0.632928, mean_q: 5.312668
 94585/100000: episode: 1914, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 49.594, mean reward: 6.199 [4.760, 8.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.660], loss: 0.749564, mae: 0.644970, mean_q: 5.342927
 94595/100000: episode: 1915, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 43.976, mean reward: 4.398 [3.828, 5.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.572], loss: 1.606486, mae: 0.702176, mean_q: 5.304381
 94603/100000: episode: 1916, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 34.733, mean reward: 4.342 [3.309, 6.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.122, 10.492], loss: 0.459485, mae: 0.728938, mean_q: 5.636574
 94611/100000: episode: 1917, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 38.232, mean reward: 4.779 [4.282, 5.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.595], loss: 0.685872, mae: 0.618465, mean_q: 5.042995
 94619/100000: episode: 1918, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 48.390, mean reward: 6.049 [4.384, 7.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.553], loss: 0.591313, mae: 0.692636, mean_q: 5.490453
 94629/100000: episode: 1919, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 37.674, mean reward: 3.767 [2.695, 5.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.629, 10.601], loss: 0.442481, mae: 0.623832, mean_q: 5.350645
 94637/100000: episode: 1920, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 31.291, mean reward: 3.911 [3.363, 4.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.563], loss: 0.398232, mae: 0.608726, mean_q: 5.287931
 94651/100000: episode: 1921, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 45.829, mean reward: 3.274 [2.656, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.589], loss: 0.353266, mae: 0.588682, mean_q: 5.305529
 94661/100000: episode: 1922, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 63.569, mean reward: 6.357 [3.950, 11.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-1.150, 10.615], loss: 0.361993, mae: 0.553414, mean_q: 5.303267
 94672/100000: episode: 1923, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 37.844, mean reward: 3.440 [2.788, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.508], loss: 0.672224, mae: 0.695593, mean_q: 5.527159
 94682/100000: episode: 1924, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 55.486, mean reward: 5.549 [4.176, 7.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.985, 10.607], loss: 0.353652, mae: 0.587287, mean_q: 5.218456
 94692/100000: episode: 1925, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 39.731, mean reward: 3.973 [3.182, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.611], loss: 0.308165, mae: 0.583935, mean_q: 5.407475
 94702/100000: episode: 1926, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 48.250, mean reward: 4.825 [3.740, 6.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.532], loss: 0.334641, mae: 0.562067, mean_q: 5.196699
 94712/100000: episode: 1927, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 33.412, mean reward: 3.341 [2.601, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.558], loss: 0.471260, mae: 0.643794, mean_q: 5.501772
 94720/100000: episode: 1928, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 35.461, mean reward: 4.433 [3.566, 6.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.490], loss: 0.321376, mae: 0.554040, mean_q: 5.255491
 94730/100000: episode: 1929, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 30.531, mean reward: 3.053 [2.225, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.847, 10.444], loss: 0.424026, mae: 0.636812, mean_q: 5.302077
 94740/100000: episode: 1930, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 36.423, mean reward: 3.642 [2.561, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.477, 10.419], loss: 0.344425, mae: 0.564405, mean_q: 5.161802
 94750/100000: episode: 1931, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 49.984, mean reward: 4.998 [3.572, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.604], loss: 0.308063, mae: 0.562573, mean_q: 5.363216
 94770/100000: episode: 1932, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 128.660, mean reward: 6.433 [3.562, 13.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.488, 10.523], loss: 0.437574, mae: 0.650462, mean_q: 5.380546
 94784/100000: episode: 1933, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 48.163, mean reward: 3.440 [2.816, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.839, 10.416], loss: 0.382344, mae: 0.632214, mean_q: 5.347111
 94794/100000: episode: 1934, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 40.474, mean reward: 4.047 [3.478, 5.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.520], loss: 0.417686, mae: 0.640507, mean_q: 5.561187
 94814/100000: episode: 1935, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 92.015, mean reward: 4.601 [2.933, 7.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.564], loss: 20.651821, mae: 1.533742, mean_q: 5.842031
 94828/100000: episode: 1936, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 48.287, mean reward: 3.449 [2.653, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.442], loss: 0.504001, mae: 0.746333, mean_q: 5.525581
 94838/100000: episode: 1937, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 54.625, mean reward: 5.463 [3.950, 8.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.460, 10.642], loss: 0.586877, mae: 0.681363, mean_q: 5.298819
 94846/100000: episode: 1938, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 40.275, mean reward: 5.034 [3.650, 7.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.590], loss: 0.375467, mae: 0.637920, mean_q: 5.570024
 94856/100000: episode: 1939, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 38.227, mean reward: 3.823 [2.584, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.516], loss: 0.404974, mae: 0.620449, mean_q: 5.295493
 94867/100000: episode: 1940, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 39.693, mean reward: 3.608 [3.042, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.514], loss: 0.459065, mae: 0.680426, mean_q: 5.540775
 94887/100000: episode: 1941, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 88.160, mean reward: 4.408 [2.516, 10.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.665, 10.363], loss: 0.450054, mae: 0.629002, mean_q: 5.609618
 94897/100000: episode: 1942, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 31.131, mean reward: 3.113 [2.672, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.088, 10.492], loss: 0.356233, mae: 0.597902, mean_q: 5.480075
 94907/100000: episode: 1943, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 40.997, mean reward: 4.100 [2.879, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.399, 10.571], loss: 1.504591, mae: 0.723320, mean_q: 5.580259
 94919/100000: episode: 1944, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 200.276, mean reward: 16.690 [4.130, 107.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.014, 10.590], loss: 0.577884, mae: 0.663700, mean_q: 5.584256
 94929/100000: episode: 1945, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 62.887, mean reward: 6.289 [4.833, 8.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.633], loss: 0.491912, mae: 0.700379, mean_q: 5.495108
 94939/100000: episode: 1946, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 190.261, mean reward: 19.026 [3.949, 72.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.492, 10.661], loss: 17.028347, mae: 1.011601, mean_q: 5.813364
 94949/100000: episode: 1947, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 33.817, mean reward: 3.382 [2.371, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.428], loss: 16.907265, mae: 1.039732, mean_q: 5.836446
 94959/100000: episode: 1948, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 33.688, mean reward: 3.369 [2.830, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.481], loss: 0.606318, mae: 0.818251, mean_q: 5.694590
 94969/100000: episode: 1949, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 32.686, mean reward: 3.269 [2.440, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.218, 10.500], loss: 0.381488, mae: 0.650852, mean_q: 5.499020
 94979/100000: episode: 1950, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 33.688, mean reward: 3.369 [2.833, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.468], loss: 0.793306, mae: 0.735201, mean_q: 5.621802
 94987/100000: episode: 1951, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 45.622, mean reward: 5.703 [4.561, 9.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.099, 10.683], loss: 23.852093, mae: 1.089364, mean_q: 5.844873
 94998/100000: episode: 1952, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 34.905, mean reward: 3.173 [2.328, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.429], loss: 8.005161, mae: 1.250078, mean_q: 6.173517
 95010/100000: episode: 1953, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 34.843, mean reward: 2.904 [1.904, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.410], loss: 1.211745, mae: 0.846768, mean_q: 5.717961
 95018/100000: episode: 1954, duration: 0.059s, episode steps: 8, steps per second: 135, episode reward: 36.599, mean reward: 4.575 [3.832, 5.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.592], loss: 0.453614, mae: 0.686814, mean_q: 5.769320
 95026/100000: episode: 1955, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 29.866, mean reward: 3.733 [2.895, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.434], loss: 0.596867, mae: 0.719352, mean_q: 5.488681
 95034/100000: episode: 1956, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 60.706, mean reward: 7.588 [5.260, 9.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.325, 10.599], loss: 3.112193, mae: 1.085034, mean_q: 6.140613
 95045/100000: episode: 1957, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 37.528, mean reward: 3.412 [2.452, 5.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.532], loss: 0.616804, mae: 0.721208, mean_q: 5.734812
 95055/100000: episode: 1958, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 40.708, mean reward: 4.071 [2.954, 8.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.523], loss: 7.587610, mae: 1.023162, mean_q: 6.118808
 95065/100000: episode: 1959, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 43.813, mean reward: 4.381 [2.972, 6.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.541], loss: 0.720029, mae: 0.797151, mean_q: 5.747102
 95075/100000: episode: 1960, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 55.178, mean reward: 5.518 [2.465, 11.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.712], loss: 0.508672, mae: 0.713668, mean_q: 5.925114
 95095/100000: episode: 1961, duration: 0.119s, episode steps: 20, steps per second: 167, episode reward: 82.813, mean reward: 4.141 [2.958, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.220, 10.535], loss: 4.168914, mae: 0.891141, mean_q: 5.889981
 95109/100000: episode: 1962, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 52.111, mean reward: 3.722 [3.022, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.505], loss: 0.726928, mae: 0.772430, mean_q: 5.751395
 95120/100000: episode: 1963, duration: 0.082s, episode steps: 11, steps per second: 134, episode reward: 37.991, mean reward: 3.454 [2.623, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.519], loss: 0.592066, mae: 0.773786, mean_q: 5.908815
 95130/100000: episode: 1964, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 82.776, mean reward: 8.278 [4.806, 13.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.717], loss: 0.508082, mae: 0.681192, mean_q: 5.732838
 95144/100000: episode: 1965, duration: 0.119s, episode steps: 14, steps per second: 118, episode reward: 46.599, mean reward: 3.329 [2.963, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.557], loss: 1.019919, mae: 0.742052, mean_q: 5.781877
 95156/100000: episode: 1966, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 53.775, mean reward: 4.481 [2.836, 7.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.506], loss: 0.531182, mae: 0.713226, mean_q: 5.726911
 95168/100000: episode: 1967, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 75.289, mean reward: 6.274 [3.805, 14.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.539], loss: 6.398798, mae: 0.864651, mean_q: 5.704389
 95180/100000: episode: 1968, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 71.790, mean reward: 5.983 [4.008, 13.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-2.008, 10.575], loss: 17.157473, mae: 1.637230, mean_q: 6.549208
[Info] FALSIFICATION!
 95182/100000: episode: 1969, duration: 0.278s, episode steps: 2, steps per second: 7, episode reward: 1005.147, mean reward: 502.574 [5.147, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.014, 10.658], loss: 7.004768, mae: 1.452191, mean_q: 4.574877
 95190/100000: episode: 1970, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 33.388, mean reward: 4.173 [3.084, 5.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.468], loss: 0.576640, mae: 0.762925, mean_q: 6.064805
 95198/100000: episode: 1971, duration: 0.057s, episode steps: 8, steps per second: 139, episode reward: 32.199, mean reward: 4.025 [2.802, 6.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.100, 10.432], loss: 1.403397, mae: 0.914517, mean_q: 6.039976
 95208/100000: episode: 1972, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 36.500, mean reward: 3.650 [2.976, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.468], loss: 0.660679, mae: 0.678994, mean_q: 5.472075
 95216/100000: episode: 1973, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 36.208, mean reward: 4.526 [3.047, 5.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.564], loss: 1919.549438, mae: 5.292389, mean_q: 6.882428
 95227/100000: episode: 1974, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 32.302, mean reward: 2.937 [2.208, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.081, 10.436], loss: 8.687720, mae: 3.365883, mean_q: 8.479712
 95237/100000: episode: 1975, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 42.771, mean reward: 4.277 [3.569, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.198, 10.556], loss: 2.728684, mae: 1.638237, mean_q: 5.133464
 95247/100000: episode: 1976, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 28.698, mean reward: 2.870 [2.134, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.429, 10.429], loss: 1.177370, mae: 1.178256, mean_q: 6.864381
 95257/100000: episode: 1977, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 27.978, mean reward: 2.798 [2.140, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-1.057, 10.410], loss: 0.789995, mae: 0.844865, mean_q: 5.792098
 95267/100000: episode: 1978, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 29.429, mean reward: 2.943 [2.683, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.434], loss: 1.781472, mae: 0.945709, mean_q: 6.166685
 95275/100000: episode: 1979, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 40.612, mean reward: 5.076 [3.960, 7.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.416, 10.660], loss: 1.355506, mae: 0.979013, mean_q: 6.373787
 95285/100000: episode: 1980, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 39.436, mean reward: 3.944 [2.791, 6.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.457, 10.528], loss: 7.796828, mae: 1.094122, mean_q: 6.345016
 95299/100000: episode: 1981, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 75.817, mean reward: 5.416 [3.050, 9.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.600], loss: 14.273483, mae: 1.186316, mean_q: 6.437152
 95309/100000: episode: 1982, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 64.328, mean reward: 6.433 [4.830, 9.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.196, 10.620], loss: 1.111638, mae: 0.953442, mean_q: 6.354796
 95321/100000: episode: 1983, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 60.070, mean reward: 5.006 [3.320, 6.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.591], loss: 0.542033, mae: 0.754185, mean_q: 6.038929
 95331/100000: episode: 1984, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 33.654, mean reward: 3.365 [2.707, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.531], loss: 1533.418823, mae: 4.235444, mean_q: 6.370645
 95342/100000: episode: 1985, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 44.439, mean reward: 4.040 [2.897, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.385, 10.467], loss: 25.762468, mae: 3.179024, mean_q: 8.547303
 95362/100000: episode: 1986, duration: 0.143s, episode steps: 20, steps per second: 140, episode reward: 150.612, mean reward: 7.531 [4.455, 13.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.139, 10.720], loss: 767.439514, mae: 2.870319, mean_q: 6.519192
 95373/100000: episode: 1987, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 34.668, mean reward: 3.152 [2.618, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.425], loss: 8.569121, mae: 3.599352, mean_q: 9.910890
 95387/100000: episode: 1988, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 67.576, mean reward: 4.827 [3.482, 6.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.587], loss: 16.167154, mae: 1.647721, mean_q: 5.659434
 95399/100000: episode: 1989, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 51.548, mean reward: 4.296 [2.880, 5.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.261, 10.513], loss: 16.742634, mae: 1.352202, mean_q: 6.754055
 95407/100000: episode: 1990, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 35.474, mean reward: 4.434 [3.913, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.562], loss: 21.425318, mae: 1.581838, mean_q: 7.215097
 95418/100000: episode: 1991, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 40.741, mean reward: 3.704 [3.201, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.485], loss: 2771.465088, mae: 6.682338, mean_q: 6.869986
 95428/100000: episode: 1992, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 28.349, mean reward: 2.835 [2.401, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.505], loss: 14.073384, mae: 4.521180, mean_q: 11.040781
 95438/100000: episode: 1993, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 51.925, mean reward: 5.192 [4.080, 6.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.585], loss: 4.082635, mae: 2.042126, mean_q: 6.304078
 95450/100000: episode: 1994, duration: 0.099s, episode steps: 12, steps per second: 122, episode reward: 41.054, mean reward: 3.421 [2.673, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.668, 10.429], loss: 1.224780, mae: 1.235794, mean_q: 6.248329
 95460/100000: episode: 1995, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 55.576, mean reward: 5.558 [4.050, 8.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.527], loss: 1.927986, mae: 1.176426, mean_q: 6.959325
 95480/100000: episode: 1996, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 124.622, mean reward: 6.231 [4.394, 12.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.681, 10.707], loss: 5.027773, mae: 1.094745, mean_q: 6.703996
 95490/100000: episode: 1997, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 60.536, mean reward: 6.054 [4.628, 10.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.698], loss: 8.993490, mae: 1.262077, mean_q: 6.894526
 95498/100000: episode: 1998, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 51.309, mean reward: 6.414 [5.230, 7.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.950, 10.605], loss: 0.823953, mae: 0.967762, mean_q: 6.847332
 95509/100000: episode: 1999, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 31.996, mean reward: 2.909 [2.296, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.188, 10.486], loss: 1.136314, mae: 0.940639, mean_q: 6.495984
[Info] Complete ISplit Iteration
[Info] Levels: [5.914305, 7.3916163, 8.859694, 10.890094]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.35]
[Info] Error Prob: 0.00035000000000000005

 95523/100000: episode: 2000, duration: 4.406s, episode steps: 14, steps per second: 3, episode reward: 69.021, mean reward: 4.930 [4.071, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.586], loss: 0.941762, mae: 0.890356, mean_q: 6.527568
 95623/100000: episode: 2001, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 198.827, mean reward: 1.988 [1.535, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.415, 10.353], loss: 6.931426, mae: 1.044414, mean_q: 6.584129
 95723/100000: episode: 2002, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.377, mean reward: 1.934 [1.476, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.680, 10.098], loss: 4.008513, mae: 0.954276, mean_q: 6.432830
 95823/100000: episode: 2003, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 206.192, mean reward: 2.062 [1.501, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.513, 10.268], loss: 315.150635, mae: 2.636147, mean_q: 7.277768
 95923/100000: episode: 2004, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.195, mean reward: 1.972 [1.479, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.098], loss: 1.896977, mae: 0.975902, mean_q: 6.517151
 96023/100000: episode: 2005, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 222.401, mean reward: 2.224 [1.490, 6.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.221, 10.098], loss: 154.896027, mae: 1.559199, mean_q: 6.784952
 96123/100000: episode: 2006, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 197.267, mean reward: 1.973 [1.478, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.349, 10.098], loss: 1.140305, mae: 0.915154, mean_q: 6.335403
 96223/100000: episode: 2007, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.142, mean reward: 1.861 [1.464, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.283, 10.178], loss: 4.828555, mae: 0.923439, mean_q: 6.268092
 96323/100000: episode: 2008, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 208.015, mean reward: 2.080 [1.500, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.006, 10.343], loss: 161.760452, mae: 1.546589, mean_q: 6.602619
 96423/100000: episode: 2009, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 219.903, mean reward: 2.199 [1.446, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.408, 10.098], loss: 2.956830, mae: 0.905174, mean_q: 6.259711
 96523/100000: episode: 2010, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 194.362, mean reward: 1.944 [1.454, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.479, 10.259], loss: 1.060647, mae: 0.843474, mean_q: 6.137286
 96623/100000: episode: 2011, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.069, mean reward: 1.851 [1.487, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.114, 10.201], loss: 4.822883, mae: 0.883978, mean_q: 6.030318
 96723/100000: episode: 2012, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.592, mean reward: 2.006 [1.464, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.034, 10.098], loss: 162.235046, mae: 1.719894, mean_q: 6.594628
 96823/100000: episode: 2013, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.232, mean reward: 1.832 [1.464, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.732, 10.098], loss: 8.007386, mae: 1.063783, mean_q: 6.334664
 96923/100000: episode: 2014, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.221, mean reward: 1.972 [1.461, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.938, 10.140], loss: 5.283606, mae: 0.920408, mean_q: 6.100346
 97023/100000: episode: 2015, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.400, mean reward: 1.984 [1.439, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.939, 10.179], loss: 156.107010, mae: 1.348196, mean_q: 6.239452
 97123/100000: episode: 2016, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 193.125, mean reward: 1.931 [1.468, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.954, 10.098], loss: 2.512129, mae: 0.901766, mean_q: 5.989559
 97223/100000: episode: 2017, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 188.691, mean reward: 1.887 [1.450, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.619, 10.145], loss: 5.087301, mae: 0.890224, mean_q: 6.000547
 97323/100000: episode: 2018, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 206.472, mean reward: 2.065 [1.470, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.072, 10.098], loss: 155.400085, mae: 1.464711, mean_q: 6.226704
 97423/100000: episode: 2019, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.002, mean reward: 1.840 [1.477, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.932, 10.159], loss: 1.616993, mae: 0.818137, mean_q: 5.925073
 97523/100000: episode: 2020, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.329, mean reward: 2.073 [1.467, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.778, 10.098], loss: 1.709922, mae: 0.799870, mean_q: 5.844875
 97623/100000: episode: 2021, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 188.781, mean reward: 1.888 [1.439, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.335, 10.323], loss: 2.508807, mae: 0.799635, mean_q: 5.808748
 97723/100000: episode: 2022, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.123, mean reward: 1.891 [1.450, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.977, 10.336], loss: 154.422897, mae: 1.331066, mean_q: 5.957243
 97823/100000: episode: 2023, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 202.080, mean reward: 2.021 [1.457, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.215, 10.257], loss: 311.269348, mae: 1.892042, mean_q: 6.270700
 97923/100000: episode: 2024, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 183.828, mean reward: 1.838 [1.470, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.715, 10.235], loss: 310.456177, mae: 2.075819, mean_q: 6.825109
 98023/100000: episode: 2025, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 186.774, mean reward: 1.868 [1.442, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.400, 10.124], loss: 5.761667, mae: 1.028874, mean_q: 6.194200
 98123/100000: episode: 2026, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.784, mean reward: 1.788 [1.430, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.253, 10.098], loss: 461.732971, mae: 2.535013, mean_q: 6.814431
 98223/100000: episode: 2027, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 232.907, mean reward: 2.329 [1.619, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.586, 10.098], loss: 1.078483, mae: 0.813816, mean_q: 5.886716
 98323/100000: episode: 2028, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 184.722, mean reward: 1.847 [1.487, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.684, 10.352], loss: 157.336304, mae: 1.454969, mean_q: 6.175368
 98423/100000: episode: 2029, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 211.433, mean reward: 2.114 [1.460, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.462, 10.136], loss: 1.428860, mae: 0.736035, mean_q: 5.537041
 98523/100000: episode: 2030, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 200.560, mean reward: 2.006 [1.434, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.287], loss: 4.922566, mae: 0.831197, mean_q: 5.474866
 98623/100000: episode: 2031, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 228.757, mean reward: 2.288 [1.452, 6.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.910, 10.562], loss: 155.962112, mae: 1.369789, mean_q: 5.834424
 98723/100000: episode: 2032, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 207.813, mean reward: 2.078 [1.537, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.928, 10.450], loss: 0.737971, mae: 0.696287, mean_q: 5.337286
 98823/100000: episode: 2033, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 193.008, mean reward: 1.930 [1.470, 5.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.373, 10.098], loss: 2.226536, mae: 0.739622, mean_q: 5.310807
 98923/100000: episode: 2034, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.868, mean reward: 1.999 [1.461, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.904, 10.098], loss: 0.758280, mae: 0.655407, mean_q: 5.130463
 99023/100000: episode: 2035, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.067, mean reward: 1.911 [1.440, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.527, 10.153], loss: 1.084960, mae: 0.850791, mean_q: 5.419862
 99123/100000: episode: 2036, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 181.278, mean reward: 1.813 [1.458, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.815, 10.198], loss: 156.170670, mae: 1.190306, mean_q: 5.344574
 99223/100000: episode: 2037, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 207.727, mean reward: 2.077 [1.476, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.379, 10.098], loss: 2.763908, mae: 0.827906, mean_q: 5.274426
 99323/100000: episode: 2038, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 223.928, mean reward: 2.239 [1.454, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.577, 10.446], loss: 0.645064, mae: 0.611794, mean_q: 5.036648
 99423/100000: episode: 2039, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 194.438, mean reward: 1.944 [1.457, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.511, 10.155], loss: 0.556809, mae: 0.557493, mean_q: 4.892801
 99523/100000: episode: 2040, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 197.186, mean reward: 1.972 [1.468, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.098], loss: 153.688187, mae: 1.038808, mean_q: 5.015553
 99623/100000: episode: 2041, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.731, mean reward: 1.987 [1.496, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.120, 10.347], loss: 153.982437, mae: 1.185933, mean_q: 5.134079
 99723/100000: episode: 2042, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.414, mean reward: 1.854 [1.484, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.552, 10.204], loss: 2.264436, mae: 0.610451, mean_q: 4.753668
 99823/100000: episode: 2043, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.840, mean reward: 1.868 [1.461, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.427, 10.172], loss: 0.404694, mae: 0.508499, mean_q: 4.603556
 99923/100000: episode: 2044, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 184.851, mean reward: 1.849 [1.447, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.934, 10.198], loss: 0.323053, mae: 0.474967, mean_q: 4.441263
done, took 617.694 seconds
[Info] End Importance Splitting. Falsification occurred 11 times.
