Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.178s, episode steps: 100, steps per second: 562, episode reward: 185.583, mean reward: 1.856 [1.435, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.886, 10.378], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.080s, episode steps: 100, steps per second: 1243, episode reward: 181.943, mean reward: 1.819 [1.435, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.679, 10.177], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 189.784, mean reward: 1.898 [1.499, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.034, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.070s, episode steps: 100, steps per second: 1421, episode reward: 192.483, mean reward: 1.925 [1.448, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.726, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.100s, episode steps: 100, steps per second: 1002, episode reward: 183.535, mean reward: 1.835 [1.460, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.654, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.066s, episode steps: 100, steps per second: 1506, episode reward: 187.441, mean reward: 1.874 [1.485, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.515, 10.231], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.068s, episode steps: 100, steps per second: 1471, episode reward: 189.352, mean reward: 1.894 [1.448, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.347, 10.190], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 196.059, mean reward: 1.961 [1.465, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.067s, episode steps: 100, steps per second: 1483, episode reward: 195.329, mean reward: 1.953 [1.442, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.565, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 193.896, mean reward: 1.939 [1.509, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.782, 10.206], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 188.143, mean reward: 1.881 [1.467, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.388, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.072s, episode steps: 100, steps per second: 1397, episode reward: 182.562, mean reward: 1.826 [1.492, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.000, 10.195], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.073s, episode steps: 100, steps per second: 1366, episode reward: 190.033, mean reward: 1.900 [1.452, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.480, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.068s, episode steps: 100, steps per second: 1476, episode reward: 197.209, mean reward: 1.972 [1.449, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.208], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.061s, episode steps: 100, steps per second: 1637, episode reward: 206.117, mean reward: 2.061 [1.476, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.701, 10.133], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.062s, episode steps: 100, steps per second: 1624, episode reward: 197.514, mean reward: 1.975 [1.503, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.005, 10.320], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.061s, episode steps: 100, steps per second: 1636, episode reward: 190.636, mean reward: 1.906 [1.489, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.799, 10.121], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.070s, episode steps: 100, steps per second: 1431, episode reward: 179.011, mean reward: 1.790 [1.442, 2.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 175.045, mean reward: 1.750 [1.451, 2.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.806, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 184.266, mean reward: 1.843 [1.483, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.956, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.071s, episode steps: 100, steps per second: 1402, episode reward: 201.569, mean reward: 2.016 [1.550, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.865, 10.154], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.066s, episode steps: 100, steps per second: 1520, episode reward: 213.760, mean reward: 2.138 [1.456, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.437, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.071s, episode steps: 100, steps per second: 1406, episode reward: 180.305, mean reward: 1.803 [1.448, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.266, 10.186], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.075s, episode steps: 100, steps per second: 1339, episode reward: 197.724, mean reward: 1.977 [1.469, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.928, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.061s, episode steps: 100, steps per second: 1638, episode reward: 194.826, mean reward: 1.948 [1.457, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 210.239, mean reward: 2.102 [1.488, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.784, 10.343], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.067s, episode steps: 100, steps per second: 1483, episode reward: 180.696, mean reward: 1.807 [1.467, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.350], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.061s, episode steps: 100, steps per second: 1637, episode reward: 185.132, mean reward: 1.851 [1.460, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.286, 10.205], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.069s, episode steps: 100, steps per second: 1456, episode reward: 205.687, mean reward: 2.057 [1.472, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 176.045, mean reward: 1.760 [1.454, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 186.118, mean reward: 1.861 [1.447, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.540, 10.156], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.062s, episode steps: 100, steps per second: 1623, episode reward: 197.974, mean reward: 1.980 [1.454, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.390, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.061s, episode steps: 100, steps per second: 1628, episode reward: 193.064, mean reward: 1.931 [1.460, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.491, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.061s, episode steps: 100, steps per second: 1631, episode reward: 196.814, mean reward: 1.968 [1.456, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.316, 10.196], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.061s, episode steps: 100, steps per second: 1626, episode reward: 193.141, mean reward: 1.931 [1.460, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.966, 10.172], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.061s, episode steps: 100, steps per second: 1629, episode reward: 174.575, mean reward: 1.746 [1.459, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.089, 10.173], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 207.769, mean reward: 2.078 [1.497, 6.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.972, 10.170], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: 174.849, mean reward: 1.748 [1.452, 2.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.125, 10.162], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1627, episode reward: 205.672, mean reward: 2.057 [1.473, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.965, 10.458], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 186.162, mean reward: 1.862 [1.484, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.226, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.070s, episode steps: 100, steps per second: 1425, episode reward: 197.036, mean reward: 1.970 [1.492, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.172, 10.196], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.061s, episode steps: 100, steps per second: 1627, episode reward: 189.973, mean reward: 1.900 [1.475, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.938, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.067s, episode steps: 100, steps per second: 1483, episode reward: 193.967, mean reward: 1.940 [1.468, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.207, 10.243], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.074s, episode steps: 100, steps per second: 1342, episode reward: 185.889, mean reward: 1.859 [1.443, 5.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.555, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.076s, episode steps: 100, steps per second: 1309, episode reward: 198.520, mean reward: 1.985 [1.478, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.283], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 214.790, mean reward: 2.148 [1.502, 6.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.809, 10.214], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1637, episode reward: 194.073, mean reward: 1.941 [1.434, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.313, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.074s, episode steps: 100, steps per second: 1352, episode reward: 185.467, mean reward: 1.855 [1.472, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.318], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.070s, episode steps: 100, steps per second: 1434, episode reward: 186.849, mean reward: 1.868 [1.456, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.082, 10.205], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.078s, episode steps: 100, steps per second: 1276, episode reward: 192.388, mean reward: 1.924 [1.452, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.227, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 182.263, mean reward: 1.823 [1.459, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.136], loss: 0.311539, mae: 0.585539, mean_q: 2.141136
  5200/100000: episode: 52, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 207.100, mean reward: 2.071 [1.466, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.023, 10.098], loss: 0.082602, mae: 0.295150, mean_q: 2.835210
  5300/100000: episode: 53, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 210.592, mean reward: 2.106 [1.483, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.308, 10.098], loss: 0.095172, mae: 0.301016, mean_q: 3.138211
  5400/100000: episode: 54, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 193.352, mean reward: 1.934 [1.440, 8.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.167], loss: 0.090359, mae: 0.298053, mean_q: 3.346709
  5500/100000: episode: 55, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.061, mean reward: 1.861 [1.456, 2.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.274, 10.235], loss: 0.114744, mae: 0.310682, mean_q: 3.542224
  5600/100000: episode: 56, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.831, mean reward: 1.898 [1.450, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.289, 10.204], loss: 0.088207, mae: 0.295543, mean_q: 3.633789
  5700/100000: episode: 57, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.180, mean reward: 1.832 [1.451, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.098], loss: 0.093674, mae: 0.296597, mean_q: 3.712980
  5800/100000: episode: 58, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 188.142, mean reward: 1.881 [1.485, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.303, 10.098], loss: 0.099378, mae: 0.298245, mean_q: 3.751007
  5900/100000: episode: 59, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 190.372, mean reward: 1.904 [1.481, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.707, 10.098], loss: 0.095010, mae: 0.292179, mean_q: 3.757045
  6000/100000: episode: 60, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 175.087, mean reward: 1.751 [1.436, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.098], loss: 0.097139, mae: 0.297081, mean_q: 3.762246
  6100/100000: episode: 61, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 189.521, mean reward: 1.895 [1.450, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.397, 10.187], loss: 0.103366, mae: 0.296513, mean_q: 3.786688
  6200/100000: episode: 62, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 186.630, mean reward: 1.866 [1.456, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.474, 10.114], loss: 0.100297, mae: 0.299048, mean_q: 3.782929
  6300/100000: episode: 63, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 240.926, mean reward: 2.409 [1.473, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.746, 10.337], loss: 0.107628, mae: 0.305941, mean_q: 3.792629
  6400/100000: episode: 64, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 233.232, mean reward: 2.332 [1.495, 5.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.512, 10.098], loss: 0.130595, mae: 0.325150, mean_q: 3.811851
  6500/100000: episode: 65, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 192.217, mean reward: 1.922 [1.472, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.098], loss: 0.118475, mae: 0.310394, mean_q: 3.814770
  6600/100000: episode: 66, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 199.247, mean reward: 1.992 [1.485, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.917, 10.198], loss: 0.113185, mae: 0.314644, mean_q: 3.816983
  6700/100000: episode: 67, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 216.798, mean reward: 2.168 [1.522, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.316], loss: 0.101872, mae: 0.311689, mean_q: 3.817033
  6800/100000: episode: 68, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 204.043, mean reward: 2.040 [1.442, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.015, 10.205], loss: 0.130919, mae: 0.323322, mean_q: 3.847375
  6900/100000: episode: 69, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 191.681, mean reward: 1.917 [1.442, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.085, 10.098], loss: 0.113816, mae: 0.325040, mean_q: 3.851216
  7000/100000: episode: 70, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 193.735, mean reward: 1.937 [1.450, 6.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.503, 10.098], loss: 0.103468, mae: 0.311042, mean_q: 3.835613
  7100/100000: episode: 71, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 219.603, mean reward: 2.196 [1.539, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.935, 10.098], loss: 0.106091, mae: 0.311997, mean_q: 3.831385
  7200/100000: episode: 72, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 208.881, mean reward: 2.089 [1.504, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.098], loss: 0.104542, mae: 0.309954, mean_q: 3.834871
  7300/100000: episode: 73, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 190.816, mean reward: 1.908 [1.452, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.060, 10.098], loss: 0.104415, mae: 0.320243, mean_q: 3.842017
  7400/100000: episode: 74, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 203.381, mean reward: 2.034 [1.472, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.641, 10.371], loss: 0.112893, mae: 0.324052, mean_q: 3.860123
  7500/100000: episode: 75, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 216.679, mean reward: 2.167 [1.470, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.767, 10.098], loss: 0.103903, mae: 0.306800, mean_q: 3.850704
  7600/100000: episode: 76, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 199.226, mean reward: 1.992 [1.482, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.565, 10.248], loss: 0.105291, mae: 0.321378, mean_q: 3.872076
  7700/100000: episode: 77, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 193.978, mean reward: 1.940 [1.493, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.119, 10.140], loss: 0.118509, mae: 0.319268, mean_q: 3.873163
  7800/100000: episode: 78, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 188.946, mean reward: 1.889 [1.491, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.962, 10.168], loss: 0.093534, mae: 0.309368, mean_q: 3.875672
  7900/100000: episode: 79, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.407, mean reward: 1.924 [1.451, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.098], loss: 0.111439, mae: 0.324049, mean_q: 3.876313
  8000/100000: episode: 80, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 200.029, mean reward: 2.000 [1.455, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.098], loss: 0.109481, mae: 0.319607, mean_q: 3.870206
  8100/100000: episode: 81, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 196.352, mean reward: 1.964 [1.430, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.521, 10.098], loss: 0.106103, mae: 0.323862, mean_q: 3.868934
  8200/100000: episode: 82, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.443, mean reward: 1.854 [1.461, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.105], loss: 0.111475, mae: 0.318514, mean_q: 3.875045
  8300/100000: episode: 83, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.174, mean reward: 1.882 [1.476, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.248], loss: 0.121052, mae: 0.328763, mean_q: 3.884745
  8400/100000: episode: 84, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 214.207, mean reward: 2.142 [1.434, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.250, 10.098], loss: 0.110977, mae: 0.317312, mean_q: 3.888879
  8500/100000: episode: 85, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.343, mean reward: 1.873 [1.457, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.577, 10.098], loss: 0.115010, mae: 0.320131, mean_q: 3.888711
  8600/100000: episode: 86, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.701, mean reward: 1.957 [1.479, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.450, 10.177], loss: 0.094190, mae: 0.307130, mean_q: 3.870182
  8700/100000: episode: 87, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 260.596, mean reward: 2.606 [1.439, 7.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.255, 10.098], loss: 0.116865, mae: 0.336097, mean_q: 3.895183
  8800/100000: episode: 88, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 208.971, mean reward: 2.090 [1.496, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.610, 10.098], loss: 0.119402, mae: 0.341087, mean_q: 3.919140
  8900/100000: episode: 89, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.671, mean reward: 1.937 [1.508, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.098], loss: 0.120888, mae: 0.332530, mean_q: 3.903986
  9000/100000: episode: 90, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.908, mean reward: 1.909 [1.484, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.552, 10.123], loss: 0.111691, mae: 0.331131, mean_q: 3.917625
  9100/100000: episode: 91, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 187.268, mean reward: 1.873 [1.452, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.394, 10.405], loss: 0.116240, mae: 0.339256, mean_q: 3.917579
  9200/100000: episode: 92, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.438, mean reward: 1.944 [1.442, 7.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.334, 10.275], loss: 0.140601, mae: 0.353765, mean_q: 3.942968
  9300/100000: episode: 93, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 211.838, mean reward: 2.118 [1.463, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.314, 10.276], loss: 0.166358, mae: 0.365439, mean_q: 3.942774
  9400/100000: episode: 94, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 184.379, mean reward: 1.844 [1.487, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.866, 10.098], loss: 0.129962, mae: 0.347481, mean_q: 3.937305
  9500/100000: episode: 95, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.878, mean reward: 1.929 [1.467, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.808, 10.098], loss: 0.128329, mae: 0.342803, mean_q: 3.946188
  9600/100000: episode: 96, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 216.635, mean reward: 2.166 [1.519, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.026, 10.098], loss: 0.107541, mae: 0.330495, mean_q: 3.931814
  9700/100000: episode: 97, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 219.481, mean reward: 2.195 [1.508, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.770, 10.461], loss: 0.132688, mae: 0.339467, mean_q: 3.916826
  9800/100000: episode: 98, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 213.065, mean reward: 2.131 [1.479, 4.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.924, 10.203], loss: 0.140779, mae: 0.359059, mean_q: 3.968331
  9900/100000: episode: 99, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.004, mean reward: 1.920 [1.479, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.754, 10.098], loss: 0.116230, mae: 0.337275, mean_q: 3.951725
 10000/100000: episode: 100, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.293, mean reward: 2.023 [1.468, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.465, 10.295], loss: 0.112180, mae: 0.329668, mean_q: 3.932582
 10100/100000: episode: 101, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 192.051, mean reward: 1.921 [1.460, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.180], loss: 0.121895, mae: 0.344484, mean_q: 3.957803
 10200/100000: episode: 102, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 186.376, mean reward: 1.864 [1.542, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.568, 10.098], loss: 0.122289, mae: 0.340688, mean_q: 3.954569
 10300/100000: episode: 103, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 214.441, mean reward: 2.144 [1.444, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.364], loss: 0.122239, mae: 0.341112, mean_q: 3.948439
 10400/100000: episode: 104, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.287, mean reward: 1.863 [1.450, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.511, 10.098], loss: 0.113256, mae: 0.327960, mean_q: 3.953538
 10500/100000: episode: 105, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 203.046, mean reward: 2.030 [1.448, 7.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.238], loss: 0.121491, mae: 0.339948, mean_q: 3.955289
 10600/100000: episode: 106, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.784, mean reward: 1.898 [1.465, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.218], loss: 0.137123, mae: 0.343617, mean_q: 3.961793
 10700/100000: episode: 107, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 194.744, mean reward: 1.947 [1.478, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.298, 10.446], loss: 0.123201, mae: 0.343532, mean_q: 3.963366
 10800/100000: episode: 108, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.757, mean reward: 1.888 [1.462, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.818, 10.098], loss: 0.127191, mae: 0.349609, mean_q: 3.967116
 10900/100000: episode: 109, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 175.675, mean reward: 1.757 [1.441, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.325, 10.146], loss: 0.124311, mae: 0.348667, mean_q: 3.967380
 11000/100000: episode: 110, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 175.212, mean reward: 1.752 [1.445, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.225], loss: 0.110985, mae: 0.340529, mean_q: 3.970779
 11100/100000: episode: 111, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.757, mean reward: 1.918 [1.496, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.098], loss: 0.125320, mae: 0.347640, mean_q: 3.957619
 11200/100000: episode: 112, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 206.178, mean reward: 2.062 [1.500, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.151, 10.383], loss: 0.132191, mae: 0.352088, mean_q: 3.974212
 11300/100000: episode: 113, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 234.680, mean reward: 2.347 [1.476, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.496, 10.293], loss: 0.126813, mae: 0.346634, mean_q: 3.977172
 11400/100000: episode: 114, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.710, mean reward: 1.907 [1.445, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.935, 10.103], loss: 0.110309, mae: 0.327931, mean_q: 3.965261
 11500/100000: episode: 115, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.103, mean reward: 1.871 [1.434, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.536, 10.098], loss: 0.112419, mae: 0.330795, mean_q: 3.952843
 11600/100000: episode: 116, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.435, mean reward: 1.944 [1.451, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.439, 10.118], loss: 0.111653, mae: 0.340545, mean_q: 3.963126
 11700/100000: episode: 117, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 197.339, mean reward: 1.973 [1.494, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.098], loss: 0.113026, mae: 0.324222, mean_q: 3.937597
 11800/100000: episode: 118, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 178.556, mean reward: 1.786 [1.438, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.918, 10.138], loss: 0.118577, mae: 0.327825, mean_q: 3.939935
 11900/100000: episode: 119, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 189.141, mean reward: 1.891 [1.456, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.241], loss: 0.118881, mae: 0.335082, mean_q: 3.918482
 12000/100000: episode: 120, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.513, mean reward: 1.855 [1.466, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.098], loss: 0.113902, mae: 0.331116, mean_q: 3.939223
 12100/100000: episode: 121, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 200.474, mean reward: 2.005 [1.480, 5.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.720, 10.098], loss: 0.104260, mae: 0.313398, mean_q: 3.912076
 12200/100000: episode: 122, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 194.270, mean reward: 1.943 [1.510, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.322, 10.104], loss: 0.117680, mae: 0.324187, mean_q: 3.909565
 12300/100000: episode: 123, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.470, mean reward: 1.895 [1.444, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.974, 10.098], loss: 0.119150, mae: 0.334555, mean_q: 3.927432
 12400/100000: episode: 124, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 203.029, mean reward: 2.030 [1.466, 8.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.681, 10.098], loss: 0.121211, mae: 0.335592, mean_q: 3.942222
 12500/100000: episode: 125, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 190.792, mean reward: 1.908 [1.447, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.123, 10.098], loss: 0.124310, mae: 0.329320, mean_q: 3.914434
 12600/100000: episode: 126, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.931, mean reward: 1.929 [1.514, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.052, 10.138], loss: 0.113258, mae: 0.324222, mean_q: 3.921948
 12700/100000: episode: 127, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 182.526, mean reward: 1.825 [1.474, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.741, 10.098], loss: 0.114996, mae: 0.328022, mean_q: 3.915430
 12800/100000: episode: 128, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 189.148, mean reward: 1.891 [1.451, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.614, 10.201], loss: 0.099093, mae: 0.315454, mean_q: 3.893958
 12900/100000: episode: 129, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 190.180, mean reward: 1.902 [1.459, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.098], loss: 0.123798, mae: 0.321056, mean_q: 3.881949
 13000/100000: episode: 130, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 182.510, mean reward: 1.825 [1.457, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.348], loss: 0.121213, mae: 0.322670, mean_q: 3.895965
 13100/100000: episode: 131, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.621, mean reward: 2.036 [1.486, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.120], loss: 0.107442, mae: 0.317514, mean_q: 3.883184
 13200/100000: episode: 132, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 210.393, mean reward: 2.104 [1.555, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.483, 10.302], loss: 0.101215, mae: 0.308569, mean_q: 3.883382
 13300/100000: episode: 133, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.244, mean reward: 1.872 [1.479, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.252, 10.098], loss: 0.108877, mae: 0.319530, mean_q: 3.885571
 13400/100000: episode: 134, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.351, mean reward: 1.814 [1.448, 2.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.096, 10.098], loss: 0.102600, mae: 0.316432, mean_q: 3.890408
 13500/100000: episode: 135, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.413, mean reward: 1.904 [1.460, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.478, 10.123], loss: 0.098582, mae: 0.311056, mean_q: 3.867102
 13600/100000: episode: 136, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.926, mean reward: 1.839 [1.446, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.285, 10.160], loss: 0.094212, mae: 0.305499, mean_q: 3.872435
 13700/100000: episode: 137, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 187.385, mean reward: 1.874 [1.475, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.355, 10.098], loss: 0.083071, mae: 0.293871, mean_q: 3.838851
 13800/100000: episode: 138, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.954, mean reward: 1.920 [1.446, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.584, 10.188], loss: 0.093070, mae: 0.306322, mean_q: 3.845206
 13900/100000: episode: 139, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 183.911, mean reward: 1.839 [1.448, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.212, 10.238], loss: 0.087354, mae: 0.291389, mean_q: 3.842390
 14000/100000: episode: 140, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.784, mean reward: 1.838 [1.448, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.193], loss: 0.094815, mae: 0.302915, mean_q: 3.838920
 14100/100000: episode: 141, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 183.683, mean reward: 1.837 [1.440, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.778, 10.178], loss: 0.106102, mae: 0.316313, mean_q: 3.856857
 14200/100000: episode: 142, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 212.024, mean reward: 2.120 [1.446, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.874, 10.098], loss: 0.096826, mae: 0.310608, mean_q: 3.851586
 14300/100000: episode: 143, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 193.045, mean reward: 1.930 [1.481, 5.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.108, 10.098], loss: 0.095830, mae: 0.301253, mean_q: 3.831678
 14400/100000: episode: 144, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 201.440, mean reward: 2.014 [1.464, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.353], loss: 0.091521, mae: 0.302988, mean_q: 3.837080
 14500/100000: episode: 145, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.675, mean reward: 1.947 [1.517, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.479, 10.098], loss: 0.091755, mae: 0.302906, mean_q: 3.835069
 14600/100000: episode: 146, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 206.576, mean reward: 2.066 [1.444, 6.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.280, 10.491], loss: 0.098523, mae: 0.306988, mean_q: 3.835487
 14700/100000: episode: 147, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 192.900, mean reward: 1.929 [1.470, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.547, 10.246], loss: 0.091513, mae: 0.299757, mean_q: 3.852618
 14800/100000: episode: 148, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.198, mean reward: 1.882 [1.436, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.730, 10.119], loss: 0.088544, mae: 0.299932, mean_q: 3.827411
 14900/100000: episode: 149, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.729, mean reward: 1.957 [1.471, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.098], loss: 0.100886, mae: 0.308217, mean_q: 3.834339
[Info] 1-TH LEVEL FOUND: 4.329875946044922, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.958s, episode steps: 100, steps per second: 20, episode reward: 195.936, mean reward: 1.959 [1.478, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.471, 10.098], loss: 0.092823, mae: 0.298062, mean_q: 3.826851
 15099/100000: episode: 151, duration: 0.568s, episode steps: 99, steps per second: 174, episode reward: 187.708, mean reward: 1.896 [1.441, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.237, 10.228], loss: 0.091053, mae: 0.296014, mean_q: 3.810259
 15125/100000: episode: 152, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 63.146, mean reward: 2.429 [2.011, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.205, 10.100], loss: 0.079966, mae: 0.292451, mean_q: 3.822964
 15164/100000: episode: 153, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 126.442, mean reward: 3.242 [1.581, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.579, 10.100], loss: 0.084883, mae: 0.295813, mean_q: 3.819965
 15261/100000: episode: 154, duration: 0.501s, episode steps: 97, steps per second: 194, episode reward: 177.903, mean reward: 1.834 [1.451, 2.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.721, 10.100], loss: 0.107926, mae: 0.309741, mean_q: 3.826407
 15283/100000: episode: 155, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 56.402, mean reward: 2.564 [1.845, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.252, 10.100], loss: 0.103044, mae: 0.313111, mean_q: 3.841567
 15382/100000: episode: 156, duration: 0.482s, episode steps: 99, steps per second: 205, episode reward: 197.204, mean reward: 1.992 [1.488, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.466, 10.100], loss: 0.103866, mae: 0.304990, mean_q: 3.835027
 15405/100000: episode: 157, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 65.050, mean reward: 2.828 [2.300, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.256, 10.100], loss: 0.088590, mae: 0.296021, mean_q: 3.826154
 15444/100000: episode: 158, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 118.229, mean reward: 3.032 [1.870, 4.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.452, 10.100], loss: 0.131283, mae: 0.322808, mean_q: 3.852319
 15543/100000: episode: 159, duration: 0.546s, episode steps: 99, steps per second: 181, episode reward: 204.851, mean reward: 2.069 [1.458, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.250, 10.298], loss: 0.103916, mae: 0.310959, mean_q: 3.848916
 15562/100000: episode: 160, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 40.104, mean reward: 2.111 [1.637, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.181, 10.100], loss: 0.084833, mae: 0.293313, mean_q: 3.831364
 15659/100000: episode: 161, duration: 0.486s, episode steps: 97, steps per second: 200, episode reward: 181.237, mean reward: 1.868 [1.445, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.726, 10.100], loss: 0.104595, mae: 0.315278, mean_q: 3.857025
 15757/100000: episode: 162, duration: 0.488s, episode steps: 98, steps per second: 201, episode reward: 184.115, mean reward: 1.879 [1.469, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.631, 10.100], loss: 0.101506, mae: 0.312271, mean_q: 3.862907
 15789/100000: episode: 163, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 75.659, mean reward: 2.364 [1.850, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.591, 10.100], loss: 0.097911, mae: 0.311903, mean_q: 3.837729
 15801/100000: episode: 164, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 30.477, mean reward: 2.540 [2.020, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.121, 10.100], loss: 0.134604, mae: 0.335303, mean_q: 3.925552
 15833/100000: episode: 165, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 82.780, mean reward: 2.587 [1.960, 5.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.641, 10.100], loss: 0.088794, mae: 0.303763, mean_q: 3.858619
 15856/100000: episode: 166, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 72.203, mean reward: 3.139 [2.489, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.412, 10.100], loss: 0.108499, mae: 0.295318, mean_q: 3.871145
 15895/100000: episode: 167, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 99.404, mean reward: 2.549 [1.517, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.356, 10.100], loss: 0.109012, mae: 0.318842, mean_q: 3.894342
 15993/100000: episode: 168, duration: 0.521s, episode steps: 98, steps per second: 188, episode reward: 205.965, mean reward: 2.102 [1.492, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.639, 10.106], loss: 0.100310, mae: 0.316036, mean_q: 3.894569
 16090/100000: episode: 169, duration: 0.486s, episode steps: 97, steps per second: 199, episode reward: 245.381, mean reward: 2.530 [1.600, 6.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-1.312, 10.100], loss: 0.107751, mae: 0.320639, mean_q: 3.903032
 16189/100000: episode: 170, duration: 0.517s, episode steps: 99, steps per second: 192, episode reward: 189.722, mean reward: 1.916 [1.455, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.668, 10.100], loss: 0.130715, mae: 0.344725, mean_q: 3.948577
 16228/100000: episode: 171, duration: 0.221s, episode steps: 39, steps per second: 177, episode reward: 174.809, mean reward: 4.482 [2.866, 12.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.075, 10.100], loss: 0.113479, mae: 0.324542, mean_q: 3.951558
 16251/100000: episode: 172, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 80.039, mean reward: 3.480 [2.607, 4.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.904, 10.100], loss: 0.138742, mae: 0.367382, mean_q: 3.954694
 16290/100000: episode: 173, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 177.067, mean reward: 4.540 [3.171, 10.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.628, 10.100], loss: 0.113386, mae: 0.330905, mean_q: 3.929326
 16316/100000: episode: 174, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 57.607, mean reward: 2.216 [1.726, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.702, 10.100], loss: 0.181615, mae: 0.379871, mean_q: 3.994120
 16413/100000: episode: 175, duration: 0.531s, episode steps: 97, steps per second: 183, episode reward: 186.891, mean reward: 1.927 [1.500, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.693, 10.100], loss: 0.170631, mae: 0.384001, mean_q: 4.000651
 16439/100000: episode: 176, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 56.853, mean reward: 2.187 [1.516, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.593, 10.129], loss: 0.165798, mae: 0.347739, mean_q: 3.992584
 16465/100000: episode: 177, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 51.755, mean reward: 1.991 [1.631, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.108, 10.100], loss: 0.125631, mae: 0.330595, mean_q: 4.005355
 16497/100000: episode: 178, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 74.995, mean reward: 2.344 [1.524, 6.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.806, 10.100], loss: 0.164140, mae: 0.359367, mean_q: 4.051916
 16520/100000: episode: 179, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 65.174, mean reward: 2.834 [2.012, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.285, 10.100], loss: 0.136746, mae: 0.341695, mean_q: 4.015115
 16552/100000: episode: 180, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 79.728, mean reward: 2.492 [2.016, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.461, 10.100], loss: 0.154004, mae: 0.374614, mean_q: 4.071850
 16651/100000: episode: 181, duration: 0.502s, episode steps: 99, steps per second: 197, episode reward: 180.353, mean reward: 1.822 [1.454, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.673, 10.100], loss: 0.139837, mae: 0.352931, mean_q: 4.018362
 16677/100000: episode: 182, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 66.612, mean reward: 2.562 [2.105, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.166, 10.100], loss: 0.243383, mae: 0.387964, mean_q: 4.030657
 16689/100000: episode: 183, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 34.923, mean reward: 2.910 [1.919, 6.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.406, 10.100], loss: 0.182151, mae: 0.390314, mean_q: 4.161690
 16728/100000: episode: 184, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 129.529, mean reward: 3.321 [2.249, 5.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.892, 10.100], loss: 0.157018, mae: 0.353347, mean_q: 4.053081
 16827/100000: episode: 185, duration: 0.500s, episode steps: 99, steps per second: 198, episode reward: 200.143, mean reward: 2.022 [1.470, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.135, 10.393], loss: 0.156403, mae: 0.352841, mean_q: 4.087063
 16859/100000: episode: 186, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 80.271, mean reward: 2.508 [1.733, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.312, 10.100], loss: 0.114244, mae: 0.348224, mean_q: 4.052633
 16958/100000: episode: 187, duration: 0.496s, episode steps: 99, steps per second: 200, episode reward: 178.950, mean reward: 1.808 [1.444, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-1.144, 10.166], loss: 0.119133, mae: 0.335576, mean_q: 4.053566
 17057/100000: episode: 188, duration: 0.515s, episode steps: 99, steps per second: 192, episode reward: 209.860, mean reward: 2.120 [1.510, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.420, 10.275], loss: 0.174889, mae: 0.360548, mean_q: 4.075273
 17069/100000: episode: 189, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 28.693, mean reward: 2.391 [2.005, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.126, 10.100], loss: 0.189476, mae: 0.437859, mean_q: 4.072288
 17168/100000: episode: 190, duration: 0.502s, episode steps: 99, steps per second: 197, episode reward: 185.392, mean reward: 1.873 [1.447, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.755, 10.100], loss: 0.141060, mae: 0.343714, mean_q: 4.045861
 17265/100000: episode: 191, duration: 0.503s, episode steps: 97, steps per second: 193, episode reward: 182.638, mean reward: 1.883 [1.487, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-1.036, 10.100], loss: 0.142764, mae: 0.340249, mean_q: 4.055433
 17362/100000: episode: 192, duration: 0.500s, episode steps: 97, steps per second: 194, episode reward: 189.321, mean reward: 1.952 [1.487, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.364, 10.100], loss: 0.146648, mae: 0.349207, mean_q: 4.079928
 17459/100000: episode: 193, duration: 0.494s, episode steps: 97, steps per second: 196, episode reward: 199.064, mean reward: 2.052 [1.515, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.211, 10.100], loss: 0.106035, mae: 0.329220, mean_q: 4.069963
 17491/100000: episode: 194, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 104.762, mean reward: 3.274 [1.729, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.823, 10.100], loss: 0.146457, mae: 0.362672, mean_q: 4.095887
 17523/100000: episode: 195, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 60.463, mean reward: 1.889 [1.694, 2.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.657, 10.100], loss: 0.128520, mae: 0.344593, mean_q: 4.074324
 17549/100000: episode: 196, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 64.418, mean reward: 2.478 [1.925, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.302, 10.100], loss: 0.147074, mae: 0.346392, mean_q: 4.045598
 17572/100000: episode: 197, duration: 0.146s, episode steps: 23, steps per second: 157, episode reward: 67.146, mean reward: 2.919 [2.040, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.675, 10.100], loss: 0.109974, mae: 0.334556, mean_q: 4.077073
 17671/100000: episode: 198, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 177.841, mean reward: 1.796 [1.441, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.232, 10.100], loss: 0.133697, mae: 0.352387, mean_q: 4.123636
 17703/100000: episode: 199, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 80.426, mean reward: 2.513 [1.707, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.670, 10.100], loss: 0.150940, mae: 0.372530, mean_q: 4.161475
 17725/100000: episode: 200, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 53.199, mean reward: 2.418 [1.588, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.100], loss: 0.123839, mae: 0.345124, mean_q: 4.092236
 17824/100000: episode: 201, duration: 0.523s, episode steps: 99, steps per second: 189, episode reward: 182.222, mean reward: 1.841 [1.435, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.073, 10.100], loss: 0.134570, mae: 0.354301, mean_q: 4.156330
 17922/100000: episode: 202, duration: 0.527s, episode steps: 98, steps per second: 186, episode reward: 189.111, mean reward: 1.930 [1.490, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.581, 10.293], loss: 0.152262, mae: 0.374795, mean_q: 4.127988
 17945/100000: episode: 203, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 65.719, mean reward: 2.857 [1.591, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.428, 10.100], loss: 0.124227, mae: 0.358739, mean_q: 4.142692
 17984/100000: episode: 204, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 145.896, mean reward: 3.741 [2.517, 6.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.337, 10.100], loss: 0.114585, mae: 0.337416, mean_q: 4.138126
 18016/100000: episode: 205, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 89.983, mean reward: 2.812 [1.728, 5.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.384, 10.100], loss: 0.179270, mae: 0.380281, mean_q: 4.198161
 18039/100000: episode: 206, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 64.788, mean reward: 2.817 [2.092, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.233, 10.100], loss: 0.127789, mae: 0.358562, mean_q: 4.185890
 18071/100000: episode: 207, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 95.239, mean reward: 2.976 [2.204, 6.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.438, 10.100], loss: 0.126028, mae: 0.348259, mean_q: 4.199913
 18170/100000: episode: 208, duration: 0.496s, episode steps: 99, steps per second: 200, episode reward: 187.683, mean reward: 1.896 [1.461, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.413, 10.100], loss: 0.131782, mae: 0.352510, mean_q: 4.182772
 18182/100000: episode: 209, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 26.883, mean reward: 2.240 [1.909, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.324, 10.100], loss: 0.183341, mae: 0.371303, mean_q: 4.238016
 18194/100000: episode: 210, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 28.595, mean reward: 2.383 [2.067, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.382, 10.100], loss: 0.127875, mae: 0.363292, mean_q: 4.258852
 18226/100000: episode: 211, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 91.517, mean reward: 2.860 [1.897, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.682, 10.100], loss: 0.138820, mae: 0.361496, mean_q: 4.199831
 18248/100000: episode: 212, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 54.491, mean reward: 2.477 [2.135, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.219, 10.100], loss: 0.130589, mae: 0.362143, mean_q: 4.192543
 18270/100000: episode: 213, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 64.936, mean reward: 2.952 [2.422, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.885, 10.100], loss: 0.164102, mae: 0.355263, mean_q: 4.243778
 18369/100000: episode: 214, duration: 0.476s, episode steps: 99, steps per second: 208, episode reward: 182.574, mean reward: 1.844 [1.489, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.232, 10.202], loss: 0.134035, mae: 0.356107, mean_q: 4.221594
 18392/100000: episode: 215, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 71.591, mean reward: 3.113 [2.037, 9.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.232, 10.100], loss: 0.143637, mae: 0.371814, mean_q: 4.163721
 18490/100000: episode: 216, duration: 0.485s, episode steps: 98, steps per second: 202, episode reward: 191.862, mean reward: 1.958 [1.472, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.773, 10.170], loss: 0.139178, mae: 0.355670, mean_q: 4.204644
 18502/100000: episode: 217, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 33.648, mean reward: 2.804 [1.981, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.210, 10.100], loss: 0.112931, mae: 0.342397, mean_q: 4.224059
 18525/100000: episode: 218, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 65.343, mean reward: 2.841 [2.136, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.885, 10.100], loss: 0.242040, mae: 0.398904, mean_q: 4.303420
 18544/100000: episode: 219, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 37.846, mean reward: 1.992 [1.695, 2.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.177, 10.100], loss: 0.132108, mae: 0.354469, mean_q: 4.208824
 18567/100000: episode: 220, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 59.404, mean reward: 2.583 [1.883, 4.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.557, 10.100], loss: 0.110800, mae: 0.347083, mean_q: 4.225433
 18593/100000: episode: 221, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 82.756, mean reward: 3.183 [2.038, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.513, 10.100], loss: 0.149191, mae: 0.367519, mean_q: 4.257401
 18632/100000: episode: 222, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 182.957, mean reward: 4.691 [2.566, 13.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.518, 10.100], loss: 0.248967, mae: 0.406867, mean_q: 4.212774
 18671/100000: episode: 223, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 120.896, mean reward: 3.100 [2.009, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.173, 10.100], loss: 0.189284, mae: 0.377836, mean_q: 4.256209
 18694/100000: episode: 224, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 63.191, mean reward: 2.747 [1.905, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.410, 10.100], loss: 0.275052, mae: 0.423553, mean_q: 4.318528
 18726/100000: episode: 225, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 73.296, mean reward: 2.290 [1.776, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.738, 10.100], loss: 0.161751, mae: 0.375861, mean_q: 4.328022
 18752/100000: episode: 226, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 62.343, mean reward: 2.398 [1.763, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.603, 10.100], loss: 0.154455, mae: 0.378535, mean_q: 4.276364
 18778/100000: episode: 227, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 74.684, mean reward: 2.872 [1.837, 5.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.398, 10.100], loss: 0.165354, mae: 0.404416, mean_q: 4.350101
 18876/100000: episode: 228, duration: 0.514s, episode steps: 98, steps per second: 191, episode reward: 199.859, mean reward: 2.039 [1.470, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.847, 10.106], loss: 0.199808, mae: 0.395885, mean_q: 4.335110
 18915/100000: episode: 229, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 113.031, mean reward: 2.898 [1.669, 5.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.049, 10.100], loss: 0.126901, mae: 0.368820, mean_q: 4.321254
 19012/100000: episode: 230, duration: 0.493s, episode steps: 97, steps per second: 197, episode reward: 202.518, mean reward: 2.088 [1.482, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.093, 10.245], loss: 0.180203, mae: 0.378044, mean_q: 4.328012
 19110/100000: episode: 231, duration: 0.509s, episode steps: 98, steps per second: 193, episode reward: 187.414, mean reward: 1.912 [1.452, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.977, 10.117], loss: 0.176760, mae: 0.396704, mean_q: 4.325658
 19207/100000: episode: 232, duration: 0.505s, episode steps: 97, steps per second: 192, episode reward: 186.084, mean reward: 1.918 [1.458, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.579, 10.100], loss: 0.188201, mae: 0.389735, mean_q: 4.316131
 19230/100000: episode: 233, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 66.648, mean reward: 2.898 [1.751, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.483, 10.100], loss: 0.130534, mae: 0.344273, mean_q: 4.277614
 19253/100000: episode: 234, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 66.824, mean reward: 2.905 [2.388, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.361, 10.100], loss: 0.226200, mae: 0.404889, mean_q: 4.350883
 19276/100000: episode: 235, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 73.908, mean reward: 3.213 [2.284, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.436, 10.100], loss: 0.135345, mae: 0.357695, mean_q: 4.328970
 19298/100000: episode: 236, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 46.172, mean reward: 2.099 [1.764, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.237, 10.100], loss: 0.161103, mae: 0.349113, mean_q: 4.269472
 19395/100000: episode: 237, duration: 0.499s, episode steps: 97, steps per second: 194, episode reward: 198.521, mean reward: 2.047 [1.480, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.971, 10.310], loss: 0.179766, mae: 0.397614, mean_q: 4.369971
 19494/100000: episode: 238, duration: 0.493s, episode steps: 99, steps per second: 201, episode reward: 195.019, mean reward: 1.970 [1.453, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.363, 10.289], loss: 0.163442, mae: 0.381602, mean_q: 4.359392
 19593/100000: episode: 239, duration: 0.529s, episode steps: 99, steps per second: 187, episode reward: 186.361, mean reward: 1.882 [1.432, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.742, 10.157], loss: 0.197445, mae: 0.385803, mean_q: 4.380720
[Info] 2-TH LEVEL FOUND: 7.932775020599365, Considering 10/90 traces
 19625/100000: episode: 240, duration: 4.192s, episode steps: 32, steps per second: 8, episode reward: 85.502, mean reward: 2.672 [2.014, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.047, 10.100], loss: 0.211821, mae: 0.398735, mean_q: 4.404612
 19660/100000: episode: 241, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 99.408, mean reward: 2.840 [2.176, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.124, 10.100], loss: 0.154594, mae: 0.382779, mean_q: 4.402794
 19698/100000: episode: 242, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 126.480, mean reward: 3.328 [2.296, 6.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.056, 10.100], loss: 0.173277, mae: 0.415174, mean_q: 4.395496
 19733/100000: episode: 243, duration: 0.191s, episode steps: 35, steps per second: 184, episode reward: 155.664, mean reward: 4.448 [2.587, 25.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.210, 10.100], loss: 0.198555, mae: 0.421606, mean_q: 4.470624
 19771/100000: episode: 244, duration: 0.225s, episode steps: 38, steps per second: 169, episode reward: 161.439, mean reward: 4.248 [2.838, 13.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.501, 10.100], loss: 0.158283, mae: 0.377391, mean_q: 4.429251
 19806/100000: episode: 245, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 114.910, mean reward: 3.283 [1.557, 5.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.697, 10.213], loss: 0.259793, mae: 0.429337, mean_q: 4.444535
 19844/100000: episode: 246, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 206.742, mean reward: 5.441 [1.803, 15.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.109, 10.100], loss: 0.260901, mae: 0.439707, mean_q: 4.532569
 19879/100000: episode: 247, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 125.611, mean reward: 3.589 [2.682, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.754, 10.100], loss: 0.258371, mae: 0.440941, mean_q: 4.510958
 19917/100000: episode: 248, duration: 0.185s, episode steps: 38, steps per second: 206, episode reward: 141.190, mean reward: 3.716 [2.183, 7.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.800, 10.100], loss: 0.193720, mae: 0.395213, mean_q: 4.536219
 19955/100000: episode: 249, duration: 0.196s, episode steps: 38, steps per second: 193, episode reward: 129.782, mean reward: 3.415 [1.993, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.332, 10.100], loss: 0.251639, mae: 0.427091, mean_q: 4.558164
 19992/100000: episode: 250, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 209.742, mean reward: 5.669 [3.427, 10.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.601, 10.100], loss: 0.206683, mae: 0.416021, mean_q: 4.632301
 20027/100000: episode: 251, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 118.755, mean reward: 3.393 [1.651, 6.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.689, 10.100], loss: 0.206518, mae: 0.400608, mean_q: 4.584306
 20065/100000: episode: 252, duration: 0.202s, episode steps: 38, steps per second: 189, episode reward: 144.310, mean reward: 3.798 [2.341, 7.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.222, 10.100], loss: 0.385025, mae: 0.421951, mean_q: 4.566667
 20103/100000: episode: 253, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 218.911, mean reward: 5.761 [2.561, 10.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.460, 10.100], loss: 0.352998, mae: 0.512120, mean_q: 4.715011
 20129/100000: episode: 254, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 99.941, mean reward: 3.844 [2.314, 6.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.949, 10.100], loss: 0.906206, mae: 0.723345, mean_q: 4.727049
 20167/100000: episode: 255, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 216.411, mean reward: 5.695 [3.224, 10.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.655, 10.100], loss: 0.368225, mae: 0.532847, mean_q: 4.827414
 20202/100000: episode: 256, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 147.442, mean reward: 4.213 [2.253, 21.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.194, 10.100], loss: 0.607952, mae: 0.575440, mean_q: 4.903069
 20240/100000: episode: 257, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 224.737, mean reward: 5.914 [3.261, 10.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.011, 10.100], loss: 0.317418, mae: 0.480092, mean_q: 4.810011
[Info] FALSIFICATION!
 20263/100000: episode: 258, duration: 0.526s, episode steps: 23, steps per second: 44, episode reward: 1247.005, mean reward: 54.218 [3.758, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.089, 9.629], loss: 0.304669, mae: 0.497243, mean_q: 4.831743
 20301/100000: episode: 259, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 107.627, mean reward: 2.832 [1.797, 6.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.369, 10.100], loss: 0.408248, mae: 0.532430, mean_q: 4.977741
 20339/100000: episode: 260, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 152.423, mean reward: 4.011 [1.976, 10.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.618, 10.100], loss: 0.406397, mae: 0.546479, mean_q: 4.911284
 20374/100000: episode: 261, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 99.635, mean reward: 2.847 [1.779, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.946, 10.100], loss: 0.446144, mae: 0.562925, mean_q: 4.990653
 20412/100000: episode: 262, duration: 0.205s, episode steps: 38, steps per second: 186, episode reward: 216.932, mean reward: 5.709 [3.464, 20.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.528, 10.100], loss: 0.321069, mae: 0.486069, mean_q: 4.978819
 20447/100000: episode: 263, duration: 0.209s, episode steps: 35, steps per second: 168, episode reward: 102.737, mean reward: 2.935 [1.492, 6.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.321, 10.100], loss: 0.539046, mae: 0.560319, mean_q: 4.987892
 20485/100000: episode: 264, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 165.634, mean reward: 4.359 [2.830, 9.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.149, 10.100], loss: 0.675274, mae: 0.587272, mean_q: 5.118481
 20520/100000: episode: 265, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 105.826, mean reward: 3.024 [2.198, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.564, 10.100], loss: 0.297341, mae: 0.487207, mean_q: 5.090592
 20558/100000: episode: 266, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 170.648, mean reward: 4.491 [2.918, 11.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.547, 10.100], loss: 5.486168, mae: 1.072592, mean_q: 5.062994
 20595/100000: episode: 267, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 152.617, mean reward: 4.125 [2.654, 9.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.959, 10.100], loss: 419.010773, mae: 2.166343, mean_q: 5.476094
 20633/100000: episode: 268, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 359.071, mean reward: 9.449 [4.024, 67.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.483, 10.100], loss: 1.196617, mae: 1.048056, mean_q: 4.952384
 20671/100000: episode: 269, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 108.530, mean reward: 2.856 [1.834, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.111, 10.100], loss: 406.226410, mae: 2.645296, mean_q: 5.991436
 20682/100000: episode: 270, duration: 0.080s, episode steps: 11, steps per second: 138, episode reward: 50.530, mean reward: 4.594 [3.875, 5.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.320, 10.100], loss: 9.490677, mae: 2.317683, mean_q: 3.147009
 20717/100000: episode: 271, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 183.571, mean reward: 5.245 [2.903, 19.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.664, 10.100], loss: 434.775238, mae: 2.449594, mean_q: 5.980568
 20754/100000: episode: 272, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 151.610, mean reward: 4.098 [2.732, 9.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.068, 10.100], loss: 418.374115, mae: 3.005983, mean_q: 6.580153
 20792/100000: episode: 273, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 100.435, mean reward: 2.643 [1.648, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.144, 10.100], loss: 400.733978, mae: 1.997135, mean_q: 4.940152
 20830/100000: episode: 274, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 182.473, mean reward: 4.802 [3.462, 14.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.130, 10.100], loss: 404.471191, mae: 2.782850, mean_q: 6.480188
 20868/100000: episode: 275, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 109.233, mean reward: 2.875 [1.701, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.141, 10.100], loss: 801.624695, mae: 4.321563, mean_q: 7.896666
 20906/100000: episode: 276, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 202.957, mean reward: 5.341 [2.851, 13.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.180, 10.100], loss: 10.020711, mae: 1.486205, mean_q: 5.157452
 20943/100000: episode: 277, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 109.757, mean reward: 2.966 [2.070, 7.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.780, 10.100], loss: 412.974335, mae: 2.366791, mean_q: 6.104782
 20981/100000: episode: 278, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 134.431, mean reward: 3.538 [2.357, 6.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.854, 10.100], loss: 397.777100, mae: 2.281701, mean_q: 6.293186
 21016/100000: episode: 279, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 112.468, mean reward: 3.213 [1.845, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.024, 10.100], loss: 3.811730, mae: 1.786578, mean_q: 6.278188
 21051/100000: episode: 280, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 195.849, mean reward: 5.596 [2.662, 12.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.557, 10.100], loss: 2.085389, mae: 1.338442, mean_q: 5.892231
 21089/100000: episode: 281, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 166.320, mean reward: 4.377 [2.077, 11.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.470, 10.100], loss: 5.773052, mae: 1.282650, mean_q: 5.994115
 21115/100000: episode: 282, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 92.744, mean reward: 3.567 [2.191, 11.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.615, 10.100], loss: 9.702341, mae: 1.340668, mean_q: 6.129296
 21153/100000: episode: 283, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 115.225, mean reward: 3.032 [1.597, 5.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.082, 10.100], loss: 5.702359, mae: 1.159224, mean_q: 5.940402
 21188/100000: episode: 284, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 104.582, mean reward: 2.988 [2.025, 6.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.388, 10.100], loss: 433.776123, mae: 2.250690, mean_q: 6.148681
 21223/100000: episode: 285, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 112.854, mean reward: 3.224 [2.147, 5.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.594, 10.100], loss: 10.057925, mae: 1.819008, mean_q: 6.625935
 21261/100000: episode: 286, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 146.300, mean reward: 3.850 [2.007, 7.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.181, 10.100], loss: 397.342041, mae: 2.205817, mean_q: 6.318789
 21296/100000: episode: 287, duration: 0.193s, episode steps: 35, steps per second: 182, episode reward: 106.180, mean reward: 3.034 [2.111, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.354, 10.100], loss: 7.163374, mae: 1.588265, mean_q: 5.869157
 21331/100000: episode: 288, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 173.189, mean reward: 4.948 [3.156, 12.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.212, 10.100], loss: 1.539673, mae: 1.135796, mean_q: 5.825465
 21368/100000: episode: 289, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 128.021, mean reward: 3.460 [2.059, 7.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.229, 10.100], loss: 1.329894, mae: 1.000052, mean_q: 5.879399
 21406/100000: episode: 290, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 162.865, mean reward: 4.286 [3.177, 6.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.850, 10.100], loss: 5.189834, mae: 0.984815, mean_q: 5.631372
 21441/100000: episode: 291, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 100.837, mean reward: 2.881 [1.750, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.246, 10.100], loss: 1.537862, mae: 0.989204, mean_q: 5.788927
 21478/100000: episode: 292, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 169.430, mean reward: 4.579 [1.974, 8.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.578, 10.100], loss: 8.246649, mae: 1.152900, mean_q: 5.828804
 21515/100000: episode: 293, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 140.588, mean reward: 3.800 [2.988, 5.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.879, 10.100], loss: 1.235337, mae: 0.956646, mean_q: 5.954402
 21553/100000: episode: 294, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 191.880, mean reward: 5.049 [1.615, 13.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.294, 10.100], loss: 402.363037, mae: 2.521647, mean_q: 6.779145
 21590/100000: episode: 295, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 123.992, mean reward: 3.351 [2.487, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.286, 10.100], loss: 1.305672, mae: 1.012063, mean_q: 5.538063
 21628/100000: episode: 296, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 103.639, mean reward: 2.727 [1.702, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.773, 10.100], loss: 2.936590, mae: 1.009185, mean_q: 5.862322
 21639/100000: episode: 297, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 42.774, mean reward: 3.889 [3.396, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.381, 10.100], loss: 2.503796, mae: 1.154099, mean_q: 5.814391
 21674/100000: episode: 298, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 110.381, mean reward: 3.154 [1.636, 18.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.499, 10.100], loss: 1.660614, mae: 0.962184, mean_q: 5.869552
 21712/100000: episode: 299, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 157.338, mean reward: 4.140 [2.429, 8.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.963, 10.100], loss: 1.584453, mae: 0.972569, mean_q: 5.915968
 21747/100000: episode: 300, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 121.475, mean reward: 3.471 [2.501, 6.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.697, 10.100], loss: 1.098389, mae: 0.877628, mean_q: 5.778007
 21784/100000: episode: 301, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 108.140, mean reward: 2.923 [1.882, 6.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.733, 10.100], loss: 9.608086, mae: 1.125735, mean_q: 6.072148
 21822/100000: episode: 302, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 195.578, mean reward: 5.147 [2.667, 26.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.501, 10.100], loss: 397.976471, mae: 2.263229, mean_q: 6.674794
 21857/100000: episode: 303, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 105.479, mean reward: 3.014 [1.867, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.299, 10.100], loss: 430.686920, mae: 2.342612, mean_q: 6.251733
 21895/100000: episode: 304, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 109.518, mean reward: 2.882 [1.974, 7.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.464, 10.100], loss: 4.364170, mae: 1.584057, mean_q: 6.544865
 21930/100000: episode: 305, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 116.354, mean reward: 3.324 [1.905, 6.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.651, 10.100], loss: 1.609761, mae: 1.090798, mean_q: 6.085427
 21967/100000: episode: 306, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 522.393, mean reward: 14.119 [2.241, 185.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.679, 10.100], loss: 408.509430, mae: 2.312313, mean_q: 6.650642
 22002/100000: episode: 307, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 138.580, mean reward: 3.959 [2.597, 6.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.409, 10.100], loss: 9.274770, mae: 1.894186, mean_q: 6.348588
 22040/100000: episode: 308, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 339.934, mean reward: 8.946 [3.597, 59.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.432, 10.100], loss: 3.401461, mae: 1.155454, mean_q: 6.222537
 22078/100000: episode: 309, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 113.227, mean reward: 2.980 [1.696, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.341, 10.100], loss: 394.499268, mae: 1.833997, mean_q: 6.172057
 22116/100000: episode: 310, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 258.036, mean reward: 6.790 [2.101, 47.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.497, 10.100], loss: 8.824049, mae: 1.944554, mean_q: 7.278107
 22154/100000: episode: 311, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 492.743, mean reward: 12.967 [1.946, 130.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.980, 10.100], loss: 4.926081, mae: 1.211376, mean_q: 6.511654
 22189/100000: episode: 312, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 143.028, mean reward: 4.087 [2.579, 8.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.988, 10.100], loss: 458.505402, mae: 2.730020, mean_q: 7.044540
 22227/100000: episode: 313, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 113.656, mean reward: 2.991 [1.815, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.442, 10.100], loss: 5.249246, mae: 1.889557, mean_q: 6.822626
 22253/100000: episode: 314, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 82.594, mean reward: 3.177 [1.906, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.915, 10.100], loss: 4.107298, mae: 1.195103, mean_q: 6.489258
 22291/100000: episode: 315, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 137.408, mean reward: 3.616 [2.271, 6.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.294, 10.100], loss: 11.533494, mae: 1.374453, mean_q: 6.714520
 22317/100000: episode: 316, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 101.312, mean reward: 3.897 [2.573, 5.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.850, 10.100], loss: 31.331055, mae: 1.553639, mean_q: 6.995747
 22355/100000: episode: 317, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 104.431, mean reward: 2.748 [1.767, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.247, 10.100], loss: 2.610809, mae: 1.098054, mean_q: 6.489850
 22393/100000: episode: 318, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 118.351, mean reward: 3.114 [1.758, 5.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.386, 10.100], loss: 3.596581, mae: 1.109649, mean_q: 6.533931
 22419/100000: episode: 319, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 102.592, mean reward: 3.946 [2.361, 5.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.161, 10.100], loss: 1.529778, mae: 1.032041, mean_q: 6.729327
 22457/100000: episode: 320, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 139.526, mean reward: 3.672 [1.943, 7.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.411, 10.100], loss: 15.800648, mae: 1.272275, mean_q: 6.571496
 22495/100000: episode: 321, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 185.915, mean reward: 4.892 [2.365, 7.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-1.165, 10.100], loss: 19.226732, mae: 1.336144, mean_q: 6.968792
 22533/100000: episode: 322, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 146.910, mean reward: 3.866 [2.180, 7.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.156, 10.100], loss: 16.140167, mae: 1.254677, mean_q: 6.597722
 22571/100000: episode: 323, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 206.109, mean reward: 5.424 [3.336, 11.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.254, 10.100], loss: 807.820679, mae: 4.765245, mean_q: 9.016383
 22582/100000: episode: 324, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 41.506, mean reward: 3.773 [2.867, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.368, 10.100], loss: 3.390563, mae: 1.790547, mean_q: 5.003004
 22620/100000: episode: 325, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 123.293, mean reward: 3.245 [2.143, 6.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.238, 10.100], loss: 6.014251, mae: 1.308630, mean_q: 6.614856
 22658/100000: episode: 326, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 169.384, mean reward: 4.457 [3.061, 11.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.624, 10.100], loss: 4.759774, mae: 1.144459, mean_q: 6.737946
 22696/100000: episode: 327, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 118.899, mean reward: 3.129 [2.248, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-1.501, 10.100], loss: 6.464357, mae: 1.176821, mean_q: 6.901794
 22734/100000: episode: 328, duration: 0.223s, episode steps: 38, steps per second: 171, episode reward: 130.649, mean reward: 3.438 [1.499, 6.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.137, 10.110], loss: 10.669795, mae: 1.288036, mean_q: 6.968354
 22769/100000: episode: 329, duration: 0.176s, episode steps: 35, steps per second: 198, episode reward: 134.487, mean reward: 3.842 [2.243, 9.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.358, 10.100], loss: 12.651157, mae: 1.482042, mean_q: 7.119667
[Info] Complete ISplit Iteration
[Info] Levels: [4.329876, 7.932775, 21.584534]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 22807/100000: episode: 330, duration: 4.450s, episode steps: 38, steps per second: 9, episode reward: 134.856, mean reward: 3.549 [2.015, 4.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.575, 10.100], loss: 15.245176, mae: 1.156377, mean_q: 6.795269
 22907/100000: episode: 331, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 185.572, mean reward: 1.856 [1.434, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.418, 10.098], loss: 3.051825, mae: 1.064458, mean_q: 6.792777
 23007/100000: episode: 332, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.499, mean reward: 1.915 [1.521, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.621, 10.098], loss: 12.578007, mae: 1.232378, mean_q: 6.930964
 23107/100000: episode: 333, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 199.867, mean reward: 1.999 [1.503, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.663, 10.302], loss: 5.867070, mae: 1.073781, mean_q: 6.894325
 23207/100000: episode: 334, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.795, mean reward: 1.838 [1.434, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.288, 10.098], loss: 158.696426, mae: 1.845884, mean_q: 7.361894
 23307/100000: episode: 335, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 182.603, mean reward: 1.826 [1.448, 2.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.459, 10.292], loss: 17.567776, mae: 1.521237, mean_q: 6.811306
 23407/100000: episode: 336, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 265.801, mean reward: 2.658 [1.474, 8.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.929, 10.098], loss: 164.068939, mae: 1.885476, mean_q: 7.449805
 23507/100000: episode: 337, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.427, mean reward: 1.884 [1.457, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.844, 10.335], loss: 153.812012, mae: 1.674202, mean_q: 7.041646
 23607/100000: episode: 338, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 208.144, mean reward: 2.081 [1.469, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.492], loss: 160.200516, mae: 2.077247, mean_q: 7.164135
 23707/100000: episode: 339, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 195.303, mean reward: 1.953 [1.463, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.373], loss: 5.615115, mae: 1.152582, mean_q: 6.848209
 23807/100000: episode: 340, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 206.548, mean reward: 2.065 [1.471, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.032, 10.098], loss: 3.904121, mae: 1.039307, mean_q: 6.717732
 23907/100000: episode: 341, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 247.349, mean reward: 2.473 [1.530, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.731, 10.310], loss: 4.402612, mae: 0.972430, mean_q: 6.595049
 24007/100000: episode: 342, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 209.660, mean reward: 2.097 [1.468, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.881, 10.170], loss: 9.247290, mae: 1.130816, mean_q: 6.754278
 24107/100000: episode: 343, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 176.609, mean reward: 1.766 [1.454, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.663, 10.098], loss: 158.092560, mae: 2.094831, mean_q: 7.155485
 24207/100000: episode: 344, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 209.110, mean reward: 2.091 [1.562, 4.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.684, 10.187], loss: 13.922371, mae: 1.528110, mean_q: 6.904521
 24307/100000: episode: 345, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.895, mean reward: 1.959 [1.437, 5.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.853, 10.294], loss: 161.845901, mae: 2.200540, mean_q: 7.136255
 24407/100000: episode: 346, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 178.357, mean reward: 1.784 [1.451, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.012, 10.098], loss: 19.712194, mae: 1.432227, mean_q: 7.013157
 24507/100000: episode: 347, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 214.126, mean reward: 2.141 [1.442, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.606, 10.277], loss: 4.246109, mae: 1.108537, mean_q: 6.874191
 24607/100000: episode: 348, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 200.344, mean reward: 2.003 [1.449, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.846, 10.172], loss: 155.864044, mae: 1.757115, mean_q: 7.073563
 24707/100000: episode: 349, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.543, mean reward: 1.905 [1.441, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.460, 10.098], loss: 11.200806, mae: 1.203397, mean_q: 6.695795
 24807/100000: episode: 350, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 191.486, mean reward: 1.915 [1.492, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.028, 10.332], loss: 8.557758, mae: 1.160267, mean_q: 6.757174
 24907/100000: episode: 351, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.295, mean reward: 2.033 [1.452, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.778, 10.098], loss: 155.696747, mae: 1.988358, mean_q: 6.768163
 25007/100000: episode: 352, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 219.954, mean reward: 2.200 [1.472, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.641, 10.479], loss: 3.217797, mae: 0.903560, mean_q: 6.241229
 25107/100000: episode: 353, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 202.720, mean reward: 2.027 [1.488, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.800, 10.098], loss: 4.083621, mae: 0.959960, mean_q: 6.403427
 25207/100000: episode: 354, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 223.033, mean reward: 2.230 [1.486, 4.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.438, 10.098], loss: 5.025378, mae: 0.946258, mean_q: 6.155615
 25307/100000: episode: 355, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.049, mean reward: 1.930 [1.450, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.442, 10.322], loss: 10.491948, mae: 0.973547, mean_q: 6.202260
 25407/100000: episode: 356, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.816, mean reward: 1.968 [1.493, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.128], loss: 8.704126, mae: 0.943063, mean_q: 6.098959
 25507/100000: episode: 357, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.228, mean reward: 2.032 [1.460, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.236, 10.154], loss: 13.393424, mae: 0.995964, mean_q: 6.017076
 25607/100000: episode: 358, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 204.742, mean reward: 2.047 [1.489, 5.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.307, 10.098], loss: 1.866528, mae: 0.734150, mean_q: 5.660162
 25707/100000: episode: 359, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 200.730, mean reward: 2.007 [1.479, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.108], loss: 7.090978, mae: 0.823881, mean_q: 5.692013
 25807/100000: episode: 360, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 187.811, mean reward: 1.878 [1.456, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.118], loss: 9.981893, mae: 0.876315, mean_q: 5.664948
 25907/100000: episode: 361, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.894, mean reward: 1.919 [1.456, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.281, 10.360], loss: 10.327632, mae: 0.924845, mean_q: 5.741102
 26007/100000: episode: 362, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.048, mean reward: 1.890 [1.439, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.724, 10.297], loss: 2.924127, mae: 0.745281, mean_q: 5.465462
 26107/100000: episode: 363, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 177.613, mean reward: 1.776 [1.453, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.473, 10.098], loss: 12.021937, mae: 0.847843, mean_q: 5.437494
 26207/100000: episode: 364, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 193.036, mean reward: 1.930 [1.435, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.451, 10.425], loss: 4.374864, mae: 0.697859, mean_q: 5.328353
 26307/100000: episode: 365, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 189.896, mean reward: 1.899 [1.477, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.754, 10.098], loss: 6.119484, mae: 0.672394, mean_q: 5.205874
 26407/100000: episode: 366, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 213.414, mean reward: 2.134 [1.483, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.909, 10.295], loss: 16.236229, mae: 0.807184, mean_q: 5.285309
 26507/100000: episode: 367, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 194.264, mean reward: 1.943 [1.469, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.151, 10.321], loss: 1.648354, mae: 0.614228, mean_q: 5.026441
 26607/100000: episode: 368, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.343, mean reward: 1.983 [1.494, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.253, 10.329], loss: 9.084317, mae: 0.687250, mean_q: 5.007781
 26707/100000: episode: 369, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 183.070, mean reward: 1.831 [1.489, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.777, 10.217], loss: 9.370116, mae: 0.724028, mean_q: 5.065971
 26807/100000: episode: 370, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 223.632, mean reward: 2.236 [1.474, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.801, 10.098], loss: 3.345426, mae: 0.603139, mean_q: 4.950755
 26907/100000: episode: 371, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 181.933, mean reward: 1.819 [1.443, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.287, 10.177], loss: 3.942151, mae: 0.592129, mean_q: 4.817313
 27007/100000: episode: 372, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.010, mean reward: 1.880 [1.472, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.439, 10.220], loss: 2.270241, mae: 0.564753, mean_q: 4.702543
 27107/100000: episode: 373, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.769, mean reward: 1.898 [1.459, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.352], loss: 0.895353, mae: 0.462755, mean_q: 4.551710
 27207/100000: episode: 374, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.652, mean reward: 1.907 [1.471, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.400], loss: 0.167066, mae: 0.384355, mean_q: 4.406186
 27307/100000: episode: 375, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 197.530, mean reward: 1.975 [1.484, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.082, 10.098], loss: 0.178213, mae: 0.377781, mean_q: 4.287008
 27407/100000: episode: 376, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.164, mean reward: 1.912 [1.439, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.431, 10.098], loss: 0.150224, mae: 0.357872, mean_q: 4.227910
 27507/100000: episode: 377, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.724, mean reward: 1.927 [1.454, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.869, 10.275], loss: 0.145273, mae: 0.348545, mean_q: 4.081850
 27607/100000: episode: 378, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 192.648, mean reward: 1.926 [1.481, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.401, 10.103], loss: 0.139515, mae: 0.347307, mean_q: 4.073711
 27707/100000: episode: 379, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.189, mean reward: 1.912 [1.510, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.252, 10.227], loss: 0.129002, mae: 0.328978, mean_q: 3.989160
 27807/100000: episode: 380, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.656, mean reward: 1.887 [1.482, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.249, 10.325], loss: 0.107955, mae: 0.314962, mean_q: 3.942226
 27907/100000: episode: 381, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.769, mean reward: 1.928 [1.451, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.824, 10.192], loss: 0.108383, mae: 0.317088, mean_q: 3.919482
 28007/100000: episode: 382, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.913, mean reward: 1.889 [1.439, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.220, 10.098], loss: 0.112714, mae: 0.319281, mean_q: 3.925665
 28107/100000: episode: 383, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.051, mean reward: 2.021 [1.581, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.745, 10.141], loss: 0.101579, mae: 0.316370, mean_q: 3.955742
 28207/100000: episode: 384, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.358, mean reward: 1.914 [1.475, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.838, 10.098], loss: 0.104999, mae: 0.313006, mean_q: 3.932611
 28307/100000: episode: 385, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.487, mean reward: 1.935 [1.448, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.256], loss: 0.105928, mae: 0.315773, mean_q: 3.944836
 28407/100000: episode: 386, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.761, mean reward: 1.838 [1.464, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.102, 10.098], loss: 0.104906, mae: 0.311869, mean_q: 3.909201
 28507/100000: episode: 387, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 179.228, mean reward: 1.792 [1.475, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.136], loss: 0.103902, mae: 0.313655, mean_q: 3.899215
 28607/100000: episode: 388, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 188.574, mean reward: 1.886 [1.451, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.193, 10.098], loss: 0.100070, mae: 0.308678, mean_q: 3.904887
 28707/100000: episode: 389, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.540, mean reward: 1.895 [1.485, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.245], loss: 0.109467, mae: 0.313860, mean_q: 3.878656
 28807/100000: episode: 390, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.941, mean reward: 1.919 [1.445, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.140], loss: 0.095194, mae: 0.308469, mean_q: 3.878731
 28907/100000: episode: 391, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 205.000, mean reward: 2.050 [1.473, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.801, 10.378], loss: 0.098652, mae: 0.302878, mean_q: 3.872097
 29007/100000: episode: 392, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.163, mean reward: 1.942 [1.503, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.459, 10.277], loss: 0.090149, mae: 0.294299, mean_q: 3.844998
 29107/100000: episode: 393, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.219, mean reward: 1.902 [1.473, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.483, 10.368], loss: 0.091553, mae: 0.289544, mean_q: 3.839919
 29207/100000: episode: 394, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 198.565, mean reward: 1.986 [1.470, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.338], loss: 0.093797, mae: 0.294801, mean_q: 3.847432
 29307/100000: episode: 395, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 226.861, mean reward: 2.269 [1.501, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.487, 10.098], loss: 0.103938, mae: 0.305969, mean_q: 3.852515
 29407/100000: episode: 396, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.338, mean reward: 1.883 [1.459, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.889, 10.098], loss: 0.096732, mae: 0.290325, mean_q: 3.874288
 29507/100000: episode: 397, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 215.050, mean reward: 2.151 [1.446, 4.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.838, 10.134], loss: 0.102221, mae: 0.303807, mean_q: 3.892116
 29607/100000: episode: 398, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 219.545, mean reward: 2.195 [1.449, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.827, 10.098], loss: 0.099533, mae: 0.298162, mean_q: 3.874125
 29707/100000: episode: 399, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.912, mean reward: 1.909 [1.480, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.860, 10.149], loss: 0.091785, mae: 0.295550, mean_q: 3.883374
 29807/100000: episode: 400, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 189.293, mean reward: 1.893 [1.453, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.832, 10.152], loss: 0.102188, mae: 0.303706, mean_q: 3.893098
 29907/100000: episode: 401, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.405, mean reward: 1.864 [1.467, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.155, 10.098], loss: 0.094453, mae: 0.295744, mean_q: 3.879299
 30007/100000: episode: 402, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 203.461, mean reward: 2.035 [1.448, 6.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.199], loss: 0.091045, mae: 0.293976, mean_q: 3.877536
 30107/100000: episode: 403, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 224.077, mean reward: 2.241 [1.452, 7.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.700, 10.098], loss: 0.080589, mae: 0.288776, mean_q: 3.869416
 30207/100000: episode: 404, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 188.956, mean reward: 1.890 [1.480, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.160, 10.100], loss: 0.089825, mae: 0.292799, mean_q: 3.858375
 30307/100000: episode: 405, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 176.084, mean reward: 1.761 [1.447, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.661, 10.098], loss: 0.092358, mae: 0.295218, mean_q: 3.870203
 30407/100000: episode: 406, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 190.542, mean reward: 1.905 [1.459, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.130, 10.098], loss: 0.086372, mae: 0.283429, mean_q: 3.850242
 30507/100000: episode: 407, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 199.210, mean reward: 1.992 [1.437, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.523, 10.098], loss: 0.087282, mae: 0.285926, mean_q: 3.845262
 30607/100000: episode: 408, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.006, mean reward: 1.940 [1.519, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.220], loss: 0.082316, mae: 0.286252, mean_q: 3.858763
 30707/100000: episode: 409, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 185.267, mean reward: 1.853 [1.480, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.697, 10.163], loss: 0.096932, mae: 0.294729, mean_q: 3.872992
 30807/100000: episode: 410, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 235.817, mean reward: 2.358 [1.443, 6.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.820, 10.416], loss: 0.115207, mae: 0.305035, mean_q: 3.872301
 30907/100000: episode: 411, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.708, mean reward: 1.927 [1.449, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.600, 10.128], loss: 0.081123, mae: 0.292436, mean_q: 3.876058
 31007/100000: episode: 412, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.021, mean reward: 1.970 [1.481, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.889, 10.098], loss: 0.101118, mae: 0.291750, mean_q: 3.862747
 31107/100000: episode: 413, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 180.686, mean reward: 1.807 [1.447, 2.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.820, 10.098], loss: 0.090777, mae: 0.290733, mean_q: 3.847360
 31207/100000: episode: 414, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 182.420, mean reward: 1.824 [1.451, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.343, 10.145], loss: 0.098044, mae: 0.300829, mean_q: 3.875393
 31307/100000: episode: 415, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.489, mean reward: 1.965 [1.455, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.115], loss: 0.093791, mae: 0.299327, mean_q: 3.891402
 31407/100000: episode: 416, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.739, mean reward: 1.997 [1.499, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.098], loss: 0.098191, mae: 0.297284, mean_q: 3.863596
 31507/100000: episode: 417, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.637, mean reward: 1.926 [1.484, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.821, 10.098], loss: 0.091183, mae: 0.291351, mean_q: 3.880688
 31607/100000: episode: 418, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.909, mean reward: 1.949 [1.469, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.098], loss: 0.096583, mae: 0.290459, mean_q: 3.850958
 31707/100000: episode: 419, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.251, mean reward: 1.833 [1.466, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.812, 10.287], loss: 0.088031, mae: 0.293643, mean_q: 3.871306
 31807/100000: episode: 420, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 187.214, mean reward: 1.872 [1.436, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.246], loss: 0.092626, mae: 0.286508, mean_q: 3.867362
 31907/100000: episode: 421, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 212.703, mean reward: 2.127 [1.461, 8.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.098], loss: 0.090720, mae: 0.290025, mean_q: 3.855968
 32007/100000: episode: 422, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.704, mean reward: 1.897 [1.442, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.360, 10.176], loss: 0.103505, mae: 0.309312, mean_q: 3.877198
 32107/100000: episode: 423, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 181.348, mean reward: 1.813 [1.446, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.894, 10.226], loss: 0.102210, mae: 0.303587, mean_q: 3.896076
 32207/100000: episode: 424, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 185.850, mean reward: 1.858 [1.453, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.217, 10.098], loss: 0.102275, mae: 0.306150, mean_q: 3.884438
 32307/100000: episode: 425, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 196.576, mean reward: 1.966 [1.467, 4.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.499, 10.284], loss: 0.085876, mae: 0.281153, mean_q: 3.863259
 32407/100000: episode: 426, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.931, mean reward: 1.969 [1.472, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.048, 10.098], loss: 0.093693, mae: 0.288234, mean_q: 3.838516
 32507/100000: episode: 427, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 184.064, mean reward: 1.841 [1.448, 2.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.115, 10.098], loss: 0.103920, mae: 0.302071, mean_q: 3.854869
 32607/100000: episode: 428, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.322, mean reward: 1.893 [1.434, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.102, 10.144], loss: 0.096682, mae: 0.303795, mean_q: 3.846351
 32707/100000: episode: 429, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.684, mean reward: 1.947 [1.438, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.387, 10.138], loss: 0.096310, mae: 0.298978, mean_q: 3.850401
[Info] 1-TH LEVEL FOUND: 5.098284721374512, Considering 10/90 traces
 32807/100000: episode: 430, duration: 4.579s, episode steps: 100, steps per second: 22, episode reward: 209.172, mean reward: 2.092 [1.509, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.245, 10.259], loss: 0.101021, mae: 0.299742, mean_q: 3.857760
 32817/100000: episode: 431, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 24.964, mean reward: 2.496 [2.230, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.540, 10.100], loss: 0.098029, mae: 0.283656, mean_q: 3.833393
 32827/100000: episode: 432, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 31.547, mean reward: 3.155 [2.357, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.233, 10.100], loss: 0.083169, mae: 0.304925, mean_q: 3.869821
 32860/100000: episode: 433, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 94.460, mean reward: 2.862 [2.024, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.394, 10.100], loss: 0.093716, mae: 0.285791, mean_q: 3.839260
 32893/100000: episode: 434, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 80.815, mean reward: 2.449 [1.760, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.523, 10.100], loss: 0.094314, mae: 0.289192, mean_q: 3.859181
 32932/100000: episode: 435, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 115.459, mean reward: 2.960 [1.922, 5.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.785, 10.100], loss: 0.111942, mae: 0.304232, mean_q: 3.904534
 33008/100000: episode: 436, duration: 0.399s, episode steps: 76, steps per second: 191, episode reward: 141.904, mean reward: 1.867 [1.481, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.679 [-0.937, 10.103], loss: 0.090907, mae: 0.297640, mean_q: 3.902660
 33084/100000: episode: 437, duration: 0.379s, episode steps: 76, steps per second: 200, episode reward: 172.389, mean reward: 2.268 [1.550, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-1.164, 10.207], loss: 0.089079, mae: 0.306319, mean_q: 3.884144
 33104/100000: episode: 438, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 53.419, mean reward: 2.671 [1.854, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.303], loss: 0.085116, mae: 0.281273, mean_q: 3.854750
 33124/100000: episode: 439, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 60.831, mean reward: 3.042 [2.171, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.709, 10.370], loss: 0.094000, mae: 0.302609, mean_q: 3.859912
 33203/100000: episode: 440, duration: 0.426s, episode steps: 79, steps per second: 185, episode reward: 180.666, mean reward: 2.287 [1.505, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.505, 10.255], loss: 0.104964, mae: 0.308995, mean_q: 3.920127
 33223/100000: episode: 441, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 69.583, mean reward: 3.479 [2.182, 5.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.431], loss: 0.116206, mae: 0.295966, mean_q: 3.878782
 33256/100000: episode: 442, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 95.472, mean reward: 2.893 [2.125, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.233, 10.100], loss: 0.135717, mae: 0.337519, mean_q: 3.947965
 33289/100000: episode: 443, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 79.976, mean reward: 2.424 [1.750, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.748, 10.100], loss: 0.103106, mae: 0.308776, mean_q: 3.954279
 33304/100000: episode: 444, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 33.919, mean reward: 2.261 [1.997, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.149, 10.100], loss: 0.089094, mae: 0.297092, mean_q: 3.900847
 33337/100000: episode: 445, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 84.433, mean reward: 2.559 [1.993, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.336, 10.100], loss: 0.145376, mae: 0.352919, mean_q: 4.003574
 33358/100000: episode: 446, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 85.266, mean reward: 4.060 [2.754, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.301, 10.526], loss: 0.120373, mae: 0.322421, mean_q: 3.958586
 33397/100000: episode: 447, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 128.132, mean reward: 3.285 [2.467, 6.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.684, 10.100], loss: 0.113993, mae: 0.339424, mean_q: 3.990806
 33417/100000: episode: 448, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 79.883, mean reward: 3.994 [2.834, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.711, 10.542], loss: 0.095032, mae: 0.307400, mean_q: 3.986631
 33427/100000: episode: 449, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 28.870, mean reward: 2.887 [2.092, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.310, 10.100], loss: 0.109073, mae: 0.315764, mean_q: 4.025639
 33448/100000: episode: 450, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 71.808, mean reward: 3.419 [2.726, 4.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.516], loss: 0.102225, mae: 0.325347, mean_q: 4.011701
 33487/100000: episode: 451, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 94.958, mean reward: 2.435 [1.472, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.790, 10.105], loss: 0.127454, mae: 0.340484, mean_q: 3.979200
 33497/100000: episode: 452, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 31.011, mean reward: 3.101 [2.370, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.180, 10.100], loss: 0.186815, mae: 0.395917, mean_q: 4.087651
 33530/100000: episode: 453, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 75.117, mean reward: 2.276 [1.826, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.269, 10.100], loss: 0.113803, mae: 0.333542, mean_q: 4.049950
 33551/100000: episode: 454, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 110.231, mean reward: 5.249 [3.600, 9.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.421, 10.594], loss: 0.101489, mae: 0.323204, mean_q: 4.034672
 33571/100000: episode: 455, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 52.518, mean reward: 2.626 [1.768, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.216, 10.251], loss: 0.218445, mae: 0.382746, mean_q: 4.076112
 33591/100000: episode: 456, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 53.588, mean reward: 2.679 [1.814, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.086, 10.275], loss: 0.087370, mae: 0.309069, mean_q: 4.050460
 33611/100000: episode: 457, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 58.153, mean reward: 2.908 [2.509, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.905, 10.309], loss: 0.162776, mae: 0.367771, mean_q: 4.106548
 33644/100000: episode: 458, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 85.545, mean reward: 2.592 [1.893, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.269, 10.100], loss: 0.134088, mae: 0.347792, mean_q: 4.070602
 33723/100000: episode: 459, duration: 0.420s, episode steps: 79, steps per second: 188, episode reward: 171.717, mean reward: 2.174 [1.520, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.656, 10.100], loss: 0.158516, mae: 0.366834, mean_q: 4.087495
 33744/100000: episode: 460, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 78.360, mean reward: 3.731 [2.533, 5.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.995, 10.404], loss: 0.112640, mae: 0.342749, mean_q: 4.002357
 33754/100000: episode: 461, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 25.828, mean reward: 2.583 [1.884, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.319, 10.100], loss: 0.144838, mae: 0.353614, mean_q: 3.994732
 33764/100000: episode: 462, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 30.257, mean reward: 3.026 [2.590, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.487, 10.100], loss: 0.136878, mae: 0.378583, mean_q: 4.117975
 33774/100000: episode: 463, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 30.444, mean reward: 3.044 [2.444, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.287, 10.100], loss: 0.126633, mae: 0.336747, mean_q: 4.117778
 33850/100000: episode: 464, duration: 0.402s, episode steps: 76, steps per second: 189, episode reward: 169.371, mean reward: 2.229 [1.657, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.680 [-0.399, 10.204], loss: 0.146444, mae: 0.371905, mean_q: 4.118847
 33860/100000: episode: 465, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 26.178, mean reward: 2.618 [2.123, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.376, 10.100], loss: 0.226399, mae: 0.409724, mean_q: 4.226250
 33870/100000: episode: 466, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 28.907, mean reward: 2.891 [2.350, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.403, 10.100], loss: 0.152508, mae: 0.363676, mean_q: 4.175935
 33949/100000: episode: 467, duration: 0.435s, episode steps: 79, steps per second: 181, episode reward: 148.704, mean reward: 1.882 [1.453, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.650 [-0.729, 10.177], loss: 0.138573, mae: 0.364319, mean_q: 4.112047
 33982/100000: episode: 468, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 64.991, mean reward: 1.969 [1.554, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.111, 10.100], loss: 0.131059, mae: 0.359193, mean_q: 4.162139
 34015/100000: episode: 469, duration: 0.165s, episode steps: 33, steps per second: 199, episode reward: 81.581, mean reward: 2.472 [1.778, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.294, 10.100], loss: 0.158837, mae: 0.374211, mean_q: 4.083359
 34048/100000: episode: 470, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 63.234, mean reward: 1.916 [1.480, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.260, 10.100], loss: 0.108186, mae: 0.331140, mean_q: 4.107499
 34063/100000: episode: 471, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 29.572, mean reward: 1.971 [1.741, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.259, 10.100], loss: 0.085776, mae: 0.315244, mean_q: 4.070288
 34139/100000: episode: 472, duration: 0.371s, episode steps: 76, steps per second: 205, episode reward: 156.014, mean reward: 2.053 [1.503, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.677 [-0.680, 10.100], loss: 0.164271, mae: 0.374728, mean_q: 4.169216
 34172/100000: episode: 473, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 71.938, mean reward: 2.180 [1.528, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.041, 10.100], loss: 0.153247, mae: 0.361749, mean_q: 4.103734
 34193/100000: episode: 474, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 69.048, mean reward: 3.288 [2.107, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.279, 10.436], loss: 0.117345, mae: 0.341270, mean_q: 4.123954
 34208/100000: episode: 475, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 33.019, mean reward: 2.201 [1.809, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.439, 10.100], loss: 0.187204, mae: 0.432187, mean_q: 4.253706
 34218/100000: episode: 476, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 24.957, mean reward: 2.496 [2.006, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.308, 10.100], loss: 0.123918, mae: 0.343019, mean_q: 4.133692
 34238/100000: episode: 477, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 86.960, mean reward: 4.348 [3.433, 5.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.265, 10.484], loss: 0.116164, mae: 0.321583, mean_q: 4.090796
 34271/100000: episode: 478, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 78.430, mean reward: 2.377 [1.690, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.415, 10.100], loss: 0.189652, mae: 0.374802, mean_q: 4.196560
 34347/100000: episode: 479, duration: 0.412s, episode steps: 76, steps per second: 184, episode reward: 155.916, mean reward: 2.052 [1.479, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-1.301, 10.118], loss: 0.155109, mae: 0.373978, mean_q: 4.182597
 34357/100000: episode: 480, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 26.614, mean reward: 2.661 [2.240, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.563, 10.100], loss: 0.133009, mae: 0.346610, mean_q: 4.096713
 34390/100000: episode: 481, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 77.140, mean reward: 2.338 [1.719, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.218, 10.100], loss: 0.184218, mae: 0.388626, mean_q: 4.187340
 34400/100000: episode: 482, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 31.013, mean reward: 3.101 [2.500, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.470, 10.100], loss: 0.115607, mae: 0.332079, mean_q: 4.138204
 34410/100000: episode: 483, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 24.406, mean reward: 2.441 [2.026, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.223, 10.100], loss: 0.198884, mae: 0.394303, mean_q: 4.198453
 34420/100000: episode: 484, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 28.346, mean reward: 2.835 [2.149, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.428, 10.100], loss: 0.127238, mae: 0.351533, mean_q: 4.100311
 34430/100000: episode: 485, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 28.379, mean reward: 2.838 [2.421, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.269, 10.100], loss: 0.205571, mae: 0.391377, mean_q: 4.232034
 34506/100000: episode: 486, duration: 0.390s, episode steps: 76, steps per second: 195, episode reward: 196.405, mean reward: 2.584 [1.733, 5.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.673 [-0.671, 10.384], loss: 0.162088, mae: 0.366091, mean_q: 4.188925
 34527/100000: episode: 487, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 65.965, mean reward: 3.141 [2.518, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.545], loss: 0.130091, mae: 0.353321, mean_q: 4.178231
 34560/100000: episode: 488, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 70.814, mean reward: 2.146 [1.602, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.273, 10.100], loss: 0.180044, mae: 0.401233, mean_q: 4.271347
 34599/100000: episode: 489, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 102.436, mean reward: 2.627 [1.503, 5.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.356, 10.100], loss: 0.138085, mae: 0.350850, mean_q: 4.193963
 34675/100000: episode: 490, duration: 0.385s, episode steps: 76, steps per second: 197, episode reward: 137.095, mean reward: 1.804 [1.454, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.680 [-1.069, 10.100], loss: 0.159491, mae: 0.387190, mean_q: 4.238792
 34690/100000: episode: 491, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 32.231, mean reward: 2.149 [1.775, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.106, 10.100], loss: 0.147452, mae: 0.389732, mean_q: 4.165791
 34711/100000: episode: 492, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 71.500, mean reward: 3.405 [2.702, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.056, 10.498], loss: 0.154269, mae: 0.351568, mean_q: 4.247493
 34721/100000: episode: 493, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 26.931, mean reward: 2.693 [2.160, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.361, 10.100], loss: 0.112462, mae: 0.324640, mean_q: 4.114249
 34736/100000: episode: 494, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 28.896, mean reward: 1.926 [1.605, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.522, 10.100], loss: 0.162413, mae: 0.377445, mean_q: 4.270373
 34757/100000: episode: 495, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 72.175, mean reward: 3.437 [2.870, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.462], loss: 0.176462, mae: 0.388596, mean_q: 4.310150
 34767/100000: episode: 496, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 24.517, mean reward: 2.452 [2.191, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.094, 10.100], loss: 0.179629, mae: 0.365365, mean_q: 4.261380
 34777/100000: episode: 497, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 28.439, mean reward: 2.844 [2.393, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.460, 10.100], loss: 0.186227, mae: 0.416172, mean_q: 4.319610
 34856/100000: episode: 498, duration: 0.417s, episode steps: 79, steps per second: 190, episode reward: 155.248, mean reward: 1.965 [1.453, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.665 [-0.479, 10.227], loss: 0.153800, mae: 0.367886, mean_q: 4.254181
 34866/100000: episode: 499, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 35.477, mean reward: 3.548 [2.516, 4.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.328, 10.100], loss: 0.169493, mae: 0.411842, mean_q: 4.393466
 34887/100000: episode: 500, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 54.048, mean reward: 2.574 [1.656, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.380, 10.243], loss: 0.161255, mae: 0.371525, mean_q: 4.277006
 34907/100000: episode: 501, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 65.533, mean reward: 3.277 [2.400, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.694, 10.405], loss: 0.148853, mae: 0.368586, mean_q: 4.229549
[Info] FALSIFICATION!
 34924/100000: episode: 502, duration: 0.330s, episode steps: 17, steps per second: 52, episode reward: 1096.637, mean reward: 64.508 [2.565, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.118, 9.484], loss: 0.167621, mae: 0.399686, mean_q: 4.403965
 34945/100000: episode: 503, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 69.180, mean reward: 3.294 [1.780, 6.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.699, 10.275], loss: 0.202027, mae: 0.402781, mean_q: 4.294429
 34955/100000: episode: 504, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 26.392, mean reward: 2.639 [2.187, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.288, 10.100], loss: 0.167859, mae: 0.383688, mean_q: 4.365100
 34988/100000: episode: 505, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 61.771, mean reward: 1.872 [1.616, 2.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.821, 10.145], loss: 0.209631, mae: 0.412577, mean_q: 4.346914
 34998/100000: episode: 506, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 27.020, mean reward: 2.702 [2.291, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.360, 10.100], loss: 0.192303, mae: 0.417708, mean_q: 4.342130
 35019/100000: episode: 507, duration: 0.114s, episode steps: 21, steps per second: 183, episode reward: 76.476, mean reward: 3.642 [2.878, 5.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.485], loss: 0.189996, mae: 0.411224, mean_q: 4.339333
 35052/100000: episode: 508, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 96.482, mean reward: 2.924 [2.119, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.208, 10.100], loss: 0.163847, mae: 0.402368, mean_q: 4.335716
 35062/100000: episode: 509, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 30.496, mean reward: 3.050 [2.193, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.431, 10.100], loss: 0.210881, mae: 0.449351, mean_q: 4.435638
 35072/100000: episode: 510, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 24.292, mean reward: 2.429 [2.070, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.271, 10.100], loss: 0.228818, mae: 0.452249, mean_q: 4.134423
 35087/100000: episode: 511, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 31.492, mean reward: 2.099 [1.917, 2.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.287, 10.100], loss: 0.204789, mae: 0.422319, mean_q: 4.448957
 35108/100000: episode: 512, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 60.390, mean reward: 2.876 [2.178, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.809, 10.463], loss: 0.274743, mae: 0.453800, mean_q: 4.330386
 35141/100000: episode: 513, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 80.831, mean reward: 2.449 [1.727, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.048, 10.100], loss: 0.180913, mae: 0.410037, mean_q: 4.345345
 35217/100000: episode: 514, duration: 0.394s, episode steps: 76, steps per second: 193, episode reward: 163.201, mean reward: 2.147 [1.460, 11.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-0.504, 10.448], loss: 0.149366, mae: 0.374659, mean_q: 4.324685
 35250/100000: episode: 515, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 102.084, mean reward: 3.093 [1.982, 6.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.252, 10.100], loss: 0.154600, mae: 0.379297, mean_q: 4.360570
 35329/100000: episode: 516, duration: 0.444s, episode steps: 79, steps per second: 178, episode reward: 178.052, mean reward: 2.254 [1.461, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.651 [-0.863, 10.335], loss: 0.202594, mae: 0.411992, mean_q: 4.357395
 35344/100000: episode: 517, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 39.145, mean reward: 2.610 [1.893, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.385, 10.100], loss: 0.169901, mae: 0.410419, mean_q: 4.379681
 35377/100000: episode: 518, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 102.060, mean reward: 3.093 [2.111, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.305, 10.100], loss: 0.136376, mae: 0.368671, mean_q: 4.402968
 35387/100000: episode: 519, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 24.520, mean reward: 2.452 [1.955, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.155, 10.100], loss: 0.118581, mae: 0.346740, mean_q: 4.393849
[Info] Complete ISplit Iteration
[Info] Levels: [5.0982847, 25.595774]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 35402/100000: episode: 520, duration: 4.307s, episode steps: 15, steps per second: 3, episode reward: 30.122, mean reward: 2.008 [1.674, 2.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.278, 10.100], loss: 0.165808, mae: 0.373967, mean_q: 4.397172
 35502/100000: episode: 521, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 182.805, mean reward: 1.828 [1.445, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.175, 10.182], loss: 148.218521, mae: 1.110908, mean_q: 4.546748
 35602/100000: episode: 522, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 215.193, mean reward: 2.152 [1.507, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.811, 10.098], loss: 0.260810, mae: 0.478212, mean_q: 4.336528
 35702/100000: episode: 523, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.010, mean reward: 1.920 [1.468, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.260, 10.344], loss: 0.175085, mae: 0.409175, mean_q: 4.345977
 35802/100000: episode: 524, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.200, mean reward: 1.812 [1.460, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.285, 10.155], loss: 152.620758, mae: 1.384829, mean_q: 4.548794
 35902/100000: episode: 525, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 186.105, mean reward: 1.861 [1.463, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.461, 10.125], loss: 298.818054, mae: 1.728220, mean_q: 4.898507
 36002/100000: episode: 526, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 183.207, mean reward: 1.832 [1.458, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.098], loss: 0.297113, mae: 0.509393, mean_q: 4.290545
 36102/100000: episode: 527, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 178.897, mean reward: 1.789 [1.447, 2.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.518, 10.098], loss: 148.737457, mae: 1.120294, mean_q: 4.659214
 36202/100000: episode: 528, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.708, mean reward: 1.967 [1.447, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.790, 10.098], loss: 147.270981, mae: 1.046991, mean_q: 4.586033
 36302/100000: episode: 529, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.980, mean reward: 1.930 [1.454, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.098], loss: 0.258093, mae: 0.451725, mean_q: 4.348220
 36402/100000: episode: 530, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.238, mean reward: 1.842 [1.449, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.151, 10.192], loss: 292.592987, mae: 1.768483, mean_q: 5.036194
 36502/100000: episode: 531, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 211.702, mean reward: 2.117 [1.464, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.468], loss: 0.393754, mae: 0.574601, mean_q: 4.290004
 36602/100000: episode: 532, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.287, mean reward: 1.833 [1.473, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.228, 10.098], loss: 144.793381, mae: 1.512579, mean_q: 4.854019
 36702/100000: episode: 533, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 220.422, mean reward: 2.204 [1.521, 5.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 144.998550, mae: 1.240723, mean_q: 4.685815
 36802/100000: episode: 534, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 190.352, mean reward: 1.904 [1.444, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.346, 10.100], loss: 0.381158, mae: 0.509170, mean_q: 4.284544
 36902/100000: episode: 535, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.595, mean reward: 1.916 [1.488, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.782, 10.098], loss: 144.631042, mae: 1.182804, mean_q: 4.750795
 37002/100000: episode: 536, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 212.592, mean reward: 2.126 [1.486, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.592, 10.367], loss: 142.840637, mae: 1.157622, mean_q: 4.616653
 37102/100000: episode: 537, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.792, mean reward: 1.858 [1.452, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.024, 10.255], loss: 0.399314, mae: 0.525902, mean_q: 4.345614
 37202/100000: episode: 538, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.708, mean reward: 1.897 [1.450, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.248, 10.147], loss: 276.532074, mae: 1.789724, mean_q: 4.897859
 37302/100000: episode: 539, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.308, mean reward: 1.873 [1.464, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.304, 10.105], loss: 140.831833, mae: 1.281757, mean_q: 4.720919
 37402/100000: episode: 540, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.528, mean reward: 1.895 [1.468, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.189, 10.160], loss: 0.487885, mae: 0.536143, mean_q: 4.370450
 37502/100000: episode: 541, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 300.283, mean reward: 3.003 [1.617, 8.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.408, 10.599], loss: 0.373890, mae: 0.506724, mean_q: 4.394921
 37602/100000: episode: 542, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 207.007, mean reward: 2.070 [1.444, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.708, 10.161], loss: 139.301895, mae: 1.263579, mean_q: 4.726448
 37702/100000: episode: 543, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.979, mean reward: 1.880 [1.448, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.240, 10.098], loss: 0.425121, mae: 0.518164, mean_q: 4.418386
 37802/100000: episode: 544, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 177.763, mean reward: 1.778 [1.450, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.746, 10.118], loss: 0.376210, mae: 0.464023, mean_q: 4.375418
 37902/100000: episode: 545, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.930, mean reward: 1.999 [1.477, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.763, 10.099], loss: 0.460760, mae: 0.462825, mean_q: 4.353287
 38002/100000: episode: 546, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 209.091, mean reward: 2.091 [1.489, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.339, 10.098], loss: 0.259214, mae: 0.461939, mean_q: 4.379434
 38102/100000: episode: 547, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 196.576, mean reward: 1.966 [1.458, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.401, 10.098], loss: 140.404587, mae: 1.194551, mean_q: 4.701005
 38202/100000: episode: 548, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.757, mean reward: 1.898 [1.458, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.590, 10.098], loss: 138.910507, mae: 1.210068, mean_q: 4.621456
 38302/100000: episode: 549, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.242, mean reward: 1.822 [1.467, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.205, 10.173], loss: 0.380509, mae: 0.515069, mean_q: 4.260433
 38402/100000: episode: 550, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.938, mean reward: 1.919 [1.450, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.951, 10.153], loss: 0.341104, mae: 0.450624, mean_q: 4.268219
 38502/100000: episode: 551, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 204.672, mean reward: 2.047 [1.484, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.229, 10.098], loss: 394.226257, mae: 2.111580, mean_q: 4.982886
 38602/100000: episode: 552, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 187.363, mean reward: 1.874 [1.468, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.301, 10.098], loss: 132.813919, mae: 1.397753, mean_q: 4.379713
 38702/100000: episode: 553, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 211.373, mean reward: 2.114 [1.499, 4.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.150], loss: 132.506943, mae: 1.252575, mean_q: 4.348457
 38802/100000: episode: 554, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.514, mean reward: 1.995 [1.484, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.235, 10.098], loss: 0.658817, mae: 0.522177, mean_q: 4.098844
 38902/100000: episode: 555, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 212.812, mean reward: 2.128 [1.438, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.239], loss: 0.498925, mae: 0.469674, mean_q: 4.120484
 39002/100000: episode: 556, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.917, mean reward: 1.939 [1.466, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.328, 10.098], loss: 0.296193, mae: 0.439701, mean_q: 4.149568
 39102/100000: episode: 557, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.788, mean reward: 1.808 [1.435, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.124], loss: 254.439743, mae: 1.635473, mean_q: 4.669672
 39202/100000: episode: 558, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 203.784, mean reward: 2.038 [1.464, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.125, 10.205], loss: 0.724392, mae: 0.538621, mean_q: 4.137791
 39302/100000: episode: 559, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 265.174, mean reward: 2.652 [1.468, 5.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.506], loss: 130.049057, mae: 1.149175, mean_q: 4.362871
 39402/100000: episode: 560, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 179.413, mean reward: 1.794 [1.469, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.691, 10.112], loss: 123.070038, mae: 0.896873, mean_q: 4.314054
 39502/100000: episode: 561, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.852, mean reward: 1.909 [1.446, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.154, 10.098], loss: 237.695847, mae: 1.585742, mean_q: 4.551815
 39602/100000: episode: 562, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.347, mean reward: 1.923 [1.470, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.839, 10.133], loss: 119.325195, mae: 1.279007, mean_q: 4.440160
 39702/100000: episode: 563, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 184.239, mean reward: 1.842 [1.465, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.658, 10.142], loss: 1.111261, mae: 0.522018, mean_q: 4.131127
 39802/100000: episode: 564, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 184.346, mean reward: 1.843 [1.439, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.341, 10.108], loss: 123.501251, mae: 1.154720, mean_q: 4.376647
 39902/100000: episode: 565, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.141, mean reward: 1.891 [1.474, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.881, 10.098], loss: 116.718758, mae: 1.223423, mean_q: 4.401264
 40002/100000: episode: 566, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 215.803, mean reward: 2.158 [1.459, 5.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.839, 10.297], loss: 0.439816, mae: 0.413953, mean_q: 3.936700
 40102/100000: episode: 567, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 214.539, mean reward: 2.145 [1.478, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.576, 10.098], loss: 0.274406, mae: 0.391107, mean_q: 3.932259
 40202/100000: episode: 568, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.955, mean reward: 1.930 [1.451, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.409, 10.266], loss: 0.382173, mae: 0.397610, mean_q: 3.945524
 40302/100000: episode: 569, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.474, mean reward: 1.885 [1.483, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.231], loss: 0.362993, mae: 0.377193, mean_q: 3.894649
 40402/100000: episode: 570, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 185.106, mean reward: 1.851 [1.469, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.709, 10.098], loss: 0.141006, mae: 0.368862, mean_q: 3.927532
 40502/100000: episode: 571, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.775, mean reward: 1.898 [1.466, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.673, 10.344], loss: 0.323789, mae: 0.387230, mean_q: 3.933868
 40602/100000: episode: 572, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.068, mean reward: 1.971 [1.454, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.604, 10.098], loss: 0.260350, mae: 0.379025, mean_q: 3.897723
 40702/100000: episode: 573, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 196.695, mean reward: 1.967 [1.510, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.749, 10.374], loss: 0.141381, mae: 0.365467, mean_q: 3.896132
 40802/100000: episode: 574, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.207, mean reward: 1.912 [1.446, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.880, 10.098], loss: 0.129371, mae: 0.350586, mean_q: 3.896802
 40902/100000: episode: 575, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.350, mean reward: 1.893 [1.464, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.422, 10.120], loss: 0.126686, mae: 0.349269, mean_q: 3.900150
 41002/100000: episode: 576, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 213.149, mean reward: 2.131 [1.437, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.656, 10.176], loss: 0.144625, mae: 0.362432, mean_q: 3.906744
 41102/100000: episode: 577, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 198.635, mean reward: 1.986 [1.463, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.322, 10.310], loss: 0.138955, mae: 0.369755, mean_q: 3.942467
 41202/100000: episode: 578, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.600, mean reward: 1.926 [1.461, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.584, 10.127], loss: 0.121679, mae: 0.348666, mean_q: 3.896947
 41302/100000: episode: 579, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 232.384, mean reward: 2.324 [1.435, 6.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.098], loss: 0.132187, mae: 0.359931, mean_q: 3.912692
 41402/100000: episode: 580, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.427, mean reward: 1.944 [1.432, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.272, 10.098], loss: 0.141889, mae: 0.357569, mean_q: 3.938974
 41502/100000: episode: 581, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.085, mean reward: 1.981 [1.481, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.396, 10.098], loss: 0.149549, mae: 0.368113, mean_q: 3.944492
 41602/100000: episode: 582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 196.931, mean reward: 1.969 [1.466, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.794, 10.391], loss: 0.124310, mae: 0.344183, mean_q: 3.903405
 41702/100000: episode: 583, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 196.328, mean reward: 1.963 [1.463, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.234, 10.098], loss: 0.138601, mae: 0.371045, mean_q: 3.929656
 41802/100000: episode: 584, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 196.770, mean reward: 1.968 [1.470, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.135, 10.119], loss: 0.143902, mae: 0.365523, mean_q: 3.924199
 41902/100000: episode: 585, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 197.527, mean reward: 1.975 [1.497, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.233, 10.177], loss: 0.133091, mae: 0.369334, mean_q: 3.962147
 42002/100000: episode: 586, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 177.913, mean reward: 1.779 [1.465, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.226, 10.121], loss: 0.138829, mae: 0.365729, mean_q: 3.927391
 42102/100000: episode: 587, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 245.632, mean reward: 2.456 [1.567, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.018, 10.098], loss: 0.133009, mae: 0.352104, mean_q: 3.932699
 42202/100000: episode: 588, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 219.575, mean reward: 2.196 [1.445, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.171, 10.098], loss: 0.137432, mae: 0.358654, mean_q: 3.930680
 42302/100000: episode: 589, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.005, mean reward: 1.930 [1.525, 5.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.646, 10.237], loss: 0.154544, mae: 0.376741, mean_q: 3.970133
 42402/100000: episode: 590, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.088, mean reward: 1.911 [1.464, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.316], loss: 0.151189, mae: 0.377209, mean_q: 3.973304
 42502/100000: episode: 591, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 187.209, mean reward: 1.872 [1.468, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.813, 10.098], loss: 0.125501, mae: 0.350237, mean_q: 3.952799
 42602/100000: episode: 592, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 179.089, mean reward: 1.791 [1.439, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.790, 10.218], loss: 0.130693, mae: 0.355923, mean_q: 3.915492
 42702/100000: episode: 593, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.710, mean reward: 1.987 [1.443, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.715, 10.285], loss: 0.122488, mae: 0.354681, mean_q: 3.912507
 42802/100000: episode: 594, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.276, mean reward: 1.933 [1.452, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.140], loss: 0.119205, mae: 0.353318, mean_q: 3.920773
 42902/100000: episode: 595, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.252, mean reward: 1.893 [1.470, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.098], loss: 0.121256, mae: 0.347141, mean_q: 3.913489
 43002/100000: episode: 596, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.634, mean reward: 1.936 [1.477, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.147, 10.098], loss: 0.117297, mae: 0.342016, mean_q: 3.929523
 43102/100000: episode: 597, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 200.109, mean reward: 2.001 [1.440, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.769, 10.190], loss: 0.127445, mae: 0.349500, mean_q: 3.938490
 43202/100000: episode: 598, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 190.871, mean reward: 1.909 [1.436, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.019, 10.101], loss: 0.107484, mae: 0.330515, mean_q: 3.900959
 43302/100000: episode: 599, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.367, mean reward: 1.964 [1.455, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.608, 10.183], loss: 0.115945, mae: 0.335584, mean_q: 3.903265
 43402/100000: episode: 600, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 201.650, mean reward: 2.017 [1.517, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.098], loss: 0.119889, mae: 0.342868, mean_q: 3.913463
 43502/100000: episode: 601, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.181, mean reward: 1.992 [1.501, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.098], loss: 0.115919, mae: 0.336366, mean_q: 3.906215
 43602/100000: episode: 602, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 203.150, mean reward: 2.032 [1.481, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.060, 10.098], loss: 0.118134, mae: 0.340584, mean_q: 3.910722
 43702/100000: episode: 603, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 196.750, mean reward: 1.967 [1.472, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.831, 10.098], loss: 0.117982, mae: 0.348694, mean_q: 3.929979
 43802/100000: episode: 604, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 207.145, mean reward: 2.071 [1.485, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.814, 10.098], loss: 0.116071, mae: 0.344251, mean_q: 3.925130
 43902/100000: episode: 605, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.872, mean reward: 1.839 [1.463, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.622, 10.316], loss: 0.107268, mae: 0.327521, mean_q: 3.906061
 44002/100000: episode: 606, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.768, mean reward: 1.978 [1.460, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.654, 10.272], loss: 0.115983, mae: 0.334849, mean_q: 3.913030
 44102/100000: episode: 607, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.578, mean reward: 2.036 [1.493, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.908, 10.098], loss: 0.108907, mae: 0.328210, mean_q: 3.892976
 44202/100000: episode: 608, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.572, mean reward: 1.896 [1.442, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.554, 10.187], loss: 0.117061, mae: 0.343811, mean_q: 3.923125
 44302/100000: episode: 609, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.060, mean reward: 1.831 [1.471, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.165, 10.098], loss: 0.096117, mae: 0.314051, mean_q: 3.904041
 44402/100000: episode: 610, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 183.063, mean reward: 1.831 [1.441, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.501, 10.102], loss: 0.093787, mae: 0.310054, mean_q: 3.888075
 44502/100000: episode: 611, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.711, mean reward: 1.827 [1.463, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.124], loss: 0.111791, mae: 0.327058, mean_q: 3.897275
 44602/100000: episode: 612, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 201.117, mean reward: 2.011 [1.487, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.895, 10.279], loss: 0.107871, mae: 0.329739, mean_q: 3.880947
 44702/100000: episode: 613, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.750, mean reward: 1.837 [1.464, 2.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.322], loss: 0.101515, mae: 0.325167, mean_q: 3.879468
 44802/100000: episode: 614, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.981, mean reward: 1.880 [1.447, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.261, 10.098], loss: 0.106117, mae: 0.322344, mean_q: 3.896896
 44902/100000: episode: 615, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.853, mean reward: 1.949 [1.442, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.467, 10.098], loss: 0.106257, mae: 0.322900, mean_q: 3.895875
 45002/100000: episode: 616, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.007, mean reward: 1.770 [1.459, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.430, 10.129], loss: 0.101316, mae: 0.318034, mean_q: 3.883916
 45102/100000: episode: 617, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.248, mean reward: 1.932 [1.466, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.093, 10.280], loss: 0.093793, mae: 0.308333, mean_q: 3.870502
 45202/100000: episode: 618, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.067, mean reward: 1.951 [1.473, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.396, 10.311], loss: 0.110125, mae: 0.326211, mean_q: 3.890555
 45302/100000: episode: 619, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 214.091, mean reward: 2.141 [1.446, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.730, 10.098], loss: 0.102104, mae: 0.320159, mean_q: 3.900711
[Info] 1-TH LEVEL FOUND: 5.392024993896484, Considering 10/90 traces
 45402/100000: episode: 620, duration: 4.643s, episode steps: 100, steps per second: 22, episode reward: 198.226, mean reward: 1.982 [1.471, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.098], loss: 0.093132, mae: 0.310474, mean_q: 3.895213
 45426/100000: episode: 621, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 56.284, mean reward: 2.345 [1.760, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.290, 10.100], loss: 0.085930, mae: 0.293586, mean_q: 3.885363
 45450/100000: episode: 622, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 45.829, mean reward: 1.910 [1.548, 2.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.100], loss: 0.103365, mae: 0.326422, mean_q: 3.882061
 45470/100000: episode: 623, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 54.299, mean reward: 2.715 [2.262, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.211, 10.100], loss: 0.089710, mae: 0.298056, mean_q: 3.869814
 45494/100000: episode: 624, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 66.934, mean reward: 2.789 [1.780, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.258, 10.100], loss: 0.078382, mae: 0.285768, mean_q: 3.870036
 45518/100000: episode: 625, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 69.547, mean reward: 2.898 [2.151, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.297, 10.100], loss: 0.077911, mae: 0.299446, mean_q: 3.874645
 45542/100000: episode: 626, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 55.687, mean reward: 2.320 [1.879, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.450, 10.100], loss: 0.098354, mae: 0.317794, mean_q: 3.927940
 45555/100000: episode: 627, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 41.561, mean reward: 3.197 [2.101, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.284, 10.100], loss: 0.097932, mae: 0.309537, mean_q: 3.923466
 45579/100000: episode: 628, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 52.735, mean reward: 2.197 [1.623, 2.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.389, 10.100], loss: 0.097896, mae: 0.315026, mean_q: 3.880736
 45592/100000: episode: 629, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 29.385, mean reward: 2.260 [1.675, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.215, 10.100], loss: 0.125579, mae: 0.352806, mean_q: 3.941697
 45616/100000: episode: 630, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 62.960, mean reward: 2.623 [2.015, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.811, 10.100], loss: 0.099651, mae: 0.318628, mean_q: 3.945264
 45640/100000: episode: 631, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 69.516, mean reward: 2.896 [2.314, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.755, 10.100], loss: 0.108142, mae: 0.343667, mean_q: 3.917489
 45664/100000: episode: 632, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 56.240, mean reward: 2.343 [1.762, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.687, 10.100], loss: 0.094371, mae: 0.313065, mean_q: 3.936483
 45688/100000: episode: 633, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 55.705, mean reward: 2.321 [1.979, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.214, 10.100], loss: 0.101707, mae: 0.315748, mean_q: 3.954005
 45712/100000: episode: 634, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 57.136, mean reward: 2.381 [1.803, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.329, 10.100], loss: 0.095522, mae: 0.314483, mean_q: 3.923866
 45736/100000: episode: 635, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 102.875, mean reward: 4.286 [2.322, 8.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.398, 10.100], loss: 0.116739, mae: 0.326641, mean_q: 3.941382
 45760/100000: episode: 636, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 65.017, mean reward: 2.709 [1.955, 4.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.604, 10.100], loss: 0.103148, mae: 0.326520, mean_q: 3.964181
 45784/100000: episode: 637, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 62.502, mean reward: 2.604 [2.044, 5.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.274, 10.100], loss: 0.132511, mae: 0.333035, mean_q: 3.995284
 45804/100000: episode: 638, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 50.656, mean reward: 2.533 [2.150, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.372, 10.100], loss: 0.115586, mae: 0.324132, mean_q: 4.005439
 45828/100000: episode: 639, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 66.238, mean reward: 2.760 [2.287, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.983, 10.100], loss: 0.144148, mae: 0.344769, mean_q: 4.014931
 45849/100000: episode: 640, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 61.905, mean reward: 2.948 [2.026, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.523, 10.100], loss: 0.114700, mae: 0.335102, mean_q: 4.008396
 45873/100000: episode: 641, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 68.347, mean reward: 2.848 [2.277, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.688, 10.100], loss: 0.142045, mae: 0.363869, mean_q: 4.061252
 45897/100000: episode: 642, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 64.042, mean reward: 2.668 [1.933, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.390, 10.100], loss: 0.103060, mae: 0.319346, mean_q: 3.966967
 45921/100000: episode: 643, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 63.183, mean reward: 2.633 [1.762, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.076, 10.100], loss: 0.118031, mae: 0.343241, mean_q: 4.021959
 45945/100000: episode: 644, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 52.864, mean reward: 2.203 [1.687, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.255, 10.100], loss: 0.126370, mae: 0.336559, mean_q: 3.984786
 45969/100000: episode: 645, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 49.396, mean reward: 2.058 [1.555, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.308, 10.100], loss: 0.120079, mae: 0.331054, mean_q: 3.985392
 45993/100000: episode: 646, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 51.084, mean reward: 2.128 [1.673, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.165, 10.100], loss: 0.109297, mae: 0.323265, mean_q: 4.013396
 46006/100000: episode: 647, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 26.618, mean reward: 2.048 [1.771, 2.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.287, 10.100], loss: 0.169858, mae: 0.338334, mean_q: 3.997135
 46020/100000: episode: 648, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 30.352, mean reward: 2.168 [1.902, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.409, 10.100], loss: 0.122898, mae: 0.341144, mean_q: 4.037987
 46044/100000: episode: 649, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 73.694, mean reward: 3.071 [2.107, 8.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.900, 10.100], loss: 0.148224, mae: 0.367748, mean_q: 4.029848
 46064/100000: episode: 650, duration: 0.127s, episode steps: 20, steps per second: 158, episode reward: 46.588, mean reward: 2.329 [1.903, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.232, 10.100], loss: 0.092023, mae: 0.311865, mean_q: 3.963408
 46088/100000: episode: 651, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 56.570, mean reward: 2.357 [1.874, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.378, 10.100], loss: 0.147141, mae: 0.349373, mean_q: 4.056631
 46101/100000: episode: 652, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 32.751, mean reward: 2.519 [2.102, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.273, 10.100], loss: 0.093056, mae: 0.307320, mean_q: 4.008276
 46111/100000: episode: 653, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 23.514, mean reward: 2.351 [1.862, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.211, 10.100], loss: 0.144958, mae: 0.379093, mean_q: 4.096401
 46132/100000: episode: 654, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 65.684, mean reward: 3.128 [2.023, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.264, 10.100], loss: 0.115252, mae: 0.347306, mean_q: 4.043752
 46146/100000: episode: 655, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 29.516, mean reward: 2.108 [1.789, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.106, 10.100], loss: 0.195254, mae: 0.416800, mean_q: 4.051742
 46170/100000: episode: 656, duration: 0.117s, episode steps: 24, steps per second: 204, episode reward: 58.108, mean reward: 2.421 [1.857, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.837, 10.100], loss: 0.136749, mae: 0.342734, mean_q: 3.992678
 46180/100000: episode: 657, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 21.763, mean reward: 2.176 [1.746, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.193, 10.100], loss: 0.123980, mae: 0.369553, mean_q: 4.087011
 46193/100000: episode: 658, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 26.934, mean reward: 2.072 [1.769, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.138, 10.100], loss: 0.161874, mae: 0.395304, mean_q: 4.049438
 46203/100000: episode: 659, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 26.153, mean reward: 2.615 [2.109, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.797, 10.100], loss: 0.163428, mae: 0.368430, mean_q: 4.151881
 46224/100000: episode: 660, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 47.287, mean reward: 2.252 [1.852, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.217, 10.100], loss: 0.135559, mae: 0.372016, mean_q: 4.053452
 46238/100000: episode: 661, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 27.319, mean reward: 1.951 [1.622, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.134, 10.100], loss: 0.125598, mae: 0.338647, mean_q: 4.082714
 46252/100000: episode: 662, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 28.847, mean reward: 2.060 [1.764, 2.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.313, 10.100], loss: 0.324327, mae: 0.365335, mean_q: 4.025322
 46265/100000: episode: 663, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 38.474, mean reward: 2.960 [2.225, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.400, 10.100], loss: 0.134733, mae: 0.373052, mean_q: 4.026932
 46289/100000: episode: 664, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 59.526, mean reward: 2.480 [2.066, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.285, 10.100], loss: 0.112477, mae: 0.329096, mean_q: 4.084699
 46299/100000: episode: 665, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 24.827, mean reward: 2.483 [2.351, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.261, 10.100], loss: 0.094453, mae: 0.305567, mean_q: 4.066947
 46312/100000: episode: 666, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 34.968, mean reward: 2.690 [2.074, 3.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.244, 10.100], loss: 0.167994, mae: 0.363569, mean_q: 4.064157
 46322/100000: episode: 667, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 20.441, mean reward: 2.044 [1.838, 2.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.189, 10.100], loss: 0.122848, mae: 0.358914, mean_q: 4.065597
 46346/100000: episode: 668, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 45.546, mean reward: 1.898 [1.454, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.588, 10.100], loss: 0.152663, mae: 0.343736, mean_q: 4.025493
 46366/100000: episode: 669, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 47.722, mean reward: 2.386 [1.905, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.907, 10.100], loss: 0.125205, mae: 0.359818, mean_q: 4.056111
 46380/100000: episode: 670, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 27.319, mean reward: 1.951 [1.658, 2.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.100], loss: 0.126823, mae: 0.363298, mean_q: 3.989590
 46404/100000: episode: 671, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 52.562, mean reward: 2.190 [1.819, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.185, 10.100], loss: 0.110475, mae: 0.341526, mean_q: 4.101647
 46414/100000: episode: 672, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 22.071, mean reward: 2.207 [1.860, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.378, 10.100], loss: 0.146510, mae: 0.344749, mean_q: 4.052355
 46427/100000: episode: 673, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 27.129, mean reward: 2.087 [1.729, 2.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.176, 10.100], loss: 0.142588, mae: 0.353900, mean_q: 3.972960
 46448/100000: episode: 674, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 48.291, mean reward: 2.300 [1.646, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.453, 10.100], loss: 0.105628, mae: 0.340018, mean_q: 4.122950
 46472/100000: episode: 675, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 56.619, mean reward: 2.359 [1.640, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.232, 10.100], loss: 0.135081, mae: 0.344901, mean_q: 4.071569
 46482/100000: episode: 676, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 21.315, mean reward: 2.131 [1.611, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.130, 10.100], loss: 0.141910, mae: 0.329458, mean_q: 4.047509
 46495/100000: episode: 677, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 54.876, mean reward: 4.221 [1.798, 22.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.341, 10.100], loss: 0.187884, mae: 0.369001, mean_q: 4.059669
 46505/100000: episode: 678, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 30.667, mean reward: 3.067 [2.448, 5.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.421, 10.100], loss: 0.126115, mae: 0.345324, mean_q: 4.133368
 46518/100000: episode: 679, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 25.895, mean reward: 1.992 [1.730, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.054, 10.100], loss: 0.119914, mae: 0.342095, mean_q: 3.976771
 46539/100000: episode: 680, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 53.124, mean reward: 2.530 [1.893, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.399, 10.100], loss: 0.445154, mae: 0.396380, mean_q: 4.069406
 46553/100000: episode: 681, duration: 0.097s, episode steps: 14, steps per second: 145, episode reward: 25.016, mean reward: 1.787 [1.578, 2.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.046, 10.100], loss: 0.135215, mae: 0.376464, mean_q: 4.070719
 46566/100000: episode: 682, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 35.047, mean reward: 2.696 [1.963, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.268, 10.100], loss: 0.114930, mae: 0.342940, mean_q: 4.075467
 46590/100000: episode: 683, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 54.821, mean reward: 2.284 [1.882, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.723, 10.100], loss: 0.111873, mae: 0.334882, mean_q: 4.038108
 46614/100000: episode: 684, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 49.361, mean reward: 2.057 [1.479, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.310, 10.100], loss: 0.111505, mae: 0.330310, mean_q: 4.005383
 46638/100000: episode: 685, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 53.655, mean reward: 2.236 [1.524, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.179, 10.100], loss: 0.148657, mae: 0.353078, mean_q: 4.097640
 46662/100000: episode: 686, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 70.482, mean reward: 2.937 [2.236, 5.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.450, 10.100], loss: 0.153727, mae: 0.380823, mean_q: 4.155629
 46686/100000: episode: 687, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 48.699, mean reward: 2.029 [1.516, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.231], loss: 0.112053, mae: 0.326578, mean_q: 4.061543
 46710/100000: episode: 688, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 58.887, mean reward: 2.454 [1.588, 6.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.188, 10.100], loss: 0.380973, mae: 0.391874, mean_q: 4.138782
 46731/100000: episode: 689, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 51.404, mean reward: 2.448 [1.933, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.369, 10.100], loss: 0.426551, mae: 0.375244, mean_q: 4.118078
 46755/100000: episode: 690, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 45.082, mean reward: 1.878 [1.545, 2.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.327, 10.100], loss: 0.124407, mae: 0.345259, mean_q: 4.076575
 46779/100000: episode: 691, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 63.456, mean reward: 2.644 [2.156, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.652, 10.100], loss: 0.619658, mae: 0.383632, mean_q: 4.147072
 46803/100000: episode: 692, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 67.793, mean reward: 2.825 [2.193, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.623, 10.100], loss: 0.116617, mae: 0.314613, mean_q: 4.009481
 46824/100000: episode: 693, duration: 0.132s, episode steps: 21, steps per second: 160, episode reward: 50.506, mean reward: 2.405 [1.959, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.200, 10.100], loss: 0.159510, mae: 0.352565, mean_q: 4.098823
 46834/100000: episode: 694, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 23.635, mean reward: 2.363 [1.979, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.140, 10.100], loss: 0.095908, mae: 0.320442, mean_q: 4.065315
 46858/100000: episode: 695, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 64.193, mean reward: 2.675 [1.980, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.210, 10.100], loss: 0.112276, mae: 0.330990, mean_q: 4.088395
 46882/100000: episode: 696, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 55.683, mean reward: 2.320 [2.019, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.210, 10.100], loss: 0.106856, mae: 0.338059, mean_q: 4.100667
 46906/100000: episode: 697, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 78.403, mean reward: 3.267 [2.586, 6.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.113, 10.100], loss: 0.134042, mae: 0.357701, mean_q: 4.143291
 46919/100000: episode: 698, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 29.575, mean reward: 2.275 [1.694, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.165, 10.100], loss: 0.136774, mae: 0.349087, mean_q: 4.115786
 46943/100000: episode: 699, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 55.281, mean reward: 2.303 [1.769, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.539, 10.100], loss: 0.117346, mae: 0.349939, mean_q: 4.073577
 46967/100000: episode: 700, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 62.765, mean reward: 2.615 [2.233, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.462, 10.100], loss: 0.377402, mae: 0.377910, mean_q: 4.113719
 46981/100000: episode: 701, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 27.751, mean reward: 1.982 [1.749, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.393, 10.100], loss: 0.141962, mae: 0.345053, mean_q: 4.079188
 46995/100000: episode: 702, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 28.728, mean reward: 2.052 [1.724, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.148, 10.100], loss: 0.122742, mae: 0.331005, mean_q: 4.120155
 47015/100000: episode: 703, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 46.356, mean reward: 2.318 [1.634, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.201, 10.100], loss: 0.123835, mae: 0.333718, mean_q: 4.113361
 47039/100000: episode: 704, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 68.079, mean reward: 2.837 [1.962, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.488, 10.100], loss: 0.370590, mae: 0.385529, mean_q: 4.162226
 47063/100000: episode: 705, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 46.001, mean reward: 1.917 [1.463, 2.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.469, 10.100], loss: 0.111759, mae: 0.321493, mean_q: 4.093907
 47083/100000: episode: 706, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 55.371, mean reward: 2.769 [2.096, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.572, 10.100], loss: 0.170584, mae: 0.363133, mean_q: 4.166700
 47107/100000: episode: 707, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 62.969, mean reward: 2.624 [1.794, 6.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.419, 10.100], loss: 0.129914, mae: 0.358740, mean_q: 4.136351
 47121/100000: episode: 708, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 28.092, mean reward: 2.007 [1.748, 2.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.336, 10.100], loss: 0.116170, mae: 0.336345, mean_q: 4.104819
 47145/100000: episode: 709, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 49.794, mean reward: 2.075 [1.622, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.517, 10.100], loss: 0.161405, mae: 0.359885, mean_q: 4.161329
[Info] 2-TH LEVEL FOUND: 6.061333656311035, Considering 10/90 traces
 47159/100000: episode: 710, duration: 4.096s, episode steps: 14, steps per second: 3, episode reward: 25.603, mean reward: 1.829 [1.455, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.159, 10.163], loss: 0.127649, mae: 0.347145, mean_q: 4.132778
 47172/100000: episode: 711, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 33.569, mean reward: 2.582 [2.277, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.259, 10.100], loss: 0.174732, mae: 0.370877, mean_q: 4.152627
 47189/100000: episode: 712, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 57.875, mean reward: 3.404 [3.049, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.391, 10.100], loss: 0.162698, mae: 0.379031, mean_q: 4.212935
 47203/100000: episode: 713, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 40.514, mean reward: 2.894 [2.381, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.278, 10.100], loss: 0.139172, mae: 0.362859, mean_q: 4.155467
 47219/100000: episode: 714, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 51.968, mean reward: 3.248 [2.366, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.388, 10.100], loss: 0.148932, mae: 0.373371, mean_q: 4.237389
 47233/100000: episode: 715, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 51.366, mean reward: 3.669 [2.814, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.342, 10.100], loss: 0.149898, mae: 0.364556, mean_q: 4.232276
 47247/100000: episode: 716, duration: 0.096s, episode steps: 14, steps per second: 145, episode reward: 65.571, mean reward: 4.684 [3.543, 8.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.332, 10.100], loss: 0.621596, mae: 0.445440, mean_q: 4.244137
 47253/100000: episode: 717, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 23.080, mean reward: 3.847 [3.397, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.404, 10.100], loss: 0.184319, mae: 0.379616, mean_q: 4.143129
 47259/100000: episode: 718, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 22.234, mean reward: 3.706 [3.039, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.413, 10.100], loss: 0.227470, mae: 0.413845, mean_q: 4.198596
 47273/100000: episode: 719, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 34.438, mean reward: 2.460 [1.866, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.304, 10.100], loss: 0.173588, mae: 0.378927, mean_q: 4.242851
 47286/100000: episode: 720, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 43.541, mean reward: 3.349 [2.765, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.296, 10.100], loss: 0.126731, mae: 0.351013, mean_q: 4.101135
 47302/100000: episode: 721, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 44.147, mean reward: 2.759 [2.226, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.186, 10.100], loss: 0.525315, mae: 0.420297, mean_q: 4.192193
 47318/100000: episode: 722, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 73.605, mean reward: 4.600 [2.942, 7.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.256, 10.100], loss: 0.185998, mae: 0.456946, mean_q: 4.180012
 47334/100000: episode: 723, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 87.414, mean reward: 5.463 [2.797, 13.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.550, 10.100], loss: 0.163617, mae: 0.378707, mean_q: 4.202930
 47350/100000: episode: 724, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 69.866, mean reward: 4.367 [3.273, 6.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.206, 10.100], loss: 0.176079, mae: 0.361788, mean_q: 4.166602
 47364/100000: episode: 725, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 46.914, mean reward: 3.351 [2.268, 8.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.367, 10.100], loss: 0.635470, mae: 0.464585, mean_q: 4.232936
 47378/100000: episode: 726, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 71.565, mean reward: 5.112 [2.523, 11.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.421, 10.100], loss: 0.181838, mae: 0.412648, mean_q: 4.216718
 47392/100000: episode: 727, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 44.842, mean reward: 3.203 [2.682, 5.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.808, 10.100], loss: 0.776498, mae: 0.517381, mean_q: 4.388478
 47405/100000: episode: 728, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 51.556, mean reward: 3.966 [2.971, 7.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.943, 10.100], loss: 0.330733, mae: 0.444242, mean_q: 4.294971
 47421/100000: episode: 729, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 90.250, mean reward: 5.641 [2.895, 9.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.655, 10.100], loss: 0.260290, mae: 0.379789, mean_q: 4.271034
 47434/100000: episode: 730, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 39.335, mean reward: 3.026 [2.334, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.300, 10.100], loss: 0.178662, mae: 0.396249, mean_q: 4.334654
 47450/100000: episode: 731, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 61.616, mean reward: 3.851 [2.852, 5.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.389, 10.100], loss: 0.212256, mae: 0.387124, mean_q: 4.346512
 47456/100000: episode: 732, duration: 0.036s, episode steps: 6, steps per second: 164, episode reward: 19.377, mean reward: 3.229 [2.701, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.369, 10.100], loss: 0.244394, mae: 0.437467, mean_q: 4.367039
 47469/100000: episode: 733, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 31.154, mean reward: 2.396 [2.092, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.658, 10.100], loss: 0.168201, mae: 0.380434, mean_q: 4.255551
 47483/100000: episode: 734, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 53.430, mean reward: 3.816 [2.651, 7.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.712, 10.100], loss: 0.143108, mae: 0.360569, mean_q: 4.250621
 47497/100000: episode: 735, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 62.725, mean reward: 4.480 [3.218, 6.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.418, 10.100], loss: 0.232088, mae: 0.421461, mean_q: 4.413373
 47511/100000: episode: 736, duration: 0.068s, episode steps: 14, steps per second: 204, episode reward: 64.865, mean reward: 4.633 [2.500, 22.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.415, 10.100], loss: 0.194240, mae: 0.398001, mean_q: 4.358192
 47527/100000: episode: 737, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 50.568, mean reward: 3.161 [2.618, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.535, 10.100], loss: 0.170587, mae: 0.396264, mean_q: 4.251094
 47541/100000: episode: 738, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 38.421, mean reward: 2.744 [2.081, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.244, 10.100], loss: 0.171990, mae: 0.395872, mean_q: 4.269165
 47552/100000: episode: 739, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 31.401, mean reward: 2.855 [1.795, 4.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.459, 10.100], loss: 0.226992, mae: 0.461208, mean_q: 4.497642
 47568/100000: episode: 740, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 46.619, mean reward: 2.914 [1.926, 5.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.244, 10.100], loss: 0.125548, mae: 0.357821, mean_q: 4.268377
 47582/100000: episode: 741, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 45.885, mean reward: 3.277 [2.716, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.453, 10.100], loss: 0.253658, mae: 0.422226, mean_q: 4.325646
 47593/100000: episode: 742, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 27.684, mean reward: 2.517 [1.871, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.406, 10.100], loss: 0.164679, mae: 0.367232, mean_q: 4.263611
 47599/100000: episode: 743, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 20.862, mean reward: 3.477 [2.953, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.518, 10.100], loss: 0.199090, mae: 0.403351, mean_q: 4.298730
 47610/100000: episode: 744, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 25.771, mean reward: 2.343 [2.042, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.239, 10.100], loss: 0.380473, mae: 0.508809, mean_q: 4.396106
 47624/100000: episode: 745, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 30.244, mean reward: 2.160 [1.750, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.231, 10.100], loss: 0.216084, mae: 0.448406, mean_q: 4.452824
 47638/100000: episode: 746, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 45.997, mean reward: 3.286 [2.495, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.298, 10.100], loss: 0.212213, mae: 0.427328, mean_q: 4.456509
 47654/100000: episode: 747, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 52.595, mean reward: 3.287 [2.414, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.340, 10.100], loss: 0.258171, mae: 0.434670, mean_q: 4.385318
 47660/100000: episode: 748, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 24.639, mean reward: 4.107 [3.833, 4.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.500, 10.100], loss: 0.182599, mae: 0.414079, mean_q: 4.340285
 47676/100000: episode: 749, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 66.904, mean reward: 4.181 [2.653, 6.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.731, 10.100], loss: 0.188262, mae: 0.436087, mean_q: 4.484233
 47682/100000: episode: 750, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 25.247, mean reward: 4.208 [3.440, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.977, 10.100], loss: 0.246008, mae: 0.478400, mean_q: 4.340536
 47696/100000: episode: 751, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 50.701, mean reward: 3.621 [2.581, 5.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.225, 10.100], loss: 0.229066, mae: 0.413641, mean_q: 4.287773
 47702/100000: episode: 752, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 28.972, mean reward: 4.829 [4.173, 5.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.708, 10.100], loss: 0.533011, mae: 0.533071, mean_q: 4.496756
 47713/100000: episode: 753, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 30.886, mean reward: 2.808 [1.692, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.449, 10.100], loss: 0.725733, mae: 0.488978, mean_q: 4.516766
 47726/100000: episode: 754, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 43.275, mean reward: 3.329 [2.238, 5.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.446, 10.100], loss: 0.216257, mae: 0.440319, mean_q: 4.470584
 47742/100000: episode: 755, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 55.490, mean reward: 3.468 [2.805, 5.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.294, 10.100], loss: 0.164605, mae: 0.396345, mean_q: 4.398077
 47759/100000: episode: 756, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 49.996, mean reward: 2.941 [1.779, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.258, 10.100], loss: 0.156561, mae: 0.382626, mean_q: 4.468894
 47776/100000: episode: 757, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 126.995, mean reward: 7.470 [3.720, 18.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.595, 10.100], loss: 0.650213, mae: 0.486562, mean_q: 4.499150
 47792/100000: episode: 758, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 49.072, mean reward: 3.067 [2.181, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.154, 10.100], loss: 0.193894, mae: 0.400927, mean_q: 4.443725
 47808/100000: episode: 759, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 42.123, mean reward: 2.633 [1.996, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.390, 10.100], loss: 0.242635, mae: 0.420426, mean_q: 4.505406
 47822/100000: episode: 760, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 43.344, mean reward: 3.096 [2.386, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.352, 10.100], loss: 0.710474, mae: 0.523370, mean_q: 4.599279
 47835/100000: episode: 761, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 35.752, mean reward: 2.750 [2.266, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.667, 10.100], loss: 0.261632, mae: 0.454451, mean_q: 4.610788
 47851/100000: episode: 762, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 41.030, mean reward: 2.564 [2.235, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.181, 10.100], loss: 0.338592, mae: 0.454238, mean_q: 4.536382
 47864/100000: episode: 763, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 32.636, mean reward: 2.510 [1.966, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.070, 10.100], loss: 0.304558, mae: 0.476790, mean_q: 4.483702
 47878/100000: episode: 764, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 43.704, mean reward: 3.122 [2.480, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.210, 10.100], loss: 0.222754, mae: 0.442441, mean_q: 4.558766
 47889/100000: episode: 765, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 29.117, mean reward: 2.647 [2.129, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.435, 10.100], loss: 0.245531, mae: 0.418278, mean_q: 4.464203
 47903/100000: episode: 766, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 46.650, mean reward: 3.332 [2.679, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.578, 10.100], loss: 0.205342, mae: 0.404407, mean_q: 4.515610
 47919/100000: episode: 767, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 61.010, mean reward: 3.813 [2.606, 5.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.940, 10.100], loss: 0.207853, mae: 0.421554, mean_q: 4.554954
 47933/100000: episode: 768, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 49.862, mean reward: 3.562 [2.194, 5.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.422, 10.100], loss: 0.711689, mae: 0.552393, mean_q: 4.769485
 47946/100000: episode: 769, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 85.095, mean reward: 6.546 [2.783, 17.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.887, 10.100], loss: 0.575173, mae: 0.542087, mean_q: 4.578289
 47960/100000: episode: 770, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 47.608, mean reward: 3.401 [2.796, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.399, 10.100], loss: 0.300540, mae: 0.499664, mean_q: 4.551843
 47966/100000: episode: 771, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 26.210, mean reward: 4.368 [3.086, 7.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.439, 10.100], loss: 0.609799, mae: 0.479577, mean_q: 4.662181
 47980/100000: episode: 772, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 44.169, mean reward: 3.155 [2.455, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.492, 10.100], loss: 0.550987, mae: 0.511756, mean_q: 4.689796
 47994/100000: episode: 773, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 56.170, mean reward: 4.012 [2.615, 8.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.650, 10.100], loss: 0.335801, mae: 0.475285, mean_q: 4.652499
 48008/100000: episode: 774, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 32.842, mean reward: 2.346 [2.042, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.595, 10.100], loss: 0.243739, mae: 0.471140, mean_q: 4.562416
 48024/100000: episode: 775, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 52.918, mean reward: 3.307 [2.673, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.693, 10.100], loss: 0.733524, mae: 0.528240, mean_q: 4.637882
 48035/100000: episode: 776, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 24.640, mean reward: 2.240 [1.684, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.331, 10.100], loss: 0.284045, mae: 0.507664, mean_q: 4.629002
 48041/100000: episode: 777, duration: 0.046s, episode steps: 6, steps per second: 132, episode reward: 33.137, mean reward: 5.523 [3.650, 7.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.174, 10.100], loss: 0.292563, mae: 0.506733, mean_q: 4.543364
 48052/100000: episode: 778, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 39.029, mean reward: 3.548 [2.211, 6.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.955, 10.100], loss: 0.329114, mae: 0.490488, mean_q: 4.658263
 48068/100000: episode: 779, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 41.023, mean reward: 2.564 [2.175, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.528, 10.100], loss: 0.307700, mae: 0.474456, mean_q: 4.703556
 48085/100000: episode: 780, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 58.511, mean reward: 3.442 [2.641, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.329, 10.100], loss: 0.334545, mae: 0.490305, mean_q: 4.709637
 48098/100000: episode: 781, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 39.692, mean reward: 3.053 [2.597, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.376, 10.100], loss: 0.234275, mae: 0.443672, mean_q: 4.601306
 48109/100000: episode: 782, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 129.958, mean reward: 11.814 [2.076, 70.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.706, 10.100], loss: 0.281423, mae: 0.489282, mean_q: 4.562619
 48123/100000: episode: 783, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 41.474, mean reward: 2.962 [2.147, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.232, 10.100], loss: 5.489223, mae: 0.784351, mean_q: 4.955872
 48137/100000: episode: 784, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 46.356, mean reward: 3.311 [2.335, 6.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.588, 10.100], loss: 1.069137, mae: 0.758478, mean_q: 4.768066
 48151/100000: episode: 785, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 35.366, mean reward: 2.526 [2.030, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.338, 10.100], loss: 0.392739, mae: 0.541075, mean_q: 4.557764
 48167/100000: episode: 786, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 44.955, mean reward: 2.810 [2.392, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.512, 10.100], loss: 0.401626, mae: 0.549258, mean_q: 4.725749
 48184/100000: episode: 787, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 53.204, mean reward: 3.130 [2.707, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.261, 10.100], loss: 0.752057, mae: 0.573311, mean_q: 4.784593
 48201/100000: episode: 788, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 54.136, mean reward: 3.184 [2.278, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.503, 10.100], loss: 0.463508, mae: 0.523697, mean_q: 4.656025
 48207/100000: episode: 789, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 23.526, mean reward: 3.921 [3.455, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.325, 10.100], loss: 0.279387, mae: 0.481208, mean_q: 4.648649
 48223/100000: episode: 790, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 62.400, mean reward: 3.900 [2.628, 6.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.127, 10.100], loss: 0.633754, mae: 0.605704, mean_q: 4.869879
 48234/100000: episode: 791, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 24.223, mean reward: 2.202 [1.702, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.353, 10.100], loss: 0.257118, mae: 0.473580, mean_q: 4.792637
 48251/100000: episode: 792, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 58.327, mean reward: 3.431 [2.706, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.397, 10.100], loss: 0.429460, mae: 0.571021, mean_q: 4.851344
 48265/100000: episode: 793, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 45.597, mean reward: 3.257 [2.178, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.264, 10.100], loss: 0.246387, mae: 0.440399, mean_q: 4.711803
 48279/100000: episode: 794, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 48.278, mean reward: 3.448 [2.648, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.312, 10.100], loss: 0.305838, mae: 0.499126, mean_q: 4.736684
 48290/100000: episode: 795, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 31.139, mean reward: 2.831 [2.599, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.378, 10.100], loss: 6.790785, mae: 0.679945, mean_q: 4.839688
 48303/100000: episode: 796, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 43.186, mean reward: 3.322 [2.610, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.344, 10.100], loss: 0.513062, mae: 0.724208, mean_q: 4.686793
 48317/100000: episode: 797, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 36.622, mean reward: 2.616 [2.101, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.233, 10.100], loss: 0.654003, mae: 0.651832, mean_q: 4.944041
 48323/100000: episode: 798, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 38.820, mean reward: 6.470 [3.881, 12.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.408, 10.100], loss: 0.224922, mae: 0.484992, mean_q: 4.891206
 48337/100000: episode: 799, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 35.928, mean reward: 2.566 [2.041, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.277, 10.100], loss: 0.746339, mae: 0.572052, mean_q: 4.849393
[Info] 3-TH LEVEL FOUND: 8.74036979675293, Considering 10/90 traces
 48348/100000: episode: 800, duration: 4.117s, episode steps: 11, steps per second: 3, episode reward: 31.224, mean reward: 2.839 [2.007, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.451, 10.100], loss: 6.778437, mae: 0.800936, mean_q: 5.098785
 48360/100000: episode: 801, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 49.014, mean reward: 4.084 [3.019, 5.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.417, 10.100], loss: 0.337672, mae: 0.568756, mean_q: 4.725316
 48370/100000: episode: 802, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 37.454, mean reward: 3.745 [2.981, 5.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.413, 10.100], loss: 0.857713, mae: 0.659472, mean_q: 4.930592
 48375/100000: episode: 803, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 18.596, mean reward: 3.719 [3.163, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.535, 10.100], loss: 0.339086, mae: 0.465147, mean_q: 4.761896
 48384/100000: episode: 804, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 28.799, mean reward: 3.200 [2.479, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.509, 10.100], loss: 0.299921, mae: 0.493090, mean_q: 4.892352
 48397/100000: episode: 805, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 73.471, mean reward: 5.652 [3.341, 8.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.516, 10.100], loss: 0.559294, mae: 0.508093, mean_q: 4.782343
 48403/100000: episode: 806, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 25.284, mean reward: 4.214 [3.445, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.521, 10.100], loss: 0.717827, mae: 0.600536, mean_q: 5.143880
 48410/100000: episode: 807, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 30.803, mean reward: 4.400 [3.127, 6.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.435, 10.100], loss: 0.323737, mae: 0.521428, mean_q: 4.777743
 48417/100000: episode: 808, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 24.719, mean reward: 3.531 [2.766, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.446, 10.100], loss: 0.346764, mae: 0.523967, mean_q: 4.962829
 48427/100000: episode: 809, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 39.732, mean reward: 3.973 [3.235, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.969, 10.100], loss: 0.418694, mae: 0.513542, mean_q: 4.585447
 48438/100000: episode: 810, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 48.984, mean reward: 4.453 [3.811, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.445, 10.100], loss: 1.068381, mae: 0.690766, mean_q: 4.924494
 48441/100000: episode: 811, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 18.904, mean reward: 6.301 [5.159, 7.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.484, 10.100], loss: 0.450960, mae: 0.545308, mean_q: 4.796878
 48447/100000: episode: 812, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 34.128, mean reward: 5.688 [3.742, 10.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.076, 10.100], loss: 12.283871, mae: 1.001984, mean_q: 5.195382
 48460/100000: episode: 813, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 79.287, mean reward: 6.099 [3.739, 9.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.359, 10.100], loss: 0.900769, mae: 0.644744, mean_q: 4.842411
 48465/100000: episode: 814, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 22.549, mean reward: 4.510 [3.835, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.481, 10.100], loss: 0.540983, mae: 0.602375, mean_q: 4.913527
 48477/100000: episode: 815, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 42.476, mean reward: 3.540 [2.261, 6.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.226, 10.100], loss: 0.329491, mae: 0.552970, mean_q: 4.848405
 48484/100000: episode: 816, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 22.601, mean reward: 3.229 [2.963, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.443, 10.100], loss: 10.673862, mae: 0.997230, mean_q: 5.217443
 48495/100000: episode: 817, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 77.354, mean reward: 7.032 [3.548, 17.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.451, 10.100], loss: 0.600292, mae: 0.683693, mean_q: 4.929717
 48504/100000: episode: 818, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 31.524, mean reward: 3.503 [2.970, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.742, 10.100], loss: 1.762880, mae: 0.785536, mean_q: 5.175143
 48510/100000: episode: 819, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 23.458, mean reward: 3.910 [3.333, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.408, 10.100], loss: 0.424587, mae: 0.546784, mean_q: 4.801450
 48523/100000: episode: 820, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 130.213, mean reward: 10.016 [3.893, 27.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.331, 10.100], loss: 5.932955, mae: 0.875542, mean_q: 5.253634
 48533/100000: episode: 821, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 33.060, mean reward: 3.306 [2.581, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.340, 10.100], loss: 1.563755, mae: 0.694267, mean_q: 4.665600
 48542/100000: episode: 822, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 43.406, mean reward: 4.823 [3.134, 9.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.706, 10.100], loss: 0.790490, mae: 0.672117, mean_q: 4.974282
 48553/100000: episode: 823, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 36.411, mean reward: 3.310 [2.772, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.311, 10.100], loss: 7.910668, mae: 1.165470, mean_q: 5.687564
 48560/100000: episode: 824, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 31.376, mean reward: 4.482 [3.228, 6.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.874, 10.100], loss: 0.445364, mae: 0.696311, mean_q: 4.445258
 48567/100000: episode: 825, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 23.894, mean reward: 3.413 [2.915, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.465, 10.100], loss: 0.572621, mae: 0.716388, mean_q: 5.287632
 48574/100000: episode: 826, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 26.025, mean reward: 3.718 [2.801, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.445, 10.100], loss: 0.447639, mae: 0.571761, mean_q: 4.962900
 48585/100000: episode: 827, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 48.771, mean reward: 4.434 [3.396, 6.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.667, 10.100], loss: 0.330921, mae: 0.506481, mean_q: 4.949066
 48593/100000: episode: 828, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 41.268, mean reward: 5.159 [3.608, 7.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.454, 10.100], loss: 0.522670, mae: 0.573730, mean_q: 4.934820
 48598/100000: episode: 829, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 19.936, mean reward: 3.987 [3.261, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.538, 10.100], loss: 1.541035, mae: 0.653498, mean_q: 5.209297
 48609/100000: episode: 830, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 39.188, mean reward: 3.563 [2.655, 7.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.403, 10.100], loss: 6.731137, mae: 0.814579, mean_q: 5.314509
 48617/100000: episode: 831, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 32.491, mean reward: 4.061 [3.442, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.496, 10.100], loss: 0.344357, mae: 0.583545, mean_q: 4.692677
 48626/100000: episode: 832, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 37.068, mean reward: 4.119 [3.138, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.630, 10.100], loss: 0.729133, mae: 0.609982, mean_q: 5.177664
 48632/100000: episode: 833, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 30.312, mean reward: 5.052 [3.351, 7.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.528, 10.100], loss: 0.553357, mae: 0.670753, mean_q: 5.200369
 48641/100000: episode: 834, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 30.909, mean reward: 3.434 [2.870, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.637, 10.100], loss: 0.372993, mae: 0.566074, mean_q: 4.958394
 48653/100000: episode: 835, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 70.394, mean reward: 5.866 [3.632, 9.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.496, 10.100], loss: 0.585623, mae: 0.631816, mean_q: 5.161620
 48666/100000: episode: 836, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 48.262, mean reward: 3.712 [2.539, 6.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.460, 10.100], loss: 0.660681, mae: 0.610038, mean_q: 5.170519
 48673/100000: episode: 837, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 24.048, mean reward: 3.435 [2.743, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.455, 10.100], loss: 0.552886, mae: 0.605886, mean_q: 5.320993
 48680/100000: episode: 838, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 32.900, mean reward: 4.700 [3.553, 7.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.526, 10.100], loss: 0.531728, mae: 0.627357, mean_q: 5.130318
 48687/100000: episode: 839, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 29.011, mean reward: 4.144 [2.444, 6.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.513, 10.100], loss: 0.477461, mae: 0.604973, mean_q: 5.187107
 48700/100000: episode: 840, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 140.721, mean reward: 10.825 [4.251, 50.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.525, 10.100], loss: 0.875862, mae: 0.676942, mean_q: 5.153350
 48708/100000: episode: 841, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 56.528, mean reward: 7.066 [3.854, 11.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.414, 10.100], loss: 0.343816, mae: 0.551321, mean_q: 4.941469
 48719/100000: episode: 842, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 44.677, mean reward: 4.062 [2.963, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.443, 10.100], loss: 0.598348, mae: 0.709601, mean_q: 5.320622
 48722/100000: episode: 843, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 62.284, mean reward: 20.761 [6.927, 41.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.507, 10.100], loss: 0.475761, mae: 0.680554, mean_q: 5.258517
 48734/100000: episode: 844, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 53.183, mean reward: 4.432 [3.342, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.593, 10.100], loss: 0.980983, mae: 0.679420, mean_q: 5.283627
 48741/100000: episode: 845, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 25.996, mean reward: 3.714 [2.891, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.567, 10.100], loss: 1.212415, mae: 0.682583, mean_q: 5.315070
 48747/100000: episode: 846, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 17.530, mean reward: 2.922 [2.689, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.583, 10.100], loss: 6.240359, mae: 0.884054, mean_q: 5.247614
 48752/100000: episode: 847, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 18.774, mean reward: 3.755 [2.968, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.596, 10.100], loss: 0.567154, mae: 0.693417, mean_q: 5.555406
 48760/100000: episode: 848, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 30.472, mean reward: 3.809 [3.263, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.395, 10.100], loss: 3.079565, mae: 0.871328, mean_q: 4.939227
 48768/100000: episode: 849, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 44.496, mean reward: 5.562 [4.117, 8.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.544, 10.100], loss: 1.366220, mae: 0.970715, mean_q: 5.506759
 48781/100000: episode: 850, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 53.388, mean reward: 4.107 [3.302, 6.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.482, 10.100], loss: 0.683558, mae: 0.745786, mean_q: 4.908319
 48784/100000: episode: 851, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 19.174, mean reward: 6.391 [5.434, 7.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.516, 10.100], loss: 0.387429, mae: 0.644294, mean_q: 5.372378
 48792/100000: episode: 852, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 77.834, mean reward: 9.729 [4.220, 15.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.499, 10.100], loss: 0.446953, mae: 0.640101, mean_q: 5.170215
 48801/100000: episode: 853, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 39.084, mean reward: 4.343 [3.248, 5.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.894, 10.100], loss: 0.550372, mae: 0.616283, mean_q: 5.168649
 48806/100000: episode: 854, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 19.750, mean reward: 3.950 [3.749, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.430, 10.100], loss: 0.420528, mae: 0.579839, mean_q: 5.163115
 48813/100000: episode: 855, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 25.970, mean reward: 3.710 [2.853, 6.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.917, 10.100], loss: 0.676858, mae: 0.783761, mean_q: 5.810830
 48826/100000: episode: 856, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 41.548, mean reward: 3.196 [2.736, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.635, 10.100], loss: 6.538046, mae: 0.889271, mean_q: 5.564588
 48831/100000: episode: 857, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 24.954, mean reward: 4.991 [3.851, 5.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.432, 10.100], loss: 0.754792, mae: 0.826112, mean_q: 5.353457
 48840/100000: episode: 858, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 29.458, mean reward: 3.273 [2.636, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.436, 10.100], loss: 0.658022, mae: 0.729213, mean_q: 5.128678
 48852/100000: episode: 859, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 64.833, mean reward: 5.403 [4.184, 7.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.269, 10.100], loss: 0.616401, mae: 0.642224, mean_q: 5.264439
 48862/100000: episode: 860, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 53.158, mean reward: 5.316 [3.252, 9.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.683, 10.100], loss: 0.930004, mae: 0.749243, mean_q: 5.367414
 48870/100000: episode: 861, duration: 0.047s, episode steps: 8, steps per second: 168, episode reward: 49.857, mean reward: 6.232 [4.135, 8.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.612, 10.100], loss: 0.502402, mae: 0.647368, mean_q: 5.093681
 48873/100000: episode: 862, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 15.568, mean reward: 5.189 [4.929, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.531, 10.100], loss: 0.966361, mae: 0.645789, mean_q: 4.878130
 48883/100000: episode: 863, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 42.517, mean reward: 4.252 [3.553, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.791, 10.100], loss: 1.091597, mae: 0.703099, mean_q: 5.350981
 48893/100000: episode: 864, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 41.002, mean reward: 4.100 [2.870, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.319, 10.100], loss: 1.158897, mae: 0.719217, mean_q: 5.467542
 48902/100000: episode: 865, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 30.106, mean reward: 3.345 [2.610, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.385, 10.100], loss: 0.758540, mae: 0.677089, mean_q: 5.350111
 48912/100000: episode: 866, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 46.724, mean reward: 4.672 [3.633, 7.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.967, 10.100], loss: 0.752514, mae: 0.729344, mean_q: 5.531098
 48915/100000: episode: 867, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 16.071, mean reward: 5.357 [5.025, 5.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.524, 10.100], loss: 1.146782, mae: 0.834379, mean_q: 5.660433
 48927/100000: episode: 868, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 45.316, mean reward: 3.776 [3.127, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.424, 10.100], loss: 0.476473, mae: 0.596578, mean_q: 5.134089
 48935/100000: episode: 869, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 37.317, mean reward: 4.665 [3.155, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.440, 10.100], loss: 1.489600, mae: 0.793038, mean_q: 5.353086
 48944/100000: episode: 870, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 30.718, mean reward: 3.413 [2.758, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.320, 10.100], loss: 0.825167, mae: 0.686130, mean_q: 5.365894
 48954/100000: episode: 871, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 54.237, mean reward: 5.424 [2.875, 9.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.657, 10.100], loss: 0.894969, mae: 0.720509, mean_q: 5.401610
 48966/100000: episode: 872, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 57.291, mean reward: 4.774 [3.413, 6.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.425, 10.100], loss: 6.360965, mae: 0.898554, mean_q: 5.650895
 48977/100000: episode: 873, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 40.926, mean reward: 3.721 [2.965, 4.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.591, 10.100], loss: 0.912323, mae: 0.836371, mean_q: 5.308489
 48986/100000: episode: 874, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 30.330, mean reward: 3.370 [2.700, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.434, 10.100], loss: 0.758406, mae: 0.761964, mean_q: 5.487354
 48992/100000: episode: 875, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 24.103, mean reward: 4.017 [3.576, 4.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.485, 10.100], loss: 0.658501, mae: 0.656047, mean_q: 4.741683
 49002/100000: episode: 876, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 39.431, mean reward: 3.943 [3.323, 4.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.371, 10.100], loss: 0.771752, mae: 0.801272, mean_q: 5.717894
 49010/100000: episode: 877, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 42.768, mean reward: 5.346 [3.825, 6.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.531, 10.100], loss: 0.739298, mae: 0.643587, mean_q: 5.271273
 49018/100000: episode: 878, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 37.318, mean reward: 4.665 [4.115, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.454, 10.100], loss: 0.377008, mae: 0.575739, mean_q: 5.496767
 49025/100000: episode: 879, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 21.316, mean reward: 3.045 [2.640, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.362, 10.100], loss: 0.355155, mae: 0.534525, mean_q: 5.187690
 49035/100000: episode: 880, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 31.641, mean reward: 3.164 [2.704, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.355, 10.100], loss: 1.053859, mae: 0.670304, mean_q: 5.478473
 49038/100000: episode: 881, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 21.360, mean reward: 7.120 [5.691, 8.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.545, 10.100], loss: 0.527336, mae: 0.651063, mean_q: 5.173610
 49049/100000: episode: 882, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 61.079, mean reward: 5.553 [3.992, 7.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.377, 10.100], loss: 0.607267, mae: 0.674022, mean_q: 5.536716
 49052/100000: episode: 883, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 20.292, mean reward: 6.764 [5.588, 8.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.606, 10.100], loss: 2.322982, mae: 0.713855, mean_q: 5.245721
 49062/100000: episode: 884, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 33.191, mean reward: 3.319 [3.016, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.336, 10.100], loss: 0.389052, mae: 0.580795, mean_q: 5.334222
 49072/100000: episode: 885, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 57.025, mean reward: 5.703 [3.834, 8.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.437, 10.100], loss: 7.939194, mae: 0.916780, mean_q: 5.645278
 49084/100000: episode: 886, duration: 0.087s, episode steps: 12, steps per second: 138, episode reward: 84.740, mean reward: 7.062 [4.505, 20.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.576, 10.100], loss: 0.708699, mae: 0.789066, mean_q: 5.514378
 49094/100000: episode: 887, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 59.449, mean reward: 5.945 [3.755, 8.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.389, 10.100], loss: 0.837537, mae: 0.723868, mean_q: 5.604386
 49107/100000: episode: 888, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 46.932, mean reward: 3.610 [2.814, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.326, 10.100], loss: 0.770184, mae: 0.689345, mean_q: 5.459630
 49113/100000: episode: 889, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 21.772, mean reward: 3.629 [3.241, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.358, 10.100], loss: 2.202443, mae: 0.934352, mean_q: 5.500120
[Info] 4-TH LEVEL FOUND: 8.9302339553833, Considering 10/90 traces
 49123/100000: episode: 890, duration: 4.165s, episode steps: 10, steps per second: 2, episode reward: 34.085, mean reward: 3.409 [2.874, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.388, 10.100], loss: 0.614127, mae: 0.724796, mean_q: 5.507050
 49130/100000: episode: 891, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 34.808, mean reward: 4.973 [4.312, 6.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.694, 10.100], loss: 0.917444, mae: 0.767354, mean_q: 5.630170
 49137/100000: episode: 892, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 32.287, mean reward: 4.612 [4.104, 5.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.525, 10.100], loss: 4.197346, mae: 1.118140, mean_q: 6.323143
 49141/100000: episode: 893, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 41.201, mean reward: 10.300 [9.815, 10.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.511, 10.100], loss: 0.813697, mae: 1.023478, mean_q: 6.509603
 49146/100000: episode: 894, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 28.018, mean reward: 5.604 [3.980, 6.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.489, 10.100], loss: 0.849078, mae: 0.827963, mean_q: 5.334625
 49151/100000: episode: 895, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 32.856, mean reward: 6.571 [5.339, 7.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.638, 10.100], loss: 0.513751, mae: 0.677364, mean_q: 5.157516
 49157/100000: episode: 896, duration: 0.045s, episode steps: 6, steps per second: 133, episode reward: 47.399, mean reward: 7.900 [4.719, 18.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.542, 10.100], loss: 1.608011, mae: 0.826959, mean_q: 5.925280
 49164/100000: episode: 897, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 33.214, mean reward: 4.745 [4.276, 5.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.424, 10.100], loss: 0.653279, mae: 0.689881, mean_q: 5.496569
 49170/100000: episode: 898, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 28.792, mean reward: 4.799 [3.023, 7.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.602, 10.100], loss: 0.609952, mae: 0.695828, mean_q: 5.401024
 49176/100000: episode: 899, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 61.779, mean reward: 10.297 [5.083, 29.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.707, 10.100], loss: 0.851033, mae: 0.849901, mean_q: 5.881145
 49182/100000: episode: 900, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 47.530, mean reward: 7.922 [5.227, 11.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.676, 10.100], loss: 1.422956, mae: 0.751222, mean_q: 5.518639
 49188/100000: episode: 901, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 42.031, mean reward: 7.005 [4.178, 10.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.885, 10.100], loss: 0.669061, mae: 0.736259, mean_q: 5.593573
 49194/100000: episode: 902, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 34.783, mean reward: 5.797 [4.785, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.592, 10.100], loss: 1.384421, mae: 0.730176, mean_q: 5.287111
 49200/100000: episode: 903, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 29.342, mean reward: 4.890 [4.186, 5.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.570, 10.100], loss: 0.671886, mae: 0.715327, mean_q: 5.420151
 49206/100000: episode: 904, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 45.549, mean reward: 7.592 [5.412, 8.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.697, 10.100], loss: 0.552997, mae: 0.736035, mean_q: 5.601257
 49212/100000: episode: 905, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 40.387, mean reward: 6.731 [5.587, 8.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.615, 10.100], loss: 0.560803, mae: 0.680382, mean_q: 5.523262
 49218/100000: episode: 906, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 58.555, mean reward: 9.759 [5.150, 16.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.563, 10.100], loss: 2.300581, mae: 1.031894, mean_q: 5.921519
 49222/100000: episode: 907, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 24.719, mean reward: 6.180 [5.525, 6.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.451, 10.100], loss: 1.141907, mae: 0.902634, mean_q: 5.892537
 49228/100000: episode: 908, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 19.666, mean reward: 3.278 [2.845, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.550, 10.100], loss: 13.458898, mae: 1.269218, mean_q: 5.970135
 49235/100000: episode: 909, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 55.867, mean reward: 7.981 [5.369, 15.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.619, 10.100], loss: 0.785150, mae: 0.839220, mean_q: 5.883927
 49241/100000: episode: 910, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 32.563, mean reward: 5.427 [3.831, 7.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.597, 10.100], loss: 6.881448, mae: 1.199187, mean_q: 5.245677
 49248/100000: episode: 911, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 40.413, mean reward: 5.773 [5.038, 6.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.463, 10.100], loss: 1.149058, mae: 1.141999, mean_q: 6.439663
 49252/100000: episode: 912, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 19.209, mean reward: 4.802 [4.103, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.513, 10.100], loss: 1.284926, mae: 0.836764, mean_q: 5.030946
 49256/100000: episode: 913, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 33.295, mean reward: 8.324 [6.190, 10.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.474, 10.100], loss: 9.192331, mae: 1.167831, mean_q: 5.210873
 49262/100000: episode: 914, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 20.703, mean reward: 3.451 [2.930, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.539, 10.100], loss: 0.771200, mae: 0.942511, mean_q: 6.294910
 49268/100000: episode: 915, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 65.518, mean reward: 10.920 [7.023, 14.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.508, 10.100], loss: 1.976848, mae: 0.953011, mean_q: 5.679054
 49274/100000: episode: 916, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 33.001, mean reward: 5.500 [4.341, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.491, 10.100], loss: 0.719043, mae: 0.754799, mean_q: 6.081525
 49279/100000: episode: 917, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 17.693, mean reward: 3.539 [3.313, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.562, 10.100], loss: 0.682074, mae: 0.753615, mean_q: 5.635116
 49286/100000: episode: 918, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 29.808, mean reward: 4.258 [3.517, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.579, 10.100], loss: 0.342517, mae: 0.582363, mean_q: 5.587136
 49292/100000: episode: 919, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 30.141, mean reward: 5.024 [3.977, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.516, 10.100], loss: 0.971579, mae: 0.703318, mean_q: 5.555410
 49296/100000: episode: 920, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 31.473, mean reward: 7.868 [4.936, 11.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.615, 10.100], loss: 2.136789, mae: 0.989276, mean_q: 5.766423
 49302/100000: episode: 921, duration: 0.042s, episode steps: 6, steps per second: 141, episode reward: 33.940, mean reward: 5.657 [3.635, 8.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.588, 10.100], loss: 0.693306, mae: 0.798647, mean_q: 5.921538
 49308/100000: episode: 922, duration: 0.052s, episode steps: 6, steps per second: 116, episode reward: 32.196, mean reward: 5.366 [5.021, 5.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.530, 10.100], loss: 1.439790, mae: 0.872182, mean_q: 5.642487
 49312/100000: episode: 923, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 34.495, mean reward: 8.624 [7.002, 9.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.606, 10.100], loss: 2.399997, mae: 0.890693, mean_q: 5.664872
 49316/100000: episode: 924, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 18.345, mean reward: 4.586 [4.327, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.460, 10.100], loss: 0.484143, mae: 0.662918, mean_q: 6.193590
 49321/100000: episode: 925, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 28.142, mean reward: 5.628 [4.336, 8.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.466, 10.100], loss: 1.307069, mae: 0.839058, mean_q: 5.457156
 49325/100000: episode: 926, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 21.321, mean reward: 5.330 [5.071, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.535, 10.100], loss: 1.064385, mae: 0.754331, mean_q: 5.893959
 49332/100000: episode: 927, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 44.304, mean reward: 6.329 [4.773, 7.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.550, 10.100], loss: 0.811808, mae: 0.724694, mean_q: 5.723264
 49338/100000: episode: 928, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 34.316, mean reward: 5.719 [4.537, 6.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.538, 10.100], loss: 0.699711, mae: 0.729601, mean_q: 5.670929
 49344/100000: episode: 929, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 18.411, mean reward: 3.068 [2.912, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.492, 10.100], loss: 1.150826, mae: 0.815578, mean_q: 5.892418
 49348/100000: episode: 930, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 36.906, mean reward: 9.227 [6.275, 15.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.502, 10.100], loss: 0.417456, mae: 0.672707, mean_q: 5.948544
 49354/100000: episode: 931, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 34.840, mean reward: 5.807 [3.559, 9.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.527, 10.100], loss: 1.262624, mae: 0.835365, mean_q: 5.738055
 49360/100000: episode: 932, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 39.921, mean reward: 6.653 [5.274, 7.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.618, 10.100], loss: 0.958244, mae: 0.815646, mean_q: 5.733953
 49366/100000: episode: 933, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 28.673, mean reward: 4.779 [4.052, 5.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.529, 10.100], loss: 0.995029, mae: 0.850771, mean_q: 6.341173
[Info] FALSIFICATION!
 49372/100000: episode: 934, duration: 0.219s, episode steps: 6, steps per second: 27, episode reward: 1042.618, mean reward: 173.770 [6.107, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.532, 10.098], loss: 0.902400, mae: 0.838072, mean_q: 5.794369
 49379/100000: episode: 935, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 47.134, mean reward: 6.733 [4.586, 8.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.586, 10.100], loss: 0.778273, mae: 0.816051, mean_q: 5.984284
 49383/100000: episode: 936, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 27.307, mean reward: 6.827 [6.256, 7.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.523, 10.100], loss: 2.371167, mae: 0.861943, mean_q: 5.714144
 49389/100000: episode: 937, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 48.630, mean reward: 8.105 [5.023, 13.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.549, 10.100], loss: 1.049501, mae: 0.798600, mean_q: 6.035532
 49395/100000: episode: 938, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 47.394, mean reward: 7.899 [5.269, 12.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.505, 10.100], loss: 0.789440, mae: 0.745505, mean_q: 5.993793
 49401/100000: episode: 939, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 41.471, mean reward: 6.912 [4.499, 11.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.556, 10.100], loss: 6.099335, mae: 0.979491, mean_q: 6.109222
 49406/100000: episode: 940, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 30.928, mean reward: 6.186 [5.396, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.687, 10.100], loss: 0.749126, mae: 0.788483, mean_q: 6.180792
 49410/100000: episode: 941, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 17.806, mean reward: 4.452 [4.204, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.559, 10.100], loss: 3832.768066, mae: 8.595405, mean_q: 5.958668
 49417/100000: episode: 942, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 39.082, mean reward: 5.583 [4.298, 6.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.497, 10.100], loss: 8.465887, mae: 3.362705, mean_q: 9.230299
 49422/100000: episode: 943, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 33.701, mean reward: 6.740 [3.920, 9.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.522, 10.100], loss: 1.677703, mae: 1.314243, mean_q: 6.174884
 49428/100000: episode: 944, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 35.944, mean reward: 5.991 [4.516, 7.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.480, 10.100], loss: 2.212469, mae: 1.620705, mean_q: 4.535238
 49433/100000: episode: 945, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 18.870, mean reward: 3.774 [3.392, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.514, 10.100], loss: 1.104306, mae: 1.054278, mean_q: 5.886929
 49439/100000: episode: 946, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 29.013, mean reward: 4.835 [3.721, 6.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.527, 10.100], loss: 1.728167, mae: 1.397979, mean_q: 6.962449
 49445/100000: episode: 947, duration: 0.044s, episode steps: 6, steps per second: 137, episode reward: 54.121, mean reward: 9.020 [6.311, 12.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.614, 10.100], loss: 0.961267, mae: 0.916721, mean_q: 6.066628
 49452/100000: episode: 948, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 44.130, mean reward: 6.304 [4.314, 9.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.535, 10.100], loss: 1.206596, mae: 0.982844, mean_q: 5.610494
 49458/100000: episode: 949, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 55.993, mean reward: 9.332 [4.727, 16.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.524, 10.100], loss: 1.473315, mae: 1.096269, mean_q: 6.517808
 49464/100000: episode: 950, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 21.988, mean reward: 3.665 [2.933, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.583, 10.100], loss: 1.121764, mae: 0.953563, mean_q: 6.238451
 49470/100000: episode: 951, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 36.715, mean reward: 6.119 [3.266, 9.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.964, 10.100], loss: 2.352745, mae: 1.019103, mean_q: 5.934074
 49477/100000: episode: 952, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 56.094, mean reward: 8.013 [6.664, 11.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.867, 10.100], loss: 1.775880, mae: 0.893746, mean_q: 6.172321
 49484/100000: episode: 953, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 50.797, mean reward: 7.257 [4.667, 10.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.467, 10.100], loss: 1.174767, mae: 0.922405, mean_q: 6.252765
 49488/100000: episode: 954, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 45.217, mean reward: 11.304 [6.896, 17.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.658, 10.100], loss: 0.603266, mae: 0.701559, mean_q: 5.965068
 49494/100000: episode: 955, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 29.399, mean reward: 4.900 [4.484, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.558, 10.100], loss: 1.092605, mae: 0.789045, mean_q: 5.942540
 49501/100000: episode: 956, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 42.573, mean reward: 6.082 [4.777, 7.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.605, 10.100], loss: 0.995889, mae: 0.900221, mean_q: 6.181628
 49505/100000: episode: 957, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 17.843, mean reward: 4.461 [4.149, 4.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.529, 10.100], loss: 0.921477, mae: 0.903795, mean_q: 6.480577
 49511/100000: episode: 958, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 31.660, mean reward: 5.277 [4.318, 6.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.401, 10.100], loss: 0.658483, mae: 0.737503, mean_q: 6.126102
 49515/100000: episode: 959, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 43.105, mean reward: 10.776 [7.542, 14.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.578, 10.100], loss: 1.123877, mae: 0.827189, mean_q: 6.018173
 49522/100000: episode: 960, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 25.041, mean reward: 3.577 [3.282, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.486, 10.100], loss: 1.624046, mae: 0.889098, mean_q: 6.088520
 49528/100000: episode: 961, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 34.800, mean reward: 5.800 [3.923, 8.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.520, 10.100], loss: 2557.615723, mae: 7.016029, mean_q: 7.294919
 49534/100000: episode: 962, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 1035.198, mean reward: 172.533 [4.052, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.581, 10.100], loss: 10.897922, mae: 4.052590, mean_q: 10.708722
 49540/100000: episode: 963, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 33.530, mean reward: 5.588 [4.285, 6.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.440, 10.100], loss: 9.736144, mae: 2.245829, mean_q: 5.372799
 49544/100000: episode: 964, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 45.460, mean reward: 11.365 [7.131, 15.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.565, 10.100], loss: 2.632992, mae: 1.946369, mean_q: 5.065210
 49549/100000: episode: 965, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 27.625, mean reward: 5.525 [4.531, 6.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.570, 10.100], loss: 2.701864, mae: 1.494281, mean_q: 6.929069
 49553/100000: episode: 966, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 29.463, mean reward: 7.366 [4.581, 10.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.495, 10.100], loss: 2.763114, mae: 1.570520, mean_q: 7.126970
 49559/100000: episode: 967, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 31.626, mean reward: 5.271 [3.369, 7.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.368, 10.100], loss: 1.839998, mae: 1.281772, mean_q: 6.188976
 49565/100000: episode: 968, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 48.863, mean reward: 8.144 [6.029, 9.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.556, 10.100], loss: 4.515753, mae: 1.316384, mean_q: 6.160980
 49571/100000: episode: 969, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 50.486, mean reward: 8.414 [5.170, 12.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.850, 10.100], loss: 1.775287, mae: 1.154654, mean_q: 6.725474
 49577/100000: episode: 970, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 34.968, mean reward: 5.828 [4.640, 7.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.602, 10.100], loss: 2.374830, mae: 1.151509, mean_q: 6.745559
 49583/100000: episode: 971, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 24.485, mean reward: 4.081 [3.918, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.484, 10.100], loss: 1.757547, mae: 1.050192, mean_q: 6.507439
 49589/100000: episode: 972, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 64.684, mean reward: 10.781 [4.893, 16.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.589, 10.100], loss: 1.646850, mae: 1.043894, mean_q: 6.520308
 49594/100000: episode: 973, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 37.400, mean reward: 7.480 [4.338, 11.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.702, 10.100], loss: 1.428917, mae: 1.055399, mean_q: 6.693276
 49600/100000: episode: 974, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 24.705, mean reward: 4.118 [3.565, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.544, 10.100], loss: 1.569141, mae: 1.107313, mean_q: 6.388840
 49604/100000: episode: 975, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 45.808, mean reward: 11.452 [6.524, 25.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.534, 10.100], loss: 1.404597, mae: 1.058664, mean_q: 6.718531
 49611/100000: episode: 976, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 28.556, mean reward: 4.079 [3.552, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.217, 10.100], loss: 1.086755, mae: 0.962042, mean_q: 6.669434
 49615/100000: episode: 977, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 25.201, mean reward: 6.300 [5.043, 6.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.428, 10.100], loss: 3.832463, mae: 1.300202, mean_q: 6.435193
 49621/100000: episode: 978, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 46.614, mean reward: 7.769 [5.648, 8.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.692, 10.100], loss: 1.588053, mae: 1.131731, mean_q: 6.894149
 49627/100000: episode: 979, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 28.885, mean reward: 4.814 [3.960, 6.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.593, 10.100], loss: 2549.981201, mae: 6.352566, mean_q: 7.028671
[Info] Complete ISplit Iteration
[Info] Levels: [5.392025, 6.0613337, 8.74037, 8.930234, 16.081572]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.64]
[Info] Error Prob: 6.400000000000002e-05

 49633/100000: episode: 980, duration: 4.445s, episode steps: 6, steps per second: 1, episode reward: 39.721, mean reward: 6.620 [4.172, 10.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.614, 10.100], loss: 9.948727, mae: 3.856882, mean_q: 10.397098
 49733/100000: episode: 981, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 215.923, mean reward: 2.159 [1.450, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.931, 10.098], loss: 2.970924, mae: 1.348053, mean_q: 6.701134
 49833/100000: episode: 982, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 199.992, mean reward: 2.000 [1.495, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.873, 10.381], loss: 2.636439, mae: 1.151894, mean_q: 6.560401
 49933/100000: episode: 983, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.564, mean reward: 1.936 [1.491, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.453, 10.147], loss: 311.960419, mae: 2.338109, mean_q: 7.335728
 50033/100000: episode: 984, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.335, mean reward: 1.893 [1.449, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.396], loss: 155.404419, mae: 1.664330, mean_q: 6.651679
 50133/100000: episode: 985, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 237.081, mean reward: 2.371 [1.537, 6.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.203, 10.098], loss: 157.033218, mae: 1.939629, mean_q: 7.308465
 50233/100000: episode: 986, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.815, mean reward: 1.838 [1.474, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.180, 10.098], loss: 613.319397, mae: 3.240561, mean_q: 8.119110
 50333/100000: episode: 987, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 208.988, mean reward: 2.090 [1.448, 5.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.808, 10.098], loss: 155.205902, mae: 1.677432, mean_q: 7.160470
 50433/100000: episode: 988, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.619, mean reward: 1.856 [1.465, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.914, 10.098], loss: 154.991226, mae: 1.671492, mean_q: 7.165640
 50533/100000: episode: 989, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 186.843, mean reward: 1.868 [1.459, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.640, 10.296], loss: 459.879303, mae: 2.542754, mean_q: 7.412582
 50633/100000: episode: 990, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 210.506, mean reward: 2.105 [1.460, 5.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.418, 10.405], loss: 3.720177, mae: 1.413400, mean_q: 6.989079
 50733/100000: episode: 991, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 232.821, mean reward: 2.328 [1.451, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.098], loss: 2.264913, mae: 1.035978, mean_q: 6.460606
 50833/100000: episode: 992, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 193.706, mean reward: 1.937 [1.460, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.686, 10.147], loss: 155.250076, mae: 1.472137, mean_q: 6.604183
 50933/100000: episode: 993, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 201.004, mean reward: 2.010 [1.438, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.697, 10.245], loss: 3.191081, mae: 1.267712, mean_q: 6.687818
 51033/100000: episode: 994, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.305, mean reward: 1.843 [1.455, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.331, 10.098], loss: 155.051376, mae: 1.576804, mean_q: 6.759391
 51133/100000: episode: 995, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 195.097, mean reward: 1.951 [1.496, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.782, 10.098], loss: 155.469742, mae: 1.655436, mean_q: 6.833238
 51233/100000: episode: 996, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.098, mean reward: 1.971 [1.489, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.964, 10.127], loss: 610.807678, mae: 3.059813, mean_q: 7.499578
 51333/100000: episode: 997, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.538, mean reward: 1.915 [1.523, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.444, 10.215], loss: 154.387222, mae: 1.706456, mean_q: 7.023581
 51433/100000: episode: 998, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.311, mean reward: 1.863 [1.452, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.381, 10.166], loss: 610.055481, mae: 3.431211, mean_q: 8.015042
 51533/100000: episode: 999, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 196.938, mean reward: 1.969 [1.447, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.344, 10.098], loss: 154.797607, mae: 1.746201, mean_q: 6.679702
 51633/100000: episode: 1000, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 178.088, mean reward: 1.781 [1.438, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.857, 10.098], loss: 1.674092, mae: 1.038972, mean_q: 6.407027
 51733/100000: episode: 1001, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 211.460, mean reward: 2.115 [1.456, 4.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.277], loss: 306.649170, mae: 2.214099, mean_q: 7.120453
 51833/100000: episode: 1002, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 178.728, mean reward: 1.787 [1.460, 2.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.098], loss: 305.698547, mae: 2.159457, mean_q: 7.203838
 51933/100000: episode: 1003, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.422, mean reward: 1.874 [1.450, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.755, 10.250], loss: 153.524139, mae: 1.491844, mean_q: 6.639677
 52033/100000: episode: 1004, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 203.259, mean reward: 2.033 [1.440, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.822, 10.098], loss: 156.518112, mae: 1.610557, mean_q: 6.617334
 52133/100000: episode: 1005, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 190.773, mean reward: 1.908 [1.438, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 3.622470, mae: 1.069707, mean_q: 6.201596
 52233/100000: episode: 1006, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.030, mean reward: 1.980 [1.441, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.489, 10.098], loss: 3.615551, mae: 0.999586, mean_q: 6.054178
 52333/100000: episode: 1007, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 219.587, mean reward: 2.196 [1.468, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.623, 10.098], loss: 308.201263, mae: 2.080059, mean_q: 6.773076
 52433/100000: episode: 1008, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.448, mean reward: 1.864 [1.462, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.599, 10.248], loss: 155.661743, mae: 1.454151, mean_q: 6.433287
 52533/100000: episode: 1009, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 198.347, mean reward: 1.983 [1.475, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.700, 10.205], loss: 307.189423, mae: 1.808077, mean_q: 6.383781
 52633/100000: episode: 1010, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 180.048, mean reward: 1.800 [1.459, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.991, 10.098], loss: 307.508911, mae: 1.868334, mean_q: 6.675766
 52733/100000: episode: 1011, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.401, mean reward: 1.904 [1.440, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.597, 10.306], loss: 3.233594, mae: 1.036556, mean_q: 6.151256
 52833/100000: episode: 1012, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.604, mean reward: 1.976 [1.457, 5.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.231, 10.138], loss: 2.232054, mae: 0.890343, mean_q: 5.771129
 52933/100000: episode: 1013, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.106, mean reward: 1.981 [1.451, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.378, 10.098], loss: 153.505081, mae: 1.244421, mean_q: 5.810722
 53033/100000: episode: 1014, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 196.955, mean reward: 1.970 [1.469, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.120], loss: 1.904227, mae: 0.940730, mean_q: 5.733189
 53133/100000: episode: 1015, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 213.805, mean reward: 2.138 [1.449, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.419, 10.098], loss: 154.377884, mae: 1.313358, mean_q: 5.847462
 53233/100000: episode: 1016, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.973, mean reward: 1.870 [1.448, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.178, 10.126], loss: 154.566742, mae: 1.265111, mean_q: 5.878602
 53333/100000: episode: 1017, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 215.467, mean reward: 2.155 [1.493, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.718, 10.383], loss: 457.454010, mae: 2.247308, mean_q: 6.285475
 53433/100000: episode: 1018, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.463, mean reward: 1.915 [1.456, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.156, 10.098], loss: 154.069458, mae: 1.405951, mean_q: 6.102048
 53533/100000: episode: 1019, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 189.593, mean reward: 1.896 [1.459, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.043, 10.239], loss: 1.882023, mae: 0.790568, mean_q: 5.403696
 53633/100000: episode: 1020, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.512, mean reward: 1.855 [1.482, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.434, 10.127], loss: 152.900253, mae: 1.048462, mean_q: 5.312440
 53733/100000: episode: 1021, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.681, mean reward: 1.837 [1.471, 2.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.038, 10.107], loss: 153.298767, mae: 1.166554, mean_q: 5.427734
 53833/100000: episode: 1022, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.542, mean reward: 1.845 [1.486, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.338, 10.169], loss: 153.448914, mae: 1.053458, mean_q: 5.235415
 53933/100000: episode: 1023, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.696, mean reward: 1.817 [1.453, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.098], loss: 456.924133, mae: 2.066901, mean_q: 5.826105
 54033/100000: episode: 1024, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 196.453, mean reward: 1.965 [1.488, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.764, 10.098], loss: 152.862915, mae: 1.157909, mean_q: 5.483908
 54133/100000: episode: 1025, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.310, mean reward: 1.843 [1.466, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.982, 10.098], loss: 152.090149, mae: 0.945524, mean_q: 4.929423
 54233/100000: episode: 1026, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.174, mean reward: 1.812 [1.472, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.214], loss: 303.752594, mae: 1.407764, mean_q: 5.017618
 54333/100000: episode: 1027, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.509, mean reward: 1.885 [1.515, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.324, 10.186], loss: 1.299250, mae: 0.639848, mean_q: 4.717399
 54433/100000: episode: 1028, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 177.259, mean reward: 1.773 [1.444, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.098], loss: 151.270569, mae: 0.882769, mean_q: 4.470973
 54533/100000: episode: 1029, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.968, mean reward: 1.880 [1.480, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.782, 10.098], loss: 0.330095, mae: 0.381491, mean_q: 4.040999
 54633/100000: episode: 1030, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 187.656, mean reward: 1.877 [1.451, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.123, 10.182], loss: 0.125894, mae: 0.333291, mean_q: 3.902164
 54733/100000: episode: 1031, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 198.958, mean reward: 1.990 [1.460, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.840, 10.098], loss: 0.125214, mae: 0.323513, mean_q: 3.854662
 54833/100000: episode: 1032, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 183.432, mean reward: 1.834 [1.440, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.071, 10.195], loss: 0.125199, mae: 0.323213, mean_q: 3.830687
 54933/100000: episode: 1033, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.857, mean reward: 1.929 [1.470, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.098], loss: 0.136393, mae: 0.326949, mean_q: 3.837337
 55033/100000: episode: 1034, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 202.140, mean reward: 2.021 [1.438, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.906, 10.200], loss: 0.131370, mae: 0.327758, mean_q: 3.847070
 55133/100000: episode: 1035, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.177, mean reward: 1.942 [1.490, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.013, 10.273], loss: 0.122113, mae: 0.323746, mean_q: 3.856802
 55233/100000: episode: 1036, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 217.326, mean reward: 2.173 [1.466, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.877, 10.109], loss: 0.116547, mae: 0.327609, mean_q: 3.865856
 55333/100000: episode: 1037, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.577, mean reward: 1.936 [1.469, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.835, 10.098], loss: 0.099409, mae: 0.310503, mean_q: 3.840481
 55433/100000: episode: 1038, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 213.371, mean reward: 2.134 [1.509, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.958, 10.329], loss: 0.101776, mae: 0.311329, mean_q: 3.839080
 55533/100000: episode: 1039, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 186.662, mean reward: 1.867 [1.479, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.264, 10.208], loss: 0.106657, mae: 0.315325, mean_q: 3.844600
 55633/100000: episode: 1040, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.924, mean reward: 1.879 [1.446, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.111], loss: 0.098451, mae: 0.304473, mean_q: 3.842587
 55733/100000: episode: 1041, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.713, mean reward: 1.867 [1.464, 6.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.858, 10.125], loss: 0.107301, mae: 0.308016, mean_q: 3.839424
 55833/100000: episode: 1042, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.601, mean reward: 1.916 [1.445, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.098], loss: 0.098819, mae: 0.307419, mean_q: 3.832227
 55933/100000: episode: 1043, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.079, mean reward: 1.891 [1.479, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.098], loss: 0.097839, mae: 0.296001, mean_q: 3.844942
 56033/100000: episode: 1044, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.664, mean reward: 1.887 [1.431, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.946, 10.200], loss: 0.099177, mae: 0.297067, mean_q: 3.819720
 56133/100000: episode: 1045, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 178.711, mean reward: 1.787 [1.467, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.425, 10.155], loss: 0.098942, mae: 0.296643, mean_q: 3.816374
 56233/100000: episode: 1046, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.654, mean reward: 1.807 [1.466, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.423, 10.137], loss: 0.089264, mae: 0.289882, mean_q: 3.812760
 56333/100000: episode: 1047, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.749, mean reward: 1.917 [1.483, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.098], loss: 0.102289, mae: 0.298090, mean_q: 3.803471
 56433/100000: episode: 1048, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.800, mean reward: 1.928 [1.482, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.502, 10.157], loss: 0.094422, mae: 0.298239, mean_q: 3.799667
 56533/100000: episode: 1049, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 177.535, mean reward: 1.775 [1.453, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.736, 10.151], loss: 0.082350, mae: 0.283627, mean_q: 3.805772
 56633/100000: episode: 1050, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 199.859, mean reward: 1.999 [1.461, 6.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.421, 10.098], loss: 0.085612, mae: 0.284274, mean_q: 3.799872
 56733/100000: episode: 1051, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.287, mean reward: 1.903 [1.448, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.420, 10.098], loss: 0.094293, mae: 0.291258, mean_q: 3.802783
 56833/100000: episode: 1052, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 180.789, mean reward: 1.808 [1.440, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.101, 10.276], loss: 0.088650, mae: 0.286797, mean_q: 3.799968
 56933/100000: episode: 1053, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 188.567, mean reward: 1.886 [1.437, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.734, 10.259], loss: 0.087406, mae: 0.289284, mean_q: 3.811026
 57033/100000: episode: 1054, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.641, mean reward: 1.856 [1.445, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.947, 10.098], loss: 0.090040, mae: 0.285535, mean_q: 3.793927
 57133/100000: episode: 1055, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 208.735, mean reward: 2.087 [1.462, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.114, 10.302], loss: 0.090716, mae: 0.291038, mean_q: 3.807564
 57233/100000: episode: 1056, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 191.404, mean reward: 1.914 [1.450, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.211], loss: 0.101341, mae: 0.302475, mean_q: 3.785508
 57333/100000: episode: 1057, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.727, mean reward: 1.887 [1.455, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.029, 10.098], loss: 0.088351, mae: 0.289512, mean_q: 3.786415
 57433/100000: episode: 1058, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 196.969, mean reward: 1.970 [1.504, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.863, 10.098], loss: 0.082807, mae: 0.278075, mean_q: 3.780080
 57533/100000: episode: 1059, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.314, mean reward: 1.873 [1.455, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.715, 10.245], loss: 0.090304, mae: 0.288890, mean_q: 3.794089
 57633/100000: episode: 1060, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 233.570, mean reward: 2.336 [1.493, 5.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.366, 10.403], loss: 0.084507, mae: 0.289276, mean_q: 3.787328
 57733/100000: episode: 1061, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 230.743, mean reward: 2.307 [1.492, 9.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.178, 10.098], loss: 0.096736, mae: 0.296274, mean_q: 3.810096
 57833/100000: episode: 1062, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 223.895, mean reward: 2.239 [1.508, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.266, 10.098], loss: 0.103520, mae: 0.295832, mean_q: 3.826984
 57933/100000: episode: 1063, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.325, mean reward: 1.953 [1.435, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.234, 10.397], loss: 0.105720, mae: 0.306241, mean_q: 3.822433
 58033/100000: episode: 1064, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.894, mean reward: 1.889 [1.448, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.593, 10.156], loss: 0.110280, mae: 0.307747, mean_q: 3.831486
 58133/100000: episode: 1065, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 183.529, mean reward: 1.835 [1.473, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.284, 10.201], loss: 0.101759, mae: 0.308997, mean_q: 3.826999
 58233/100000: episode: 1066, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.549, mean reward: 1.915 [1.476, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.148, 10.245], loss: 0.107500, mae: 0.312135, mean_q: 3.836164
 58333/100000: episode: 1067, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 194.061, mean reward: 1.941 [1.461, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.902, 10.098], loss: 0.097427, mae: 0.297963, mean_q: 3.790714
 58433/100000: episode: 1068, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 184.645, mean reward: 1.846 [1.463, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.463, 10.131], loss: 0.099859, mae: 0.300990, mean_q: 3.827271
 58533/100000: episode: 1069, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 196.184, mean reward: 1.962 [1.459, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.585, 10.207], loss: 0.085696, mae: 0.283288, mean_q: 3.797464
 58633/100000: episode: 1070, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 180.386, mean reward: 1.804 [1.470, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.695, 10.135], loss: 0.083817, mae: 0.282313, mean_q: 3.807390
 58733/100000: episode: 1071, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.675, mean reward: 1.937 [1.497, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.395, 10.098], loss: 0.093283, mae: 0.292552, mean_q: 3.811722
 58833/100000: episode: 1072, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.894, mean reward: 1.829 [1.452, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.825, 10.098], loss: 0.092616, mae: 0.293239, mean_q: 3.810998
 58933/100000: episode: 1073, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.750, mean reward: 1.937 [1.455, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.264], loss: 0.086323, mae: 0.291718, mean_q: 3.814762
 59033/100000: episode: 1074, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.328, mean reward: 2.033 [1.460, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.076, 10.098], loss: 0.098042, mae: 0.305340, mean_q: 3.829674
 59133/100000: episode: 1075, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 189.352, mean reward: 1.894 [1.474, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.263], loss: 0.092599, mae: 0.292965, mean_q: 3.799098
 59233/100000: episode: 1076, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.953, mean reward: 1.910 [1.460, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.857, 10.098], loss: 0.109590, mae: 0.301135, mean_q: 3.821167
 59333/100000: episode: 1077, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 198.347, mean reward: 1.983 [1.466, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.179], loss: 0.103054, mae: 0.304119, mean_q: 3.808563
 59433/100000: episode: 1078, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 203.962, mean reward: 2.040 [1.442, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.111], loss: 0.091769, mae: 0.294266, mean_q: 3.819095
 59533/100000: episode: 1079, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 191.390, mean reward: 1.914 [1.454, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.183], loss: 0.094229, mae: 0.301445, mean_q: 3.843664
[Info] 1-TH LEVEL FOUND: 5.213897705078125, Considering 10/90 traces
 59633/100000: episode: 1080, duration: 4.660s, episode steps: 100, steps per second: 21, episode reward: 200.842, mean reward: 2.008 [1.483, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.422, 10.098], loss: 0.102976, mae: 0.307150, mean_q: 3.853878
 59645/100000: episode: 1081, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 25.879, mean reward: 2.157 [1.888, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.301, 10.100], loss: 0.084842, mae: 0.294206, mean_q: 3.852681
 59657/100000: episode: 1082, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 26.234, mean reward: 2.186 [1.814, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.337, 10.100], loss: 0.082241, mae: 0.273032, mean_q: 3.781261
 59748/100000: episode: 1083, duration: 0.459s, episode steps: 91, steps per second: 198, episode reward: 178.946, mean reward: 1.966 [1.488, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-1.042, 10.134], loss: 0.106671, mae: 0.312494, mean_q: 3.844007
 59772/100000: episode: 1084, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 52.703, mean reward: 2.196 [1.881, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.412, 10.100], loss: 0.082006, mae: 0.283574, mean_q: 3.787406
 59803/100000: episode: 1085, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 79.257, mean reward: 2.557 [2.094, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.268, 10.100], loss: 0.112506, mae: 0.321872, mean_q: 3.833617
 59814/100000: episode: 1086, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 31.967, mean reward: 2.906 [2.302, 4.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.776, 10.100], loss: 0.091742, mae: 0.306114, mean_q: 3.872666
 59825/100000: episode: 1087, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 26.890, mean reward: 2.445 [1.898, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.121, 10.100], loss: 0.106449, mae: 0.315451, mean_q: 3.840042
 59853/100000: episode: 1088, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 61.133, mean reward: 2.183 [1.646, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.025, 10.100], loss: 0.125709, mae: 0.332793, mean_q: 3.867746
 59865/100000: episode: 1089, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 33.551, mean reward: 2.796 [1.921, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.545, 10.100], loss: 0.091940, mae: 0.321617, mean_q: 3.886893
 59956/100000: episode: 1090, duration: 0.474s, episode steps: 91, steps per second: 192, episode reward: 165.030, mean reward: 1.814 [1.433, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-0.373, 10.191], loss: 0.109881, mae: 0.323044, mean_q: 3.882658
 59987/100000: episode: 1091, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 67.277, mean reward: 2.170 [1.523, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.257, 10.278], loss: 0.120195, mae: 0.338760, mean_q: 3.880241
 60004/100000: episode: 1092, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 40.140, mean reward: 2.361 [1.838, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.780, 10.100], loss: 0.097610, mae: 0.313600, mean_q: 3.851702
 60016/100000: episode: 1093, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 23.332, mean reward: 1.944 [1.692, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.310, 10.100], loss: 0.121367, mae: 0.333095, mean_q: 3.925093
 60040/100000: episode: 1094, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 59.501, mean reward: 2.479 [1.916, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.732, 10.100], loss: 0.121560, mae: 0.323169, mean_q: 3.872370
 60066/100000: episode: 1095, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 56.407, mean reward: 2.170 [1.497, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.171, 10.100], loss: 0.122195, mae: 0.323477, mean_q: 3.893460
 60078/100000: episode: 1096, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 21.531, mean reward: 1.794 [1.611, 2.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.132, 10.100], loss: 0.098658, mae: 0.301636, mean_q: 3.811297
 60102/100000: episode: 1097, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 79.036, mean reward: 3.293 [1.778, 6.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.315, 10.100], loss: 0.140716, mae: 0.326260, mean_q: 3.887660
 60130/100000: episode: 1098, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 58.598, mean reward: 2.093 [1.661, 2.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.541, 10.100], loss: 0.126204, mae: 0.333641, mean_q: 3.905874
 60142/100000: episode: 1099, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 31.049, mean reward: 2.587 [2.190, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.617, 10.100], loss: 0.132029, mae: 0.363725, mean_q: 3.950104
 60153/100000: episode: 1100, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 37.763, mean reward: 3.433 [2.921, 5.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.664, 10.100], loss: 0.109290, mae: 0.317501, mean_q: 3.898937
 60179/100000: episode: 1101, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 57.229, mean reward: 2.201 [1.577, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.320, 10.100], loss: 0.085215, mae: 0.291667, mean_q: 3.863627
 60212/100000: episode: 1102, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 130.914, mean reward: 3.967 [1.577, 7.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.500, 10.100], loss: 0.122792, mae: 0.325765, mean_q: 3.872082
 60240/100000: episode: 1103, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 70.312, mean reward: 2.511 [2.030, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.211, 10.100], loss: 0.145615, mae: 0.344140, mean_q: 3.922582
 60268/100000: episode: 1104, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 112.701, mean reward: 4.025 [2.572, 8.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.150, 10.100], loss: 0.103742, mae: 0.312973, mean_q: 3.920265
 60280/100000: episode: 1105, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 31.425, mean reward: 2.619 [1.710, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.290, 10.100], loss: 0.205149, mae: 0.384518, mean_q: 3.965777
 60308/100000: episode: 1106, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 83.600, mean reward: 2.986 [1.930, 8.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.077, 10.100], loss: 0.119263, mae: 0.342313, mean_q: 3.949141
 60332/100000: episode: 1107, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 46.454, mean reward: 1.936 [1.637, 2.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.220, 10.100], loss: 0.152596, mae: 0.358391, mean_q: 3.966929
 60344/100000: episode: 1108, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 29.011, mean reward: 2.418 [1.863, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.389, 10.100], loss: 0.132035, mae: 0.343674, mean_q: 3.927228
 60356/100000: episode: 1109, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 24.723, mean reward: 2.060 [1.857, 2.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.434, 10.100], loss: 0.140289, mae: 0.357173, mean_q: 4.038177
 60447/100000: episode: 1110, duration: 0.469s, episode steps: 91, steps per second: 194, episode reward: 179.494, mean reward: 1.972 [1.465, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-1.585, 10.259], loss: 0.120507, mae: 0.331206, mean_q: 3.902301
 60464/100000: episode: 1111, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 42.468, mean reward: 2.498 [1.973, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.339, 10.100], loss: 0.124479, mae: 0.355393, mean_q: 3.939361
 60481/100000: episode: 1112, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 35.208, mean reward: 2.071 [1.684, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.267, 10.100], loss: 0.115088, mae: 0.332447, mean_q: 3.907512
 60572/100000: episode: 1113, duration: 0.471s, episode steps: 91, steps per second: 193, episode reward: 226.417, mean reward: 2.488 [1.455, 7.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.633, 10.446], loss: 0.156249, mae: 0.361668, mean_q: 3.955326
 60663/100000: episode: 1114, duration: 0.443s, episode steps: 91, steps per second: 206, episode reward: 182.359, mean reward: 2.004 [1.453, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.701, 10.430], loss: 0.125153, mae: 0.340677, mean_q: 3.954135
 60687/100000: episode: 1115, duration: 0.138s, episode steps: 24, steps per second: 173, episode reward: 51.641, mean reward: 2.152 [1.561, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.152, 10.100], loss: 0.147274, mae: 0.345821, mean_q: 3.966229
 60711/100000: episode: 1116, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 55.622, mean reward: 2.318 [1.755, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.219, 10.100], loss: 0.161461, mae: 0.355039, mean_q: 3.978335
 60737/100000: episode: 1117, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 83.792, mean reward: 3.223 [2.009, 5.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.435, 10.100], loss: 0.133958, mae: 0.346542, mean_q: 3.942326
 60768/100000: episode: 1118, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 81.220, mean reward: 2.620 [2.103, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.253, 10.100], loss: 0.115739, mae: 0.331917, mean_q: 3.925062
 60785/100000: episode: 1119, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 36.055, mean reward: 2.121 [1.796, 2.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.182, 10.100], loss: 0.170861, mae: 0.386198, mean_q: 3.976482
 60797/100000: episode: 1120, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 23.781, mean reward: 1.982 [1.706, 2.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.298, 10.100], loss: 0.145118, mae: 0.381260, mean_q: 3.959704
 60809/100000: episode: 1121, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 23.650, mean reward: 1.971 [1.538, 2.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.357, 10.100], loss: 0.167203, mae: 0.354043, mean_q: 4.000480
 60837/100000: episode: 1122, duration: 0.135s, episode steps: 28, steps per second: 207, episode reward: 69.628, mean reward: 2.487 [1.546, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.333, 10.100], loss: 0.160225, mae: 0.369674, mean_q: 4.003945
 60861/100000: episode: 1123, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 52.862, mean reward: 2.203 [1.635, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.304, 10.100], loss: 0.119717, mae: 0.336462, mean_q: 3.953897
 60873/100000: episode: 1124, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 29.883, mean reward: 2.490 [1.642, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.348, 10.100], loss: 0.178967, mae: 0.377580, mean_q: 4.015758
 60901/100000: episode: 1125, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 86.522, mean reward: 3.090 [2.375, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.461, 10.100], loss: 0.177331, mae: 0.375622, mean_q: 3.992538
 60927/100000: episode: 1126, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 83.785, mean reward: 3.222 [2.138, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.405, 10.100], loss: 0.134849, mae: 0.362752, mean_q: 4.039531
 60960/100000: episode: 1127, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 92.082, mean reward: 2.790 [1.587, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.035, 10.170], loss: 0.161868, mae: 0.370038, mean_q: 4.043922
 60971/100000: episode: 1128, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 24.730, mean reward: 2.248 [1.920, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.082, 10.100], loss: 0.190601, mae: 0.364944, mean_q: 3.924443
 61004/100000: episode: 1129, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 98.185, mean reward: 2.975 [2.230, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.530, 10.100], loss: 0.154994, mae: 0.389453, mean_q: 4.062893
 61016/100000: episode: 1130, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 37.312, mean reward: 3.109 [2.062, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.626, 10.100], loss: 0.188354, mae: 0.400876, mean_q: 4.195638
 61042/100000: episode: 1131, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 59.296, mean reward: 2.281 [1.650, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.793, 10.100], loss: 0.157347, mae: 0.384955, mean_q: 4.100190
 61059/100000: episode: 1132, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 34.827, mean reward: 2.049 [1.639, 2.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.195, 10.100], loss: 0.161802, mae: 0.372540, mean_q: 4.106912
 61087/100000: episode: 1133, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 83.897, mean reward: 2.996 [2.178, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.303, 10.100], loss: 0.159112, mae: 0.368998, mean_q: 4.099777
 61099/100000: episode: 1134, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 29.078, mean reward: 2.423 [2.039, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.348, 10.100], loss: 0.177452, mae: 0.395776, mean_q: 4.098364
 61132/100000: episode: 1135, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 75.293, mean reward: 2.282 [1.770, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.166, 10.100], loss: 0.215088, mae: 0.406134, mean_q: 4.089671
 61165/100000: episode: 1136, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 76.270, mean reward: 2.311 [1.604, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.154, 10.116], loss: 0.183209, mae: 0.398074, mean_q: 4.152921
 61198/100000: episode: 1137, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 102.138, mean reward: 3.095 [1.914, 6.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.184, 10.100], loss: 0.151331, mae: 0.382621, mean_q: 4.108915
 61222/100000: episode: 1138, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 52.513, mean reward: 2.188 [1.718, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.301, 10.100], loss: 0.156140, mae: 0.351876, mean_q: 4.110273
 61250/100000: episode: 1139, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 77.535, mean reward: 2.769 [2.111, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.911, 10.100], loss: 0.156355, mae: 0.367885, mean_q: 4.098909
 61278/100000: episode: 1140, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 77.022, mean reward: 2.751 [1.920, 5.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.625, 10.100], loss: 0.183384, mae: 0.413888, mean_q: 4.158305
 61304/100000: episode: 1141, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 79.177, mean reward: 3.045 [1.965, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.359, 10.100], loss: 0.200943, mae: 0.410831, mean_q: 4.239350
 61332/100000: episode: 1142, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 79.788, mean reward: 2.850 [1.977, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.222, 10.100], loss: 0.188866, mae: 0.408344, mean_q: 4.147151
 61349/100000: episode: 1143, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 36.156, mean reward: 2.127 [1.546, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.368, 10.104], loss: 0.126866, mae: 0.374785, mean_q: 4.154122
 61366/100000: episode: 1144, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 36.242, mean reward: 2.132 [1.850, 2.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.173, 10.100], loss: 0.118940, mae: 0.345734, mean_q: 4.162078
 61399/100000: episode: 1145, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 97.112, mean reward: 2.943 [2.081, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.392, 10.100], loss: 0.189662, mae: 0.396002, mean_q: 4.158055
 61416/100000: episode: 1146, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 37.137, mean reward: 2.185 [1.742, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.184, 10.100], loss: 0.162885, mae: 0.387638, mean_q: 4.163995
 61428/100000: episode: 1147, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 26.437, mean reward: 2.203 [1.820, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.079, 10.100], loss: 0.156976, mae: 0.384631, mean_q: 4.177444
 61452/100000: episode: 1148, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 51.360, mean reward: 2.140 [1.652, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.115, 10.100], loss: 0.134450, mae: 0.370494, mean_q: 4.184122
 61469/100000: episode: 1149, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 37.421, mean reward: 2.201 [1.731, 2.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.466, 10.100], loss: 0.151073, mae: 0.395600, mean_q: 4.197722
 61481/100000: episode: 1150, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 26.525, mean reward: 2.210 [1.559, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.174, 10.100], loss: 0.187731, mae: 0.425760, mean_q: 4.240780
 61505/100000: episode: 1151, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 74.509, mean reward: 3.105 [1.931, 5.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.211, 10.100], loss: 0.188946, mae: 0.387974, mean_q: 4.137321
 61522/100000: episode: 1152, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 54.950, mean reward: 3.232 [2.108, 6.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.790, 10.100], loss: 0.196499, mae: 0.414371, mean_q: 4.234285
 61550/100000: episode: 1153, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 108.881, mean reward: 3.889 [2.413, 6.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.742, 10.100], loss: 0.225400, mae: 0.446150, mean_q: 4.257281
 61576/100000: episode: 1154, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 84.996, mean reward: 3.269 [1.990, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.363, 10.100], loss: 0.178174, mae: 0.424232, mean_q: 4.308307
 61667/100000: episode: 1155, duration: 0.478s, episode steps: 91, steps per second: 190, episode reward: 172.346, mean reward: 1.894 [1.454, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-1.067, 10.100], loss: 0.161264, mae: 0.397499, mean_q: 4.239983
 61693/100000: episode: 1156, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 69.667, mean reward: 2.680 [1.701, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.223, 10.100], loss: 0.144427, mae: 0.364933, mean_q: 4.214476
 61710/100000: episode: 1157, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 33.364, mean reward: 1.963 [1.580, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.255, 10.100], loss: 0.148668, mae: 0.375113, mean_q: 4.153262
 61741/100000: episode: 1158, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 65.241, mean reward: 2.105 [1.529, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.887, 10.137], loss: 0.205730, mae: 0.409433, mean_q: 4.267663
 61774/100000: episode: 1159, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 91.793, mean reward: 2.782 [2.239, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.456, 10.100], loss: 0.160769, mae: 0.396273, mean_q: 4.195334
 61798/100000: episode: 1160, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 61.895, mean reward: 2.579 [1.770, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.958, 10.100], loss: 0.168378, mae: 0.390227, mean_q: 4.261846
 61826/100000: episode: 1161, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 70.620, mean reward: 2.522 [1.858, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.293, 10.100], loss: 0.187204, mae: 0.418970, mean_q: 4.296253
 61859/100000: episode: 1162, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 104.101, mean reward: 3.155 [1.991, 13.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.280, 10.100], loss: 0.239933, mae: 0.448329, mean_q: 4.345862
 61871/100000: episode: 1163, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 33.673, mean reward: 2.806 [1.521, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.384, 10.100], loss: 0.219359, mae: 0.431565, mean_q: 4.416110
 61883/100000: episode: 1164, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 35.036, mean reward: 2.920 [2.112, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.277, 10.100], loss: 0.202816, mae: 0.456125, mean_q: 4.368456
 61909/100000: episode: 1165, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 68.164, mean reward: 2.622 [2.028, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.529, 10.100], loss: 0.161969, mae: 0.398467, mean_q: 4.247100
 61940/100000: episode: 1166, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 95.713, mean reward: 3.088 [1.887, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.349, 10.100], loss: 0.190451, mae: 0.416827, mean_q: 4.308251
 61951/100000: episode: 1167, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 26.735, mean reward: 2.430 [2.119, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.121, 10.100], loss: 0.150564, mae: 0.382964, mean_q: 4.371873
 61963/100000: episode: 1168, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 28.118, mean reward: 2.343 [1.679, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.345, 10.100], loss: 0.220954, mae: 0.447390, mean_q: 4.340842
 61989/100000: episode: 1169, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 56.281, mean reward: 2.165 [1.647, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.191, 10.100], loss: 0.186105, mae: 0.418906, mean_q: 4.294747
[Info] 2-TH LEVEL FOUND: 7.025600910186768, Considering 10/90 traces
 62020/100000: episode: 1170, duration: 4.245s, episode steps: 31, steps per second: 7, episode reward: 78.766, mean reward: 2.541 [2.112, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.957, 10.100], loss: 0.193578, mae: 0.429928, mean_q: 4.288523
 62035/100000: episode: 1171, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 56.664, mean reward: 3.778 [2.565, 6.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.250, 10.100], loss: 0.208244, mae: 0.417606, mean_q: 4.347707
 62052/100000: episode: 1172, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 64.481, mean reward: 3.793 [2.965, 5.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.288, 10.100], loss: 0.154480, mae: 0.398722, mean_q: 4.372258
 62064/100000: episode: 1173, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 32.051, mean reward: 2.671 [1.912, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.362, 10.100], loss: 0.359378, mae: 0.459230, mean_q: 4.363904
 62076/100000: episode: 1174, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 37.144, mean reward: 3.095 [2.044, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.296, 10.100], loss: 0.178876, mae: 0.421722, mean_q: 4.358262
 62088/100000: episode: 1175, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 33.149, mean reward: 2.762 [1.984, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.845, 10.100], loss: 0.367781, mae: 0.453427, mean_q: 4.412001
 62108/100000: episode: 1176, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 65.438, mean reward: 3.272 [2.622, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.365, 10.100], loss: 0.267724, mae: 0.437494, mean_q: 4.429880
 62129/100000: episode: 1177, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 150.373, mean reward: 7.161 [2.489, 18.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.436, 10.100], loss: 0.214192, mae: 0.425142, mean_q: 4.358364
 62141/100000: episode: 1178, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 27.765, mean reward: 2.314 [1.790, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.441, 10.100], loss: 0.261119, mae: 0.391409, mean_q: 4.399905
 62145/100000: episode: 1179, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 20.503, mean reward: 5.126 [4.402, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.525, 10.100], loss: 0.311737, mae: 0.487042, mean_q: 4.452820
 62165/100000: episode: 1180, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 71.649, mean reward: 3.582 [2.304, 7.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.751, 10.100], loss: 0.220345, mae: 0.456284, mean_q: 4.414725
 62169/100000: episode: 1181, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 16.982, mean reward: 4.245 [3.375, 5.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.496, 10.100], loss: 0.287661, mae: 0.445791, mean_q: 4.374765
 62194/100000: episode: 1182, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 93.725, mean reward: 3.749 [2.591, 5.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.507, 10.100], loss: 0.253194, mae: 0.448429, mean_q: 4.440583
 62208/100000: episode: 1183, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 53.020, mean reward: 3.787 [2.510, 4.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.110, 10.100], loss: 0.205202, mae: 0.425278, mean_q: 4.345376
 62223/100000: episode: 1184, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 65.302, mean reward: 4.353 [3.240, 8.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.511, 10.100], loss: 0.319395, mae: 0.469603, mean_q: 4.406124
 62227/100000: episode: 1185, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 17.133, mean reward: 4.283 [3.727, 4.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.494, 10.100], loss: 0.224883, mae: 0.490708, mean_q: 4.577402
 62248/100000: episode: 1186, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 64.075, mean reward: 3.051 [2.522, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.387, 10.100], loss: 0.267304, mae: 0.473421, mean_q: 4.475622
 62273/100000: episode: 1187, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 110.424, mean reward: 4.417 [2.601, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.023, 10.100], loss: 0.251784, mae: 0.457452, mean_q: 4.448212
 62294/100000: episode: 1188, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 69.477, mean reward: 3.308 [2.665, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.225, 10.100], loss: 0.318596, mae: 0.516321, mean_q: 4.438111
 62306/100000: episode: 1189, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 31.860, mean reward: 2.655 [2.395, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.304, 10.100], loss: 0.248566, mae: 0.484769, mean_q: 4.725075
 62320/100000: episode: 1190, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 52.047, mean reward: 3.718 [2.832, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.467, 10.100], loss: 0.247507, mae: 0.470773, mean_q: 4.425471
 62332/100000: episode: 1191, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 42.899, mean reward: 3.575 [2.619, 5.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.527, 10.100], loss: 0.249081, mae: 0.484914, mean_q: 4.588650
 62346/100000: episode: 1192, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 43.015, mean reward: 3.073 [2.515, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.263, 10.100], loss: 0.282200, mae: 0.481930, mean_q: 4.630428
 62350/100000: episode: 1193, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 16.077, mean reward: 4.019 [3.394, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.530, 10.100], loss: 0.235394, mae: 0.456677, mean_q: 4.455536
 62365/100000: episode: 1194, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 71.617, mean reward: 4.774 [3.321, 8.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.333, 10.100], loss: 0.357998, mae: 0.509766, mean_q: 4.599377
 62385/100000: episode: 1195, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 68.698, mean reward: 3.435 [2.554, 5.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.347, 10.100], loss: 0.233994, mae: 0.465814, mean_q: 4.598027
 62397/100000: episode: 1196, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 25.242, mean reward: 2.104 [1.891, 2.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.349, 10.100], loss: 0.364429, mae: 0.509098, mean_q: 4.622387
 62409/100000: episode: 1197, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 36.841, mean reward: 3.070 [2.240, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.459, 10.100], loss: 0.223355, mae: 0.449202, mean_q: 4.481937
 62434/100000: episode: 1198, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 84.782, mean reward: 3.391 [2.380, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.795, 10.100], loss: 0.343536, mae: 0.477759, mean_q: 4.543465
 62448/100000: episode: 1199, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 44.100, mean reward: 3.150 [2.133, 5.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.501, 10.100], loss: 0.191353, mae: 0.436490, mean_q: 4.544320
 62465/100000: episode: 1200, duration: 0.087s, episode steps: 17, steps per second: 194, episode reward: 48.443, mean reward: 2.850 [2.277, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.245, 10.100], loss: 0.277972, mae: 0.466494, mean_q: 4.571088
 62485/100000: episode: 1201, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 85.750, mean reward: 4.288 [2.784, 7.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.672, 10.100], loss: 0.267440, mae: 0.468627, mean_q: 4.644063
 62506/100000: episode: 1202, duration: 0.107s, episode steps: 21, steps per second: 195, episode reward: 61.834, mean reward: 2.944 [2.170, 5.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.558, 10.100], loss: 0.248713, mae: 0.450499, mean_q: 4.643375
 62518/100000: episode: 1203, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 40.032, mean reward: 3.336 [2.253, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.372, 10.100], loss: 0.368314, mae: 0.555249, mean_q: 4.740611
 62533/100000: episode: 1204, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 61.687, mean reward: 4.112 [2.985, 6.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.297, 10.100], loss: 0.180402, mae: 0.410897, mean_q: 4.518046
 62545/100000: episode: 1205, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 29.731, mean reward: 2.478 [1.930, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.624, 10.100], loss: 0.451349, mae: 0.539033, mean_q: 4.803609
 62549/100000: episode: 1206, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 23.866, mean reward: 5.967 [4.798, 8.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.379, 10.100], loss: 0.206021, mae: 0.459305, mean_q: 4.397602
 62563/100000: episode: 1207, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 46.358, mean reward: 3.311 [2.697, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.471, 10.100], loss: 0.247354, mae: 0.473693, mean_q: 4.474924
 62575/100000: episode: 1208, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 44.221, mean reward: 3.685 [3.201, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.402, 10.100], loss: 0.288728, mae: 0.496916, mean_q: 4.752547
 62592/100000: episode: 1209, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 67.491, mean reward: 3.970 [3.338, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.411, 10.100], loss: 0.266068, mae: 0.473577, mean_q: 4.592724
 62606/100000: episode: 1210, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 59.723, mean reward: 4.266 [3.420, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.988, 10.100], loss: 0.216656, mae: 0.459996, mean_q: 4.622356
 62610/100000: episode: 1211, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 14.840, mean reward: 3.710 [3.187, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.482, 10.100], loss: 0.194129, mae: 0.478698, mean_q: 4.625361
 62625/100000: episode: 1212, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 49.527, mean reward: 3.302 [2.364, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.214, 10.100], loss: 0.229474, mae: 0.455264, mean_q: 4.664850
 62645/100000: episode: 1213, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 57.526, mean reward: 2.876 [1.835, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.284, 10.100], loss: 0.243575, mae: 0.472397, mean_q: 4.678856
 62660/100000: episode: 1214, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 68.778, mean reward: 4.585 [2.858, 8.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.341, 10.100], loss: 0.332342, mae: 0.461631, mean_q: 4.614293
 62674/100000: episode: 1215, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 45.799, mean reward: 3.271 [2.170, 9.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.262, 10.100], loss: 0.266011, mae: 0.475157, mean_q: 4.631269
 62686/100000: episode: 1216, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 32.562, mean reward: 2.713 [2.231, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.204, 10.100], loss: 0.351316, mae: 0.531979, mean_q: 4.749099
 62706/100000: episode: 1217, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 103.485, mean reward: 5.174 [2.474, 22.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.700, 10.100], loss: 0.266305, mae: 0.504635, mean_q: 4.717690
 62731/100000: episode: 1218, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 111.224, mean reward: 4.449 [2.590, 6.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.508, 10.100], loss: 0.343392, mae: 0.531292, mean_q: 4.718164
 62752/100000: episode: 1219, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 69.107, mean reward: 3.291 [2.224, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.503, 10.100], loss: 0.343958, mae: 0.503003, mean_q: 4.581402
 62767/100000: episode: 1220, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 54.573, mean reward: 3.638 [2.429, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.179, 10.100], loss: 0.359003, mae: 0.572732, mean_q: 4.809282
 62779/100000: episode: 1221, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 37.878, mean reward: 3.156 [2.295, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.139, 10.100], loss: 0.252834, mae: 0.490500, mean_q: 4.788047
 62804/100000: episode: 1222, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 97.085, mean reward: 3.883 [2.070, 9.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.493, 10.100], loss: 0.309722, mae: 0.477807, mean_q: 4.759124
 62819/100000: episode: 1223, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 60.572, mean reward: 4.038 [2.652, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.459, 10.100], loss: 0.248741, mae: 0.471628, mean_q: 4.781628
 62834/100000: episode: 1224, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 70.440, mean reward: 4.696 [2.969, 9.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.669, 10.100], loss: 0.257767, mae: 0.474332, mean_q: 4.857694
 62848/100000: episode: 1225, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 51.834, mean reward: 3.702 [2.577, 10.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.395, 10.100], loss: 0.214771, mae: 0.468434, mean_q: 4.708315
 62863/100000: episode: 1226, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 47.366, mean reward: 3.158 [2.526, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.448, 10.100], loss: 0.317312, mae: 0.515742, mean_q: 4.785805
 62884/100000: episode: 1227, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 51.463, mean reward: 2.451 [1.636, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.188, 10.100], loss: 0.267675, mae: 0.474312, mean_q: 4.773095
 62888/100000: episode: 1228, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 24.544, mean reward: 6.136 [5.269, 7.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.367, 10.100], loss: 0.326263, mae: 0.532620, mean_q: 4.645645
 62903/100000: episode: 1229, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 42.322, mean reward: 2.821 [2.285, 4.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.274, 10.100], loss: 0.339486, mae: 0.545981, mean_q: 4.828475
 62918/100000: episode: 1230, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 83.865, mean reward: 5.591 [3.281, 8.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.703, 10.100], loss: 0.325936, mae: 0.558177, mean_q: 4.857914
 62939/100000: episode: 1231, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 62.762, mean reward: 2.989 [2.587, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.353, 10.100], loss: 0.278917, mae: 0.499882, mean_q: 4.858467
 62951/100000: episode: 1232, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 39.084, mean reward: 3.257 [1.951, 4.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.460, 10.100], loss: 0.301795, mae: 0.525631, mean_q: 4.876158
 62965/100000: episode: 1233, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 38.051, mean reward: 2.718 [1.839, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.219, 10.100], loss: 0.359800, mae: 0.534378, mean_q: 4.765788
 62979/100000: episode: 1234, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 48.186, mean reward: 3.442 [2.751, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.769, 10.100], loss: 0.310144, mae: 0.527514, mean_q: 4.834489
 63000/100000: episode: 1235, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 60.612, mean reward: 2.886 [2.313, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.682, 10.100], loss: 0.374326, mae: 0.539600, mean_q: 4.844251
 63015/100000: episode: 1236, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 52.736, mean reward: 3.516 [2.917, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.374, 10.100], loss: 0.756109, mae: 0.586607, mean_q: 4.873182
 63029/100000: episode: 1237, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 46.615, mean reward: 3.330 [2.582, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.266, 10.100], loss: 0.355189, mae: 0.547781, mean_q: 4.908013
 63054/100000: episode: 1238, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 86.977, mean reward: 3.479 [2.382, 5.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.492, 10.100], loss: 0.618873, mae: 0.611038, mean_q: 4.973346
 63074/100000: episode: 1239, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 76.147, mean reward: 3.807 [2.292, 6.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.458, 10.100], loss: 0.352029, mae: 0.551750, mean_q: 5.087602
 63091/100000: episode: 1240, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 62.632, mean reward: 3.684 [2.663, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.370, 10.100], loss: 0.275245, mae: 0.492615, mean_q: 4.845751
 63116/100000: episode: 1241, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 192.072, mean reward: 7.683 [3.098, 20.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.632, 10.100], loss: 0.319897, mae: 0.524750, mean_q: 4.976471
 63136/100000: episode: 1242, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 120.160, mean reward: 6.008 [2.810, 19.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.782, 10.100], loss: 0.436541, mae: 0.582892, mean_q: 4.956124
 63150/100000: episode: 1243, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 47.539, mean reward: 3.396 [2.599, 7.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.016, 10.100], loss: 0.342011, mae: 0.536397, mean_q: 4.984177
 63162/100000: episode: 1244, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 26.011, mean reward: 2.168 [1.874, 2.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.300, 10.100], loss: 0.375410, mae: 0.534652, mean_q: 4.963276
 63166/100000: episode: 1245, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 15.639, mean reward: 3.910 [3.546, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.494, 10.100], loss: 0.381767, mae: 0.557582, mean_q: 5.395754
 63187/100000: episode: 1246, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 69.801, mean reward: 3.324 [2.660, 5.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.597, 10.100], loss: 0.676255, mae: 0.674031, mean_q: 5.010145
 63191/100000: episode: 1247, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 15.938, mean reward: 3.985 [3.824, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.396, 10.100], loss: 0.438927, mae: 0.611043, mean_q: 5.100992
 63203/100000: episode: 1248, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 43.264, mean reward: 3.605 [2.516, 4.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.466, 10.100], loss: 0.344465, mae: 0.583848, mean_q: 5.079031
 63218/100000: episode: 1249, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 73.204, mean reward: 4.880 [3.345, 7.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.581, 10.100], loss: 0.329861, mae: 0.565789, mean_q: 5.013152
 63233/100000: episode: 1250, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 65.931, mean reward: 4.395 [3.355, 6.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.327, 10.100], loss: 0.411747, mae: 0.552618, mean_q: 4.978457
 63237/100000: episode: 1251, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 16.594, mean reward: 4.149 [3.660, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.428, 10.100], loss: 0.309759, mae: 0.536991, mean_q: 5.159208
 63252/100000: episode: 1252, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 57.720, mean reward: 3.848 [2.807, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.361, 10.100], loss: 0.549487, mae: 0.609963, mean_q: 5.182998
 63272/100000: episode: 1253, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 54.433, mean reward: 2.722 [2.034, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.139, 10.100], loss: 0.477565, mae: 0.561463, mean_q: 5.092550
 63292/100000: episode: 1254, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 62.440, mean reward: 3.122 [2.496, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.543, 10.100], loss: 0.540066, mae: 0.701939, mean_q: 5.162469
 63309/100000: episode: 1255, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 64.310, mean reward: 3.783 [2.545, 5.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.956, 10.100], loss: 0.745197, mae: 0.644010, mean_q: 5.232399
 63330/100000: episode: 1256, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 51.884, mean reward: 2.471 [2.094, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.571, 10.100], loss: 0.334440, mae: 0.551736, mean_q: 5.141249
 63350/100000: episode: 1257, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 70.650, mean reward: 3.532 [2.028, 5.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.509, 10.100], loss: 0.346282, mae: 0.551395, mean_q: 5.079865
 63371/100000: episode: 1258, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 68.094, mean reward: 3.243 [2.428, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.312, 10.100], loss: 0.367883, mae: 0.564260, mean_q: 5.100080
 63391/100000: episode: 1259, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 72.327, mean reward: 3.616 [2.455, 8.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.320, 10.100], loss: 0.409877, mae: 0.559314, mean_q: 5.048220
[Info] 3-TH LEVEL FOUND: 10.871530532836914, Considering 10/90 traces
 63416/100000: episode: 1260, duration: 4.235s, episode steps: 25, steps per second: 6, episode reward: 94.929, mean reward: 3.797 [2.571, 8.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.814, 10.100], loss: 0.497534, mae: 0.628162, mean_q: 5.242959
 63427/100000: episode: 1261, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 53.749, mean reward: 4.886 [3.201, 8.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.474, 10.100], loss: 0.348091, mae: 0.578802, mean_q: 5.208107
 63438/100000: episode: 1262, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 60.622, mean reward: 5.511 [4.844, 6.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.578, 10.100], loss: 0.346881, mae: 0.542269, mean_q: 5.066775
 63453/100000: episode: 1263, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 51.931, mean reward: 3.462 [2.932, 5.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.309, 10.100], loss: 0.212124, mae: 0.451471, mean_q: 5.039518
 63463/100000: episode: 1264, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 95.417, mean reward: 9.542 [5.442, 17.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.118, 10.100], loss: 0.373868, mae: 0.555079, mean_q: 5.198926
 63474/100000: episode: 1265, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 42.012, mean reward: 3.819 [2.880, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.363, 10.100], loss: 1.635816, mae: 0.840236, mean_q: 5.701086
 63488/100000: episode: 1266, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 77.355, mean reward: 5.525 [3.934, 7.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.414, 10.100], loss: 0.661452, mae: 0.641147, mean_q: 5.347096
 63499/100000: episode: 1267, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 56.294, mean reward: 5.118 [3.543, 7.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.404, 10.100], loss: 0.431661, mae: 0.608261, mean_q: 5.226066
 63507/100000: episode: 1268, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 48.834, mean reward: 6.104 [4.641, 8.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.481, 10.100], loss: 0.801106, mae: 0.613349, mean_q: 5.213694
 63518/100000: episode: 1269, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 93.441, mean reward: 8.495 [4.963, 12.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.497, 10.100], loss: 0.438397, mae: 0.644078, mean_q: 5.336295
 63526/100000: episode: 1270, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 30.424, mean reward: 3.803 [3.018, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.526, 10.100], loss: 0.660765, mae: 0.657981, mean_q: 5.280936
 63540/100000: episode: 1271, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 62.857, mean reward: 4.490 [3.077, 7.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.363, 10.100], loss: 0.736728, mae: 0.659767, mean_q: 5.401719
 63551/100000: episode: 1272, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 69.459, mean reward: 6.314 [4.158, 9.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.429, 10.100], loss: 1.043753, mae: 0.657390, mean_q: 5.330544
 63565/100000: episode: 1273, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 113.588, mean reward: 8.113 [4.119, 22.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.409, 10.100], loss: 0.548879, mae: 0.669535, mean_q: 5.429281
 63580/100000: episode: 1274, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 101.348, mean reward: 6.757 [4.482, 10.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.603, 10.100], loss: 0.614449, mae: 0.610951, mean_q: 5.331571
 63594/100000: episode: 1275, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 105.174, mean reward: 7.512 [4.087, 14.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.623, 10.100], loss: 0.623208, mae: 0.702213, mean_q: 5.616861
 63605/100000: episode: 1276, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 49.560, mean reward: 4.505 [3.629, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.001, 10.100], loss: 0.491372, mae: 0.628197, mean_q: 5.370934
 63616/100000: episode: 1277, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 81.575, mean reward: 7.416 [4.043, 9.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.531, 10.100], loss: 0.702703, mae: 0.726432, mean_q: 5.476828
 63631/100000: episode: 1278, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 92.003, mean reward: 6.134 [3.631, 8.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.640, 10.100], loss: 1.307876, mae: 0.885379, mean_q: 5.584648
 63642/100000: episode: 1279, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 42.071, mean reward: 3.825 [3.154, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.408, 10.100], loss: 0.804591, mae: 0.743489, mean_q: 5.404918
 63657/100000: episode: 1280, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 60.001, mean reward: 4.000 [2.809, 5.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.292, 10.100], loss: 1.099500, mae: 0.868854, mean_q: 5.653552
 63662/100000: episode: 1281, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 23.075, mean reward: 4.615 [4.046, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.467, 10.100], loss: 0.279488, mae: 0.528515, mean_q: 5.488203
 63676/100000: episode: 1282, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 113.414, mean reward: 8.101 [5.354, 12.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.372, 10.100], loss: 0.744844, mae: 0.676756, mean_q: 5.537087
 63691/100000: episode: 1283, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 62.955, mean reward: 4.197 [3.211, 4.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.325, 10.100], loss: 0.784707, mae: 0.733968, mean_q: 5.796555
 63702/100000: episode: 1284, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 78.777, mean reward: 7.162 [5.488, 9.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.521, 10.100], loss: 0.777274, mae: 0.624769, mean_q: 5.609905
 63716/100000: episode: 1285, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 81.851, mean reward: 5.846 [4.354, 9.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.542, 10.100], loss: 1.122420, mae: 0.840306, mean_q: 5.682395
 63724/100000: episode: 1286, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 37.108, mean reward: 4.638 [3.841, 6.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.487, 10.100], loss: 0.891729, mae: 0.697149, mean_q: 5.817724
 63735/100000: episode: 1287, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 68.772, mean reward: 6.252 [4.612, 10.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.463, 10.100], loss: 0.513925, mae: 0.683826, mean_q: 5.753248
 63749/100000: episode: 1288, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 71.123, mean reward: 5.080 [3.604, 7.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.369, 10.100], loss: 0.946229, mae: 0.721014, mean_q: 5.525490
 63760/100000: episode: 1289, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 51.082, mean reward: 4.644 [3.394, 7.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.430, 10.100], loss: 0.639690, mae: 0.708976, mean_q: 5.337288
 63774/100000: episode: 1290, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 167.469, mean reward: 11.962 [4.942, 23.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.367, 10.100], loss: 0.661296, mae: 0.753882, mean_q: 5.842467
 63785/100000: episode: 1291, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 37.987, mean reward: 3.453 [2.787, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.404, 10.100], loss: 0.639078, mae: 0.710432, mean_q: 5.650990
[Info] FALSIFICATION!
 63795/100000: episode: 1292, duration: 0.314s, episode steps: 10, steps per second: 32, episode reward: 1241.905, mean reward: 124.191 [7.171, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.429, 10.046], loss: 12.017955, mae: 1.121124, mean_q: 5.927293
 63805/100000: episode: 1293, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 51.662, mean reward: 5.166 [4.348, 6.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.443, 10.100], loss: 1.068680, mae: 1.027999, mean_q: 5.420341
 63815/100000: episode: 1294, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 60.952, mean reward: 6.095 [4.784, 7.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.605, 10.100], loss: 11.658236, mae: 1.279453, mean_q: 6.317320
 63823/100000: episode: 1295, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 83.297, mean reward: 10.412 [6.331, 16.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.485, 10.100], loss: 2.028396, mae: 1.219384, mean_q: 5.705749
 63834/100000: episode: 1296, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 66.199, mean reward: 6.018 [3.205, 10.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.594, 10.100], loss: 1.300428, mae: 1.033091, mean_q: 5.843753
 63845/100000: episode: 1297, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 53.769, mean reward: 4.888 [4.093, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.518, 10.100], loss: 0.870774, mae: 0.738497, mean_q: 5.718487
 63856/100000: episode: 1298, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 61.039, mean reward: 5.549 [4.169, 7.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.525, 10.100], loss: 0.652018, mae: 0.695007, mean_q: 5.928731
 63867/100000: episode: 1299, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 49.330, mean reward: 4.485 [3.543, 6.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.542, 10.100], loss: 0.579342, mae: 0.654467, mean_q: 5.869477
 63872/100000: episode: 1300, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 22.972, mean reward: 4.594 [3.912, 5.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.599, 10.100], loss: 0.694993, mae: 0.745383, mean_q: 5.970155
 63887/100000: episode: 1301, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 116.334, mean reward: 7.756 [3.238, 39.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.290, 10.100], loss: 1028.601074, mae: 5.304429, mean_q: 8.642953
[Info] FALSIFICATION!
 63897/100000: episode: 1302, duration: 0.312s, episode steps: 10, steps per second: 32, episode reward: 1120.733, mean reward: 112.073 [6.564, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.828, 10.046], loss: 1524.631470, mae: 5.016429, mean_q: 6.401772
 63908/100000: episode: 1303, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 47.404, mean reward: 4.309 [3.321, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.361, 10.100], loss: 2.957015, mae: 1.666384, mean_q: 6.514458
 63913/100000: episode: 1304, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 25.555, mean reward: 5.111 [3.970, 6.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.584, 10.100], loss: 1.762128, mae: 1.544040, mean_q: 4.732170
 63924/100000: episode: 1305, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 44.948, mean reward: 4.086 [3.255, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.434, 10.100], loss: 1.213964, mae: 1.058430, mean_q: 5.569892
[Info] FALSIFICATION!
 63931/100000: episode: 1306, duration: 0.297s, episode steps: 7, steps per second: 24, episode reward: 1116.661, mean reward: 159.523 [4.507, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.541, 10.067], loss: 1.076474, mae: 0.963467, mean_q: 6.537318
 63942/100000: episode: 1307, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 56.456, mean reward: 5.132 [4.057, 7.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.566, 10.100], loss: 1.209741, mae: 0.903915, mean_q: 6.363883
 63957/100000: episode: 1308, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 93.049, mean reward: 6.203 [3.533, 11.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.398, 10.100], loss: 8.006994, mae: 0.907352, mean_q: 6.149755
 63967/100000: episode: 1309, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 49.449, mean reward: 4.945 [4.119, 7.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.570, 10.100], loss: 1516.895874, mae: 4.198820, mean_q: 6.738272
 63981/100000: episode: 1310, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 62.516, mean reward: 4.465 [3.573, 5.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.442, 10.100], loss: 12.230990, mae: 3.632185, mean_q: 9.588518
 63989/100000: episode: 1311, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 47.861, mean reward: 5.983 [4.269, 12.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.382, 10.100], loss: 16.580061, mae: 2.394441, mean_q: 4.812027
 64004/100000: episode: 1312, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 57.776, mean reward: 3.852 [2.716, 5.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.461, 10.100], loss: 1.651113, mae: 1.352371, mean_q: 5.460398
 64015/100000: episode: 1313, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 48.077, mean reward: 4.371 [3.158, 5.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.505, 10.100], loss: 1.963298, mae: 1.108470, mean_q: 6.592088
 64030/100000: episode: 1314, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 58.654, mean reward: 3.910 [3.118, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.472, 10.100], loss: 1019.782288, mae: 3.609210, mean_q: 7.445896
 64035/100000: episode: 1315, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 22.764, mean reward: 4.553 [3.804, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.522, 10.100], loss: 9.519595, mae: 3.414171, mean_q: 9.815233
 64049/100000: episode: 1316, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 58.012, mean reward: 4.144 [2.650, 6.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.942, 10.100], loss: 8.909826, mae: 1.575609, mean_q: 6.922447
 64060/100000: episode: 1317, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 60.405, mean reward: 5.491 [3.858, 9.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.380, 10.100], loss: 1.834130, mae: 1.398387, mean_q: 5.385673
 64071/100000: episode: 1318, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 43.750, mean reward: 3.977 [3.090, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.658, 10.100], loss: 9.801221, mae: 1.128899, mean_q: 6.507051
 64086/100000: episode: 1319, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 894.671, mean reward: 59.645 [4.438, 812.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.443, 10.100], loss: 1.672886, mae: 1.044075, mean_q: 6.673152
 64101/100000: episode: 1320, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 128.827, mean reward: 8.588 [5.221, 13.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.360, 10.100], loss: 1023.733276, mae: 6.094931, mean_q: 10.202600
 64116/100000: episode: 1321, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 70.825, mean reward: 4.722 [3.105, 8.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.938, 10.100], loss: 5.963554, mae: 2.550101, mean_q: 7.457403
 64124/100000: episode: 1322, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 35.629, mean reward: 4.454 [3.655, 5.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.462, 10.100], loss: 4.706811, mae: 2.331622, mean_q: 4.534550
 64135/100000: episode: 1323, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 66.787, mean reward: 6.072 [4.330, 8.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.447, 10.100], loss: 2.025064, mae: 1.264584, mean_q: 6.406134
 64150/100000: episode: 1324, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 105.971, mean reward: 7.065 [4.348, 17.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.393, 10.100], loss: 1.909219, mae: 1.095050, mean_q: 7.063365
 64158/100000: episode: 1325, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 32.272, mean reward: 4.034 [2.945, 4.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.553, 10.100], loss: 1.497329, mae: 0.972285, mean_q: 6.548265
 64169/100000: episode: 1326, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 58.849, mean reward: 5.350 [4.444, 7.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.891, 10.100], loss: 3.594485, mae: 1.108172, mean_q: 6.747318
 64179/100000: episode: 1327, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 70.096, mean reward: 7.010 [4.612, 14.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.609, 10.100], loss: 1.165372, mae: 0.933214, mean_q: 6.560544
 64194/100000: episode: 1328, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 60.960, mean reward: 4.064 [3.122, 10.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.448, 10.100], loss: 1021.723450, mae: 4.205863, mean_q: 8.205488
 64202/100000: episode: 1329, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 49.119, mean reward: 6.140 [4.773, 7.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.441, 10.100], loss: 1864.843384, mae: 6.408991, mean_q: 9.355537
 64216/100000: episode: 1330, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 78.473, mean reward: 5.605 [3.823, 8.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.579, 10.100], loss: 1073.347656, mae: 5.052004, mean_q: 9.319371
 64230/100000: episode: 1331, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 77.768, mean reward: 5.555 [4.376, 7.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.433, 10.100], loss: 1068.550659, mae: 5.981461, mean_q: 10.537872
 64235/100000: episode: 1332, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 31.037, mean reward: 6.207 [3.860, 8.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.593, 10.100], loss: 12.894030, mae: 3.401745, mean_q: 9.395176
 64246/100000: episode: 1333, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 73.709, mean reward: 6.701 [4.205, 10.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.137, 10.100], loss: 4.991919, mae: 2.501110, mean_q: 6.157548
 64257/100000: episode: 1334, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 61.012, mean reward: 5.547 [4.071, 8.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.387, 10.100], loss: 5.709921, mae: 2.307920, mean_q: 5.400713
 64272/100000: episode: 1335, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 147.031, mean reward: 9.802 [5.196, 15.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.627, 10.100], loss: 2.242105, mae: 1.241783, mean_q: 6.573729
 64286/100000: episode: 1336, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 70.008, mean reward: 5.001 [3.225, 7.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.277, 10.100], loss: 1096.595459, mae: 3.711835, mean_q: 7.671208
 64296/100000: episode: 1337, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 56.170, mean reward: 5.617 [4.586, 6.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.413, 10.100], loss: 9.931372, mae: 3.142537, mean_q: 10.019174
 64310/100000: episode: 1338, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 61.549, mean reward: 4.396 [2.917, 8.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.804, 10.100], loss: 2.175033, mae: 1.302463, mean_q: 7.116452
 64321/100000: episode: 1339, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 61.228, mean reward: 5.566 [3.987, 6.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.932, 10.100], loss: 2.977186, mae: 1.490892, mean_q: 6.036196
 64336/100000: episode: 1340, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 77.557, mean reward: 5.170 [2.639, 6.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.233, 10.100], loss: 1.302559, mae: 1.060022, mean_q: 6.639109
 64350/100000: episode: 1341, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 81.202, mean reward: 5.800 [4.402, 7.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.600, 10.100], loss: 1.862224, mae: 1.152971, mean_q: 7.119802
 64361/100000: episode: 1342, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 53.011, mean reward: 4.819 [4.123, 5.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.685, 10.100], loss: 1389.441528, mae: 5.031116, mean_q: 9.192657
 64369/100000: episode: 1343, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 37.652, mean reward: 4.706 [4.042, 5.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.448, 10.100], loss: 6.725671, mae: 2.314810, mean_q: 9.071194
 64383/100000: episode: 1344, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 162.580, mean reward: 11.613 [4.474, 46.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.676, 10.100], loss: 2.148929, mae: 1.291193, mean_q: 6.992914
 64394/100000: episode: 1345, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 43.723, mean reward: 3.975 [3.129, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.058, 10.100], loss: 1.969720, mae: 1.249332, mean_q: 6.306662
 64405/100000: episode: 1346, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 56.932, mean reward: 5.176 [3.846, 7.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.647, 10.100], loss: 2.008547, mae: 1.177650, mean_q: 6.713715
 64413/100000: episode: 1347, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 37.362, mean reward: 4.670 [3.913, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.421, 10.100], loss: 1867.931885, mae: 6.433988, mean_q: 9.444674
 64421/100000: episode: 1348, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 37.851, mean reward: 4.731 [4.020, 6.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.436, 10.100], loss: 11.422691, mae: 3.453130, mean_q: 10.728701
 64429/100000: episode: 1349, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 43.986, mean reward: 5.498 [3.513, 8.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.495, 10.100], loss: 7.089378, mae: 2.547842, mean_q: 9.432377
[Info] Complete ISplit Iteration
[Info] Levels: [5.2138977, 7.025601, 10.871531, 16.641863]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.4]
[Info] Error Prob: 0.00040000000000000013

 64439/100000: episode: 1350, duration: 4.310s, episode steps: 10, steps per second: 2, episode reward: 41.715, mean reward: 4.171 [3.334, 5.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.411, 10.100], loss: 3.269208, mae: 1.581969, mean_q: 8.107456
 64539/100000: episode: 1351, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 226.276, mean reward: 2.263 [1.483, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.000, 10.098], loss: 458.399872, mae: 3.339423, mean_q: 8.152395
 64639/100000: episode: 1352, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.750, mean reward: 1.817 [1.438, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.098], loss: 305.759583, mae: 2.185551, mean_q: 7.786366
 64739/100000: episode: 1353, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 176.161, mean reward: 1.762 [1.460, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.425, 10.098], loss: 257.518921, mae: 2.242589, mean_q: 7.934255
 64839/100000: episode: 1354, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 222.131, mean reward: 2.221 [1.459, 18.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.234], loss: 299.430420, mae: 2.422004, mean_q: 7.789976
 64939/100000: episode: 1355, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.346, mean reward: 1.913 [1.462, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.840, 10.183], loss: 409.513214, mae: 2.728194, mean_q: 7.917014
 65039/100000: episode: 1356, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.241, mean reward: 1.882 [1.437, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.096, 10.098], loss: 301.926788, mae: 2.609928, mean_q: 8.145365
 65139/100000: episode: 1357, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.898, mean reward: 1.839 [1.468, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.077, 10.098], loss: 153.966599, mae: 1.693941, mean_q: 7.343924
 65239/100000: episode: 1358, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 189.335, mean reward: 1.893 [1.452, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.407], loss: 152.291519, mae: 1.870614, mean_q: 7.367395
 65339/100000: episode: 1359, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 185.045, mean reward: 1.850 [1.459, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.166], loss: 703.440613, mae: 3.742774, mean_q: 8.427179
 65439/100000: episode: 1360, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.288, mean reward: 1.883 [1.463, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.179], loss: 209.501114, mae: 2.169361, mean_q: 7.923417
 65539/100000: episode: 1361, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.142, mean reward: 1.951 [1.437, 5.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.710, 10.098], loss: 509.020905, mae: 2.784105, mean_q: 7.879930
 65639/100000: episode: 1362, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.512, mean reward: 2.065 [1.452, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.695, 10.098], loss: 607.338501, mae: 3.624009, mean_q: 8.473797
 65739/100000: episode: 1363, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 223.573, mean reward: 2.236 [1.486, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.402, 10.098], loss: 258.425964, mae: 2.137945, mean_q: 7.681656
 65839/100000: episode: 1364, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 175.353, mean reward: 1.754 [1.461, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.593, 10.098], loss: 2.480294, mae: 1.277748, mean_q: 7.223068
 65939/100000: episode: 1365, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 201.180, mean reward: 2.012 [1.505, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.726, 10.192], loss: 303.547424, mae: 2.355317, mean_q: 7.747249
 66039/100000: episode: 1366, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 199.988, mean reward: 2.000 [1.477, 4.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.985, 10.098], loss: 299.112213, mae: 1.957729, mean_q: 7.061285
 66139/100000: episode: 1367, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 182.158, mean reward: 1.822 [1.437, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.191, 10.098], loss: 697.407715, mae: 4.253468, mean_q: 8.972911
 66239/100000: episode: 1368, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.413, mean reward: 1.904 [1.470, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.067, 10.150], loss: 154.227936, mae: 1.818978, mean_q: 7.378677
 66339/100000: episode: 1369, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.995, mean reward: 1.940 [1.453, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.835, 10.363], loss: 510.756287, mae: 3.575259, mean_q: 8.539254
 66439/100000: episode: 1370, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 182.411, mean reward: 1.824 [1.467, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.182], loss: 706.616882, mae: 3.832350, mean_q: 8.758662
 66539/100000: episode: 1371, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 186.608, mean reward: 1.866 [1.456, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.296], loss: 703.891479, mae: 3.670818, mean_q: 8.381708
 66639/100000: episode: 1372, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 190.076, mean reward: 1.901 [1.444, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.557, 10.224], loss: 153.463211, mae: 2.018423, mean_q: 7.916417
 66739/100000: episode: 1373, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 202.820, mean reward: 2.028 [1.446, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.298], loss: 255.789703, mae: 2.046255, mean_q: 7.251533
 66839/100000: episode: 1374, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 187.585, mean reward: 1.876 [1.490, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.702, 10.098], loss: 611.200012, mae: 3.649304, mean_q: 8.832280
 66939/100000: episode: 1375, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.797, mean reward: 1.968 [1.491, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.547, 10.340], loss: 1041.940186, mae: 5.188661, mean_q: 9.460918
 67039/100000: episode: 1376, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 177.659, mean reward: 1.777 [1.449, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.133, 10.098], loss: 152.161575, mae: 2.005807, mean_q: 7.447818
 67139/100000: episode: 1377, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 174.188, mean reward: 1.742 [1.446, 2.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.432, 10.098], loss: 302.791992, mae: 2.309285, mean_q: 7.261644
 67239/100000: episode: 1378, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 187.521, mean reward: 1.875 [1.468, 2.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.228, 10.199], loss: 299.166534, mae: 2.400790, mean_q: 7.325380
 67339/100000: episode: 1379, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 195.985, mean reward: 1.960 [1.450, 9.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.062, 10.148], loss: 292.705170, mae: 2.071611, mean_q: 6.752650
 67439/100000: episode: 1380, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.764, mean reward: 2.008 [1.466, 5.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.566, 10.098], loss: 159.595474, mae: 1.836389, mean_q: 6.962337
 67539/100000: episode: 1381, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.983, mean reward: 1.820 [1.452, 2.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.499, 10.098], loss: 305.553070, mae: 2.236627, mean_q: 7.033988
 67639/100000: episode: 1382, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 189.972, mean reward: 1.900 [1.463, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.733, 10.376], loss: 1395.033936, mae: 5.783895, mean_q: 8.863523
 67739/100000: episode: 1383, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 245.006, mean reward: 2.450 [1.473, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.641, 10.198], loss: 310.016113, mae: 2.863699, mean_q: 7.796343
 67839/100000: episode: 1384, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.790, mean reward: 2.038 [1.460, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.967, 10.290], loss: 362.559967, mae: 2.180917, mean_q: 6.897978
 67939/100000: episode: 1385, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 180.690, mean reward: 1.807 [1.443, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.098], loss: 158.245300, mae: 1.629221, mean_q: 6.685697
 68039/100000: episode: 1386, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.278, mean reward: 1.913 [1.438, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.718, 10.175], loss: 352.154663, mae: 2.383241, mean_q: 7.045907
 68139/100000: episode: 1387, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 230.837, mean reward: 2.308 [1.533, 8.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.840, 10.144], loss: 255.069687, mae: 1.913344, mean_q: 6.465822
 68239/100000: episode: 1388, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 184.472, mean reward: 1.845 [1.459, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.137], loss: 450.898285, mae: 2.429034, mean_q: 6.837920
 68339/100000: episode: 1389, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.015, mean reward: 1.940 [1.469, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.609, 10.119], loss: 151.562851, mae: 1.422567, mean_q: 6.021088
 68439/100000: episode: 1390, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.569, mean reward: 1.906 [1.469, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.738, 10.098], loss: 149.841217, mae: 1.452044, mean_q: 6.033279
 68539/100000: episode: 1391, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.381, mean reward: 1.814 [1.448, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.052, 10.130], loss: 405.776642, mae: 2.180151, mean_q: 6.419470
 68639/100000: episode: 1392, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 195.742, mean reward: 1.957 [1.476, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.605, 10.181], loss: 259.628479, mae: 1.781553, mean_q: 6.225205
 68739/100000: episode: 1393, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 209.397, mean reward: 2.094 [1.447, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.857, 10.383], loss: 302.021454, mae: 1.802929, mean_q: 5.954253
 68839/100000: episode: 1394, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 195.583, mean reward: 1.956 [1.473, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.481, 10.294], loss: 251.873795, mae: 1.658083, mean_q: 5.591119
 68939/100000: episode: 1395, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 219.051, mean reward: 2.191 [1.477, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.098], loss: 1.138976, mae: 0.635596, mean_q: 4.885208
 69039/100000: episode: 1396, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 198.693, mean reward: 1.987 [1.444, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.098], loss: 1.519335, mae: 0.569414, mean_q: 4.484627
 69139/100000: episode: 1397, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 197.846, mean reward: 1.978 [1.485, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.820, 10.098], loss: 0.842742, mae: 0.497603, mean_q: 4.383032
 69239/100000: episode: 1398, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.219, mean reward: 1.882 [1.440, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.115], loss: 0.395950, mae: 0.422614, mean_q: 4.145109
 69339/100000: episode: 1399, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.422, mean reward: 1.944 [1.515, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.564, 10.098], loss: 0.560907, mae: 0.389418, mean_q: 3.973335
 69439/100000: episode: 1400, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.490, mean reward: 1.925 [1.510, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.311, 10.098], loss: 0.172957, mae: 0.328688, mean_q: 3.863138
 69539/100000: episode: 1401, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 177.382, mean reward: 1.774 [1.451, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.098], loss: 0.165296, mae: 0.327008, mean_q: 3.834694
 69639/100000: episode: 1402, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.143, mean reward: 1.961 [1.458, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.019, 10.203], loss: 0.112010, mae: 0.311171, mean_q: 3.823630
 69739/100000: episode: 1403, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 198.520, mean reward: 1.985 [1.473, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.221], loss: 0.179870, mae: 0.325349, mean_q: 3.838694
 69839/100000: episode: 1404, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 183.787, mean reward: 1.838 [1.448, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.599, 10.317], loss: 0.125373, mae: 0.323508, mean_q: 3.834208
 69939/100000: episode: 1405, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 201.484, mean reward: 2.015 [1.471, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.641, 10.228], loss: 0.105816, mae: 0.313939, mean_q: 3.836796
 70039/100000: episode: 1406, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 201.767, mean reward: 2.018 [1.469, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.598, 10.099], loss: 0.126812, mae: 0.328691, mean_q: 3.831166
 70139/100000: episode: 1407, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.412, mean reward: 1.864 [1.456, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.206, 10.148], loss: 0.113948, mae: 0.319629, mean_q: 3.834302
 70239/100000: episode: 1408, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 184.803, mean reward: 1.848 [1.477, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.116346, mae: 0.313626, mean_q: 3.835086
 70339/100000: episode: 1409, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.105, mean reward: 1.951 [1.450, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.132], loss: 0.124708, mae: 0.321834, mean_q: 3.828538
 70439/100000: episode: 1410, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 211.738, mean reward: 2.117 [1.540, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.943, 10.194], loss: 0.105522, mae: 0.317993, mean_q: 3.847685
 70539/100000: episode: 1411, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 214.424, mean reward: 2.144 [1.500, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.215, 10.098], loss: 0.133502, mae: 0.331821, mean_q: 3.859876
 70639/100000: episode: 1412, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 228.039, mean reward: 2.280 [1.465, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.828, 10.294], loss: 0.105464, mae: 0.318863, mean_q: 3.855650
 70739/100000: episode: 1413, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 189.009, mean reward: 1.890 [1.480, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.210], loss: 0.126940, mae: 0.329372, mean_q: 3.857854
 70839/100000: episode: 1414, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 198.880, mean reward: 1.989 [1.461, 5.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.201, 10.199], loss: 0.131543, mae: 0.331095, mean_q: 3.865092
 70939/100000: episode: 1415, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.953, mean reward: 1.880 [1.495, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.509, 10.205], loss: 0.127323, mae: 0.335481, mean_q: 3.875111
 71039/100000: episode: 1416, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.838, mean reward: 1.908 [1.463, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.642, 10.201], loss: 0.104949, mae: 0.323180, mean_q: 3.857527
 71139/100000: episode: 1417, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 202.138, mean reward: 2.021 [1.452, 4.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.723, 10.250], loss: 0.106176, mae: 0.320191, mean_q: 3.868480
 71239/100000: episode: 1418, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 220.467, mean reward: 2.205 [1.484, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.150], loss: 0.129415, mae: 0.338080, mean_q: 3.884234
 71339/100000: episode: 1419, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 179.106, mean reward: 1.791 [1.480, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.745, 10.098], loss: 0.125550, mae: 0.343040, mean_q: 3.879807
 71439/100000: episode: 1420, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.200, mean reward: 1.902 [1.487, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.145, 10.098], loss: 0.127098, mae: 0.326148, mean_q: 3.867716
 71539/100000: episode: 1421, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 184.025, mean reward: 1.840 [1.443, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.569, 10.276], loss: 0.123941, mae: 0.334320, mean_q: 3.898553
 71639/100000: episode: 1422, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.211, mean reward: 1.842 [1.447, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.573, 10.229], loss: 0.113465, mae: 0.326137, mean_q: 3.876390
 71739/100000: episode: 1423, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.153, mean reward: 1.902 [1.490, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.500, 10.172], loss: 0.116172, mae: 0.330288, mean_q: 3.872507
 71839/100000: episode: 1424, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.931, mean reward: 1.949 [1.538, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.352, 10.197], loss: 0.122076, mae: 0.330766, mean_q: 3.885169
 71939/100000: episode: 1425, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 201.827, mean reward: 2.018 [1.459, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.208], loss: 0.114626, mae: 0.325675, mean_q: 3.872231
 72039/100000: episode: 1426, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.547, mean reward: 1.875 [1.447, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.775, 10.156], loss: 0.112917, mae: 0.326843, mean_q: 3.883372
 72139/100000: episode: 1427, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.016, mean reward: 1.970 [1.492, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.121, 10.289], loss: 0.128672, mae: 0.338971, mean_q: 3.882620
 72239/100000: episode: 1428, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 203.827, mean reward: 2.038 [1.450, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.359], loss: 0.110517, mae: 0.329231, mean_q: 3.893316
 72339/100000: episode: 1429, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.457, mean reward: 1.835 [1.457, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.538, 10.127], loss: 0.117792, mae: 0.328286, mean_q: 3.890363
 72439/100000: episode: 1430, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.728, mean reward: 1.817 [1.457, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.130], loss: 0.136540, mae: 0.340008, mean_q: 3.913396
 72539/100000: episode: 1431, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 218.845, mean reward: 2.188 [1.474, 5.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.164, 10.098], loss: 0.103076, mae: 0.321393, mean_q: 3.888009
 72639/100000: episode: 1432, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.499, mean reward: 1.925 [1.441, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.449, 10.098], loss: 0.103941, mae: 0.317142, mean_q: 3.888716
 72739/100000: episode: 1433, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.990, mean reward: 2.040 [1.456, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.926, 10.098], loss: 0.102011, mae: 0.316087, mean_q: 3.875518
 72839/100000: episode: 1434, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.252, mean reward: 1.973 [1.509, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.593, 10.098], loss: 0.117113, mae: 0.319802, mean_q: 3.885153
 72939/100000: episode: 1435, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.605, mean reward: 1.956 [1.498, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.155, 10.098], loss: 0.094503, mae: 0.308434, mean_q: 3.868057
 73039/100000: episode: 1436, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 185.897, mean reward: 1.859 [1.463, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.296, 10.128], loss: 0.088619, mae: 0.305552, mean_q: 3.910485
 73139/100000: episode: 1437, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 181.576, mean reward: 1.816 [1.466, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.014, 10.098], loss: 0.097837, mae: 0.312513, mean_q: 3.888721
 73239/100000: episode: 1438, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.177, mean reward: 2.022 [1.430, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.042, 10.161], loss: 0.105927, mae: 0.320166, mean_q: 3.876859
 73339/100000: episode: 1439, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.623, mean reward: 1.826 [1.474, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.324, 10.098], loss: 0.090074, mae: 0.305710, mean_q: 3.886184
 73439/100000: episode: 1440, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.875, mean reward: 1.939 [1.481, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.467, 10.098], loss: 0.094480, mae: 0.309784, mean_q: 3.869094
 73539/100000: episode: 1441, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 191.686, mean reward: 1.917 [1.460, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.331, 10.218], loss: 0.094621, mae: 0.303087, mean_q: 3.875128
 73639/100000: episode: 1442, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.629, mean reward: 1.806 [1.450, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.011, 10.193], loss: 0.100222, mae: 0.313555, mean_q: 3.885356
 73739/100000: episode: 1443, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 182.037, mean reward: 1.820 [1.460, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.238, 10.252], loss: 0.103197, mae: 0.316587, mean_q: 3.866178
 73839/100000: episode: 1444, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.682, mean reward: 1.887 [1.449, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.537, 10.098], loss: 0.096970, mae: 0.316504, mean_q: 3.853993
 73939/100000: episode: 1445, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.830, mean reward: 2.008 [1.453, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.285], loss: 0.093075, mae: 0.310237, mean_q: 3.841417
 74039/100000: episode: 1446, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.182, mean reward: 1.942 [1.509, 2.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.759, 10.098], loss: 0.099999, mae: 0.311350, mean_q: 3.846768
 74139/100000: episode: 1447, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.189, mean reward: 1.862 [1.446, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.985, 10.098], loss: 0.093522, mae: 0.306495, mean_q: 3.842092
 74239/100000: episode: 1448, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.354, mean reward: 1.894 [1.449, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.992, 10.255], loss: 0.092973, mae: 0.300830, mean_q: 3.841940
 74339/100000: episode: 1449, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.170, mean reward: 1.932 [1.445, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.016, 10.218], loss: 0.082344, mae: 0.295633, mean_q: 3.850059
[Info] 1-TH LEVEL FOUND: 4.668717861175537, Considering 10/90 traces
 74439/100000: episode: 1450, duration: 4.591s, episode steps: 100, steps per second: 22, episode reward: 211.058, mean reward: 2.111 [1.464, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.164], loss: 0.091626, mae: 0.304007, mean_q: 3.848975
 74531/100000: episode: 1451, duration: 0.474s, episode steps: 92, steps per second: 194, episode reward: 169.567, mean reward: 1.843 [1.450, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.270, 10.134], loss: 0.101168, mae: 0.318761, mean_q: 3.846394
 74631/100000: episode: 1452, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.020, mean reward: 2.030 [1.479, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.563, 10.148], loss: 0.094150, mae: 0.310536, mean_q: 3.834996
 74723/100000: episode: 1453, duration: 0.465s, episode steps: 92, steps per second: 198, episode reward: 188.141, mean reward: 2.045 [1.461, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-0.720, 10.244], loss: 0.102404, mae: 0.319037, mean_q: 3.862155
 74811/100000: episode: 1454, duration: 0.441s, episode steps: 88, steps per second: 200, episode reward: 175.080, mean reward: 1.990 [1.433, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.576 [-0.817, 10.179], loss: 0.098511, mae: 0.313992, mean_q: 3.877686
 74902/100000: episode: 1455, duration: 0.457s, episode steps: 91, steps per second: 199, episode reward: 172.558, mean reward: 1.896 [1.493, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.989, 10.100], loss: 0.092787, mae: 0.311325, mean_q: 3.871056
 74921/100000: episode: 1456, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 53.773, mean reward: 2.830 [1.912, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.277, 10.100], loss: 0.092654, mae: 0.300970, mean_q: 3.801230
 75013/100000: episode: 1457, duration: 0.481s, episode steps: 92, steps per second: 191, episode reward: 198.386, mean reward: 2.156 [1.495, 8.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-1.284, 10.255], loss: 0.119109, mae: 0.332088, mean_q: 3.884094
 75032/100000: episode: 1458, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 79.497, mean reward: 4.184 [2.681, 9.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.511, 10.100], loss: 0.114185, mae: 0.330203, mean_q: 3.890931
 75128/100000: episode: 1459, duration: 0.490s, episode steps: 96, steps per second: 196, episode reward: 181.678, mean reward: 1.892 [1.462, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-1.199, 10.100], loss: 0.090925, mae: 0.304617, mean_q: 3.861725
 75228/100000: episode: 1460, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.520, mean reward: 1.895 [1.433, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.454, 10.237], loss: 0.112570, mae: 0.329469, mean_q: 3.890866
 75247/100000: episode: 1461, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 46.144, mean reward: 2.429 [1.959, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.286, 10.100], loss: 0.113697, mae: 0.331158, mean_q: 3.910958
 75339/100000: episode: 1462, duration: 0.477s, episode steps: 92, steps per second: 193, episode reward: 182.288, mean reward: 1.981 [1.444, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-0.308, 10.182], loss: 0.129457, mae: 0.329173, mean_q: 3.899472
 75435/100000: episode: 1463, duration: 0.488s, episode steps: 96, steps per second: 197, episode reward: 202.885, mean reward: 2.113 [1.504, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.925, 10.187], loss: 0.105084, mae: 0.325476, mean_q: 3.893242
 75527/100000: episode: 1464, duration: 0.471s, episode steps: 92, steps per second: 195, episode reward: 170.478, mean reward: 1.853 [1.479, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.549, 10.156], loss: 0.107871, mae: 0.315879, mean_q: 3.882953
 75618/100000: episode: 1465, duration: 0.469s, episode steps: 91, steps per second: 194, episode reward: 184.232, mean reward: 2.025 [1.470, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.549 [-1.291, 10.497], loss: 0.101046, mae: 0.305051, mean_q: 3.859308
 75710/100000: episode: 1466, duration: 0.461s, episode steps: 92, steps per second: 200, episode reward: 193.360, mean reward: 2.102 [1.467, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-0.412, 10.100], loss: 0.109156, mae: 0.321043, mean_q: 3.874969
 75798/100000: episode: 1467, duration: 0.446s, episode steps: 88, steps per second: 197, episode reward: 164.353, mean reward: 1.868 [1.449, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.570 [-0.949, 10.277], loss: 0.110976, mae: 0.324732, mean_q: 3.868969
 75890/100000: episode: 1468, duration: 0.467s, episode steps: 92, steps per second: 197, episode reward: 177.290, mean reward: 1.927 [1.482, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-1.011, 10.100], loss: 0.107762, mae: 0.320396, mean_q: 3.858128
 75911/100000: episode: 1469, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 80.370, mean reward: 3.827 [2.939, 7.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.333, 10.100], loss: 0.128625, mae: 0.340522, mean_q: 3.911798
 75932/100000: episode: 1470, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 59.643, mean reward: 2.840 [2.152, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.975, 10.100], loss: 0.100795, mae: 0.319609, mean_q: 3.902067
 76032/100000: episode: 1471, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 218.485, mean reward: 2.185 [1.557, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.849, 10.168], loss: 0.108255, mae: 0.312360, mean_q: 3.870947
 76123/100000: episode: 1472, duration: 0.457s, episode steps: 91, steps per second: 199, episode reward: 165.055, mean reward: 1.814 [1.473, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-0.421, 10.100], loss: 0.109564, mae: 0.327554, mean_q: 3.895033
 76129/100000: episode: 1473, duration: 0.047s, episode steps: 6, steps per second: 126, episode reward: 14.341, mean reward: 2.390 [2.254, 2.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.396, 10.100], loss: 0.097674, mae: 0.304813, mean_q: 3.894138
 76220/100000: episode: 1474, duration: 0.472s, episode steps: 91, steps per second: 193, episode reward: 168.202, mean reward: 1.848 [1.436, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.859, 10.100], loss: 0.097431, mae: 0.310308, mean_q: 3.900221
 76320/100000: episode: 1475, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 182.693, mean reward: 1.827 [1.457, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.767, 10.100], loss: 0.110657, mae: 0.314606, mean_q: 3.871835
 76416/100000: episode: 1476, duration: 0.494s, episode steps: 96, steps per second: 194, episode reward: 178.148, mean reward: 1.856 [1.485, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-1.054, 10.178], loss: 0.091242, mae: 0.302546, mean_q: 3.879954
 76516/100000: episode: 1477, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 197.072, mean reward: 1.971 [1.474, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.266, 10.153], loss: 0.104578, mae: 0.315016, mean_q: 3.874591
 76535/100000: episode: 1478, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 61.196, mean reward: 3.221 [2.297, 6.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.718, 10.100], loss: 0.096681, mae: 0.307507, mean_q: 3.869416
 76635/100000: episode: 1479, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.422, mean reward: 2.054 [1.450, 9.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.892, 10.100], loss: 0.102991, mae: 0.306956, mean_q: 3.883668
 76735/100000: episode: 1480, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.902, mean reward: 1.909 [1.460, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.570, 10.121], loss: 0.103484, mae: 0.316894, mean_q: 3.889899
 76754/100000: episode: 1481, duration: 0.094s, episode steps: 19, steps per second: 203, episode reward: 56.248, mean reward: 2.960 [2.392, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.906, 10.100], loss: 0.103770, mae: 0.317052, mean_q: 3.889729
 76846/100000: episode: 1482, duration: 0.489s, episode steps: 92, steps per second: 188, episode reward: 188.329, mean reward: 2.047 [1.470, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.970, 10.100], loss: 0.107524, mae: 0.321186, mean_q: 3.889749
 76938/100000: episode: 1483, duration: 0.480s, episode steps: 92, steps per second: 192, episode reward: 190.908, mean reward: 2.075 [1.460, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.718, 10.100], loss: 0.109571, mae: 0.311092, mean_q: 3.902134
 77029/100000: episode: 1484, duration: 0.484s, episode steps: 91, steps per second: 188, episode reward: 174.168, mean reward: 1.914 [1.445, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-0.282, 10.100], loss: 0.112939, mae: 0.329232, mean_q: 3.917128
 77121/100000: episode: 1485, duration: 0.473s, episode steps: 92, steps per second: 194, episode reward: 177.970, mean reward: 1.934 [1.493, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.530, 10.376], loss: 0.088202, mae: 0.305888, mean_q: 3.887466
 77127/100000: episode: 1486, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 15.331, mean reward: 2.555 [2.319, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.373, 10.100], loss: 0.201713, mae: 0.346287, mean_q: 3.905304
 77219/100000: episode: 1487, duration: 0.475s, episode steps: 92, steps per second: 194, episode reward: 183.400, mean reward: 1.993 [1.489, 5.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.879, 10.220], loss: 0.122463, mae: 0.333056, mean_q: 3.920159
 77311/100000: episode: 1488, duration: 0.464s, episode steps: 92, steps per second: 198, episode reward: 199.423, mean reward: 2.168 [1.445, 6.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.389, 10.322], loss: 0.112850, mae: 0.321991, mean_q: 3.900632
 77399/100000: episode: 1489, duration: 0.437s, episode steps: 88, steps per second: 201, episode reward: 168.773, mean reward: 1.918 [1.474, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-1.169, 10.227], loss: 0.109835, mae: 0.331265, mean_q: 3.913065
 77420/100000: episode: 1490, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 70.822, mean reward: 3.372 [2.471, 5.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.299, 10.100], loss: 0.159520, mae: 0.379302, mean_q: 3.975488
 77512/100000: episode: 1491, duration: 0.502s, episode steps: 92, steps per second: 183, episode reward: 183.624, mean reward: 1.996 [1.476, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.949, 10.113], loss: 0.118515, mae: 0.329776, mean_q: 3.933531
 77533/100000: episode: 1492, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 90.007, mean reward: 4.286 [2.690, 9.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.494, 10.100], loss: 0.096149, mae: 0.314733, mean_q: 3.891514
 77621/100000: episode: 1493, duration: 0.433s, episode steps: 88, steps per second: 203, episode reward: 176.729, mean reward: 2.008 [1.450, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.582 [-0.919, 10.116], loss: 0.105943, mae: 0.324263, mean_q: 3.930897
 77713/100000: episode: 1494, duration: 0.477s, episode steps: 92, steps per second: 193, episode reward: 178.886, mean reward: 1.944 [1.520, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.703, 10.100], loss: 0.116196, mae: 0.315229, mean_q: 3.933653
 77805/100000: episode: 1495, duration: 0.475s, episode steps: 92, steps per second: 194, episode reward: 189.514, mean reward: 2.060 [1.449, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-1.234, 10.253], loss: 0.110930, mae: 0.325783, mean_q: 3.941049
 77905/100000: episode: 1496, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.048, mean reward: 1.950 [1.469, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.733, 10.100], loss: 0.107994, mae: 0.319176, mean_q: 3.939678
 77997/100000: episode: 1497, duration: 0.477s, episode steps: 92, steps per second: 193, episode reward: 184.911, mean reward: 2.010 [1.450, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.478, 10.276], loss: 0.120561, mae: 0.328643, mean_q: 3.935096
 78018/100000: episode: 1498, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 86.828, mean reward: 4.135 [2.939, 6.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.242, 10.100], loss: 0.118032, mae: 0.336539, mean_q: 4.021261
 78118/100000: episode: 1499, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.733, mean reward: 1.957 [1.460, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.768, 10.100], loss: 0.105126, mae: 0.317563, mean_q: 3.940487
 78210/100000: episode: 1500, duration: 0.477s, episode steps: 92, steps per second: 193, episode reward: 183.047, mean reward: 1.990 [1.485, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.400, 10.190], loss: 0.119969, mae: 0.328807, mean_q: 3.965180
 78306/100000: episode: 1501, duration: 0.487s, episode steps: 96, steps per second: 197, episode reward: 187.747, mean reward: 1.956 [1.488, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.753, 10.269], loss: 0.127341, mae: 0.343121, mean_q: 3.982328
 78394/100000: episode: 1502, duration: 0.432s, episode steps: 88, steps per second: 204, episode reward: 169.555, mean reward: 1.927 [1.489, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.562 [-0.992, 10.100], loss: 0.133680, mae: 0.332281, mean_q: 3.976180
 78486/100000: episode: 1503, duration: 0.468s, episode steps: 92, steps per second: 197, episode reward: 167.227, mean reward: 1.818 [1.460, 2.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-0.650, 10.100], loss: 0.118810, mae: 0.327817, mean_q: 3.966155
 78578/100000: episode: 1504, duration: 0.474s, episode steps: 92, steps per second: 194, episode reward: 196.345, mean reward: 2.134 [1.476, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.966, 10.100], loss: 0.105527, mae: 0.319217, mean_q: 3.951236
 78670/100000: episode: 1505, duration: 0.474s, episode steps: 92, steps per second: 194, episode reward: 173.411, mean reward: 1.885 [1.435, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.365, 10.100], loss: 0.120107, mae: 0.336279, mean_q: 3.996092
 78770/100000: episode: 1506, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.570, mean reward: 1.926 [1.441, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.811, 10.326], loss: 0.120960, mae: 0.333508, mean_q: 3.982241
 78791/100000: episode: 1507, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 69.334, mean reward: 3.302 [2.267, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.550, 10.100], loss: 0.098469, mae: 0.312431, mean_q: 3.939182
 78812/100000: episode: 1508, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 62.222, mean reward: 2.963 [2.154, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.185, 10.100], loss: 0.174700, mae: 0.374841, mean_q: 4.068247
 78912/100000: episode: 1509, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 179.022, mean reward: 1.790 [1.438, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.953, 10.266], loss: 0.114541, mae: 0.336167, mean_q: 4.001829
 79003/100000: episode: 1510, duration: 0.485s, episode steps: 91, steps per second: 188, episode reward: 179.210, mean reward: 1.969 [1.449, 5.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-1.482, 10.100], loss: 0.127979, mae: 0.336630, mean_q: 4.027441
 79095/100000: episode: 1511, duration: 0.484s, episode steps: 92, steps per second: 190, episode reward: 178.158, mean reward: 1.937 [1.552, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-1.044, 10.100], loss: 0.127918, mae: 0.342427, mean_q: 4.018646
 79187/100000: episode: 1512, duration: 0.443s, episode steps: 92, steps per second: 207, episode reward: 185.457, mean reward: 2.016 [1.465, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.378, 10.100], loss: 0.148699, mae: 0.356577, mean_q: 4.029247
 79287/100000: episode: 1513, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 202.929, mean reward: 2.029 [1.495, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.238, 10.100], loss: 0.124458, mae: 0.340407, mean_q: 4.016371
 79375/100000: episode: 1514, duration: 0.457s, episode steps: 88, steps per second: 192, episode reward: 182.609, mean reward: 2.075 [1.499, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.569 [-0.611, 10.100], loss: 0.124700, mae: 0.332639, mean_q: 4.018625
 79466/100000: episode: 1515, duration: 0.482s, episode steps: 91, steps per second: 189, episode reward: 172.580, mean reward: 1.896 [1.465, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-2.143, 10.248], loss: 0.130977, mae: 0.337908, mean_q: 4.015115
 79557/100000: episode: 1516, duration: 0.478s, episode steps: 91, steps per second: 190, episode reward: 188.756, mean reward: 2.074 [1.460, 10.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-1.144, 10.222], loss: 0.134235, mae: 0.335614, mean_q: 4.005837
 79649/100000: episode: 1517, duration: 0.451s, episode steps: 92, steps per second: 204, episode reward: 174.110, mean reward: 1.893 [1.478, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-1.174, 10.100], loss: 0.128407, mae: 0.323995, mean_q: 4.008369
 79741/100000: episode: 1518, duration: 0.470s, episode steps: 92, steps per second: 196, episode reward: 180.410, mean reward: 1.961 [1.492, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-1.447, 10.304], loss: 0.114662, mae: 0.332902, mean_q: 3.973721
 79833/100000: episode: 1519, duration: 0.469s, episode steps: 92, steps per second: 196, episode reward: 166.472, mean reward: 1.809 [1.443, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-1.371, 10.100], loss: 0.142226, mae: 0.343101, mean_q: 4.005578
 79925/100000: episode: 1520, duration: 0.448s, episode steps: 92, steps per second: 205, episode reward: 190.709, mean reward: 2.073 [1.441, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.485, 10.286], loss: 0.132314, mae: 0.342677, mean_q: 3.983024
 79931/100000: episode: 1521, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 13.840, mean reward: 2.307 [2.144, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.326, 10.100], loss: 0.096892, mae: 0.324125, mean_q: 4.128796
 80023/100000: episode: 1522, duration: 0.476s, episode steps: 92, steps per second: 193, episode reward: 179.223, mean reward: 1.948 [1.449, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.455, 10.100], loss: 0.142064, mae: 0.331704, mean_q: 3.992367
 80119/100000: episode: 1523, duration: 0.490s, episode steps: 96, steps per second: 196, episode reward: 174.210, mean reward: 1.815 [1.485, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-1.917, 10.100], loss: 0.135460, mae: 0.338867, mean_q: 3.963115
 80125/100000: episode: 1524, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 12.932, mean reward: 2.155 [1.946, 2.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.383, 10.100], loss: 0.090003, mae: 0.311687, mean_q: 4.000406
 80146/100000: episode: 1525, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 55.885, mean reward: 2.661 [1.602, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.106, 10.100], loss: 0.120972, mae: 0.331076, mean_q: 3.976773
 80246/100000: episode: 1526, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.180, mean reward: 1.952 [1.446, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.453, 10.100], loss: 0.124742, mae: 0.323845, mean_q: 3.976956
 80265/100000: episode: 1527, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 162.828, mean reward: 8.570 [2.600, 69.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.267, 10.100], loss: 0.160450, mae: 0.343422, mean_q: 3.954776
 80365/100000: episode: 1528, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.265, mean reward: 1.823 [1.526, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.931, 10.100], loss: 0.109761, mae: 0.324091, mean_q: 3.959910
 80456/100000: episode: 1529, duration: 0.464s, episode steps: 91, steps per second: 196, episode reward: 199.991, mean reward: 2.198 [1.471, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-0.508, 10.404], loss: 0.129514, mae: 0.320374, mean_q: 3.978511
 80462/100000: episode: 1530, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 13.905, mean reward: 2.317 [1.960, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.190, 10.100], loss: 0.120835, mae: 0.330982, mean_q: 3.944651
 80481/100000: episode: 1531, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 70.926, mean reward: 3.733 [2.640, 5.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.629, 10.100], loss: 0.204681, mae: 0.376459, mean_q: 4.097645
 80577/100000: episode: 1532, duration: 0.516s, episode steps: 96, steps per second: 186, episode reward: 187.506, mean reward: 1.953 [1.466, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.441, 10.132], loss: 0.877529, mae: 0.404186, mean_q: 4.022950
 80669/100000: episode: 1533, duration: 0.463s, episode steps: 92, steps per second: 199, episode reward: 193.441, mean reward: 2.103 [1.469, 8.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-1.196, 10.100], loss: 0.266920, mae: 0.351124, mean_q: 4.012270
 80675/100000: episode: 1534, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 16.117, mean reward: 2.686 [2.177, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.270, 10.100], loss: 0.074114, mae: 0.275512, mean_q: 4.003990
 80681/100000: episode: 1535, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 19.918, mean reward: 3.320 [2.437, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.342, 10.100], loss: 0.160563, mae: 0.367775, mean_q: 4.083581
 80700/100000: episode: 1536, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 68.938, mean reward: 3.628 [2.457, 4.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.320, 10.100], loss: 0.102851, mae: 0.327582, mean_q: 4.037968
 80792/100000: episode: 1537, duration: 0.482s, episode steps: 92, steps per second: 191, episode reward: 182.139, mean reward: 1.980 [1.496, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-1.514, 10.100], loss: 0.296182, mae: 0.384773, mean_q: 4.046571
 80888/100000: episode: 1538, duration: 0.505s, episode steps: 96, steps per second: 190, episode reward: 178.989, mean reward: 1.864 [1.463, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.485, 10.100], loss: 0.924362, mae: 0.403008, mean_q: 4.037596
 80894/100000: episode: 1539, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 14.583, mean reward: 2.430 [2.266, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.064, 10.100], loss: 0.108289, mae: 0.341529, mean_q: 4.018155
[Info] 2-TH LEVEL FOUND: 7.529191493988037, Considering 10/90 traces
 80986/100000: episode: 1540, duration: 4.494s, episode steps: 92, steps per second: 20, episode reward: 183.736, mean reward: 1.997 [1.483, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-0.650, 10.100], loss: 0.118070, mae: 0.319754, mean_q: 4.005427
 80999/100000: episode: 1541, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 66.230, mean reward: 5.095 [3.557, 15.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.309, 10.100], loss: 0.138155, mae: 0.332027, mean_q: 3.976062
 81012/100000: episode: 1542, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 56.121, mean reward: 4.317 [2.846, 6.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.169, 10.100], loss: 5.538300, mae: 0.619837, mean_q: 4.236369
 81028/100000: episode: 1543, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 71.427, mean reward: 4.464 [3.038, 7.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.790, 10.100], loss: 0.320206, mae: 0.415764, mean_q: 3.934503
 81047/100000: episode: 1544, duration: 0.111s, episode steps: 19, steps per second: 170, episode reward: 49.854, mean reward: 2.624 [1.970, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.447, 10.100], loss: 0.237388, mae: 0.401381, mean_q: 4.122898
 81065/100000: episode: 1545, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 50.364, mean reward: 2.798 [1.994, 4.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.372, 10.100], loss: 0.158166, mae: 0.335896, mean_q: 4.074646
 81082/100000: episode: 1546, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 58.594, mean reward: 3.447 [2.630, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.852, 10.100], loss: 0.881841, mae: 0.407170, mean_q: 4.110842
 81098/100000: episode: 1547, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 126.273, mean reward: 7.892 [2.979, 39.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.767, 10.100], loss: 0.139971, mae: 0.360276, mean_q: 4.040483
 81114/100000: episode: 1548, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 283.032, mean reward: 17.689 [3.560, 191.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.536, 10.100], loss: 0.134890, mae: 0.351827, mean_q: 4.129163
 81132/100000: episode: 1549, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 66.592, mean reward: 3.700 [2.786, 5.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.309, 10.100], loss: 30.370781, mae: 0.994023, mean_q: 4.509994
 81148/100000: episode: 1550, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 57.966, mean reward: 3.623 [2.949, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.764, 10.100], loss: 0.610147, mae: 0.810485, mean_q: 3.892616
 81165/100000: episode: 1551, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 50.926, mean reward: 2.996 [2.438, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.149, 10.100], loss: 0.350029, mae: 0.484695, mean_q: 4.215997
 81181/100000: episode: 1552, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 37.729, mean reward: 2.358 [1.865, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.355, 10.100], loss: 0.265227, mae: 0.410505, mean_q: 4.106858
 81197/100000: episode: 1553, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 64.080, mean reward: 4.005 [3.190, 6.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.387, 10.100], loss: 0.230287, mae: 0.418506, mean_q: 4.142175
 81210/100000: episode: 1554, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 51.865, mean reward: 3.990 [3.081, 4.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.319, 10.100], loss: 0.111241, mae: 0.340969, mean_q: 4.215989
 81225/100000: episode: 1555, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 80.438, mean reward: 5.363 [3.291, 9.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.315, 10.100], loss: 0.914660, mae: 0.409074, mean_q: 4.190331
 81242/100000: episode: 1556, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 60.363, mean reward: 3.551 [2.915, 5.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.515, 10.100], loss: 0.155222, mae: 0.355715, mean_q: 4.079300
 81255/100000: episode: 1557, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 58.710, mean reward: 4.516 [2.735, 13.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.364, 10.100], loss: 0.203683, mae: 0.393713, mean_q: 4.220294
 81274/100000: episode: 1558, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 75.922, mean reward: 3.996 [2.971, 4.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.421, 10.100], loss: 0.194496, mae: 0.379169, mean_q: 4.196695
 81287/100000: episode: 1559, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 49.355, mean reward: 3.797 [2.816, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.423, 10.100], loss: 0.158757, mae: 0.352497, mean_q: 4.091735
 81304/100000: episode: 1560, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 80.484, mean reward: 4.734 [2.897, 7.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.374, 10.100], loss: 4.189338, mae: 0.519333, mean_q: 4.300629
 81319/100000: episode: 1561, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 56.628, mean reward: 3.775 [2.718, 6.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.463, 10.100], loss: 0.284470, mae: 0.471591, mean_q: 4.309400
 81332/100000: episode: 1562, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 40.914, mean reward: 3.147 [2.669, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.302, 10.100], loss: 0.184906, mae: 0.397312, mean_q: 4.339139
 81345/100000: episode: 1563, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 54.103, mean reward: 4.162 [3.127, 10.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.223, 10.100], loss: 0.273341, mae: 0.400335, mean_q: 4.243784
 81358/100000: episode: 1564, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 80.777, mean reward: 6.214 [3.537, 10.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.757, 10.100], loss: 0.200834, mae: 0.391644, mean_q: 4.142118
 81374/100000: episode: 1565, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 47.017, mean reward: 2.939 [1.969, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.287, 10.100], loss: 0.571689, mae: 0.476373, mean_q: 4.360307
 81392/100000: episode: 1566, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 57.258, mean reward: 3.181 [1.812, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.431, 10.100], loss: 30.231554, mae: 1.216083, mean_q: 4.949034
 81411/100000: episode: 1567, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 120.413, mean reward: 6.338 [3.102, 18.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.599, 10.100], loss: 4.231833, mae: 0.821408, mean_q: 4.262744
 81428/100000: episode: 1568, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 57.204, mean reward: 3.365 [2.776, 4.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.431, 10.100], loss: 0.700842, mae: 0.530279, mean_q: 4.331864
 81441/100000: episode: 1569, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 38.268, mean reward: 2.944 [2.067, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.297, 10.100], loss: 1.809824, mae: 0.584353, mean_q: 4.472865
 81456/100000: episode: 1570, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 65.402, mean reward: 4.360 [2.554, 7.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.310, 10.100], loss: 4.744256, mae: 0.642135, mean_q: 4.532306
 81469/100000: episode: 1571, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 37.368, mean reward: 2.874 [2.559, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.329, 10.100], loss: 0.481373, mae: 0.448256, mean_q: 4.229499
 81485/100000: episode: 1572, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 104.739, mean reward: 6.546 [3.194, 12.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.362, 10.100], loss: 0.289708, mae: 0.473268, mean_q: 4.587836
 81502/100000: episode: 1573, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 69.700, mean reward: 4.100 [2.609, 8.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.578, 10.100], loss: 0.220949, mae: 0.408635, mean_q: 4.476333
 81519/100000: episode: 1574, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 54.467, mean reward: 3.204 [2.225, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.358, 10.100], loss: 0.261188, mae: 0.422054, mean_q: 4.481620
 81532/100000: episode: 1575, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 41.566, mean reward: 3.197 [2.547, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.428, 10.100], loss: 5.506134, mae: 0.579163, mean_q: 4.506582
 81549/100000: episode: 1576, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 56.243, mean reward: 3.308 [2.426, 6.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.382, 10.100], loss: 0.374624, mae: 0.500400, mean_q: 4.596695
 81566/100000: episode: 1577, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 68.442, mean reward: 4.026 [3.110, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.692, 10.100], loss: 0.486643, mae: 0.491351, mean_q: 4.503927
 81582/100000: episode: 1578, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 70.749, mean reward: 4.422 [2.754, 6.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.609, 10.100], loss: 0.260202, mae: 0.399320, mean_q: 4.378321
 81599/100000: episode: 1579, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 60.562, mean reward: 3.562 [2.758, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.411, 10.100], loss: 0.400439, mae: 0.456252, mean_q: 4.406393
 81618/100000: episode: 1580, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 53.939, mean reward: 2.839 [2.080, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.587, 10.100], loss: 28.200586, mae: 0.968644, mean_q: 4.917235
 81631/100000: episode: 1581, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 47.041, mean reward: 3.619 [2.714, 4.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.300, 10.100], loss: 0.511470, mae: 0.668440, mean_q: 4.286397
 81648/100000: episode: 1582, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 45.671, mean reward: 2.687 [2.117, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.348, 10.100], loss: 8.726925, mae: 0.916350, mean_q: 4.893915
 81666/100000: episode: 1583, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 65.821, mean reward: 3.657 [2.392, 6.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.352, 10.100], loss: 29.634443, mae: 1.026249, mean_q: 4.874650
 81684/100000: episode: 1584, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 76.479, mean reward: 4.249 [2.665, 7.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.635, 10.100], loss: 0.309453, mae: 0.513197, mean_q: 4.391446
 81701/100000: episode: 1585, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 113.771, mean reward: 6.692 [3.319, 12.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.548, 10.100], loss: 0.716093, mae: 0.592920, mean_q: 4.713224
 81720/100000: episode: 1586, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 74.041, mean reward: 3.897 [2.997, 6.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.297, 10.100], loss: 0.881016, mae: 0.473298, mean_q: 4.556555
 81733/100000: episode: 1587, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 53.953, mean reward: 4.150 [3.125, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.323, 10.100], loss: 0.320380, mae: 0.444197, mean_q: 4.568327
 81746/100000: episode: 1588, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 43.284, mean reward: 3.330 [2.547, 5.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.412, 10.100], loss: 0.292902, mae: 0.458827, mean_q: 4.560932
 81759/100000: episode: 1589, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 34.188, mean reward: 2.630 [1.951, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.187, 10.100], loss: 40.378643, mae: 0.974084, mean_q: 4.796864
 81776/100000: episode: 1590, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 84.519, mean reward: 4.972 [3.193, 7.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.426, 10.100], loss: 0.642975, mae: 0.706519, mean_q: 4.676767
 81789/100000: episode: 1591, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 49.140, mean reward: 3.780 [2.990, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.366, 10.100], loss: 40.341866, mae: 1.094828, mean_q: 4.851552
 81804/100000: episode: 1592, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 65.596, mean reward: 4.373 [2.759, 6.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.487, 10.100], loss: 0.676261, mae: 0.659115, mean_q: 4.855145
 81817/100000: episode: 1593, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 45.718, mean reward: 3.517 [2.743, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.339, 10.100], loss: 0.345073, mae: 0.479640, mean_q: 4.498404
 81836/100000: episode: 1594, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 57.747, mean reward: 3.039 [2.665, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.232, 10.100], loss: 27.558321, mae: 0.868916, mean_q: 4.818631
 81854/100000: episode: 1595, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 117.788, mean reward: 6.544 [4.409, 8.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.552, 10.100], loss: 1.413551, mae: 0.754281, mean_q: 5.105715
 81871/100000: episode: 1596, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 88.818, mean reward: 5.225 [2.924, 26.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.399, 10.100], loss: 1.131075, mae: 0.549855, mean_q: 4.765967
 81884/100000: episode: 1597, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 47.907, mean reward: 3.685 [3.193, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.290, 10.100], loss: 2.759969, mae: 0.739736, mean_q: 4.925752
 81897/100000: episode: 1598, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 46.600, mean reward: 3.585 [2.557, 5.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.930, 10.100], loss: 0.424204, mae: 0.488185, mean_q: 4.790983
 81910/100000: episode: 1599, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 46.970, mean reward: 3.613 [2.922, 5.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.484, 10.100], loss: 0.253275, mae: 0.440782, mean_q: 4.588540
 81927/100000: episode: 1600, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 64.644, mean reward: 3.803 [2.670, 4.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.549, 10.100], loss: 0.443676, mae: 0.543106, mean_q: 4.890460
 81944/100000: episode: 1601, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 59.056, mean reward: 3.474 [2.534, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.714, 10.100], loss: 4.387203, mae: 0.600671, mean_q: 4.663472
 81960/100000: episode: 1602, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 52.985, mean reward: 3.312 [2.537, 7.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.370, 10.100], loss: 0.402904, mae: 0.524476, mean_q: 4.907959
 81973/100000: episode: 1603, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 44.823, mean reward: 3.448 [2.472, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.212, 10.100], loss: 0.376092, mae: 0.480222, mean_q: 4.709672
 81990/100000: episode: 1604, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 55.905, mean reward: 3.289 [2.546, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.364, 10.100], loss: 0.389251, mae: 0.515168, mean_q: 4.894558
 82007/100000: episode: 1605, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 71.168, mean reward: 4.186 [2.891, 8.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.480, 10.100], loss: 0.220316, mae: 0.419378, mean_q: 4.775269
 82024/100000: episode: 1606, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 62.571, mean reward: 3.681 [2.885, 5.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.382, 10.100], loss: 0.385727, mae: 0.492742, mean_q: 4.843741
 82041/100000: episode: 1607, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 54.441, mean reward: 3.202 [2.324, 5.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.458, 10.100], loss: 31.126030, mae: 0.959934, mean_q: 5.023454
 82057/100000: episode: 1608, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 71.146, mean reward: 4.447 [3.068, 8.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.474, 10.100], loss: 4.795698, mae: 0.671742, mean_q: 4.766098
 82072/100000: episode: 1609, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 56.914, mean reward: 3.794 [2.704, 7.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.468, 10.100], loss: 1.316082, mae: 0.695516, mean_q: 5.263653
 82089/100000: episode: 1610, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 65.204, mean reward: 3.836 [2.698, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.505, 10.100], loss: 1.047568, mae: 0.600810, mean_q: 4.815335
 82105/100000: episode: 1611, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 99.520, mean reward: 6.220 [3.329, 11.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.901, 10.100], loss: 5.433727, mae: 0.732927, mean_q: 4.929549
 82123/100000: episode: 1612, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 71.788, mean reward: 3.988 [2.567, 7.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.189, 10.100], loss: 1.603919, mae: 0.578065, mean_q: 4.924173
 82139/100000: episode: 1613, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 63.089, mean reward: 3.943 [3.310, 5.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.349, 10.100], loss: 0.675975, mae: 0.567749, mean_q: 4.959306
 82158/100000: episode: 1614, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 84.863, mean reward: 4.466 [3.524, 5.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.931, 10.100], loss: 1.421524, mae: 0.623154, mean_q: 5.052734
 82173/100000: episode: 1615, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 53.497, mean reward: 3.566 [2.190, 5.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.324, 10.100], loss: 0.607579, mae: 0.553605, mean_q: 4.920400
 82189/100000: episode: 1616, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 66.345, mean reward: 4.147 [2.615, 8.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.317, 10.100], loss: 4.780601, mae: 0.725668, mean_q: 5.151349
 82202/100000: episode: 1617, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 49.209, mean reward: 3.785 [2.933, 6.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.460, 10.100], loss: 40.515118, mae: 1.014675, mean_q: 5.052920
 82217/100000: episode: 1618, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 44.816, mean reward: 2.988 [2.539, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.796, 10.100], loss: 0.906336, mae: 0.890835, mean_q: 5.468103
 82235/100000: episode: 1619, duration: 0.135s, episode steps: 18, steps per second: 133, episode reward: 69.187, mean reward: 3.844 [2.570, 5.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.769, 10.100], loss: 0.385625, mae: 0.566221, mean_q: 4.673820
 82248/100000: episode: 1620, duration: 0.113s, episode steps: 13, steps per second: 115, episode reward: 54.566, mean reward: 4.197 [3.429, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.576, 10.100], loss: 0.772677, mae: 0.595741, mean_q: 4.997695
 82263/100000: episode: 1621, duration: 0.143s, episode steps: 15, steps per second: 105, episode reward: 84.011, mean reward: 5.601 [3.042, 14.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.305, 10.100], loss: 1.330232, mae: 0.584495, mean_q: 5.049736
 82281/100000: episode: 1622, duration: 0.142s, episode steps: 18, steps per second: 127, episode reward: 73.479, mean reward: 4.082 [2.994, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.502, 10.100], loss: 0.403509, mae: 0.526468, mean_q: 4.904343
 82298/100000: episode: 1623, duration: 0.113s, episode steps: 17, steps per second: 150, episode reward: 50.556, mean reward: 2.974 [2.295, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.407, 10.100], loss: 0.512397, mae: 0.599320, mean_q: 5.180892
 82316/100000: episode: 1624, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 69.115, mean reward: 3.840 [2.522, 5.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.391, 10.100], loss: 0.469200, mae: 0.510354, mean_q: 4.901071
 82333/100000: episode: 1625, duration: 0.189s, episode steps: 17, steps per second: 90, episode reward: 67.641, mean reward: 3.979 [2.880, 6.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.560, 10.100], loss: 31.580416, mae: 1.183992, mean_q: 5.499316
 82346/100000: episode: 1626, duration: 0.126s, episode steps: 13, steps per second: 103, episode reward: 40.795, mean reward: 3.138 [2.540, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.201, 10.100], loss: 1.868110, mae: 0.706024, mean_q: 4.915251
 82364/100000: episode: 1627, duration: 0.265s, episode steps: 18, steps per second: 68, episode reward: 60.796, mean reward: 3.378 [2.562, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.996, 10.100], loss: 0.459567, mae: 0.578732, mean_q: 5.188336
 82383/100000: episode: 1628, duration: 0.219s, episode steps: 19, steps per second: 87, episode reward: 64.934, mean reward: 3.418 [2.291, 5.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.276, 10.100], loss: 1.652300, mae: 0.601823, mean_q: 4.995908
 82402/100000: episode: 1629, duration: 0.173s, episode steps: 19, steps per second: 110, episode reward: 60.849, mean reward: 3.203 [2.548, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.405, 10.100], loss: 1.479039, mae: 0.614904, mean_q: 4.968043
[Info] 3-TH LEVEL FOUND: 10.4935941696167, Considering 10/90 traces
 82415/100000: episode: 1630, duration: 7.097s, episode steps: 13, steps per second: 2, episode reward: 49.674, mean reward: 3.821 [2.958, 5.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.285, 10.100], loss: 5.776110, mae: 0.811062, mean_q: 5.421910
 82422/100000: episode: 1631, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 23.977, mean reward: 3.425 [3.025, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.476, 10.100], loss: 0.655572, mae: 0.586101, mean_q: 5.030338
 82434/100000: episode: 1632, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 98.144, mean reward: 8.179 [4.359, 15.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.447, 10.100], loss: 0.575915, mae: 0.486451, mean_q: 4.881307
 82435/100000: episode: 1633, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 6.504, mean reward: 6.504 [6.504, 6.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.574, 10.200], loss: 0.347267, mae: 0.582994, mean_q: 4.789326
 82442/100000: episode: 1634, duration: 0.054s, episode steps: 7, steps per second: 129, episode reward: 23.158, mean reward: 3.308 [2.848, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.497, 10.100], loss: 0.302023, mae: 0.475499, mean_q: 5.045820
 82445/100000: episode: 1635, duration: 0.029s, episode steps: 3, steps per second: 104, episode reward: 13.537, mean reward: 4.512 [3.981, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.628, 10.100], loss: 0.348439, mae: 0.512197, mean_q: 5.471806
 82446/100000: episode: 1636, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 9.027, mean reward: 9.027 [9.027, 9.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.537, 10.100], loss: 519.611389, mae: 6.088719, mean_q: 4.791515
 82458/100000: episode: 1637, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 153.134, mean reward: 12.761 [7.544, 23.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.581, 10.100], loss: 1.173668, mae: 0.964789, mean_q: 5.716699
 82461/100000: episode: 1638, duration: 0.026s, episode steps: 3, steps per second: 115, episode reward: 24.169, mean reward: 8.056 [6.376, 10.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.567, 10.100], loss: 0.416518, mae: 0.610690, mean_q: 4.302555
 82473/100000: episode: 1639, duration: 0.096s, episode steps: 12, steps per second: 124, episode reward: 65.448, mean reward: 5.454 [3.486, 11.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.609, 10.100], loss: 0.507291, mae: 0.602151, mean_q: 4.959371
 82477/100000: episode: 1640, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 29.815, mean reward: 7.454 [6.710, 8.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.873, 10.100], loss: 0.743054, mae: 0.642886, mean_q: 5.134877
 82481/100000: episode: 1641, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 24.071, mean reward: 6.018 [4.760, 6.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.632, 10.100], loss: 1.040938, mae: 0.624348, mean_q: 5.399210
 82482/100000: episode: 1642, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 4.598, mean reward: 4.598 [4.598, 4.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.568, 10.100], loss: 0.138145, mae: 0.357530, mean_q: 5.037459
 82483/100000: episode: 1643, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 3.632, mean reward: 3.632 [3.632, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.538, 10.100], loss: 0.382817, mae: 0.596752, mean_q: 5.080396
 82484/100000: episode: 1644, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 4.577, mean reward: 4.577 [4.577, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.622, 10.100], loss: 0.348040, mae: 0.486542, mean_q: 4.823417
 82488/100000: episode: 1645, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 31.772, mean reward: 7.943 [5.639, 9.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.580, 10.100], loss: 0.279104, mae: 0.432074, mean_q: 4.835129
 82491/100000: episode: 1646, duration: 0.029s, episode steps: 3, steps per second: 105, episode reward: 25.538, mean reward: 8.513 [6.163, 9.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.604, 10.100], loss: 25.633085, mae: 1.417969, mean_q: 5.358479
 82498/100000: episode: 1647, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 32.045, mean reward: 4.578 [3.782, 5.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.615, 10.100], loss: 0.465480, mae: 0.616182, mean_q: 5.311873
 82510/100000: episode: 1648, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 88.785, mean reward: 7.399 [5.291, 13.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.319, 10.100], loss: 0.961615, mae: 0.642351, mean_q: 5.187960
 82522/100000: episode: 1649, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 84.167, mean reward: 7.014 [5.222, 9.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.288, 10.100], loss: 0.647570, mae: 0.622105, mean_q: 5.311115
 82526/100000: episode: 1650, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 19.602, mean reward: 4.901 [4.577, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.697, 10.100], loss: 18.010075, mae: 1.263192, mean_q: 5.611947
 82530/100000: episode: 1651, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 43.959, mean reward: 10.990 [7.569, 14.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.590, 10.100], loss: 1.077258, mae: 0.708673, mean_q: 5.278186
 82531/100000: episode: 1652, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 4.033, mean reward: 4.033 [4.033, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.597, 10.100], loss: 0.330431, mae: 0.576452, mean_q: 6.010743
 82535/100000: episode: 1653, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 25.511, mean reward: 6.378 [4.320, 7.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.497, 10.100], loss: 1.003522, mae: 0.766081, mean_q: 5.463234
 82547/100000: episode: 1654, duration: 0.091s, episode steps: 12, steps per second: 131, episode reward: 75.573, mean reward: 6.298 [3.938, 10.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.512, 10.100], loss: 1.182890, mae: 0.670423, mean_q: 5.039667
 82548/100000: episode: 1655, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 4.231, mean reward: 4.231 [4.231, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.562, 10.100], loss: 1.919868, mae: 0.989012, mean_q: 5.464833
 82549/100000: episode: 1656, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 8.066, mean reward: 8.066 [8.066, 8.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.571, 10.200], loss: 0.321200, mae: 0.602095, mean_q: 5.633525
 82556/100000: episode: 1657, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 32.612, mean reward: 4.659 [3.200, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.518, 10.100], loss: 1.314461, mae: 0.799081, mean_q: 5.672973
 82566/100000: episode: 1658, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 43.960, mean reward: 4.396 [3.020, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.440, 10.100], loss: 0.643921, mae: 0.593900, mean_q: 5.266588
 82576/100000: episode: 1659, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 45.152, mean reward: 4.515 [3.480, 6.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.556, 10.100], loss: 0.528495, mae: 0.572954, mean_q: 5.065211
 82588/100000: episode: 1660, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 197.011, mean reward: 16.418 [4.841, 100.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.504, 10.100], loss: 0.584123, mae: 0.638564, mean_q: 5.101130
 82592/100000: episode: 1661, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 44.892, mean reward: 11.223 [8.866, 13.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.581, 10.100], loss: 0.773736, mae: 0.706245, mean_q: 5.863061
 82596/100000: episode: 1662, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 52.549, mean reward: 13.137 [5.999, 24.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.515, 10.100], loss: 0.759540, mae: 0.697306, mean_q: 5.458519
 82600/100000: episode: 1663, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 56.397, mean reward: 14.099 [4.213, 28.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.685, 10.100], loss: 0.932178, mae: 0.664683, mean_q: 5.394389
 82601/100000: episode: 1664, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 4.067, mean reward: 4.067 [4.067, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.628, 10.100], loss: 0.528879, mae: 0.558332, mean_q: 4.742087
 82608/100000: episode: 1665, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 39.006, mean reward: 5.572 [3.481, 7.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.580, 10.100], loss: 0.821075, mae: 0.721304, mean_q: 5.368036
 82609/100000: episode: 1666, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 3.692, mean reward: 3.692 [3.692, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.563, 10.100], loss: 0.165353, mae: 0.469286, mean_q: 5.082383
 82619/100000: episode: 1667, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 58.399, mean reward: 5.840 [4.682, 6.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.520, 10.100], loss: 8.825336, mae: 0.966399, mean_q: 5.224890
 82631/100000: episode: 1668, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 117.261, mean reward: 9.772 [5.696, 21.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.580, 10.100], loss: 2.210598, mae: 0.818978, mean_q: 5.757115
 82638/100000: episode: 1669, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 36.932, mean reward: 5.276 [3.720, 7.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.554, 10.100], loss: 1.219161, mae: 0.674920, mean_q: 5.140532
 82639/100000: episode: 1670, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 4.029, mean reward: 4.029 [4.029, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.561, 10.100], loss: 0.436488, mae: 0.524330, mean_q: 5.518650
 82651/100000: episode: 1671, duration: 0.139s, episode steps: 12, steps per second: 86, episode reward: 84.413, mean reward: 7.034 [3.757, 18.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.195, 10.100], loss: 0.694613, mae: 0.671653, mean_q: 5.486087
 82661/100000: episode: 1672, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 48.866, mean reward: 4.887 [3.035, 6.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.344, 10.100], loss: 0.898634, mae: 0.656783, mean_q: 5.579714
 82668/100000: episode: 1673, duration: 0.079s, episode steps: 7, steps per second: 89, episode reward: 31.685, mean reward: 4.526 [3.977, 5.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.470, 10.100], loss: 0.872147, mae: 0.675404, mean_q: 5.439051
 82674/100000: episode: 1674, duration: 0.048s, episode steps: 6, steps per second: 126, episode reward: 36.383, mean reward: 6.064 [4.981, 7.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.503, 10.100], loss: 0.471562, mae: 0.573540, mean_q: 5.300579
 82677/100000: episode: 1675, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 20.793, mean reward: 6.931 [4.844, 10.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.478, 10.100], loss: 0.532798, mae: 0.602043, mean_q: 5.129353
 82678/100000: episode: 1676, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 9.301, mean reward: 9.301 [9.301, 9.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.577, 10.200], loss: 2.570091, mae: 1.113315, mean_q: 5.700315
 82688/100000: episode: 1677, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 147.347, mean reward: 14.735 [4.505, 31.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.026, 10.100], loss: 1.065947, mae: 0.696346, mean_q: 5.215157
 82692/100000: episode: 1678, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 29.395, mean reward: 7.349 [6.274, 8.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.955, 10.100], loss: 0.743388, mae: 0.612750, mean_q: 5.355736
 82696/100000: episode: 1679, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 25.223, mean reward: 6.306 [4.873, 8.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.566, 10.100], loss: 1.336475, mae: 0.897112, mean_q: 5.785811
 82703/100000: episode: 1680, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 31.245, mean reward: 4.464 [4.009, 5.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.408, 10.100], loss: 11.476438, mae: 1.259532, mean_q: 5.990051
 82704/100000: episode: 1681, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 3.933, mean reward: 3.933 [3.933, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.592, 10.100], loss: 3.829047, mae: 1.409733, mean_q: 6.578609
 82714/100000: episode: 1682, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 60.357, mean reward: 6.036 [4.856, 7.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.572, 10.100], loss: 1.086402, mae: 0.746495, mean_q: 5.448089
 82715/100000: episode: 1683, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 9.971, mean reward: 9.971 [9.971, 9.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.537, 10.200], loss: 0.205568, mae: 0.429760, mean_q: 4.936254
[Info] FALSIFICATION!
 82717/100000: episode: 1684, duration: 0.282s, episode steps: 2, steps per second: 7, episode reward: 1024.789, mean reward: 512.395 [24.789, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.523, 10.093], loss: 0.411457, mae: 0.625144, mean_q: 5.502779
 82721/100000: episode: 1685, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 24.198, mean reward: 6.049 [5.286, 6.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.511, 10.100], loss: 0.963855, mae: 0.738064, mean_q: 5.373164
 82731/100000: episode: 1686, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 60.815, mean reward: 6.081 [5.128, 7.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.473, 10.100], loss: 0.758875, mae: 0.642446, mean_q: 5.355989
 82732/100000: episode: 1687, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 15.706, mean reward: 15.706 [15.706, 15.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.537, 10.200], loss: 1.085699, mae: 0.905755, mean_q: 5.685736
 82744/100000: episode: 1688, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 163.187, mean reward: 13.599 [8.170, 22.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.521, 10.100], loss: 1.732602, mae: 0.756320, mean_q: 5.464319
 82745/100000: episode: 1689, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 4.748, mean reward: 4.748 [4.748, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.588, 10.100], loss: 0.423488, mae: 0.624967, mean_q: 5.708636
 82746/100000: episode: 1690, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 7.040, mean reward: 7.040 [7.040, 7.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.577, 10.200], loss: 0.605196, mae: 0.699738, mean_q: 5.885964
 82747/100000: episode: 1691, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 4.210, mean reward: 4.210 [4.210, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.612, 10.100], loss: 0.891492, mae: 1.016264, mean_q: 6.782024
 82757/100000: episode: 1692, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 68.068, mean reward: 6.807 [4.424, 12.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.451, 10.100], loss: 1532.245850, mae: 4.478932, mean_q: 6.690657
 82761/100000: episode: 1693, duration: 0.033s, episode steps: 4, steps per second: 122, episode reward: 22.121, mean reward: 5.530 [5.152, 6.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.563, 10.100], loss: 12.648063, mae: 3.819204, mean_q: 9.611219
 82765/100000: episode: 1694, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 34.904, mean reward: 8.726 [6.627, 13.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.621, 10.100], loss: 4.291534, mae: 1.965248, mean_q: 7.251686
 82775/100000: episode: 1695, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 50.794, mean reward: 5.079 [4.012, 10.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.407, 10.100], loss: 1.474969, mae: 1.093452, mean_q: 5.233896
 82776/100000: episode: 1696, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 3.985, mean reward: 3.985 [3.985, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.559, 10.100], loss: 0.820986, mae: 1.093717, mean_q: 4.116671
 82783/100000: episode: 1697, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 30.651, mean reward: 4.379 [3.096, 8.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.438, 10.100], loss: 1.280508, mae: 1.012783, mean_q: 4.673721
 82789/100000: episode: 1698, duration: 0.049s, episode steps: 6, steps per second: 123, episode reward: 44.871, mean reward: 7.478 [4.494, 12.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.491, 10.100], loss: 1.401540, mae: 0.804018, mean_q: 5.636299
 82795/100000: episode: 1699, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 67.870, mean reward: 11.312 [4.785, 28.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.506, 10.100], loss: 2548.194092, mae: 6.561243, mean_q: 6.518984
 82805/100000: episode: 1700, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 66.514, mean reward: 6.651 [4.270, 10.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.576, 10.100], loss: 1520.005981, mae: 6.398967, mean_q: 9.322680
 82812/100000: episode: 1701, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 36.557, mean reward: 5.222 [3.367, 6.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.568, 10.100], loss: 10.472971, mae: 3.279192, mean_q: 9.113340
 82813/100000: episode: 1702, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 3.262, mean reward: 3.262 [3.262, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.533, 10.100], loss: 5.714180, mae: 2.290084, mean_q: 8.437664
 82814/100000: episode: 1703, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 3.727, mean reward: 3.727 [3.727, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.560, 10.100], loss: 2.027895, mae: 1.520977, mean_q: 6.757823
 82815/100000: episode: 1704, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 8.367, mean reward: 8.367 [8.367, 8.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.523, 10.200], loss: 4.586444, mae: 1.676066, mean_q: 6.262922
 82816/100000: episode: 1705, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 3.798, mean reward: 3.798 [3.798, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.627, 10.100], loss: 4.141576, mae: 1.792497, mean_q: 5.568771
 82819/100000: episode: 1706, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 35.018, mean reward: 11.673 [10.194, 14.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.553, 10.100], loss: 1.772500, mae: 1.134635, mean_q: 5.174799
 82825/100000: episode: 1707, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 22.734, mean reward: 3.789 [3.395, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.444, 10.100], loss: 2.177481, mae: 1.469149, mean_q: 5.162834
 82835/100000: episode: 1708, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 57.267, mean reward: 5.727 [2.832, 7.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.393, 10.100], loss: 8.444598, mae: 1.261610, mean_q: 4.971130
 82842/100000: episode: 1709, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 36.574, mean reward: 5.225 [3.905, 8.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.566, 10.100], loss: 74.387772, mae: 1.650547, mean_q: 5.782124
 82846/100000: episode: 1710, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 21.957, mean reward: 5.489 [4.776, 6.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.571, 10.100], loss: 1.164096, mae: 0.911261, mean_q: 6.525700
 82847/100000: episode: 1711, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 9.176, mean reward: 9.176 [9.176, 9.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.549, 10.200], loss: 1.576266, mae: 1.078560, mean_q: 6.433205
 82853/100000: episode: 1712, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 33.811, mean reward: 5.635 [4.235, 8.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.635, 10.100], loss: 13.651329, mae: 1.558347, mean_q: 6.772765
 82865/100000: episode: 1713, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 169.735, mean reward: 14.145 [8.831, 24.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.698, 10.100], loss: 13.023151, mae: 1.210985, mean_q: 6.319185
 82877/100000: episode: 1714, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 81.651, mean reward: 6.804 [4.717, 12.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.278, 10.100], loss: 1.816291, mae: 0.844966, mean_q: 5.764425
 82878/100000: episode: 1715, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 4.113, mean reward: 4.113 [4.113, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.562, 10.100], loss: 0.550129, mae: 0.659495, mean_q: 5.613844
 82882/100000: episode: 1716, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 31.165, mean reward: 7.791 [6.201, 10.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.482, 10.100], loss: 0.960241, mae: 0.729140, mean_q: 6.030373
 82886/100000: episode: 1717, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 21.353, mean reward: 5.338 [4.344, 6.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.609, 10.100], loss: 128.121506, mae: 2.112965, mean_q: 5.943725
 82892/100000: episode: 1718, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 29.803, mean reward: 4.967 [4.350, 5.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.461, 10.100], loss: 2542.312988, mae: 6.375908, mean_q: 6.753413
 82898/100000: episode: 1719, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 34.184, mean reward: 5.697 [3.996, 8.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.891, 10.100], loss: 6.272014, mae: 2.392526, mean_q: 7.905315
[Info] Complete ISplit Iteration
[Info] Levels: [4.668718, 7.5291915, 10.493594, 17.26588]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.65]
[Info] Error Prob: 0.0006500000000000002

 82899/100000: episode: 1720, duration: 5.119s, episode steps: 1, steps per second: 0, episode reward: 4.776, mean reward: 4.776 [4.776, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.513, 10.100], loss: 5.042324, mae: 2.222523, mean_q: 8.011684
 82999/100000: episode: 1721, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 211.463, mean reward: 2.115 [1.462, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.027, 10.098], loss: 4.134345, mae: 1.051338, mean_q: 6.198942
 83099/100000: episode: 1722, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 191.232, mean reward: 1.912 [1.503, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.204], loss: 8.922359, mae: 0.968157, mean_q: 6.120695
 83199/100000: episode: 1723, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 188.802, mean reward: 1.888 [1.441, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.882, 10.160], loss: 313.539734, mae: 1.985754, mean_q: 6.462531
 83299/100000: episode: 1724, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 194.461, mean reward: 1.945 [1.467, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.814, 10.098], loss: 165.225357, mae: 1.660016, mean_q: 6.644828
 83399/100000: episode: 1725, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: 188.389, mean reward: 1.884 [1.444, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.098], loss: 3.240115, mae: 0.921685, mean_q: 6.045770
 83499/100000: episode: 1726, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 191.741, mean reward: 1.917 [1.470, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.422, 10.100], loss: 163.928665, mae: 1.700361, mean_q: 6.616867
 83599/100000: episode: 1727, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 185.677, mean reward: 1.857 [1.495, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.766, 10.186], loss: 13.661716, mae: 1.030361, mean_q: 6.247049
 83699/100000: episode: 1728, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 234.509, mean reward: 2.345 [1.479, 8.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.803, 10.098], loss: 2.255919, mae: 0.914286, mean_q: 6.038248
 83799/100000: episode: 1729, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 179.436, mean reward: 1.794 [1.474, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.876, 10.131], loss: 308.056671, mae: 1.968112, mean_q: 6.625572
 83899/100000: episode: 1730, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 192.247, mean reward: 1.922 [1.482, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.076, 10.098], loss: 9.470272, mae: 1.049461, mean_q: 6.177938
 83999/100000: episode: 1731, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 189.394, mean reward: 1.894 [1.483, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.945, 10.176], loss: 8.186393, mae: 0.958078, mean_q: 5.982155
 84099/100000: episode: 1732, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 192.275, mean reward: 1.923 [1.466, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.299, 10.198], loss: 2.562728, mae: 0.905386, mean_q: 6.022374
 84199/100000: episode: 1733, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 182.205, mean reward: 1.822 [1.478, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.230, 10.155], loss: 309.109894, mae: 2.176030, mean_q: 6.704922
 84299/100000: episode: 1734, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 180.264, mean reward: 1.803 [1.468, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.183, 10.224], loss: 9.030892, mae: 0.998861, mean_q: 6.114641
 84399/100000: episode: 1735, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 197.580, mean reward: 1.976 [1.471, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 12.242200, mae: 1.105904, mean_q: 6.150831
 84499/100000: episode: 1736, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 187.532, mean reward: 1.875 [1.485, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.285, 10.201], loss: 312.043671, mae: 1.840848, mean_q: 6.415664
[Info] FALSIFICATION!
 84586/100000: episode: 1737, duration: 1.077s, episode steps: 87, steps per second: 81, episode reward: 1214.419, mean reward: 13.959 [1.474, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.311 [-1.295, 9.629], loss: 5.107502, mae: 1.221267, mean_q: 6.454885
 84686/100000: episode: 1738, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 246.153, mean reward: 2.462 [1.492, 5.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.102, 10.293], loss: 308.094940, mae: 2.141566, mean_q: 6.876304
 84786/100000: episode: 1739, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 180.818, mean reward: 1.808 [1.452, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.565, 10.112], loss: 616.116150, mae: 3.166621, mean_q: 7.534184
 84886/100000: episode: 1740, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.873, mean reward: 1.929 [1.440, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.150, 10.098], loss: 164.174744, mae: 1.514790, mean_q: 6.725011
 84986/100000: episode: 1741, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 204.265, mean reward: 2.043 [1.521, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.878, 10.211], loss: 162.462097, mae: 1.691711, mean_q: 6.885932
 85086/100000: episode: 1742, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 206.386, mean reward: 2.064 [1.544, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.116, 10.098], loss: 474.315063, mae: 2.571453, mean_q: 7.291402
 85186/100000: episode: 1743, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 193.044, mean reward: 1.930 [1.479, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.947, 10.174], loss: 3.492804, mae: 1.142798, mean_q: 6.782333
 85286/100000: episode: 1744, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.617, mean reward: 1.916 [1.491, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.205, 10.243], loss: 154.811371, mae: 1.447366, mean_q: 6.604317
 85386/100000: episode: 1745, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 201.723, mean reward: 2.017 [1.476, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.177, 10.146], loss: 8.247697, mae: 1.021926, mean_q: 6.153528
 85486/100000: episode: 1746, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 179.929, mean reward: 1.799 [1.469, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.140, 10.247], loss: 3.268658, mae: 0.892734, mean_q: 6.048693
 85586/100000: episode: 1747, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.227, mean reward: 1.872 [1.473, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.026, 10.236], loss: 165.271561, mae: 1.368007, mean_q: 6.066368
 85686/100000: episode: 1748, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.815, mean reward: 1.978 [1.475, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.342, 10.098], loss: 16.987028, mae: 1.133504, mean_q: 6.279246
 85786/100000: episode: 1749, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 195.607, mean reward: 1.956 [1.453, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.920, 10.098], loss: 324.931610, mae: 2.082454, mean_q: 6.589190
 85886/100000: episode: 1750, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 185.205, mean reward: 1.852 [1.446, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.603, 10.277], loss: 2.523222, mae: 0.998199, mean_q: 6.268771
 85986/100000: episode: 1751, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.884, mean reward: 1.879 [1.442, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.420, 10.126], loss: 918.182129, mae: 4.117260, mean_q: 7.822494
 86086/100000: episode: 1752, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 208.132, mean reward: 2.081 [1.454, 6.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.441], loss: 154.418518, mae: 1.563693, mean_q: 6.731578
 86186/100000: episode: 1753, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 183.017, mean reward: 1.830 [1.474, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.073, 10.338], loss: 609.802795, mae: 3.125282, mean_q: 7.304590
 86286/100000: episode: 1754, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.830, mean reward: 1.988 [1.499, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.465, 10.098], loss: 155.152283, mae: 1.506522, mean_q: 6.406684
 86386/100000: episode: 1755, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.752, mean reward: 1.938 [1.456, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.549, 10.446], loss: 2.178878, mae: 1.089813, mean_q: 6.177097
 86486/100000: episode: 1756, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 175.808, mean reward: 1.758 [1.464, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.394, 10.150], loss: 306.664307, mae: 1.879429, mean_q: 6.322239
 86586/100000: episode: 1757, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.980, mean reward: 1.930 [1.485, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.365, 10.098], loss: 2.146432, mae: 1.006021, mean_q: 5.799252
 86686/100000: episode: 1758, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 194.071, mean reward: 1.941 [1.508, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.193, 10.265], loss: 4.166203, mae: 0.874601, mean_q: 5.551058
 86786/100000: episode: 1759, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 200.786, mean reward: 2.008 [1.459, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.641, 10.098], loss: 309.832428, mae: 1.815096, mean_q: 5.804597
 86886/100000: episode: 1760, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 230.597, mean reward: 2.306 [1.463, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.913, 10.316], loss: 1.918032, mae: 0.896271, mean_q: 5.486865
 86986/100000: episode: 1761, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 183.040, mean reward: 1.830 [1.455, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.792, 10.098], loss: 155.623932, mae: 1.262362, mean_q: 5.521886
 87086/100000: episode: 1762, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 190.633, mean reward: 1.906 [1.454, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.260, 10.274], loss: 1.381470, mae: 0.699189, mean_q: 5.017341
 87186/100000: episode: 1763, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 179.191, mean reward: 1.792 [1.442, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.098], loss: 1.255181, mae: 0.655590, mean_q: 4.813404
 87286/100000: episode: 1764, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.543, mean reward: 1.925 [1.471, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.191], loss: 610.314392, mae: 2.441297, mean_q: 5.585325
 87386/100000: episode: 1765, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 181.070, mean reward: 1.811 [1.446, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.742, 10.187], loss: 306.690491, mae: 2.141550, mean_q: 6.020401
 87486/100000: episode: 1766, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 199.983, mean reward: 2.000 [1.467, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.128], loss: 305.572510, mae: 1.559037, mean_q: 5.268423
 87586/100000: episode: 1767, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.522, mean reward: 1.975 [1.459, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.079, 10.311], loss: 1.363814, mae: 0.744599, mean_q: 4.824048
 87686/100000: episode: 1768, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.397, mean reward: 1.814 [1.456, 2.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.224, 10.098], loss: 0.955850, mae: 0.595338, mean_q: 4.523321
 87786/100000: episode: 1769, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 186.310, mean reward: 1.863 [1.449, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.211, 10.310], loss: 153.284058, mae: 0.940002, mean_q: 4.416178
 87886/100000: episode: 1770, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 206.690, mean reward: 2.067 [1.447, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.899, 10.098], loss: 0.419712, mae: 0.476049, mean_q: 4.096780
 87986/100000: episode: 1771, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 275.010, mean reward: 2.750 [1.461, 6.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.146, 10.098], loss: 152.772675, mae: 0.918000, mean_q: 4.245294
 88086/100000: episode: 1772, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.759, mean reward: 1.858 [1.458, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.954, 10.098], loss: 0.445195, mae: 0.483911, mean_q: 4.100224
 88186/100000: episode: 1773, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 198.398, mean reward: 1.984 [1.477, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.800, 10.098], loss: 0.359928, mae: 0.419344, mean_q: 4.004595
 88286/100000: episode: 1774, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 201.312, mean reward: 2.013 [1.468, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.613, 10.358], loss: 0.240301, mae: 0.381482, mean_q: 3.972743
 88386/100000: episode: 1775, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.743, mean reward: 1.947 [1.455, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.550, 10.098], loss: 152.780090, mae: 0.743530, mean_q: 4.050663
 88486/100000: episode: 1776, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.020, mean reward: 1.970 [1.441, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.824, 10.116], loss: 151.982315, mae: 1.116068, mean_q: 4.464530
 88586/100000: episode: 1777, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 187.165, mean reward: 1.872 [1.500, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.276, 10.098], loss: 302.016296, mae: 1.650711, mean_q: 4.811067
 88686/100000: episode: 1778, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.211, mean reward: 1.852 [1.471, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.131, 10.098], loss: 0.534383, mae: 0.537869, mean_q: 4.170863
 88786/100000: episode: 1779, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 185.334, mean reward: 1.853 [1.459, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.628, 10.098], loss: 0.358512, mae: 0.428887, mean_q: 4.064254
 88886/100000: episode: 1780, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.513, mean reward: 1.795 [1.471, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.056, 10.118], loss: 0.238519, mae: 0.363227, mean_q: 3.958384
 88986/100000: episode: 1781, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 186.247, mean reward: 1.862 [1.473, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.258, 10.118], loss: 152.522202, mae: 0.802944, mean_q: 4.119928
 89086/100000: episode: 1782, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 181.437, mean reward: 1.814 [1.452, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.916, 10.102], loss: 151.553314, mae: 0.852572, mean_q: 4.138253
 89186/100000: episode: 1783, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.007, mean reward: 1.970 [1.463, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.438, 10.098], loss: 1.123510, mae: 0.654526, mean_q: 4.284891
 89286/100000: episode: 1784, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 200.265, mean reward: 2.003 [1.446, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.604, 10.098], loss: 151.742523, mae: 0.975364, mean_q: 4.303676
 89386/100000: episode: 1785, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 202.792, mean reward: 2.028 [1.463, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.784, 10.098], loss: 151.308640, mae: 0.840256, mean_q: 4.182180
 89486/100000: episode: 1786, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 189.843, mean reward: 1.898 [1.473, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.596, 10.224], loss: 1.186071, mae: 0.664970, mean_q: 4.284379
 89586/100000: episode: 1787, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.219, mean reward: 2.002 [1.449, 5.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.504, 10.126], loss: 0.382640, mae: 0.448359, mean_q: 4.023431
 89686/100000: episode: 1788, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 184.082, mean reward: 1.841 [1.437, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.098], loss: 0.278724, mae: 0.388403, mean_q: 4.002687
 89786/100000: episode: 1789, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.751, mean reward: 1.848 [1.431, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.741, 10.098], loss: 0.176675, mae: 0.338825, mean_q: 3.909933
 89886/100000: episode: 1790, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 200.878, mean reward: 2.009 [1.476, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.749, 10.098], loss: 0.179363, mae: 0.339791, mean_q: 3.885536
 89986/100000: episode: 1791, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.696, mean reward: 1.927 [1.451, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.048, 10.098], loss: 0.157570, mae: 0.329571, mean_q: 3.877957
 90086/100000: episode: 1792, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.352, mean reward: 1.904 [1.480, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.528, 10.098], loss: 0.146324, mae: 0.319946, mean_q: 3.865886
 90186/100000: episode: 1793, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 224.920, mean reward: 2.249 [1.477, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.675, 10.111], loss: 0.116711, mae: 0.311534, mean_q: 3.841388
 90286/100000: episode: 1794, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 181.211, mean reward: 1.812 [1.445, 2.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.098], loss: 0.145781, mae: 0.324143, mean_q: 3.852928
 90386/100000: episode: 1795, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.058, mean reward: 1.951 [1.492, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.629, 10.120], loss: 0.145994, mae: 0.333198, mean_q: 3.850916
 90486/100000: episode: 1796, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 186.501, mean reward: 1.865 [1.492, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.447, 10.367], loss: 0.127508, mae: 0.318518, mean_q: 3.851431
 90586/100000: episode: 1797, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.979, mean reward: 1.850 [1.480, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.136], loss: 0.125260, mae: 0.316141, mean_q: 3.848799
 90686/100000: episode: 1798, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.121, mean reward: 2.001 [1.501, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.625, 10.098], loss: 0.132034, mae: 0.329716, mean_q: 3.849575
 90786/100000: episode: 1799, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 185.515, mean reward: 1.855 [1.481, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.848, 10.189], loss: 0.115139, mae: 0.308588, mean_q: 3.828734
 90886/100000: episode: 1800, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 191.766, mean reward: 1.918 [1.477, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.537, 10.098], loss: 0.119643, mae: 0.309041, mean_q: 3.829263
 90986/100000: episode: 1801, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.142, mean reward: 1.941 [1.456, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.240], loss: 0.123836, mae: 0.311122, mean_q: 3.839956
 91086/100000: episode: 1802, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 197.327, mean reward: 1.973 [1.443, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.402, 10.098], loss: 0.141040, mae: 0.334957, mean_q: 3.851359
 91186/100000: episode: 1803, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 184.933, mean reward: 1.849 [1.438, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.098], loss: 0.126377, mae: 0.312019, mean_q: 3.826097
 91286/100000: episode: 1804, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.610, mean reward: 1.846 [1.469, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.098], loss: 0.107472, mae: 0.301755, mean_q: 3.830394
 91386/100000: episode: 1805, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.762, mean reward: 1.918 [1.438, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.164, 10.104], loss: 0.120821, mae: 0.311136, mean_q: 3.824645
 91486/100000: episode: 1806, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.683, mean reward: 1.987 [1.465, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.029, 10.274], loss: 0.114205, mae: 0.313246, mean_q: 3.841346
 91586/100000: episode: 1807, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 193.999, mean reward: 1.940 [1.476, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.281], loss: 0.128735, mae: 0.320895, mean_q: 3.842846
 91686/100000: episode: 1808, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.352, mean reward: 1.884 [1.469, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.290, 10.129], loss: 0.111410, mae: 0.308506, mean_q: 3.845847
 91786/100000: episode: 1809, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 194.216, mean reward: 1.942 [1.446, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.461], loss: 0.117398, mae: 0.305093, mean_q: 3.831359
 91886/100000: episode: 1810, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.528, mean reward: 1.975 [1.488, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.643, 10.160], loss: 0.115402, mae: 0.305993, mean_q: 3.820184
 91986/100000: episode: 1811, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 213.338, mean reward: 2.133 [1.433, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.057, 10.098], loss: 0.108120, mae: 0.304545, mean_q: 3.824318
 92086/100000: episode: 1812, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.526, mean reward: 1.885 [1.465, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.258], loss: 0.110357, mae: 0.302023, mean_q: 3.825345
 92186/100000: episode: 1813, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.621, mean reward: 1.976 [1.440, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.488, 10.100], loss: 0.111684, mae: 0.303409, mean_q: 3.838774
 92286/100000: episode: 1814, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 217.194, mean reward: 2.172 [1.500, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.834, 10.118], loss: 0.123031, mae: 0.312363, mean_q: 3.840024
 92386/100000: episode: 1815, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 222.641, mean reward: 2.226 [1.480, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.470], loss: 0.109815, mae: 0.301078, mean_q: 3.841883
 92486/100000: episode: 1816, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 192.378, mean reward: 1.924 [1.496, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.694, 10.178], loss: 0.109333, mae: 0.309130, mean_q: 3.847461
 92586/100000: episode: 1817, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.887, mean reward: 1.889 [1.477, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.320, 10.259], loss: 0.118019, mae: 0.317062, mean_q: 3.845934
 92686/100000: episode: 1818, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 202.404, mean reward: 2.024 [1.487, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.240], loss: 0.120098, mae: 0.316081, mean_q: 3.865815
 92786/100000: episode: 1819, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.190, mean reward: 1.932 [1.445, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.441], loss: 0.126482, mae: 0.325531, mean_q: 3.868267
[Info] Complete ISplit Iteration
[Info] Levels: [14.776713]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 92886/100000: episode: 1820, duration: 4.763s, episode steps: 100, steps per second: 21, episode reward: 200.429, mean reward: 2.004 [1.449, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.164, 10.172], loss: 0.139615, mae: 0.330433, mean_q: 3.876158
 92986/100000: episode: 1821, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.036, mean reward: 1.840 [1.447, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.401, 10.160], loss: 0.102501, mae: 0.297855, mean_q: 3.857682
 93086/100000: episode: 1822, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 204.439, mean reward: 2.044 [1.478, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.587, 10.098], loss: 0.100467, mae: 0.298849, mean_q: 3.848963
 93186/100000: episode: 1823, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 178.291, mean reward: 1.783 [1.450, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.155], loss: 0.111085, mae: 0.307651, mean_q: 3.858073
 93286/100000: episode: 1824, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 179.605, mean reward: 1.796 [1.442, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.659, 10.098], loss: 0.106353, mae: 0.309620, mean_q: 3.859509
 93386/100000: episode: 1825, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 184.364, mean reward: 1.844 [1.446, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.318, 10.098], loss: 0.105406, mae: 0.301703, mean_q: 3.850269
 93486/100000: episode: 1826, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 199.996, mean reward: 2.000 [1.553, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.285, 10.229], loss: 0.110891, mae: 0.311241, mean_q: 3.852964
 93586/100000: episode: 1827, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.623, mean reward: 1.886 [1.455, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.708, 10.238], loss: 0.098493, mae: 0.296485, mean_q: 3.828572
 93686/100000: episode: 1828, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.192, mean reward: 1.872 [1.453, 2.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.962, 10.098], loss: 0.105461, mae: 0.304045, mean_q: 3.843186
 93786/100000: episode: 1829, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.122, mean reward: 1.961 [1.470, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.395, 10.098], loss: 0.092223, mae: 0.292339, mean_q: 3.829011
 93886/100000: episode: 1830, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 180.881, mean reward: 1.809 [1.439, 2.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.209, 10.098], loss: 0.102373, mae: 0.305220, mean_q: 3.844524
 93986/100000: episode: 1831, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.347, mean reward: 1.833 [1.444, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.786, 10.223], loss: 0.098746, mae: 0.296553, mean_q: 3.837987
 94086/100000: episode: 1832, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.644, mean reward: 1.936 [1.471, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.143], loss: 0.103259, mae: 0.303811, mean_q: 3.845209
 94186/100000: episode: 1833, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.466, mean reward: 1.885 [1.486, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.091, 10.098], loss: 0.104820, mae: 0.307313, mean_q: 3.845071
 94286/100000: episode: 1834, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 187.136, mean reward: 1.871 [1.451, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.268], loss: 0.096345, mae: 0.298910, mean_q: 3.852525
 94386/100000: episode: 1835, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 186.789, mean reward: 1.868 [1.448, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.823, 10.220], loss: 0.086909, mae: 0.287504, mean_q: 3.842587
 94486/100000: episode: 1836, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.412, mean reward: 1.834 [1.444, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.813, 10.098], loss: 0.096073, mae: 0.294283, mean_q: 3.838849
 94586/100000: episode: 1837, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 209.388, mean reward: 2.094 [1.442, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.951, 10.154], loss: 0.093051, mae: 0.294347, mean_q: 3.834023
 94686/100000: episode: 1838, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.631, mean reward: 1.896 [1.446, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.168, 10.165], loss: 0.089517, mae: 0.298043, mean_q: 3.845652
 94786/100000: episode: 1839, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.717, mean reward: 1.947 [1.440, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.165], loss: 0.090626, mae: 0.287596, mean_q: 3.825709
 94886/100000: episode: 1840, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 203.667, mean reward: 2.037 [1.453, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.270, 10.098], loss: 0.100054, mae: 0.302612, mean_q: 3.846645
 94986/100000: episode: 1841, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.398, mean reward: 1.844 [1.447, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.197], loss: 0.088816, mae: 0.279977, mean_q: 3.821739
 95086/100000: episode: 1842, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 201.206, mean reward: 2.012 [1.500, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.986, 10.098], loss: 0.088412, mae: 0.281766, mean_q: 3.823876
 95186/100000: episode: 1843, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.946, mean reward: 1.869 [1.458, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.252, 10.098], loss: 0.094571, mae: 0.296019, mean_q: 3.837180
 95286/100000: episode: 1844, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 236.436, mean reward: 2.364 [1.456, 10.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.766, 10.366], loss: 0.096550, mae: 0.295495, mean_q: 3.819734
 95386/100000: episode: 1845, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 182.416, mean reward: 1.824 [1.455, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.195, 10.123], loss: 0.093121, mae: 0.296524, mean_q: 3.847211
 95486/100000: episode: 1846, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 195.509, mean reward: 1.955 [1.451, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.172, 10.098], loss: 0.086508, mae: 0.284269, mean_q: 3.821759
 95586/100000: episode: 1847, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 187.243, mean reward: 1.872 [1.459, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.249, 10.100], loss: 0.103492, mae: 0.306209, mean_q: 3.847030
 95686/100000: episode: 1848, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 193.817, mean reward: 1.938 [1.439, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.083, 10.098], loss: 0.121357, mae: 0.306967, mean_q: 3.862677
 95786/100000: episode: 1849, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 181.125, mean reward: 1.811 [1.435, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.012, 10.098], loss: 0.090731, mae: 0.291604, mean_q: 3.842265
 95886/100000: episode: 1850, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 180.025, mean reward: 1.800 [1.470, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.632, 10.165], loss: 0.099898, mae: 0.297018, mean_q: 3.849149
 95986/100000: episode: 1851, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 227.524, mean reward: 2.275 [1.493, 6.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.774, 10.098], loss: 0.092659, mae: 0.297011, mean_q: 3.843717
 96086/100000: episode: 1852, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 187.147, mean reward: 1.871 [1.440, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.362, 10.098], loss: 0.100639, mae: 0.291442, mean_q: 3.851246
 96186/100000: episode: 1853, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 200.037, mean reward: 2.000 [1.499, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.209, 10.338], loss: 0.087053, mae: 0.292681, mean_q: 3.839250
 96286/100000: episode: 1854, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 208.448, mean reward: 2.084 [1.457, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.704, 10.098], loss: 0.090778, mae: 0.294239, mean_q: 3.848168
 96386/100000: episode: 1855, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.467, mean reward: 1.985 [1.477, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.372, 10.282], loss: 0.089624, mae: 0.279845, mean_q: 3.855727
 96486/100000: episode: 1856, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 186.059, mean reward: 1.861 [1.465, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.965, 10.118], loss: 0.163787, mae: 0.320039, mean_q: 3.901103
 96586/100000: episode: 1857, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 175.678, mean reward: 1.757 [1.454, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.090, 10.098], loss: 0.113360, mae: 0.308532, mean_q: 3.889648
 96686/100000: episode: 1858, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 204.710, mean reward: 2.047 [1.483, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.687, 10.098], loss: 0.109436, mae: 0.296473, mean_q: 3.859080
 96786/100000: episode: 1859, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.929, mean reward: 1.979 [1.493, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.724, 10.166], loss: 0.088062, mae: 0.301876, mean_q: 3.856906
 96886/100000: episode: 1860, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 219.136, mean reward: 2.191 [1.483, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.339, 10.176], loss: 0.080207, mae: 0.289206, mean_q: 3.836068
 96986/100000: episode: 1861, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 195.615, mean reward: 1.956 [1.471, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.012, 10.140], loss: 0.117587, mae: 0.310323, mean_q: 3.847817
 97086/100000: episode: 1862, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.572, mean reward: 1.956 [1.476, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.275], loss: 0.108643, mae: 0.305319, mean_q: 3.865597
 97186/100000: episode: 1863, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 196.187, mean reward: 1.962 [1.489, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.327, 10.098], loss: 0.104096, mae: 0.305837, mean_q: 3.860144
 97286/100000: episode: 1864, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 177.048, mean reward: 1.770 [1.438, 2.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.195], loss: 0.094321, mae: 0.304084, mean_q: 3.843284
 97386/100000: episode: 1865, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.040, mean reward: 1.970 [1.494, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.750, 10.098], loss: 0.088213, mae: 0.289990, mean_q: 3.837524
 97486/100000: episode: 1866, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.921, mean reward: 1.889 [1.469, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.018, 10.242], loss: 0.100954, mae: 0.297282, mean_q: 3.823276
 97586/100000: episode: 1867, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.106, mean reward: 1.911 [1.470, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.833, 10.217], loss: 0.088719, mae: 0.288992, mean_q: 3.824625
 97686/100000: episode: 1868, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.489, mean reward: 1.935 [1.505, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.530, 10.098], loss: 0.093469, mae: 0.296979, mean_q: 3.844499
 97786/100000: episode: 1869, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.498, mean reward: 1.975 [1.486, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.414, 10.098], loss: 0.104913, mae: 0.293156, mean_q: 3.832706
 97886/100000: episode: 1870, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.259, mean reward: 1.963 [1.466, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.098], loss: 0.082621, mae: 0.286617, mean_q: 3.822707
 97986/100000: episode: 1871, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.307, mean reward: 1.913 [1.465, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.229, 10.140], loss: 0.094374, mae: 0.306717, mean_q: 3.833331
 98086/100000: episode: 1872, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.982, mean reward: 1.970 [1.455, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.037, 10.098], loss: 0.095204, mae: 0.301109, mean_q: 3.831975
 98186/100000: episode: 1873, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 197.490, mean reward: 1.975 [1.462, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.363, 10.264], loss: 0.082852, mae: 0.294520, mean_q: 3.843429
 98286/100000: episode: 1874, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 206.613, mean reward: 2.066 [1.474, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.098], loss: 0.082775, mae: 0.288921, mean_q: 3.833572
 98386/100000: episode: 1875, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 196.684, mean reward: 1.967 [1.445, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.122], loss: 0.082296, mae: 0.290678, mean_q: 3.846233
 98486/100000: episode: 1876, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.439, mean reward: 2.004 [1.451, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.181, 10.098], loss: 0.090076, mae: 0.303645, mean_q: 3.828462
 98586/100000: episode: 1877, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 179.488, mean reward: 1.795 [1.441, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.585, 10.246], loss: 0.097396, mae: 0.298830, mean_q: 3.858663
 98686/100000: episode: 1878, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 174.983, mean reward: 1.750 [1.452, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.458, 10.261], loss: 0.086691, mae: 0.291526, mean_q: 3.831850
 98786/100000: episode: 1879, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 186.018, mean reward: 1.860 [1.448, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.594, 10.098], loss: 0.104267, mae: 0.303128, mean_q: 3.839600
 98886/100000: episode: 1880, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 213.956, mean reward: 2.140 [1.492, 9.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.105], loss: 0.104756, mae: 0.314698, mean_q: 3.835298
 98986/100000: episode: 1881, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 180.691, mean reward: 1.807 [1.445, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.322, 10.116], loss: 0.103299, mae: 0.300033, mean_q: 3.857172
 99086/100000: episode: 1882, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.426, mean reward: 1.794 [1.475, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.498, 10.290], loss: 0.104258, mae: 0.303573, mean_q: 3.841761
 99186/100000: episode: 1883, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.118, mean reward: 1.881 [1.447, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.180], loss: 0.107118, mae: 0.301071, mean_q: 3.836836
 99286/100000: episode: 1884, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 197.910, mean reward: 1.979 [1.485, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.371, 10.098], loss: 0.125990, mae: 0.310995, mean_q: 3.845794
 99386/100000: episode: 1885, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 180.936, mean reward: 1.809 [1.448, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.589, 10.098], loss: 0.110196, mae: 0.309924, mean_q: 3.848400
 99486/100000: episode: 1886, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 189.080, mean reward: 1.891 [1.458, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.943, 10.306], loss: 0.100071, mae: 0.305389, mean_q: 3.841153
 99586/100000: episode: 1887, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 207.399, mean reward: 2.074 [1.471, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.438, 10.476], loss: 0.095806, mae: 0.294464, mean_q: 3.851155
 99686/100000: episode: 1888, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.492, mean reward: 1.875 [1.451, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.499, 10.098], loss: 0.089339, mae: 0.291558, mean_q: 3.847394
 99786/100000: episode: 1889, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 188.633, mean reward: 1.886 [1.475, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.604, 10.098], loss: 0.103347, mae: 0.297366, mean_q: 3.833513
 99886/100000: episode: 1890, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 176.531, mean reward: 1.765 [1.477, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.346, 10.206], loss: 0.078915, mae: 0.288097, mean_q: 3.844873
 99986/100000: episode: 1891, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.001, mean reward: 1.970 [1.516, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.539, 10.205], loss: 0.096244, mae: 0.290753, mean_q: 3.826728
done, took 590.445 seconds
[Info] End Importance Splitting. Falsification occurred 8 times.
